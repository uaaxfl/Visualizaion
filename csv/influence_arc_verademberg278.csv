2020.coling-demos.3,N04-4028,0,0.0171077,"oceedings of the 27th International Conference on Computational Linguistics, pages 12–17 Barcelona, Spain (Online), December 12, 2020. System DART AlpacaTag (Lin et al., 2019) YEDDA (Yang et al., 2018) Label Types Programming Language Has Label Recommendation ? User Interface Use Active Learning ? Text Tag Tag Python Python Python X X X GUI GUI GUI X X 7 Table 1: A general comparison of relevant GUI-based annotation tools. reduces structured data-to-text annotation efforts by incorporating automatic label suggestion and the uncertainty-based active learning algorithm (Lewis and Catlett, 1994; Culotta and McCallum, 2004). DART serves as a natural complement to downstream data-to-text systems, rather than an end-to-end NLG system. As such, it can assist in the development of both traditional rule-based systems (e.g. (Reiter, 2007)), and the recent neural systems (e.g. (Balakrishnan et al., 2019; Chang et al., 2020; Shen et al., 2020; Hong et al., 2019)). As a lightweight, standalone desktop application, DART can be easily distributed to domain experts and installed on local devices. DART consists of a user-friendly interface that allows experts to iteratively improve the overall corpus quality with partial cor"
2020.coling-demos.3,D19-6310,1,0.777257,"GUI X X 7 Table 1: A general comparison of relevant GUI-based annotation tools. reduces structured data-to-text annotation efforts by incorporating automatic label suggestion and the uncertainty-based active learning algorithm (Lewis and Catlett, 1994; Culotta and McCallum, 2004). DART serves as a natural complement to downstream data-to-text systems, rather than an end-to-end NLG system. As such, it can assist in the development of both traditional rule-based systems (e.g. (Reiter, 2007)), and the recent neural systems (e.g. (Balakrishnan et al., 2019; Chang et al., 2020; Shen et al., 2020; Hong et al., 2019)). As a lightweight, standalone desktop application, DART can be easily distributed to domain experts and installed on local devices. DART consists of a user-friendly interface that allows experts to iteratively improve the overall corpus quality with partial corrections. Overall, the toolkit provides three advantages: (1) It reduces labeling difficulty by automatically providing natural language label recommendations; (2) It efficiently solicits data for which it has low confidence (or high uncertainty) in its generated text to be annotated, so that overall annotation efforts can be reduced;"
2020.coling-demos.3,P19-3010,0,0.0206409,"t al., 2015; Crook et al., 2018) utilize intermediate meaning representation (data) as input to generate natural language sentences. In practice, however, these systems are highly reliant on the use of large-scale labeled data. Each new domain requires additional annotations to pair the new data with matching text. With the rise in development of natural language generation (NLG) systems from structured data, there is also an increased need for annotation tools that reduce labeling time and effort for constructing complex sentence labels. Unlike other labeling tasks, such as sequence tagging (Lin et al., 2019), where the labels are non-complex and correspond to fixed sets of classes, data-to-text generation entails providing complete sentence labels for each data instance. To construct textual description is time-consuming and therefore it can be beneficial for the system to automatically suggest texts and allow the annotators to accept or partially correct them. To this end, we propose to create an interactive annotation tool: Data AnnotatoR Tool (DART1 ) that This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 Demo is availa"
2020.coling-demos.3,W16-6644,0,0.136127,"sentation of d, we simply find the most similar d0 (cosine similarity) in the labeled pool of (di , ti ) pairs, and use its text label ti as the suggestion6 . The sampling process continues until either all data instances are labeled or a satisfactory threshold value is reached for the quality metric on the labeled corpus (as defined in section 2.3). 2.3 Quality Estimator To better manage the annotation process, we include the diversity metrics used in (Balakrishnan et al., 2019): number of unique tokens, number of unique trigrams, Shannon token entropy, conditional bigram entropy. Following (Novikova et al., 2016), we also measure various types of lexical richness including type-token ratio (TTR) and Mean Segmental TTR (MSTTR) (Lu, 2012), where higher values of TTR and MSTTR correspond to more diverse corpus. DART displays these scores on the Status Display as shown in Figure 2. These scores serve as on-the-fly quality estimates that help experts decide when sufficient labels have been collected. 3 User Interface Figure 2: Screenshot of sample annotation interface showing the provided data, and the text box for annotation. On the bottom right, user can select either the statistics or plots indicating t"
2020.coling-demos.3,W17-5525,0,0.208745,"Missing"
2020.coling-demos.3,W07-2315,0,0.0599167,"anguage Has Label Recommendation ? User Interface Use Active Learning ? Text Tag Tag Python Python Python X X X GUI GUI GUI X X 7 Table 1: A general comparison of relevant GUI-based annotation tools. reduces structured data-to-text annotation efforts by incorporating automatic label suggestion and the uncertainty-based active learning algorithm (Lewis and Catlett, 1994; Culotta and McCallum, 2004). DART serves as a natural complement to downstream data-to-text systems, rather than an end-to-end NLG system. As such, it can assist in the development of both traditional rule-based systems (e.g. (Reiter, 2007)), and the recent neural systems (e.g. (Balakrishnan et al., 2019; Chang et al., 2020; Shen et al., 2020; Hong et al., 2019)). As a lightweight, standalone desktop application, DART can be easily distributed to domain experts and installed on local devices. DART consists of a user-friendly interface that allows experts to iteratively improve the overall corpus quality with partial corrections. Overall, the toolkit provides three advantages: (1) It reduces labeling difficulty by automatically providing natural language label recommendations; (2) It efficiently solicits data for which it has low"
2020.coling-demos.3,P16-1162,0,0.00549984,"ctured input, the delimiter (e.g. “ ”) is used to identify the attribute 5 6 Using the implemented version from https://scikit-learn.org There are no suggestions for the initial batch of annotations. 14 Figure 3: Performance comparison between DART’s data sampler, random sampling, and retrieving labels from the full dataset (ALL) on E2E (Left) and the Weather (Right) datasets (42k data instances for E2E and 32k for weather), using the same retrieval method. tags instead. Note that the system supports three granularity of tokenization: (1) word, (2) character, and (3) byte-pair encoding (BPE) (Sennrich et al., 2016). The top half of Figure 2 shows the main annotation page where experts can input constructed sentences into the text boxes based on suggested texts and the provided image (or short clip) 7 . As the expert annotates, the progress bar below the text box indicates when the background uncertainty scorer training session will begin. The bottom half of Figure 2 shows the annotation progress statistics, including the percentage of data types that have been annotated and the quality of overall templates. When a specified number of annotations has been created, experts can download both the annotated"
2020.coling-demos.3,2020.acl-main.641,1,0.722699,"ython X X X GUI GUI GUI X X 7 Table 1: A general comparison of relevant GUI-based annotation tools. reduces structured data-to-text annotation efforts by incorporating automatic label suggestion and the uncertainty-based active learning algorithm (Lewis and Catlett, 1994; Culotta and McCallum, 2004). DART serves as a natural complement to downstream data-to-text systems, rather than an end-to-end NLG system. As such, it can assist in the development of both traditional rule-based systems (e.g. (Reiter, 2007)), and the recent neural systems (e.g. (Balakrishnan et al., 2019; Chang et al., 2020; Shen et al., 2020; Hong et al., 2019)). As a lightweight, standalone desktop application, DART can be easily distributed to domain experts and installed on local devices. DART consists of a user-friendly interface that allows experts to iteratively improve the overall corpus quality with partial corrections. Overall, the toolkit provides three advantages: (1) It reduces labeling difficulty by automatically providing natural language label recommendations; (2) It efficiently solicits data for which it has low confidence (or high uncertainty) in its generated text to be annotated, so that overall annotation effo"
2020.coling-demos.3,D15-1199,0,0.023009,"Missing"
2020.coling-demos.3,W15-4622,0,0.0298303,"area[city centre] Linearizer Data Scorer Raw Text Uncertainty Scorer conﬁdence score __DG_INFORM__ __ARG_EATTYPE_PUB__ eattype_pub __ARG_NAME__ name __ARG_NEAR__ near ( Data, Raw Text ) Update Data Text Expert name Blue Spice eatType coffee shop area city centre Data Sampler Quality Estimator Send to Label Quality Scores ( Data, Text ) Expert Figure 1: Left: Overview of the DART toolkit usage. Right: Diagram of the framework architecture. Past example datasets include Restaurants (Wen et al., 2015) or graph-structure inputs (Balakrishnan et al., 2019). Analogously, most conversation systems (Williams et al., 2015; Crook et al., 2018) utilize intermediate meaning representation (data) as input to generate natural language sentences. In practice, however, these systems are highly reliant on the use of large-scale labeled data. Each new domain requires additional annotations to pair the new data with matching text. With the rise in development of natural language generation (NLG) systems from structured data, there is also an increased need for annotation tools that reduce labeling time and effort for constructing complex sentence labels. Unlike other labeling tasks, such as sequence tagging (Lin et al.,"
2020.coling-demos.3,P18-4006,0,0.0583457,"Missing"
2020.coling-main.212,W19-3405,0,0.0106278,"erent manner. Human evaluation show that our model substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my groceries . i put my groceries in my cart and left . Figure 1: A story about grocery shopping generat"
2020.coling-main.212,N18-1204,0,0.0226861,"s the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my groceries . i put my groceries in my cart and left . Figure 1: A story about grocery shopping generated by Zhai et al. (2019), which is globally coherent but really bo"
2020.coling-main.212,P18-1082,0,0.0297658,"establish global coherence; and (2) a detailer, which supplies relevant details to the story in a locally coherent manner. Human evaluation show that our model substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register"
2020.coling-main.212,P19-1254,0,0.0215458,"herence; and (2) a detailer, which supplies relevant details to the story in a locally coherent manner. Human evaluation show that our model substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my gro"
2020.coling-main.212,D16-1032,0,0.0229484,"substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my groceries . i put my groceries in my cart and left . Figure 1: A story about grocery shopping generated by Zhai et al. (2019), which is globally c"
2020.coling-main.212,N16-1014,0,0.0275405,"ch incorporates little information. This is the main technical challenge of the paper, which we tackle with the following efforts. (1) To select the content of the current segment, we condition the decoding process on its most important context: the previous regular event e− and the successive regular event e+ . Thus at each decoding step t, it produces: qt = Dec(ϕw (tokt−1 ); att(dt−1 , fi ); ϕe (e− ); ϕe (e+ )) (3) Here, Dec(·) is once again a single layer Bi-LSTM. (2) The detailer also adopts the maximum mutual information (MMI) objective. First used in conjunction with SEQ 2 SEQ models by Li et al. (2016), the technique promotes the generation of specific, meaningful texts by moderately suppressing generic language generations. The idea is, instead of maximizing data likelihood, one could maximize the mutual information I(c, s) between the context c and the generation s to promote the correspondence between the two, thus improving the informativeness of the text. The MMI decoding objective generalizes to: s = arg max[log(p(s|c)) − λ · log(p(s))] (4) s which is the maximum likelihood objective minus a language model term, so it is also termed an anti-LM objective. In practice, equivalently, we"
2020.coling-main.212,D15-1166,0,0.0458126,"Missing"
2020.coling-main.212,L16-1555,0,0.320966,"Missing"
2020.coling-main.212,D14-1162,0,0.0880954,"gment by segment, alternating between the decoders to address one event at a time. Thus technically, the neural part of the model receives (1) the input sequence, i.e. segments s = hs1 . . . si−1 i that are already generated and (2) the agenda a = he1 . . . en i that consists of n events in total as input; as the output, it generates the next text segment si which corresponds to the current event ei . Encoder We encode the input sequence in an agenda-aware manner: fi = Enc(ϕw (s<i ); ϕe (e<i )) (1) Here ϕw (·) denotes the word embeddings, which we initialize with pre-trained Glove embeddings (Pennington et al. (2014)); ϕe (·) is the event embeddings, which we initialize randomly. The embedding of each token in the history s<i is concatenated (;) with the embedding ϕe (ei ) of its corresponding event ei so the encoding process is aware of the agenda. Enc(·) is a single layer Bi-LSTM sequence encoder. The outcome fi is a list of vectors, each of which collects the features of its corresponding input token. Outliner The outliner generates the text segment corresponding to the next event ei in the agenda, if it is regular. Here we use a Bi-LSTM sequence decoder which has access to a dot-product attention (Luo"
2020.coling-main.212,N16-3012,1,0.863989,"Missing"
2020.coling-main.212,W19-3404,1,0.663415,"vant details to the story in a locally coherent manner. Human evaluation show that our model substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my groceries . i put my groceries in my cart and left ."
2020.conll-1.34,2020.acl-main.640,0,0.042965,"Missing"
2020.conll-1.34,W05-0909,0,0.17994,"the attention mechanism contribute to the final results. We compare the full model with the following simplified models: VGG global is an seq2seq model using VGG16 (Simonyan and Zisserman, 2015) global features. ResNet global is a seq2seq model using ResNetReference-Based Evaluation We first evaluate our model and ablations using automatic reference-based metrics on the test set to quantify the similarity between the generated stories w.r.t. human-written ones. We use metrics including unigram (B-1), bigram (B-2), trigram (B3), and 4-gram (B-4) BLEU scores (Papineni et al., 2002), METEOR (M; Banerjee and Lavie, 2005), ROUGE-L (R; Lin, 2004), and CIDEr (C; Vedantam et al., 2015), based on Wang et al. (2018b)’s evaluation code. Comparison with baselines. We compare our model with baselines on reference-based metrics in table 1. Our model outperforms all previous methods which do not utilize scene graphs (except SGVST) on BLEU-4 and METEOR. Compared to the recent work using scene graphs, SGVST, we obtain a better score on BLEU-4 and competitive results on BLEU-3, METEOR, and ROUGE-L, although we perform with lower scores on BLEU-1, BLEU-2, and CIDEr. This indicates that the relations between objects in scene"
2020.conll-1.34,N19-1423,0,0.00802912,"available in VIST. Since human written captions refer to salient objects appearing the image, we posit that a relevant story should also refer to these objects as much as possible. Based on this we can quantify the relevance of the generated stories. First, we automatically match the noun phrases in 427 the generated stories with the noun phrases in the corresponding human image captions. The matching is based on the head noun in the noun phrase. We experimented with Lin’s similarity on WordNet synsets (Lin, 1998) and cosine similarity using GloVe and BERT embeddings (Pennington et al., 2014; Devlin et al., 2019). The threshold value for counting a match was optimised to minimise false positives on a set of human annotated matches (number=194) from 10 stories in the validation set. We obtained the highest precision using GloVe embeddings, with a threshold of 0.85 (precision=0.82, recall=0.11). This metric is then computed on our model as well as the baselines. The results in table 5 show that the stories generated by our model have higher matches with entities in human-generated captions. Our scene graph embedding model also outperforms the model using the stronger ResNet features, showing that explic"
2020.conll-1.34,D19-6302,0,0.020368,"node in the graph with a corresponding visual feature vector. This is done by extracting Regions of Interest (RoIs) of each object from the backbone Faster R-CNN model of KERN. Then we apply the RoI align algorithm (He et al., 2017) to extract visual features corresponding to each node. The global node is assigned the mean features of all the nodes in the graph. 3.2 Scene Graph Embedding We employ graph convolution networks (GCN; Kipf and Welling, 2017) to encode our augmented scene graph, since they have been effective in learning representations with graph-like structures like parse trees (Du and Black, 2019) and knowledge graphs (Song et al., 2020). When it comes to the learning of the scene graph encoders, we are inspired by human behaviours in image description task. Objects that appear earlier in image captions usually attract more human attention and are more visually salient to humans (Griffin and Bock, 2000; He et al., 2019). There is a large agreement between human attended regions and activation maps of the last convolutional layer of a VGG-16 network, even though the VGG-16 network is not fine-tuned for captioning (He et al., 2019). If a region of the feature maps is highly activated, it"
2020.conll-1.34,D19-6310,1,0.924128,"s from the VIST dataset (Huang et al., 2016)1 . Visual storytelling is the generation of a coherent narrative from a series of images (Huang et al., 2016). In this paper, we address a particular challenge in visual storytelling: reflecting human preferences in narrative structure, especially the choice of content words and phrases that comprise a readable story. Humans prefer to use diverse words and phrases to construct the storyline to avoid repetitions within or across sentences. For example, in the human-written story in Fig. 1, very few content words are repeated. However, Modi and Parde (2019) have found that recent work often generate repetitive words and phrases which leads to repetitions across sentences and makes stories less diverse. For example, in the first story of Fig. 1, the model generates a verb phrase had a great time and then repeats it in the fifth sentence. These words Typo generated by human: ”have” instead of ”gave”. man Scene graph Introduction 1 table table and phrases are usually overly generic. We argue that this is because relations between objects in the last image are not well-represented in the image embedding, forcing the model to produce generic alternat"
2020.conll-1.34,N16-1147,0,0.161797,"ere . he was very happy to be there . we had a great time . they all had a great time . Ours: i went to the party last week . the chef was preparing the food . he was very happy to see him . they had a great time . after the ceremony was over , everyone gathered together to talk about their plans . Figure 1: Example of extracting scene graphs from images and their relationship to content words and phrases in the stories. The first story (Baseline) is generated by AREL (Wang et al., 2018b). The second story (Ours) is generated by our proposed model. The Human story comes from the VIST dataset (Huang et al., 2016)1 . Visual storytelling is the generation of a coherent narrative from a series of images (Huang et al., 2016). In this paper, we address a particular challenge in visual storytelling: reflecting human preferences in narrative structure, especially the choice of content words and phrases that comprise a readable story. Humans prefer to use diverse words and phrases to construct the storyline to avoid repetitions within or across sentences. For example, in the human-written story in Fig. 1, very few content words are repeated. However, Modi and Parde (2019) have found that recent work often gen"
2020.conll-1.34,P18-1110,0,0.0220375,"Missing"
2020.conll-1.34,W04-1013,0,0.0657081,"the final results. We compare the full model with the following simplified models: VGG global is an seq2seq model using VGG16 (Simonyan and Zisserman, 2015) global features. ResNet global is a seq2seq model using ResNetReference-Based Evaluation We first evaluate our model and ablations using automatic reference-based metrics on the test set to quantify the similarity between the generated stories w.r.t. human-written ones. We use metrics including unigram (B-1), bigram (B-2), trigram (B3), and 4-gram (B-4) BLEU scores (Papineni et al., 2002), METEOR (M; Banerjee and Lavie, 2005), ROUGE-L (R; Lin, 2004), and CIDEr (C; Vedantam et al., 2015), based on Wang et al. (2018b)’s evaluation code. Comparison with baselines. We compare our model with baselines on reference-based metrics in table 1. Our model outperforms all previous methods which do not utilize scene graphs (except SGVST) on BLEU-4 and METEOR. Compared to the recent work using scene graphs, SGVST, we obtain a better score on BLEU-4 and competitive results on BLEU-3, METEOR, and ROUGE-L, although we perform with lower scores on BLEU-1, BLEU-2, and CIDEr. This indicates that the relations between objects in scene graph embeddings empowe"
2020.conll-1.34,D15-1166,0,0.0151858,") (1) u∈N (v) where W ∈ Rd×h is a trainable parameter. N (v) is the set of all neighbours of node v. f is the ReLU non-linearity. “◦” is the Hadamard product, Wdir(e) ∈ Rd×h , dir(e) ∈ {in, out} is the direction of the edge eu,v connecting u and v. re ∈ Rh is an embedding of the label of the edge eu,v . Each layer aggregates the direct neighbours of each node. We stack L GCN layers to encode the full graph. Object label generator. We use a two-layer LSTM (Hochreiter and Schmidhuber, 1997) to merge the node representations and generate the sequence of object labels. We apply global attention (Luong et al., 2015) to re-weight the hidden representations from the first layer and merge them into a global hidden vector h0 G . The we feed the global hidden vector into two-layer feed-forward networks to get the global encoder output hG . The probability of node label yt conditioned on input G and previous node label y1:t−1 is obtained by 423 applying a softmax layer on the decoder output as P (yt |y1:t−1 , G) = sof tmax(g(hG , hC )), where g is a perceptron. Pre-training. The graph-to-sequence model is trained Qt=1 to maximize the likelihood function ll = |Y |P (yt |y1:t−1 , G). We use extracted visual feat"
2020.conll-1.34,W18-6501,0,0.0185991,"objects in the images as well as semantic event relations (actions and their participants). Relations like (man, near, food) in the scene graph in Fig. 1 are essential to generate more specific noun phrases (e.g., the chef ) instead of generic ones (e.g., everyone). In our approach, we extract scene graphs from the images and then learn scene graph embeddings 420 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 420–430 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 using graph neural networks (Marcheggiani and Perez-Beltrachini, 2018) for each image, which combine the visual features and the discrete semantic information from the scene graphs. A combination of story-wide and individual-image scene graph features is then decoded in the form of a story; parameter-sharing in the decoder encourages narrative coherence. One difficulty in learning scene graph embeddings together with an end-to-end visual storytelling model is that they introduce a large number of parameters, increasing both computational and learning complexity and making them more difficult to integrate into larger, computationallyexpensive learning approaches."
2020.conll-1.34,W19-1805,0,0.0111415,"uman story comes from the VIST dataset (Huang et al., 2016)1 . Visual storytelling is the generation of a coherent narrative from a series of images (Huang et al., 2016). In this paper, we address a particular challenge in visual storytelling: reflecting human preferences in narrative structure, especially the choice of content words and phrases that comprise a readable story. Humans prefer to use diverse words and phrases to construct the storyline to avoid repetitions within or across sentences. For example, in the human-written story in Fig. 1, very few content words are repeated. However, Modi and Parde (2019) have found that recent work often generate repetitive words and phrases which leads to repetitions across sentences and makes stories less diverse. For example, in the first story of Fig. 1, the model generates a verb phrase had a great time and then repeats it in the fifth sentence. These words Typo generated by human: ”have” instead of ”gave”. man Scene graph Introduction 1 table table and phrases are usually overly generic. We argue that this is because relations between objects in the last image are not well-represented in the image embedding, forcing the model to produce generic alternat"
2020.conll-1.34,P02-1040,0,0.107001,"ether the scene graph embedding and the attention mechanism contribute to the final results. We compare the full model with the following simplified models: VGG global is an seq2seq model using VGG16 (Simonyan and Zisserman, 2015) global features. ResNet global is a seq2seq model using ResNetReference-Based Evaluation We first evaluate our model and ablations using automatic reference-based metrics on the test set to quantify the similarity between the generated stories w.r.t. human-written ones. We use metrics including unigram (B-1), bigram (B-2), trigram (B3), and 4-gram (B-4) BLEU scores (Papineni et al., 2002), METEOR (M; Banerjee and Lavie, 2005), ROUGE-L (R; Lin, 2004), and CIDEr (C; Vedantam et al., 2015), based on Wang et al. (2018b)’s evaluation code. Comparison with baselines. We compare our model with baselines on reference-based metrics in table 1. Our model outperforms all previous methods which do not utilize scene graphs (except SGVST) on BLEU-4 and METEOR. Compared to the recent work using scene graphs, SGVST, we obtain a better score on BLEU-4 and competitive results on BLEU-3, METEOR, and ROUGE-L, although we perform with lower scores on BLEU-1, BLEU-2, and CIDEr. This indicates that"
2020.conll-1.34,D14-1162,0,0.0986408,"ruth image captions also available in VIST. Since human written captions refer to salient objects appearing the image, we posit that a relevant story should also refer to these objects as much as possible. Based on this we can quantify the relevance of the generated stories. First, we automatically match the noun phrases in 427 the generated stories with the noun phrases in the corresponding human image captions. The matching is based on the head noun in the noun phrase. We experimented with Lin’s similarity on WordNet synsets (Lin, 1998) and cosine similarity using GloVe and BERT embeddings (Pennington et al., 2014; Devlin et al., 2019). The threshold value for counting a match was optimised to minimise false positives on a set of human annotated matches (number=194) from 10 stories in the validation set. We obtained the highest precision using GloVe embeddings, with a threshold of 0.85 (precision=0.82, recall=0.11). This metric is then computed on our model as well as the baselines. The results in table 5 show that the stories generated by our model have higher matches with entities in human-generated captions. Our scene graph embedding model also outperforms the model using the stronger ResNet feature"
2020.conll-1.34,P18-1083,0,0.203512,"a jacket to show their appreciation. Baseline: everyone was excited for the ceremony to begin . everyone was excited to be there . he was very happy to be there . we had a great time . they all had a great time . Ours: i went to the party last week . the chef was preparing the food . he was very happy to see him . they had a great time . after the ceremony was over , everyone gathered together to talk about their plans . Figure 1: Example of extracting scene graphs from images and their relationship to content words and phrases in the stories. The first story (Baseline) is generated by AREL (Wang et al., 2018b). The second story (Ours) is generated by our proposed model. The Human story comes from the VIST dataset (Huang et al., 2016)1 . Visual storytelling is the generation of a coherent narrative from a series of images (Huang et al., 2016). In this paper, we address a particular challenge in visual storytelling: reflecting human preferences in narrative structure, especially the choice of content words and phrases that comprise a readable story. Humans prefer to use diverse words and phrases to construct the storyline to avoid repetitions within or across sentences. For example, in the human-wr"
2020.conll-1.34,2020.acl-main.712,0,0.0340554,"sual feature vector. This is done by extracting Regions of Interest (RoIs) of each object from the backbone Faster R-CNN model of KERN. Then we apply the RoI align algorithm (He et al., 2017) to extract visual features corresponding to each node. The global node is assigned the mean features of all the nodes in the graph. 3.2 Scene Graph Embedding We employ graph convolution networks (GCN; Kipf and Welling, 2017) to encode our augmented scene graph, since they have been effective in learning representations with graph-like structures like parse trees (Du and Black, 2019) and knowledge graphs (Song et al., 2020). When it comes to the learning of the scene graph encoders, we are inspired by human behaviours in image description task. Objects that appear earlier in image captions usually attract more human attention and are more visually salient to humans (Griffin and Bock, 2000; He et al., 2019). There is a large agreement between human attended regions and activation maps of the last convolutional layer of a VGG-16 network, even though the VGG-16 network is not fine-tuned for captioning (He et al., 2019). If a region of the feature maps is highly activated, it is very likely to be classified as an ob"
2020.conll-1.34,D17-1101,0,0.0470232,"Missing"
2020.conll-1.34,D19-6405,0,0.014337,"els to extract scene graph representations from images (Zellers et al., 2018; Chen et al., 2019). These scene graph represenations have proven effective on various tasks like image retrieval (Johnson et al., 2015) and image generation (Johnson et al., 2018). Scene graph based image captioning. A sequential scene graph representation is used to encode images in Gao et al. (2018) to improve image captioning. Yang et al. (2019b) propose auto-encoding text-based scene graphs to learn a shared dictionary between visual and text based graphs, achieving state-of-the-art image captioning performance. Wang et al. (2019b) show that image scene graphs 421 extracted using a trained model can match the captioning performance of an oracle with access to ground-truth graphs. Aligning text- and imagebased scene graphs has also been used to generate image captions without paired data (Gu et al., 2019). 3 Model Design The task of visual storytelling can be decomposed into two distinct parts: (1) extracting relevant information from input images I into compact features and (2) generating stories using these visual features. We improve the visual feature representation by switching from commonly-used global feature ve"
2021.acl-short.116,N19-1423,0,0.0608996,"Missing"
2021.acl-short.116,D14-1168,0,0.0643279,"Missing"
2021.acl-short.116,P14-1065,0,0.0787935,"Missing"
2021.acl-short.116,P14-1092,0,0.0811962,"Missing"
2021.acl-short.116,C18-1049,0,0.0179431,"sions instead of using simple majority voting, and by moving to transformer architectures. Our second approach addresses these points by employing a transformer architecture which can take the SPO triple information into account for more richly encoding the relational arguments. 4 Figure 2: The structure of K-BERT. It is equipped with an editable knowledge graph which can be adapted to its application domain. Picture taken from Liu et al. (2020). DRC with an entity-augmented transformer Integrating external domain-specific knowledge into the model is beneficial for this task has been found by Kishimoto et al. (2018), who integrated the ConceptNet relations as additional knowledge into the LSTM network and achieved better performance on the PDTB. We here aim to explore whether model performance can be further improved by exploiting richer entity representations in specialized texts 928 like the biomedical domain. The pipeline with softmatching proposed in the above section provides us with SPO triples from related documents for each implicit relation instance in the test set. We here employ the recently proposed Knowledgeenabled Language Representation model (Liu et al., 2020, K-BERT) to integrate the ext"
2021.acl-short.116,P02-1047,0,0.547602,"Missing"
2021.acl-short.116,N18-1202,0,0.0204985,"Missing"
2021.acl-short.116,prasad-etal-2008-penn,0,0.0475193,"olves automatically inferring the logical link between different text segments (such as causal, contrastive, temporal etc.). It has been shown to be a valuable preprocessing step to many downstream natural language processing tasks such as machine translation (Guzm´an et al., 2014; Meyer et al., 2015), text summarization (Gerani et al., 2014) and questionanswering (Jansen et al., 2014). A main obstacle to a wider usage of automatic DR classifiers however lies in getting the classifiers to work reliably on domains other than the WSJ, that discourse relation parsers are usually trained on PDTB (Prasad et al., 2008) and RST (Carlson et al., 2003). Moving to a different domain is particularly challenging in DRC because the overall distribution of relations typically differs between domains, and because many of the content words that classifiers may rely on are very different between domains. We here focus on the most challenging subtask of implicit discourse relation classification, which involves classifying those relations that are not linked by any explicit connectives like “because” or “but”. In order to correctly recognize implicit relations, the classifier needs to recognize subtle surface cues (whi"
2021.acl-short.116,P17-1093,0,0.0534333,"Missing"
2021.acl-short.116,E17-2024,1,0.893076,"Missing"
2021.acl-short.116,W19-0416,1,0.913106,"relations, the classifier needs to recognize subtle surface cues (which may differ between domains) and learn about typical content-related relations. For instance, from the example “it’s hot outside, therefore I’d like to eat an icecream”, the words “hot outside” and “icecream” are relevant cues for the relation. An overview of typical cues for determining a coherence relation is provided in Das and Taboada (2018). The key to improving automatic DRC on a new domain hence consists of better encoding of the discourse relational arguments. As we will show below (in line with earlier findings by Shi and Demberg, 2019b), it makes a big difference to have at least a small amount of in-domain discourse annotated data. We here explore DRC on the biomedical domain, which seems particularly suitable because a discourse-annotated corpus is available (BioDRB; Prasad et al., 2011), which we can use for evaluation, as well as a setting with a small amount of indomain training data. Furthermore, the biomedical domain does have large raw text corpora available. An example instance from BioDRB (Prasad et al., 2011) is shown below: 1. [These abnormalities in active RA are thought to be induced mainly after chronic expo"
2021.acl-short.116,D19-1586,1,0.841458,"relations, the classifier needs to recognize subtle surface cues (which may differ between domains) and learn about typical content-related relations. For instance, from the example “it’s hot outside, therefore I’d like to eat an icecream”, the words “hot outside” and “icecream” are relevant cues for the relation. An overview of typical cues for determining a coherence relation is provided in Das and Taboada (2018). The key to improving automatic DRC on a new domain hence consists of better encoding of the discourse relational arguments. As we will show below (in line with earlier findings by Shi and Demberg, 2019b), it makes a big difference to have at least a small amount of in-domain discourse annotated data. We here explore DRC on the biomedical domain, which seems particularly suitable because a discourse-annotated corpus is available (BioDRB; Prasad et al., 2011), which we can use for evaluation, as well as a setting with a small amount of indomain training data. Furthermore, the biomedical domain does have large raw text corpora available. An example instance from BioDRB (Prasad et al., 2011) is shown below: 1. [These abnormalities in active RA are thought to be induced mainly after chronic expo"
2021.acl-short.116,I17-1049,1,0.890441,"Missing"
2021.acl-short.2,2021.eacl-main.69,1,0.74817,"rform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available at https://gitlab.com/erniecyc/ few-selector. We hope that this work will call for more attention on this largely unexplored area. 1 L Few-shot learning Figure 1: Training scenario: U represents unlabeled data and L indicates labeled instances. The annotation budget only allows selecting K data for annotating the reference text. Sch¨utze, 2020a; Zhang et al., 2020; Kale, 2020; Chang et al., 2020, 2021b; Li and Liang, 2021; Chang et al., 2021a). However, all the previous works simulate the few-shot scenario by randomly sampling a subset from the full training data. Little to no attention has been paid to the selection strategies. In this work, we present a preliminary study at searching for an optimal strategy to select the fewshot training instances. Studying the selection strategy is motivated by two rationales. First, random sampling leads to a large variance of model performance (Zhang et al., 2020; Schick and Sch¨utze, 2020a,b). Yet current works sample their own training data which makes it difficult to compare across differ"
2021.acl-short.2,2021.eacl-main.64,1,0.758589,"rform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available at https://gitlab.com/erniecyc/ few-selector. We hope that this work will call for more attention on this largely unexplored area. 1 L Few-shot learning Figure 1: Training scenario: U represents unlabeled data and L indicates labeled instances. The annotation budget only allows selecting K data for annotating the reference text. Sch¨utze, 2020a; Zhang et al., 2020; Kale, 2020; Chang et al., 2020, 2021b; Li and Liang, 2021; Chang et al., 2021a). However, all the previous works simulate the few-shot scenario by randomly sampling a subset from the full training data. Little to no attention has been paid to the selection strategies. In this work, we present a preliminary study at searching for an optimal strategy to select the fewshot training instances. Studying the selection strategy is motivated by two rationales. First, random sampling leads to a large variance of model performance (Zhang et al., 2020; Schick and Sch¨utze, 2020a,b). Yet current works sample their own training data which makes it difficult to compare across differ"
2021.acl-short.2,C18-1154,0,0.0333479,"Missing"
2021.acl-short.2,2020.acl-main.18,0,0.0787025,"dget only allows annotating a limited amount of data. Studying the optimal selection strategy can help make the most use of our annotation budget. Specifically, we focus on Introduction Few-shot text generation is an important research topic since obtaining large-scale training data for each individual downstream task is prohibitively expensive. Recently, pretraining large neural networks with a language modeling objective has led to significant improvement across different fewshot text generation tasks (Radford et al., 2019; Lewis et al., 2020) and many techniques are proposed based on them (Chen et al., 2020; Schick and ∗ Selecting k Samples Few-shot learning Equal contribution. X.shen is now at Amazon Alexa AI. 8 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 8–13 August 1–6, 2021. ©2021 Association for Computational Linguistics the label-free setting where the selection can only condition on the unannotated data. Although leveraging the reference text may benefit the selection strategy, it conflicts with the realistic setting where we need to first select the"
2021.acl-short.2,2020.inlg-1.14,0,0.0739021,"ng-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available at https://gitlab.com/erniecyc/ few-selector. We hope that this work will call for more attention on this largely unexplored area. 1 L Few-shot learning Figure 1: Training scenario: U represents unlabeled data and L indicates labeled instances. The annotation budget only allows selecting K data for annotating the reference text. Sch¨utze, 2020a; Zhang et al., 2020; Kale, 2020; Chang et al., 2020, 2021b; Li and Liang, 2021; Chang et al., 2021a). However, all the previous works simulate the few-shot scenario by randomly sampling a subset from the full training data. Little to no attention has been paid to the selection strategies. In this work, we present a preliminary study at searching for an optimal strategy to select the fewshot training instances. Studying the selection strategy is motivated by two rationales. First, random sampling leads to a large variance of model performance (Zhang et al., 2020; Schick and Sch¨utze, 2020a,b). Yet current works sample their"
2021.acl-short.2,Q17-1010,0,0.0153945,"able 1: Comparisons of random sampling, within-cluster random sampling (IC-Random) and K-means selection on the E2E, CNNDM, and SQUAD corpus (BLEU-4 reported). Embedding BART BART-FT GloVe FastText E2E Mean Sum 26.28 25.59 26.46 26.32 25.18 23.36 27.13 24.85 CNNDM Mean Sum 34.30 34.46 36.31 34.18 33.59 31.45 33.23 34.30 SQUAD Mean Sum 8.89 8.56 9.55 8.12 7.99 7.56 9.33 9.42 of the different embedding methods. Apart from the finetuned Bart, we compare with embeddings extracted from (1) Bart without being finetuned on the task-specific data, (2) Glove (Pennington et al., 2014) and (3) FastText (Bojanowski et al., 2017), both finetuned on the task-specific data. For each embedding method, we compare using mean pooling and sum pooling to extract the final vector representation. The results show that finetuned Bart generally outperforms the other embedding choices. We attribute this to the similarity in the embedding space between selection with BART embeddings and and the BART generation model. Moreover, FastText offers a strong baseline as it does relatively well on two scenarios in E2E and SQUAD respectively. Further, we observe that mean pooling is generally better than the sum of word vectors, which is al"
2021.acl-short.2,2020.coling-demos.3,1,0.697396,"roach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available at https://gitlab.com/erniecyc/ few-selector. We hope that this work will call for more attention on this largely unexplored area. 1 L Few-shot learning Figure 1: Training scenario: U represents unlabeled data and L indicates labeled instances. The annotation budget only allows selecting K data for annotating the reference text. Sch¨utze, 2020a; Zhang et al., 2020; Kale, 2020; Chang et al., 2020, 2021b; Li and Liang, 2021; Chang et al., 2021a). However, all the previous works simulate the few-shot scenario by randomly sampling a subset from the full training data. Little to no attention has been paid to the selection strategies. In this work, we present a preliminary study at searching for an optimal strategy to select the fewshot training instances. Studying the selection strategy is motivated by two rationales. First, random sampling leads to a large variance of model performance (Zhang et al., 2020; Schick and Sch¨utze, 2020a,b). Yet current works sample their own training data wh"
2021.acl-short.2,2020.acl-main.703,0,0.289389,"ypical training scenario for text generation where the annotation budget only allows annotating a limited amount of data. Studying the optimal selection strategy can help make the most use of our annotation budget. Specifically, we focus on Introduction Few-shot text generation is an important research topic since obtaining large-scale training data for each individual downstream task is prohibitively expensive. Recently, pretraining large neural networks with a language modeling objective has led to significant improvement across different fewshot text generation tasks (Radford et al., 2019; Lewis et al., 2020) and many techniques are proposed based on them (Chen et al., 2020; Schick and ∗ Selecting k Samples Few-shot learning Equal contribution. X.shen is now at Amazon Alexa AI. 8 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 8–13 August 1–6, 2021. ©2021 Association for Computational Linguistics the label-free setting where the selection can only condition on the unannotated data. Although leveraging the reference text may benefit the selection strategy, it conf"
2021.acl-short.2,2021.acl-long.353,0,0.0300241,"Missing"
2021.acl-short.2,W17-5525,0,0.162681,"Missing"
2021.acl-short.2,D14-1162,0,0.0854483,"7 4.77±3.61 7.65±3.61 6.97±2.55 9.33±2.15 Table 1: Comparisons of random sampling, within-cluster random sampling (IC-Random) and K-means selection on the E2E, CNNDM, and SQUAD corpus (BLEU-4 reported). Embedding BART BART-FT GloVe FastText E2E Mean Sum 26.28 25.59 26.46 26.32 25.18 23.36 27.13 24.85 CNNDM Mean Sum 34.30 34.46 36.31 34.18 33.59 31.45 33.23 34.30 SQUAD Mean Sum 8.89 8.56 9.55 8.12 7.99 7.56 9.33 9.42 of the different embedding methods. Apart from the finetuned Bart, we compare with embeddings extracted from (1) Bart without being finetuned on the task-specific data, (2) Glove (Pennington et al., 2014) and (3) FastText (Bojanowski et al., 2017), both finetuned on the task-specific data. For each embedding method, we compare using mean pooling and sum pooling to extract the final vector representation. The results show that finetuned Bart generally outperforms the other embedding choices. We attribute this to the similarity in the embedding space between selection with BART embeddings and and the BART generation model. Moreover, FastText offers a strong baseline as it does relatively well on two scenarios in E2E and SQUAD respectively. Further, we observe that mean pooling is generally bette"
2021.acl-short.2,D16-1264,0,0.02792,"uster sampling. For K-means selection, the variance comes from the choice of initial center points. We can see random sampling and within-cluster random sampling have a very large variance of up to 7.12 for K = 100. This further suggests that comparing few-shot models based on random sampling can be be prone to variability and prevent drawing reliable conclusions. K-means-based selection, on 2. Document Summarization: We use the CNN/Dailymail dataset (non-anonymized version) (Hermann et al., 2015) which contains 312,084 document-summary pairs. 3. Question generation: We use the SQuAD dataset (Rajpurkar et al., 2016) with over 100k questions. Following Du et al. (2017), we focus on the answer-independent scenario to directly generate questions from passages. 10 E2E Random IC-Random K-means CNNDM SQUAD 10 50 100 10 50 100 10 50 100 4.38±7.12 2.15±4.58 6.22±2.33 11.57±4.29 9.80±2.62 11.89±1.39 26.22±2.58 24.71±2.71 27.13±2.22 13.51±6.47 12.30±3.89 14.28±2.35 24.81±3.77 24.71±2.45 25.19±3.28 35.24±2.89 33.29±1.92 36.31±1.08 1.23±6.22 1.34±3.23 1.56±2.34 3.33±5.89 1.79±3.77 4.77±3.61 7.65±3.61 6.97±2.55 9.33±2.15 Table 1: Comparisons of random sampling, within-cluster random sampling (IC-Random) and K-means s"
2021.acl-short.2,2021.naacl-main.185,0,0.154037,"Missing"
2021.codi-main.7,2020.inlg-1.8,0,0.0422177,"12) deploy word pairs as well as a set of linguistically motivated features. More recently, Qin et al. (2017); Shi and Demberg (2019) and Kurfalı and Östling (2021) have used connective prediction for implicit relations as a secondary or adversarial task to improve discourse relation classification. The current study tackles a different task than these studies, where the relation label is assumed not to be available. Instead, we assume that the generation system knows what relation should be conveyed, and the remaining problem is the lexicalization using a connective. Another related study is Ko and Li (2020), who analyse the discourse abilities of GPT-2 and find 73 connectives. In addition, we test whether a language model (here, GPT-2, Radford et al., 2019) can select the appropriate connective from the same set of options also given to the human participants. The preferred connective is chosen based on the cross entropy loss of the language model. If the connective with the highest probability based on the language model is identical to the preferred choice by humans, this suggests that a language model is sufficient for generating an appropriate connective for a given discourse relation. If, o"
2021.codi-main.7,2021.unimplicit-1.1,0,0.0236257,"e. On the other hand, if the choices made by the crowdworkers are evenly distributed per instance, it indicates that the discourse relation instance in question could be interchangeably marked by alternative There has been continuing interest in the task of connective prediction in recent years, but mostly as an auxiliary task to coherence relation classification. Zhou et al. (2010) use a language model for the task of connective prediction and Xu et al. (2012) deploy word pairs as well as a set of linguistically motivated features. More recently, Qin et al. (2017); Shi and Demberg (2019) and Kurfalı and Östling (2021) have used connective prediction for implicit relations as a secondary or adversarial task to improve discourse relation classification. The current study tackles a different task than these studies, where the relation label is assumed not to be available. Instead, we assume that the generation system knows what relation should be conveyed, and the remaining problem is the lexicalization using a connective. Another related study is Ko and Li (2020), who analyse the discourse abilities of GPT-2 and find 73 connectives. In addition, we test whether a language model (here, GPT-2, Radford et al.,"
2021.codi-main.7,L18-1260,0,0.0279789,"Missing"
2021.codi-main.7,C10-2172,0,0.0214388,"choices of connectives made by the crowdworkers reproduce the original connective in the data, it indicates that this particular connective should be preferred over the other alternatives for that specific discourse relation instance. On the other hand, if the choices made by the crowdworkers are evenly distributed per instance, it indicates that the discourse relation instance in question could be interchangeably marked by alternative There has been continuing interest in the task of connective prediction in recent years, but mostly as an auxiliary task to coherence relation classification. Zhou et al. (2010) use a language model for the task of connective prediction and Xu et al. (2012) deploy word pairs as well as a set of linguistically motivated features. More recently, Qin et al. (2017); Shi and Demberg (2019) and Kurfalı and Östling (2021) have used connective prediction for implicit relations as a secondary or adversarial task to improve discourse relation classification. The current study tackles a different task than these studies, where the relation label is assumed not to be available. Instead, we assume that the generation system knows what relation should be conveyed, and the remainin"
2021.codi-main.7,W18-4710,0,0.0442184,"le this is representative of distributions found in natural language, it also means that we have very few observations for some relation types. We thus ran an additional study using the same setup on a balanced sample, in order to assess the preference of connectives for less frequent relations. Data We used discourse-annotated data where each explicit connective is labelled with a discourse relation type. Two datasets were used: 1) the complete English part of the TED Multilingual Discourse Bank (TED-MDB) (Zeyrek et al., 2019); 2) a balanced sample of the Penn Discourse Treebank 3.0 (PDTB-3; Prasad et al., 2018). The balanced sample from PDTB-3 consists of the 12 most frequent explicit relation types in PDTB-3. For each of these, we selected approximately 18 items per relation, resulting in a set of 206 items. We furthermore balanced the number of items marked by different connectives for each relation type. For instance, the A RG 2- AS - DENIER relation can be marked using a range of different connectives, including but, however, though, still and yet as the most frequent ones. We selected the most common connectives with respect to the relation type and included a similar number of instances of rel"
2021.codi-main.7,N16-3012,1,0.824865,"Missing"
2021.codi-main.7,P17-1093,0,0.0207272,"s for that specific discourse relation instance. On the other hand, if the choices made by the crowdworkers are evenly distributed per instance, it indicates that the discourse relation instance in question could be interchangeably marked by alternative There has been continuing interest in the task of connective prediction in recent years, but mostly as an auxiliary task to coherence relation classification. Zhou et al. (2010) use a language model for the task of connective prediction and Xu et al. (2012) deploy word pairs as well as a set of linguistically motivated features. More recently, Qin et al. (2017); Shi and Demberg (2019) and Kurfalı and Östling (2021) have used connective prediction for implicit relations as a secondary or adversarial task to improve discourse relation classification. The current study tackles a different task than these studies, where the relation label is assumed not to be available. Instead, we assume that the generation system knows what relation should be conveyed, and the remaining problem is the lexicalization using a connective. Another related study is Ko and Li (2020), who analyse the discourse abilities of GPT-2 and find 73 connectives. In addition, we test"
2021.codi-main.7,W19-0416,1,0.76486,"discourse relation instance. On the other hand, if the choices made by the crowdworkers are evenly distributed per instance, it indicates that the discourse relation instance in question could be interchangeably marked by alternative There has been continuing interest in the task of connective prediction in recent years, but mostly as an auxiliary task to coherence relation classification. Zhou et al. (2010) use a language model for the task of connective prediction and Xu et al. (2012) deploy word pairs as well as a set of linguistically motivated features. More recently, Qin et al. (2017); Shi and Demberg (2019) and Kurfalı and Östling (2021) have used connective prediction for implicit relations as a secondary or adversarial task to improve discourse relation classification. The current study tackles a different task than these studies, where the relation label is assumed not to be available. Instead, we assume that the generation system knows what relation should be conveyed, and the remaining problem is the lexicalization using a connective. Another related study is Ko and Li (2020), who analyse the discourse abilities of GPT-2 and find 73 connectives. In addition, we test whether a language model"
2021.codi-main.8,2020.codi-1.15,0,0.0319112,"ction 3. Section 4 describes the method used to identify the connectives in Pidgin. We elaborate on the NaijaLex lexicon that we created using this method in Section 5 and evaluate our method in Section 6. 2 2.1 Related work Lexicon creation Connective inventories have been developed for various languages, including German (Stede and Umbach, 1998), French (Roze et al., 2012), Chinese (Zhou and Xue, 2015) and English (Das et al., 2018), among others (see also Stede et al., 2018). Recently, these efforts have been extended with several multi-lingual connective databases (Bourgonje et al., 2017; Kurfalı et al., 2020). One challenging aspect about constructing a connective lexicon for a new language is defining the category of connectives. Traditionally, the PDTB (Prasad et al., 2008) has placed strong syntactic restrictions on which lexical items can function as connectives, considering only subordinating and coordinating conjunctions and adverbials. Stede et al. (2018) point out that for multilingual lexicons, such a strict definition is not feasible, as languages differ highly in the way coherence relations are expressed. Still, Stede et al. (2018) restrict the class of connectives syntactically, by pos"
2021.codi-main.8,C14-1058,0,0.876024,"c parsers is that a discourse connective often also has a non-connective 85 lel text corpora to find Dutch connectives and subsequently manually checked and annotated them for relation sense. However, this approach does not automatically distinguish between connective and non-connective usages of connectives in the source languages, which always needs to be resolved manually. The current research attempts to resolve this by using a shallow discourse parser to identify connectives in the translation text automatically. nectives in the parallel text. This was, however, not taken into account by Laali and Kosseim (2014). The present research therefore evaluates and extends their methodology for lexicon creation with a parallel corpus. 3 Nigerian Pidgin Nigerian Pidgin is a contact language spoken all over Nigeria. Although officially a pidgin, it is also often considered a creole (e.g., Courtin et al., 2018), as it is a relatively stable language with a fullydeveloped vocabulary and grammar. In addition to approximately 5 million native speakers, there are estimated to be almost 100 million speakers of Nigerian Pidgin as a second language. It is often also referred to simply as ‘Pidgin’ by Nigerian Pidgin sp"
2021.codi-main.8,W19-7803,0,0.134746,"ctives found in the source language by only taking into account the connectives in the translation text. We will therefore also assess how well the explicit connectives in the translated text reflect the discourse relations in the source text. Nigerian Pidgin is also influenced by other European languages (e.g. Portuguese, illustrated by sabi in the example above) as well as various indigenous languages it has been in contact with, such as Hausa, Igbo and Yoruba. Nigerian Pidgin is characterized by focus constructions (see e.g. me in (1)), which are often indicated with the focus particle na (Caron et al., 2019). In addition, serial verb constructions are typical for Nigerian Pidgin. As a result, coordinating conjunctions are less frequent in Nigerian Pidgin than in English (Courtin et al., 2018). An example of such a serial verb construction is provided in Example (2), which has been translated in English with the coordinating conjunction and. This suggests that discourse relations might be expressed differently in Nigerian Pidgin. (2) Con dey hustle, con hustle money go give am. Then we hustled, got money and went to give it to her. Finally, connective lexicons contain information about the relatio"
2021.codi-main.8,W19-2715,0,0.0525334,"Missing"
2021.codi-main.8,P09-2004,0,0.333331,"information to source connectives is to use parallel corpora. Versley (2010) projected discourse connectives across an English-German parallel corpus, but found that simple word alignment is not enough to reliably identify connectives in text and that performance increased when bootstrapping using an established connective list. Bourgonje et al. (2018) combined a German connective lexicon with paralConnective identification In the last decade, shallow discourse parsing of explicit connectives has received considerable attention in the computational discourse community (e.g. Lin et al., 2014; Pitler and Nenkova, 2009; Wang and Lan, 2015; Zeldes et al., 2019). One challenge for such automatic parsers is that a discourse connective often also has a non-connective 85 lel text corpora to find Dutch connectives and subsequently manually checked and annotated them for relation sense. However, this approach does not automatically distinguish between connective and non-connective usages of connectives in the source languages, which always needs to be resolved manually. The current research attempts to resolve this by using a shallow discourse parser to identify connectives in the translation text automatically. n"
2021.codi-main.8,prasad-etal-2008-penn,0,0.904469,"eir relation senses and develop a discourse-annotated corpus in a low-resource language. Connectives and their relation senses were extracted from a parallel corpus combining automatic (PDTB end-to-end parser) and manual annotations. This resulted in Naija-Lex, a lexicon of discourse connectives in Nigerian Pidgin with English translations. The lexicon shows that the majority of Nigerian Pidgin connectives are borrowed from its English lexifier, but that there are also some connectives that are unique to Nigerian Pidgin. 1 Introduction Resources such as discourse-annotated corpora (e.g. PDTB, Prasad et al., 2008) and connective lexicons (e.g. DimLEX, Stede and Umbach, 1998) have proven highly valuable in investigating discourse structure and coherence marking, but they are only available for a limited number of languages. One promising way to expand these resources is to project annotations from languages for which such resources are available to underresourced languages. The current paper sets out to investigate how automatic discourse parsing can be combined with manual annotation to construct a connective lexicon in a low-resource language. We apply this method to create Naija-Lex, a lexicon of Nig"
2021.codi-main.8,W18-5042,0,0.27411,"y on an out-of-domain translated text. Section 2 discusses earlier work on automatic discourse annotation and lexicon construction, after which Nigerian Pidgin is introduced in Section 3. Section 4 describes the method used to identify the connectives in Pidgin. We elaborate on the NaijaLex lexicon that we created using this method in Section 5 and evaluate our method in Section 6. 2 2.1 Related work Lexicon creation Connective inventories have been developed for various languages, including German (Stede and Umbach, 1998), French (Roze et al., 2012), Chinese (Zhou and Xue, 2015) and English (Das et al., 2018), among others (see also Stede et al., 2018). Recently, these efforts have been extended with several multi-lingual connective databases (Bourgonje et al., 2017; Kurfalı et al., 2020). One challenging aspect about constructing a connective lexicon for a new language is defining the category of connectives. Traditionally, the PDTB (Prasad et al., 2008) has placed strong syntactic restrictions on which lexical items can function as connectives, considering only subordinating and coordinating conjunctions and adverbials. Stede et al. (2018) point out that for multilingual lexicons, such a strict"
2021.codi-main.8,2020.lrec-1.138,0,0.0303375,"for this reason. Similar to certain other lexicons (e.g. LDM-PT, Mendes and Lejeune, 2016), the lexicon we present here will maintain less strict requirements on the syntactic category and modifiability of the lexical item. In this way, we can allow for more variation that might be present in a relatively young language and obtain a more complete overview of discourse marking in Nigerian Pidgin. 2.2 2.3 Connective projection Several previous studies have used research available on connectives in one language (mostly English) to identify connectives in another language. Zhou and Xue (2012) and Das et al. (2020) have used bilingual dictionaries to translate English connectives to Chinese and Bangla respectively as part of identifying connectives in these target languages to construct a lexicon. However, no dictionaries are available for Nigerian Pidgin. Another line of research in using cross-lingual information to source connectives is to use parallel corpora. Versley (2010) projected discourse connectives across an English-German parallel corpus, but found that simple word alignment is not enough to reliably identify connectives in text and that performance increased when bootstrapping using an est"
2021.codi-main.8,L16-1165,1,0.881507,"Missing"
2021.codi-main.8,2020.codi-1.7,0,0.39675,"Missing"
2021.codi-main.8,W19-2713,0,0.0138039,"arallel corpora. Versley (2010) projected discourse connectives across an English-German parallel corpus, but found that simple word alignment is not enough to reliably identify connectives in text and that performance increased when bootstrapping using an established connective list. Bourgonje et al. (2018) combined a German connective lexicon with paralConnective identification In the last decade, shallow discourse parsing of explicit connectives has received considerable attention in the computational discourse community (e.g. Lin et al., 2014; Pitler and Nenkova, 2009; Wang and Lan, 2015; Zeldes et al., 2019). One challenge for such automatic parsers is that a discourse connective often also has a non-connective 85 lel text corpora to find Dutch connectives and subsequently manually checked and annotated them for relation sense. However, this approach does not automatically distinguish between connective and non-connective usages of connectives in the source languages, which always needs to be resolved manually. The current research attempts to resolve this by using a shallow discourse parser to identify connectives in the translation text automatically. nectives in the parallel text. This was, ho"
2021.codi-main.8,2021.codi-main.9,1,0.689645,"e detection task (Zeldes et al., 2019) has yielded some high performing systems in other languages as well (Muller et al., 2019; Yu et al., 2019). However, these parsers still require training data, which is not available for low-resource languages. Using parallel corpora might therefore be a promising method to identify connectives and then project their annotations to a low-resource language. One possible difficulty is that these corpora might be from different domains than what the parsers have been trained on, which leads to lower performance (Ramesh and Yu, 2010; Knaebel and Stede, 2020; Scholman et al., 2021). We will use the PDTB end-to-end parser (Lin et al., 2014), as it has been shown to perform better on spontaneous spoken data than other parsers (Scholman et al., 2021). However, performance of the parser drops considerably when comparing spoken language with the newspaper text data it was trained on. We will therefore also assess its reliability on an out-of-domain translated text. Section 2 discusses earlier work on automatic discourse annotation and lexicon construction, after which Nigerian Pidgin is introduced in Section 3. Section 4 describes the method used to identify the connectives"
2021.codi-main.8,P12-1008,0,0.0177524,"erbs should be excluded for this reason. Similar to certain other lexicons (e.g. LDM-PT, Mendes and Lejeune, 2016), the lexicon we present here will maintain less strict requirements on the syntactic category and modifiability of the lexical item. In this way, we can allow for more variation that might be present in a relatively young language and obtain a more complete overview of discourse marking in Nigerian Pidgin. 2.2 2.3 Connective projection Several previous studies have used research available on connectives in one language (mostly English) to identify connectives in another language. Zhou and Xue (2012) and Das et al. (2020) have used bilingual dictionaries to translate English connectives to Chinese and Bangla respectively as part of identifying connectives in these target languages to construct a lexicon. However, no dictionaries are available for Nigerian Pidgin. Another line of research in using cross-lingual information to source connectives is to use parallel corpora. Versley (2010) projected discourse connectives across an English-German parallel corpus, but found that simple word alignment is not enough to reliably identify connectives in text and that performance increased when boot"
2021.codi-main.8,P98-2202,0,0.725786,"s in a low-resource language. Connectives and their relation senses were extracted from a parallel corpus combining automatic (PDTB end-to-end parser) and manual annotations. This resulted in Naija-Lex, a lexicon of discourse connectives in Nigerian Pidgin with English translations. The lexicon shows that the majority of Nigerian Pidgin connectives are borrowed from its English lexifier, but that there are also some connectives that are unique to Nigerian Pidgin. 1 Introduction Resources such as discourse-annotated corpora (e.g. PDTB, Prasad et al., 2008) and connective lexicons (e.g. DimLEX, Stede and Umbach, 1998) have proven highly valuable in investigating discourse structure and coherence marking, but they are only available for a limited number of languages. One promising way to expand these resources is to project annotations from languages for which such resources are available to underresourced languages. The current paper sets out to investigate how automatic discourse parsing can be combined with manual annotation to construct a connective lexicon in a low-resource language. We apply this method to create Naija-Lex, a lexicon of Nigerian Pidgin, but the method can also be adapted to apply to o"
2021.codi-main.8,K15-2002,0,0.208981,"nectives is to use parallel corpora. Versley (2010) projected discourse connectives across an English-German parallel corpus, but found that simple word alignment is not enough to reliably identify connectives in text and that performance increased when bootstrapping using an established connective list. Bourgonje et al. (2018) combined a German connective lexicon with paralConnective identification In the last decade, shallow discourse parsing of explicit connectives has received considerable attention in the computational discourse community (e.g. Lin et al., 2014; Pitler and Nenkova, 2009; Wang and Lan, 2015; Zeldes et al., 2019). One challenge for such automatic parsers is that a discourse connective often also has a non-connective 85 lel text corpora to find Dutch connectives and subsequently manually checked and annotated them for relation sense. However, this approach does not automatically distinguish between connective and non-connective usages of connectives in the source languages, which always needs to be resolved manually. The current research attempts to resolve this by using a shallow discourse parser to identify connectives in the translation text automatically. nectives in the paral"
2021.codi-main.8,K16-2001,0,0.044692,"Missing"
2021.codi-main.9,2020.acl-main.463,0,0.0115203,"ers on can provide new connective candidates as well. other domains and the need for domain adaption of connective identification models and classifiers. With regards to running the models, we observed The results further indicated that performance is a trade-off between accuracy and computational 102 cost: the training and testing of Discopy (a bertbased model) required much more computational energy than any traditional parser (Bender et al., 2021). When comparing the success of neural methods and the other three parsers, it is important to be clear about the context in which they are used (Bender and Koller, 2020). Finally, we note that our results did not perfectly replicate those of the original authors. This is likely due to a difference in the experimental set-up used (e.g., the dependency parse method used by Wang and Lan (2015) was not publicly available) and a difference in evaluation methods. To ensure that the lack of replicability was not due to incorrect implementation, the second author and another, unrelated researcher independently implemented all parse methods, and neither were able to perfectly replicate the results. Note that Han et al. (2020) also obtained different results than the o"
2021.codi-main.9,W18-5042,0,0.217361,"text. the parsers to provide more coverage. One place to Similar evaluations have not been done on other do- start would be PDTB 3.0’s connective list (Webber et al., 2019). However, even this list will not cover mains, nor have the parsers been applied frequently to other domains (but see Laali and Kosseim, 2014; all connective types that might also occur in other genres (e.g. hereafter occurs in BioDRB but not Marchal et al., 2021, for an application of the e2e PDTB). Extending connective lists with a general parser to spoken translated data). This underlines lexicon of English connectives (Das et al., 2018) the importance of evaluation of existing parsers on can provide new connective candidates as well. other domains and the need for domain adaption of connective identification models and classifiers. With regards to running the models, we observed The results further indicated that performance is a trade-off between accuracy and computational 102 cost: the training and testing of Discopy (a bertbased model) required much more computational energy than any traditional parser (Bender et al., 2021). When comparing the success of neural methods and the other three parsers, it is important to be cl"
2021.codi-main.9,W16-5110,0,0.0286758,"God and as a result he asked me...”). It is unclear how parsers developed for the written domain would handle such cases that are more typical of the spoken domain. Even within the written domain, differences in connective usage can occur between various types of written text. For example, Roman et al. (2016) found a higher rate of discourse connectives used in science textbooks compared to social studies textbooks. Moreover, specific connectives can occur more in one domain than another; for example, in summary occurs more commonly in biomedical abstracts than in the general written domain (Gopalan and Devi, 2016). 3 3.1 to capture more syntactic context information from the connective. As part of the CONLL2015 shared task, competitors had access to the dependency tree, which we do not have access to. Instead, we generated parse trees using Stanford’s CoreNLP Natural Language Processing Toolkit (the same parser that was used by the e2e parser). We note that the parse tree result after running Stanford CoreNLP might be different from the one that is given by CONLL2015, which in turn might affect performance of this model. DisSent In this model, connectives are used in the downstream task for learning se"
2021.codi-main.9,2020.jeptalnrecital-recital.10,0,0.0184312,"the context in which they are used (Bender and Koller, 2020). Finally, we note that our results did not perfectly replicate those of the original authors. This is likely due to a difference in the experimental set-up used (e.g., the dependency parse method used by Wang and Lan (2015) was not publicly available) and a difference in evaluation methods. To ensure that the lack of replicability was not due to incorrect implementation, the second author and another, unrelated researcher independently implemented all parse methods, and neither were able to perfectly replicate the results. Note that Han et al. (2020) also obtained different results than the original when replicating the e2e parser (although they observed improved scores), and attributed these divergences to minor variations between experimental set-ups in terms of implementations, hyperparameter settings and/or choice of the type of F1 score reported on. These results emphasize the general need in the field for more transparent reporting and a more consistent approach to evaluation (see also Kim et al., 2020, for implicit relation classification). 6 Conclusion A comparison of different parse methods revealed that Discopy, a neural parser"
2021.codi-main.9,N16-1037,0,0.0624386,"Missing"
2021.codi-main.9,2020.acl-main.480,0,0.0301365,"related researcher independently implemented all parse methods, and neither were able to perfectly replicate the results. Note that Han et al. (2020) also obtained different results than the original when replicating the e2e parser (although they observed improved scores), and attributed these divergences to minor variations between experimental set-ups in terms of implementations, hyperparameter settings and/or choice of the type of F1 score reported on. These results emphasize the general need in the field for more transparent reporting and a more consistent approach to evaluation (see also Kim et al., 2020, for implicit relation classification). 6 Conclusion A comparison of different parse methods revealed that Discopy, a neural parser using sentence embeddings, generally outperforms parsers using syntactic and lexical features. The results also showed a severe performance drop when applying the parsers to other domains, especially spontaneous spoken discourse. This can be attributed to genrespecific syntactic structures, issues with the gold standards, and differences between connective lexicons. These results emphasise the need for out-ofdomain training and evaluation, and provide insight as"
2021.codi-main.9,2020.codi-1.7,0,0.256232,"be able to distinguish between the discourse connectiveusage of yet in Example (1), compared to the nonconnective usage in (2) and (3). Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of four parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang and Lan, 2015; DisSent, Nie et al., 2019; and Discopy, Knaebel and Stede, 2020), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely newspaper text (PDTB), scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that Discopy outperforms the other parse methods in all datasets, with the exception of DiscoSPICE. Moreover, performance drops significantly from the PDTB to all other datasets. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to parse, in order to highlight the areas where gains can be made. 1"
2021.codi-main.9,2021.unimplicit-1.1,0,0.0171967,"o the dependency parse of the dataset. 5 Discussion affected by the connectives’ usage as non-discourse connectives and the connectives’ frequency. One solution, as suggested by Lin et al. (2014), is to separately train a model for each highly ambiguous connective and another generic model to identify the remaining connectives. Another solution could be to provide parsers with more training data for infrequent connectives, which can be obtained via connective generation; this approach has recently been applied to address the lack of training data for implicit relations (Shi and Demberg, 2019; Kurfalı and Östling, 2021). Explicit connective identification can be done relatively reliably by existing parsers, but gains can still be made in this area. We therefore aimed to evaluate existing parsers and uncover more fineWhen analyzing connective-specific results, we grained errors. The results showed that Discopy found that poor performance could often be at(Knaebel and Stede, 2020) outperformed the other tributed to issues with the gold label. Some of the parsers in three out of four datasets: PDTB, Biofalse positives seemed to be valid connectives, and DRB, and TED-MDB. This indicates that the conmight therefo"
2021.codi-main.9,C14-1058,0,0.59604,"rs because et al., 2011). Of course, these results cannot be considered surprising, given that prior work on dis- they rely on PDTB2’s connective list. This list is not exhaustive and would need to be expanded for course parsing has heavily focused on the written domain, with a strong bias towards newspaper text. the parsers to provide more coverage. One place to Similar evaluations have not been done on other do- start would be PDTB 3.0’s connective list (Webber et al., 2019). However, even this list will not cover mains, nor have the parsers been applied frequently to other domains (but see Laali and Kosseim, 2014; all connective types that might also occur in other genres (e.g. hereafter occurs in BioDRB but not Marchal et al., 2021, for an application of the e2e PDTB). Extending connective lists with a general parser to spoken translated data). This underlines lexicon of English connectives (Das et al., 2018) the importance of evaluation of existing parsers on can provide new connective candidates as well. other domains and the need for domain adaption of connective identification models and classifiers. With regards to running the models, we observed The results further indicated that performance is"
2021.codi-main.9,D17-1134,0,0.0396009,"Missing"
2021.codi-main.9,2021.codi-main.8,1,0.689645,"n PDTB2’s connective list. This list is not exhaustive and would need to be expanded for course parsing has heavily focused on the written domain, with a strong bias towards newspaper text. the parsers to provide more coverage. One place to Similar evaluations have not been done on other do- start would be PDTB 3.0’s connective list (Webber et al., 2019). However, even this list will not cover mains, nor have the parsers been applied frequently to other domains (but see Laali and Kosseim, 2014; all connective types that might also occur in other genres (e.g. hereafter occurs in BioDRB but not Marchal et al., 2021, for an application of the e2e PDTB). Extending connective lists with a general parser to spoken translated data). This underlines lexicon of English connectives (Das et al., 2018) the importance of evaluation of existing parsers on can provide new connective candidates as well. other domains and the need for domain adaption of connective identification models and classifiers. With regards to running the models, we observed The results further indicated that performance is a trade-off between accuracy and computational 102 cost: the training and testing of Discopy (a bertbased model) required"
2021.codi-main.9,L16-1165,1,0.778272,"2-21, evaluated using sec 22, and tested on sec 23. Wang and Lan (2015) reimplemented well-established techniques from Pitler and Nenkova (2009) and Lin et al. (2014), and added the POS tags of nodes from the connective’s parent 3.2 Data Parser performance was measured on four different datasets in order to determine how well the parsers can identify connectives in various domains. 97 All datasets have already been annotated with discourse relations, and therefore we can use the gold connectives from these annotations. connective list), such as at, by, in and through. Disco-SPICE Disco-SPICE (Rehbein et al., 2016) is a corpus of transcribed broadcast interviews and telephone conversations from the SPICEIreland corpus (Kallen and Kirk, 2008), annotated in PDTB3-style. These texts represent a more informal, spontaneous spoken genre than the TED talks. This dataset contains 1163 explicit connective tokens with 50 unique connective types. Again, the reduced number of connective types can be attributed to the domain. PDTB2 sec 23 We evaluated all parsers on the Penn Discourse Treebank 2.0 (PDTB2, Prasad et al., 2008). The PDTB consists of discourse annotations on the Wall Street Journal texts. We here evalu"
2021.codi-main.9,W19-0416,1,0.723969,"e extremely sensitive to the dependency parse of the dataset. 5 Discussion affected by the connectives’ usage as non-discourse connectives and the connectives’ frequency. One solution, as suggested by Lin et al. (2014), is to separately train a model for each highly ambiguous connective and another generic model to identify the remaining connectives. Another solution could be to provide parsers with more training data for infrequent connectives, which can be obtained via connective generation; this approach has recently been applied to address the lack of training data for implicit relations (Shi and Demberg, 2019; Kurfalı and Östling, 2021). Explicit connective identification can be done relatively reliably by existing parsers, but gains can still be made in this area. We therefore aimed to evaluate existing parsers and uncover more fineWhen analyzing connective-specific results, we grained errors. The results showed that Discopy found that poor performance could often be at(Knaebel and Stede, 2020) outperformed the other tributed to issues with the gold label. Some of the parsers in three out of four datasets: PDTB, Biofalse positives seemed to be valid connectives, and DRB, and TED-MDB. This indicat"
2021.codi-main.9,K15-2002,0,0.645305,"connectives. A simple dictionary lookup would therefore not be able to distinguish between the discourse connectiveusage of yet in Example (1), compared to the nonconnective usage in (2) and (3). Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of four parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang and Lan, 2015; DisSent, Nie et al., 2019; and Discopy, Knaebel and Stede, 2020), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely newspaper text (PDTB), scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that Discopy outperforms the other parse methods in all datasets, with the exception of DiscoSPICE. Moreover, performance drops significantly from the PDTB to all other datasets. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to"
2021.codi-main.9,P19-1442,0,0.112157,"nary lookup would therefore not be able to distinguish between the discourse connectiveusage of yet in Example (1), compared to the nonconnective usage in (2) and (3). Existing parse methods use varying approaches to identify explicit discourse connectives, but their performance has not been consistently evaluated in comparison to each other, nor have they been evaluated consistently on text other than newspaper articles. We here assess the performance on explicit connective identification of four parse methods (PDTB e2e, Lin et al., 2014; the winner of CONLL2015, Wang and Lan, 2015; DisSent, Nie et al., 2019; and Discopy, Knaebel and Stede, 2020), along with a simple heuristic. We also examine how well these systems generalize to different datasets, namely newspaper text (PDTB), scientific text (BioDRB), prepared spoken text (TED-MDB) and spontaneous spoken text (Disco-SPICE). The results show that Discopy outperforms the other parse methods in all datasets, with the exception of DiscoSPICE. Moreover, performance drops significantly from the PDTB to all other datasets. We provide a more fine-grained analysis of domain differences and connectives that prove difficult to parse, in order to highligh"
2021.codi-main.9,P09-2004,0,0.317955,"s. For multi-word connectives, the contextualized embeddings of the single words are averaged. The model is then trained in a multi-task setting, to predict the connective or predict the coherence relation. They used PDTB sec 02–22 for training and sec 23-24 for testing. Here, we assess their best performing bert-based model on connective identification task, with context size of 1. CONLL2015 Similar to the PDTB e2e parser, the CONLL2015 winning parser is trained on PDTB sec 02-21, evaluated using sec 22, and tested on sec 23. Wang and Lan (2015) reimplemented well-established techniques from Pitler and Nenkova (2009) and Lin et al. (2014), and added the POS tags of nodes from the connective’s parent 3.2 Data Parser performance was measured on four different datasets in order to determine how well the parsers can identify connectives in various domains. 97 All datasets have already been annotated with discourse relations, and therefore we can use the gold connectives from these annotations. connective list), such as at, by, in and through. Disco-SPICE Disco-SPICE (Rehbein et al., 2016) is a corpus of transcribed broadcast interviews and telephone conversations from the SPICEIreland corpus (Kallen and Kirk,"
2021.codi-main.9,prasad-etal-2008-penn,0,0.72483,"yet does not function in a relation with two complete discourse arguments, and would therefore not be annotated as a connective. An accurate parser would need to be able to distinguish between these fine-grained differences in the usage of connective candidates. Further, connective usage diverges between domains. For example, but and so are often used as discourse structuring markers in spoken language, rather than discourse connectives. Existing parsers, however, have mainly been evaluated on newspaper text, since the largest discourse-annotated corpus available comes from this domain (PDTB, Prasad et al., 2008). Performance of existing discourse connective identification parsers on domains other than the written one is currently not well known. Finally, we note that there is a lack of information on which connectives remain difficult to identify, even in state-of-the-art parsers. This is due to the tendency of studies reporting only the general accuracy, without providing detail on the accuracy on specific connectives. However, such information can provide the field with more information on what to focus on. Introduction Understanding the discourse relations that hold between segments in natural lan"
2021.eacl-main.61,P05-1018,0,0.602021,"Missing"
2021.eacl-main.61,P00-1037,0,0.281699,"strategy to also include model competence (Kocmi and Bojar, 2017; Zhou et al., 2020; Platanios et al., 2019; Liu et al., 2020; Zhang et al., 2018, 2019). While sample difficulty can be assessed for text samples and data samples or jointly, various measures have been proposed for text samples including n-gram frequency Haffari (2009); Platanios et al. (2019), token rarity, and sentence length (Liu et al., 2020; Platanios et al., 2019). Our approach considers data and text jointly, similar to edit distance metric – Levenshtein (Levenshtein, 1966) and Damerau-Levenshtein Distance (Damerau, 1964; Brill and Moore, 2000a), which was used as a content ordering metric in Wiseman et al. (2017) to measure the extent of alignment between data slots and text tokens. 3 Preliminaries of Curriculum Learning Algorithm 1: Curriculum Learning Algorithm 1 2 3 4 5 6 7 Input: Training set, D = {sd , st }M i=1 , consisting of M samples, model (T ), difficulty metric (d), and competence function (c). Compute the difficulty, d(si ), for each data-text pair ∈ D (Section 4). ¯ i ) of d(si ), where Compute the CDF score d(s ¯ i ) ∈ [0, 1] (See Figure 2). d(s for training step t = 1, . . . do Compute the model competence c(t) wit"
2021.eacl-main.61,W16-6626,0,0.160698,"Missing"
2021.eacl-main.61,2009.mtsummit-tutorials.2,0,0.051451,"an, 1993; Krueger and Dayan, 2009; Plunkett and Marchman, 1993). Bengio et al. (2009) first demonstrated empirically that curriculum learning approaches can decrease training times and improve generalization; later approach address these issues by changing the minibatch sampling strategy to also include model competence (Kocmi and Bojar, 2017; Zhou et al., 2020; Platanios et al., 2019; Liu et al., 2020; Zhang et al., 2018, 2019). While sample difficulty can be assessed for text samples and data samples or jointly, various measures have been proposed for text samples including n-gram frequency Haffari (2009); Platanios et al. (2019), token rarity, and sentence length (Liu et al., 2020; Platanios et al., 2019). Our approach considers data and text jointly, similar to edit distance metric – Levenshtein (Levenshtein, 1966) and Damerau-Levenshtein Distance (Damerau, 1964; Brill and Moore, 2000a), which was used as a content ordering metric in Wiseman et al. (2017) to measure the extent of alignment between data slots and text tokens. 3 Preliminaries of Curriculum Learning Algorithm 1: Curriculum Learning Algorithm 1 2 3 4 5 6 7 Input: Training set, D = {sd , st }M i=1 , consisting of M samples, model"
2021.eacl-main.61,kocmi-bojar-2017-curriculum,0,0.073061,"a of teaching algorithms in a similar manner as humans, incrementally from easy concepts to more difficult ones dates back to incremental learning, which was discussed in light of theories of cognitive development relating to the processes of acquisition in young children (Elman, 1993; Krueger and Dayan, 2009; Plunkett and Marchman, 1993). Bengio et al. (2009) first demonstrated empirically that curriculum learning approaches can decrease training times and improve generalization; later approach address these issues by changing the minibatch sampling strategy to also include model competence (Kocmi and Bojar, 2017; Zhou et al., 2020; Platanios et al., 2019; Liu et al., 2020; Zhang et al., 2018, 2019). While sample difficulty can be assessed for text samples and data samples or jointly, various measures have been proposed for text samples including n-gram frequency Haffari (2009); Platanios et al. (2019), token rarity, and sentence length (Liu et al., 2020; Platanios et al., 2019). Our approach considers data and text jointly, similar to edit distance metric – Levenshtein (Levenshtein, 1966) and Damerau-Levenshtein Distance (Damerau, 1964; Brill and Moore, 2000a), which was used as a content ordering me"
2021.eacl-main.61,D16-1128,0,0.0452483,"Missing"
2021.eacl-main.61,2020.acl-main.41,0,0.217423,"Missing"
2021.eacl-main.61,W17-5525,0,0.152991,"Missing"
2021.eacl-main.61,N19-1119,0,0.263316,"er as humans, incrementally from easy concepts to more difficult ones dates back to incremental learning, which was discussed in light of theories of cognitive development relating to the processes of acquisition in young children (Elman, 1993; Krueger and Dayan, 2009; Plunkett and Marchman, 1993). Bengio et al. (2009) first demonstrated empirically that curriculum learning approaches can decrease training times and improve generalization; later approach address these issues by changing the minibatch sampling strategy to also include model competence (Kocmi and Bojar, 2017; Zhou et al., 2020; Platanios et al., 2019; Liu et al., 2020; Zhang et al., 2018, 2019). While sample difficulty can be assessed for text samples and data samples or jointly, various measures have been proposed for text samples including n-gram frequency Haffari (2009); Platanios et al. (2019), token rarity, and sentence length (Liu et al., 2020; Platanios et al., 2019). Our approach considers data and text jointly, similar to edit distance metric – Levenshtein (Levenshtein, 1966) and Damerau-Levenshtein Distance (Damerau, 1964; Brill and Moore, 2000a), which was used as a content ordering metric in Wiseman et al. (2017) to measure th"
2021.eacl-main.61,N19-1189,0,0.0390545,"Missing"
2021.eacl-main.61,2020.acl-main.620,0,0.031579,"s in a similar manner as humans, incrementally from easy concepts to more difficult ones dates back to incremental learning, which was discussed in light of theories of cognitive development relating to the processes of acquisition in young children (Elman, 1993; Krueger and Dayan, 2009; Plunkett and Marchman, 1993). Bengio et al. (2009) first demonstrated empirically that curriculum learning approaches can decrease training times and improve generalization; later approach address these issues by changing the minibatch sampling strategy to also include model competence (Kocmi and Bojar, 2017; Zhou et al., 2020; Platanios et al., 2019; Liu et al., 2020; Zhang et al., 2018, 2019). While sample difficulty can be assessed for text samples and data samples or jointly, various measures have been proposed for text samples including n-gram frequency Haffari (2009); Platanios et al. (2019), token rarity, and sentence length (Liu et al., 2020; Platanios et al., 2019). Our approach considers data and text jointly, similar to edit distance metric – Levenshtein (Levenshtein, 1966) and Damerau-Levenshtein Distance (Damerau, 1964; Brill and Moore, 2000a), which was used as a content ordering metric in Wiseman et"
2021.eacl-main.64,W18-6555,0,0.0243306,"ery few text samples exist (we assume that these text samples are paired with corresponding data samples). We aim to answer how we can make the most of the scarce annotations, together with large amounts of unlabelled data, in order to push the limit of the neural data-to-text models. Figure 1 illustrates the scenario. To address the limited-data challenge, we propose a simple yet effective way of augmenting the text side with the pretrained language model (LM) GPT-2 (Radford et al., 2019). Unlike other text augmentation work employed in data-to-text generation systems (Freitag and Roy, 2018; Agarwal et al., 2018), our proposal assumes little to no domain-dependent heuristics. It consists of two steps: (1) information augmentation by slot-value replacement and (2) LM augmentation by GPT-2 758 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 758–768 April 19 - 23, 2021. ©2021 Association for Computational Linguistics generation. Once we have augmented the set of text samples, we are essentially in a similar setting as previously proposed semi-supervised approaches to data-totext generation Schmitt and Sch¨utze (2019); Qader et al. (2019);"
2021.eacl-main.64,P19-1309,0,0.0604424,"y objective in order to learn a pairing for the data samples. The cycle consistency objective tries to make sure that data samples can be reconstructed correctly from their textual formulations, and similarly that texts can be reconstructed after having been parsed into a data representation. As the automatically generated text samples from GPT-2 might be very noisy and not pair well with data samples, we align each augmented text sample with its most similar unlabeled data sample, as defined in their encoded vector space. This idea is inspired by recent work on representation matching in MT (Artetxe and Schwenk, 2019; Ruiter et al., 2019). To ensure good quality of the training data, only pairs above a certain similarity threshold  are retained as pseudo pairs for training. The quality of the pseudo pairs will gradually improve as the encoder improves in the training process. In return, the learning of the encoder will also be facilitated with the improved quality of pseudo pairs as a virtuous cycle. On two data-to-text benchmarks E2E (Novikova et al., 2017) and WebNLG (Gardent et al., 2017), we show that our LM-augmented weakly supervised model succeeds on outperforming fully supervised seq2seq model, t"
2021.eacl-main.64,P05-1018,0,0.273378,"Missing"
2021.eacl-main.64,N19-1071,0,0.0249911,"der the cycle consistency, we further consider matching each synthetic text with its most similar data sample and treating them as supplementary training pairs. Compared with the pseudo d0 obtained from back translation (Eq. 2), the matched data samples are extracted from the existing corpus DU and thereby are guaranteed to be clean. This can provide a much more stable training signal especially at the initial training stage5 . Previous work has used representation matching to automatically extract pseudo training pairs for machine translation (Artetxe and Schwenk, 2019; Ruiter et al., 2019). Baziotis et al. (2019); Chu and 3 In the MT community, the equivalent step is usually called back translation (Sennrich et al., 2016; Lample et al., 2018). 761 4 The shared encoding has also been shown effective in other tasks like machine translation (Lample et al., 2018) and image transition (Zhu et al., 2017). We further tried sharing the decoder as in Johnson et al. (2017) but find no improvement (see Table 2). 5 In theory, as we can fabricate arbitrary possible data samples from the predefined schema and add to the corpus, we can always find one matched data for a text samples. Liu (2019) also demonstrate that"
2021.eacl-main.64,D19-5602,0,0.0984666,"Missing"
2021.eacl-main.64,2020.coling-demos.3,1,0.464964,"is shown to be complementary with current seq2seq pretraining techniques, and can offer orthogonal improvements when combining both. 2 Building neural data-to-text systems with few paired samples (but a large set of unpaired samples) has been a hot research topic recently. Most works adopt the idea of cycle consistency (Zhu et al., 2017), which has been used in many text generation tasks like machine translation (Artetxe et al., 2017; Lample et al., 2017) and style transfer (Prabhumoye et al., 2018; Subramanian et al., 2018). Schmitt and Sch¨utze (2019); Qader et al. (2019); Su et al. (2020); Chang et al. (2020, 2021a,b) applied this idea to the task of data-to-text generation and reported promising results. Ma et al. (2019) separate the generation process into few-shot content selection and surface realization components and learn them separately. Nonetheless, all of these approaches assume the existence of huge quantity of unpaired text samples, which, as we mentioned, is an unrealistic assumption for the task of data-totext generation. Freitag and Roy (2018) proposes to reconstruct usable sequences re-written from data with rules for unsupervised data-to-text generation. Unfortunately, designing"
2021.eacl-main.64,2021.eacl-main.69,1,0.805549,"Missing"
2021.eacl-main.64,2021.eacl-main.61,1,0.740723,"Missing"
2021.eacl-main.64,2020.acl-main.18,0,0.266811,"al. (2019) separate the generation process into few-shot content selection and surface realization components and learn them separately. Nonetheless, all of these approaches assume the existence of huge quantity of unpaired text samples, which, as we mentioned, is an unrealistic assumption for the task of data-totext generation. Freitag and Roy (2018) proposes to reconstruct usable sequences re-written from data with rules for unsupervised data-to-text generation. Unfortunately, designing these rules require efforts similar to building a template-based system. (Budzianowski and Vuli´c, 2019; Chen et al., 2020; Peng et al., 2020) tackle the few-shot challenge by finetuning a pretrained LM to incorporate prior knowledge from general-domain text or data-text pairs. We show that our technique is complementary with them and can offer orthogonal improvements when combining both. 3 1. We study the few-shot data-to-text scenario where, unlike previous works, no further target-side text is available. 2. We present an effective way of automatically augmenting target text by resorting to the pretrained LM GPT-2. 3. We propose utilizing the augmented text by a combination of cycle consistency and repRelated W"
2021.eacl-main.64,W16-6626,0,0.0405474,"Missing"
2021.eacl-main.64,W17-4715,0,0.0198347,"20). Similarly, starting from a text t ∈ T , the objective is to ensure the consistency in the direction of t → d0 → t:3 0 0 max Et∼p(T ) log pθ (t|d ); d ∼ pφ (d|t) θ (2) Finally, we further add two denoising autoencoding objectives on both the data and text sides: ˜ θ (t|t˜) max Ed∼p(D),t∼p(T ) log pφ (d|d)p θ,φ (3) where d˜ and t˜ are the corrupted versions of d and t. We use the same noise function as in Lample et al. (2018) which randomly permutes and pads a portion of the input. This can encourage the encoder to learn meaningful latent representations by reconstructing the input itself (Currey et al., 2017; Lample et al., 2018). Figure 3 illustrates all the four directions of the cycle consistency objective. 4.3 Representation Matching Apart from training under the cycle consistency, we further consider matching each synthetic text with its most similar data sample and treating them as supplementary training pairs. Compared with the pseudo d0 obtained from back translation (Eq. 2), the matched data samples are extracted from the existing corpus DU and thereby are guaranteed to be clean. This can provide a much more stable training signal especially at the initial training stage5 . Previous work"
2021.eacl-main.64,D18-1426,0,0.100067,"scenarios where only very few text samples exist (we assume that these text samples are paired with corresponding data samples). We aim to answer how we can make the most of the scarce annotations, together with large amounts of unlabelled data, in order to push the limit of the neural data-to-text models. Figure 1 illustrates the scenario. To address the limited-data challenge, we propose a simple yet effective way of augmenting the text side with the pretrained language model (LM) GPT-2 (Radford et al., 2019). Unlike other text augmentation work employed in data-to-text generation systems (Freitag and Roy, 2018; Agarwal et al., 2018), our proposal assumes little to no domain-dependent heuristics. It consists of two steps: (1) information augmentation by slot-value replacement and (2) LM augmentation by GPT-2 758 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 758–768 April 19 - 23, 2021. ©2021 Association for Computational Linguistics generation. Once we have augmented the set of text samples, we are essentially in a similar setting as previously proposed semi-supervised approaches to data-totext generation Schmitt and Sch¨utze (2019"
2021.eacl-main.64,D18-1549,0,0.405396,"sides. Each data sample is flattened into a sequence by making a list of slot value pairs and fed into the same encoder. Using the same encoder for both types of input gives the model an inductive bias to project similar data/text into surrounding latent space. We will show later that encoder sharing is essential for a good performance under the few-shot scenario. From the shared encoded space, two separate decoders Decd and Dect are used to decode d and t respectively4 . ~ d d Figure 3: Four directions of cycle consistency. Gradients are backpropagated only through solid lines. been made in Lample et al. (2018); He et al. (2020); Garcia et al. (2020). Similarly, starting from a text t ∈ T , the objective is to ensure the consistency in the direction of t → d0 → t:3 0 0 max Et∼p(T ) log pθ (t|d ); d ∼ pφ (d|t) θ (2) Finally, we further add two denoising autoencoding objectives on both the data and text sides: ˜ θ (t|t˜) max Ed∼p(D),t∼p(T ) log pφ (d|d)p θ,φ (3) where d˜ and t˜ are the corrupted versions of d and t. We use the same noise function as in Lample et al. (2018) which randomly permutes and pads a portion of the input. This can encourage the encoder to learn meaningful latent representations"
2021.eacl-main.64,2020.findings-emnlp.283,0,0.0198262,"to a sequence by making a list of slot value pairs and fed into the same encoder. Using the same encoder for both types of input gives the model an inductive bias to project similar data/text into surrounding latent space. We will show later that encoder sharing is essential for a good performance under the few-shot scenario. From the shared encoded space, two separate decoders Decd and Dect are used to decode d and t respectively4 . ~ d d Figure 3: Four directions of cycle consistency. Gradients are backpropagated only through solid lines. been made in Lample et al. (2018); He et al. (2020); Garcia et al. (2020). Similarly, starting from a text t ∈ T , the objective is to ensure the consistency in the direction of t → d0 → t:3 0 0 max Et∼p(T ) log pθ (t|d ); d ∼ pφ (d|t) θ (2) Finally, we further add two denoising autoencoding objectives on both the data and text sides: ˜ θ (t|t˜) max Ed∼p(D),t∼p(T ) log pφ (d|d)p θ,φ (3) where d˜ and t˜ are the corrupted versions of d and t. We use the same noise function as in Lample et al. (2018) which randomly permutes and pads a portion of the input. This can encourage the encoder to learn meaningful latent representations by reconstructing the input itself (Cur"
2021.eacl-main.64,D16-1128,0,0.0161174,"Missing"
2021.eacl-main.64,W17-3518,0,0.023931,"l improvements when combining both. 3 1. We study the few-shot data-to-text scenario where, unlike previous works, no further target-side text is available. 2. We present an effective way of automatically augmenting target text by resorting to the pretrained LM GPT-2. 3. We propose utilizing the augmented text by a combination of cycle consistency and repRelated Work Problem Formulation We represent the data samples as D and the text samples as T. In our work, we do not restrict the format of the data. Each d ∈ D can be a set of key-value pairs, as in Figure 1, or in form of RDF triples as in Gardent et al. (2017). Each text t ∈ T consists of a sequence of words. In few-shot settings, we are assumed to have (1) k labeled pairs (DL , TL ) and (2) large quantities of unlabeled data 759 DU where |DU | k &gt; 01 . This, we believe, is a more realistic setting as unlabeled data are usually abundant and also can be easily fabricated from predefined schemata. Notably, we assume no access to outside resources containing in-domain text. The k annotations are all we know about the text side. 4 the LM and continuing to generate words. People have also also shown that GPT-2 is able to improve classification tasks vi"
2021.eacl-main.64,W18-6504,0,0.027847,"Missing"
2021.eacl-main.64,P19-1197,0,0.0249377,"ombining both. 2 Building neural data-to-text systems with few paired samples (but a large set of unpaired samples) has been a hot research topic recently. Most works adopt the idea of cycle consistency (Zhu et al., 2017), which has been used in many text generation tasks like machine translation (Artetxe et al., 2017; Lample et al., 2017) and style transfer (Prabhumoye et al., 2018; Subramanian et al., 2018). Schmitt and Sch¨utze (2019); Qader et al. (2019); Su et al. (2020); Chang et al. (2020, 2021a,b) applied this idea to the task of data-to-text generation and reported promising results. Ma et al. (2019) separate the generation process into few-shot content selection and surface realization components and learn them separately. Nonetheless, all of these approaches assume the existence of huge quantity of unpaired text samples, which, as we mentioned, is an unrealistic assumption for the task of data-totext generation. Freitag and Roy (2018) proposes to reconstruct usable sequences re-written from data with rules for unsupervised data-to-text generation. Unfortunately, designing these rules require efforts similar to building a template-based system. (Budzianowski and Vuli´c, 2019; Chen et al."
2021.eacl-main.64,W17-5525,0,0.1152,"Missing"
2021.eacl-main.64,N19-4009,0,0.0659368,"Missing"
2021.eacl-main.64,W14-4407,0,0.0191325,"pose a pipeline approach to augment the text samples by (1) information augmentation and (2) LM augmentation. 4.1.1 Information Augmentation We generate additional text samples by performing slot-value replacements. As many data values are exactly copied to the text samples, these copied information can be easily detected and replaced with other values (for the same slot type) to enrich the information space of the text samples. This can be considered as a simplified version of traditional methods of template mining where key words are extracted to construct templates (Kondadadi et al., 2013; Oya et al., 2014). An example is shown in Figure 2. Each text sample is augmented with 10 more distinct text samples or with all possible values being replaced. The slot-value replacement is efficient to implement. However, it can only detect identical values and augment text with the same combinatorial patterns as the few-shot annotations. To enrich the linguistic realizations of text sentences and enable new combinations of information, we further propose a LM augmentation approach using GPT-2. 4.1.2 LM Augmentation GPT-2 (Radford et al., 2019) is a language model pretrained on the collected WebText. It has"
2021.eacl-main.64,N18-1014,0,0.101058,"Missing"
2021.eacl-main.64,2020.findings-emnlp.17,0,0.0779401,"e the generation process into few-shot content selection and surface realization components and learn them separately. Nonetheless, all of these approaches assume the existence of huge quantity of unpaired text samples, which, as we mentioned, is an unrealistic assumption for the task of data-totext generation. Freitag and Roy (2018) proposes to reconstruct usable sequences re-written from data with rules for unsupervised data-to-text generation. Unfortunately, designing these rules require efforts similar to building a template-based system. (Budzianowski and Vuli´c, 2019; Chen et al., 2020; Peng et al., 2020) tackle the few-shot challenge by finetuning a pretrained LM to incorporate prior knowledge from general-domain text or data-text pairs. We show that our technique is complementary with them and can offer orthogonal improvements when combining both. 3 1. We study the few-shot data-to-text scenario where, unlike previous works, no further target-side text is available. 2. We present an effective way of automatically augmenting target text by resorting to the pretrained LM GPT-2. 3. We propose utilizing the augmented text by a combination of cycle consistency and repRelated Work Problem Formulat"
2021.eacl-main.64,P13-1138,0,0.0300297,"text samples T , we propose a pipeline approach to augment the text samples by (1) information augmentation and (2) LM augmentation. 4.1.1 Information Augmentation We generate additional text samples by performing slot-value replacements. As many data values are exactly copied to the text samples, these copied information can be easily detected and replaced with other values (for the same slot type) to enrich the information space of the text samples. This can be considered as a simplified version of traditional methods of template mining where key words are extracted to construct templates (Kondadadi et al., 2013; Oya et al., 2014). An example is shown in Figure 2. Each text sample is augmented with 10 more distinct text samples or with all possible values being replaced. The slot-value replacement is efficient to implement. However, it can only detect identical values and augment text with the same combinatorial patterns as the few-shot annotations. To enrich the linguistic realizations of text sentences and enable new combinations of information, we further propose a LM augmentation approach using GPT-2. 4.1.2 LM Augmentation GPT-2 (Radford et al., 2019) is a language model pretrained on the collect"
2021.eacl-main.64,P18-1080,0,0.0772085,"Missing"
2021.eacl-main.64,W19-8669,0,0.1301,"Missing"
2021.eacl-main.64,P19-1178,0,0.0562832,"Missing"
2021.eacl-main.64,P16-1009,0,0.0221223,"nd treating them as supplementary training pairs. Compared with the pseudo d0 obtained from back translation (Eq. 2), the matched data samples are extracted from the existing corpus DU and thereby are guaranteed to be clean. This can provide a much more stable training signal especially at the initial training stage5 . Previous work has used representation matching to automatically extract pseudo training pairs for machine translation (Artetxe and Schwenk, 2019; Ruiter et al., 2019). Baziotis et al. (2019); Chu and 3 In the MT community, the equivalent step is usually called back translation (Sennrich et al., 2016; Lample et al., 2018). 761 4 The shared encoding has also been shown effective in other tasks like machine translation (Lample et al., 2018) and image transition (Zhu et al., 2017). We further tried sharing the decoder as in Johnson et al. (2017) but find no improvement (see Table 2). 5 In theory, as we can fabricate arbitrary possible data samples from the predefined schema and add to the corpus, we can always find one matched data for a text samples. Liu (2019) also demonstrate that the representation similarity between input-output pairs can serve as a useful regularization for unsupervise"
2021.eacl-main.64,2020.acl-main.63,0,0.270392,"our proposal assumes little to no domain-dependent heuristics. It consists of two steps: (1) information augmentation by slot-value replacement and (2) LM augmentation by GPT-2 758 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 758–768 April 19 - 23, 2021. ©2021 Association for Computational Linguistics generation. Once we have augmented the set of text samples, we are essentially in a similar setting as previously proposed semi-supervised approaches to data-totext generation Schmitt and Sch¨utze (2019); Qader et al. (2019); Su et al. (2020), which assume the presence of vast amounts of unpaired data and text instances. These approaches exploit a cycle consistency objective in order to learn a pairing for the data samples. The cycle consistency objective tries to make sure that data samples can be reconstructed correctly from their textual formulations, and similarly that texts can be reconstructed after having been parsed into a data representation. As the automatically generated text samples from GPT-2 might be very noisy and not pair well with data samples, we align each augmented text sample with its most similar unlabeled da"
2021.eacl-main.69,P19-1080,0,0.334421,"odels in a two-step process with semisupervised learning objectives (Tseng et al., 2020). First, we use pretrained models to estimate quality scores for each sample. Then, we down-weight the 818 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 818–829 April 19 - 23, 2021. ©2021 Association for Computational Linguistics loss updates in the back-propagation phase using the estimated quality scores. This way, the models are guided to avoid mistakes of the weak annotator. On two benchmarks, E2E (Novikova et al., 2017b) and Weather (Balakrishnan et al., 2019), we utilize varying amount of labeled data and show that the framework is able to successfully learn from the synthetic data generated by weak annotator, thereby allowing jointly-trained NLG and NLU models to outperform other baseline systems. This work makes the following contributions: 1. We propose an automatic method to overcome the lack of text labels by using a fine-tuned language model as a weak annotator to construct text labels for the vast amount of MR samples, resulting in a much larger labeled dataset. 2. We propose an effective two-step weak supervision using the dual mutual info"
2021.eacl-main.69,P05-1018,0,0.472817,"Missing"
2021.eacl-main.69,2020.emnlp-main.615,0,0.0487064,"Missing"
2021.eacl-main.69,D19-5602,0,0.0993337,"Missing"
2021.eacl-main.69,2020.acl-main.149,0,0.027554,"by training with SGD, where L(·) is the loss of predicting yˆ for an input xi when the label is y˜. The weighted step is then c(xi , y˜i )∇L(ˆ y , y˜), where c(·) is a scoring function learned by the teacher taking as input MR xi and its noisy text label y˜i . In essence, we control the degree of parameter updates to the student based on how reliable its labels are according to the teacher. We denote c(·) as the function of the label quality based on the dual mutual information (DMI), defined as the absolute difference between mutual information (MI)4 in inference directions x → y and y → x. Bugliarello et al. (2020) shows that MIx→y correlates to the difficulty in predicting y from x, and vice versa. Thus we expect the difference between MIx→y and MIy→x for clean sample (x, y) to be relatively small compared to noisy samples, since the level of difficulty is largely proportional between NLU and NLG on the samples – difficulty in inferring x from y will result in harder prediction of y from x. Based on this intuition, the DMI score of the sample (x, y) is defined as:   qAUTO (x) qAUTO (y) − log exp log . qNLG (y|x) qNLU (x|y) | {z } | {z } MIx→y MIy→x where q(·) are the two respective models. The DMI fo"
2021.eacl-main.69,P13-2131,0,0.0207494,"of utterance with “hallucinated” facts (MR values) from MR leads to considerable performance gain, since inconsistent MR-Text correspondence might misguide systems to generate incorrect facts and deteriorate the NLG outputs. We filter out the synthetic, poor quality MR-text 820 1 We adopt the Top-k random sampling with k = 2 to encourage diversity and reduce repetition (Radford et al., 2019) pairs by training a separate NLU model on the original labeled data to predict MR from generated text labels. These MRs can then be checked against the paired MR in DW via pattern matching as inspired by Cai and Knight (2013); Wiseman et al. (2017). Specifically, we use a measure of semantic similarity in terms of f-score via matching of slots between the two MRs. We keep all MR-text pairs with f-scores above 0.7, as we found empirically that this criterion retains a sufficiently large amount of high-quality data. The removed text sentences are used for unsupervised training objectives as in Eq. 1-3. Using this method, we create a collection of parallel MR-text samples (~500k) an order of magnitude larger than even the full training sets (~40k for E2E and ~25k for Weather). 5 Starting from x ∈ X, its objective is:"
2021.eacl-main.69,2020.coling-demos.3,1,0.500422,"Missing"
2021.eacl-main.69,2021.eacl-main.64,1,0.805549,"Missing"
2021.eacl-main.69,2021.eacl-main.61,1,0.602707,"Missing"
2021.eacl-main.69,2020.acl-main.18,0,0.184026,"ta to establish new state-of-the-art performance. 2 Related Work Learning with Weak Supervision. Learning with weak supervision is a well-studied area that is popularized by the rise of data-driven neural approaches (Ratner et al., 2017; Safranchik et al., 2020; Bach et al., 2017; Wu et al., 2018; Dehghani et al., 2018; Jiang et al., 2018; Chang et al., 2020a; de Souza et al., 2018). Our approach incorporates similar line of work, by providing noisy labels (text) with a fine-tuned LM which incorporates prior knowledge from general-domain text and data-text pair (Budzianowski and Vuli´c, 2019; Chen et al., 2020; Peng et al., 2020; Mager et al., 2020; Harkous et al., 2020; Shen et al., 2020; Chang et al., 2020b, 2021b,a), and use it as the weak annotator, similar by functionality to that of fidelity-weighted learning (Dehghani et al., 2017), or data creation tool Snorkel (Ratner et al., 2017). Learning with Semi-Supervision. Work on semi-supervised learning considers settings with some labeled data and a much larger set of unlabeled data, and then leverages both labeled the unlabeled data as in machine translation (Artetxe et al., 2017; Lample et al., 2017), data-to-text generation (Schmitt and Sch¨u"
2021.eacl-main.69,E17-1060,0,0.0134421,"020), we also adopt the variational optimization objective upon the latent variable z which was shown to pull the inferred posteriors q(z|x) and q(z|y) closer to each other. In this case, the parameters of both NLG and NLU models are updated. Supervised Learning. Apart from the above unsupervised objectives, we can impose the supervised objective on the k labeled pairs: max Ex,y∼p(XL ,YL ) log pθ (y|x)+log pφ (x|y) (4) θ,φ Each MR is flattened into a sequence and fed into the NLG encoder, giving NLG and NLU models an inductive bias to project similar MR/text into the surrounding latent space (Chisholm et al., 2017). As we observed anecdotally3 , the information flow enabled by REINFORCE allows the models to utilize unlabeled MR and text, boosting the performance in our scenarios. 821 2 This direction is usually termed as back translation in MT community (Sennrich et al., 2016; Lample et al., 2018) 3 Tseng et al. (2020) noticed similar trend in the experiments. 6 Learning with Weak Supervision The primary challenge that arises from the synthetic data is the noise introduced during the generation process. Noisy and poor quality labels tend to bring little to no improvements (Elman, 1993; Fr´enay and Verle"
2021.eacl-main.69,P16-2008,0,0.0601315,"Missing"
2021.eacl-main.69,2020.coling-main.218,0,0.105833,"Missing"
2021.eacl-main.69,N18-1014,0,0.0958254,"Missing"
2021.eacl-main.69,D18-1549,0,0.0364775,"ervised objectives, we can impose the supervised objective on the k labeled pairs: max Ex,y∼p(XL ,YL ) log pθ (y|x)+log pφ (x|y) (4) θ,φ Each MR is flattened into a sequence and fed into the NLG encoder, giving NLG and NLU models an inductive bias to project similar MR/text into the surrounding latent space (Chisholm et al., 2017). As we observed anecdotally3 , the information flow enabled by REINFORCE allows the models to utilize unlabeled MR and text, boosting the performance in our scenarios. 821 2 This direction is usually termed as back translation in MT community (Sennrich et al., 2016; Lample et al., 2018) 3 Tseng et al. (2020) noticed similar trend in the experiments. 6 Learning with Weak Supervision The primary challenge that arises from the synthetic data is the noise introduced during the generation process. Noisy and poor quality labels tend to bring little to no improvements (Elman, 1993; Fr´enay and Verleysen, 2013). To better train on the large and noisy corpus described in section §4 (size ~500k), we employ a two-step training process motivated by fidelity-weighted learning (Dehghani et al., 2018). The two-step process consists of (1) pretraining and (2) quality-weighted fine-tuning to"
2021.eacl-main.69,N19-1236,0,0.0611163,"Missing"
2021.eacl-main.69,P17-1163,0,0.193359,"Missing"
2021.eacl-main.69,D17-1238,0,0.0389287,"Missing"
2021.eacl-main.69,W17-5525,0,0.149408,"Missing"
2021.eacl-main.69,2020.findings-emnlp.17,0,0.055879,"state-of-the-art performance. 2 Related Work Learning with Weak Supervision. Learning with weak supervision is a well-studied area that is popularized by the rise of data-driven neural approaches (Ratner et al., 2017; Safranchik et al., 2020; Bach et al., 2017; Wu et al., 2018; Dehghani et al., 2018; Jiang et al., 2018; Chang et al., 2020a; de Souza et al., 2018). Our approach incorporates similar line of work, by providing noisy labels (text) with a fine-tuned LM which incorporates prior knowledge from general-domain text and data-text pair (Budzianowski and Vuli´c, 2019; Chen et al., 2020; Peng et al., 2020; Mager et al., 2020; Harkous et al., 2020; Shen et al., 2020; Chang et al., 2020b, 2021b,a), and use it as the weak annotator, similar by functionality to that of fidelity-weighted learning (Dehghani et al., 2017), or data creation tool Snorkel (Ratner et al., 2017). Learning with Semi-Supervision. Work on semi-supervised learning considers settings with some labeled data and a much larger set of unlabeled data, and then leverages both labeled the unlabeled data as in machine translation (Artetxe et al., 2017; Lample et al., 2017), data-to-text generation (Schmitt and Sch¨utze, 2019; Qader et"
2021.eacl-main.69,W19-8669,0,0.138852,"Missing"
2021.eacl-main.69,2020.acl-main.641,1,0.837276,"eak Supervision. Learning with weak supervision is a well-studied area that is popularized by the rise of data-driven neural approaches (Ratner et al., 2017; Safranchik et al., 2020; Bach et al., 2017; Wu et al., 2018; Dehghani et al., 2018; Jiang et al., 2018; Chang et al., 2020a; de Souza et al., 2018). Our approach incorporates similar line of work, by providing noisy labels (text) with a fine-tuned LM which incorporates prior knowledge from general-domain text and data-text pair (Budzianowski and Vuli´c, 2019; Chen et al., 2020; Peng et al., 2020; Mager et al., 2020; Harkous et al., 2020; Shen et al., 2020; Chang et al., 2020b, 2021b,a), and use it as the weak annotator, similar by functionality to that of fidelity-weighted learning (Dehghani et al., 2017), or data creation tool Snorkel (Ratner et al., 2017). Learning with Semi-Supervision. Work on semi-supervised learning considers settings with some labeled data and a much larger set of unlabeled data, and then leverages both labeled the unlabeled data as in machine translation (Artetxe et al., 2017; Lample et al., 2017), data-to-text generation (Schmitt and Sch¨utze, 2019; Qader et al., 2019) or more relevantly the joint learning framework f"
2021.eacl-main.69,W18-6530,1,0.877838,"Missing"
2021.eacl-main.69,P19-1545,0,0.0568544,"Missing"
2021.eacl-main.69,2020.acl-main.63,0,0.344083,"(outer). Introduction Natural language generation (NLG) is the task that transforms meaning representations (MR) into natural language descriptions (Reiter and Dale, 2000; Barzilay and Lapata, 2005); while natural language understanding (NLU) is the opposite process where text is converted into MR (Zhang and Wang, 2016). These two processes can thus constrain each other – recent exploration of the duality of neural natural language generation (NLG) and understanding (NLU) has led to successful semi-supervised learning techniques where both labeled and unlabeled data can be used for training (Su et al., 2020; Tseng et al., 2020; Schmitt and Sch¨utze, 2019; Qader et al., 2019; Su et al., 2020). Standard supervised learning for NLG and NLU depends on the access to labeled training data – a major bottleneck in developing new applications. In particular, neural methods require a large annotated dataset for each specific task. The collection process is often prohibitively expensive, especially when specialized domain expertise is required. On the other hand, learning with weak supervision from noisy labels offers a potential solution as it automatically builds imperfect training sets from low cost lab"
2021.eacl-main.69,D19-6122,0,0.0209664,"star rating. you can ﬁnd it near the sunshine vegetarian cafe Inference Eq. 2 Step 1. Train on weak data Eq. 3 ～ [MR] restaurant_name=blue spice, food=Indian, price_range=high, customer_rating=average [TEXT] Blue spice is an expensive Indian restaurant with an average customer rating. ～ x x x x x x x x Step 2. Train on combined data x x x x x x x x x x x x Figure 2: Depiction of the proposed framework. In joint learning, gradients are back-propagated through solid lines. 4 Creating Weakly Labeled Data directly, we employ them in a process analogous to knowledge distillation (Tan et al., 2018; Tang et al., 2019; Baziotis et al., 2020) where the finetuned GPT-2 provides supervisory signals instead of being used directly for generation. We now describe the process of GPT-2 finetuning. Given the sequential MR representation x1 · · · xM and a sentence y1 · · · yN in the labeled dataset (XL , YL ), we maximize the joint probability pGPT-2 (XL , YL ), where each sequence is concatenated into “[MR] x1 · · · xM [TEXT] y1 · · · yN ”. In addition, we also freeze the input embeddings when fine-tuning had positive impact on performance, following Mager et al. (2020). At test time, we provide the MR samples as c"
2021.eacl-main.69,2020.acl-main.163,0,0.228296,"ction Natural language generation (NLG) is the task that transforms meaning representations (MR) into natural language descriptions (Reiter and Dale, 2000; Barzilay and Lapata, 2005); while natural language understanding (NLU) is the opposite process where text is converted into MR (Zhang and Wang, 2016). These two processes can thus constrain each other – recent exploration of the duality of neural natural language generation (NLG) and understanding (NLU) has led to successful semi-supervised learning techniques where both labeled and unlabeled data can be used for training (Su et al., 2020; Tseng et al., 2020; Schmitt and Sch¨utze, 2019; Qader et al., 2019; Su et al., 2020). Standard supervised learning for NLG and NLU depends on the access to labeled training data – a major bottleneck in developing new applications. In particular, neural methods require a large annotated dataset for each specific task. The collection process is often prohibitively expensive, especially when specialized domain expertise is required. On the other hand, learning with weak supervision from noisy labels offers a potential solution as it automatically builds imperfect training sets from low cost labeling rules or pretr"
2021.eacl-main.69,J09-4008,0,0.0530784,"Missing"
2021.eacl-main.69,W19-8639,0,0.0206139,"ioning on samples from the augmented MR set (XU ). Rather than using GPT-2 outputs The fine-tuned LM conditions on augmented MR sample set XU to generate the in-domain text1 , forming the weak label dataset DW = (XU , Y˜L ) with noisy labels y˜i ∈ Y˜L . In practice, the finetuned LM produces malformed, synthetic text which does not fully match with the MR it was conditioned on, as it might hallucinate additional values not consistent with its MR counterpart. Thus, it is necessary to check for factual consistency (Moryossef et al., 2019). We address this point next. Past findings showed (e.g. (Wang, 2019)) that the removal of utterance with “hallucinated” facts (MR values) from MR leads to considerable performance gain, since inconsistent MR-Text correspondence might misguide systems to generate incorrect facts and deteriorate the NLG outputs. We filter out the synthetic, poor quality MR-text 820 1 We adopt the Top-k random sampling with k = 2 to encourage diversity and reduce repetition (Radford et al., 2019) pairs by training a separate NLU model on the original labeled data to predict MR from generated text labels. These MRs can then be checked against the paired MR in DW via pattern matchi"
2021.eacl-main.69,P16-1009,0,0.0514398,"rt from the above unsupervised objectives, we can impose the supervised objective on the k labeled pairs: max Ex,y∼p(XL ,YL ) log pθ (y|x)+log pφ (x|y) (4) θ,φ Each MR is flattened into a sequence and fed into the NLG encoder, giving NLG and NLU models an inductive bias to project similar MR/text into the surrounding latent space (Chisholm et al., 2017). As we observed anecdotally3 , the information flow enabled by REINFORCE allows the models to utilize unlabeled MR and text, boosting the performance in our scenarios. 821 2 This direction is usually termed as back translation in MT community (Sennrich et al., 2016; Lample et al., 2018) 3 Tseng et al. (2020) noticed similar trend in the experiments. 6 Learning with Weak Supervision The primary challenge that arises from the synthetic data is the noise introduced during the generation process. Noisy and poor quality labels tend to bring little to no improvements (Elman, 1993; Fr´enay and Verleysen, 2013). To better train on the large and noisy corpus described in section §4 (size ~500k), we employ a two-step training process motivated by fidelity-weighted learning (Dehghani et al., 2018). The two-step process consists of (1) pretraining and (2) quality-w"
2021.eacl-main.69,P18-2067,0,0.287109,"Missing"
2021.inlg-1.36,2020.acl-main.18,0,0.0274372,"selecting K data for annotating the reference text. We aim to identify the K most representative instances that, when annotated and trained on them, leads to a best model performance. Few-shot text generation is an important research topic since obtaining large-scale training data for each individual downstream task is prohibitively expensive. Recently, pretraining large neural networks with a language modeling objective has led to significant improvements across different fewshot text generation tasks (Radford et al., 2019; Lewis et al., 2020) and many techniques are proposed based on them (Chen et al., 2020; Schick and Sch¨utze, 2020a; Zhang et al., 2020; Kale, 2020; Chang et al., 2020, 2021a,b; Li and Liang, 2021). However, all previous works simulate the few-shot scenario by randomly sampling a subset from the full training data. Little to no attention has been paid to the selection strategies. The goal of the proposal is to call for innovative ideas on searching for an optimal strategy to select the few-shot training instances, as well as a comprehensive analysis of how the selection strategy would affect the model performance. The study of selection strategies is motivated by two rationales:"
2021.inlg-1.36,P17-1123,0,0.0222305,"tion strategy can be seamlessly applied to same tasks in other languages. • Model agnostic. The selection strategy can select most informative instances that improve 326 1. Data-to-text: We use the dataset for the E2E challenge (Novikova et al., 2017) which contain 50,602 data-text pairs with 8 unique slots in the restaurant domain. 2. Document Summarization: We use the CNN/Dailymail dataset (non-anonymized version) (Hermann et al., 2015) which contains 312,084 document-summary pairs. 3. Question generation: We use the SQuAD dataset (Rajpurkar et al., 2016) with over 100k questions. Following Du et al. (2017), we focus on the answer-independent scenario to directly generate questions from passages. All the above datasets contain parallel inputoutput pairs for train/validation/test. We can simulate our few-shot scenario by only allowing leveraging K input-output pairs from the training set. The participants can decide which K training instances to select based on all the inputs in the training set 1 . Once the selected instances are determined, the model can then be trained on the K input-output pairs. It is also worth mentioning that in order to simulate the true few-shot scenario, participants ca"
2021.inlg-1.36,2020.acl-main.454,0,0.0125681,"erlap between the model output and the gold references, including many popular metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). • Semantic relevance, which measures the semantic similarity between the model output and the gold references, including the newly proposed BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). • Consistency with task input, which measures if the output contains consistent information with the task input and no hallucinations. Many works have proposed metrics based on question answering (Eyal et al., 2019; Durmus et al., 2020), natural language inference (Kumar and Talukdar, 2020) and mutual information (Shen et al., 2018; Zhang et al., 2018). • Output diversity, which measures if the model can produce diverse outputs with different inputs, including metrics like the count and entropy of distinct uni/bi-grams (Li et al., 2016; Duˇsek et al., 2020). • Other task-specific requirement, e.g., sloterror rate for data-to-text and compression rate for document summarization. After the submission system opens, we will announce the metrics we picked for the automatic evaluation and provide the evaluation script. 3.2 Human E"
2021.inlg-1.36,N19-1395,0,0.0549186,"Missing"
2021.inlg-1.36,2020.inlg-1.14,0,0.0205205,"tify the K most representative instances that, when annotated and trained on them, leads to a best model performance. Few-shot text generation is an important research topic since obtaining large-scale training data for each individual downstream task is prohibitively expensive. Recently, pretraining large neural networks with a language modeling objective has led to significant improvements across different fewshot text generation tasks (Radford et al., 2019; Lewis et al., 2020) and many techniques are proposed based on them (Chen et al., 2020; Schick and Sch¨utze, 2020a; Zhang et al., 2020; Kale, 2020; Chang et al., 2020, 2021a,b; Li and Liang, 2021). However, all previous works simulate the few-shot scenario by randomly sampling a subset from the full training data. Little to no attention has been paid to the selection strategies. The goal of the proposal is to call for innovative ideas on searching for an optimal strategy to select the few-shot training instances, as well as a comprehensive analysis of how the selection strategy would affect the model performance. The study of selection strategies is motivated by two rationales: Equal contribution. X.shen is now at Amazon Alexa AI. L Few"
2021.inlg-1.36,2020.acl-main.771,0,0.0214726,"nces, including many popular metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). • Semantic relevance, which measures the semantic similarity between the model output and the gold references, including the newly proposed BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). • Consistency with task input, which measures if the output contains consistent information with the task input and no hallucinations. Many works have proposed metrics based on question answering (Eyal et al., 2019; Durmus et al., 2020), natural language inference (Kumar and Talukdar, 2020) and mutual information (Shen et al., 2018; Zhang et al., 2018). • Output diversity, which measures if the model can produce diverse outputs with different inputs, including metrics like the count and entropy of distinct uni/bi-grams (Li et al., 2016; Duˇsek et al., 2020). • Other task-specific requirement, e.g., sloterror rate for data-to-text and compression rate for document summarization. After the submission system opens, we will announce the metrics we picked for the automatic evaluation and provide the evaluation script. 3.2 Human Evaluation We will also provide human evaluation scores"
2021.inlg-1.36,2021.emnlp-main.243,0,0.0387393,"Missing"
2021.inlg-1.36,2020.acl-main.703,0,0.203326,"and L indicates labeled instances. The annotation budget only allows selecting K data for annotating the reference text. We aim to identify the K most representative instances that, when annotated and trained on them, leads to a best model performance. Few-shot text generation is an important research topic since obtaining large-scale training data for each individual downstream task is prohibitively expensive. Recently, pretraining large neural networks with a language modeling objective has led to significant improvements across different fewshot text generation tasks (Radford et al., 2019; Lewis et al., 2020) and many techniques are proposed based on them (Chen et al., 2020; Schick and Sch¨utze, 2020a; Zhang et al., 2020; Kale, 2020; Chang et al., 2020, 2021a,b; Li and Liang, 2021). However, all previous works simulate the few-shot scenario by randomly sampling a subset from the full training data. Little to no attention has been paid to the selection strategies. The goal of the proposal is to call for innovative ideas on searching for an optimal strategy to select the few-shot training instances, as well as a comprehensive analysis of how the selection strategy would affect the model performance."
2021.inlg-1.36,N16-1014,0,0.0315794,"proposed BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). • Consistency with task input, which measures if the output contains consistent information with the task input and no hallucinations. Many works have proposed metrics based on question answering (Eyal et al., 2019; Durmus et al., 2020), natural language inference (Kumar and Talukdar, 2020) and mutual information (Shen et al., 2018; Zhang et al., 2018). • Output diversity, which measures if the model can produce diverse outputs with different inputs, including metrics like the count and entropy of distinct uni/bi-grams (Li et al., 2016; Duˇsek et al., 2020). • Other task-specific requirement, e.g., sloterror rate for data-to-text and compression rate for document summarization. After the submission system opens, we will announce the metrics we picked for the automatic evaluation and provide the evaluation script. 3.2 Human Evaluation We will also provide human evaluation scores on the system outputs since none of the automatic metrics can correlate perfectly with the generation quality. We will follow the recently proposed taxonomy of human evaluation measures by Belz et al. (2020); Su et al. (2020) and follow the reporting"
2021.inlg-1.36,2021.acl-long.353,0,0.0284703,"Missing"
2021.inlg-1.36,2020.acl-main.704,0,0.021188,"m is open. Participants are encouraged not to focus on one specific metric to avoid overfitting to it. The final evaluation will adopt metrics following into the following categories: 327 • Lexical similarity, which measure the lexical overlap between the model output and the gold references, including many popular metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). • Semantic relevance, which measures the semantic similarity between the model output and the gold references, including the newly proposed BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). • Consistency with task input, which measures if the output contains consistent information with the task input and no hallucinations. Many works have proposed metrics based on question answering (Eyal et al., 2019; Durmus et al., 2020), natural language inference (Kumar and Talukdar, 2020) and mutual information (Shen et al., 2018; Zhang et al., 2018). • Output diversity, which measures if the model can produce diverse outputs with different inputs, including metrics like the count and entropy of distinct uni/bi-grams (Li et al., 2016; Duˇsek et al., 2020). • Other task-specific requirement"
2021.inlg-1.36,D18-1463,1,0.831524,"pineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). • Semantic relevance, which measures the semantic similarity between the model output and the gold references, including the newly proposed BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). • Consistency with task input, which measures if the output contains consistent information with the task input and no hallucinations. Many works have proposed metrics based on question answering (Eyal et al., 2019; Durmus et al., 2020), natural language inference (Kumar and Talukdar, 2020) and mutual information (Shen et al., 2018; Zhang et al., 2018). • Output diversity, which measures if the model can produce diverse outputs with different inputs, including metrics like the count and entropy of distinct uni/bi-grams (Li et al., 2016; Duˇsek et al., 2020). • Other task-specific requirement, e.g., sloterror rate for data-to-text and compression rate for document summarization. After the submission system opens, we will announce the metrics we picked for the automatic evaluation and provide the evaluation script. 3.2 Human Evaluation We will also provide human evaluation scores on the system outputs since none of the au"
2021.inlg-1.36,2020.emnlp-main.535,1,0.736088,"of distinct uni/bi-grams (Li et al., 2016; Duˇsek et al., 2020). • Other task-specific requirement, e.g., sloterror rate for data-to-text and compression rate for document summarization. After the submission system opens, we will announce the metrics we picked for the automatic evaluation and provide the evaluation script. 3.2 Human Evaluation We will also provide human evaluation scores on the system outputs since none of the automatic metrics can correlate perfectly with the generation quality. We will follow the recently proposed taxonomy of human evaluation measures by Belz et al. (2020); Su et al. (2020) and follow the reporting strategies proposed by Howcroft et al. (2020). The human evaluation will be focused on the following two parts, which are specifically hard to be accurately measured by automatic metrics: • Fluency. If the output itself is a fluent sentence that can be well understood by humans, defined by a 5-scale Likert score. • Consistency. If the output is consistent with the input and does not contain hallucinations, defined by a binary true/false score. The human evaluation will be conducted after collecting all the submissions. It will be performed under a unified pipeline and"
2021.inlg-1.36,2020.acl-main.163,0,0.0230227,"selection algorithm by finetuning the open-sourced Bart model (Lewis et al., 2020) on the selected training instances with maximum likelihood. Bart is pretrained with a denoising autoencoder objective on large amount of text data and has been the state-of-the-arts for many text generation tasks. Therefore, we recommend to first test with this standard generative model. There have been many algorithms proposed for improved generation quality under the few-shot scenario like pattern exploitation training (Schick and Sch¨utze, 2020a; Li and Liang, 2021; Lester et al., 2021) and cyclic training (Tseng et al., 2020; Chang et al., 2021a; Guo et al., 2021). We welcome test results using different types of generative models. Nonetheless, the focus of the shared task is on the instance selection algorithm but not the few-shot generative model. While it is nice to provide data points that demonstrate state-of-the-art results, generating with the most advanced model for better evaluation scores is by not means the main purpose. 2.4 3 Evaluation The final evaluation will be conducted on the following two settings: 1. We apply the submitted selection algorithms to select K training instances and then finetune o"
2021.inlg-1.36,W17-5525,0,0.0606186,"Missing"
2021.inlg-1.36,P02-1040,0,0.109594,"anism. Model outputs will be compared with both automatic metrics and human evaluation. 3.1 Automatic Evaluation The evaluation metrics differ according to the downstream tasks. The metrics used for the final evaluation will be announced after the submission system is open. Participants are encouraged not to focus on one specific metric to avoid overfitting to it. The final evaluation will adopt metrics following into the following categories: 327 • Lexical similarity, which measure the lexical overlap between the model output and the gold references, including many popular metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). • Semantic relevance, which measures the semantic similarity between the model output and the gold references, including the newly proposed BertScore (Zhang et al., 2019) and BLEURT (Sellam et al., 2020). • Consistency with task input, which measures if the output contains consistent information with the task input and no hallucinations. Many works have proposed metrics based on question answering (Eyal et al., 2019; Durmus et al., 2020), natural language inference (Kumar and Talukdar, 2020) and mutual information (Shen et al., 2018; Z"
2021.inlg-1.36,D16-1264,0,0.0226802,"various input-output formats. • Language agnostic. The selection strategy can be seamlessly applied to same tasks in other languages. • Model agnostic. The selection strategy can select most informative instances that improve 326 1. Data-to-text: We use the dataset for the E2E challenge (Novikova et al., 2017) which contain 50,602 data-text pairs with 8 unique slots in the restaurant domain. 2. Document Summarization: We use the CNN/Dailymail dataset (non-anonymized version) (Hermann et al., 2015) which contains 312,084 document-summary pairs. 3. Question generation: We use the SQuAD dataset (Rajpurkar et al., 2016) with over 100k questions. Following Du et al. (2017), we focus on the answer-independent scenario to directly generate questions from passages. All the above datasets contain parallel inputoutput pairs for train/validation/test. We can simulate our few-shot scenario by only allowing leveraging K input-output pairs from the training set. The participants can decide which K training instances to select based on all the inputs in the training set 1 . Once the selected instances are determined, the model can then be trained on the K input-output pairs. It is also worth mentioning that in order to"
2021.inlg-1.36,2021.naacl-main.185,0,0.0751525,"Missing"
2021.lchange-1.1,2020.coling-demos.3,1,0.71446,"past attempts at building such translation systems yield poor performance that renders them practically unusable as-is in the practical settings (Zhang et al., 2018; Liu et al., 2019) – these efforts are still largely limited as parallel data is scarce for some eras. Recent advances in machine translation and text style transfer/generation utilize semi-supervised techniques to tackle similar challenges by aligning latent representations from different styles for the low resource scenarios (Shen et al., 2017; Hu et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Jin et al., 2019; Chang et al., 2020, 2021c). To this end, we aim to bridge this gap that makes the following contributions: Abstract In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text: (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following: We reframe the task as a multi-label prediction task where the model predicts both the transla"
2021.lchange-1.1,2021.eacl-main.69,1,0.823974,"Missing"
2021.lchange-1.1,2021.eacl-main.64,1,0.798859,"Missing"
2021.lchange-1.1,2021.eacl-main.61,1,0.722017,"tten. 3 Proposed Framework 4.1 Semi-Supervised Translation Model Our sequence-to-sequence model is based on the Transformer (Vaswani et al., 2017) encoderdecoder architecture. Given an input, the encoder first converts it into an intermediate vector, and then the decoder takes the intermediate representation as input to generate a target output. In what follows, we describe the training objectives that allows the translation model to utilize augmented monolingual data. Semi-Supervised Objectives. Inspired by the previous work on CycleGANs (Zhu et al., 2017) and dual learning (He et al., 2016; Chang et al., 2021a,b), our method trains the initial model in both forward and backward directions, and defines a semi-supervised optimization objective that combines direct supervision (Lsupervised ) and a language model loss (Llm ) over the parallel data P , and two monolingual corpora A and M : Task Formulation L = Lsupervised (P ) + Llm (A) + Llm (M ) We assume two nonparallel datasets A and M of sentences in Ancient Chinese (zh-a) and Modern Chinese (zh-m) respectively.A parallel dataset P that contains the pairs of sentences in both variants of text is also present. The sizes of the three datasets are de"
2021.lchange-1.1,P18-1080,0,0.0240096,"their generalizability. On the other hand, past attempts at building such translation systems yield poor performance that renders them practically unusable as-is in the practical settings (Zhang et al., 2018; Liu et al., 2019) – these efforts are still largely limited as parallel data is scarce for some eras. Recent advances in machine translation and text style transfer/generation utilize semi-supervised techniques to tackle similar challenges by aligning latent representations from different styles for the low resource scenarios (Shen et al., 2017; Hu et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Jin et al., 2019; Chang et al., 2020, 2021c). To this end, we aim to bridge this gap that makes the following contributions: Abstract In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text: (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following: We reframe the task as a multi-label prediction task whe"
2021.lchange-1.1,N18-1012,0,0.0134745,"lation systems and limits their generalizability. On the other hand, past attempts at building such translation systems yield poor performance that renders them practically unusable as-is in the practical settings (Zhang et al., 2018; Liu et al., 2019) – these efforts are still largely limited as parallel data is scarce for some eras. Recent advances in machine translation and text style transfer/generation utilize semi-supervised techniques to tackle similar challenges by aligning latent representations from different styles for the low resource scenarios (Shen et al., 2017; Hu et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Jin et al., 2019; Chang et al., 2020, 2021c). To this end, we aim to bridge this gap that makes the following contributions: Abstract In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text: (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following: We reframe the task as a multi-"
2021.lchange-1.1,D19-1499,0,0.0279798,"Missing"
C12-1163,P02-1026,0,0.17611,"Missing"
C12-1163,D09-1036,0,0.205183,"Missing"
C12-1163,prasad-etal-2008-penn,0,0.343061,"Missing"
C12-1163,W10-4326,0,0.0240851,"Missing"
C16-1144,D12-1023,0,\N,Missing
C16-1144,C10-1012,0,\N,Missing
C16-1144,P02-1003,0,\N,Missing
C16-1144,P06-1130,0,\N,Missing
C16-1144,P08-1022,0,\N,Missing
C16-1144,J07-3004,0,\N,Missing
C16-1144,W13-2104,1,\N,Missing
C16-1144,P96-1027,0,\N,Missing
C16-1144,W05-1605,0,\N,Missing
C16-1144,W11-2827,0,\N,Missing
C16-1144,C12-1124,0,\N,Missing
D12-1033,P81-1022,0,0.616909,"Missing"
D12-1033,W10-2010,0,0.032828,"Missing"
D12-1033,N01-1021,0,0.908683,"Missing"
D12-1033,J01-2004,0,0.0917442,"Missing"
D12-1033,D09-1034,0,0.299554,"Missing"
D14-1036,W09-1206,0,0.0606412,"Missing"
D14-1036,W05-0620,0,0.405746,"Missing"
D14-1036,P00-1058,0,0.0769682,"modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefix tree. of non-predictive elementary trees. An example of a PLTAG derivation is given in Figure 2. In step 1, a prediction tree is introduced through substitution, which then allows the adjunction of an adverb in step 2. Step 3 involves the verification of the marker introduced by the prediction tree against the elementary tree for open. In order to efficiently parse PLTAG, Demberg et al. (2013) introduce the concept of fringes. Fringes captu"
D14-1036,J13-4008,1,0.901452,"letions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental dependency parser, do not share this advantage, as we will discuss in Section 4.3. 2 semantic role labeling is a novel task. Our model builds on an incremental Tree Adjoining Grammar parser (Demberg et al., 2013) which predicts the syntactic structure of upcoming input. This all"
D14-1036,J05-1004,0,0.125834,"ay hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fri"
D14-1036,W07-2416,0,0.0663346,"Missing"
D14-1036,C92-2066,0,0.6767,"edicts the syntactic structure of upcoming input. This allows us to perform incremental parsing and incremental SRL in tandem, exploiting the predictive component of the parser to assign (potentially incomplete) semantic roles on a word-by-word basis. Similar to work on incremental parsing that evaluates incomplete trees (Sangati and Keller, 2013), we evaluate the incomplete semantic structures produced by our model. 3 Psycholinguistically Motivated TAG Demberg et al. (2013) introduce Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG), a grammar formalism that extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. Elementary trees that contain a foot node are called auxiliary trees; those that do not are called initial trees. Examples for TAG elementary trees are given in Figure 1a–c. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the"
D14-1036,P10-2012,1,0.843954,"mantic garden paths occur because 301 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301–312, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics provide semantically informed completions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental depen"
D14-1036,W13-2607,1,0.850665,"sponding nodes in Tv . For simplicity of presentation, we will use a concrete example, see Figure 5. Figure 5a shows the lexicon entries for the words of the sentence Semantic Role Lexicon Recall that Propbank is used to construct the PLTAG treebank, in order to distinguish between arguments and modifiers, which result in elementary trees with substitution nodes, and auxiliary trees, i.e., trees with a foot node, respectively (see Figure 1). Conveniently, we can use the same information to also enrich the extracted lexicon with the semantic role annotations, following the process described by Sayeed and Demberg (2013).1 For arguments, annotations are retained on the substitution node in the parental tree, while for modifiers, the role annotation is displayed on the foot node of the auxiliary tree. Note that we display role annotation on traces that are leaf nodes, 1 Contrary to Sayeed and Demberg (2013) we put role label annotations for PPs on the preposition rather than their NP child, following of the CoNLL 2005 shared task (Carreras and M`arquez, 2005). 2 Prediction tree T in our algorithm is only used during pr verification, so it set to nil for substitution and adjunction operations. 304 Banks refused"
D14-1036,J14-3006,1,0.841387,"nd argument identification. In this respect it is analogous to unlabeled dependency accuracy reported in the parsing literature. We exSystem Comparison We evaluated three configurations of our system. The first configuration (iSRL) uses all semantic roles for each PLTAG lexicon entry, applies the PLTAG parser, IRPA, and both classifiers to perform identification and disambiguation, as described in Section 4. The second one (MajorityBaseline), solves the problem of argument identification and role disambiguation without the classifiers. For the former we employ a set of heuristics according to Lang and Lapata (2014), that rely on gold syntactic dependency information, sourced from CoNLL input. For the latter, we choose the most frequent role given the gold standard dependency relation label for the particular argument. Note that dependencies have been produced in view of the whole sentence and not incrementally. 308 System iSRL-Oracle iSRL Majority-Baseline Malt-Baseline Prec 91.00 81.48 71.05 60.90 Rec 80.26 75.51 58.10 46.14 F1 85.29 78.38 63.92 52.50 prefixes (up to word 10), presumably as it does not benefit from syntactic prediction, and thus cannot generate incomplete triples early in the sentence,"
D14-1036,D07-1062,0,0.165244,"essing as the path from an argument to the predicate can be very informative but is often quite complicated, and depends on the syntactic formalism used. Many paths through the parse tree are likely to occur infrequently (or not at all), resulting in very sparse information for the classifier to learn from. Moreover, as we will discuss in Section 4.4, such path information is not always available when the input is processed incrementally. There is previous SRL work employing Tree Adjoining Grammar, albeit in a non-incremental setting, as a means to reduce the sparsity of syntaxbased features. Liu and Sarkar (2007) extract a rich feature set from TAG derivations and demonstrate that this improves SRL performance. In contrast to incremental parsing, incremental 302 (a) NP (b) S NNS NP↓ Banks (d) S1 (c) VP VP AP VB RB open rarely VP* A1 A0 NP1 ↓ VP11 Banks refused nsbj a B↓ a B C↓ a B↓ b (a) valid open aux today hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) aux"
D14-1036,W08-2121,0,0.0710313,"Missing"
D14-1036,J93-2004,0,0.0497726,"anks (d) S1 (c) VP VP AP VB RB open rarely VP* A1 A0 NP1 ↓ VP11 Banks refused nsbj a B↓ a B C↓ a B↓ b (a) valid open aux today hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a p"
D14-1036,J08-2001,0,0.0889078,"Missing"
D14-1036,W00-1307,0,0.0413792,"h with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefi"
D14-1036,W09-1201,0,\N,Missing
D14-1036,Q13-1010,1,\N,Missing
D16-1017,P14-1023,0,0.443736,"se sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for development and testing, and the rest were used for training. In order to construct our incremental model and compare it to n-gram language models, we needed a precise mapping between the lemmatized argument words and their positions in the original sentence. This required aligning the SENNA tokenization and the original ukWaC tokenization used for MaltParser. Because of the heterogeneous nature of web data, this alignment was"
D16-1017,J10-4006,0,0.298302,"Greenberg (patient) Pado+McRae+Ferretti # ratings 414 1444 274 248 720 2380 Roles ARG0, ARG1, ARG2 ARG0, ARG1 ARGM-LOC ARGM-MNR ARG1 NN RF 0.52 (8) 0.38 (20) 0.44 (3) 0.45 (6) 0.61 (8) 0.41 (37) BL2010 0.53 (0) 0.32 (70) 0.23 (3) 0.36 (17) 0.46 (18) 0.35 (90) GSD2015 0.53 (0) 0.36 (70) 0.29 (3) 0.42 (17) 0.48 (18) 0.38 (90) BDK2014 0.41 0.28 - Table 2: Thematic fit evaluation scores, consisting of Spearman’s ρ correlations between average human judgements and model output, with numbers of missing values (due to missing vocabulary entries) in brackets. The baseline scores come from the TypeDM (Baroni and Lenci, 2010) model, further developed and evaluated in Greenberg et al. (2015a,b) and the neural network predict model described in Baroni et al. (2014). NN RF is the non-incremental model presented in this article. Our model maps ARG2 in Pado to OTHER role. Significances were calculated using paired two-tailed significance tests for correlations (Steiger, 1980). NN RF was significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better than BL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for"
D16-1017,P07-1028,0,0.177316,"y significant for Pado and Ferretti instruments. 4.1 Related work State-of-the-art computational models of thematic fit quantify the similarity between a role filler of a verb and the proto-typical filler for that role for the verb based on distributional vector space models. For example, the thematic fit of grass as a patient for the verb eat would be determined by the cosine of a distributional vector representation of grass and a prototypical patient of eat. The proto-typical patient is in turn obtained from averaging representations of words that typically occur as a patient of eat (e.g., Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Greenberg et al., 2015b). For more than one role, information from both the agent and the predicate can be used to jointly to predict a patient (e.g., Lenci, 2011). 4.2 Data Previous studies obtained thematic fit ratings from humans by asking experimental participants to rate how common, plausible, typical, or appropriate some test role-fillers are for given verbs on a scale from 1 (least plausible) to 7 (most plausible) (McRae et al., 1998; Ferretti et al., 2001; Binder et al., 2001; Pad´o, 2007; Pad´o et al., 2009; Vandekerckhove et al., 20"
D16-1017,W15-1106,1,0.918364,"720 2380 Roles ARG0, ARG1, ARG2 ARG0, ARG1 ARGM-LOC ARGM-MNR ARG1 NN RF 0.52 (8) 0.38 (20) 0.44 (3) 0.45 (6) 0.61 (8) 0.41 (37) BL2010 0.53 (0) 0.32 (70) 0.23 (3) 0.36 (17) 0.46 (18) 0.35 (90) GSD2015 0.53 (0) 0.36 (70) 0.29 (3) 0.42 (17) 0.48 (18) 0.38 (90) BDK2014 0.41 0.28 - Table 2: Thematic fit evaluation scores, consisting of Spearman’s ρ correlations between average human judgements and model output, with numbers of missing values (due to missing vocabulary entries) in brackets. The baseline scores come from the TypeDM (Baroni and Lenci, 2010) model, further developed and evaluated in Greenberg et al. (2015a,b) and the neural network predict model described in Baroni et al. (2014). NN RF is the non-incremental model presented in this article. Our model maps ARG2 in Pado to OTHER role. Significances were calculated using paired two-tailed significance tests for correlations (Steiger, 1980). NN RF was significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better than BL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for Pado and Ferretti instruments. 4.1 Related work State-of-the-art"
D16-1017,P98-1035,0,0.167769,"sad Sayeed and Dietrich Klakow and Stefan Thater Saarland University 66123 Saarbr¨ucken, Germany {vera,asayeed,stth} @coli.uni-sb.de; dietrich.klakow@lsv.uni-sb.de Abstract sense, the task is closely related to work on selectional preference acquisition (Van de Cruys, 2014). We focus here on the roles agent, patient, location, time, manner and the predicate itself. The model we develop is trained to represent the eventrelevant context and hence systematically captures long-range dependencies. This has been previously shown to be beneficial also for more general language modelling tasks (e.g., Chelba and Jelinek, 1998; Tan et al., 2012). A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be"
D16-1017,P07-1071,0,0.278611,"inputs, while our model gives a probability distribution over all words for the queried target role. We discuss the components necessary for our model in more detail in section 3. 2 Data source Our source of training data is the ukWaC corpus, which is part of the WaCky project, as well as the British National Corpus. The corpus consists of web pages crawled from the .uk web domain, containing approximately 138 million sentences. These sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for de"
D16-1017,C14-1134,0,0.0662196,"Missing"
D16-1017,N15-1003,1,0.727949,"diction, selectional preference models score the inputs, while our model gives a probability distribution over all words for the queried target role. We discuss the components necessary for our model in more detail in section 3. 2 Data source Our source of training data is the ukWaC corpus, which is part of the WaCky project, as well as the British National Corpus. The corpus consists of web pages crawled from the .uk web domain, containing approximately 138 million sentences. These sentences were run through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fo"
D16-1017,J15-1004,0,0.0645383,"however also model the interaction between different roles; see Figure 2 for an example of model predictions. We are only aware of one small dataset that can be used to systematically test the effectiveness of the compositionality for this task. The Bicknell et al. (2010) dataset contains triples like journalist check 178 Evaluation of event representations: sentence similarity To show that our model learns to represent input words and their roles in a useful way that reflects the meaning and interactions between inputs, we evaluate our non-incremental model on a sentence similarity task from Grefenstette and Sadrzadeh (2015). We assign similarity scores to sentence pairs by computing representations for each sentence by tak# ratings 199 NN RF 0.34 Kronecker 0.26 W2V 0.13 Humans 0.62 Table 5: Sentence similarity evaluation scores on GS2013 dataset (Grefenstette and Sadrzadeh, 2015), consisting of Spearman’s ρ correlations between human judgements and model output. Kronecker is the best performing model from Grefenstette and Sadrzadeh (2015). NN RF is the non-incremental model presented in this article, and W2V is the word2vec baseline. Human performance (inter-annotator agreement) shows the upper bound. Figure 2:"
D16-1017,D14-1140,0,0.0613349,"Missing"
D16-1017,P15-1115,0,0.0871915,". The latter could be useful for inferring missing information in entailment tasks or improving identification of thematic roles outside the sentence containing the predicate. Potential applications also include predicate prediction based on arguments and roles, which has been noted to be relevant for simultaneous machine translation for a verb-final to a verb-medial source language (Grissom II et al., 2014). Within cognitive modelling, our model could help to more accurately estimate semantic surprisal for broadcoverage texts, when used in combination with an incremental role labeller (e.g., Konstas and Keller, 2015), or to provide surprisal estimates for content words as a control variable for psycholinguistic experimental materials. In this work, we focus on the predictability of verbs and nouns, and we suggest that the predictability of these words depends to a large extent on the relationship of these words to other nouns and 171 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 171–182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics verbs, especially those connected via the same event. We choose a neural network (NN) mod"
D16-1017,W11-0607,0,0.788422,"t role for the verb based on distributional vector space models. For example, the thematic fit of grass as a patient for the verb eat would be determined by the cosine of a distributional vector representation of grass and a prototypical patient of eat. The proto-typical patient is in turn obtained from averaging representations of words that typically occur as a patient of eat (e.g., Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Greenberg et al., 2015b). For more than one role, information from both the agent and the predicate can be used to jointly to predict a patient (e.g., Lenci, 2011). 4.2 Data Previous studies obtained thematic fit ratings from humans by asking experimental participants to rate how common, plausible, typical, or appropriate some test role-fillers are for given verbs on a scale from 1 (least plausible) to 7 (most plausible) (McRae et al., 1998; Ferretti et al., 2001; Binder et al., 2001; Pad´o, 2007; Pad´o et al., 2009; Vandekerckhove et al., 2009; Greenberg et al., 2015a). The datasets include agent, patient, location and instrument roles. For example, in the Pad´o et al. (2009) dataset, the noun sound has a very low rating of 1.1 as the subject of hear a"
D16-1017,J12-3007,0,0.0185998,"akow and Stefan Thater Saarland University 66123 Saarbr¨ucken, Germany {vera,asayeed,stth} @coli.uni-sb.de; dietrich.klakow@lsv.uni-sb.de Abstract sense, the task is closely related to work on selectional preference acquisition (Van de Cruys, 2014). We focus here on the roles agent, patient, location, time, manner and the predicate itself. The model we develop is trained to represent the eventrelevant context and hence systematically captures long-range dependencies. This has been previously shown to be beneficial also for more general language modelling tasks (e.g., Chelba and Jelinek, 1998; Tan et al., 2012). A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be used to provide a"
D16-1017,D14-1004,0,0.242497,"Missing"
D16-1017,E09-1094,0,0.680448,"through a semantic role labeller and head words were extracted as described in Sayeed et al. (2015). The semantic role labeller used, SENNA (Collobert and Weston, 2007), generates PropBank-style role labels. While PropBank argument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly representing “classical” thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., Baroni et al., 2014; Vandekerckhove et al., 2009), For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for development and testing, and the rest were used for training. In order to construct our incremental model and compare it to n-gram language models, we needed a precise mapping between the lemmatized argument words and their positions in the original sentence. This required aligning the SENNA tokenization and the original ukWaC tokenization used for MaltParser. Because of the heterogeneous nature of web data, this alignment was not always achievable—we skipp"
D19-1586,N19-1423,0,0.0430329,"implicit discourse relation classification on different settings of PDTB level 2 relations. NSP refers to the subtask “next sentence prediction” in the pre-training of BERT. Numbers in bold signal significant improvements over the previous state of the art (p&lt;0.01). Numbers with ∗ denote significant improvements over BERT + WSJ w/o NSP with p&lt;0.01. ment 2 are separated with token “[SEP]”; “[CLS]” is the special classification embedding while “C” is the same as “[CLS]” in pre-training but the ground-truth label in the fine-tuning. In the experiments, we used the uncased base model1 provided by Devlin et al. (2019), which is trained on BooksCorpus and English Wikipedia with 3300M tokens in total. 3.1 Evaluation on PDTB We used the Penn Discourse Tree Bank (Prasad et al., 2008), the largest available manually annotated discourse corpus. It provides a three level hierarchy of relation tags. Following the experimental settings and evaluation metrics in Bai and Zhao (2018), we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009), which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015), which uses 2-20, 0-1, 21-22 as tr"
D19-1586,P14-1092,0,0.185773,"Missing"
D19-1586,Q15-1024,0,0.0328036,"ed the uncased base model1 provided by Devlin et al. (2019), which is trained on BooksCorpus and English Wikipedia with 3300M tokens in total. 3.1 Evaluation on PDTB We used the Penn Discourse Tree Bank (Prasad et al., 2008), the largest available manually annotated discourse corpus. It provides a three level hierarchy of relation tags. Following the experimental settings and evaluation metrics in Bai and Zhao (2018), we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009), which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015), which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score. In addition, we also performed 10-fold cross validation among sections 022, as promoted in Shi and Demberg (2017). We also follow the standard in the literature to formulate the task as an 11-way classification task. Results are presented in Table 1. We evaluated three versions of the BERT-based model. All of our BERT models use the pre-trained representations and are fine-tuned on the PDTB training data. The version marked as “BERT” does not do any additional pre-training. BERT+WSJ in ad"
D19-1586,N16-1037,0,0.0292306,"Missing"
D19-1586,C18-1049,0,0.364077,"ing. We added this variant to measure the benefit of in-domain NSP on discourse relation classification (note though that the downloaded pre-trained BERT model contains the NSP task in the original pre-training). We compared the results with four state-of-theart systems: Cai and Zhao (2017) proposed a model that takes a step towards calculating discourse expectations by using attention over an encoding of the first argument, to generate the representation of the second argument, and then learning a classifier based on the concatenation of the encodings of the two discourse relation arguments. Kishimoto et al. (2018) fed external world knowledge (ConceptNet relations and coreferences) explicitly into MAGE-GRU (Dhingra et al., 2017) and achieved improvements compared to only using the relational arguments. However, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. Shi and Demberg (2019) used a seq2seq model that learns better argument representations due to being trained to explicitate the implicit connective. In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during"
D19-1586,P14-2047,0,0.0652614,"Missing"
D19-1586,D09-1036,0,0.222761,"Missing"
D19-1586,W12-1614,0,0.0411763,"Missing"
D19-1586,D14-1162,0,0.090746,"Missing"
D19-1586,N18-1202,0,0.154369,"Missing"
D19-1586,C08-2022,0,0.0929451,"Missing"
D19-1586,prasad-etal-2008-penn,0,0.167044,"of BERT. Numbers in bold signal significant improvements over the previous state of the art (p&lt;0.01). Numbers with ∗ denote significant improvements over BERT + WSJ w/o NSP with p&lt;0.01. ment 2 are separated with token “[SEP]”; “[CLS]” is the special classification embedding while “C” is the same as “[CLS]” in pre-training but the ground-truth label in the fine-tuning. In the experiments, we used the uncased base model1 provided by Devlin et al. (2019), which is trained on BooksCorpus and English Wikipedia with 3300M tokens in total. 3.1 Evaluation on PDTB We used the Penn Discourse Tree Bank (Prasad et al., 2008), the largest available manually annotated discourse corpus. It provides a three level hierarchy of relation tags. Following the experimental settings and evaluation metrics in Bai and Zhao (2018), we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009), which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015), which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score. In addition, we also performed 10-fold cross validation among sections 022, as promoted in Sh"
D19-1586,C16-1180,0,0.0269506,"Missing"
D19-1586,P17-1093,0,0.475253,"Missing"
D19-1586,E17-1027,1,0.904235,"Missing"
D19-1586,E14-1068,0,0.0365218,"Missing"
D19-1586,K16-2001,0,0.123099,"Missing"
D19-1586,D14-1196,0,0.0318801,"Missing"
D19-1586,D15-1266,0,0.0363304,"Missing"
D19-1586,E17-2024,1,0.862576,"8), the largest available manually annotated discourse corpus. It provides a three level hierarchy of relation tags. Following the experimental settings and evaluation metrics in Bai and Zhao (2018), we use two most-used splitting methods of PDTB data, denoted as PDTB-Lin (Lin et al., 2009), which uses sections 2-21, 22, 23 as training, validation and test sets, and PDTB-Ji (Ji and Eisenstein, 2015), which uses 2-20, 0-1, 21-22 as training, validation and test sets and report the overall accuracy score. In addition, we also performed 10-fold cross validation among sections 022, as promoted in Shi and Demberg (2017). We also follow the standard in the literature to formulate the task as an 11-way classification task. Results are presented in Table 1. We evaluated three versions of the BERT-based model. All of our BERT models use the pre-trained representations and are fine-tuned on the PDTB training data. The version marked as “BERT” does not do any additional pre-training. BERT+WSJ in addition performs further pre-training on the 1 https://github.com/google-research/ bert#pre-trained-models parts of the Wall Street Journal corpus that do not have discourse relation annotation. The model version “BERT+WJ"
D19-1586,W19-0416,1,0.823104,"ating discourse expectations by using attention over an encoding of the first argument, to generate the representation of the second argument, and then learning a classifier based on the concatenation of the encodings of the two discourse relation arguments. Kishimoto et al. (2018) fed external world knowledge (ConceptNet relations and coreferences) explicitly into MAGE-GRU (Dhingra et al., 2017) and achieved improvements compared to only using the relational arguments. However, we here show that it works even better when we learn this knowledge implicit through next sentence prediction task. Shi and Demberg (2019) used a seq2seq model that learns better argument representations due to being trained to explicitate the implicit connective. In addition, their classifier also uses a memory network that is intended to help remember similar argument pairs encountered during training. The current best performance was achieved by Bai and Zhao (2018), who combined representations from different grained em5792 beddings including contextualized word vectors from ELMo (Peters et al., 2018), which has been proved very helpful. In addition, we compared our results with a simple bidirectional LSTM network and pre-tra"
D19-1586,W19-2703,1,0.776982,"Missing"
D19-1586,I17-1049,1,0.888247,"Missing"
D19-1586,K15-2001,0,0.126287,"Missing"
D19-6310,W18-3606,0,0.270408,"tion Shared Task 2019 (MSR’19) is to generate fluent text from Universal Dependencies (UD) structures. The task makes available UD-annotated resources in 11 languages for the shallow task, and three languages (English, Spanish, French) for the deep track. Developing surface generation systems that are largely language-independent is a central objective of the shared task (Mille et al., 2018). To generate sentences based on the UD structure and morphological features, recent neural approaches mainly adopt neural sequenceto-sequence architectures (Cabezudo and Pardo, 2018; Madsack et al., 2018; Elder and Hokamp, 2018). While representing the feature-rich data in a linearized manner proved to be a viable option, we argue that these linear sequences do not optimally exploit the input information. We therefore propose to encode the dependency trees using a graph convolutional network (GCN) and find that this GCN encoder leads to a substantial boost in performance, compared to a sequential encoder. The datasets in the deep track consist of semantic representations induced from syntactic dependency parses, see Figure 1 for an example. This 1. We show that a GCN encoder for UD input structures outperforms sequen"
D19-6310,P16-1154,0,0.0270964,"of graphical representation. graph-structure input explicitly. Given a directed graph G, we represent each node with an embedding vector xv ∈ Rd . Then the l-th R-GCN layer compute the hidden representation for node v in (l + 1)-th layer as follows: X l hl+1 = f (Wh + We hlu ) (1) v v generates the representations of output token at each time step. We use global attention (Luong et al., 2015) to re-weight the hidden representations from the first layer and merge them into a global hidden vector hG . In order to generate the placeholder directly from the input, we apply the copying mechanism (Gu et al., 2016), which is effective when using lexcalization. The probability of token yt conditioned on input G and previous token y1:t−1 is obtained by applying a softmax layer on the decoder output as P (yt |y1:t−1 , G) = sof tmax(g(hG , hC )), where g is a perceptron. The model is Q trained to maximize the likelihood function L = t=1 |Y |P (yt |y1:t−1 , G). u∈N (v) where W, We ∈ Rd×h and e ∈ E. f is the linear rectifier (ReLU), a non-linear activation function. N (v) is the set of all neighbours of node v. This design is over-parameterized and there is no parameter sharing between similar edge labels. Th"
D19-6310,D18-1112,0,0.27246,"hological features, our system achieves the second rank in the deep track without using data augmentation techniques or additional components (such as a re-ranker). 1 Figure 1: An example of a UD structure with concatenated feature embeddings from the MSR’19 deep task. task is reflects the information that’s realistically available in real-world natural language generation task. Our method works as follows: We first apply delexicalization to the datasets, replacing rare tokens with placeholders. Next, encode the dependency trees using graph representation learning techniques (Li et al., 2015; Xu et al., 2018a), in order to improve the encoding of structured data within the encoder-decoder architecture. Our model hence learns a mapping between graph inputs and sequence outputs. Our ablation study in the evaluation demonstrates that encoding UD structure in this manner does embed additional semantic information and subsequently improves the performance across the three languages available for the deep track (i.e. English, French, and Spanish). Finally, we use an LSTM decoder with copy mechanism and attention to generate surface text. Our contributions are as follows: Introduction The goal in the Mu"
D19-6310,P17-4012,0,0.0178456,"EWT (enewtud-dev) corpus. We compare different encoders while keeping decoder constant, i.e., LSTM decoder with copy mechanism and coverage attention. For beam search we maintain a constant use of blocking 3-gram. 3.3 LSTM Decoder We apply stacked LSTM layers (Hochreiter and Schmidhuber, 1997) as the decoder on top of the GCN. The first layer is an input-feed LSTM (Luong et al., 2015) that aggregates the hidden representations of nodes into one hidden vector hC for the whole graph. The second LSTM layer decodes the hidden vector and Experiments We built our system on a variant of OpenNMT-py (Klein et al., 2017) from Marcheggiani and PerezBeltrachini (2018) with customized encoders. We construct the training and validation datasets by concatenating corresponding splits of all available corpora for each language. We stack 4 R-GCN 77 Encoder BiLSTM GCN RGCN Gold Output President Bush threw two members to replace manufacturers in the Washington area to replace manufacturers in federal nations. In Tuesday, President Bush commissioned two connections to replace the federal individual of federal statements in the Washington area. In Tuesday, President Bush nominated two individuals to replace jurist trials"
D19-6310,C18-1089,0,0.0265239,"Missing"
D19-6310,D15-1166,0,0.38885,"M ... Add Dense Connection (B) (A) Figure 2: (A): depicts the conceptual relationship between the GCN encoder and LSTM decoder. (B): Addition of a dense layer is analogous to adding extra connections between multiple layers of graphical representation. graph-structure input explicitly. Given a directed graph G, we represent each node with an embedding vector xv ∈ Rd . Then the l-th R-GCN layer compute the hidden representation for node v in (l + 1)-th layer as follows: X l hl+1 = f (Wh + We hlu ) (1) v v generates the representations of output token at each time step. We use global attention (Luong et al., 2015) to re-weight the hidden representations from the first layer and merge them into a global hidden vector hG . In order to generate the placeholder directly from the input, we apply the copying mechanism (Gu et al., 2016), which is effective when using lexcalization. The probability of token yt conditioned on input G and previous token y1:t−1 is obtained by applying a softmax layer on the decoder output as P (yt |y1:t−1 , G) = sof tmax(g(hG , hC )), where g is a perceptron. The model is Q trained to maximize the likelihood function L = t=1 |Y |P (yt |y1:t−1 , G). u∈N (v) where W, We ∈ Rd×h and"
D19-6310,W18-3607,0,0.0403046,"Missing"
D19-6310,W18-6501,0,0.0607604,"Missing"
D19-6310,W18-3601,0,0.0366371,"rack (i.e. English, French, and Spanish). Finally, we use an LSTM decoder with copy mechanism and attention to generate surface text. Our contributions are as follows: Introduction The goal in the Multilingual Surface Realization Shared Task 2019 (MSR’19) is to generate fluent text from Universal Dependencies (UD) structures. The task makes available UD-annotated resources in 11 languages for the shallow task, and three languages (English, Spanish, French) for the deep track. Developing surface generation systems that are largely language-independent is a central objective of the shared task (Mille et al., 2018). To generate sentences based on the UD structure and morphological features, recent neural approaches mainly adopt neural sequenceto-sequence architectures (Cabezudo and Pardo, 2018; Madsack et al., 2018; Elder and Hokamp, 2018). While representing the feature-rich data in a linearized manner proved to be a viable option, we argue that these linear sequences do not optimally exploit the input information. We therefore propose to encode the dependency trees using a graph convolutional network (GCN) and find that this GCN encoder leads to a substantial boost in performance, compared to a sequen"
D19-6310,Q19-1002,0,0.0277186,"er the dependency tree is informative enough for surface realization. However, we performed additional experiments to show the effectiveness of GCN encoder with selected concatenated features, see Table 1. Graph-to-text Generation Considering the fact that a dependency tree is a special case of a directed acyclic graph, surface realization is a graph-to-text generation tasks. Graph neural networks have been successfully applied to different graph to text generation task like SQL to text generation (Xu et al., 2018b), AMR-to-text generation (Beck et al., 2018) and semantic machine translation (Song et al., 2019). LSTM can be modified to model graph-level information (Song et al., 2018). Graph Convolutional Networks (GCN), originally designed for semi-supervised learning of node representations in graphs (Kipf and Welling, 2017), explicitly exploit tree structure data and outperform LSTM and TreeLSTM on AMR-to-text generation (Damonte and Cohen, 2019). To also model different types of edges in graphs, Relational Graph Convolutional Networks (R-GCN) represent each type of edge with a corresponding parameter matrix (Schlichtkrull et al., 2018). We leverage the R-GCN by grouping inedge and out-edge toget"
D19-6310,P18-1150,0,0.0324178,"er, we performed additional experiments to show the effectiveness of GCN encoder with selected concatenated features, see Table 1. Graph-to-text Generation Considering the fact that a dependency tree is a special case of a directed acyclic graph, surface realization is a graph-to-text generation tasks. Graph neural networks have been successfully applied to different graph to text generation task like SQL to text generation (Xu et al., 2018b), AMR-to-text generation (Beck et al., 2018) and semantic machine translation (Song et al., 2019). LSTM can be modified to model graph-level information (Song et al., 2018). Graph Convolutional Networks (GCN), originally designed for semi-supervised learning of node representations in graphs (Kipf and Welling, 2017), explicitly exploit tree structure data and outperform LSTM and TreeLSTM on AMR-to-text generation (Damonte and Cohen, 2019). To also model different types of edges in graphs, Relational Graph Convolutional Networks (R-GCN) represent each type of edge with a corresponding parameter matrix (Schlichtkrull et al., 2018). We leverage the R-GCN by grouping inedge and out-edge together and apply to a graphto-text generation task. 3.2 Model The graph-to-tex"
E06-1009,P04-1009,0,0.4848,"learly, alternative strategies to sequential presentation of information in SDS are needed. Recently, two approaches have been proposed. In the user-model (UM) based approach, the system identifies a small number of options that best match the user’s preferences (Moore et al., 2004; Walker et al., 2004). In the summarize and refine (SR) approach, the system structures the large number of options into a small number of clusters that share attributes. The system summarizes the clusters based on their attributes and then prompts the user to provide additional constraints (Polifroni et al., 2003; Chung, 2004). In this paper, we present an algorithm that combines the benefits of these two approaches in an approach to information presentation that integrates user modelling with automated clustering. 65 Thus, the system provides detail only about those options that are of some relevance to the user, where relevance is determined by the user model. If there are multiple relevant options, a clusterbased tree structure orders these options to allow for stepwise refinement. The effectiveness of the tree structure, which directs the dialogue flow, is optimized by taking the user’s preferences into account"
E06-1009,P01-1066,0,0.276542,"ably, increases overall user satisfaction, and significantly improves the user’s overview of the available options. Moreover, our results suggest that presenting users with a brief summary of the irrelevant options increases users’ confidence in having heard about all relevant options. 1 Introduction The goal of spoken dialogue systems (SDS) is to offer efficient and natural access to applications and services, such as email and calendars, travel and entertainment booking, and product recommendation. In evaluating nine SDS in the DARPA Communicator domain (flight, hotel, and rental car hire), Walker et al. (2001) found that (1) shorter task duration correlates with higher user satisfaction, and (2) the information presentation phase of dialogues is the primary contributor to dialogue duration. During this phase, the typical system enumerates the set of options that match the user’s constraints, as shown in Figure 1. The user can then refine these options by offering new constraints. When the number of options to be presented is large, this process can be painstaking, leading to reduced user satisfaction. Moreover, as Johanna D. Moore School of Informatics University of Edinburgh Edinburgh, EH8 9LW, GB"
E06-1009,W03-2123,0,\N,Missing
E06-1009,W04-0601,0,\N,Missing
E06-1009,P96-1039,0,\N,Missing
E06-1009,W02-2110,0,\N,Missing
E17-1027,P14-1129,0,0.0999089,"Missing"
E17-1027,D14-1080,0,0.0812409,"Missing"
E17-1027,Q15-1024,0,0.126333,"rs: The principle of compositionality leads us to believe that the semantics of the argument vector should be determined by the syntactic structures and the meanings of the constituents. For a fair comparison with the sequential model, we apply the same formulation of LSTM on the binarized constituent parse tree. The hidden state vector now corresponds to a constituent in the tree. These hidden state vectors are then used in the same fashion as the sequential LSTM. The mathematical formulation is the same as Tai et al. (2015). This model is similar to the recursive neural networks proposed by Ji and Eisenstein (2015). Our model differs from their model in several ways. We use the LSTM networks instead of the “vanilla” RNN formula and expect better results due to less complication with vanishing and exploding gradients during training. Furthermore, our purpose is to compare the influence of the model structures. Therefore, we must use LSTM cells in both sequential and tree LSTM models for a fair and meaningful comparison. The more indepth comparison of our work and recursive neural network model by Ji and Eisenstein (2015) is provided in the discussion section. h1 = tanh(W1 · a1 + W2 · a2 + bh1 ) where W1"
E17-1027,N16-1037,0,0.169768,"they have not been much explored in the realm of discourse parsing. LSTM models have been notably used to encode the meaning of source language sentence in neural machine translation (Cho et al., 2014; Devlin et al., 2014) and recently used to encode the meaning of an entire sentence to be used as features (Kiros et al., 2015). Many neural architectures have been explored and evaluated, but there is no single technique that is decidedly better across all tasks. The LSTM-based models such as Kiros et al. (2015) perform well across tasks but do not outperform some other strong neural baselines. Ji et al. (2016) uses a joint discourse language model to improve the performance on the coarse-grained label in the PDTB, but in our case, we would like to deduce how well LSTM fares in fine-grained implicit discourse relation classification, which is more practical for application. sets, and our findings are valid across languages. Our Chinese model outperforms all of the feature sets known to work well in English despite using only word vectors. The choice of neural architecture used for inducing Chinese word vectors turns out to be crucial. Chinese word vectors from Skipgram model perform consistently bet"
E17-1027,D12-1050,0,0.0282312,"v, Model Architectures Following previous work, we assume that the two arguments of an implicit discourse relation are given so that we can focus on predicting the senses of the implicit discourse relations. The input to our model is a pair of text segments called Arg1 and Arg2, and the label is one of the senses defined in the Penn 282 at word position t, we compute the corresponding hidden state vector st and the memory cell vector from the previous step, using standard formula for LSTM. The argument vectors are the results of applying a pooling function over the hidden state vectors. 2014; Blacoe and Lapata, 2012; Mikolov et al., 2013b; Braud and Denis, 2015). The Arg1 vector a1 and Arg2 vector a2 are computed by applying element-wise pooling function f on all of the N1 1 word vectors in Arg1 w1:N and all of the N2 word 1 2 vectors in Arg2 w1:N2 respectively: a1i = f (s11:N1 ,i ) 1 a1i = f (w1:N ) 1 ,i a2i = f (s21:N2 ,i ) 2 a2i = f (w1:N ) 2 ,i In addition to the three pooling functions that we describe in the previous subsection, we also consider using only the last hidden state vector, which should theoretically be able to encode the semantics of the entire word sequence. We consider three differen"
E17-1027,D15-1262,0,0.229217,"we assume that the two arguments of an implicit discourse relation are given so that we can focus on predicting the senses of the implicit discourse relations. The input to our model is a pair of text segments called Arg1 and Arg2, and the label is one of the senses defined in the Penn 282 at word position t, we compute the corresponding hidden state vector st and the memory cell vector from the previous step, using standard formula for LSTM. The argument vectors are the results of applying a pooling function over the hidden state vectors. 2014; Blacoe and Lapata, 2012; Mikolov et al., 2013b; Braud and Denis, 2015). The Arg1 vector a1 and Arg2 vector a2 are computed by applying element-wise pooling function f on all of the N1 1 word vectors in Arg1 w1:N and all of the N2 word 1 2 vectors in Arg2 w1:N2 respectively: a1i = f (s11:N1 ,i ) 1 a1i = f (w1:N ) 1 ,i a2i = f (s21:N2 ,i ) 2 a2i = f (w1:N ) 2 ,i In addition to the three pooling functions that we describe in the previous subsection, we also consider using only the last hidden state vector, which should theoretically be able to encode the semantics of the entire word sequence. We consider three different pooling functions namely max, summation, and"
E17-1027,D15-1278,0,0.0227129,"not be complete without explaining our results in relation to the recursive neural network model proposed by Ji and Eisenstein (2015). Why do sequential LSTM models outperform recursive neural networks or tree LSTM models? Although this first comes as a surprise to us, the results are consistent with recent works that use sequential LSTM to encode syntactic information. For example, Vinyals et al. (2015) use sequential LSTM to encode the features for syntactic parse output. Tree LSTM seems to show improvement when there is a need to model longdistance dependency in the data (Tai et al., 2015; Li et al., 2015). Furthermore, the benefits of tree LSTM are not readily apparent for a model that discards the syntactic categories in the intermediate nodes and makes no distinction between heads and their dependents, which are at the core of syntactic representations. Another point of contrast between our work and Ji and Eisenstein’s (2015) is the modeling choice for inter-argument interaction. Our experimental results show that the hidden layers are an important contributor to the performance for all of our models. We choose linear inter-argument interaction instead of bilinear interaction, and this decis"
E17-1027,D09-1036,0,0.676043,"Missing"
E17-1027,P09-1077,0,0.759739,"discourse relations has proved to be notoriously hard and remained one of the last missing pieces in an end-to-end discourse parser (Xue et al., 2015). In the absence of explicit discourse connectives, implicit discourse relations have to be inferred from their two arguments. Previous approaches on inferring implicit discourse relations have typically relied on features extracted from their two arguments. These features include the Cartesian products of the word tokens in the two arguments as well as features manually crafted from various lexicons such as verb classes and sentiment lexicons (Pitler et al., 2009; Rutherford and Xue, 2014). These lexicons are used mainly to offset the data sparsity problem created by pairs of word tokens used directly as features. Neural network models are an attractive alternative for this task, but it is not clear how well they will fare with a small dataset, typically found in discourse annotation projects. Many neural approaches have been proposed. However, we lack a unified standard comparison to really learn whether we make any progress at all because not all past studies agree on the same experimental settings such as label sets to use. Previous work used four"
E17-1027,prasad-etal-2008-penn,0,0.640866,"igured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Further, we compare our best feedforward system with competitive convolutional and recurrent networks and find that feedforward can actually be more effective. For the first time for this task, we compile and publish outputs from previous neural and nonneural systems to establish the standard for further comparison. 1 Introduction The discourse structure of a natural language text has been analyzed and conceptualized under various frameworks (Mann and Thompson, 1988; Lascarides and Asher, 2007; Prasad et al., 2008). The Penn Discourse TreeBank (PDTB) and the Chinese Discourse Treebank (CDTB), currently the largest corpora annotated with discourse structures in English and Chinese respectively, view the discourse structure of a text as a set of discourse relations (Prasad et al., 2008; Zhou and Xue, 2012). Each discourse relation (e.g. causal or temporal) is grounded by a discourse connective (e.g. because or meanwhile) taking two text segments as argu281 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 281–291, c Va"
E17-1027,P02-1047,0,0.215223,"ME + Word pairs ME + All feature sets Our best feedforward variant 77.14 80.81 82.34 82.36 82.48 82.98 83.13 84.16 85.45 Table 5: Our best feedforward variant significantly outperforms the systems with surface features (p < 0.05). ME=Maximum Entropy model Number of Hidden Layers CBOW 0 1 2 Skipgram Accuracy 0.84 0.82 0.80 0.78 100 200 300 100 200 300 Dimensionality of word vectors Figure 5: Comparing the accuracies across Chinese word vectors for feedforward model. dependency rule pairs, production rule pairs (Lin et al., 2009), Brown cluster pairs (Rutherford and Xue, 2014), and word pairs (Marcu and Echihabi, 2002). We use information gain criteria to select the best subset of each feature set, which is crucial in feature-based discourse parsing. Chinese word vectors are induced through CBOW and Skipgram architecture in word2vec (Mikolov et al., 2013a) on Chinese Gigaword corpus (Graff and Chen, 2005) using default settings. The number of dimensions that we try are 50, 100, 150, 200, 250, and 300. We induce 1,000 and 3,000 Brown clusters on the Gigaword corpus. Table 5 shows the results for the models which are best tuned on the number of hidden units, hidden layers, and the types of word vectors. The f"
E17-1027,K16-2010,0,0.0283883,"tting used in CoNLL 2015-2016 Shared Task. To compare our results against previous systems, we compile all of the official system outputs, and make them publicly available. The label set is modified by the shared task organizers into 15 different senses including EntRel as another sense (Xue et al., 2015; Xue et al., 2016). We use the 300-dimensional word vector used in the previous experiment and tune the number of hidden layers and hidden units on the development set. We consider the following models: Bidirectional-LSTM (Akanksha and Eisenstein, 2016), two flavors of convolutional networks (Qin et al., 2016; Wang and Lan, 2016), two variations of simple argument pooling (Mihaylov and Frank, 2016; Schenk et al., 2016), and the best system using surface features alone (Wang and Lan, 2015). The comparison results and brief system descriptions are shown in Table 4. Our model presents the state-of-the-art system on the blind test set in English. We once again confirm that manual features are not necessary for this task and that our feedforward network outperforms the best available LSTM and convolutional networks in many settings despite its simplicity. While performing well in-domain, convolutional"
E17-1027,J93-2004,0,0.0601867,"Missing"
E17-1027,E14-1068,1,0.860517,"has proved to be notoriously hard and remained one of the last missing pieces in an end-to-end discourse parser (Xue et al., 2015). In the absence of explicit discourse connectives, implicit discourse relations have to be inferred from their two arguments. Previous approaches on inferring implicit discourse relations have typically relied on features extracted from their two arguments. These features include the Cartesian products of the word tokens in the two arguments as well as features manually crafted from various lexicons such as verb classes and sentiment lexicons (Pitler et al., 2009; Rutherford and Xue, 2014). These lexicons are used mainly to offset the data sparsity problem created by pairs of word tokens used directly as features. Neural network models are an attractive alternative for this task, but it is not clear how well they will fare with a small dataset, typically found in discourse annotation projects. Many neural approaches have been proposed. However, we lack a unified standard comparison to really learn whether we make any progress at all because not all past studies agree on the same experimental settings such as label sets to use. Previous work used four binary classification (Pitl"
E17-1027,K16-2014,0,0.0388664,"systems, we compile all of the official system outputs, and make them publicly available. The label set is modified by the shared task organizers into 15 different senses including EntRel as another sense (Xue et al., 2015; Xue et al., 2016). We use the 300-dimensional word vector used in the previous experiment and tune the number of hidden layers and hidden units on the development set. We consider the following models: Bidirectional-LSTM (Akanksha and Eisenstein, 2016), two flavors of convolutional networks (Qin et al., 2016; Wang and Lan, 2016), two variations of simple argument pooling (Mihaylov and Frank, 2016; Schenk et al., 2016), and the best system using surface features alone (Wang and Lan, 2015). The comparison results and brief system descriptions are shown in Table 4. Our model presents the state-of-the-art system on the blind test set in English. We once again confirm that manual features are not necessary for this task and that our feedforward network outperforms the best available LSTM and convolutional networks in many settings despite its simplicity. While performing well in-domain, convolutional networks degrade sharply when tested on the blind slightly out-of-domain dataset. 5.2 Acc."
E17-1027,N15-1081,1,0.826695,"pairs of word tokens used directly as features. Neural network models are an attractive alternative for this task, but it is not clear how well they will fare with a small dataset, typically found in discourse annotation projects. Many neural approaches have been proposed. However, we lack a unified standard comparison to really learn whether we make any progress at all because not all past studies agree on the same experimental settings such as label sets to use. Previous work used four binary classification (Pitler et al., 2008; Rutherford and Xue, 2014) , 4-way coarse sense classification (Rutherford and Xue, 2015) , and intermediate sense classification (Lin et al., 2009). CoNLL Shared Task introduces a unified scheme for evaluation along with a new unseen test set in English in 2015 (Xue et al., 2015) and in Chinese in 2016 (Xue et al., 2016). We want to corroboInferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this task is not unified, so we could hardly draw clear conclusions about the effectiveness of various architectures. Here, we propo"
E17-1027,K16-2005,0,0.249378,"Missing"
E17-1027,W12-1614,0,0.617306,"roach for this task is to use surface features derived from various semantic lexicons (Pitler et al., 2009), reducing the number of parameters by mapping raw word tokens in the arguments of discourse relations to a limited number of entries in a semantic lexicon such as polarity and verb classes. Along the same vein, Brown cluster assignments have also been used as a general purpose lexicon that requires no human manual annotation (Rutherford and Xue, 2014). However, these solutions still suffer from the data sparsity problem and almost always require extensive feature selection to work well (Park and Cardie, 2012; Lin et al., 2009; Ji and Eisenstein, 2015). The work we report here explores the use of the expressive power of distributed representations to overcome the data sparsity problem found in the traditional feature engineering paradigm. Neural network modeling has been explored to some extent in the context of this task. Recently, Braud and Denis (2015) tested various word vectors as features for implicit discourse relation classification and show that distributed features achieve the same level of accuracy as onehot representations in some experimental settings. Ji et al. (2015; 2016) advance t"
E17-1027,D14-1162,0,0.0816999,"Missing"
E17-1027,P15-1150,0,0.149259,"Missing"
E17-1027,P06-1055,0,0.0605873,"Missing"
E17-1027,K15-2002,0,0.0411252,"Missing"
E17-1027,K16-2004,0,0.0666199,"L 2015-2016 Shared Task. To compare our results against previous systems, we compile all of the official system outputs, and make them publicly available. The label set is modified by the shared task organizers into 15 different senses including EntRel as another sense (Xue et al., 2015; Xue et al., 2016). We use the 300-dimensional word vector used in the previous experiment and tune the number of hidden layers and hidden units on the development set. We consider the following models: Bidirectional-LSTM (Akanksha and Eisenstein, 2016), two flavors of convolutional networks (Qin et al., 2016; Wang and Lan, 2016), two variations of simple argument pooling (Mihaylov and Frank, 2016; Schenk et al., 2016), and the best system using surface features alone (Wang and Lan, 2015). The comparison results and brief system descriptions are shown in Table 4. Our model presents the state-of-the-art system on the blind test set in English. We once again confirm that manual features are not necessary for this task and that our feedforward network outperforms the best available LSTM and convolutional networks in many settings despite its simplicity. While performing well in-domain, convolutional networks degrade shar"
E17-1027,K15-2001,1,0.909064,"niversity Waltham, MA, USA teruth@yelp.com vera@coli.uni-saarland.de xuen@brandeis.edu Abstract ments (Prasad et al., 2008). Implicit discourse relations are those where discourse connectives are omitted from the text and yet the discourse relations still hold. While classifying explicit discourse relations is relatively easy, as the discourse connective itself provides a strong cue for the discourse relation (Pitler et al., 2008), the classification of implicit discourse relations has proved to be notoriously hard and remained one of the last missing pieces in an end-to-end discourse parser (Xue et al., 2015). In the absence of explicit discourse connectives, implicit discourse relations have to be inferred from their two arguments. Previous approaches on inferring implicit discourse relations have typically relied on features extracted from their two arguments. These features include the Cartesian products of the word tokens in the two arguments as well as features manually crafted from various lexicons such as verb classes and sentiment lexicons (Pitler et al., 2009; Rutherford and Xue, 2014). These lexicons are used mainly to offset the data sparsity problem created by pairs of word tokens used"
E17-1027,K16-2001,1,0.924705,"approaches have been proposed. However, we lack a unified standard comparison to really learn whether we make any progress at all because not all past studies agree on the same experimental settings such as label sets to use. Previous work used four binary classification (Pitler et al., 2008; Rutherford and Xue, 2014) , 4-way coarse sense classification (Rutherford and Xue, 2015) , and intermediate sense classification (Lin et al., 2009). CoNLL Shared Task introduces a unified scheme for evaluation along with a new unseen test set in English in 2015 (Xue et al., 2015) and in Chinese in 2016 (Xue et al., 2016). We want to corroboInferring implicit discourse relations in natural language text is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this task is not unified, so we could hardly draw clear conclusions about the effectiveness of various architectures. Here, we propose neural network models that are based on feedforward and long-short term memory architecture and systematically study the effects of varying structures. To our surprise, the best-configured feedforward architecture outperforms LSTM-"
E17-1027,P12-1008,1,0.861782,"compile and publish outputs from previous neural and nonneural systems to establish the standard for further comparison. 1 Introduction The discourse structure of a natural language text has been analyzed and conceptualized under various frameworks (Mann and Thompson, 1988; Lascarides and Asher, 2007; Prasad et al., 2008). The Penn Discourse TreeBank (PDTB) and the Chinese Discourse Treebank (CDTB), currently the largest corpora annotated with discourse structures in English and Chinese respectively, view the discourse structure of a text as a set of discourse relations (Prasad et al., 2008; Zhou and Xue, 2012). Each discourse relation (e.g. causal or temporal) is grounded by a discourse connective (e.g. because or meanwhile) taking two text segments as argu281 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 281–291, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Output layer rate this new evaluation scheme by running more benchmark results and providing the output under this evaluation scheme. We systematically compare the relative advantages of different neural architecture"
E17-1090,N16-1120,0,0.243326,"Missing"
E17-1090,N15-1022,0,0.124046,"imes a reader needs to push or pop a connected component to or from their memory store. These features comprise the E MBEDDING model. To extract the remaining features, we first ran the Stanford dependency parser on both corpora. The program icy-parses uses part-of-speech tags and head-dependent relations to determine the total, average, and maximum integration cost across a sentence. Here average integration cost functions as another kind of memory load estimate while the maximum value models the mostCorpora We used two corpora in this work. The English and Simple English Wikipedia corpus of Hwang et al. (2015, ESEW) is a new corpus of more than 150k sentence pairs designed to address the flaws of the Parallel Wikipedia Corpus of Zhu et al. (2010, PWKP), which was previously dominant in work on text simplification, by using a more sophisticated method of aligning pairs of English and Simple English sentences. We used the section labeled as having ‘good’ alignments for our work and assumed that, in every sentence pair, the Simple English sentence should be ranked as easier than the English sentence (rank=1 &lt; rank=2 in Table 1). This provides a large corpus with noisy labels, as there are likely to b"
E17-1090,E09-1027,0,0.0371773,"st work in this direction was by L. A. Sherman, who proposed a quantitative analysis of text difficulty based on the number of clauses per sentence, among other features (Sherman, 1893). Where Sherman’s pedagogical focus was on literature, Lively & Pressey (1923) focused on vocabulary as a bottleneck in science education. Work in this vein led to the development of a number of readability formulae in the mid-20th century1 , including the familiar FleschKincaid Grade-Level score (Kincaid et al., 1975). Recent work has also looked at features related to discourse and working memory constraints. Feng et al. (2009) worked on a model of readability for adults with intellectual disabilities. Considering working memory constraints, they extracted features related to the number of entities mentioned in a document and the ‘lexical chains’ (Galley and McKeown, 2003) that connected them. They found that their features resulted in a better correlation (Pearson’s r = −0.352) compared to both Flesch-Kincaid score (r = −0.270) and a number of ‘basic’ linguistic features based on those used by Petersen & Ostendorf (2009) (r = −0.283).3 Coh-Metrix (Graesser et al., 2004) also includes a number of measures related to"
E17-1090,W12-2019,0,0.114102,"Missing"
E17-1090,E14-1031,0,0.580434,"al sentences can cause difficulties for readers. Translation tools can aim to preserve not just meaning but also the approximate difficulty of the sentences they are translating or use a sentence-level difficulty metric to target output that is easier to understand. Furthermore, information retrieval systems also benefit when they can return not merely relevant texts, but also texts appropriate to the reading level of the user. Recently there has been an increased interest in sentential models of text difficulty in the automatic text simplification and summarization communities in particular (Vajjala and Meurers, 2014; Macdonald and Siddharthan, 2016). One area that has produced a lot of research on sentence level processing difficulty is psycholinguistics. Over the past three decades, a number of theories of human sentence processing (i.e. reading) have been proposed and validated in a large variety of experimental studies. The most important sentence processing theories have furthermore been implemented based on broad-coverage tools, so that estimates for arbitrary sentences can be generated automatically. For example, eyetracking studies of reading times on a large corpus of newspaper text have found th"
E17-1090,W10-0406,0,0.28795,"Missing"
E17-1090,P10-1121,0,0.0222515,"risal significantly correlated with reading times. Although they found that integration cost did not significantly contribute to predicting eye-tracking reading times in general, its contribution was significant when restricted to nouns and verbs. They also found that surprisal and integration cost were uncorrelated, suggesting that they should be considered complementary factors in a model of reading times. Another eyetracking study divided surprisal into lexical and synactic components, finding that lexical surprisal was a significant factor but not syntactic surprisal (Roark et al., 2009). Wu et al. (2010) examined surprisal, entropy reduction, and embedding depth in a study of psycholinguistic complexity metrics. Their study of the reading times of 23 native English speakers reading four narratives indicated that embedding difference was a significant predictor of reading times for closed class words. Moreover, this contribution was independent of the contribution of surprisal, indicating that the two measures are 5 Gildea & Temperley (2010) measure dependencies in terms of word span, such that adjacent words have a dependency length of one. This approach produces similar difficulty estimates"
E17-1090,C10-1152,0,0.0491838,"act the remaining features, we first ran the Stanford dependency parser on both corpora. The program icy-parses uses part-of-speech tags and head-dependent relations to determine the total, average, and maximum integration cost across a sentence. Here average integration cost functions as another kind of memory load estimate while the maximum value models the mostCorpora We used two corpora in this work. The English and Simple English Wikipedia corpus of Hwang et al. (2015, ESEW) is a new corpus of more than 150k sentence pairs designed to address the flaws of the Parallel Wikipedia Corpus of Zhu et al. (2010, PWKP), which was previously dominant in work on text simplification, by using a more sophisticated method of aligning pairs of English and Simple English sentences. We used the section labeled as having ‘good’ alignments for our work and assumed that, in every sentence pair, the Simple English sentence should be ranked as easier than the English sentence (rank=1 &lt; rank=2 in Table 1). This provides a large corpus with noisy labels, as there are likely to be instances where the English and Simple English sentences are not substantially different or the English sentence is the easier one.8 For"
E17-1090,D09-1034,0,0.0660912,"d that increased surprisal significantly correlated with reading times. Although they found that integration cost did not significantly contribute to predicting eye-tracking reading times in general, its contribution was significant when restricted to nouns and verbs. They also found that surprisal and integration cost were uncorrelated, suggesting that they should be considered complementary factors in a model of reading times. Another eyetracking study divided surprisal into lexical and synactic components, finding that lexical surprisal was a significant factor but not syntactic surprisal (Roark et al., 2009). Wu et al. (2010) examined surprisal, entropy reduction, and embedding depth in a study of psycholinguistic complexity metrics. Their study of the reading times of 23 native English speakers reading four narratives indicated that embedding difference was a significant predictor of reading times for closed class words. Moreover, this contribution was independent of the contribution of surprisal, indicating that the two measures are 5 Gildea & Temperley (2010) measure dependencies in terms of word span, such that adjacent words have a dependency length of one. This approach produces similar dif"
E17-2024,Q15-1024,0,0.653847,"ing some improvement on the spe150 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 150–156, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics available annotated corpora of discourse relations, opened the door to machine learning based discourse relation classification. stances, so that it’s important to make maximal use of both the data set for training and testing. The test set that is currently most often used for 11 way classification is section 23 (Lin et al., 2009; Ji and Eisenstein, 2015; Rutherford et al., 2017), which contains only about 761 implicit relations. This small size implies that a gain of 1 percentage point in accuracy corresponds to just classifying an additional 7-8 instances correctly. This paper therefore aims to demonstrate the degree to which conclusions about the effectiveness of including certain features would depend on whether one evaluates on the standard test section only, or performs cross validation on the whole dataset for second-level discourse relation classification. The model that we use is a neural network that takes the words occurring in the"
E17-2024,N16-1037,0,0.22046,"ent in the most-used split. For instance, temporal relations are under-represented which may lead to a misestimation of the usefulness of features that are relevant for classifying temporal relations. For our cross validation experiments, we evenly divided all the instances in section 0-24 into 10 balanced folds1 . The proportions of each class in the training and testing set are identical. With the same distribution of each class, we here avoid having an unbalanced number of instances per class among training and testing set. tations for each discourse argument with recursive neural network. Ji et al. (2016) introduced a latent variable to recurrent neural network and outperformed in two tasks. Chen et al. (2016) adopted a gated relevance network to capture the semantic interaction between word pairs. Zhang et al. (2016) proposed a neural discourse relation recognizer with a semantic memory and attention weights for implicit discourse relation recognition. The model we use in this paper is most closely related to the neural network model proposed in Rutherford et al. (2017). The model also has access to the traditional features, which are concatenated to the neural representations of the argument"
E17-2024,D12-1091,0,0.0141804,"is illustrated in Figure 1. Regarding the initialization, regularization and learning algorithm, we follow all the settings in (Rutherford et al., 2017). We adopt cross-entropy as our cost function, adagrad as the optimization algorithm, initialized all the weights in the model with uniform random and set dropout layers after the embedding and output layer with a drop rate of 0.2 and 0.5 respectively. 5 the verb features seem to indicate a substantial improvement in relation classification accuracy on the standard test set, but there is no effect at all across the folds. Other works, such as Berg-Kirkpatrick et al. (2012) strongly recommend significance testing to validate metric gains in NLP tasks, even though the relationship between metric gain and statistical significance is complex. We observed that recent papers in discourse relation parsing do not always perform significance testing, and if they do report significance, then oftentimes they do not report the test that was used. We would here like to argue in favour of significance testing with cross validation, as opposed to boot strapping methods that only use the standard test set. Due to the larger amount of data, calculating significance based on the"
E17-2024,D09-1036,0,0.769011,"Missing"
E17-2024,P13-2013,0,0.0918261,"Missing"
E17-2024,D15-1262,0,0.0262583,"Missing"
E17-2024,P16-1163,0,0.647685,"8). In implicit relations, no such strong cue is available and the discourse relation instead needs to be inferred based on the two textual arguments. In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016). For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen inIntroduction Discourse-level relation analysis is relevant to a variety of NLP tasks such as summarization (Yoshida et al., 2014), question answering (Jansen et al., 2014) and machine translation (Meyer et al., 2015). Recent years have seen more and more works on this topic, including two CoNNL shared tasks (Xue et al., 2015; Xue et al., 2016). The community most often uses the Penn Discourse Treebank (PDTB) (Prasad et al."
E17-2024,D10-1039,0,0.244773,"Missing"
E17-2024,W12-1614,0,0.211355,"Missing"
E17-2024,C08-2022,0,0.340454,"ssification task by the community. 1 “ Typically, money-fund yields beat comparable short-term investments because portfolio managers can vary maturities and go after the highest rates” (Explicit, Contingency.Cause) “ They desperately needed somebody who showed they cared for them, who loved them. (But) The last thing they needed was another drag-down blow.” (Implicit, Comparison.Contrast) Previous studies show that the presence of connectives can greatly help with classification of the relation and can be disambiguated with 0.93 accuracy (4-ways) solely on the discourse relation connectives (Pitler et al., 2008). In implicit relations, no such strong cue is available and the discourse relation instead needs to be inferred based on the two textual arguments. In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Ch"
E17-2024,P16-1135,0,0.0747787,"Missing"
E17-2024,P09-1077,0,0.764293,"ly on the discourse relation connectives (Pitler et al., 2008). In implicit relations, no such strong cue is available and the discourse relation instead needs to be inferred based on the two textual arguments. In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016). For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen inIntroduction Discourse-level relation analysis is relevant to a variety of NLP tasks such as summarization (Yoshida et al., 2014), question answering (Jansen et al., 2014) and machine translation (Meyer et al., 2015). Recent years have seen more and more works on this topic, including two CoNNL shared tasks (Xue et al., 2015; Xue et al., 2016). The community most o"
E17-2024,prasad-etal-2008-penn,0,0.907475,"et al., 2016). For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen inIntroduction Discourse-level relation analysis is relevant to a variety of NLP tasks such as summarization (Yoshida et al., 2014), question answering (Jansen et al., 2014) and machine translation (Meyer et al., 2015). Recent years have seen more and more works on this topic, including two CoNNL shared tasks (Xue et al., 2015; Xue et al., 2016). The community most often uses the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) as a resource, and has adopted the usual split into training and test data as used for other tasks such as parsing. Because discourse relation annotation is at a higher level than syntactic annotation, this however means that the test set is rather small, and with the amount of alternative features and, more recently, neural network architectures being applied to the problem, we run a serious risk as a community of believing in features that are successful in getting some improvement on the spe150 Proceedings of the 15th Conference of the European Chapter of the Association for Computational"
E17-2024,D14-1196,0,0.186917,"ons, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016). For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen inIntroduction Discourse-level relation analysis is relevant to a variety of NLP tasks such as summarization (Yoshida et al., 2014), question answering (Jansen et al., 2014) and machine translation (Meyer et al., 2015). Recent years have seen more and more works on this topic, including two CoNNL shared tasks (Xue et al., 2015; Xue et al., 2016). The community most often uses the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) as a resource, and has adopted the usual split into training and test data as used for other tasks such as parsing. Because discourse relation annotation is at a higher level than syntactic annotation, this however means that the test set is rather small, and with the amount of alternative feat"
E17-2024,L16-1165,1,0.880809,"Missing"
E17-2024,D15-1266,0,0.45182,"(Pitler et al., 2008). In implicit relations, no such strong cue is available and the discourse relation instead needs to be inferred based on the two textual arguments. In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016). For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen inIntroduction Discourse-level relation analysis is relevant to a variety of NLP tasks such as summarization (Yoshida et al., 2014), question answering (Jansen et al., 2014) and machine translation (Meyer et al., 2015). Recent years have seen more and more works on this topic, including two CoNNL shared tasks (Xue et al., 2015; Xue et al., 2016). The community most often uses the Penn Discourse Treebank ("
E17-2024,E14-1068,0,0.449992,"eebank (PDTB) We use the Penn Discourse Treebank (Prasad et al., 2008), the largest available manually annotated corpora of discourse on top of one million word tokens from the Wall Street Journal (WSJ). The PDTB provides annotations for explicit and implicit discourse relations. By definition, an explicit relation contains an explicit discourse connective while the implicit one does not. The PDTB provides a three level hierarchy of relation tags for its annotation. Previous work in this task has been done over two schemes of evaluation: first-level 4-ways classification (Pitler et al., 2009; Rutherford and Xue, 2014; Chen et al., 2016), second-level 11-way classification (Lin et al., 2009; Ji and Eisenstein, 2015). The distribution of second-level relations in PDTB is illustrated in Table 1. We follow the preprocessing method in (Lin et al., 2009; Rutherford et al., 2017). If the instance is annotated with two relations, we adopt the first one shown up, and remove those relations with too few instances. We treat section 2-21 as training set, section 22 as development set and section 23 as test set for our results reported as “mostused split”. In order to investigate whether the results for benefit of inc"
E17-2024,C10-2172,0,0.320584,"elation connectives (Pitler et al., 2008). In implicit relations, no such strong cue is available and the discourse relation instead needs to be inferred based on the two textual arguments. In recent studies, various classes of features are explored to capture lexical and semantic regularities for identifying the sense of implicit relations, including linguistically informed features like polarity tags, Levin verb classes, length of verb phrases, language model based features, contextual features, constituent parse features and dependency parse features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016). For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen inIntroduction Discourse-level relation analysis is relevant to a variety of NLP tasks such as summarization (Yoshida et al., 2014), question answering (Jansen et al., 2014) and machine translation (Meyer et al., 2015). Recent years have seen more and more works on this topic, including two CoNNL shared tasks (Xue et al., 2015; Xue et al., 2016). The community most often uses the Penn"
E17-2024,N15-1081,0,0.463858,"Missing"
E17-2024,E17-1027,1,0.903671,"he spe150 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 150–156, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics available annotated corpora of discourse relations, opened the door to machine learning based discourse relation classification. stances, so that it’s important to make maximal use of both the data set for training and testing. The test set that is currently most often used for 11 way classification is section 23 (Lin et al., 2009; Ji and Eisenstein, 2015; Rutherford et al., 2017), which contains only about 761 implicit relations. This small size implies that a gain of 1 percentage point in accuracy corresponds to just classifying an additional 7-8 instances correctly. This paper therefore aims to demonstrate the degree to which conclusions about the effectiveness of including certain features would depend on whether one evaluates on the standard test section only, or performs cross validation on the whole dataset for second-level discourse relation classification. The model that we use is a neural network that takes the words occurring in the relation arguments as inp"
E17-2024,N03-1030,0,0.10485,"buted word representations (Bengio et al., 2003; Mikolov et al., 2013) have shown an advantage in dealing with data sparsity problem (Braud and Denis, 2015). Many deep learning methods have been proved to be helpful in discourse relation parsing and achieved some significant progresses. Zhang et al. (2015) proposed a shallow convolutional neural network for implicit discourse recognition to alleviate the overfitting problem and help preserve the recognition and generalization ability with the model. Ji et al. (2015) computed distributed meaning represenBackground on Discourse Relation Parsing Soricut and Marcu (2003) firstly addressed the task of parsing discourse structure within the same sentence. Many of the useful features proposed by them, syntax in particular, revealed that both arguments of the connectives are found in the same sentence. The release of PDTB, the largest 151 data for our demonstration of variability between folds. For best practice when testing new models, we instead recommend to keep the validation set completely separate and do cross-validation for the remaining data. Also note that you might want to choose repeated cross-validation (which simply repeats the cross-validation step"
E17-2024,C12-1168,0,0.528171,"Missing"
E17-2024,K15-2001,0,0.280686,"ncy parse features (Lin et al., 2009; Pitler et al., 2009; Zhou et al., 2010; Zhang et al., 2015; Chen et al., 2016). For some of second-level relations (a level of granularity that should be much more meaningful to downstream tasks than the four-way distinction), there are only a dozen inIntroduction Discourse-level relation analysis is relevant to a variety of NLP tasks such as summarization (Yoshida et al., 2014), question answering (Jansen et al., 2014) and machine translation (Meyer et al., 2015). Recent years have seen more and more works on this topic, including two CoNNL shared tasks (Xue et al., 2015; Xue et al., 2016). The community most often uses the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) as a resource, and has adopted the usual split into training and test data as used for other tasks such as parsing. Because discourse relation annotation is at a higher level than syntactic annotation, this however means that the test set is rather small, and with the amount of alternative features and, more recently, neural network architectures being applied to the problem, we run a serious risk as a community of believing in features that are successful in getting some improvement on"
E17-2024,K16-2001,0,\N,Missing
I17-1049,C12-1163,1,0.853934,"y marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-learning methods to improve recognition of patterns that typically signal causal discourse relations. Related Work Early works addressing discourse relation parsing were trying to classify unmarked discourse relations by training on explicit discourse relations with the marker been removed (Marcu and Echihabi, 2002). While this method promised to provide almost unlimited training data, it was shown that explicit relations differ in systematic ways from implicit relations (Asr and Demberg, 2012), so that performance on implicits is very poor when learning on explicits only (Sporleder and Lascarides, 2008). The release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples, lead to substantial improvements in classification of implicit relations, and spurred The approach proposed here differs from previous approaches, because we extend our train485 of the end-to-end parser. Implicit relations in the PDTB are only ever annotated between consecutive sentences. Therefore, we specifically extract pairs of consecutive sentences on the source English s"
I17-1049,P13-2013,0,0.0441453,"no et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 201"
I17-1049,P16-1163,0,0.0343774,"012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just"
I17-1049,P17-4012,0,0.019691,"et al., 2016; R¨onnqvist et al., 2017). The model is illustrated in Figure 2, where each word from the two discourse relational arguments is represented as a vector, which is found through a look-up word embedding. Given the word representations [w1 ,w2 ,...,wn ] as the input sequence, an Machine Translation System We train an MT system to back-translate the target side of the parallel corpus to English. To produce the highest-quality back-translation, we use a neural MT system trained on the same parallel corpus. The system is implemented by Open-source Neural Machine Translation (OpenNMT) (Klein et al., 2017). Source words are first mapped to word vectors and then fed into a recurrent neural network. 4 Case sensitive BLEU implemented in mteval-v13a.pl. Test sets available at http://www.statmt.org/ wmt15/translation-task.html 5 The non-explicit sense classification module of this parser is thus not used in the proposed method. 3 All corpora are available at http://cl.haifa.ac. il/projects/translationese/ 488 Figure 2: The bidirectional LSTM Network for the task of implicit discourse relation classification. LSTM computes the state sequence [h1 ,h2 ,...,hn ] with the following equations: it = σ(Wiw"
I17-1049,P11-1132,0,0.034184,"sists of just 12763 implicit instances in the usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore crucial to obtain extra data for machine learning. In this paper, we propose a simple approach to automatically extract samples of implicit discourse relations from parallel corpus via backtranslation: Our approach is motivated by the fact that humans sometimes omit connectives during translation (implicitation), or insert connectives not originally present in the source text (explicitation) (Laali and Kosseim, 2014; Koppel and Ordan, 2011; Cartoni et al., 2011; Hoek and Zufferey, 2015; Zufferey, 2016). When explicitating an implicit relation, the human translator is, in other words, disambiguating the source implicit relation with an explicit DC in the target language. Our contribution is twofold: Firstly, we propose a pipeline to automatically label English implicit discourse relation samples based on explicitation of DCs in human translation, which is the target side of a parallel corpus. Secondly, we show that the extra instances mined by the proposed method improve the performance of a standard neural classifier by a large"
I17-1049,D14-1168,0,0.0447042,"or them, who loved them.]Arg1 [The last thing they needed was another drag-down blow.]Arg2 —Implicit, Comparison.Contrast Introduction When humans comprehend language, their interpretation consists of more than just the sum of the content of the sentences. Additional semantic relations (known as coherence relations or discourse relations) are inferred between sentences in the text. Identification of discourse relations is useful for various NLP applications such as question answering (Jansen et al., 2014; Liakata et al., 2013), summarization (Maskey and Hirschberg, 2005; Yoshida et al., 2014; Gerani et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015) and information extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify r"
I17-1049,C14-1058,0,0.106724,"sed by the community, consists of just 12763 implicit instances in the usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore crucial to obtain extra data for machine learning. In this paper, we propose a simple approach to automatically extract samples of implicit discourse relations from parallel corpus via backtranslation: Our approach is motivated by the fact that humans sometimes omit connectives during translation (implicitation), or insert connectives not originally present in the source text (explicitation) (Laali and Kosseim, 2014; Koppel and Ordan, 2011; Cartoni et al., 2011; Hoek and Zufferey, 2015; Zufferey, 2016). When explicitating an implicit relation, the human translator is, in other words, disambiguating the source implicit relation with an explicit DC in the target language. Our contribution is twofold: Firstly, we propose a pipeline to automatically label English implicit discourse relation samples based on explicitation of DCs in human translation, which is the target side of a parallel corpus. Secondly, we show that the extra instances mined by the proposed method improve the performance of a standard neur"
I17-1049,P14-1065,0,0.120711,"Missing"
I17-1049,P13-1047,0,0.052225,"dea has been to select explicit discourse instances that are similar to implicit ones to add to the training set. Wang et al. (2012) proposed to differentiate typical and atypical examples for each discourse relation, and augment training data for implicits only by typical explicits. In a similar vein, Rutherford and Xue (2015) proposed criteria for selecting among explicitly marked relations ones that contain discourse connectives which can be omitted without changing the interpretation of the discourse. These relations are then added to the implicit instances in training. On the other hand, Lan et al. (2013) presented multi-task learning based systems, which in addition to the main implicit relation classification task, contain the task of predicting previously removed connectives for explicit relations, and profit from shared representations between the tasks. Similarly, Hernault et al. (2010) observes features that occur in both implicit and explicit discourse relations, and exploit such feature co-occurrence to extend the features for classifying implicits using explicitly marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-l"
I17-1049,D10-1039,0,0.0212566,"a similar vein, Rutherford and Xue (2015) proposed criteria for selecting among explicitly marked relations ones that contain discourse connectives which can be omitted without changing the interpretation of the discourse. These relations are then added to the implicit instances in training. On the other hand, Lan et al. (2013) presented multi-task learning based systems, which in addition to the main implicit relation classification task, contain the task of predicting previously removed connectives for explicit relations, and profit from shared representations between the tasks. Similarly, Hernault et al. (2010) observes features that occur in both implicit and explicit discourse relations, and exploit such feature co-occurrence to extend the features for classifying implicits using explicitly marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-learning methods to improve recognition of patterns that typically signal causal discourse relations. Related Work Early works addressing discourse relation parsing were trying to classify unmarked discourse relations by training on explicit discourse relations with the marker been removed (M"
I17-1049,D13-1070,0,0.0145759,"Explicit, Contingency.Cause 2. [They desperately needed somebody who showed they cared for them, who loved them.]Arg1 [The last thing they needed was another drag-down blow.]Arg2 —Implicit, Comparison.Contrast Introduction When humans comprehend language, their interpretation consists of more than just the sum of the content of the sentences. Additional semantic relations (known as coherence relations or discourse relations) are inferred between sentences in the text. Identification of discourse relations is useful for various NLP applications such as question answering (Jansen et al., 2014; Liakata et al., 2013), summarization (Maskey and Hirschberg, 2005; Yoshida et al., 2014; Gerani et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015) and information extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused"
I17-1049,P16-1135,0,0.0824352,"implicit instances in training. On the other hand, Lan et al. (2013) presented multi-task learning based systems, which in addition to the main implicit relation classification task, contain the task of predicting previously removed connectives for explicit relations, and profit from shared representations between the tasks. Similarly, Hernault et al. (2010) observes features that occur in both implicit and explicit discourse relations, and exploit such feature co-occurrence to extend the features for classifying implicits using explicitly marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-learning methods to improve recognition of patterns that typically signal causal discourse relations. Related Work Early works addressing discourse relation parsing were trying to classify unmarked discourse relations by training on explicit discourse relations with the marker been removed (Marcu and Echihabi, 2002). While this method promised to provide almost unlimited training data, it was shown that explicit relations differ in systematic ways from implicit relations (Asr and Demberg, 2012), so that performance on implicits is very poor when lear"
I17-1049,D09-1036,0,0.505892,"Missing"
I17-1049,W15-0205,0,0.0220059,"usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore crucial to obtain extra data for machine learning. In this paper, we propose a simple approach to automatically extract samples of implicit discourse relations from parallel corpus via backtranslation: Our approach is motivated by the fact that humans sometimes omit connectives during translation (implicitation), or insert connectives not originally present in the source text (explicitation) (Laali and Kosseim, 2014; Koppel and Ordan, 2011; Cartoni et al., 2011; Hoek and Zufferey, 2015; Zufferey, 2016). When explicitating an implicit relation, the human translator is, in other words, disambiguating the source implicit relation with an explicit DC in the target language. Our contribution is twofold: Firstly, we propose a pipeline to automatically label English implicit discourse relation samples based on explicitation of DCs in human translation, which is the target side of a parallel corpus. Secondly, we show that the extra instances mined by the proposed method improve the performance of a standard neural classifier by a large margin, when evaluated on the PDTB 2.0 benchma"
I17-1049,P14-1092,0,0.122089,"re incomplete.]Arg2 — Explicit, Contingency.Cause 2. [They desperately needed somebody who showed they cared for them, who loved them.]Arg1 [The last thing they needed was another drag-down blow.]Arg2 —Implicit, Comparison.Contrast Introduction When humans comprehend language, their interpretation consists of more than just the sum of the content of the sentences. Additional semantic relations (known as coherence relations or discourse relations) are inferred between sentences in the text. Identification of discourse relations is useful for various NLP applications such as question answering (Jansen et al., 2014; Liakata et al., 2013), summarization (Maskey and Hirschberg, 2005; Yoshida et al., 2014; Gerani et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015) and information extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Ear"
I17-1049,P02-1047,0,0.385847,") observes features that occur in both implicit and explicit discourse relations, and exploit such feature co-occurrence to extend the features for classifying implicits using explicitly marked relations. Mih˘ail˘a and Ananiadou (2014) and Hidey and McKeown (2016) proposed semi-supervised learning and self-learning methods to improve recognition of patterns that typically signal causal discourse relations. Related Work Early works addressing discourse relation parsing were trying to classify unmarked discourse relations by training on explicit discourse relations with the marker been removed (Marcu and Echihabi, 2002). While this method promised to provide almost unlimited training data, it was shown that explicit relations differ in systematic ways from implicit relations (Asr and Demberg, 2012), so that performance on implicits is very poor when learning on explicits only (Sporleder and Lascarides, 2008). The release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples, lead to substantial improvements in classification of implicit relations, and spurred The approach proposed here differs from previous approaches, because we extend our train485 of the end-to-end p"
I17-1049,Q15-1024,0,0.196373,"iran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not re"
I17-1049,E17-1027,1,0.892968,"e Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just 761 relations). 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). Both streams of methods suffer from insufficient annotated data (Wang et al., 2015), since the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which is the d"
I17-1049,E14-1068,0,0.152296,"ncreasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al.,"
I17-1049,W12-1614,0,0.103706,"y, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Che"
I17-1049,P09-1077,0,0.120841,"res to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estima"
I17-1049,N15-1081,0,0.130077,"osed method improve the performance of a standard neural classifier by a large margin, when evaluated on the PDTB 2.0 benchmark test set as well as by crossvalidation (Shi and Demberg, 2017). 2 Data extension has therefore been a longstanding goal in discourse relation classification. The main idea has been to select explicit discourse instances that are similar to implicit ones to add to the training set. Wang et al. (2012) proposed to differentiate typical and atypical examples for each discourse relation, and augment training data for implicits only by typical explicits. In a similar vein, Rutherford and Xue (2015) proposed criteria for selecting among explicitly marked relations ones that contain discourse connectives which can be omitted without changing the interpretation of the discourse. These relations are then added to the implicit instances in training. On the other hand, Lan et al. (2013) presented multi-task learning based systems, which in addition to the main implicit relation classification task, contain the task of predicting previously removed connectives for explicit relations, and profit from shared representations between the tasks. Similarly, Hernault et al. (2010) observes features t"
I17-1049,C08-2022,0,0.175133,"Missing"
I17-1049,E17-2024,1,0.676276,"; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just 761 relations). 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). Both streams of methods suffer from insufficient annotated data (Wang et al., 2015), since the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which is the discourse annotated resource mostly used by the community, consists of just 12763 implicit instances in the usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore cr"
I17-1049,prasad-etal-2008-penn,0,0.281943,"n training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just 761 relations). 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). Both streams of methods suffer from insufficient annotated data (Wang et al., 2015), since the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which is the discourse annotated resource mostly used by the community, consists of just 12763 implicit instances in the usual training set and 761 relations in the test set. Some second-level relations only have about a dozen instances. It is therefore crucial to obtain extra data for machine learning. In this paper, we propose a simple approach to automatically extract samples of implicit discourse relations from parallel corpus via backtranslation: Our approach is motivated by the fact that humans sometimes omit connectives during translation (implicitation), or insert connectives not ori"
I17-1049,C16-1180,0,0.770166,"Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the task of inferring the type of relation between given text spans, presents a problem both in training (Rutherford et al. (2017) find that a simple feed-forward architecture can outperform more complex architectures, and argues that the larger number of parameters can not be estimated adequately on the small amount of training data) and testing (Shi and Demberg (2017) report experiments showing that results on the standard test set are not reliable due to the small set of just 761 relations). 20"
I17-1049,P17-1093,0,0.385933,"samples are extracted, of which 25, 086 are inter-sentential relations and 77, 228 are intrasentential7 . Inter-sentential relations are much less abundant because stricter screening strategy is applied (the end of point 3 in Section 3). From Table 1 we can also see that majority of DCs in the “→” means from source to target side. Table 1: Numbers of intra/inter-sentence samples extracted from parallel corpora. 7 A dataset containing these additional instances will be made available to researchers upon publication of the paper. 489 Models Most common class Lin et al. (2009) Qin et al. (2016) Qin et al. (2017) Rutherford et al. (2017) Shi and Demberg (2017) (no surface features) Ours 1 PDTB only PDTB + inter-sentential samples PDTB + intra-sentential samples PDTB + all samples PDTB Test Set Cross Validation 25.36 40.20 43.81 44.65 39.56 37.68 25.59 34.44 34.32 42.29 44.29 45.50 30.01 34.14 35.08 37.84 “-” means no result currently. Table 2: Accuracy of 11-way classification of implicit discourse relations on PDTB test set and by cross validation. be largely orthogonal to the choice of classification model, we are here most interested in seeing whether adding the new instances improves over the base"
I17-1049,C12-1168,0,0.19068,"Missing"
I17-1049,D16-1253,0,0.398231,"Missing"
I17-1049,K16-2001,0,0.170987,"Missing"
I17-1049,D14-1196,0,0.0899231,"ho showed they cared for them, who loved them.]Arg1 [The last thing they needed was another drag-down blow.]Arg2 —Implicit, Comparison.Contrast Introduction When humans comprehend language, their interpretation consists of more than just the sum of the content of the sentences. Additional semantic relations (known as coherence relations or discourse relations) are inferred between sentences in the text. Identification of discourse relations is useful for various NLP applications such as question answering (Jansen et al., 2014; Liakata et al., 2013), summarization (Maskey and Hirschberg, 2005; Yoshida et al., 2014; Gerani et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015) and information extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more"
I17-1049,D15-1266,0,0.329857,"xpressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 2015; Ji and Eisenstein, 2015; Ji et al., 2016; Chen et al., 2016; Qin et al., 2016, 2017). However, the limited size of the annotated corpus, in combination with the difficulty of the t"
I17-1049,C12-2138,0,0.0232655,"anslation. Parallel corpora have been exploited as a resource of discourse relation data in previous work but have mostly been used with goals different from ours: Cartoni et al. (2013) and Meyer et al. (2015) use parallel corpora to label and disambiguate discourse connectives in the target language based on explicitly marked English relations, in order to help machine translation. A second application has been to project discourse annotation from English onto other languages through parallel corpora, in order to construct discourse annotated resources for the target language (Versley, 2010; Zhou et al., 2012; Laali and Kosseim, 2014). The approach that is in spirit most similar to ours is by Wu et al. (2016), who extracted bilingual-constrained synthetic implicit data from a sentence-aligned English-Chinese corpus and got improvements by incorporating these data via a multi-task neural network on the 4-way classification. 3 • that are identified as the Arg1 and Arg2 of an implicit discourse relation1 ; • whose corresponding back-translated target sentences are identified as the Arg1 and Arg2 of an explicit relation; • that are not part of the Arg1 or Arg2 of any other discourse relations2 . 4. La"
I17-1049,P16-2034,1,0.813794,"other types of relations, i.e. explicit relations or no relation, in order to extract implicit-to-explicit DC translation from the parallel corpus5 . On the back-translation, the end-to-end parser is applied to identify only explicitly marked discourse relations. 4.4 Implicit relation classification model We use a Bidirectional Long Short-Term Memory (LSTM) network as the implicit relation classification model to evaluate the samples extracted by the proposed method. This architecture inspects both left and right contextual information and has been proven effective in relation classification (Zhou et al., 2016; R¨onnqvist et al., 2017). The model is illustrated in Figure 2, where each word from the two discourse relational arguments is represented as a vector, which is found through a look-up word embedding. Given the word representations [w1 ,w2 ,...,wn ] as the input sequence, an Machine Translation System We train an MT system to back-translate the target side of the parallel corpus to English. To produce the highest-quality back-translation, we use a neural MT system trained on the same parallel corpus. The system is implemented by Open-source Neural Machine Translation (OpenNMT) (Klein et al.,"
I17-1049,C10-2172,0,0.160486,"n extraction (Cimiano et al., 2005). Recently, the task has drawn increasing attention, including two CoNLL shared tasks (Xue et al., 2015, 2016). Discourse relations are sometimes expressed In order to classify an implicit discourse relation, it is necessary to represent the semantic content of the relational arguments, which may give a cue to the coherence relation, e.g. “care” – “dragdown blow” in 2. Early methods have focused on designing various features to overcome data sparsity and more effectively identify relevant concepts in the two discourse relational arguments. (Lin et al., 2009; Zhou et al., 2010; Biran and McKeown, 2013; Park and Cardie, 2012; Rutherford and Xue, 2014), while recent efforts use distributed representations with neural network architectures (Zhang et al., 2015; Ji and Eisenstein, 484 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 484–495, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a variety of approaches to the task, including feature-based methods (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014) and neural network models (Zhang et al., 201"
J11-3003,E06-1009,1,0.764138,"designed to select attributes that generalize well over the data (i.e., produce large clusters of options), and thus lead to efﬁcient summarization. Hence attributes that partition the data set into a small number of clusters are preferred. If the attribute that is best for summarization is not of interest to a particular user, dialog duration is increased unnecessarily. This in turn may lead to reduced user satisfaction, as the results of our evaluation suggest (see Section 4.1.3). 3. Our Approach: User Model Based Summarize and Reﬁne (UMSR) Our approach, the UMSR approach ﬁrst described in Demberg and Moore (2006), is intended to capture the complementary strengths of the two previous approaches. It exploits information from a user model to reduce dialog duration by selecting only options that are relevant to the user. In addition, we introduce a content structuring algorithm that supports stepwise reﬁnement, as in Polifroni, Chung, and Seneff (2003), 494 Demberg, Winterboer, and Moore A Strategy for Information Presentation in SDSs but in which the structuring reﬂects the user’s preferences. Thus our approach maintains the beneﬁts of user tailoring, while also being capable of dealing with a large num"
J11-3003,W03-2123,0,\N,Missing
J11-3003,W06-1304,0,\N,Missing
J11-3003,H01-1015,0,\N,Missing
J11-3003,P01-1066,0,\N,Missing
J11-3003,W09-3901,1,\N,Missing
J11-3003,P04-1009,0,\N,Missing
J11-3003,P03-1033,0,\N,Missing
J11-3003,P08-1055,0,\N,Missing
J11-3003,P03-2038,0,\N,Missing
J11-3003,J10-2001,1,\N,Missing
J13-4008,J99-2004,0,0.0217554,"hich creates a great number of new prefix trees: At each prediction step, thousands of prediction trees can potentially be combined with all prefix trees; this is computationally not feasible. Non-incremental parsers, which do not use the unlexicalized prediction trees, have to deal with the much lower level of ambiguity among canonical trees (about 50 trees per word on average if using a lexicon the size of our canonical lexicon). In our parser implementation, we use supertagging to select only the best prediction trees in each step, which reduces the search space considerably. Supertagging (Bangalore and Joshi 1999) is a common approach used in the context of TAG and CCG parsing; the idea is to limit the elementary trees for each word to those that are evaluated highly by some shallow statistical model. We only use supertagging for prediction trees; for canonical trees, we use all (lexicalized) trees that the grammar contains for the word (rare words are replaced by “UNK”). 1041 Computational Linguistics Volume 39, Number 4 Because our parser must run incrementally, the supertagger should not be allowed to have any look-ahead. We found, however, that not having any look-ahead has a detrimental impact on"
J13-4008,A00-1031,0,0.0260399,"he leaf node on the spine slpredict , and the probability of some tree with first fringe fpredict and category of the leaf node on the spine slpredict given a prefix tree with current fringe fp and estimated POS tag of the next word twi+1 . A further simplification is that we represent the current fringes fpredict and fp as an alphabetically ordered set of the categories occurring on it. The reasoning behind this decision is that the order of nodes is less important than the identity of the nodes as possible integration sites. The supertagging model is smoothed with the procedure described by Brants (2000), as it yielded better results than WittenBell smoothing (which suffers from data sparsity in the supertagging task). We use one level of back-off where we estimate P( fpredict , slpredict |fp , ti+1 ) based only on the most likely integration site np instead of the whole fringe fp : max P(fpredict , tpredict |np , twi+1 ) np (12) The reason for backing off to the most probable integration site is that a fringe with more unique categories should not have a lower probability of a particular tree adjoining into it than a fringe containing the same category, but fewer other categories. 5. Treeban"
J13-4008,P00-1058,0,0.0649511,"re 4, the head of the S node is sleeps and the head of the NP node is Peter), but could also be the non-lexical leaf of a prediction tree (the head of the upper VP node in the third prefix tree is the lower VP node). The head of any node on the spine of a canonical elementary tree is always the lexical anchor. 3.6 Probability Model We are now ready to define the probability model for PLTAG. This model allows us to define a probability distribution over the derivations of any given PLTAG grammar. It makes the same independence assumptions as standard models for probabilistic TAG (Resnik 1992b; Chiang 2000): Any two applications of derivation rules are statistically independent events. We deviate from these models, however, with regard to what these events are. Earlier approaches always modeled the probability of substituting or adjoining the lower elementary tree, given the upper elementary tree and the integration site. This is inconsistent with the incremental perspective we take here, which assumes that the prefix tree is given, and we must decide how to integrate an elementary tree for the next word with it. We therefore model the probability of substituting, adjoining, or verifying the ele"
J13-4008,J03-4003,0,0.0323503,"ows us to distinguish high and low attachment. The probability models are now obtained via maximum likelihood estimation from the training data. Many of the substitution and adjunction events are seen rarely or not at all with their full contexts, which indicates the need for smoothing. We use back-off with deleted interpolation, as detailed in Table 1. The weight for each of these contexts is automatically determined by a variant of Witten-Bell smoothing, which calculates a weight for each of the back-off levels for each context (Witten and Bell 1991). We implemented the version described by Collins (2003). For the verification operation, data sparsity for the probability of the tree template τv is less of an issue because the probability of a tree template verifying a prediction tree is conditioned only on the identity of the prediction tree and the trace feature. 6. Evaluation In order to compare the PLTAG parser to other probabilistic parsers, we evaluated parsing accuracy on the Penn Treebank (PTB). We first converted the PTB into a PLTAG 1047 Computational Linguistics Volume 39, Number 4 treebank as described in Section 5. We then trained the parser on Sections 2–21 of the Penn Treebank an"
J13-4008,P04-1015,0,0.358872,"Missing"
J13-4008,W12-4623,1,0.833782,"r to the use of type raising in incremental derivations in CCG (Steedman 2000). For example, the prediction tree in Figure 1d effectively raises the NP in Figure 1a to type (S/(SNP)) so that it can compose with the adverb in Figure 1c. Prediction trees, however, are more powerful in terms of the incremental derivations they support: Some psycholinguistically crucial 1032 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar constructions (such as object relative clauses) are handled easily by PLTAG, but are not incrementally derivable in standard CCG (Demberg 2012). According to Demberg, this problem can be overcome by generalizing the CCG categories involved (in the case of object relative clauses, the category of the relative pronoun needs to be changed). 3.3 Verification Markers are eliminated from a partial derived tree through a new operation called verification. Recall that markers indicate nodes that were predicted during the derivation, without having been introduced by a word that was actually observed so far. The verification operation removes these markers by matching them with the nodes of the canonical elementary tree for a word in the sent"
J13-4008,W08-2304,1,0.394592,"incorrect. Presumably, the human sentence processor uses prediction mechanisms to enable efficient comprehension in real time. The three concepts of incrementality, connectedness, and prediction are fundamentally interrelated: Maintaining connected partial analyses is only nontrivial if the parsing process is incremental, and prediction means that a connected analysis is required also for words the parser has not yet seen. In this article, we exploit the interrelatedness of incrementality, connectedness, and prediction to develop a parsing model for psycholinguistically motivated TAG (PLTAG; Demberg and Keller 2008b). This formalism augments standard tree-adjoining grammar (TAG; Joshi, Levy, and Takahashi 1975) with a predictive lexicon and a verification operation for validating predicted structures. As we show in Section 2, these operations are motivated by psycholinguistic findings. We argue that our PLTAG parser can form the basis for a new model of human sentence processing. We successfully evaluate the predictions of this model against reading time data from an eye-tracking corpus, showing that it provides a better fit with the psycholinguistic data than the standard surprisal model of human sente"
J13-4008,W12-1706,0,0.0167801,"a (Demberg and Keller 2008a). It is important to note, though, that adding verification cost to the baseline LME model increases model fit significantly, which provides some evidence for effectiveness of the verification cost component. 10 The result for Roark structural surprisal differs from that reported by Demberg and Keller (2008a) and Demberg-Winterfors (2010). This can be attributed to the different outlier removal and more conservative treatment of random effects in the present article. 11 Surprisal has subsequently been reported to be a significant predictor of Dundee reading time by Fossum and Levy (2012), who used a context-free grammar induced using the state-split model of Petrov and Klein (2007) in combination with a standard probabilistic Earley parser to compute surprisal estimates. 1056 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar Table 4 Linear mixed effects models of first-pass time for predictors of theoretical interest: Prediction Theory cost, PLTAG surprisal, PLTAG verification cost, Roark lexical surprisal, and Roark structural surprisal, each residualized against low-level predictors (see text for details). Random intercepts of pa"
J13-4008,N01-1021,0,0.605496,"emental parser is to develop a more realistic model of human language processing. A treebank-based evaluation as in the previous section does not directly provide evidence of psycholinguistic validity; however, a parser with good coverage and high parsing accuracy is a prerequisite for an evaluation on eye-tracking corpora, which Keller (2010) argues are the benchmark for models of human sentence processing. In what follows, we report an evaluation study that uses our PLTAG parser to predict human reading times, and compares its performance on this task to a standard model based on surprisal (Hale 2001). Surprisal assumes that processing difficulty is associated with expectations built up by the sentence processor: A word that is unexpected given its preceding context is harder to process. Mathematically, the amount of surprisal at word wi can be formalized as the negative logarithm of the conditional probability of wi given the preceding words in the sentence w1 . . . wi−1 : Surprisalwi = − log P(wi |w1 . . . wi−1 ) (13) P(w1 . . . wi ) P(w1 . . . wi−1 )  = − log P(τpw ...w ) + log = − log τpw 1 i 1 ...wi  τpw P(τpw 1 ...wi−1 ) 1 ...wi−1 Here, P(τpw ...w ) is the probability of the prefix"
J13-4008,J07-3004,0,0.0406482,"Missing"
J13-4008,C92-2066,0,0.739181,"his property is essential for testing psycholinguistic models on realistic data, including eye-tracking corpora. The PLTAG formalism was first proposed by Demberg-Winterfors (2010), who also presents an earlier version of the parsing algorithm, probability model, implementation, and evaluation described in the current article. 3. The PLTAG Formalism We start by introducing the PLTAG formalism, which we will use throughout the article. 3.1 Incremental TAG Parsing Tree Adjoining Grammar (TAG) is a grammar formalism based on combining trees. In what follows we will focus on lexicalized TAG (TAG; Joshi and Schabes 1992), which is the most widely used version of TAG. In this formalism, a TAG lexicon consists of a finite set of elementary trees whose nodes are labeled with nonterminal or terminal symbols. Each elementary tree contains an anchor, a leaf node labeled with a terminal symbol. At most one other leaf—the foot node—may carry a label of the form A∗, where A is a nonterminal symbol. All other leaves are substitution nodes and labeled with symbols of the form A↓. Elementary trees that contain a foot node are called auxiliary trees; those that contain no foot nodes are initial trees. We will generally ca"
J13-4008,kaeshammer-demberg-2012-german,1,0.886542,"Missing"
J13-4008,W04-0302,0,0.0731294,"Missing"
J13-4008,P10-2012,1,0.943603,"psycholinguistically motivated way. We achieve this by exploiting the fact that these three concepts are closely related: In order to guarantee that the syntactic structure of a sentence prefix is fully connected, it may be necessary to build phrases whose lexical anchors (the words that they relate to) have not been encountered yet. In other words, the parser needs to predict upcoming syntactic structure in order to ensure connectedness. This prediction scheme is complemented by an explicit verification mechanism in our approach. Furthermore, unlike most existing psycholinguistic models (see Keller 2010 for an overview), our model achieves broad coverage and acceptable parsing performance on a standard test corpus. This property is essential for testing psycholinguistic models on realistic data, including eye-tracking corpora. The PLTAG formalism was first proposed by Demberg-Winterfors (2010), who also presents an earlier version of the parsing algorithm, probability model, implementation, and evaluation described in the current article. 3. The PLTAG Formalism We start by introducing the PLTAG formalism, which we will use throughout the article. 3.1 Incremental TAG Parsing Tree Adjoining Gr"
J13-4008,P10-1021,1,0.378233,"Missing"
J13-4008,W04-0308,0,0.157879,"Missing"
J13-4008,N07-1051,0,0.0145811,"the baseline LME model increases model fit significantly, which provides some evidence for effectiveness of the verification cost component. 10 The result for Roark structural surprisal differs from that reported by Demberg and Keller (2008a) and Demberg-Winterfors (2010). This can be attributed to the different outlier removal and more conservative treatment of random effects in the present article. 11 Surprisal has subsequently been reported to be a significant predictor of Dundee reading time by Fossum and Levy (2012), who used a context-free grammar induced using the state-split model of Petrov and Klein (2007) in combination with a standard probabilistic Earley parser to compute surprisal estimates. 1056 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar Table 4 Linear mixed effects models of first-pass time for predictors of theoretical interest: Prediction Theory cost, PLTAG surprisal, PLTAG verification cost, Roark lexical surprisal, and Roark structural surprisal, each residualized against low-level predictors (see text for details). Random intercepts of participant and random slopes under participants for the predictors of interest were also included"
J13-4008,C92-1032,0,0.693292,"plicit prediction and verification mechanism (WCDG includes prediction, but not verification), which means that they cannot be used to model psycholinguistic results that involve verification cost.1 A simple form of prediction can be achieved in a chart parser (incomplete edges in the chart can be seen as predictive), but in order to maintain psycholinguistic plausibility, an arc-eager left-corner parsing strategy needs to be used. Other parsing strategies fail to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson, Dixon, and Lamping 1991; Resnik 1992a). This is an argument against using a top–down parser such as Roark’s for psycholinguistic modeling. Furthermore, it is important to emphasize that a full model of human parsing needs to not only model prediction, but also account for processing difficulty associated with the verification of predictions (we will return to this point in Section 7). None of the existing incremental parsing models includes an explicit verification component. 1 As Demberg and Keller (2009) show, some psycholinguistic results can be accounted for by a model without verification, such as the either . . . or findin"
J13-4008,C92-2065,0,0.795513,"plicit prediction and verification mechanism (WCDG includes prediction, but not verification), which means that they cannot be used to model psycholinguistic results that involve verification cost.1 A simple form of prediction can be achieved in a chart parser (incomplete edges in the chart can be seen as predictive), but in order to maintain psycholinguistic plausibility, an arc-eager left-corner parsing strategy needs to be used. Other parsing strategies fail to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson, Dixon, and Lamping 1991; Resnik 1992a). This is an argument against using a top–down parser such as Roark’s for psycholinguistic modeling. Furthermore, it is important to emphasize that a full model of human parsing needs to not only model prediction, but also account for processing difficulty associated with the verification of predictions (we will return to this point in Section 7). None of the existing incremental parsing models includes an explicit verification component. 1 As Demberg and Keller (2009) show, some psycholinguistic results can be accounted for by a model without verification, such as the either . . . or findin"
J13-4008,J01-2004,0,0.0519268,"oach, however, allows multiple unconnected subtrees for a sentence prefix and uses a look-ahead of two words, that is, it does not build connected structures. An example of a TAG parser that is both incremental and builds connected structures is the work of Kato, Matsubara, and Inagaki (2004). This comes at the price of strong simplifying assumptions with respect to the TAG formalism, such as not distinguishing modifiers and arguments. (We will return to a discussion of other TAG parsers in Section 6.1.) An example of an incremental parser based on context-free grammars is the one proposed by Roark (2001). That parser uses a top–down algorithm to build fully connected structures; it is also able to compute probabilities for sentence prefixes, which makes it attractive for psycholinguistic modeling, where prefix probabilities are often used to predict human processing difficulty (see Section 7 for details). The Roark parser has been shown to successfully model psycholinguistic data from eye-tracking corpora (Demberg and Keller 2008a; Frank 2009) and other reading time data (Roark et al. 2009). It therefore is a good candidate for a broad-coverage model of human parsing, and 1028 Demberg, Keller"
J13-4008,D09-1034,0,0.0796811,"rs in Section 6.1.) An example of an incremental parser based on context-free grammars is the one proposed by Roark (2001). That parser uses a top–down algorithm to build fully connected structures; it is also able to compute probabilities for sentence prefixes, which makes it attractive for psycholinguistic modeling, where prefix probabilities are often used to predict human processing difficulty (see Section 7 for details). The Roark parser has been shown to successfully model psycholinguistic data from eye-tracking corpora (Demberg and Keller 2008a; Frank 2009) and other reading time data (Roark et al. 2009). It therefore is a good candidate for a broad-coverage model of human parsing, and 1028 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar will serve as a standard of comparison for the model proposed in the current article in Section 7. The Roark parser has been extended with discriminative training (Collins and Roark 2004), resulting in a boost in parsing accuracy. Prefix probabilities cannot be computed straightforwardly in a discriminative framework, however, making this approach less interesting from a psycholinguistic modeling point of view. W"
J13-4008,J10-1001,0,0.0422888,"Missing"
J13-4008,H05-1102,0,0.364427,"ent work has provided evidence for connectedness in a range of other phenomena, including sluicing and ellipsis (Aoshima, Yoshida, and Phillips 2009; Yoshida, Walsh-Dickey, and Sturt 2013). 2.2 Incremental Parsing Models In the previous section, we identified incrementality, connectedness, and prediction as key desiderata for computational models of human parsing. In what follows, we will review work on parsing in computational linguistics in the light of these desiderata. Incremental parsers for a range of grammatical formalisms have been proposed in the literature. An example is the work of Shen and Joshi (2005), who propose an efficient incremental parser for a variant of TAG, spinal TAG. This approach, however, allows multiple unconnected subtrees for a sentence prefix and uses a look-ahead of two words, that is, it does not build connected structures. An example of a TAG parser that is both incremental and builds connected structures is the work of Kato, Matsubara, and Inagaki (2004). This comes at the price of strong simplifying assumptions with respect to the TAG formalism, such as not distinguishing modifiers and arguments. (We will return to a discussion of other TAG parsers in Section 6.1.) A"
J13-4008,P91-1012,0,0.572012,"Missing"
J13-4008,P07-1031,0,0.0242415,"r, we first retrieve the elementary trees for the upcoming lexeme. If the word occurred with more than one POS tag, we choose the POS tag with highest conditional probability given the previous two POS tags. 1042 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar of treebank conversion and lexicon induction here; the reader is referred to DembergWinterfors (2010) for full details. Our PLTAG lexicon (both canonical trees and prediction trees) is derived from the Wall Street Journal section of the Penn Treebank, complemented by noun phrase annotation (Vadas and Curran 2007), and Propbank (Palmer, Gildea, and Kingsbury 2003), as well as a slightly modified version of the head percolation table of Magerman (1994). These additional resources are used to determine the elementary trees for a TAG lexicon, following the procedures proposed by Xia, Palmer, and Joshi (2000). This involves first adding noun phrase annotation to the Penn Treebank, and then determining heads with the head percolation table, augmented with more detailed heuristics for noun phrases.4 As a next step, information from Propbank is used to establish argument and modifier status and to determine w"
J13-4008,C88-2147,0,0.119973,"e as only having an upper half, which again makes a whole node. We assume that lexical leaves only have an upper half, too; this makes no difference, as no substitution or adjunction can be performed on those nodes anyway. The process is illustrated in Figure 3, which shows the recombination of node halves from different elementary trees in the adjunction step of Figure 2: Black node halves come from the elementary tree for sleeps, gray node halves from Peter, and white ones from often. The idea of distinguishing upper and lower node halves that are pushed apart by adjunction comes from FTAG (Vijay-Shanker and Joshi 1988), which equips each node half with a separate feature structure; at the end of the derivation process, the upper and lower feature structures of each node are unified with each other. Node halves will also play a crucial role in PLTAG. 3.2 Prediction Trees We have argued earlier that a psycholinguistic model of sentence processing should be incremental. In the context of TAG and related formalisms, this means that a derivation Figure 3 Fine structure of adjunction. The semicircles represent node halves; all node halves from the same elementary tree are drawn in the same color. 1031 Computation"
J13-4008,P10-1121,0,0.0482399,"Missing"
J13-4008,W00-1307,0,0.27522,"Missing"
J13-4008,N01-1023,0,\N,Missing
J13-4008,P95-1037,0,\N,Missing
J13-4008,J05-1004,0,\N,Missing
kaeshammer-demberg-2012-german,J93-2004,0,\N,Missing
kaeshammer-demberg-2012-german,W10-1407,0,\N,Missing
kaeshammer-demberg-2012-german,A97-1014,0,\N,Missing
kaeshammer-demberg-2012-german,W00-1307,0,\N,Missing
kaeshammer-demberg-2012-german,E91-1005,0,\N,Missing
kaeshammer-demberg-2012-german,P00-1058,0,\N,Missing
kaeshammer-demberg-2012-german,J05-1004,0,\N,Missing
kaeshammer-demberg-2012-german,P07-1031,0,\N,Missing
kaeshammer-demberg-2012-german,W08-2304,1,\N,Missing
L16-1165,afantenos-etal-2012-empirical,0,0.138689,"automatic discourse relation labellers. Keywords: Annotation of discourse relations (DRs), interoperability of annotation schemes, DRs in spoken and written genres 1. Introduction Over the last decade, research in NLP has widened its scope, moving beyond the sentence level to analysing the discourse structure of a text. This has resulted in the creation of discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the Rhetorical Structure Theory (RST) Treebank (Carlson et al., 2002), and the Annodis corpus (Segmented Discourse Representation Theory, SDRT) (Afantenos et al., 2012), to name a few. However, as of yet, the different frameworks are not inter-operable, nor is there a unified scheme for discourse annotation (but see the proposals by Benamara and Taboada (2015), Chiarcos (2014), Bunt et al. (2012), and Sanders et al. (In preparation)). Most discourse-annotated corpora are based on written rather than spoken text. This point is crucial, as spoken and written texts are produced and processed differently (Cuenca, 2015): Spoken communication is characterised by a high degree of interactivity that requires turn-taking devices for discourse management; sentence len"
L16-1165,S15-1016,0,0.243898,"last decade, research in NLP has widened its scope, moving beyond the sentence level to analysing the discourse structure of a text. This has resulted in the creation of discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the Rhetorical Structure Theory (RST) Treebank (Carlson et al., 2002), and the Annodis corpus (Segmented Discourse Representation Theory, SDRT) (Afantenos et al., 2012), to name a few. However, as of yet, the different frameworks are not inter-operable, nor is there a unified scheme for discourse annotation (but see the proposals by Benamara and Taboada (2015), Chiarcos (2014), Bunt et al. (2012), and Sanders et al. (In preparation)). Most discourse-annotated corpora are based on written rather than spoken text. This point is crucial, as spoken and written texts are produced and processed differently (Cuenca, 2015): Spoken communication is characterised by a high degree of interactivity that requires turn-taking devices for discourse management; sentence length on average is shorter, and the pressure of rapid online processing often leads to disfluent structures. In contrast to written communication, the speaker and the hearer have access to additi"
L16-1165,chiarcos-2014-towards,0,0.106958,"has widened its scope, moving beyond the sentence level to analysing the discourse structure of a text. This has resulted in the creation of discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the Rhetorical Structure Theory (RST) Treebank (Carlson et al., 2002), and the Annodis corpus (Segmented Discourse Representation Theory, SDRT) (Afantenos et al., 2012), to name a few. However, as of yet, the different frameworks are not inter-operable, nor is there a unified scheme for discourse annotation (but see the proposals by Benamara and Taboada (2015), Chiarcos (2014), Bunt et al. (2012), and Sanders et al. (In preparation)). Most discourse-annotated corpora are based on written rather than spoken text. This point is crucial, as spoken and written texts are produced and processed differently (Cuenca, 2015): Spoken communication is characterised by a high degree of interactivity that requires turn-taking devices for discourse management; sentence length on average is shorter, and the pressure of rapid online processing often leads to disfluent structures. In contrast to written communication, the speaker and the hearer have access to additional channels of"
L16-1165,W11-0401,0,0.0241548,"his endeavour. Chiarcos observes that in order to fully map the annotations from the RST and PDTB corpora, structural transformations are necessary, e.g. a conversion of the annotations to dependency DAGs. Benamara and Taboada (2015) propose a taxonomy for mapping discourse relations across two different frameworks, Rhetorical Structure Theory (RST) and Segmented Discourse Representation Theory (SDRT), and use it to map the annotations in three resources (the RST-DT English corpus (Carlson et al., 2002), the SDRT Annodis French corpus (Afantenos et al., 2012), and the RST Spanish Treebank (da Cunha et al., 2011)). The authors claim that their taxonomy is robust across theoretical frameworks and can be applied to corpora from different languages. However, they identify some problems for the mapping, e.g. SDRT does not distinguish between causal and epistemic uses of causal relations, and the C AUSE - RESULT and C ONSEQUENCE relations in the RST-DT corpus are similar and can correspond to either R EASON or R ESULT. Sanders et al. (In preparation) propose to use unifying dimensions to be able to ‘translate’ annotations from one framework to another. In their proposal, they extend the dimensions distingu"
L16-1165,W14-4916,0,0.183831,"Missing"
L16-1165,W15-0210,0,0.0159121,"is the second event. 3. Two different strands of research are relevant to our work. The first one focusses on the unification of different annotation schemes for the annotation of discourse relations, the second is on developing or adapting discourse annotation schemes for spoken language. 3.1. CCR The annotation framework of CCR is based on a cognitive theory of coherence relations, proposed by Sanders et al. (1992). The theory has since been used to annotate various types of discourse, including spoken text, chat fragRelated Work Mapping annotations across frameworks Bunt et al. (2012) and Prasad and Bunt (2015) describe efforts to create an international standard for the annotation of discourse with semantic relations, based on different frameworks for discourse annotation. They define a set of 20 core 1040 discourse relations (Prasad and Bunt, 2015) that they consider to be indispensible for the annotation of DRs, together with clear definitions and examples for each relation. In future work, they plan to provide mappings from their core set to the annotation labels in different DR frameworks such as the PDTB, RST, SDRT and CCR. Such an ambitious enterprise raises questions about the inter-operabil"
L16-1165,prasad-etal-2008-penn,0,0.820417,"her extensions. The new corpus has roughly the size of the CoNLL 2015 Shared Task test set, and we hence hope that it will be a valuable resource for the evaluation of automatic discourse relation labellers. Keywords: Annotation of discourse relations (DRs), interoperability of annotation schemes, DRs in spoken and written genres 1. Introduction Over the last decade, research in NLP has widened its scope, moving beyond the sentence level to analysing the discourse structure of a text. This has resulted in the creation of discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the Rhetorical Structure Theory (RST) Treebank (Carlson et al., 2002), and the Annodis corpus (Segmented Discourse Representation Theory, SDRT) (Afantenos et al., 2012), to name a few. However, as of yet, the different frameworks are not inter-operable, nor is there a unified scheme for discourse annotation (but see the proposals by Benamara and Taboada (2015), Chiarcos (2014), Bunt et al. (2012), and Sanders et al. (In preparation)). Most discourse-annotated corpora are based on written rather than spoken text. This point is crucial, as spoken and written texts are produced and processed di"
L16-1165,tonelli-etal-2010-annotation,0,0.534873,"pping, that was created based on theoretical assumptions, can be considered as verified if a substantial part of the annotations in the two frameworks can be translated into each other, using the mapping. In Section 4., we present such a validation experiment, using the CCR and PDTB 3.0 frameworks, and analyse the results with respect to the predictions made by our mapping. 3.2. Annotating DRs in spoken language While most annotation projects have focussed on annotating discourse relations in written text, there are some studies on annotating discourse-relational devices in the spoken domain. Tonelli et al. (2010) adapt the PDTB 2.0 annotation scheme to make it more suitable for annotating spoken data from the LUNA Corpus, which is a language resource with helpdesk dialogues on the topic of software/hardware troubleshooting. The special properties of spoken conversation caused them to change the PDTB sense hierarchy and include new relations. The most important change is the addition of speechact relations in the sense of Sweetser (1990). Other changes include the G OAL relation, likely motivated by the peculiarities of the task-oriented help-desk dialogues. They also discarded the L IST relation, as i"
L16-1165,P09-1076,0,0.346103,"e PDTB framework to annotate spoken data are Demirs¸ahin and Zeyrek (2014) and Stoyanchev and Bangalore (2015). However, they have not made any further changes to the PDTB framework. 4. Data & Method The data we use in our annotation experiment comes from the SPICE-Ireland corpus (Kallen and Kirk, 2012), a corpus of spoken Irish English containing a variety of different genres, from which we selected broadcast interviews and telephone conversations for discourse relation annotation. The PDTB corpus also includes texts from different genres: essays, summaries, news and letters, as described in Webber (2009). This will allow us to investigate the differences in the use of DRs in different genres from the spoken and written domain. SPICE-Ireland corpus The corpus includes the spoken part of the ICE-Ireland corpus (Kallen and Kirk, 2008), with texts ranging over 15 different discourse settings (e.g. broadcast discussions, broadcast news, classroom discussions, private telephone conversations, or parliamentary debates, amongst others). SPICE-Ireland comprises pragmatic annotations on top of the transcriptions from the ICEIreland corpus, including mark-up for speech-act functions and discourse marker"
L18-1488,J10-4006,0,0.332414,"Missing"
L18-1488,P07-1071,0,0.0296243,"othesis-testing about the ability of the new techniques to acquire the latent information about semantic relationships within the sentence. We contribute towards addressing this in this paper by introducing the Rollenwechsel-English (RW-eng) corpus. RWeng1 is labelled automatically with semantic roles which are then reprocessed heuristically to yield a rich representation of verb-noun relationships and subcategorization frames. RW-eng is based on the full ukWaC corpus (Ferraresi et al., 2008) and the British National Corpus (BNC Consortium, 2007). The semantic role labelling is done by SENNA (Collobert and Weston, 2007; Collobert et al., 2011), a labeller that does not, as most other SRL tools do, rely directly on the syntactic parse of the sentence, allowing it to capture relationships that syntax-based SRL does not, and therefore implicitly permitting some investigation of semantic roles that are not totally confounded with an underlying syntactic theory. In the remainder of this paper, we outline the process by which the sentences are labelled, the output format of the corpus and thus the data and relationships the corpus encodes, and publicly-available tools for corpus generation and access. We also dis"
L18-1488,N15-1003,1,0.906367,"Missing"
L18-1488,kingsbury-palmer-2002-treebank,0,0.198716,"ristic with the above sentence, in which SENNA has assigned ARGM-LOC (AM-LOC in RW-eng’s format) to the“the garden doorway”. Transitively passing through “in”, we find that “doorway” is the head, as it is the first item we encounter. process to detect the head of the span, based on a combination of part of speech tags and the MaltParser dependency trees we received with the sentences. 1. We ran the sentence through the SENNA labeller using the default model supplied with SENNA. The output consists of identified verbal predicates, and labelled spans of text connected with PropBank-style roles (Kingsbury and Palmer, 2002) to each predicate. SENNA can label overlapping stretches of text with different roles for different predicates. In our output, there is always guaranteed to be at least one verbal predicate associated with a SENNA-identified rolelabelled span. Head-detection is an issue where the nominal span is not a singleton word. This is most often the case. We take as heads only items that have nominal or verbal (open-class) POS tags. We used three heuristics for identifying the head: 2. Each predicate verb and its associated role-labelled spans were identified in the SENNA output and grouped together. •"
L18-1488,P16-4024,1,0.848098,"Missing"
L18-1488,D16-1017,1,0.885553,"Missing"
L18-1488,E12-1003,0,0.0267454,"tics. This corpus is of a scale and size suitable for new deep learning approaches to language modelling and distributional semantics, particularly as it pertains to generalized event knowledge. We describe the structure of this corpus, tools for its use, and successful use cases. Keywords: semantic roles, web corpus, labelled data 1. Motivation Semantic role labelling is a comparatively mature task in natural language processing. Typically, it takes the form of supervised classification or language modeling task, although there is more recent work in unsupervised induction of semantic roles (Titov and Klementiev, 2012). These approaches tend to have application goals in traditional areas of text-based natural language processing. A major area of psycholinguistic research is the influence of semantic structure on adult sentence processing. Does the association between “cake” and “cutting” have an effect on processing difficulty of future sentence constituents in a sentence that starts with “The child cut the cake. . . ”, and to what extent is this association influenced by the semantic role-based selectional preferences of the verb (McRae et al., 1998; McRae et al., 2005; Ferretti et al., 2001; Bicknell et a"
L18-1570,P98-1013,0,0.470843,"vision are the two primary human communication channels. They have a lot of complementary information, which was successfully used by prior work. Regneri et al. (2013) showed that video features help to improve the similarity estimation for sentences describing actions. Yatskar et al. (2016a) relied on images to extract common sense knowledge about objects and spatial relations between them. Tandon et al. (2016) used image tags to learn the part-of relation between objects. Yatskar et al. (2016b) proposed a dataset of images and “situations” (a verb with e.g. agent, tool), based on FrameNet (Baker et al., 1998). Prior work has shown that vision can benefit various linguistic similarity tasks (Bruni et al., 2014; Silberer and Lapata, 2014). For such work, it is however a crucial precondition to have a dataset on which system performance can be meaningfully evaluated. Hence, the dataset proposed in this work allows to study the impact of vision for the task of thematic role filling. 3. Dataset In order to be able to measure progress on the task of learning typical locations of a given event (here approximated via a verb), it is necessary to have a dataset that contains a representative set of verbs an"
L18-1570,J10-4006,0,0.606778,"on that is explicitly stated in text. However, tasks that require the ability to make additional common sense inferences are increasingly coming into focus (Levesque et al., 2011; Roemmele et al., 2011; Mostafazadeh et al., 2016). We here propose a dataset which addresses a sub-task towards this goal, i.e. inferring typical locations for a given verb. This task is related to selectional preference tasks, and taps into the kind of inferences humans make when comprehending language. The task consists of predicting a location, given other information from the text (here: the predicate), see also Baroni and Lenci (2010; Sayeed et al. (2015; Tilk et al. (2016). Given the verb to eat, a good thematic role filling model would for instance prefer the location restaurant over office. Previous work has found that locations are a particularly difficult role to predict (Sayeed et al., 2016). This could be due to the fact that location modifiers are often omitted in text when the location is inferable from common sense knowledge, known as reporting bias (Gordon and Van Durme, 2013; Misra et al., 2016). It is hence a logical next step to use multimodal data, and in particular, data from vision. However, there are to"
L18-1570,P14-1023,0,0.0492732,"their study, Ferretti et al. (2001) chose 40 transitive verbs. Many highly frequent verbs like move were not included in their study due to the selection criterion they used (the verb should activate a “distinct prototype”), and hence performance on such verbs was not evaluated. Our proposed dataset includes the 100 most frequent verbs in MS COCO image captions (Lin et al., 2014), and is hence intended to be more representative, and makes it possible to evaluate multimodal systems on the task. Thematic role filling. Many thematic role filling models have been proposed (Baroni and Lenci, 2010; Baroni et al., 2014; Greenberg et al., 2015; Lenci, 2011; Sayeed and Demberg, 2014). Typically, they are evaluated by computing a correlation to human judgments, as motivated by Pad´o et al. (2009). A related line of work focuses on selectional preference estimation (Erk, 2007; Van de Cruys, 2014). Recently, Tilk et al. (2016) proposed a neural network model for thematic role filling. The system distributes probability over the possible role fillers of specific missing roles. Their model learns the interactions between different roles and achieves state-of-the-art performance on multiple datasets. As a baseline"
L18-1570,P06-4018,0,0.0115469,"e captions of the MS COCO (Common Objects in Context) (Lin et al., 2014) dataset (training/validation sets). The MS COCO dataset contains 123,287 images of people, animals and other objects “in context”, i.e. in realistic environments. For each image, five captions are provided (see Figure 1). As verbs are hard to recognize automatically from an image, the availability of captions, together with the situational context of the images, makes this corpus a good choice for studying relations between verbs and locations. We processed the available captions with the Natural Language Toolkit (NLTK) (Bird, 2006) to extract the verbs. Each of the extracted 100 most frequent verbs occurred in the captions of at least 100 images. For each of those images, we ran a state-of-the-art neural network based scene classifier, ResNet (He et al., 2016), trained to distinguish 365 locations of the Places365 dataset (Zhou et al., 2016)1 . The classifier applies a Softmax function to the last fully connected layer representation distributing a probability of 1 over all 365 locations. Figure 1 provides an example MSCOCO image with associated captions, that e.g. mention the verbs to sit and to work, as well as the to"
L18-1570,P07-1071,0,0.0138554,"mly choosing 4 top ranked (from the top 10), 2 middle ranked (from rank 11 to 20), and 4 low ranked (from rank 21 onwards) locations. The ranking of a location for a particular verb was based on increasing average probability (predicted score) of the location across all images relevant for that verb (see Equation 1). Next we selected a set of locations present in language data for the same set of 100 verbs as follows. From a chosen vocabulary of 50,000 most frequent words in the ukWaC corpus (Ferraresi et al., 2008), we selected candidate words, labeled as locations by the SENNA role labeler (Collobert and Weston, 2007), similar to Tilk et al. (2016), and excluded proper names using the Stanford Named Entity Recognizer (Finkel et al., 2005). We manually removed all non-physical locations (e.g. the Web) from the resulting set, which resulted in a total set of 423 candidate physical locations. We again selected for each verb 4 likely, 2 middle ranked and 4 unlikely locations, relying on the probabilities from the best model of Tilk et al. (2016). Overall, this resulted in a total of 2,000 verb/location pairs (100 verbs, with about 10 locations selected based on the visual domain and about 10 locations selected"
L18-1570,P07-1028,0,0.0394336,"evaluated. Our proposed dataset includes the 100 most frequent verbs in MS COCO image captions (Lin et al., 2014), and is hence intended to be more representative, and makes it possible to evaluate multimodal systems on the task. Thematic role filling. Many thematic role filling models have been proposed (Baroni and Lenci, 2010; Baroni et al., 2014; Greenberg et al., 2015; Lenci, 2011; Sayeed and Demberg, 2014). Typically, they are evaluated by computing a correlation to human judgments, as motivated by Pad´o et al. (2009). A related line of work focuses on selectional preference estimation (Erk, 2007; Van de Cruys, 2014). Recently, Tilk et al. (2016) proposed a neural network model for thematic role filling. The system distributes probability over the possible role fillers of specific missing roles. Their model learns the interactions between different roles and achieves state-of-the-art performance on multiple datasets. As a baseline for this dataset, we use their best model, denoted as Language baseline. The Vision baseline system presented in this work leverages visual information from the captioned images as opposed to purely relying on text. 3606 Vision helps language. Language and v"
L18-1570,P05-1045,0,0.0120272,"ns. The ranking of a location for a particular verb was based on increasing average probability (predicted score) of the location across all images relevant for that verb (see Equation 1). Next we selected a set of locations present in language data for the same set of 100 verbs as follows. From a chosen vocabulary of 50,000 most frequent words in the ukWaC corpus (Ferraresi et al., 2008), we selected candidate words, labeled as locations by the SENNA role labeler (Collobert and Weston, 2007), similar to Tilk et al. (2016), and excluded proper names using the Stanford Named Entity Recognizer (Finkel et al., 2005). We manually removed all non-physical locations (e.g. the Web) from the resulting set, which resulted in a total set of 423 candidate physical locations. We again selected for each verb 4 likely, 2 middle ranked and 4 unlikely locations, relying on the probabilities from the best model of Tilk et al. (2016). Overall, this resulted in a total of 2,000 verb/location pairs (100 verbs, with about 10 locations selected based on the visual domain and about 10 locations selected based on the language system). Some of the most frequent verbs from MS COCO, which were included in our proposed dataset,"
L18-1570,N15-1003,1,0.921386,"et al. (2001) chose 40 transitive verbs. Many highly frequent verbs like move were not included in their study due to the selection criterion they used (the verb should activate a “distinct prototype”), and hence performance on such verbs was not evaluated. Our proposed dataset includes the 100 most frequent verbs in MS COCO image captions (Lin et al., 2014), and is hence intended to be more representative, and makes it possible to evaluate multimodal systems on the task. Thematic role filling. Many thematic role filling models have been proposed (Baroni and Lenci, 2010; Baroni et al., 2014; Greenberg et al., 2015; Lenci, 2011; Sayeed and Demberg, 2014). Typically, they are evaluated by computing a correlation to human judgments, as motivated by Pad´o et al. (2009). A related line of work focuses on selectional preference estimation (Erk, 2007; Van de Cruys, 2014). Recently, Tilk et al. (2016) proposed a neural network model for thematic role filling. The system distributes probability over the possible role fillers of specific missing roles. Their model learns the interactions between different roles and achieves state-of-the-art performance on multiple datasets. As a baseline for this dataset, we use"
L18-1570,W11-0607,0,0.449896,"transitive verbs. Many highly frequent verbs like move were not included in their study due to the selection criterion they used (the verb should activate a “distinct prototype”), and hence performance on such verbs was not evaluated. Our proposed dataset includes the 100 most frequent verbs in MS COCO image captions (Lin et al., 2014), and is hence intended to be more representative, and makes it possible to evaluate multimodal systems on the task. Thematic role filling. Many thematic role filling models have been proposed (Baroni and Lenci, 2010; Baroni et al., 2014; Greenberg et al., 2015; Lenci, 2011; Sayeed and Demberg, 2014). Typically, they are evaluated by computing a correlation to human judgments, as motivated by Pad´o et al. (2009). A related line of work focuses on selectional preference estimation (Erk, 2007; Van de Cruys, 2014). Recently, Tilk et al. (2016) proposed a neural network model for thematic role filling. The system distributes probability over the possible role fillers of specific missing roles. Their model learns the interactions between different roles and achieves state-of-the-art performance on multiple datasets. As a baseline for this dataset, we use their best m"
L18-1570,N16-1098,0,0.0589831,"ural networks based thematic fit model learned from linguistic data, a model estimating typical locations based on the MSCOCO dataset and a simple combination of the systems. Keywords: human judgments, thematic fit, vision 1. Introduction based and a simple multimodal model for a common sense inference task using this dataset. Most of automatic language understanding today is based on information that is explicitly stated in text. However, tasks that require the ability to make additional common sense inferences are increasingly coming into focus (Levesque et al., 2011; Roemmele et al., 2011; Mostafazadeh et al., 2016). We here propose a dataset which addresses a sub-task towards this goal, i.e. inferring typical locations for a given verb. This task is related to selectional preference tasks, and taps into the kind of inferences humans make when comprehending language. The task consists of predicting a location, given other information from the text (here: the predicate), see also Baroni and Lenci (2010; Sayeed et al. (2015; Tilk et al. (2016). Given the verb to eat, a good thematic role filling model would for instance prefer the location restaurant over office. Previous work has found that locations are"
L18-1570,N16-3012,1,0.807282,"Missing"
L18-1570,Q13-1003,1,0.751401,"over the possible role fillers of specific missing roles. Their model learns the interactions between different roles and achieves state-of-the-art performance on multiple datasets. As a baseline for this dataset, we use their best model, denoted as Language baseline. The Vision baseline system presented in this work leverages visual information from the captioned images as opposed to purely relying on text. 3606 Vision helps language. Language and vision are the two primary human communication channels. They have a lot of complementary information, which was successfully used by prior work. Regneri et al. (2013) showed that video features help to improve the similarity estimation for sentences describing actions. Yatskar et al. (2016a) relied on images to extract common sense knowledge about objects and spatial relations between them. Tandon et al. (2016) used image tags to learn the part-of relation between objects. Yatskar et al. (2016b) proposed a dataset of images and “situations” (a verb with e.g. agent, tool), based on FrameNet (Baker et al., 1998). Prior work has shown that vision can benefit various linguistic similarity tasks (Bruni et al., 2014; Silberer and Lapata, 2014). For such work, it"
L18-1570,W16-2518,1,0.868298,"s a sub-task towards this goal, i.e. inferring typical locations for a given verb. This task is related to selectional preference tasks, and taps into the kind of inferences humans make when comprehending language. The task consists of predicting a location, given other information from the text (here: the predicate), see also Baroni and Lenci (2010; Sayeed et al. (2015; Tilk et al. (2016). Given the verb to eat, a good thematic role filling model would for instance prefer the location restaurant over office. Previous work has found that locations are a particularly difficult role to predict (Sayeed et al., 2016). This could be due to the fact that location modifiers are often omitted in text when the location is inferable from common sense knowledge, known as reporting bias (Gordon and Van Durme, 2013; Misra et al., 2016). It is hence a logical next step to use multimodal data, and in particular, data from vision. However, there are to date no datasets which can be used to evaluate the contribution of a vision model, as the only existing location fit dataset is very small (Ferretti et al., 2001), comprising only 277 verb/location pairs, and includes many rare verbs. We here present a new dataset base"
L18-1570,P14-1068,0,0.0346177,"lly used by prior work. Regneri et al. (2013) showed that video features help to improve the similarity estimation for sentences describing actions. Yatskar et al. (2016a) relied on images to extract common sense knowledge about objects and spatial relations between them. Tandon et al. (2016) used image tags to learn the part-of relation between objects. Yatskar et al. (2016b) proposed a dataset of images and “situations” (a verb with e.g. agent, tool), based on FrameNet (Baker et al., 1998). Prior work has shown that vision can benefit various linguistic similarity tasks (Bruni et al., 2014; Silberer and Lapata, 2014). For such work, it is however a crucial precondition to have a dataset on which system performance can be meaningfully evaluated. Hence, the dataset proposed in this work allows to study the impact of vision for the task of thematic role filling. 3. Dataset In order to be able to measure progress on the task of learning typical locations of a given event (here approximated via a verb), it is necessary to have a dataset that contains a representative set of verbs and locations together with judgments of how typical these locations are. In order to enable evaluations that correlate automatic fi"
L18-1570,D16-1017,1,0.90534,", tasks that require the ability to make additional common sense inferences are increasingly coming into focus (Levesque et al., 2011; Roemmele et al., 2011; Mostafazadeh et al., 2016). We here propose a dataset which addresses a sub-task towards this goal, i.e. inferring typical locations for a given verb. This task is related to selectional preference tasks, and taps into the kind of inferences humans make when comprehending language. The task consists of predicting a location, given other information from the text (here: the predicate), see also Baroni and Lenci (2010; Sayeed et al. (2015; Tilk et al. (2016). Given the verb to eat, a good thematic role filling model would for instance prefer the location restaurant over office. Previous work has found that locations are a particularly difficult role to predict (Sayeed et al., 2016). This could be due to the fact that location modifiers are often omitted in text when the location is inferable from common sense knowledge, known as reporting bias (Gordon and Van Durme, 2013; Misra et al., 2016). It is hence a logical next step to use multimodal data, and in particular, data from vision. However, there are to date no datasets which can be used to eva"
L18-1570,D14-1004,0,0.0549639,"Missing"
L18-1570,E09-1094,0,0.0206702,"l and linguistic information for learning common sense knowledge about locations. It builds on MS COCO (Lin et al., 2014), a dataset of images with captions; locations for these pictures were labeled using a scene classifier trained to distinguish 365 locations of the Places365 dataset (Zhou et al., 2016). Along with the dataset, we provide performance baselines for a language-based, a vision2. Related work Datasets. Datasets with human ratings on a scale of 1 (least common) to 7 (most common) for agent and patient roles were made available as part of McRae et al. (1997), Pad´o et al. (2009), Vandekerckhove et al. (2009). Ferretti et al. (2001) created a dataset of 277 location ratings (FerrettiLoc) using questions like “How common is it for someone to eat at the following locations”? 40 participants provided ratings on a 7 point scale, e.g.: eat/restaurant: 7. For their study, Ferretti et al. (2001) chose 40 transitive verbs. Many highly frequent verbs like move were not included in their study due to the selection criterion they used (the verb should activate a “distinct prototype”), and hence performance on such verbs was not evaluated. Our proposed dataset includes the 100 most frequent verbs in MS COCO i"
L18-1570,N16-1023,0,0.140266,"hieves state-of-the-art performance on multiple datasets. As a baseline for this dataset, we use their best model, denoted as Language baseline. The Vision baseline system presented in this work leverages visual information from the captioned images as opposed to purely relying on text. 3606 Vision helps language. Language and vision are the two primary human communication channels. They have a lot of complementary information, which was successfully used by prior work. Regneri et al. (2013) showed that video features help to improve the similarity estimation for sentences describing actions. Yatskar et al. (2016a) relied on images to extract common sense knowledge about objects and spatial relations between them. Tandon et al. (2016) used image tags to learn the part-of relation between objects. Yatskar et al. (2016b) proposed a dataset of images and “situations” (a verb with e.g. agent, tool), based on FrameNet (Baker et al., 1998). Prior work has shown that vision can benefit various linguistic similarity tasks (Bruni et al., 2014; Silberer and Lapata, 2014). For such work, it is however a crucial precondition to have a dataset on which system performance can be meaningfully evaluated. Hence, the d"
N15-1003,J10-4006,0,0.12475,"proving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering Clayton Greenberg, Asad Sayeed and Vera Demberg Computational Linguistics and Phonetics / M2 CI Cluster of Excellence Saarland University 66123 Saarbr¨ucken, Germany {claytong,asayeed,vera}@coli.uni-saarland.de Abstract A common method for estimating the thematic fit between a verb and a proposed role filler involves computing a centroid, or vector average, over the most typical role fillers for that verb, and then calculating the cosine similarity between this centroid and the proposed role filler (Baroni and Lenci, 2010; Blacoe and Lapata, 2012; Erk, 2012). For instance, we use the cosine of the angle between the banana vector and a vector average of the 20 nouns that, according to training data, are most likely to be mashed as a score for how well banana fits as the patient of mash. Hopefully, the banana vector will be closer to the centroid than milk, so banana will have a higher cosine similarity to the centroid, and thus a higher thematic fit score, than milk. Most recent unsupervised methods in vector space semantics for assessing thematic fit (e.g. Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg,"
N15-1003,D12-1050,0,0.0296491,"tor-space thematic fit evaluation via role-filler prototype clustering Clayton Greenberg, Asad Sayeed and Vera Demberg Computational Linguistics and Phonetics / M2 CI Cluster of Excellence Saarland University 66123 Saarbr¨ucken, Germany {claytong,asayeed,vera}@coli.uni-saarland.de Abstract A common method for estimating the thematic fit between a verb and a proposed role filler involves computing a centroid, or vector average, over the most typical role fillers for that verb, and then calculating the cosine similarity between this centroid and the proposed role filler (Baroni and Lenci, 2010; Blacoe and Lapata, 2012; Erk, 2012). For instance, we use the cosine of the angle between the banana vector and a vector average of the 20 nouns that, according to training data, are most likely to be mashed as a score for how well banana fits as the patient of mash. Hopefully, the banana vector will be closer to the centroid than milk, so banana will have a higher cosine similarity to the centroid, and thus a higher thematic fit score, than milk. Most recent unsupervised methods in vector space semantics for assessing thematic fit (e.g. Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014) create prototypica"
N15-1003,P07-1071,0,0.575128,"Missing"
N15-1003,P07-1028,0,0.236146,"d and the proposed role filler (Baroni and Lenci, 2010; Blacoe and Lapata, 2012; Erk, 2012). For instance, we use the cosine of the angle between the banana vector and a vector average of the 20 nouns that, according to training data, are most likely to be mashed as a score for how well banana fits as the patient of mash. Hopefully, the banana vector will be closer to the centroid than milk, so banana will have a higher cosine similarity to the centroid, and thus a higher thematic fit score, than milk. Most recent unsupervised methods in vector space semantics for assessing thematic fit (e.g. Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014) create prototypical rolefillers without performing word sense disambiguation. This leads to a kind of sparsity problem: candidate role-fillers for different senses of the verb end up being measured by the same “yardstick”, the single prototypical role-filler. In this work, we use three different feature spaces to construct robust unsupervised models of distributional semantics. We show that correlation with human judgements on thematic fit estimates can be improved consistently by clustering typical role-fillers and then calculating similarit"
N15-1003,J10-4007,0,0.283543,"Missing"
N15-1003,P14-3010,1,0.825829,"instance. This is quite intuitive because if serve and cake occur next to each other, the chance that a non-food sense of the word serve was intended would be extremely small, in fact much smaller than a corpuswide distribution would predict. These systems have been effective at improving correlation with human judgements for a verb-object composition model, i.e. approximating a vector for serve cake given a vector for serve and a vector for cake (Kartsaklis et al., 2014), and also reducing noise in similarity scores for a nearest neighbor-based prepositional phrase attachment disambiguator (Greenberg, 2014). It remains a choice of the system whether to store explicit senses separately, and relatedly, whether to consult a knowledge base for the number of senses for each word, or even for meaning representations of those senses. Using a task-general knowledge base, in addition to the inherent cost of building one, is not particularly suited for our task because the items to be disambiguated are verb-role pairs, as opposed to just verbs, and usually such knowledge bases do not handle individual thematic roles separately. For instance, it may be optimal to analyze serve as having three senses with r"
N15-1003,W09-0205,0,0.0621099,"Missing"
N15-1003,P12-1092,0,0.0574599,"mantic categories to the slots of a verb subcategorization frame harks back to work by Resnik (1996) and Rooth et al. (1999). Resnik’s work presupposes predefined noun classes obtained from WordNet. Rooth et al. induced latent role-filler classes via expectation maximization. Erk et al. (2010) found that neither are good models of thematic fit. Pad´o et al. (2009) provided thematic fit scores that take into account verb class using a supervised model. In the vector space context, inducing different vectors for multiple verb senses has been investigated recently by Reisinger and Mooney (2010), Huang et al. (2012), and Neelakantan et al. (2014), although these were not focused on rolefillers for verbs. Our contribution is to make use of a large-scale, unsupervised vector space model to provide thematic fit scores after inducing implicit verb sense classes relative to thematic role. 3 Methods We begin our discussion of sense disambiguation for thematic fit with the following insight: the baseline (Centroid ) method takes as input a set of typical role-fillers, the highest-ranked ones according to the DM, and returns a single prototype vector. However, if we allow the system to return a set of prototype"
N15-1003,P14-2035,0,0.0217448,"5 the base vector for the potentially ambiguous word is contextualized, as in scaled element-wise, by the vectors of the neighboring words for that instance. This is quite intuitive because if serve and cake occur next to each other, the chance that a non-food sense of the word serve was intended would be extremely small, in fact much smaller than a corpuswide distribution would predict. These systems have been effective at improving correlation with human judgements for a verb-object composition model, i.e. approximating a vector for serve cake given a vector for serve and a vector for cake (Kartsaklis et al., 2014), and also reducing noise in similarity scores for a nearest neighbor-based prepositional phrase attachment disambiguator (Greenberg, 2014). It remains a choice of the system whether to store explicit senses separately, and relatedly, whether to consult a knowledge base for the number of senses for each word, or even for meaning representations of those senses. Using a task-general knowledge base, in addition to the inherent cost of building one, is not particularly suited for our task because the items to be disambiguated are verb-role pairs, as opposed to just verbs, and usually such knowled"
N15-1003,D14-1113,0,0.0234057,"slots of a verb subcategorization frame harks back to work by Resnik (1996) and Rooth et al. (1999). Resnik’s work presupposes predefined noun classes obtained from WordNet. Rooth et al. induced latent role-filler classes via expectation maximization. Erk et al. (2010) found that neither are good models of thematic fit. Pad´o et al. (2009) provided thematic fit scores that take into account verb class using a supervised model. In the vector space context, inducing different vectors for multiple verb senses has been investigated recently by Reisinger and Mooney (2010), Huang et al. (2012), and Neelakantan et al. (2014), although these were not focused on rolefillers for verbs. Our contribution is to make use of a large-scale, unsupervised vector space model to provide thematic fit scores after inducing implicit verb sense classes relative to thematic role. 3 Methods We begin our discussion of sense disambiguation for thematic fit with the following insight: the baseline (Centroid ) method takes as input a set of typical role-fillers, the highest-ranked ones according to the DM, and returns a single prototype vector. However, if we allow the system to return a set of prototype vectors, then the framework gai"
N15-1003,N10-1013,0,0.03892,"spect to agents. Assigning semantic categories to the slots of a verb subcategorization frame harks back to work by Resnik (1996) and Rooth et al. (1999). Resnik’s work presupposes predefined noun classes obtained from WordNet. Rooth et al. induced latent role-filler classes via expectation maximization. Erk et al. (2010) found that neither are good models of thematic fit. Pad´o et al. (2009) provided thematic fit scores that take into account verb class using a supervised model. In the vector space context, inducing different vectors for multiple verb senses has been investigated recently by Reisinger and Mooney (2010), Huang et al. (2012), and Neelakantan et al. (2014), although these were not focused on rolefillers for verbs. Our contribution is to make use of a large-scale, unsupervised vector space model to provide thematic fit scores after inducing implicit verb sense classes relative to thematic role. 3 Methods We begin our discussion of sense disambiguation for thematic fit with the following insight: the baseline (Centroid ) method takes as input a set of typical role-fillers, the highest-ranked ones according to the DM, and returns a single prototype vector. However, if we allow the system to retur"
N15-1003,P99-1014,0,0.120674,"e senses. Using a task-general knowledge base, in addition to the inherent cost of building one, is not particularly suited for our task because the items to be disambiguated are verb-role pairs, as opposed to just verbs, and usually such knowledge bases do not handle individual thematic roles separately. For instance, it may be optimal to analyze serve as having three senses with respect to instruments, two senses with respect to patients, and one sense with respect to agents. Assigning semantic categories to the slots of a verb subcategorization frame harks back to work by Resnik (1996) and Rooth et al. (1999). Resnik’s work presupposes predefined noun classes obtained from WordNet. Rooth et al. induced latent role-filler classes via expectation maximization. Erk et al. (2010) found that neither are good models of thematic fit. Pad´o et al. (2009) provided thematic fit scores that take into account verb class using a supervised model. In the vector space context, inducing different vectors for multiple verb senses has been investigated recently by Reisinger and Mooney (2010), Huang et al. (2012), and Neelakantan et al. (2014), although these were not focused on rolefillers for verbs. Our contributi"
N15-1003,I11-1127,0,0.0522496,"Missing"
N15-1003,E09-1094,0,0.2118,"ecording distributional information about linguistic co-occurrence. Distributional Memory (DM) records frequency information about links between words in a sentence as a third order tensor, in which words or lemmata are represented as two of the tensor axes and the syntactic or semantic link between them is the third axis. The following corpora were used to construct the Baroni and Lenci (2010) version of DM: Table 1: Sample of judgements from Pad´o (2007). In order to model thematic roles, we use the insight that thematic fit correlates with human plausibility judgements (Pad´o et al., 2009; Vandekerckhove et al., 2009). Therefore, we can use datasets of human plausibility judgements to evaluate computational thematic fit estimates. One such dataset by Pad´o (2007) includes 18 verbs with up to 12 candidate nominal arguments and totals 414 verb-nounrole triples. The words were chosen based on their frequencies in the Penn Treebank and FrameNet. Human participants were asked to rate the appropriateness of given nouns as agents and as patients for given verbs on a scale from 1 to 7. The judgements were then averaged. We provide a small sample of these judgements in Table 1. We use three other datasets as well."
N16-1067,P08-1090,0,0.833207,"hman and Ng, 2012), as well as for psycholinguistic models of human language processing, which need to represent event knowledge to model human expectations (Zwaan et al., 1995; Sch¨utz-Bosbach and Prinz, 2007) of upcoming referents and utterances. One recent line of research has tried to learn scripts in an unsupervised way from large text collections. The core idea in Chambers and Jurafsky Pichotta and Mooney (2014) (P&M) have demonstrated that using richer event representations containing multiple arguments improves prediciton accuracy on the narrative cloze task over the simpler models by Chambers and Jurafsky (2008). While they represent a script event as a pair of a verb and a dependency (an example of an event chain would be &lt;call,obj&gt;; &lt;bring,subj&gt;; &lt;take,subj&gt;), which is problematic for weak verbs and verb ambiguity, P&M represent events using a multi-argument event representation, e.g., call(guest,waiter,*); bring(waiter,menu,*); take(waiter,order,*). This richer event representation however still has some shortcomings. As the representation is based on coreference chains, the model runs into difficulties for entities that are in a chain of length one. Entities in a chain are internally mapped onto"
N16-1067,P09-1068,0,0.36636,"Missing"
N16-1067,E12-1034,0,0.277658,"Missing"
N16-1067,P14-5010,0,0.00383617,"and automaticallyannotated script participants vs. gold participant annotation. We evaluate our approach on the Dinners from Hell corpus (Rudinger et al., 2015), as well as the newly available InScript corpus (Modi et al., 2016). Following earlier work, we evaluate the quality of script models using the so-called narrative cloze task, where the model has to predict a missing event given surrounding events in the text. 2 2.1 Methods Participant-labeled Events data, we first extract syntactic relations between verbs and their arguments as well as coreference information using Stanford CoreNLP (Manning et al. (2014)). We then use the max-hypernym heuristic described in Kampmann et al. (2015) to label the arguments with participant roles. This approach assigns to token w the participant label with the highest hyponym-similarity score between the wordnetsynsets associated with the label and one of the synsets of any word present in the coreference chain connected to w. Where an argument slot of the event is not filled syntactically or the argument is not a participant of the script, a dummy participant O serves as a placeholder to indicate the absence of a labeled argument. Every extracted event that conta"
N16-1067,L16-1555,0,0.726748,"WordNet (telling us e.g., that a steak is a kind of food). In this paper, we extend the existing approach by P&M and demonstrate that explicitly labelling participants (instead of using coreference chains) leads to improved event prediction performance. We furthermore provide a systematic evaluation of the effect of automatically-annotated coreference chains vs. gold coreference chains, and automaticallyannotated script participants vs. gold participant annotation. We evaluate our approach on the Dinners from Hell corpus (Rudinger et al., 2015), as well as the newly available InScript corpus (Modi et al., 2016). Following earlier work, we evaluate the quality of script models using the so-called narrative cloze task, where the model has to predict a missing event given surrounding events in the text. 2 2.1 Methods Participant-labeled Events data, we first extract syntactic relations between verbs and their arguments as well as coreference information using Stanford CoreNLP (Manning et al. (2014)). We then use the max-hypernym heuristic described in Kampmann et al. (2015) to label the arguments with participant roles. This approach assigns to token w the participant label with the highest hyponym-sim"
N16-1067,E14-1024,0,0.566707,"events, e.g., menu, waiter, food, guest. Script knowledge is a form of structured world knowledge that is useful in NLP applications for natural language understanding tasks (e.g., ambiguity resolution Rahman and Ng, 2012), as well as for psycholinguistic models of human language processing, which need to represent event knowledge to model human expectations (Zwaan et al., 1995; Sch¨utz-Bosbach and Prinz, 2007) of upcoming referents and utterances. One recent line of research has tried to learn scripts in an unsupervised way from large text collections. The core idea in Chambers and Jurafsky Pichotta and Mooney (2014) (P&M) have demonstrated that using richer event representations containing multiple arguments improves prediciton accuracy on the narrative cloze task over the simpler models by Chambers and Jurafsky (2008). While they represent a script event as a pair of a verb and a dependency (an example of an event chain would be &lt;call,obj&gt;; &lt;bring,subj&gt;; &lt;take,subj&gt;), which is problematic for weak verbs and verb ambiguity, P&M represent events using a multi-argument event representation, e.g., call(guest,waiter,*); bring(waiter,menu,*); take(waiter,order,*). This richer event representation however stil"
N16-1067,D12-1071,0,0.0309061,"InScript corpus, that this knowledge helps us to significantly improve prediction performance on the narrative cloze task. 1 Introduction Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Script knowledge is a form of structured world knowledge that is useful in NLP applications for natural language understanding tasks (e.g., ambiguity resolution Rahman and Ng, 2012), as well as for psycholinguistic models of human language processing, which need to represent event knowledge to model human expectations (Zwaan et al., 1995; Sch¨utz-Bosbach and Prinz, 2007) of upcoming referents and utterances. One recent line of research has tried to learn scripts in an unsupervised way from large text collections. The core idea in Chambers and Jurafsky Pichotta and Mooney (2014) (P&M) have demonstrated that using richer event representations containing multiple arguments improves prediciton accuracy on the narrative cloze task over the simpler models by Chambers and Juraf"
N16-1067,P10-1100,0,0.568935,"Missing"
N16-1067,S15-1024,1,0.827316,"Missing"
N16-3012,N12-1085,1,0.848367,"Missing"
N16-3012,W11-1510,1,0.798584,"into phases in order to narrow down the discourse type, with LingoTurk presenting a subsidiary tableau of connective phrases depending on the result of the first tableau. The selection of connective tableaux is controlled via the item entry interface on the administrative side of LingoTurk. Data analysis for this task is still on-going. The advantage of a drag-and-drop paradigm is that it requires the subject to make an explicit choice but also to use a little bit of effort in doing so. This reduces the bias that might be introduced by the least-effort of choosing the first or the last item (Sayeed et al., 2011). Script alignment by connector drawing Wanzare et al. (2016) use the LingoTurk system to present a task involving the alignment of collected narratives to prepared scripts (e.g., for baking a cake) for an on-going project that is investigating the psycholinguistic aspects of script knowledge as 60 well as developing script corpora. In this paradigm, steps of the narrative (an account of an action collected from subjects during previous research) are presented as tiles in a column on the left side of the window, while steps of the standardized script are presented on the right side. Subjects d"
N16-3012,P11-1122,0,0.0808058,"Missing"
N16-3012,L16-1556,0,\N,Missing
P07-1013,P96-1041,0,0.118877,"Missing"
P07-1013,P07-1116,1,0.814641,"pect to CELEX manual annotation. Unsupervised Morphological Systems Most attractive among automatic systems are methods that use unsupervised learning, because these require neither an expert linguist to build large rule-sets and lexica nor large manually annotated word lists, but only large amounts of tokenized text, which can be acquired e.g. from the internet. Unsupervised methods are in principle6 languageindependent, and can therefore easily be applied to other languages. We compared four different state-of-the-art unsupervised systems for morphological decomposition (cf. (Demberg, 2006; Demberg, 2007)). The algorithms were trained on a German newspaper corpus (taz), containing about 240 million words. The same algorithms have previously been shown to help a speech recognition task (Kurimo et al., 2006). 5 Experimental Evaluations 5.1 Training Set and Test Set Design The German corpus used in these experiments is CELEX (German Linguistic User Guide, 1995). CELEX contains a phonemic representation of each 4 Eloquent Technology, Inc. (ETI) TTS system. http://www.mindspring.com/˜ssshp/ssshp_cd/ ss_eloq.htm 5 The lexicon used by SMOR, IMSLEX, contains morphologically complex entries, which lead"
P07-1013,P01-1053,0,0.171481,"Missing"
P07-1013,C86-1063,0,0.618043,"weight (such as German or Dutch), it is important to know where exactly the syllable boundaries are in order to correctly calculate syllable weight. For German, (M¨uller, 2001) show that information about stress assignment and the position of a syllable within a word improve g2p conversion. 1.2 Morphological Preprocessing It has been argued that using morphological information is important for languages where morphology has an important influence on pronunciation, syllabification and word stress such as German, Dutch, Swedish or, to a smaller extent, also English (Sproat, 1996; M¨obius, 2001; Pounder and Kommenda, 1986; Black et al., 1998; Taylor, 2005). Unfortunately, these papers do not quantify the contribution of morphological preprocessing in the task. Important questions when considering the integration of a morphological component into a speech 1 This issue is controversial among linguists; for an overview see (Jessen, 1998). Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96–103, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics synthesis system are 1) How large are the improvements to be gained from morphological prepro"
P07-1013,schmid-etal-2004-smor,1,0.837765,"Missing"
P07-1013,J00-2003,0,\N,Missing
P07-1116,W98-1239,0,0.112059,"Missing"
P07-1116,P07-1013,1,0.806899,"to find boundaries between stems and suffixes. The standard evaluation procedure does not differentiate between the types of mistakes made. Finally, only evaluation on a task can provide information as to whether high precision or high recall is more important, therefore, the decision as to which version of the algorithm should be chosen can only be taken given a specific task. For these reasons we decided to evaluate the segmentation from the new versions of the RePortS algorithm on a German grapheme-to-phoneme (g2p) conversion task. The evaluation on this task is motivated by the fact that (Demberg, 2007) showed that good-quality morphological preprocessing can improve g2p conversion results. We here compare the effect of using our system’s segmentations to a range of different morphological segmentations from other systems. We ran each of the rule-based systems (ETI, SMOR-disamb1, SMOR-disamb2) and the unsupervised algorithms (original RePortS, Bernhard, Morfessor 1.0, Bordag) on the CELEX data set and retrained our decision tree (an implementation based on (Lucassen and Mercer, 1984)) on the different morphological segmentations. Table 4 shows the F-score of the different systems when evalua"
P07-1116,W99-0904,0,0.147339,"phoneme conversion task. We finally summarize our results in section 6. 2 Previous Work tie the orthographic form of the word to the morphemes. They are thus not well-suited for coping with stem changes or modifications at the edges of morphemes. Only very few approaches have addressed word internal variations (Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002). A popular and effective approach for detecting inflectional paradigms and filter affix lists is to cluster together affixes or regular transformational patterns that occur with the same stem (Monson et al., 2004; Goldsmith, 2001; Gaussier, 1999; Schone and Jurafsky, 2000; Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002; Jacquemin, 1997). We draw from this idea of clustering in order to detect orthographic variants of stems; see Section 4.3. A few approaches also take into account syntactic and semantic information from the context the word occurs (Schone and Jurafsky, 2000; Bordag, 2006; Yarowski and Wicentowski, 2000; Jacquemin, 1997). Exploiting semantic and syntactic information is very attractive because it adds an additional dimension, but these approaches have to cope with more severe data sparseness issues than approac"
P07-1116,J01-2001,0,0.470662,"on a grapheme-to-phoneme conversion task. We finally summarize our results in section 6. 2 Previous Work tie the orthographic form of the word to the morphemes. They are thus not well-suited for coping with stem changes or modifications at the edges of morphemes. Only very few approaches have addressed word internal variations (Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002). A popular and effective approach for detecting inflectional paradigms and filter affix lists is to cluster together affixes or regular transformational patterns that occur with the same stem (Monson et al., 2004; Goldsmith, 2001; Gaussier, 1999; Schone and Jurafsky, 2000; Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002; Jacquemin, 1997). We draw from this idea of clustering in order to detect orthographic variants of stems; see Section 4.3. A few approaches also take into account syntactic and semantic information from the context the word occurs (Schone and Jurafsky, 2000; Bordag, 2006; Yarowski and Wicentowski, 2000; Jacquemin, 1997). Exploiting semantic and syntactic information is very attractive because it adds an additional dimension, but these approaches have to cope with more severe data sparseness iss"
P07-1116,H05-1085,0,0.0177464,"scribes an algorithm that draws from previous approaches and combines them into a simple model for morphological segmentation that outperforms other approaches on English and German, and also yields good results on agglutinative languages such as Finnish and Turkish. We also propose a method for detecting variation within stems in an unsupervised fashion. The segmentation quality reached with the new algorithm is good enough to improve grapheme-to-phoneme conversion. 1 Introduction Morphological segmentation has been shown to be beneficial to a number of NLP tasks such as machine translation (Goldwater and McClosky, 2005), speech recognition (Kurimo et al., 2006), information retrieval (Monz and de Rijke, 2002) and question answering. Segmenting a word into meaningbearing units is particularly interesting for morphologically complex languages where words can be composed of several morphemes through inflection, derivation and composition. Data sparseness for such languages can be significantly decreased when 920 words are decomposed morphologically. There exist a number of rule-based morphological segmentation systems for a range of languages. However, expert knowledge and labour are expensive, and the analyzer"
P07-1116,W04-0107,0,0.109939,"luates the algorithm on a grapheme-to-phoneme conversion task. We finally summarize our results in section 6. 2 Previous Work tie the orthographic form of the word to the morphemes. They are thus not well-suited for coping with stem changes or modifications at the edges of morphemes. Only very few approaches have addressed word internal variations (Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002). A popular and effective approach for detecting inflectional paradigms and filter affix lists is to cluster together affixes or regular transformational patterns that occur with the same stem (Monson et al., 2004; Goldsmith, 2001; Gaussier, 1999; Schone and Jurafsky, 2000; Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002; Jacquemin, 1997). We draw from this idea of clustering in order to detect orthographic variants of stems; see Section 4.3. A few approaches also take into account syntactic and semantic information from the context the word occurs (Schone and Jurafsky, 2000; Bordag, 2006; Yarowski and Wicentowski, 2000; Jacquemin, 1997). Exploiting semantic and syntactic information is very attractive because it adds an additional dimension, but these approaches have to cope with more severe da"
P07-1116,W02-0604,0,0.176135,"rtS algorithm in section 3 and explain the modifications to the original algorithm in section 4. Section 5 compares results for different languages, quantifies the gains from the modifications on the algorithm and evaluates the algorithm on a grapheme-to-phoneme conversion task. We finally summarize our results in section 6. 2 Previous Work tie the orthographic form of the word to the morphemes. They are thus not well-suited for coping with stem changes or modifications at the edges of morphemes. Only very few approaches have addressed word internal variations (Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002). A popular and effective approach for detecting inflectional paradigms and filter affix lists is to cluster together affixes or regular transformational patterns that occur with the same stem (Monson et al., 2004; Goldsmith, 2001; Gaussier, 1999; Schone and Jurafsky, 2000; Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002; Jacquemin, 1997). We draw from this idea of clustering in order to detect orthographic variants of stems; see Section 4.3. A few approaches also take into account syntactic and semantic information from the context the word occurs (Schone and Jurafsky, 2000; Bordag, 20"
P07-1116,schmid-etal-2004-smor,0,0.0630491,"Missing"
P07-1116,W00-0712,0,0.1055,"on task. We finally summarize our results in section 6. 2 Previous Work tie the orthographic form of the word to the morphemes. They are thus not well-suited for coping with stem changes or modifications at the edges of morphemes. Only very few approaches have addressed word internal variations (Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002). A popular and effective approach for detecting inflectional paradigms and filter affix lists is to cluster together affixes or regular transformational patterns that occur with the same stem (Monson et al., 2004; Goldsmith, 2001; Gaussier, 1999; Schone and Jurafsky, 2000; Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002; Jacquemin, 1997). We draw from this idea of clustering in order to detect orthographic variants of stems; see Section 4.3. A few approaches also take into account syntactic and semantic information from the context the word occurs (Schone and Jurafsky, 2000; Bordag, 2006; Yarowski and Wicentowski, 2000; Jacquemin, 1997). Exploiting semantic and syntactic information is very attractive because it adds an additional dimension, but these approaches have to cope with more severe data sparseness issues than approaches that emphasize word-int"
P07-1116,P00-1027,0,0.885234,"ne of the main steps of the RePortS algorithm in section 3 and explain the modifications to the original algorithm in section 4. Section 5 compares results for different languages, quantifies the gains from the modifications on the algorithm and evaluates the algorithm on a grapheme-to-phoneme conversion task. We finally summarize our results in section 6. 2 Previous Work tie the orthographic form of the word to the morphemes. They are thus not well-suited for coping with stem changes or modifications at the edges of morphemes. Only very few approaches have addressed word internal variations (Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002). A popular and effective approach for detecting inflectional paradigms and filter affix lists is to cluster together affixes or regular transformational patterns that occur with the same stem (Monson et al., 2004; Goldsmith, 2001; Gaussier, 1999; Schone and Jurafsky, 2000; Yarowski and Wicentowski, 2000; Neuvel and Fulop, 2002; Jacquemin, 1997). We draw from this idea of clustering in order to detect orthographic variants of stems; see Section 4.3. A few approaches also take into account syntactic and semantic information from the context the word occurs (Schone and J"
P07-1116,N01-1024,0,\N,Missing
P07-1116,P07-1012,0,\N,Missing
P10-1021,N01-1021,0,0.576616,"ted by Staub and Clifton (2006): following the word either, readers predict or and the complement that follows it, and process it faster compared to a control condition without either. Thus, human language processing takes advantage of the constraints imposed by the preceding semantic and syntactic context to derive expectations about the upcoming input. Much recent work has focused on developing computational measures of these constraints and expectations. Again, the literature is split into syntactic and semantic models. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficu"
P10-1021,P10-2012,1,0.839713,"to modeling semantic and syntactic costs disjointly using a mixture of probabilistic and nonprobabilistic measures. An interesting question is which aspects of semantics our model is able to capture, i.e., why does the combination of LSA or LDA representations with an incremental parser yield a better fit of the behavioral data. In the psycholinguistic literature, various types of semantic information have been investigated: lexical semantics (word senses, selectional restrictions, thematic roles), sentential semantics (scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference). A key"
P10-1021,P10-1120,0,0.0761262,"scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference). A key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly. For example, we could envisage a parser that uses semantic representations to guide its search, e.g., by pruning syntactic analyses that have a low semantic probability. At the same time, the semantic model should have access to syntactic information, i.e., the composition of word representations should take their syntactic relationships into account, rather than just linear order. Refer"
P10-1021,J01-2004,0,0.26386,"ords). (Rayner 1998) demonstrates that eye-movements are related to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore"
P10-1021,P04-1003,0,0.185212,"rsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficulty. Other work (McDonald and Brew 2004) models contextual constraint in information theoretic terms. The assumption is that words carry prior semantic expectations which are updated upon seeing the next word. Expectations are represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. The measures discussed above are typically computed automatically on real-language corpora using data-driven methods and their predictions are verified through analysis of eye-movements that people make while reading. Ample evidence The analysis of reading times can provide insights into the proc"
P10-1021,D09-1034,0,0.451527,"ng times. Mc197 Donald and Shillcock (2003) show that forward and backward transitional probabilities are predictive of first fixation and first pass durations: the higher the transitional probability, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk |wk−1 ). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk |wk+1 ). (e.g., left-to-right vs. top-down, PCFGs vs dependency parsing) and different degrees of lexicalization (see Roark et al. 2009 for an overview) . For instance, unlexicalized surprisal can be easily derived by substituting the words in Equation (1) with parts of speech (Demberg and Keller 2008). Surprisal could be also defined using a vanilla language model that does not take any structural or grammatical information into account (Frank 2009). 2.2 2.3 Syntactic Constraint Distributional models of meaning have been commonly used to quantify the semantic relation between a word and its context in computational studies of lexical processing. These models are based on the idea that words with similar meanings will be foun"
P10-1021,P08-1028,1,0.735474,"ation is updated using a Bayesian inference mechanism to reflect the newly arrived information. Like LSA, ICD is based on word co-occurrence vectors, however it does not employ singular value decomposition, and constructs a word-word rather than a word-document co-occurrence matrix. Although this model has been shown to successfully simulate single- and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). Their aim is not so much to model processing difficulty, but to construct vector-based meaning representations that go beyond individual words. They introduce a h = f (u, v) (2) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus deriv"
P10-1021,D09-1045,1,0.30952,"lated to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore, the models of Narayanan and Jurafsky (2002) and Pad´o et al. (2009) do"
P10-1021,J07-2002,1,0.660376,"Missing"
P15-1074,N01-1021,0,0.916092,"e to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively constant rate of information transfer during speech production (e.g., Jurafsky et al., 2001; Aylett and Turk, 2006; Frank and Jaeger, 2008). The rate of information transfer is thereby quantified using as each words’ Surprisal (Hale, 2001), that is, a word’s negative log probability in context. Surprisal(wi ) = − log P (wi |w1 ..wi−1 ) 763 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 763–773, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Frank and Jaeger (2008) investigated UID effects in the SWITCHBOARD corpus at a morphosyntactic level wherein speakers avoid using English contracted forms (“you are” vs. “you’re”) when the contractible phrase is also transmitting a hig"
P15-1074,J10-4006,0,0.132884,"Missing"
P15-1074,P10-1021,1,0.945117,"rk of distributional semantics. Regarding our main hypothesis—that speakers adapt their speech rate as a function of a word’s information content—it is particularly important to In order to build psycholinguistic models of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we specifically consider surprisal, a word’s predictability in context. Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. 1 I"
P15-1074,D12-1033,1,0.915695,"” vs. “you’re”) when the contractible phrase is also transmitting a high degree of information in context. In this case, n-gram surprisal was used as the information density measure. Related hypotheses have been suggested by Jurafsky et al. (2001), who related speech durations to bigram probabilities on the Switchboard corpus, and Aylett and Turk (2006), who investigated information density effects at the syllable level. They used a read-aloud English speech synthesis corpus, and they found that there is an inverse relationship between the pronunciation duration and the N-gram predictability. Demberg et al. (2012) also use the AMI corpus used in this work, and show that syntactic surprisal (i.e., the surprisal estimated from Roark’s (2009) PCFG parser) can predict word durations in natural speech. us to test this hypothesis on fully “natural” conversational data. Therefore, we use the AMI corpus, which contains transcripts of English-language conversations with orthographically correct transcriptions and precise word pronunciation boundaries in terms of time. We will explain the calculation of semantic surprisal in section 4 (this is so far only described in Mitchell’s 2011 PhD thesis), and then evalua"
P15-1074,D09-1034,0,0.0270045,"0.131 -0.051 0.011 0.015 -0.007 -6.58 144.04 -57.04 -19.41 17.61 4.99 -4.44 *** *** *** *** *** *** *** Table 5: Linear mixed effects model for spoken word durations in the AMI corpus, for a model including both syntactic and semantic surprisal as a predictor as well as a random intercept and slope for DMARY and SSemantics under speaker. prisal (χ2 = 309.5; p < 0.00001), and that semantic surprisal improves model fit over and above a model including syntactic surprisal and trigram surprisal (χ2 = 28.5; p < 0.00001). Table 5 shows the model containing both syntactic based on the Roark parser ((Roark et al., 2009); see also Demberg et al. (2012) for use of syntactic surprisal for estimating spoken word durations) and semantic surprisal. Finally, we split our dataset into data from native and non-native speakers of English (305 native speakers, vs. 376 non-native speakers). Table 6 shows generally larger effects for native than non-native speakers. In particular, the interaction between duration estimates and word frequencies, and semantic surprisal were not significant predictors in the non-native speaker model (however, random slopes for semantic surprisal under speaker still improved model fit very s"
P15-1074,P13-2152,0,0.242783,"n its previous (linguistic) context. It is a measure of information content in which a high surprisal implies low predictability. The use of surprisal in psycholinguistic research goes back to Hale (2001), who used a probabilistic Earley Parser to model the difficulty in parsing so-called garden path sentences (e.g. “The horse raced past the barn fell”), wherein the unexpectedness of an upcoming word or structure influences the language processor’s difficulty. Recent work in psycholinguistics has provided increasing support (e.g., Levy (2008); Demberg and Keller (2008); Smith and Levy (2013); Frank et al. (2013)) for the hypothesis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human. The Uniform Information Density (UID) hypothesis (Frank and Jaeger, 2008) holds that speakers tend distribute information uniformly across an utterance (in the limits of grammaticality). Information density is quantified in terms of the surprisal of each word (or other linguistic unit) in the utterance. These notions go back to Shannon (1948), who showed that conveying information uniformly close to channel ca"
P15-1074,N15-1003,1,0.844436,"Missing"
P16-4024,P14-1023,0,0.049598,"Missing"
P16-4024,J10-4006,0,0.0978499,"ries without knowing how to set all possible parameters. • Facilitate presentations and demonstrations about thematic fit evaluation. • Serve queries reasonably quickly, ideally at “web speed”, so that it is reasonable to “play around” with the models. This puts a constraint on the kinds of projections and dimensionality reduction we can use. 2 2.1 Vector-space thematic fit modeling Distributional Memory The currently best-performing models on the thematic fit task, in terms of correlation with human judgements, are the Distributional Memory (DM) models, based on a technique first proposed by Baroni and Lenci (2010). A DM model is an order-3 tensor with two axes that represent words in the model’s vocabulary and one axis that represents links between the words, so that the cell of the tensor is a positive real value for the occurrence of a triple &lt;word0,link,word1&gt;. That occurrence is an adjusted count of frequency such as Local Mutual Information (LMI). The link between the words is a connection acquired from processing a corpus, 2.2 Provided models In our demonstration version of Roleo, we provide two models, the TypeDM model from Baroni and Lenci and the “Malt-only SDDM” model from Sayeed et al. (2015"
P16-4024,P07-1071,0,0.266969,"by interpreting these links. Roleo allows for the query of agent roles (via subject links), patient roles (via object links), instrument roles (via the preposition “with”), and location roles (via the prepositions “in”, “at”, and “on”). Malt-only SDDM (just SDDM from now on) is derived from a set of corpora similar to that of TypeDM: BNC and ukWaC. The main difference between TypeDM and SDDM are the link types, which in SDDM are PropBank roles, derived from applying the SENNA semantic role labeller to the corpora. The links are therefore the PropBank roles that connect verbs to nouns. SENNA (Collobert and Weston, 2007), however, labels entire noun chunks with roles, often including adjectives and whole relative clauses. Sayeed et al. experiment with a number of algorithms for extracting the noun head or bare noun phrase; the best performing SENNA-based technique is to use the MaltParser dependencies produced by Baroni and Lenci, but simply as a guide for headidentification. Sayeed et al. show that PropBankbased roles and TypeDM roles help cover different aspects of the thematic fit problem. This process can be trivially reversed to represent the plausible verbs given a noun-role combination and to produce a"
Q17-1003,N16-1067,1,0.720394,"depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution duri"
Q17-1003,J10-4006,0,0.0235405,"ree-valued feature indicates whether the previous mention of the candidate DR d is a pronoun, a non-pronominal noun phrase, or has never been observed before. 4.2.2 Selectional Preferences Feature The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as"
Q17-1003,P14-1023,0,0.0594907,"ture The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as a stand-in for a semantic link. The values for each (w0 , r, w1 ) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of larg"
Q17-1003,P08-1090,0,0.722488,"uman expectations. • testing the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz"
Q17-1003,P09-1068,0,0.121207,"the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Sc"
Q17-1003,E14-1006,1,0.888764,"chank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, en"
Q17-1003,E12-1034,0,0.242357,"Missing"
Q17-1003,W14-1606,1,0.882589,"), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure i"
Q17-1003,L16-1555,1,0.929396,"have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation. They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge. We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge. InScript is a crowdsourced corpus of simple narrative texts. Participants were asked to write about a specific activity (e.g., a restaurant visit, a bus ride, or a grocery shopping event) which they personally experienced, and they were instructed to tell the story as if explaining the activity to a child. This resulted in stories that are centered around a specific scenario and that explicitly mention mundane details. Thus, they generally realize longer event chains associated with a single script, which makes them particularly appropriate"
Q17-1003,K16-1008,1,0.856447,"or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications f"
Q17-1003,N16-1098,0,0.0419051,"that were used; specifically, we would expect that larger predictability effects might be observable at script boundaries, rather than within a script, as is the case in our stories. A next step in moving our participant prediction model towards NLP applications would be to replicate our modelling results on automatic textto-script mapping instead of gold-standard data as done here (in order to approximate human level of processing). Furthermore, we aim to move to more complex text types that include reference to several scripts. We plan to consider the recently published ROC Stories corpus (Mostafazadeh et al., 2016), a large crowdsourced collection of topically unrestricted short and simple narratives, as a basis for these next steps in our research. Acknowledgments We thank the editors and the anonymous reviewers for their insightful suggestions. We would like to thank Florian Pusse for helping with the Amazon Mechanical Turk experiment. We would also like to thank Simon Ostermann and Tatjana Anikina for helping with the InScript corpus. This research was partially supported by the German Research Foundation (DFG) as part of SFB 1102 ‘Information Density and Linguistic Encoding’, European Research Counc"
Q17-1003,N15-1082,0,0.014094,"anical Turk experiment (Figure 2), our referent prediction model is asked to guess the upcoming DR. relation r, we collect all the predicates in the training set which have the participant type p in the position r. The embedding of the DR xp,r is given by the average embedding of these predicates. The feature is computed as the dot product of xp,r and the word embedding of the predicate v. Predicate schemas The following feature captures a specific aspect of knowledge about prototypical sequences of events. This knowledge is called predicate schemas in the recent co-reference modeling work of Peng et al. (2015). In predicate schemas, the goal is to model pairs of events such that if a DR d participated in the first event (in a specific role), it is likely to participate in the second event (again, in a specific role). For example, in the restaurant scenario, if one observes a phrase John ordered, one is likely to see John waited somewhere later in the document. Specific arguments are not that important (where it is John or some other DR), what is important is that the argument is reused across the predicates. This would correspond to the rule X-subject-of-order → X-subject-of-eat.4 Unlike the previo"
Q17-1003,E14-1024,0,0.0438992,"hoice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown"
Q17-1003,D12-1071,0,0.0196634,"nowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation. They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge. We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge. InScript is a crowdsourced corpus of simple narrative"
Q17-1003,P10-1100,1,0.908959,"cal event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, i"
Q17-1003,S15-1024,1,0.925336,"Missing"
Q17-1003,W16-2518,1,0.85711,"preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as a stand-in for a semantic link. The values for each (w0 , r, w1 ) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of large corpora (ukWaC, BNC,"
S15-1024,P08-1090,0,0.577726,"m the website “Dinners from Hell.” Our results suggest that applying these techniques to a domain-specific dataset may be reasonable way to learn domain-specific scripts. The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson’s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called “Dinners from Hell.” Our models learn narrative chains, script-like structures that we evaluate with the “narrative cloze” task (Chambers and Jurafsky, 2008). 1 2 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et"
S15-1024,P09-1068,0,0.325579,"” task (Chambers and Jurafsky, 2008). 1 2 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attemp"
S15-1024,P11-1098,0,0.0291101,"lar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for"
S15-1024,J90-1003,0,0.34642,"y exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ord"
S15-1024,de-marneffe-etal-2006-generating,0,0.0309946,"Missing"
S15-1024,E12-1034,0,0.444975,"Missing"
S15-1024,P14-5010,0,0.00819738,"arrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e0 whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit stories about their terrible restaurant experiences. For an example story, see Figure 1. To process the raw data, we stripped all HTML and other non-story content from each file and processed the remaining text with the Stanford CoreNLP pipeline version 3.3.1 (Manning et al., 2014). Of the 237 stories obtained, we manually filtered out 94 stories that were “off-topic” (e.g., letters to the webmaster, dinners not at restaurants), leaving a total of 143 stories. The average story length is 352 words. 4.1 Annotation For the purposes of evaluation only, we hired four undergraduates to annotate every non-copular verb in each story as either corresponding to an event “related to the experience of eating in a restaurant” (e.g., ordered a steak), “unrelated to the experience of eating in a restaurant” (e.g., answered the phone), or uncertain. We used the WebAnno platform for an"
S15-1024,N04-1041,0,0.0726893,"al models are introduced by Jans et al. (2012) and we use them here, as well. First, the ordered pmi model, eˆ = arg max e∈V k X pmi(ei , e) + i=1 n X pmi(e, ei ) i=k+1 (4) where C(e1 , e2 ) is asymmetric, i.e., C(e1 , e2 ) counts only cases in which e1 occurs before e2 . Second, the bigram probability model: eˆ = arg max e∈V where p(e2 |e1 ) = metric. k Y p(e|ei ) i=1 C(e1 ,e2 ) C(e1 ,∗) n Y p(ei |e) (5) i=k+1 and C(e1 , e2 ) is asymDiscounting For each model, we add an option for discounting the computed scores. In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by Chambers and Jurafsky (2008). For the bigram probability model, this PMI discount score would be inappropriate, so we instead use absolute discounting. 207 Document Threshold We include a document threshold parameter, D, that ensures that, in any narrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e0 whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit"
S15-1024,E14-1024,0,0.534804,"uction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by"
S15-1024,P10-1100,1,0.740772,"Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by eliciting event sequence descriptions (ESDs) from humans to which they apply multiple sequence alignment (MSA) to yield one global structure per script. (Orr et al. (2014) learn similar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learnin"
S15-1024,H91-1059,0,0.0679119,"at we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers"
S15-1024,P13-4001,0,0.0436435,"Missing"
S18-2002,N15-1003,1,0.726665,"her than all other values in the same column, where (**) p &lt; 0.01. of verb-noun-relation tuples from a large-scale mixed corpus smoothed by local mutual information. The key idea in applying DM models to the thematic fit rating task is to construct a “prototype filler”, and compare candidate fillers against the prototype using cosine similarity. The baseline models we compare against include NNRF and: TypeDM: This is best-performing DM model from Baroni and Lenci (2010). Relations of verb-noun pairs are obtained using hand-crafted rules. The results of this model are from reimplementations in Greenberg et al. (2015a,b). SDDM-mo: This DM comes from Sayeed and Demberg (2014) and is constructed with automatically-extracted semantic information. GSD15: This is the overall best-performing model from Greenberg et al. (2015b) using hierarchical clustering of typical role-fillers to construct prototype on TypeDM. SCLB17: This is the best-performing model on F-Inst from Santus et al. (2017). The number of fillers used in prototype construction is 30 and the number of top features is 2000. We report the highest results among the different types of dependency contexts in their framework. (Bj¨orkelund et al., 2010;"
S18-2002,J10-4006,0,0.900873,"ts of semantic role classification given verb-head pairs. P is precision, R is recall and F1 is Fmeasure. F1 values with a mark are significantly higher than all other values in the same column, where (**) p &lt; 0.01. of verb-noun-relation tuples from a large-scale mixed corpus smoothed by local mutual information. The key idea in applying DM models to the thematic fit rating task is to construct a “prototype filler”, and compare candidate fillers against the prototype using cosine similarity. The baseline models we compare against include NNRF and: TypeDM: This is best-performing DM model from Baroni and Lenci (2010). Relations of verb-noun pairs are obtained using hand-crafted rules. The results of this model are from reimplementations in Greenberg et al. (2015a,b). SDDM-mo: This DM comes from Sayeed and Demberg (2014) and is constructed with automatically-extracted semantic information. GSD15: This is the overall best-performing model from Greenberg et al. (2015b) using hierarchical clustering of typical role-fillers to construct prototype on TypeDM. SCLB17: This is the best-performing model on F-Inst from Santus et al. (2017). The number of fillers used in prototype construction is 30 and the number of"
S18-2002,J15-1004,0,0.125978,"the ablation study (see Section 8). 7 7.1 Event Similarity Lastly, we evaluate the quality of the event embeddings learned via the multi-task network models. While word embeddings from tools like word2vec (Mikolov et al., 2013) are standard methods for obtaining word similarities, identifying a suitable method for more general event similarity estimation is still a relevant problem. The model proposed here constitutes an interesting method for obtaining event embeddings, as it is trained on two semantics-focused prediction tasks. For evaluation, we use the sentence similarity task proposed by Grefenstette and Sadrzadeh (2015) (second experiment in their paper). For evaluation, we use the re-annotated dataset, named GS13, constructed in 2013 by Kartsaklis and Sadrzadeh (2014). Each row in the dataset contains a participant ID, two sentences, a human evaluation score of their similarity from 1 to 7, and a HIGH/LOW tag indicating the similarity group of two sentences. An example entry is: Evaluation: Compositionality The thematic fit judgements from the tasks discussed in section 6 only contain ratings of the fit between the predicate and one role-filler. However, other event participants contained in a clause can af"
S18-2002,C10-3009,0,0.0527967,"Missing"
S18-2002,W05-0620,0,0.240499,"Missing"
S18-2002,N18-1076,0,0.02231,"ork-internal event representations. These results indicate that ResRoFA-MT-based event embeddings may be suitable for applications and tasks where similarity estimates for larger phrases are needed (cf. Wanzare et al., 2017). 8 9 Related Work Modi et al. (2017) proposed a compositional neural model for referent prediction in which event embeddings were constructed via the sum of predicate and argument embeddings. Weber et al. (2017) proposed a tensor-based composition model to construct event embeddings with agents and patients. They represented predicates as tensors and arguments as vectors. Cheng and Erk (2018) proposed a neural-based model to predict implicit arguments with event knowledge in which the event embeddings are composed with a twolayer feed-forward neural network. Ablation Study: Single-task Variants From the evaluations above, we notice that the performance of the multi-task model with simple addition composition method (NNRF-MT) is not significantly different from the single task model (NNRF). In order to test whether the additional training task improves model performance, we develop single-task variants for RoFA-MT and ResRoFA-MT models, named RoFA-ST and ResRoFA-ST correspondingly,"
S18-2002,P15-1115,0,0.0181896,"xt only. The prospect of applying our models independently to SRL tasks suggests an area of potential future work. Our models currently use only the predicates and head words of arguments. Instead of depending on corpora with extracted head words, we can integrate an attention mechanism (Vaswani et al., 2017) to capture the position of syntactic heads. We are working on extending our models to use all words, which will enable testing as an SRL tool. Finally, the predictive nature of this type of model can potentially enable its deployment in incremental semantic parsing (Konstas et al., 2014; Konstas and Keller, 2015) by combining the multitask design with the incremental architecture in (Tilk et al., 2016). We are continuing to develop this and other ways of employing models of event representation that simultaneously predict event participants and assess the fit of given participants. mating event similarity. We also performed a study regarding the usefulness of our purely semantics-based representations for semantic role labelling. While many semantic role labellers rely predominantly on syntax, our approach addresses the likelihood that a semantic role should be assigned purely based on its plausibilit"
S18-2002,D14-1036,1,0.85509,"can be learned from text only. The prospect of applying our models independently to SRL tasks suggests an area of potential future work. Our models currently use only the predicates and head words of arguments. Instead of depending on corpora with extracted head words, we can integrate an attention mechanism (Vaswani et al., 2017) to capture the position of syntactic heads. We are working on extending our models to use all words, which will enable testing as an SRL tool. Finally, the predictive nature of this type of model can potentially enable its deployment in incremental semantic parsing (Konstas et al., 2014; Konstas and Keller, 2015) by combining the multitask design with the incremental architecture in (Tilk et al., 2016). We are continuing to develop this and other ways of employing models of event representation that simultaneously predict event participants and assess the fit of given participants. mating event similarity. We also performed a study regarding the usefulness of our purely semantics-based representations for semantic role labelling. While many semantic role labellers rely predominantly on syntax, our approach addresses the likelihood that a semantic role should be assigned pure"
S18-2002,W15-1106,1,0.909113,"her than all other values in the same column, where (**) p &lt; 0.01. of verb-noun-relation tuples from a large-scale mixed corpus smoothed by local mutual information. The key idea in applying DM models to the thematic fit rating task is to construct a “prototype filler”, and compare candidate fillers against the prototype using cosine similarity. The baseline models we compare against include NNRF and: TypeDM: This is best-performing DM model from Baroni and Lenci (2010). Relations of verb-noun pairs are obtained using hand-crafted rules. The results of this model are from reimplementations in Greenberg et al. (2015a,b). SDDM-mo: This DM comes from Sayeed and Demberg (2014) and is constructed with automatically-extracted semantic information. GSD15: This is the overall best-performing model from Greenberg et al. (2015b) using hierarchical clustering of typical role-fillers to construct prototype on TypeDM. SCLB17: This is the best-performing model on F-Inst from Santus et al. (2017). The number of fillers used in prototype construction is 30 and the number of top features is 2000. We report the highest results among the different types of dependency contexts in their framework. (Bj¨orkelund et al., 2010;"
S18-2002,W11-0607,0,0.254638,"50 49 43.3 43.2 52.2** 53.0** McRae05 33 27 36 28 35.9 36.1 41.9** 42.5** F-Loc 23 13 29 37 46.5 46.3 45.9 46.3 F-Inst 36 28 42 50 52.1 50.0* 49.4* 47.7** GDS 46 48 57.6 57.2 60.7** 60.8** avg 40.8 40.5 44.2 44.0 48.6** 48.9** Table 3: Results on human thematic fit judgement correlation task (Spearman’s ρ × 100) compared to previous work. The last column reports the weighted average results by numbers of entries of all five datasets. Values with a mark are significantly different from the baseline model (NNRF), where (*) p &lt; 0.05, (**) p &lt; 0.01. choosing the tags uniformly at random. Lenci11: Lenci (2011) proposed a composition model for TypeDM. Table 4 shows that our new composition method based on parametric weighted average outperforms previous models; the RoFA-MT model achieves the highest accuracy overall and outperforms the baseline (NNRF) significantly. Table 3 shows results for all models and datasets. The ResRoFA-MT model performs best overall, improving more than 4 points over the baseline. The multi-task model (NNRF-MT) has performance similar to baseline (NNRF). Our new architecture using a parametric weighted average over event participant embeddings (RoFA-MT) outperforms simple s"
S18-2002,P98-2127,0,0.100418,"Missing"
S18-2002,W16-2518,1,0.817256,"rototypical participants of events and their structured compositions, plays a crucial role in human sentence processing, especially from the perspective of thematic fit: the extent to which humans perceive given event participants as “fitting” given predicate-role combinations (Ferretti et al., 2001; McRae et al., 2005; Bicknell et al., 2010). Therefore, computational models of language processing should also consist of event representations that reflect thematic fit. To evaluate this aspect empirically, a popular approach in previous work has been to compare model output to human judgements (Sayeed et al., 2016). The best-performing recent work has been the model of Tilk et al. (2016), who effectively simulate thematic fit via selectional preferences: generating a probability distribution over the full vocab11 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 11–21 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics fectively predict semantic roles for event participants as well as perform role-filler prediction1 . Furthermore, we obtain significant improvements and better-performing event embeddings by an adjustment to the architec"
S18-2002,L18-1488,1,0.882076,"Missing"
S18-2002,D16-1017,1,0.661803,"a crucial role in human sentence processing, especially from the perspective of thematic fit: the extent to which humans perceive given event participants as “fitting” given predicate-role combinations (Ferretti et al., 2001; McRae et al., 2005; Bicknell et al., 2010). Therefore, computational models of language processing should also consist of event representations that reflect thematic fit. To evaluate this aspect empirically, a popular approach in previous work has been to compare model output to human judgements (Sayeed et al., 2016). The best-performing recent work has been the model of Tilk et al. (2016), who effectively simulate thematic fit via selectional preferences: generating a probability distribution over the full vocab11 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 11–21 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics fectively predict semantic roles for event participants as well as perform role-filler prediction1 . Furthermore, we obtain significant improvements and better-performing event embeddings by an adjustment to the architecture (parametric weighted average of role-filler embeddings) which helps t"
S18-2002,Q17-1003,1,0.850557,"0/PRD/ARG1) into each model. We then extract the event representation vectors for both sentences and compute their cosine similarity. Table 5 shows correlation coefficients in Spearman’s ρ × 100 between sentence-pair similarities and human judgement scores. ResRoFA-MT obtains best results, indicating that the secondary task helped also to improve the network-internal event representations. These results indicate that ResRoFA-MT-based event embeddings may be suitable for applications and tasks where similarity estimates for larger phrases are needed (cf. Wanzare et al., 2017). 8 9 Related Work Modi et al. (2017) proposed a compositional neural model for referent prediction in which event embeddings were constructed via the sum of predicate and argument embeddings. Weber et al. (2017) proposed a tensor-based composition model to construct event embeddings with agents and patients. They represented predicates as tensors and arguments as vectors. Cheng and Erk (2018) proposed a neural-based model to predict implicit arguments with event knowledge in which the event embeddings are composed with a twolayer feed-forward neural network. Ablation Study: Single-task Variants From the evaluations above, we not"
S18-2002,N16-1098,0,0.0503236,"Missing"
S18-2002,W17-0901,0,0.0219489,"eed all three words and their roles (ARG0/PRD/ARG1) into each model. We then extract the event representation vectors for both sentences and compute their cosine similarity. Table 5 shows correlation coefficients in Spearman’s ρ × 100 between sentence-pair similarities and human judgement scores. ResRoFA-MT obtains best results, indicating that the secondary task helped also to improve the network-internal event representations. These results indicate that ResRoFA-MT-based event embeddings may be suitable for applications and tasks where similarity estimates for larger phrases are needed (cf. Wanzare et al., 2017). 8 9 Related Work Modi et al. (2017) proposed a compositional neural model for referent prediction in which event embeddings were constructed via the sum of predicate and argument embeddings. Weber et al. (2017) proposed a tensor-based composition model to construct event embeddings with agents and patients. They represented predicates as tensors and arguments as vectors. Cheng and Erk (2018) proposed a neural-based model to predict implicit arguments with event knowledge in which the event embeddings are composed with a twolayer feed-forward neural network. Ablation Study: Single-task Varian"
S18-2002,L18-1570,1,0.879601,"Missing"
S18-2002,J07-2002,0,0.164456,"Missing"
S18-2002,S15-1027,0,0.0188218,"ming event embeddings by an adjustment to the architecture (parametric weighted average of role-filler embeddings) which helps to capture role-specific information for participants during the composition process. The new event embeddings exhibit state-of-the-art performance on a correlation task with human thematic fit judgements and an event similarity task. Our model is the first joint model for selectional preferences (SPs) prediction and semantic role classification (SRC) to the best of our knowledge. Previous works used distributional similarity-based (Zapirain et al., 2013) or LDAbased (Wu and Palmer, 2015) SPs for semantic role labelling to leverage lexical sparsity. However, when it comes to a situation with domain shift, single task SP models that rely heavily on syntax have high generalisation error. We show that the multi-task architecture is better suited to generalise in that situation and can be potentially applied to improve current semantic role labelling systems which rely on small annotated corpora. Our approach is a conceptual improvement on previous models because we address multiple event-representation tasks in a single model: thematic fit evaluation, role-filler prediction/gener"
S18-2002,P09-2019,0,0.064133,"Missing"
S18-2002,D14-1045,0,0.0253071,",b). SDDM-mo: This DM comes from Sayeed and Demberg (2014) and is constructed with automatically-extracted semantic information. GSD15: This is the overall best-performing model from Greenberg et al. (2015b) using hierarchical clustering of typical role-fillers to construct prototype on TypeDM. SCLB17: This is the best-performing model on F-Inst from Santus et al. (2017). The number of fillers used in prototype construction is 30 and the number of top features is 2000. We report the highest results among the different types of dependency contexts in their framework. (Bj¨orkelund et al., 2010; Roth and Woodsend, 2014) leads to a small but statistically significant improvement of 0.11 points in F1 score on the outof-domain dataset used in the CoNLL-2009 semantic role labelling task (Hajiˇc et al., 2009). 6 Evaluation: Thematic Fit Modelling Next, we evaluate our multi-task models against human thematic fit ratings in order to assess whether the inclusion of the multi-task architecture leads to improvements on this task, following Pad´o et al. (2009); Baroni and Lenci (2010); Greenberg et al. (2015b); Sayeed et al. (2016). 6.1 Datasets The human judgement data consists of verbs, a verbal argument with its ro"
S18-2002,D17-1068,0,0.46552,"re against include NNRF and: TypeDM: This is best-performing DM model from Baroni and Lenci (2010). Relations of verb-noun pairs are obtained using hand-crafted rules. The results of this model are from reimplementations in Greenberg et al. (2015a,b). SDDM-mo: This DM comes from Sayeed and Demberg (2014) and is constructed with automatically-extracted semantic information. GSD15: This is the overall best-performing model from Greenberg et al. (2015b) using hierarchical clustering of typical role-fillers to construct prototype on TypeDM. SCLB17: This is the best-performing model on F-Inst from Santus et al. (2017). The number of fillers used in prototype construction is 30 and the number of top features is 2000. We report the highest results among the different types of dependency contexts in their framework. (Bj¨orkelund et al., 2010; Roth and Woodsend, 2014) leads to a small but statistically significant improvement of 0.11 points in F1 score on the outof-domain dataset used in the CoNLL-2009 semantic role labelling task (Hajiˇc et al., 2009). 6 Evaluation: Thematic Fit Modelling Next, we evaluate our multi-task models against human thematic fit ratings in order to assess whether the inclusion of the"
W08-2304,W00-1307,0,0.522358,"Missing"
W08-2304,W04-0308,0,\N,Missing
W08-2304,W04-0302,0,\N,Missing
W08-2304,P97-1012,0,\N,Missing
W08-2304,J01-2004,0,\N,Missing
W08-2304,P91-1012,0,\N,Missing
W08-2304,J05-1004,0,\N,Missing
W08-2304,H05-1102,0,\N,Missing
W08-2304,P07-1031,0,\N,Missing
W08-2304,meyers-etal-2004-annotating,0,\N,Missing
W12-4608,W10-4417,0,0.0304754,"sentence ∃ &gt; ∀, i.e. there exists a single reservation request, and that it is to be sent indiscriminately to all restaurants. This reading reflects the linear order that quantifiers occurred in. It misses the other interpretation ∀ &gt; ∃ (which was likely intended by the user). When sentences are derived incrementally, we cannot choose in the derivation to first substitute the second NP in order to get the other reading. Therefore, we need to systematically use underspecification to get both readings. Note that this problem also exists to a certain extent in standard LTAG (Joshi et al., 2007; Barker, 2010). However, not all quantifier orderings are permissible given the syntax. Joshi et al. (2007) and Romero and Kallmeyer (2005) present an example of what can be seen as a challenge for current TAG-based and movementless approaches: 69 (7) Two politicians spied on some person from every city. They describe a situation where syntactic constraints prevent some readings out of all possible permutations of quantifiers. In order to forbid their hypothetical widest matrix scope reading of every city, we use dominance constraints (Koller et al., 2003) to implement restrictions based on the argument-adj"
W12-4608,W07-1210,0,0.307501,"ses existentiallybound event variables (∃e) to connect verb predicates and their subcategorized arguments and separates predicate arguments into their own, separate event-modifying predicates connected through conjunctions. This permits a flexible means to underspecify function composition and argument structure (Hunter, 2009) by greatly limiting recursion (Example (3-a)) in semantic expressions. (3) a. Happily(Eating(Candy)) b. ∃x1 ∃eCandy(x1 )&Eaten(e, x1 )& Eating(e)&Happily(e) The Neo-Davidsonian approach has been implemented in formalisms such as Robust Minimal Recursion Semantics (RMRS; Copestake, 2007). We use a variant exemplified in (3-b). Neo-Davidsonian semantics has some advantages in the case of a strictly incremental parsing process: in incremental PLTAG parsing, the prediction trees do not always specify the full subcategorization frame of a predicate. For example, when processing the words Peter often, the NP tree for Peter and the auxiliary tree for often (see Fig. 1(b)) will be connected with a prediction 65 S {∃e} S {∃e} VP  V NP↓ NP↓ {Qx1 Recipient(e, x1 )}{Qx2 Sent(e, x2 )} send {Send(e)} VP  V NP↓ PPk2 {Qx2 Sent(e, x2 )} {Qx1 Recipient(e, x1 )} send {Send(e)} TOk2 k2 tok2 k"
W12-4608,W08-2304,1,0.862348,"Missing"
W12-4608,P03-1047,0,0.549245,"Missing"
W12-4608,P10-1021,1,0.832432,"generate an answer (Aist et al., 2007; Schuler et al., 2009; Skantze and Schlangen, 2009)) before the request is fully stated. Another use of formalisms that support strict incrementality is psycholinguistic modelling: As there is a substantial amount of evidence that human sentence processing is highly incremental, computational models of human sentence processing should be incremental to the same degree. Such models can then be used to calculate measures of human sentence processing difficulty, such as surprisal, which have been demonstrated to correspond to reading times (e.g., Levy, 2008; Mitchell et al., 2010). Two strictly incremental versions of treeadjoining grammar (TAG; Joshi et al., 1975) which have been proposed in recent years are DVTAG (Mazzei et al., 2007) and PLTAG (DembergWinterfors, 2010). Incremental syntax is however only of limited interest without a corresponding mechanism for calculating the incremental semantic interpretation. And for that semantic model to be practically useful in psycholinguistic modelling or NLP applications such as speech recognition or dialogue systems, we believe that the semantic representation should ideally be simple, flat and usefully underspecified, in"
W12-4608,J09-3001,0,0.119293,"e a particularly interesting question in the context of incremental TAG. 1 Introduction Incremental processing formalisms have increasing importance due to the growing ubiquity of spoken dialogue systems that require understanding and generation in real-time using rich, robust semantics. Dialogue systems benefit from incremental processing in terms of shorter response time to the user’s requests when the dialogue system can start interpreting and serving the request (e.g. by consulting databases, doing reference resolution, backchannelling or starting to generate an answer (Aist et al., 2007; Schuler et al., 2009; Skantze and Schlangen, 2009)) before the request is fully stated. Another use of formalisms that support strict incrementality is psycholinguistic modelling: As there is a substantial amount of evidence that human sentence processing is highly incremental, computational models of human sentence processing should be incremental to the same degree. Such models can then be used to calculate measures of human sentence processing difficulty, such as surprisal, which have been demonstrated to correspond to reading times (e.g., Levy, 2008; Mitchell et al., 2010). Two strictly incremental versions o"
W12-4608,E09-1085,0,0.0202385,"esting question in the context of incremental TAG. 1 Introduction Incremental processing formalisms have increasing importance due to the growing ubiquity of spoken dialogue systems that require understanding and generation in real-time using rich, robust semantics. Dialogue systems benefit from incremental processing in terms of shorter response time to the user’s requests when the dialogue system can start interpreting and serving the request (e.g. by consulting databases, doing reference resolution, backchannelling or starting to generate an answer (Aist et al., 2007; Schuler et al., 2009; Skantze and Schlangen, 2009)) before the request is fully stated. Another use of formalisms that support strict incrementality is psycholinguistic modelling: As there is a substantial amount of evidence that human sentence processing is highly incremental, computational models of human sentence processing should be incremental to the same degree. Such models can then be used to calculate measures of human sentence processing difficulty, such as surprisal, which have been demonstrated to correspond to reading times (e.g., Levy, 2008; Mitchell et al., 2010). Two strictly incremental versions of treeadjoining grammar (TAG;"
W12-4623,W09-0509,0,0.0219676,"e almost fully incremental. This paper explores the syntactic constructions for which full incrementality is not possible in standard CCG, a point that recent work on incremental CCG parsing has glossed over. 1 Introduction In recent years, there has been an increasing interest in (strictly) incremental and connected processing, both from a cognitive modelling perspective (Mazzei et al., 2007; Demberg and Keller, 2008; Schuler et al., 2008; Reitter et al., 2006) and from a perspective of NLP applications like spoken dialogue system (e.g., Purver and Kempson, 2004; Schlangen and Skantze, 2009; Atterer and Schlangen, 2009), machine translation (Hassan et al., 2008; Hefny et al., 2011) and speech recognition (Roark, 2001) that set out to process linguistic input in real time and therefore require the greedy generation of hypotheses about the input without delaying decisions about how words are connected. CCG (Steedman, 1996, 2000) as a grammar formalism seems particularly well-suited for incremental connected processing due to its flexible constituency structure and direct syntaxsemantic interface, which allows to simultaneously construct an incremental syntactic and semantic derivation. Indeed, Steedman (2000,"
W12-4623,N09-1043,0,0.0318737,"analysis is high – feeding into the other processing layers interpretations which later turn out to be incorrect causes frequent revisions and corrections in all processing layers, which can be very costly. Because connecting all words generally means to spell out the different ways in which the words might be connected while still lacking some of the evidence, a significant amount of uncertainty concerning which interpretation is correct can be expected. In praxis, one therefore has to consider the trade-off between the degree of incrementality or connectedness and accuracy (see for example Baumann et al., 2009; Kato et al., 2004). 3 CCG Combinatory Categorial Grammar (CCG, Steedman, 1996, 2000) consists of a lexicalized grammar and a small set of rules that allow the lexical entries to be combined into parse trees. Each word in the lexicon is assigned one or more categories that define its behaviour in the sentence. Categories for a word can either be atomic e.g., NP, 200 Forward Application: Backward Application: Forward Composition: Backward Composition: Forward Generalized Composition: Backward Crossed Composition: Forward Type-raising: Coordination: X/Y Y X/Y Y X/Y Y X X conj Y XY Y/Z XY"
W12-4623,W08-2304,1,0.93832,"ily in order to achieve shorter response times in spoken dialogue systems, for speech recognition and machine translation. CCG allows for a variety of different derivations, including derivations that are almost fully incremental. This paper explores the syntactic constructions for which full incrementality is not possible in standard CCG, a point that recent work on incremental CCG parsing has glossed over. 1 Introduction In recent years, there has been an increasing interest in (strictly) incremental and connected processing, both from a cognitive modelling perspective (Mazzei et al., 2007; Demberg and Keller, 2008; Schuler et al., 2008; Reitter et al., 2006) and from a perspective of NLP applications like spoken dialogue system (e.g., Purver and Kempson, 2004; Schlangen and Skantze, 2009; Atterer and Schlangen, 2009), machine translation (Hassan et al., 2008; Hefny et al., 2011) and speech recognition (Roark, 2001) that set out to process linguistic input in real time and therefore require the greedy generation of hypotheses about the input without delaying decisions about how words are connected. CCG (Steedman, 1996, 2000) as a grammar formalism seems particularly well-suited for incremental connected"
W12-4623,W04-0302,0,0.546382,"eding into the other processing layers interpretations which later turn out to be incorrect causes frequent revisions and corrections in all processing layers, which can be very costly. Because connecting all words generally means to spell out the different ways in which the words might be connected while still lacking some of the evidence, a significant amount of uncertainty concerning which interpretation is correct can be expected. In praxis, one therefore has to consider the trade-off between the degree of incrementality or connectedness and accuracy (see for example Baumann et al., 2009; Kato et al., 2004). 3 CCG Combinatory Categorial Grammar (CCG, Steedman, 1996, 2000) consists of a lexicalized grammar and a small set of rules that allow the lexical entries to be combined into parse trees. Each word in the lexicon is assigned one or more categories that define its behaviour in the sentence. Categories for a word can either be atomic e.g., NP, 200 Forward Application: Backward Application: Forward Composition: Backward Composition: Forward Generalized Composition: Backward Crossed Composition: Forward Type-raising: Coordination: X/Y Y X/Y Y X/Y Y X X conj Y XY Y/Z XY (Y/Z)/$1 XY ⇒> ⇒&lt; ⇒"
W12-4623,E95-1017,0,0.79253,"2000, p. 226) claims that “combinatory grammars are particularly well suited to the incremental, essentially word-by-word assembly of semantic interpretations”. However, existing work on using CCG incrementally (Reitter et al., 2006; Hefny et al., 2011) either did not use fully connected incremental derivations, or introduced additional operations which are not part of the standard CCG rule set. From the existing literature, it remains largely unclear in what kinds of cases a fully connected incremental analysis is impossible (with the exception of coordination, see Sturt and Lombardo, 2005). Milward (1995) notes in a footnote that “CCG doesn’t provide a type for all initial fragments of sentences. For example, it gives a type to John thinks Mary but not to John thinks each”. While Demberg and Keller (2008) briefly mentions that object relative clauses (like The woman that the man saw laughed.) are problematic to process strictly incrementally with CCG, they do not provide a detailed explanation. The goal of this paper is to provide an overview of the different cases and explain in detail when and why fully connected word-by-word derivations are not possible with standard CCG. This paper will fi"
W12-4623,W04-0312,0,0.028949,"of different derivations, including derivations that are almost fully incremental. This paper explores the syntactic constructions for which full incrementality is not possible in standard CCG, a point that recent work on incremental CCG parsing has glossed over. 1 Introduction In recent years, there has been an increasing interest in (strictly) incremental and connected processing, both from a cognitive modelling perspective (Mazzei et al., 2007; Demberg and Keller, 2008; Schuler et al., 2008; Reitter et al., 2006) and from a perspective of NLP applications like spoken dialogue system (e.g., Purver and Kempson, 2004; Schlangen and Skantze, 2009; Atterer and Schlangen, 2009), machine translation (Hassan et al., 2008; Hefny et al., 2011) and speech recognition (Roark, 2001) that set out to process linguistic input in real time and therefore require the greedy generation of hypotheses about the input without delaying decisions about how words are connected. CCG (Steedman, 1996, 2000) as a grammar formalism seems particularly well-suited for incremental connected processing due to its flexible constituency structure and direct syntaxsemantic interface, which allows to simultaneously construct an incremental"
W12-4623,W06-1637,0,0.371622,"n spoken dialogue systems, for speech recognition and machine translation. CCG allows for a variety of different derivations, including derivations that are almost fully incremental. This paper explores the syntactic constructions for which full incrementality is not possible in standard CCG, a point that recent work on incremental CCG parsing has glossed over. 1 Introduction In recent years, there has been an increasing interest in (strictly) incremental and connected processing, both from a cognitive modelling perspective (Mazzei et al., 2007; Demberg and Keller, 2008; Schuler et al., 2008; Reitter et al., 2006) and from a perspective of NLP applications like spoken dialogue system (e.g., Purver and Kempson, 2004; Schlangen and Skantze, 2009; Atterer and Schlangen, 2009), machine translation (Hassan et al., 2008; Hefny et al., 2011) and speech recognition (Roark, 2001) that set out to process linguistic input in real time and therefore require the greedy generation of hypotheses about the input without delaying decisions about how words are connected. CCG (Steedman, 1996, 2000) as a grammar formalism seems particularly well-suited for incremental connected processing due to its flexible constituency"
W12-4623,J01-2004,0,0.587945,"sible in standard CCG, a point that recent work on incremental CCG parsing has glossed over. 1 Introduction In recent years, there has been an increasing interest in (strictly) incremental and connected processing, both from a cognitive modelling perspective (Mazzei et al., 2007; Demberg and Keller, 2008; Schuler et al., 2008; Reitter et al., 2006) and from a perspective of NLP applications like spoken dialogue system (e.g., Purver and Kempson, 2004; Schlangen and Skantze, 2009; Atterer and Schlangen, 2009), machine translation (Hassan et al., 2008; Hefny et al., 2011) and speech recognition (Roark, 2001) that set out to process linguistic input in real time and therefore require the greedy generation of hypotheses about the input without delaying decisions about how words are connected. CCG (Steedman, 1996, 2000) as a grammar formalism seems particularly well-suited for incremental connected processing due to its flexible constituency structure and direct syntaxsemantic interface, which allows to simultaneously construct an incremental syntactic and semantic derivation. Indeed, Steedman (2000, p. 226) claims that “combinatory grammars are particularly well suited to the incremental, essential"
W12-4623,E09-1081,0,0.0820553,"including derivations that are almost fully incremental. This paper explores the syntactic constructions for which full incrementality is not possible in standard CCG, a point that recent work on incremental CCG parsing has glossed over. 1 Introduction In recent years, there has been an increasing interest in (strictly) incremental and connected processing, both from a cognitive modelling perspective (Mazzei et al., 2007; Demberg and Keller, 2008; Schuler et al., 2008; Reitter et al., 2006) and from a perspective of NLP applications like spoken dialogue system (e.g., Purver and Kempson, 2004; Schlangen and Skantze, 2009; Atterer and Schlangen, 2009), machine translation (Hassan et al., 2008; Hefny et al., 2011) and speech recognition (Roark, 2001) that set out to process linguistic input in real time and therefore require the greedy generation of hypotheses about the input without delaying decisions about how words are connected. CCG (Steedman, 1996, 2000) as a grammar formalism seems particularly well-suited for incremental connected processing due to its flexible constituency structure and direct syntaxsemantic interface, which allows to simultaneously construct an incremental syntactic and semantic deriva"
W12-4623,C08-1099,0,0.0188058,"orter response times in spoken dialogue systems, for speech recognition and machine translation. CCG allows for a variety of different derivations, including derivations that are almost fully incremental. This paper explores the syntactic constructions for which full incrementality is not possible in standard CCG, a point that recent work on incremental CCG parsing has glossed over. 1 Introduction In recent years, there has been an increasing interest in (strictly) incremental and connected processing, both from a cognitive modelling perspective (Mazzei et al., 2007; Demberg and Keller, 2008; Schuler et al., 2008; Reitter et al., 2006) and from a perspective of NLP applications like spoken dialogue system (e.g., Purver and Kempson, 2004; Schlangen and Skantze, 2009; Atterer and Schlangen, 2009), machine translation (Hassan et al., 2008; Hefny et al., 2011) and speech recognition (Roark, 2001) that set out to process linguistic input in real time and therefore require the greedy generation of hypotheses about the input without delaying decisions about how words are connected. CCG (Steedman, 1996, 2000) as a grammar formalism seems particularly well-suited for incremental connected processing due to its"
W12-4623,W10-4301,0,0.0172962,"le there is a considerable amount of supporting evidence for connectedness in human sentence processing, these studies can only make claims about connectedness at a specific point in a particular construction, but cannot answer the question whether human processing is fully connected at every point in every sentence. 2.2 Connectedness for fast Interpretation in Dialogue Systems Dialogue systems which interact with the user in real time have been shown to exhibit more natural behaviour when they process the language input incrementally (Schlangen and Skantze, 2009; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). They can then start constructing hypotheses of what is being said, and react to the language input (e.g. by searching a database, formulating a response, a backchannel or a clarification question) much more quickly than if they wait for the whole utterance to be completed. If the partial derivation of a sentence is fully connected at each point in time, a semantic interpretation will be available more quickly, and the system can thus react more quickly than in a nonconnected system. Similarly, speech recognition and machine translation systems can profit from interpretations (and their proba"
W12-4623,E09-1085,0,0.0230698,"y incremental processing. While there is a considerable amount of supporting evidence for connectedness in human sentence processing, these studies can only make claims about connectedness at a specific point in a particular construction, but cannot answer the question whether human processing is fully connected at every point in every sentence. 2.2 Connectedness for fast Interpretation in Dialogue Systems Dialogue systems which interact with the user in real time have been shown to exhibit more natural behaviour when they process the language input incrementally (Schlangen and Skantze, 2009; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010). They can then start constructing hypotheses of what is being said, and react to the language input (e.g. by searching a database, formulating a response, a backchannel or a clarification question) much more quickly than if they wait for the whole utterance to be completed. If the partial derivation of a sentence is fully connected at each point in time, a semantic interpretation will be available more quickly, and the system can thus react more quickly than in a nonconnected system. Similarly, speech recognition and machine translation systems can profit from"
W12-4703,C12-1163,1,0.697159,"e available annotations. In such studies, the implicitness of a particular relation in a text should not be investigated solely in terms of the presence of a discourse marker. The markedness would strongly rely on the strength of the link between the relation type and the applied discourse cue and should be treated as a continuous feature. For example, Reason relations in the corpus which include “and” as their connective, are not really explicit causal relations, rather the causality is left implicit in the content of the arguments (this could further inform recent studies such as the one by Asr and Demberg (2012)). 4.2 Automatic Identification of Discourse Relations Another aspect which is particularly important for computational linguists and NLP researchers is to develop a methods for automatically identifying discourse relations in a given text or utterance – which happens after defining a set of desired relation senses. We would suggest consideration of the following points both for human annotators and for development of automatic tools: Discourse cues should be looked at with respect to their specificity, e.g., the measure we proposed to determine the marking strength of a word group. Every phra"
W12-4703,prasad-etal-2008-penn,0,0.843535,"tic level is established with the help of sharing entities in the discourse or relations between statements, which are called discourse relations. The discourse relations are usually described in terms of their relation sense (e.g., causal, temporal, additive). Identification of these relations, i.e., first coming up with a set of possible relation senses and then assigning labels to the segments of a given text, is an essential first step in both theoretical and application-based studies on discourse processing. Given a set of sense labels (like the ones in the Penn Discourse Treebank (PDTB, Prasad et al., 2008)), identification of the relations between neighboring segments of a text is a difficult task when the text segments do not include an explicit discourse connector. For example, in (1-a) the connective “because” is a marker of a causal relation between the two clauses, whereas in (1-b) the relation is not marked explicitly with a discourse connector. (1) a. b. Bill took his daughter to the hospital, because she looked pale and sick in the morning. I was very tired last night. I went to sleep earlier than usual. The presence of explicit cues makes it easier for humans to infer discourse relatio"
W12-4703,C10-2118,0,0.328521,"ording to our world knowledge. Although all words in the arguments contribute to the shaping of the relation, discourse cues as defined in the literature typically refer to a specific category of words or phrases which have an operator-like function in the discourse level. For example, Stede (2011) distinguishes discourse connectives as closed-class, non-inflectable words or word groups syntactically from adverbial, subordinate/coordinate conjunction, or preposition categories which themselves can only be interpreted successfully when they appear in a relation between two discourse segments. (Prasad et al., 2010), however, suggest that a variety of expressions exist that mark discourse relations, but they are not from the typically-considered syntactic categories, and in some places they are not even structurally frozen (e.g., “that would follow”). Whatever syntactic or sematic function a discourse cue is associated with, the relative frequency of its occurrence in a particular type of discourse relation is what makes it interesting. Our focus is not on the structural properties of a discourse marker, but instead on the strength of the marker for indicating a specific discourse relation. Given a segme"
W12-4703,C10-2172,0,0.12435,"is a meaningful term in a simple probabilistic modeling of relation identification. Strength values can be calculated directly from the distribution of the discourse cues in a given corpus. Indeed, such a term should be used in a clear formulation along with the prior probability of the relation, i.e., the general expectation for a particular relation. Researchers have examined the classification of explicit and implicit discourse relations by only looking at the most typical relation that a discourse connective marks and obtained good accuracy for a coarse classification(Pitler et al., 2008; Zhou et al., 2010). To get an acceptable result for identification of fine-grained relation senses, one should definitely look into the strength of the discourse cue as some of them might not be reliable markers in a given context. Furthermore, it has been found that implicit relations are very difficult to classify correctly when learning only on explicit discourse relations (Sporleder, 2008). We would expect that weakly marked relations are similar to the unmarked relations; hence, one could possibly make use of this subset of explicit relations as training data for identification of implicit relations and ge"
W13-2607,P10-1021,1,0.860426,"n the result is adjusted by a set of functions that take into account conflicts between the models. In relation to the approach proposed here, it is also important to note that the semantic components in (Pad´o et al., 2009; Jurafsky, 2002) are limited to semantic role information, while the architecture proposed in this paper can build complete semantic expressions for a sentence. Furthermore, these models do not model the prediction and verification process (in particular, they do not make any semantic role predictions of upcoming input) which has been observed in human language processing. Mitchell et al. (2010) propose an integrated measure of syntactic and semantic surprisal as a model of processing difficulty, and show that the semantic component improves modelling results over a syntax-only model. However, the syntactic and semantic surprisal components are only very loosely integrated with one another, as the semantic model is a distributional bag-of-words model which does not take syntax into account. Finally, the syntactic model underlying (Pad´o et al., 2009; Mitchell et al., 2010) is an incremental top-down PCFG parser (Roark, 2001), which due to its parsing strategy fails to predict human p"
W13-2607,W07-1210,0,0.0316795,"l. 4 Neo-Davidsonian semantics PLTAG syntax PLTAG uses the standard operations of TAG: substitution and adjunction. The order in which they are applied during a parse is constrained by in59 S {∃e&? = e} incrementality-friendly characteristic1 . The NeoDavidsonian representation allows us separate the semantic prediction of a role from its syntactic fulfillment, permitting the type of flexible framework we are proposing in this paper. We adopt a neo-Davidsonian approach to semantics by a formalism that bears similarity to existing frameworks such as (R)MRS (Robust Minimal Recursion Semantics) (Copestake, 2007). However, this paper is intended to explore what architecture is minimally required to augment the PLTAG syntactic framework, so we do not adopt these existing frameworks wholesale. Our examples such as figures 4, 5d, and several others demonstrate how this looks in practice. 6 NP↓ {Q1 x1 ARG0(e, x1 )} VP {e} NP* VP {Q1 x1 {e} ARG0(e, x1 ) &? = x1 V NP↓ V NP↓ &? = Q1 } likes {Q2 x2 including {Q2 x2 {Like(e)} ARG1(e, x2 )} {Include(e)} ARG1(e, x2 )} Figure 1: Verbal elementary trees extracted from example sentence Pete likes sugary drinks including alcoholic ones. minimally-needed connecting s"
W13-2607,W08-2304,1,0.75843,"n agent nor a recipient under normal circumstances. Likewise, “the man” is not a typical patient in this situation. If there is a psycholinguistic effect of semantic plausibility, we would expect that an incomplete sentence like “The woman slid the butter” would generate an expectation in the listener of a PO construction (rather than DO) with preposition “to”, as well as an expectation of a noun phrase and an expectation that that noun phrase would belong to the class of entities that are plausible recipients for entities that are slid. Introduction PLTAG (PsychoLinguistically-motivated TAG, Demberg and Keller, 2008; Demberg et al., 2014) is a variant of Tree-Adjoining Grammar (TAG) which is designed to allow the construction of TAG parsers that enforce strict incrementality and full connectedness through (1) constraints on the order of operations, (2) a new type of unlexicalized tree, so-called prediction trees, and (3) a verification mechanism that matches up and extends predicted structures with later evidence. Psycholinguistic evaluation has shown that PLTAG operations can be used to predict data from eyetracking experiments, lending this syntactic formalism greater psycholinguistic support. If this"
W13-2607,C92-1032,0,0.386932,"and show that the semantic component improves modelling results over a syntax-only model. However, the syntactic and semantic surprisal components are only very loosely integrated with one another, as the semantic model is a distributional bag-of-words model which does not take syntax into account. Finally, the syntactic model underlying (Pad´o et al., 2009; Mitchell et al., 2010) is an incremental top-down PCFG parser (Roark, 2001), which due to its parsing strategy fails to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson et al., 1991; Resnik, 1992). Using the PLTAG parsing model is thus more psycholinguistically adequate. crementality. This also implies that, in addition to the standard operations, there are reverse Up versions of these operations where the prefix tree is substituted or adjoined into a new elementary tree (see figure 4). In order to achieve strict incrementality and full connectedness at the same time while still using linguistically motivated elementary trees, PLTAG has an additional type of (usually) unlexicalized elementary tree called prediction trees. Each node in a prediction tree is marked with upper and/or lower"
W13-2607,J01-2004,0,0.0433454,"h has been observed in human language processing. Mitchell et al. (2010) propose an integrated measure of syntactic and semantic surprisal as a model of processing difficulty, and show that the semantic component improves modelling results over a syntax-only model. However, the syntactic and semantic surprisal components are only very loosely integrated with one another, as the semantic model is a distributional bag-of-words model which does not take syntax into account. Finally, the syntactic model underlying (Pad´o et al., 2009; Mitchell et al., 2010) is an incremental top-down PCFG parser (Roark, 2001), which due to its parsing strategy fails to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson et al., 1991; Resnik, 1992). Using the PLTAG parsing model is thus more psycholinguistically adequate. crementality. This also implies that, in addition to the standard operations, there are reverse Up versions of these operations where the prefix tree is substituted or adjoined into a new elementary tree (see figure 4). In order to achieve strict incrementality and full connectedness at the same time while still using linguistically motivated el"
W13-2607,W12-4608,1,0.811164,"n in the context of our formalism (Koller et al., 2003). &∃x1 Person(x1 )&? = x1 &? = ∃ &Q0 = ∃&x0 = x1 (Result of QuantEquate and VarEquate) Ψ = ∃e∃x0 ARG0(e, x0 )&Person(x0 ) (Result of Resolve) Figure 4: An example incremental step from the semantic perspective. and semantic well-formedness that must be handled post hoc at every step. For example, universal quantifiers require a distinction to be made between the restrictor of the quantified variable and the nuclear scope. It is possible within a neoDavidsonian representation to perform such representational adjustments easily, as shown by Sayeed and Demberg (2012). Example Now that we have described the procedure, we provide an example of how this semantic augmentation of PLTAG can represent role labeling and prediction inside the syntactic parsing system. We perform a relevant segment of the parse of example (1-a), “The woman slid the butter to the man.” In this sentence, we expect that the parser will already know the expected role of the NP “the man” before it actually receives it. That is, it will know in advance that there is an upcoming NP to be predicted such that it is compatible with a recipient (ARG2) role, and this knowledge will be represen"
W13-2607,P91-1012,0,0.0715508,"processing difficulty, and show that the semantic component improves modelling results over a syntax-only model. However, the syntactic and semantic surprisal components are only very loosely integrated with one another, as the semantic model is a distributional bag-of-words model which does not take syntax into account. Finally, the syntactic model underlying (Pad´o et al., 2009; Mitchell et al., 2010) is an incremental top-down PCFG parser (Roark, 2001), which due to its parsing strategy fails to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson et al., 1991; Resnik, 1992). Using the PLTAG parsing model is thus more psycholinguistically adequate. crementality. This also implies that, in addition to the standard operations, there are reverse Up versions of these operations where the prefix tree is substituted or adjoined into a new elementary tree (see figure 4). In order to achieve strict incrementality and full connectedness at the same time while still using linguistically motivated elementary trees, PLTAG has an additional type of (usually) unlexicalized elementary tree called prediction trees. Each node in a prediction tree is marked with upp"
W13-2607,P07-1031,0,0.0117582,"ticle verbs like show up and some handcoded constructions in which the first part is predictive of the second part, such as either . . . or or both . . . and. Here we extend this set of trees with Semantics for PLTAG 6.1 NP {∃e} Semantic augmentation for the lexicon Constructing the lexicon for a semantically augmented PLTAG uses a process based on the one for “purely syntactic” PLTAG. The PLTAG lexicon is extracted automatically from the PLTAG treebank, which has been derived from the Penn Treebank using heuristics for binarizing flat structures as well as additional noun phrase annotations (Vadas and Curran, 2007), PropBank (Palmer et al., 2003), and a slightly modified version of the head percolation table of Magerman (1994). PLTAG trees in the treebank are annotated with syntactic headedness information as well as information that allows one to distinguish arguments and modifiers. Given the PLTAG treebank, we extract the canonical lexicon using well-established approaches from the LTAG literature (in particular (Xia et al., 2000): we traverse the converted tree from each leaf up towards the root, as long as the parental node is the head child of its parent. If a subtree is not the head child of its p"
W13-2607,W00-1307,0,0.525189,"y from the PLTAG treebank, which has been derived from the Penn Treebank using heuristics for binarizing flat structures as well as additional noun phrase annotations (Vadas and Curran, 2007), PropBank (Palmer et al., 2003), and a slightly modified version of the head percolation table of Magerman (1994). PLTAG trees in the treebank are annotated with syntactic headedness information as well as information that allows one to distinguish arguments and modifiers. Given the PLTAG treebank, we extract the canonical lexicon using well-established approaches from the LTAG literature (in particular (Xia et al., 2000): we traverse the converted tree from each leaf up towards the root, as long as the parental node is the head child of its parent. If a subtree is not the head child of its parent, we extract it as an elementary tree and proceed in this way for each word of the converted tree. Given the argument/modifier distinction, we then create substitution nodes in the parent tree for arguments or a root and foot node in the child tree for modifiers. Prediction trees are extracted automatically by calculating the minimal amount of structure needed to connect each word into a structure including all previo"
W13-2607,P03-1047,0,0.0332397,"mbine these representations during incremental parsing in a manner fully synchronized with the existing PLTAG syntactic operations. We demonstrated that we can represent thematic role prediction in a case that is known to be relevant to an on-going stream of psycholinguistic research. Ongoing and future work includes the development of a joint syntacticsemantic statistical model for PLTAG and experimental validation of predictions made by our semantic augmentation. We are also considering higher-order semantic issues such as quantifier scope underspecification in the context of our formalism (Koller et al., 2003). &∃x1 Person(x1 )&? = x1 &? = ∃ &Q0 = ∃&x0 = x1 (Result of QuantEquate and VarEquate) Ψ = ∃e∃x0 ARG0(e, x0 )&Person(x0 ) (Result of Resolve) Figure 4: An example incremental step from the semantic perspective. and semantic well-formedness that must be handled post hoc at every step. For example, universal quantifiers require a distinction to be made between the restrictor of the quantified variable and the nuclear scope. It is possible within a neoDavidsonian representation to perform such representational adjustments easily, as shown by Sayeed and Demberg (2012). Example Now that we have des"
W13-2607,J05-1004,0,\N,Missing
W13-2607,J13-4008,1,\N,Missing
W13-2610,C12-1163,1,0.483697,"urse processing emphasizes on the procedural role of the connectives to constrain the way readers relate the propositions in a text (Blakemore, 1992; Blass, 1993). Experimental findings suggest that these markers can facilitate the inference of specific discourse relations (Degand and Sanders, 2002), and that discourse connectors are processed incrementally K¨ohne and Demberg (2013). People can however infer discourse relations also in the absence of discourse connecotrs, relying on the propositional content of the sentences and their world-knowledge (Hobbs, 1979; Asher and Lascarides, 1998). Asr and Demberg (2012b) point out that similar inferences are also necessary for discourse relations which are only marked with a weak connector which can be used for many relations, such as and. Furthermore, we know that the inference of discourse relations is affected by a set of general cognitive biases. To illuminate the role of these factors let’s have a look at (1). While the type of relation between the two events is clearly inferable in (1-a) and (1-b) due to the discourse connectives, in (1-c), the reader would have to access their knowledge, e.g., about Harry (from larger context) or the usual affairs be"
W13-2610,P11-1100,0,0.0557444,"Missing"
W13-2610,P10-1021,1,0.849003,"se hierarchy. This will open a way to the computational modeling of discourse processing. 1 Introduction A central question in psycholinguistic modeling is the development of models for human sentence processing difficulty. An approach that has received a lot of interest in recent years is the information-theoretic measure of surprisal (Hale, 2001). Recent studies have shown that surprisal can successfully account for a range of psycholinguistic effects (Levy, 2008), as well as account for effects in naturalistic broad-coverage texts (Demberg and Keller, 2008; Roark et al., 2009; Frank, 2009; Mitchell et al., 2010). : what work of Roark and Frank you mean here? Under the notion of the Uniform Information Density hypothesis (UID, Levy and Jaeger, 2007; Frank and Jaeger, 2008), surprisal has also been used to explain choices in language production: When their language gives people the option to choose between different linguistic encodings, people tend to choose the encod84 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 84–93, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics neutral connective, i.e., and is used. Levinson (2000) note"
W13-2610,N01-1021,0,0.371223,"ers Fatemeh Torabi Asr MMCI Cluster of Excellence Saarland University Germany fatemeh@coli.uni-saarland.de Vera Demberg MMCI Cluster of Excellence Saarland University Germany vera@coli.uni-saarland.de Abstract ing that distributes the information more uniformly across the sentence (where the information conveyed by a word is its surprisal). When using surprisal as a cognitive model of processing difficulty, we hypothesize that the processing difficulty incurred by the human when processing the word is proportional to the update of the interpretation, i.e. the information conveyed by the word (Hale, 2001; Levy, 2008). We can try to estimate particular aspects of the information conveyed by a word, e.g., the information conveyed about the syntactic structure of the sentence, the semantic interpretation, or about discourse relations within the text. This paper does not go all the way to proposing a model of discourse relation surprisal, but discusses first steps towards a model for the information conveyed by discourse connectors about discourse relations, based on available resources like the Penn Discourse Treebank (Prasad et al., 2008). First, we quantify how unambiguously specific discourse"
W13-2610,W12-1614,0,0.115847,"Missing"
W13-2610,prasad-etal-2008-penn,0,0.347131,"enhancement01 (because) describes how much information gain because provides for distinguishing the level-1 relations it marks from other relations. Similarly, high enhancement23 (because) indicates that this connective is important for distinguishing among level 3 relations (here, distinguishing CONTINGENCY.Cause.reason from CONTINGENCY.Cause.result relations), while low enhancement23 (if ) indicates that if does not contribute almost any information for distinguishing among the subtypes of the CONTINGENCY.Condition relation. 2 Note that this formulation is closely related to surprisal: Levy (2008) shows that surprisal(wk+1 ) = − log P (wk+1 |w1 ..wk ) is equivalent to the KL divergence D(P (T |w1..j+1 )||P (T |w1..j )) for “any stochastic generative process P , conditioned on some (possibly null) external context, that generates complete structures T , each consisting at least partly of surface strings to be identified with serial linguistic input”. Note however that in our current formulation of a discourse relation, the simplification to general structure-independent surprisal does not hold (DKL (p(r|c)||p(r)) 6= − log p(c)) because our relations (as they are defined here) do not sat"
W13-2610,D09-1034,0,0.0231297,"ifferent levels of a relation sense hierarchy. This will open a way to the computational modeling of discourse processing. 1 Introduction A central question in psycholinguistic modeling is the development of models for human sentence processing difficulty. An approach that has received a lot of interest in recent years is the information-theoretic measure of surprisal (Hale, 2001). Recent studies have shown that surprisal can successfully account for a range of psycholinguistic effects (Levy, 2008), as well as account for effects in naturalistic broad-coverage texts (Demberg and Keller, 2008; Roark et al., 2009; Frank, 2009; Mitchell et al., 2010). : what work of Roark and Frank you mean here? Under the notion of the Uniform Information Density hypothesis (UID, Levy and Jaeger, 2007; Frank and Jaeger, 2008), surprisal has also been used to explain choices in language production: When their language gives people the option to choose between different linguistic encodings, people tend to choose the encod84 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 84–93, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics neutral connective, i."
W13-2610,W13-0124,0,0.0869823,"discriminative feature through the hierarchical classification of the relations. This work is a first step towards the computational modeling of the discourse processing with respect to the linguistic markers of the abstract discourse relations. In future work, we would like to look at the contribution of different types of relational markers including sentence connectives, sentiment words, implicit causality verbs, negation markers, event modals etc., which in the laboratory setup have proven to affect the expectation of the readers about an upcoming discourse relation (Kehler et al., 2008; Webber, 2013). Discourse Relation Hierarchy and Feature Space Dimensions Psycholingusitic models that need to be trained on annotated data from computational linguistics resources also have to be concerned about the psycholinguistic adequacy of the annotation. In particular, for a model of discourse relation surprisal, we need to ask which discourse relations are relevant to humans, and which distinctions between relations are relevant to them? For example, it may be possible that the distinction between cause and consequence (3rd level PDTB hierarchy) is more important in the inference process than the di"
W15-0117,C12-1163,1,0.933655,"Missing"
W15-0117,W13-2610,1,0.895663,"urse connectives are know as the best markers of discourse relations, empirical studies such as Knott (1996) and Asr and Demberg (2012b; 2013) indicate that connective types vary a lot in 1 Other than explicit and implicit discourse relations, PDTB contains a set of other types of relations which are not considered here. 2 Interested readers are referred to Prasad et al. (2008) for more details on the annotation guidelines. 120 terms of the granularity and amount of relational information they encode and this goes beyond the typical coarse-grained categorization of connectives. In particular, Asr and Demberg (2013) show that instead and because are highly informative (or strongly constraining) markers which should play an important role in identification of very specific discourse relations, whereas and and but occur in a wide range of contexts, thus do not disambiguate the relation down to a very specific sense. 2.3 Other relational cues Even when connectives are absent or do not exhibit enough information about the relation sense between two consecutive discourse units, readers can still infer relations by relying on their world-knowledge and the information encoded in the arguments of the relation; S"
W15-0117,D12-1033,1,0.832689,"at speakers indeed behave in this way, and choose among meaning-equivalent alternatives the ones that correspond to a more uniform rate of information transmission has been provided by a range of experimental and corpus-based studies at the level of spoken word duration and articulation (Aylett and Turk, 2004; Buz et al., 2014), morphology (Kurumada and Jaeger, 2013), syntax (Jaeger, 2010), lexical choices (Piantadosi et al., 2011; Mahowald et al., 2013), referring expressions (Tily and Piantadosi, 2009; Kravtchenko, 2014), and across levels, e.g., effect from syntax on spoken word durations (Demberg et al., 2012). We here investigate whether UID can also explain discourse-level phenomena, such as the insertion vs. omission of discourse connectives. Some first evidence for this hypothesis comes from Asr and Demberg (2012a), who looked into the discourse-relation annotated Penn Discourse Treebank (PDTB, Prasad et al. 2008) corpus to explore what relation senses tend to be expressed without discourse connectives. Asr and Demberg observe that discourse relations which are predictable given general cognitive biases in relation interpretation, such as continuity and causality are more likely to be expressed"
W15-0117,D09-1036,0,0.0467357,"on the discovery of other types of relational markers in natural text (Prasad et al., 2010; Das and Taboada, 2013; Duque, 2013; Webber, 2013). The obtained annotations throughout these studies indicate that, in fact, a lot of discourse relations benefit from other types of cues besides explicit discourse connectives. Furthermore, machine learning attempts for detection of implicit discourse relations reveal that lexical, syntactic and clause-level properties of the arguments can determine the relation sense with good accuracy, at least for a coarse-grained classification (Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). 2.4 Implicitness Regarding the question of implicitness, Asr and Demberg (2012a, 2013) study causal and continuous discourse relations which based on cognitive theories and lab experiments tend to be eagerly inferred by readers when sentences are encountered consecutively. In both studies, Asr and Demberg extract different senses of relations from PDTB and find a higher degree of implicitness (proportion of the implicit to the explicit occurrences) for causal and continuous relation senses compared with other types of relat"
W15-0117,W12-1614,0,0.0479066,"lational markers in natural text (Prasad et al., 2010; Das and Taboada, 2013; Duque, 2013; Webber, 2013). The obtained annotations throughout these studies indicate that, in fact, a lot of discourse relations benefit from other types of cues besides explicit discourse connectives. Furthermore, machine learning attempts for detection of implicit discourse relations reveal that lexical, syntactic and clause-level properties of the arguments can determine the relation sense with good accuracy, at least for a coarse-grained classification (Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). 2.4 Implicitness Regarding the question of implicitness, Asr and Demberg (2012a, 2013) study causal and continuous discourse relations which based on cognitive theories and lab experiments tend to be eagerly inferred by readers when sentences are encountered consecutively. In both studies, Asr and Demberg extract different senses of relations from PDTB and find a higher degree of implicitness (proportion of the implicit to the explicit occurrences) for causal and continuous relation senses compared with other types of relations, e.g. comparison or backward temporal"
W15-0117,D13-1094,0,0.141739,"Missing"
W15-0117,P09-1077,0,0.104508,"have been conducted on the discovery of other types of relational markers in natural text (Prasad et al., 2010; Das and Taboada, 2013; Duque, 2013; Webber, 2013). The obtained annotations throughout these studies indicate that, in fact, a lot of discourse relations benefit from other types of cues besides explicit discourse connectives. Furthermore, machine learning attempts for detection of implicit discourse relations reveal that lexical, syntactic and clause-level properties of the arguments can determine the relation sense with good accuracy, at least for a coarse-grained classification (Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). 2.4 Implicitness Regarding the question of implicitness, Asr and Demberg (2012a, 2013) study causal and continuous discourse relations which based on cognitive theories and lab experiments tend to be eagerly inferred by readers when sentences are encountered consecutively. In both studies, Asr and Demberg extract different senses of relations from PDTB and find a higher degree of implicitness (proportion of the implicit to the explicit occurrences) for causal and continuous relation senses compared with ot"
W15-0117,prasad-etal-2008-penn,0,0.759418,"ission. Optional discourse markers should thus be omitted if they would lead to a trough in information density, and be inserted in order to avoid peaks in information density. We here test this hypothesis by observing how far a specific cue, negation in any form, affects the discourse relations that can be predicted to hold in a text, and how the presence of this cue in turn affects the use of explicit discourse connectives. 1 Introduction Discourse connectives are known as optional linguistic elements to construct relations between clausal units in text: in the Penn Discourse Treebank (PDTB Prasad et al. 2008), only about half of the annotated discourse relations are marked in the text by a discourse connective. In the remaining cases, the discourse relation can still be recovered without an explicit marker. Consider for example the sentences below, which stand in a causal relationship: (1) a. b. John did not go to the concert. He was ill. John did not go to the concert, because he was ill. The question we would like to discuss in this paper is whether there is a principled explanation of when such optional discourse markers are inserted by speakers / writers, and when they are omitted. In Grician"
W15-0117,C10-2118,0,0.0149549,"specifically shows that people predict the relation between two sentences as soon as they encounter a cue, be it the discourse connective or other linguistic cues yet within the first clause. Implicit causality verbs, such as admire in the above example, trigger expectation for a causal continuation and given the time course of the effect in Rohde and Horton’s experiment one can infer that the connective (in this case because) would then be a redundant operator. Recently, a few systematic corpus studies have been conducted on the discovery of other types of relational markers in natural text (Prasad et al., 2010; Das and Taboada, 2013; Duque, 2013; Webber, 2013). The obtained annotations throughout these studies indicate that, in fact, a lot of discourse relations benefit from other types of cues besides explicit discourse connectives. Furthermore, machine learning attempts for detection of implicit discourse relations reveal that lexical, syntactic and clause-level properties of the arguments can determine the relation sense with good accuracy, at least for a coarse-grained classification (Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). 2.4"
W15-0117,E14-1068,0,0.0218861,"ural text (Prasad et al., 2010; Das and Taboada, 2013; Duque, 2013; Webber, 2013). The obtained annotations throughout these studies indicate that, in fact, a lot of discourse relations benefit from other types of cues besides explicit discourse connectives. Furthermore, machine learning attempts for detection of implicit discourse relations reveal that lexical, syntactic and clause-level properties of the arguments can determine the relation sense with good accuracy, at least for a coarse-grained classification (Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). 2.4 Implicitness Regarding the question of implicitness, Asr and Demberg (2012a, 2013) study causal and continuous discourse relations which based on cognitive theories and lab experiments tend to be eagerly inferred by readers when sentences are encountered consecutively. In both studies, Asr and Demberg extract different senses of relations from PDTB and find a higher degree of implicitness (proportion of the implicit to the explicit occurrences) for causal and continuous relation senses compared with other types of relations, e.g. comparison or backward temporal relations. This finding ha"
W15-0117,W13-0124,0,0.505648,"en two sentences as soon as they encounter a cue, be it the discourse connective or other linguistic cues yet within the first clause. Implicit causality verbs, such as admire in the above example, trigger expectation for a causal continuation and given the time course of the effect in Rohde and Horton’s experiment one can infer that the connective (in this case because) would then be a redundant operator. Recently, a few systematic corpus studies have been conducted on the discovery of other types of relational markers in natural text (Prasad et al., 2010; Das and Taboada, 2013; Duque, 2013; Webber, 2013). The obtained annotations throughout these studies indicate that, in fact, a lot of discourse relations benefit from other types of cues besides explicit discourse connectives. Furthermore, machine learning attempts for detection of implicit discourse relations reveal that lexical, syntactic and clause-level properties of the arguments can determine the relation sense with good accuracy, at least for a coarse-grained classification (Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). 2.4 Implicitness Regarding the question of implicitnes"
W15-0117,C10-2172,0,0.01368,"f other types of relational markers in natural text (Prasad et al., 2010; Das and Taboada, 2013; Duque, 2013; Webber, 2013). The obtained annotations throughout these studies indicate that, in fact, a lot of discourse relations benefit from other types of cues besides explicit discourse connectives. Furthermore, machine learning attempts for detection of implicit discourse relations reveal that lexical, syntactic and clause-level properties of the arguments can determine the relation sense with good accuracy, at least for a coarse-grained classification (Pitler et al., 2009; Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). 2.4 Implicitness Regarding the question of implicitness, Asr and Demberg (2012a, 2013) study causal and continuous discourse relations which based on cognitive theories and lab experiments tend to be eagerly inferred by readers when sentences are encountered consecutively. In both studies, Asr and Demberg extract different senses of relations from PDTB and find a higher degree of implicitness (proportion of the implicit to the explicit occurrences) for causal and continuous relation senses compared with other types of relations, e.g. comparis"
W15-1106,J10-4006,0,0.743359,"rb and its argument involves calculating typical role-fillers of that verb, calculating a centroid (or average) over the most typical role-fillers in a vector space model, and then calculating the similarity between the centroid 48 Proceedings of CMCL 2015, pages 48–57, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics and the proposed role-filler via a similarity measure. Arguments that have high similarities with the prototypical centroid vector representing most common role-fillers for a given verb-role combination are then asserted to have good thematic fit (Baroni and Lenci, 2010; Erk, 2012). This conceptualization, however, assumes that there is a single type of most typical filler for a role and that all good fillers will be distributionally similar. This assumption leads to problems when this process is to be applied to ambiguous verbs; when a verb has many different senses, there can exist typical role-fillers for each sense which are all highly suitable role-fillers for the given role but are distributionally very different from one another. This means that the calculated prototypical role-filler will be a mixture of the arguments that are typical role-fillers fo"
W15-1106,P07-1071,0,0.0999295,"agents vs. patients of a verb). The TypeDM model1 (Baroni and Lenci, 2010) is constructed from the ukWaC, BNC, and WaCkype1 Available at http://clic.cimec.unitn.it/dm/. dia corpora. In TypeDM, the links represent both connections between words in the corpora found via the dependency parser MaltParser (Nivre et al., 2007) and further semantic dependencies derived from these connections via hand-crafted rules. An alternative way of constructing DMs was proposed by Sayeed and Demberg (2014), where links between words are derived directly from SENNA, a neural network-based semantic role labeller (Collobert and Weston, 2007; Collobert et al., 2011). This DM is called SENNA-DepDM, or SDDM for short in this paper. Unlike TypeDM, the links in this tensor are not processed by hand-crafted rules. SDDMX2 is a version of SDDM with one expansion: it includes additional links between role-fillers that are found to be related via a verb. Both SDDM and SDDMX are trained on ukWaC and the BNC. Greenberg et al. (2015) tested TypeDM, SDDM, and SDDMX on multiple datasets of human judgements for agent, patient, location, and instrument roles. They used multiple models and datasets because robustness of trends across these differ"
W15-1106,J10-4007,0,0.339384,"Missing"
W15-1106,N15-1003,1,0.834689,"Missing"
W15-1106,E09-1094,0,0.241577,"te thematic fit (e.g., is cake a good patient of cut?) can be useful both for a wide range of NLP applications and for cognitive models of human language processing difficulty, as human processing difficulty is highly sensitive to semantic plausibilities (Ehrlich and Rayner, 1981). Previous studies obtained quantitative thematic fit data by asking human participants to rate how common, plausible, typical, or appropriate some test role-fillers are for given verbs on a scale from 1 (least plausible) to 7 (most plausible) (McRae et al., 1998; Binder et al., 2001; Pad´o, 2007; Pad´o et al., 2009; Vandekerckhove et al., 2009). For example, While these datasets are very useful, e.g. for evaluating automatic systems for estimating thematic fit via correlations with these human judgements, they do not systematically vary polysemy of verbs or frequency of role-fillers. Further, it is unclear what effect polysemy and frequency have on thematic fit judgements. We thus ask: (1) are thematically well-fitting role-fillers for more polysemous verbs (e.g., “execute killer” or “execute will”) judged to be equally well-fitting as thematically well-fitting role-fillers for less polysemous verbs (“jail criminal”)? (2) Is a proto"
W15-4718,J11-3003,1,0.888211,"Missing"
W15-4718,W08-0130,0,0.049453,"Missing"
W15-4718,N01-1021,0,0.0148096,"ow which utterance is a “complex” one? We can draw on psycholinguistic models of human sentence processing difficulty, such as dependency locality theory (measuring dependency lengths within the sentence; longer dependencies are more difficult), information density (measuring surprisal – the amount of information conveyed in a certain time unit; a higher rate of information per time unit is more difficult) or words-per-concept (how many words are used to convey a concept). In this paper, we focus on the measure of information density, which uses the information-theoretic measure of surprisal (Hale, 2001; Levy, 2008), as well as the ratio of concepts per words. Our aim is to flexibly generate utterances that differ in information density, producing high-density and low-density formulations for the same underlying semantic representation. We evaluate different parametrisations of our approach by evaluating how many different high vs. low density utterances can be generated. We additionally present judgments from human evaluators rating both grammaticality and meaningfulness. We collect a small corpus of utterances from the target domain and have them annotated by naive participants with a very"
W15-4718,P10-1157,0,0.0564733,"Missing"
W15-4718,2007.mtsummit-ucnlg.4,0,0.0555371,"terances that are comprehensible and convey the intended meaning, but language that is adaptive to different users as well as situations (see also (Dethlefs, 2014) for an overview). Adaptation can happen at different levels, concerning content as well as the formulation of generated sentences. We here focus only on sentence formulation with the goal of being able to automatically generate a large variety of different realisations of a given semantic representation. Our study explores the combination of a data-driven approach (Mairesse et al., 2010) with a grammar-based approach using OpenCCG (White et al., 2007). The use of templates is a common and well-performing approach to natural language generation. Usually, either the generation process consists of selecting appropriate fillers for manually-built patterns, or the semantic specification constrains the allowable surface constructions so strongly that it effectively constitutes a form of template as well. While such approaches do guarantee grammaticality when templates (or grammars, respectively) are well-designed, the amount of formulation variation that can be generated based on templates is either very low, or requires a huge manual Proceeding"
W16-2518,P14-1023,0,0.547628,"contains: BL2010 Results from the TypeDM system of Baroni and Lenci (2010). This space is constructed from counts of rule-selected dependency tree snippets taken from a large web crawl corpus, adjusted via local mutual information (LMI) but is otherwise unsupervised. The approach they take generates a vector space above a 100 million dimensions. The top 20 typical role-fillers by LMI are chosen for prototype construction. Some of the datasets presented were only created and tested later by Sayeed et al. (2015) (*) and Greenberg et al. (2015a) (**). BDK2014 Tests of word embedding spaces from Baroni et al. (2014), constructed via word2vec (Mikolov et al., 2013a). These are the best systems reported in their paper. The selection of typical role-fillers for constructing the prototype role-filler comes from TypeDM, which is not consulted for the vectors themselves. Evaluation approaches The dominant approach in recent work in thematic fit evaluation has been, given a verb/role/noun combination, to use the vector space to construct a prototype filler of the given role for the given verb, and then to compare the given noun to that prototype (Baroni and Lenci, 2010). The prototype fillers are constructed by"
W16-2518,J10-4006,0,0.899679,"words may rarely appear in the same discourses, swords and knives sometimes may. A robust vector space allows the representation of unseen indirect associations between these items. In order to understand the progress made on the thematic fit question, we therefore look at a sample of recent attempts at exploring the feature space and the handling of the vector space as a whole. Comparing recent results In table 2, we sample results from recent vector-space modeling efforts in the literature in order to understand the progress made. The table contains: BL2010 Results from the TypeDM system of Baroni and Lenci (2010). This space is constructed from counts of rule-selected dependency tree snippets taken from a large web crawl corpus, adjusted via local mutual information (LMI) but is otherwise unsupervised. The approach they take generates a vector space above a 100 million dimensions. The top 20 typical role-fillers by LMI are chosen for prototype construction. Some of the datasets presented were only created and tested later by Sayeed et al. (2015) (*) and Greenberg et al. (2015a) (**). BDK2014 Tests of word embedding spaces from Baroni et al. (2014), constructed via word2vec (Mikolov et al., 2013a). The"
W16-2518,N13-1090,0,0.128918,"er and how we can find judgements that are a suitable gold standard for evaluating automatic systems. We seek in this paper to shed some light on the aspects of this problem relevant to vector-space word representation and to highlight the evaluation data currently available for this task. This task differs from other ways of evaluating word representations because it focuses partly on the psychological plausibility of models of predicate-argument function application. Analogy task evaluations, for example, involve comparisons of word representations that are similar in their parts of speech (Mikolov et al., 2013b). Here we are evaluating relations between words that are “counterparts” of one another and that exist overall in complementary distribution to one another. There are other forms of evaluation that attempt to replicate role assignments or predict more plausible role-fillers given observed text data (Van de Cruys, 2014), but this does not directly capture human biases as to plausibility: infrequent predicate-argument combinations can nevertheless have high human ratings. Consequently, we view this task as a useful contribution to the family of evaluations that would test different aspects of"
W16-2518,W15-1106,1,0.792003,"dataset of 414 agent and patient ratings (P07) to be included in a sentence processing model. The verbs were chosen based on their frequencies in the Penn Treebank and FrameNet. Role-fillers were selected to give a wide distribution of scores within each verb. The final dataset contains fine-grained distinctions from FrameNet, which many systems map to familiar agent and patient roles. Judgements were obtained on a seven point scale using questions of the form “How common is it for an analyst to tell [something]?” (subject) and “How common is it for an analyst to be told?” (object). Finally, Greenberg et al. (2015a) created a dataset of 720 patient ratings (GDS-all) that were designed to be different from the others in two ways. First, they changed the format of the judgement elicitation question, since they believed that asking how common/typical something is would lead the participants to consider frequency of occurrence rather than semantic plausibility. Instead, they asked participants how much they agreed on a 1-7 scale with statements such as “cream is something that is whipped”. This dataset was constructed to vary word frequency and verb polysemy systematically; the experimental subset of the d"
W16-2518,N15-1003,1,0.473545,"dataset of 414 agent and patient ratings (P07) to be included in a sentence processing model. The verbs were chosen based on their frequencies in the Penn Treebank and FrameNet. Role-fillers were selected to give a wide distribution of scores within each verb. The final dataset contains fine-grained distinctions from FrameNet, which many systems map to familiar agent and patient roles. Judgements were obtained on a seven point scale using questions of the form “How common is it for an analyst to tell [something]?” (subject) and “How common is it for an analyst to be told?” (object). Finally, Greenberg et al. (2015a) created a dataset of 720 patient ratings (GDS-all) that were designed to be different from the others in two ways. First, they changed the format of the judgement elicitation question, since they believed that asking how common/typical something is would lead the participants to consider frequency of occurrence rather than semantic plausibility. Instead, they asked participants how much they agreed on a 1-7 scale with statements such as “cream is something that is whipped”. This dataset was constructed to vary word frequency and verb polysemy systematically; the experimental subset of the d"
W16-2518,D14-1004,0,0.123181,"Missing"
W16-2518,E09-1094,0,0.44686,"fils the selectional preference of a verb given a role. This can be quantified in thematic fit ratings, human judgements that apply to combinations of verb, role, and noun1 . One of the goals of this type of evaluation is both for cognitive modeling and for future application. From a cognitive modeling perspective, thematic fit judgements offer a window into the decision-making process of language users in assigning semantic representations to complex expressions. Psycholinguistic work has shown that these introspective judgements map well to underlying processing notions (Pad´o et al., 2009; Vandekerckhove et al., 2009). One of our goals in developing this type of evaluation is to provide another method of testing systems designed for applications in which predicateargument relations may have a significant effect on performance, especially in user interaction. This particularly applies in tasks where non-local dependencies have semantic relevance, for example, such as in judging the plausibility of a candidate coreferent from elsewhere in the discourse. Such applications include statistical sentence generation in spoken dialog contexts, where systems must make plausible lexical choices in context. This is pa"
W16-2518,W11-0607,0,\N,Missing
W17-0803,J08-4004,0,0.24745,"Computer Science Language Science and Technology Saarland University Saarland University Saarbr¨ucken, Germany Saarbr¨ucken, Germany {m.c.j.scholman,vera}@coli.uni-saarland.de Abstract the standard practice of using two trained, expert annotators to code data. Not only is this procedure time-consuming and therefore costly, it also raises questions regarding the reliability and validity of the data. When using trained, expert annotators, they may agree because they share implicit knowledge and know the purpose of the research well, rather than because they are carefully following instructions (Artstein and Poesio, 2008; Riezler, 2014). Krippendorff (2004) therefore notes that the more annotators participate in the process and the less expert they are, the more likely they can ensure the reliability of the data. In this paper, we investigate how useful crowdsourcing can be in obtaining discourse annotations. We present an experiment in which subjects were asked to insert (“drag and drop”) a connecting phrase from a pre-defined list between the two segments of coherence relations. By employing non-trained, non-expert (also referred as na¨ıve) subjects to code the data, large amounts of data can be coded in a"
W17-0803,N16-3012,1,0.85193,"Missing"
W17-0803,L16-1165,1,0.930654,"ost of these, the PDTB and RST-DT annotators were not in agreement. Lower agreement on these relations is therefore also expected in the current experiment. The 234 items were divided into 12 batches, with 2 C AUSE, 2 C ONJUNCTION, 3 C ONCES SION , 3 C ONTRAST , 4 or 5 I NSTANTIATION and Although researchers agree that relations should be supplied with linguistic context in order to be annotated reliably, there are no clear guidelines for how much context is needed. As a result, studies have diverged in their methodology. For some annotation experiments, coders annotate the entire text (e.g., Rehbein et al., 2016; Zufferey et al., 2012). In these cases, they automatically take the context of the relation at hand into account when they annotate a text linearly. By contrast, in experiments where the entire text does not have to be annotated, or the task is split into smaller tasks for crowdsourcing purposes, the relations (or connectives) are often presented with a certain amount of context preceding and following the segments under investigation (e.g., Hoek 26 Participants were allowed to complete more than one batch, but they were never able to complete the same batch in both conditions. 5 S PECIFICAT"
W17-0803,J14-1009,0,0.0182816,"cience and Technology Saarland University Saarland University Saarbr¨ucken, Germany Saarbr¨ucken, Germany {m.c.j.scholman,vera}@coli.uni-saarland.de Abstract the standard practice of using two trained, expert annotators to code data. Not only is this procedure time-consuming and therefore costly, it also raises questions regarding the reliability and validity of the data. When using trained, expert annotators, they may agree because they share implicit knowledge and know the purpose of the research well, rather than because they are carefully following instructions (Artstein and Poesio, 2008; Riezler, 2014). Krippendorff (2004) therefore notes that the more annotators participate in the process and the less expert they are, the more likely they can ensure the reliability of the data. In this paper, we investigate how useful crowdsourcing can be in obtaining discourse annotations. We present an experiment in which subjects were asked to insert (“drag and drop”) a connecting phrase from a pre-defined list between the two segments of coherence relations. By employing non-trained, non-expert (also referred as na¨ıve) subjects to code the data, large amounts of data can be coded in a short period of"
W17-0803,W16-1713,0,0.0378289,"Missing"
W17-0803,W16-1707,0,0.273434,"two existing corpora. Moreover, we examine the effect of the design of the task on the reliability of the data. Researchers agree that discourse relations should be supplied with linguistic context in order to be annotated reliably but there are no clear guidelines for how much context is needed. The current contribution experimentally examines the influence of context on the interpretation of a discourse relation, with a specific focus on whether there is an interaction between characteristics of the segment and the presence of context. The contributions of this paper include the following: Rohde et al. (2016) showed that readers can infer an additional reading for a discourse relation connected by an adverbial. By obtaining many observations for a single fragment rather than only two, they were able to identify patterns of cooccurring relations; for example, readers can often infer an additional causal reading for a relation marked by otherwise. These results highlight a problem with double-coded data: Without a substantial number of observations, differences in annotations might be written off as annotator error or disagreement. In reality, there might be multiple interpretations for a relation,"
W17-0803,W15-0205,0,0.0452401,"Missing"
W17-0803,C14-1027,0,0.0756616,"dies employing na¨ıve annotators have found high agreement between these annotators and expert annotators for Natural Language tasks (e.g., Snow et al., 2008). Classifying coherence relations, however, is considered to be a different and especially difficult type of task due to the complex semantic interpretations of relations and the fact that textual coherence does not reside in the verbal material, but rather in the readers’ mental representation (Spooren and Degand, 2010). Nevertheless, na¨ıve annotators have recently also been employed successfully in coherence relation annotation tasks (Kawahara et al., 2014; Scholman et al., 2016) and connective insertion tasks (Rohde et al., 2015, 2016) similar to the one reported in this paper. on a classification of connective substitutability by Knott and Dale (1994). We investigate how reliable the obtained annotations are by comparing them to expert annotations from two existing corpora. Moreover, we examine the effect of the design of the task on the reliability of the data. Researchers agree that discourse relations should be supplied with linguistic context in order to be annotated reliably but there are no clear guidelines for how much context is neede"
W17-0803,D08-1027,0,0.129562,"Missing"
W17-0803,P92-1001,0,0.602372,"ld be designed. One aspect of this design is the inclusion of context. The benefits of context are widely acknowledged in the field of discourse analysis. Context is necessary to ground the discourse being constructed (Cornish, 2009), and the interpretation of any sentence other than the first in a discourse is therefore constrained by the preceding context (Song, 2010). This preceding context has significant effects on essential parts of discourse annotation, such as determining the rhetorical role each sentence plays in the discourse, and the temporal relations between the events described (Lascarides et al., 1992; Spooren and Degand, 2010). The knowledge of context is therefore assumed to be a requirement for discourse analysis. 3.1 Participants 167 native English speakers completed one or more batches of this experiment. They were recruited via Prolific Academic and reimbursed for their participation (2 GBP per batch with context; 1.5 GBP per batch without context). Their education level ranged between an undergraduate degree and a doctorate degree. 3.2 Materials The experimental passages consisted of 192 implicit and 42 explicit relations from Wall Street Journal texts. These relations are part of b"
W17-0803,prasad-etal-2008-penn,0,0.59386,"Knowing how much context is minimally needed to be able to reliably annotate data will save resources; after all, the less context annotators have to read, the less time they need to spend on the task. The goal of the current experiment is therefore to test the reliability of crowdsourced discourse annotations compared to original corpus annotations, as well as the effect of context on the reliability of the task. course annotation. The design of the current study also differs from other connective-based annotation approaches such as Rohde et al. (2016) and the Penn Discourse Treebank (PDTB, Prasad et al., 2008) in that the connectives were selected to unambiguously mark a specific type of relation. Certain connectives are known to mark different types of relations, such as but, which can mark C ONTRAST, C ONCESSION and A LTERNATIVE relations. In the current study, we excluded such ambiguous connectives in order to be able to derive relation types from the insertions. For example, the connecting phrase AS AN ILLUSTRATION is taken to be a dominant marker for I NSTANTIATION relations. The procedure for selecting phrases will be explained in Section 3. 3 Method Participants were asked to insert connecti"
W17-3522,P89-1009,0,0.259334,"essions for German. Using images of furniture as stimuli similarly to the TUNA and D-TUNA corpora, our corpus extends on these corpora by providing data collected in a simulated driving dual-task setting, and additionally provides exact duration annotations for the spoken referring expressions. This corpus will hence allow researchers to analyze the interaction between referring expression length and speech rate, under conditions where the listener is under high vs. low cognitive load. 1 Introduction Referring expression generation (REG) is an important problem in natural language generation (Dale, 1989; Dale and Reiter, 1995; van Deemter, 2002; Krahmer et al., 2003). The challenge of generating referring expressions (REs) that can pick out a specific object among a set of similar objects represents a prominent subtask in REG. One important goal for REG is to produce human-like referring expressions which are not only logically correct but also sound natural to native speakers of the target language. Corpora that include referring expressions and contain transparent semantic annotation are an important resource for being able to evaluate the naturalness of an REG algorithm. Because naturally"
W17-3522,W07-2307,0,0.719359,"Missing"
W17-3522,koolen-krahmer-2010-tuna,0,0.674562,"al Language Generation conference, pages 149–153, c Santiago de Compostela, Spain, September 4-7 2017. 2017 Association for Computational Linguistics 2 Corpus Collection We built a corpus of spoken referring expressions, aimed at investigating how speakers accommodate listeners who are under cognitive load. We created a more natural speech environment by having pairs of naive participants describe objects to each other in a simulated driving context, while retaining the same collection of furniture images used in both previous TUNA corpora. We did not use the people domain (Gatt et al., 2007; Koolen and Krahmer, 2010), because previous analyses on the TUNA corpora made clear that this domain results in a large amount of linguistic variation that is hard to capture by a semantic annotation. In addition, the detail in the images did not show up well in the driving simulator. Furthermore, the fact that this corpus was collected in German means that it provides a third language for cross-linguistic comparison. There are two other corpora associated with the analysis of REs in German, namely the GIVE-2 corpus (Gargett et al., 2010) and the P ENTO R EF corpus (Zarrieß et al., 2016). However, the virtual environm"
W17-3522,J03-1003,0,0.0878078,"Missing"
W17-3522,W06-1420,0,0.0884448,"Missing"
W17-3522,J02-1003,0,0.147654,"Missing"
W17-3522,L16-1019,0,0.0220305,"domain (Gatt et al., 2007; Koolen and Krahmer, 2010), because previous analyses on the TUNA corpora made clear that this domain results in a large amount of linguistic variation that is hard to capture by a semantic annotation. In addition, the detail in the images did not show up well in the driving simulator. Furthermore, the fact that this corpus was collected in German means that it provides a third language for cross-linguistic comparison. There are two other corpora associated with the analysis of REs in German, namely the GIVE-2 corpus (Gargett et al., 2010) and the P ENTO R EF corpus (Zarrieß et al., 2016). However, the virtual environment in which the data for the GIVE-2 corpus were collected, coupled with the freedom subjects had to move around the environment, makes the corpus poorly suited to the sort of systematic evaluation of REs that is enabled by the TUNA, D-TUNA, and now the G-TUNA corpora. While the P ENTO R EF corpus provides data for both English and German accompanied by utterance-level timing information, the task-oriented dialogue with feedback from ‘Instruction Followers’ and the different target objects make comparisons to the TUNA corpora more challenging. Our corpus thus pro"
W17-3522,gargett-etal-2010-give,0,\N,Missing
W18-6002,W15-5301,0,0.0522692,"Missing"
W18-6546,P16-2008,0,0.0339663,"Missing"
W18-6546,C90-3045,0,0.213632,"the equivalent of a CFG derivation for the tree consisting of rules like but → First Next, First → have, have → Arg0 Arg1, and so on. These rules would be very general, but the derivation then requires many more steps. The third option, illustrating the appeal of using a tree substitution grammar, involves elementary trees of intermediate size, like those in Figure 3. The rules in Figure 3 represent a combination Synchronous TSGs Synchronous tree substitution grammars (TSGs) are a subset of synchronous tree adjoining grammars, both of which represent the relationships between pairs of trees (Shieber and Schabes, 1990; Eisner, 2003). A tree substitution grammar consists of a set of elementary trees which can be used to expand non-terminal nodes into a complete tree. Consider the example in Figure 2, which shows the text plan and logical form trees for the sentence, Sonia Rose has very good food quality, but Bienvenue has excellent food quality. The logical form in this figure could be derived 392 5 of small, CFG-like rules (e.g. the elementary tree rooted at but), larger trees representing memorized chunks (i.e. the rule involving Bienvenue), and intermediate trees, like the one including have → quality →"
W18-6546,P03-2041,0,0.139378,"rivation for the tree consisting of rules like but → First Next, First → have, have → Arg0 Arg1, and so on. These rules would be very general, but the derivation then requires many more steps. The third option, illustrating the appeal of using a tree substitution grammar, involves elementary trees of intermediate size, like those in Figure 3. The rules in Figure 3 represent a combination Synchronous TSGs Synchronous tree substitution grammars (TSGs) are a subset of synchronous tree adjoining grammars, both of which represent the relationships between pairs of trees (Shieber and Schabes, 1990; Eisner, 2003). A tree substitution grammar consists of a set of elementary trees which can be used to expand non-terminal nodes into a complete tree. Consider the example in Figure 2, which shows the text plan and logical form trees for the sentence, Sonia Rose has very good food quality, but Bienvenue has excellent food quality. The logical form in this figure could be derived 392 5 of small, CFG-like rules (e.g. the elementary tree rooted at but), larger trees representing memorized chunks (i.e. the rule involving Bienvenue), and intermediate trees, like the one including have → quality → food. In these"
W18-6546,W17-3518,0,0.0314461,"ms as training input. In learning a synchronous TSG, the model presented here aims to avoid using hand-crafted rule templates, which are more dependent on the specific representation chosen for surface realizer input. As mentioned in the introduction, there have been a number of attempts in recent years to learn end-to-end generation systems which produce text directly from database records (Konstas and Lapata, 2012), dialogue acts with slot-value pairs (Mairesse et al., 2010; Wen et al., 2015; Duˇsek and Jurˇc´ıcˇ ek, 2016), or semantic triples like those used in the recent WebNLG challenge (Gardent et al., 2017). In contrast, we assume that content selection and discourse structuring are handled before sentence planning. In principle, however, our methods can be applied to any generation subtask involving tree-to-tree mappings. As a testbed during development we used the SPaRKy Restaurant Corpus (2007), a corpus of restaurant recommendations and comparisons generated by a hand-crafted NLG system. While the controlled nature of this corpus is ideal for testing during development, our future evaluations will also use the more varied Extended SRC (Howcroft et al., 2017).4 After training on about 700 TP-"
W18-6546,D15-1199,0,0.0708543,"Missing"
W18-6546,P06-1064,0,0.0274435,"· ) ∼DP(1.0, Uniform({l· })) 5.3 (8) (9) (10) Sampling Our Gibbs sampler adapts the blocked sampling approach of (Cohn et al., 2010) to synchronous grammars. For each text in the corpus, we resample a synchronous derivation for the entire text before updating the associated model parameters. N |l ∼DP(1.0, Uniform({n ∈ corpus})) (4) 6 A|n ∼DP(1.0, Uniform({a ∈ corpus})) (5) While our pipeline can in principle work with any reversible parser-realizer, our current implementation uses OpenCCG2 (White, 2006; White and Rajkumar, 2012). We use the broad-coverage grammar for English based on CCGbank (Hockenmaier, 2006). The ‘logical forms’ associated with this grammar are more or less syntactic in nature, encoding the lemmas to be used, the dependencies among them, and morphosyntactic annotations in a dependency semantics. Parsing the corpus with OpenCCG provides the LFs we use for training. After training the model, we have a collection of synchronous TSG rules which can be applied to (unseen) text plans to produce new LFs. For this rule application we use Alto3 (Koller and Kuhlmann, 2012) because of its efficient implementation of parsing for synchronous grammars. The final stage in the generation pipelin"
W18-6546,W06-1403,0,0.0560587,"appropriate, while l similarly represents tree locations. 5.2 (7) P (lTP , lLF ) =P (lTP )P (lLF ) P (l· ) ∼DP(1.0, Uniform({l· })) 5.3 (8) (9) (10) Sampling Our Gibbs sampler adapts the blocked sampling approach of (Cohn et al., 2010) to synchronous grammars. For each text in the corpus, we resample a synchronous derivation for the entire text before updating the associated model parameters. N |l ∼DP(1.0, Uniform({n ∈ corpus})) (4) 6 A|n ∼DP(1.0, Uniform({a ∈ corpus})) (5) While our pipeline can in principle work with any reversible parser-realizer, our current implementation uses OpenCCG2 (White, 2006; White and Rajkumar, 2012). We use the broad-coverage grammar for English based on CCGbank (Hockenmaier, 2006). The ‘logical forms’ associated with this grammar are more or less syntactic in nature, encoding the lemmas to be used, the dependencies among them, and morphosyntactic annotations in a dependency semantics. Parsing the corpus with OpenCCG provides the LFs we use for training. After training the model, we have a collection of synchronous TSG rules which can be applied to (unseen) text plans to produce new LFs. For this rule application we use Alto3 (Koller and Kuhlmann, 2012) because"
W18-6546,W15-4704,1,0.878594,"Missing"
W18-6546,W12-4616,0,0.0237976,"ntation uses OpenCCG2 (White, 2006; White and Rajkumar, 2012). We use the broad-coverage grammar for English based on CCGbank (Hockenmaier, 2006). The ‘logical forms’ associated with this grammar are more or less syntactic in nature, encoding the lemmas to be used, the dependencies among them, and morphosyntactic annotations in a dependency semantics. Parsing the corpus with OpenCCG provides the LFs we use for training. After training the model, we have a collection of synchronous TSG rules which can be applied to (unseen) text plans to produce new LFs. For this rule application we use Alto3 (Koller and Kuhlmann, 2012) because of its efficient implementation of parsing for synchronous grammars. The final stage in the generation pipeline is to realize these LFs using OpenCCG, optionally performing reranking on the resulting texts. Some examples of the resulting texts are provided in the next section. HDP for sTSG Derivations Our synchronous TSG model has two additional distributions: (1) a distribution over pairs of TP and LF elementary trees; and (2) a distribution over pairs of tree locations representing the probability of those locations being aligned to each other. Similarly to the generative story for"
W18-6546,D12-1023,0,0.0609381,"Missing"
W18-6546,N12-1093,0,0.139933,"we are working with are the product of document planning, and we use an existing parser-realizer for surface realization. This allows us to constrain the learning problem by limiting our search to the set of tree-to-tree mappings which produce valid input for the surIntroduction Developing and adapting natural language generation (NLG) systems for new domains requires substantial human effort and attention, even when using off-the-shelf systems for surface realization. This observation has spurred recent interest in automatically learning end-to-end generation systems (Mairesse et al., 2010; Konstas and Lapata, 2012; Wen et al., 2015; Duˇsek and Jurˇc´ıcˇ ek, 2016); however, these approaches tend to use shallow meaning representations (Howcroft et al., 2017) and do not make effective use of prior work on surface realization to constrain the learning problem or to ensure grammaticality in the resulting texts. Based on these observations, we propose a Bayesian nonparametric approach to learning sentence planning rules for a conventional NLG system. Making use of existing systems for surface realization along with more sophisticated meaning representations allows us to cast the problem as a grammar inductio"
W19-0416,Q15-1024,0,0.134188,"Missing"
W19-0416,N16-1037,0,0.187043,"on is one of the most important components in discourse parsing. With the release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples with discourse relation labels and implicit connectives, a lot of previous works focused on typical statistical machine learning solutions with manually crafted sparse features (Rutherford and Xue, 2014). Recently, neural networks have shown an advantage of dealing with data sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al., 2015), recurrent (Ji et al., 2016), character-based (Qin et al., 2016a), adversarial (Qin et al., 2017) neural networks, and pair-aware neural sentence modeling (Cai and Zhao, 2017). Multi-task learning has also been shown to be beneficial on this task (Lan et al., 2017). However, most neural based methods suffer from insufficient annotated data.Wu et al. (2016) extracted bilingual-constrained synthetic implicit data from a sentence-aligned English-Chinese corpus. Shi et al. (2017, 2018) proposed to acquire additional training data by exploiting explicitation of connectives during translation. Explicitation refers to the fact"
W19-0416,P14-1062,0,0.0551217,"he predictive vectors for the whole target sequence hˆd = hd1 , hd2 , ..., hdn . Ideally, it contains the information of exposed implicit connectives. Gated Interaction In order to predict the coherent discourse relation of the input sequence, we take both the hencoder and the predictive word vectors hd into account. K-max pooling can “draw together” features that are most discriminative and among many positions apart in the sentences, especially on both the two relational arguments in our task here; this method has been proved to be effective in choosing active features in sentence modeling (Kalchbrenner et al., 2014). We employ an average k-max pooling layer which takes average of the top k-max values among the whole time-steps as in Equation 9 and 10: k X ¯e = 1 h topk(he ) k i=1 (9) k X ¯d = 1 topk(hˆd ) h k i=1 (10) ¯ e and h ¯ d are then combined using a linear layer (Lan et al., 2017). As illustrated in Equation 11, the h linear layer acts as a gate to determine how much information from the sequence-to-sequence network should be mixed into the original sentence’s representations from the encoder. Compared with bilinear layer, it also has less parameters and allows us to use high dimensional word vec"
W19-0416,D17-1134,0,0.577035,"ot of previous works focused on typical statistical machine learning solutions with manually crafted sparse features (Rutherford and Xue, 2014). Recently, neural networks have shown an advantage of dealing with data sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al., 2015), recurrent (Ji et al., 2016), character-based (Qin et al., 2016a), adversarial (Qin et al., 2017) neural networks, and pair-aware neural sentence modeling (Cai and Zhao, 2017). Multi-task learning has also been shown to be beneficial on this task (Lan et al., 2017). However, most neural based methods suffer from insufficient annotated data.Wu et al. (2016) extracted bilingual-constrained synthetic implicit data from a sentence-aligned English-Chinese corpus. Shi et al. (2017, 2018) proposed to acquire additional training data by exploiting explicitation of connectives during translation. Explicitation refers to the fact that translators sometimes add connectives into Classifier y1 Decoder y2 y3 yn … &lt;eos> Softmax Softmax Layer Attention Knowledge Vector Update … h* … hˆd Gated-Interaction Layer Context Vector Context Memory c1 Global Attention he α1 αn"
W19-0416,D09-1036,0,0.151932,"ition.]Arg2 — Implicit, Comparison.Contrast The key in implicit discourse relation classification lies in extracting relevant information for the relation label from (the combination of) the discourse relational arguments. Informative signals can consist of surface cues, as well as the semantics of the relational arguments. Statistical approaches have typically relied on linguistically informed features which capture both of these aspects, like temporal markers, polarity tags, Levin verb classes and sentiment lexicons, as well as the Cartesian products of the word tokens in the two arguments (Lin et al., 2009). More recent efforts use distributed representations with neural network architectures (Qin et al., 2016a). The main question in designing neural networks for discourse relation classification is how to get the neural networks to effectively encode the discourse relational arguments such that all of the aspects relevant to the classification of the relation are represented, in particular in the face of very limited amounts of annotated training data, see e.g. Rutherford et al. (2017). The crucial intuition in the present paper is to make use of the annotated implicit connectives in the PDTB:"
W19-0416,N18-1050,0,0.0228472,"k i=1 (9) k X ¯d = 1 topk(hˆd ) h k i=1 (10) ¯ e and h ¯ d are then combined using a linear layer (Lan et al., 2017). As illustrated in Equation 11, the h linear layer acts as a gate to determine how much information from the sequence-to-sequence network should be mixed into the original sentence’s representations from the encoder. Compared with bilinear layer, it also has less parameters and allows us to use high dimensional word vectors. ¯ e ⊕ σ(Wi h ¯ d + bi ) h∗ = h (11) Explicit Context Knowledge To further capture common knowledge in contexts, we here employ a memory network proposed in Liu et al. (2018), to get explicit context representations of contexts training examples. We use a memory matrix M ∈ RK×N , where K, N denote hidden size and number of training instances respectively. During training, the memory matrix remembers the information of training examples and then retrieves them when predicting labels. Given a representation h∗ from the interaction layer, we generate a knowledge vector by weighted memory reading: k = M sof tmax(M T h∗ ) (12) We here use dot product attention, which is faster and space-efficient than additive attention, to calculate the scores for each training instan"
W19-0416,D16-1130,0,0.0273653,"Missing"
W19-0416,D15-1166,0,0.0501027,"uence auto-encoder to better represent sentence in an unsupervised way and showed impressive performances on different tasks. The main difference between our model and this one is that we have different input and output (the output contains a connective while the input doesn’t). In this way, the model is forced to explicitate implicit relation and try to learn the latent pattern and discourse relation between implicit arguments and connectives and then generate more discriminative representations. 3 Methodology Our model is based on the sequence-to-sequence model used for machine translation (Luong et al., 2015), an adaptation of an LSTM (Hochreiter and Schmidhuber, 1997) that encodes a variable length input as a fix-length vector, then decodes it into a variable length of outputs. As illustrated in Figure 1, our model consists of three components: Encoder, Decoder and Discourse Relation Classifier. We here use different LSTMs for the encoding and decoding tasks to help keep the independence between those two parts. The task of implicit discourse relation recognition is to recognize the senses of the implicit relations, given the two arguments. For each discourse relation instance, The Penn Discourse"
W19-0416,prasad-etal-2008-penn,0,0.197876,"and acquire more explicit context knowledge. We show that our method outperforms previous approaches on the 11-way classification on the PDTB 2.0 benchmark. The remaining of the paper is organized as follows: Section 2 discusses related work; Section 3 describes our proposed method; Section 4 gives the training details and experimental results, which is followed by conclusion and future work in section 5. 2 2.1 Related Work Implicit Discourse Relation Classification Implicit discourse relation recognition is one of the most important components in discourse parsing. With the release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples with discourse relation labels and implicit connectives, a lot of previous works focused on typical statistical machine learning solutions with manually crafted sparse features (Rutherford and Xue, 2014). Recently, neural networks have shown an advantage of dealing with data sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al., 2015), recurrent (Ji et al., 2016), character-based (Qin et al., 2016a), adversarial (Qin et al., 2017) neural networks, and pair"
W19-0416,C16-1180,0,0.176083,"Missing"
W19-0416,D16-1246,0,0.0824503,"Missing"
W19-0416,P17-1093,0,0.483947,"h the release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples with discourse relation labels and implicit connectives, a lot of previous works focused on typical statistical machine learning solutions with manually crafted sparse features (Rutherford and Xue, 2014). Recently, neural networks have shown an advantage of dealing with data sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al., 2015), recurrent (Ji et al., 2016), character-based (Qin et al., 2016a), adversarial (Qin et al., 2017) neural networks, and pair-aware neural sentence modeling (Cai and Zhao, 2017). Multi-task learning has also been shown to be beneficial on this task (Lan et al., 2017). However, most neural based methods suffer from insufficient annotated data.Wu et al. (2016) extracted bilingual-constrained synthetic implicit data from a sentence-aligned English-Chinese corpus. Shi et al. (2017, 2018) proposed to acquire additional training data by exploiting explicitation of connectives during translation. Explicitation refers to the fact that translators sometimes add connectives into Classifier y1 Decoder"
W19-0416,E17-1027,1,0.865874,"Levin verb classes and sentiment lexicons, as well as the Cartesian products of the word tokens in the two arguments (Lin et al., 2009). More recent efforts use distributed representations with neural network architectures (Qin et al., 2016a). The main question in designing neural networks for discourse relation classification is how to get the neural networks to effectively encode the discourse relational arguments such that all of the aspects relevant to the classification of the relation are represented, in particular in the face of very limited amounts of annotated training data, see e.g. Rutherford et al. (2017). The crucial intuition in the present paper is to make use of the annotated implicit connectives in the PDTB: in addition to the typical relation label classification task, we also train the model to encode and decode the discourse relational arguments, and at the same time predict the implicit connective. This novel secondary task forces the internal representation to more completely encode the semantics of the relational arguments (in order to allow the model to decode later), and to make a more fine-grained classification (predicting the implicit connective) than is necessary for the overa"
W19-0416,E14-1068,0,0.0232586,"our proposed method; Section 4 gives the training details and experimental results, which is followed by conclusion and future work in section 5. 2 2.1 Related Work Implicit Discourse Relation Classification Implicit discourse relation recognition is one of the most important components in discourse parsing. With the release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples with discourse relation labels and implicit connectives, a lot of previous works focused on typical statistical machine learning solutions with manually crafted sparse features (Rutherford and Xue, 2014). Recently, neural networks have shown an advantage of dealing with data sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al., 2015), recurrent (Ji et al., 2016), character-based (Qin et al., 2016a), adversarial (Qin et al., 2017) neural networks, and pair-aware neural sentence modeling (Cai and Zhao, 2017). Multi-task learning has also been shown to be beneficial on this task (Lan et al., 2017). However, most neural based methods suffer from insufficient annotated data.Wu et al. (2016) extracted bilingual-constrained"
W19-0416,N15-1081,0,0.266477,"Missing"
W19-0416,E17-2024,1,0.88445,"Missing"
W19-0416,I17-1049,1,0.836583,"sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al., 2015), recurrent (Ji et al., 2016), character-based (Qin et al., 2016a), adversarial (Qin et al., 2017) neural networks, and pair-aware neural sentence modeling (Cai and Zhao, 2017). Multi-task learning has also been shown to be beneficial on this task (Lan et al., 2017). However, most neural based methods suffer from insufficient annotated data.Wu et al. (2016) extracted bilingual-constrained synthetic implicit data from a sentence-aligned English-Chinese corpus. Shi et al. (2017, 2018) proposed to acquire additional training data by exploiting explicitation of connectives during translation. Explicitation refers to the fact that translators sometimes add connectives into Classifier y1 Decoder y2 y3 yn … &lt;eos> Softmax Softmax Layer Attention Knowledge Vector Update … h* … hˆd Gated-Interaction Layer Context Vector Context Memory c1 Global Attention he α1 αn … h1d … Encoder cn … hnd … … w 1 w 2 w 3 … w n-1 w n &lt;sos> y1 y2 … yn-1 yn Figure 1: The Architecture of Proposed Model. the text in the target language which were not originally present in the source language. The"
W19-0416,D16-1253,0,0.200165,"rafted sparse features (Rutherford and Xue, 2014). Recently, neural networks have shown an advantage of dealing with data sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al., 2015), recurrent (Ji et al., 2016), character-based (Qin et al., 2016a), adversarial (Qin et al., 2017) neural networks, and pair-aware neural sentence modeling (Cai and Zhao, 2017). Multi-task learning has also been shown to be beneficial on this task (Lan et al., 2017). However, most neural based methods suffer from insufficient annotated data.Wu et al. (2016) extracted bilingual-constrained synthetic implicit data from a sentence-aligned English-Chinese corpus. Shi et al. (2017, 2018) proposed to acquire additional training data by exploiting explicitation of connectives during translation. Explicitation refers to the fact that translators sometimes add connectives into Classifier y1 Decoder y2 y3 yn … &lt;eos> Softmax Softmax Layer Attention Knowledge Vector Update … h* … hˆd Gated-Interaction Layer Context Vector Context Memory c1 Global Attention he α1 αn … h1d … Encoder cn … hnd … … w 1 w 2 w 3 … w n-1 w n &lt;sos> y1 y2 … yn-1 yn Figure 1: The Arch"
W19-0416,K15-2001,0,0.0772846,"ers, are provided in Table 2. The model is trained firstly to minimize the loss in Equation 14 until convergence, we use scheduled sampling (Bengio et al., 2015) during training to avoid “teacher-forcing problem”. And then to minimize the joint loss in Equation 16 to train the implicit discourse relation classifier. 4 4.1 Experiments and Results Experimental Setup We evaluate our model on the PDTB. While early work only evaluated classification performance between the four main PDTB relation classes, more recent work including the CoNLL 2015 and 2016 shared tasks on Shallow Discourse Parsing (Xue et al., 2015, 2016) have set the standard to second-level classification. The second-level classification is more useful for most downstream tasks. Following other Settings PDTB-Lin PDTB-Ji Cross valid. per fold avg. Train 13351 12826 12085 Dev 515 1165 1486 Test 766 1039 14861 Table 1: Numbers of train, development and test set on different settings for 11-way classification task. Instances annotated with two labels are double-counted and some relations with few instances have been removed. works we directly compare to in our evaluation, we here use the setting where AltLex, EntRel and NoRel tags are ign"
W19-0416,D15-1266,0,0.0706308,"cit discourse relation recognition is one of the most important components in discourse parsing. With the release of PDTB (Prasad et al., 2008), the largest available corpus which annotates implicit examples with discourse relation labels and implicit connectives, a lot of previous works focused on typical statistical machine learning solutions with manually crafted sparse features (Rutherford and Xue, 2014). Recently, neural networks have shown an advantage of dealing with data sparsity problem, and many deep learning methods have been proposed for discourse parsing, including convolutional (Zhang et al., 2015), recurrent (Ji et al., 2016), character-based (Qin et al., 2016a), adversarial (Qin et al., 2017) neural networks, and pair-aware neural sentence modeling (Cai and Zhao, 2017). Multi-task learning has also been shown to be beneficial on this task (Lan et al., 2017). However, most neural based methods suffer from insufficient annotated data.Wu et al. (2016) extracted bilingual-constrained synthetic implicit data from a sentence-aligned English-Chinese corpus. Shi et al. (2017, 2018) proposed to acquire additional training data by exploiting explicitation of connectives during translation. Expl"
W19-0416,P16-2034,1,0.791581,"2018), the small size of the test set makes reported results potentially unreliable. Preprocessing We first convert tokens in PDTB to lowercase and normalize strings, which removes special characters. The word embeddings used for initializing the word representations are trained with the CBOW architecture in Word2Vec2 (Mikolov et al., 2013) on PDTB training set. All the weights in the model are initialized with uniform random. To better locate the connective positions in the target side, we use two position indicators (hconni, h/conni) which specify the starting and ending of the connectives (Zhou et al., 2016), which also indicate the spans of discourse arguments. Since our main task here is not generating arguments, it is better to have representations generated by correct words rather than by wrongly predicted ones. So at test time, instead of using the predicted word from previous time step as current input, we use the source sentence as the decoder’s input and target. As the implicit connective is not available at test time, we use a random vector, which we used as “impl conn” in Figure 2, as a placeholder to inform the sequence that the upcoming word should be a connective. Hyper-parameters Th"
W19-0416,C10-2172,0,0.487361,"trying to integrate implicit connectives into the representation of implicit discourse relations with a joint learning framework of sequence-to-sequence network. We conduct experiments with different settings on PDTB benchmark, the results show that our proposed method can achieve state-of-the-art performance on recognizing the implicit discourse relations and the improvements are not only brought by the increasing number of parameters. The model also has great potential abilities in implicit connective prediction in the future. Our proposed method shares similar spirit with previous work in Zhou et al. (2010), who also tried to leverage implicit connectives to help extract discriminative features from implicit discourse instances. Comparing with the adversarial method proposed by Qin et al. (2017), our proposed model more closely mimics humans’ annotation process of implicit discourse relations and is trained to directly explicitate the implicit relations before classification. With the representation of the original implicit sentence and the explicitated one from decoder, and the help of the explicit knowledge vector from memory network, the implicit relation could be classified with higher accur"
W19-2703,P14-1092,0,0.0642947,"erent language families: French (a Romance language), German (from the Germanic language family) and Czech (a Slavic language). As all three of these languages are part of the EuroParl corpus, this also allows us to directly test whether higher quality Introduction Discourse relations connect two sentences/clauses to each other. The identification of discourse relations is an important step in natural language understanding and is beneficial to various downstream NLP applications such as text summarization (Yoshida et al., 2014; Gerani et al., 2014), question answering (Verberne et al., 2007; Jansen et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015), and so on. Discourse relations can be marked explicitly using a discourse connective or discourse adverbial such as “because”, “but”, “however”, see example 1. Explicitly marked relations are relatively easy to classify automatically (Pitler et al., 2008). In example 2, the causal relation is not marked explicitly, and can only be inferred from the texts. This second type of case is empirically even more common than explicitly marked relations (Prasad et al., 2008), but is much harder to classify automatically. 12 Proceedings of"
W19-2703,P09-1077,0,0.0357531,"Related Work Recognizing implicit discourse relation, as one of the most important and challenging part of discourse parser system, has drawn a lot of attention in recent years after the release of PDTB (Prasad et al., 2008), the largest available corpus with annotated implicit examples, including two shared task in CoNLL-2015 and CoNLL-2016 (Xue et al., 2015, 2016). Early attempts focused on statistical machine learning solutions with sparse linguistic features and linear models. They used several linguistically informed features like polarity tags, Levin verb classes and brown cluster etc. (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014). Recent methods for discourse relation classification have increasingly relied on neural network architectures (Ji et al., 2016; Qin et al., 2016, 2017; Shi and Demberg, 2018). However, with the high number of parameters to be trained in more and more complicated deep neural network architectures, the demand for more reliable annotated data has become even more urgent. Data extension has been a longstanding goal in implicit discourse relation classification. Wang et al. (2012) proposed to differentiate typical and atypical examples for each re"
W19-2703,Q15-1024,0,0.335373,"corpora used here are from Europarl (Koehn, 2005), it contains about 2.05M English-French, 1.96M EnglishGerman and 0.65M English-Czech pairs. After preprocessing, we got about 0.53M parallel sentence pairs in all these four languages. The Penn Discourse Treebank (PDTB): PDTB (Prasad et al., 2008) is the largest available manually annotated corpus of discourse relations from Wall Street Journal. Each discourse relation has been annotated in three hierarchy levels. In this paper, we follow the previous conventional settings and focus on the second-level 11-ways classification (Lin et al., 2009; Ji and Eisenstein, 2015; Rutherford et al., 2017; Shi et al., 2017), after removing the relations with few instances. 4.2 Figure 3: Bi-LSTM network for implicit discoure relation classification. bidirectional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network. A LSTM recurrent neural network processes a variable-length sequence x = (x1 , x2 , ..., xn ). At time step t, the state of memory cell ct and hidden ht are calculated with the Equations 1:    it σ  ft   σ   o  =  σ  W · [ht−1 , xt ] t cˆt tanh  Implicit discourse relation classification To evaluate whether the extracted data"
W19-2703,N16-1037,0,0.157127,"the release of PDTB (Prasad et al., 2008), the largest available corpus with annotated implicit examples, including two shared task in CoNLL-2015 and CoNLL-2016 (Xue et al., 2015, 2016). Early attempts focused on statistical machine learning solutions with sparse linguistic features and linear models. They used several linguistically informed features like polarity tags, Levin verb classes and brown cluster etc. (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014). Recent methods for discourse relation classification have increasingly relied on neural network architectures (Ji et al., 2016; Qin et al., 2016, 2017; Shi and Demberg, 2018). However, with the high number of parameters to be trained in more and more complicated deep neural network architectures, the demand for more reliable annotated data has become even more urgent. Data extension has been a longstanding goal in implicit discourse relation classification. Wang et al. (2012) proposed to differentiate typical and atypical examples for each relation and augment training data for implicit only by typical explicits. Rutherford and Xue (2015) designed criteria for selecting explicit samples in which connectives can be om"
W19-2703,prasad-etal-2008-penn,0,0.341613,"Gerani et al., 2014), question answering (Verberne et al., 2007; Jansen et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015), and so on. Discourse relations can be marked explicitly using a discourse connective or discourse adverbial such as “because”, “but”, “however”, see example 1. Explicitly marked relations are relatively easy to classify automatically (Pitler et al., 2008). In example 2, the causal relation is not marked explicitly, and can only be inferred from the texts. This second type of case is empirically even more common than explicitly marked relations (Prasad et al., 2008), but is much harder to classify automatically. 12 Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 12–21 c Minneapolis, MN, June 6, 2019. 2019 Association for Computational Linguistics aligned cross-lingual parallel corpora and majority votes to get more reliable and in-topic annotated implicit discourse relation instances. can be achieved by using those instances that were consistently explicitated in several languages. We use cross-lingual explicitation to acquire more reliable implicit discourse relation instances with separate arguments that are from adjacent"
W19-2703,C16-1180,0,0.410558,"DTB (Prasad et al., 2008), the largest available corpus with annotated implicit examples, including two shared task in CoNLL-2015 and CoNLL-2016 (Xue et al., 2015, 2016). Early attempts focused on statistical machine learning solutions with sparse linguistic features and linear models. They used several linguistically informed features like polarity tags, Levin verb classes and brown cluster etc. (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014). Recent methods for discourse relation classification have increasingly relied on neural network architectures (Ji et al., 2016; Qin et al., 2016, 2017; Shi and Demberg, 2018). However, with the high number of parameters to be trained in more and more complicated deep neural network architectures, the demand for more reliable annotated data has become even more urgent. Data extension has been a longstanding goal in implicit discourse relation classification. Wang et al. (2012) proposed to differentiate typical and atypical examples for each relation and augment training data for implicit only by typical explicits. Rutherford and Xue (2015) designed criteria for selecting explicit samples in which connectives can be omitted without chan"
W19-2703,2005.mtsummit-papers.11,0,0.106272,". 4 4.1 Figure 2: Numbers of implicit discourse relation instances from different agreements of explicit instances in three back-translations. En-Fr denotes instances that are implicit in English but explicit in back-translation of French, same for En-De and En-Cz. The overlap means they share the same relational arguments. The numbers under “Two-Votes” and “Three-Votes” are the numbers of discourse relation agreement / disagreement between explicits in back-translations of two or three languages. Experiments and Results Data Europarl Corpora: The parallel corpora used here are from Europarl (Koehn, 2005), it contains about 2.05M English-French, 1.96M EnglishGerman and 0.65M English-Czech pairs. After preprocessing, we got about 0.53M parallel sentence pairs in all these four languages. The Penn Discourse Treebank (PDTB): PDTB (Prasad et al., 2008) is the largest available manually annotated corpus of discourse relations from Wall Street Journal. Each discourse relation has been annotated in three hierarchy levels. In this paper, we follow the previous conventional settings and focus on the second-level 11-ways classification (Lin et al., 2009; Ji and Eisenstein, 2015; Rutherford et al., 2017;"
W19-2703,P17-1093,0,0.109113,"Missing"
W19-2703,P07-2045,0,0.00447775,"amples from the outputs of the parser that don’t have consecutive arguments. • Filtering: remove those sentences that don’t have all the three translations in French, German or Czech. • ID matching: re-group each sentence into different documents by the sentence IDs. • Re-paragraph: rank the sentences in each documents by the ID and re-paragraph them. 3.2 Discourse Parser Machine Translation 3.4 We train three MT systems to back-translate French, German and Czech to English. To have word alignments, better and stable backtranslations, we employ a statistical machine translation system M OSES (Koehn et al., 2007), trained on the same parallel corpora. Source and target sentences are first tokenized, true-cased and then fed into the system for training. In our case, Majority Vote After parsing the back-translations of French, German and Czech, we can compare whether they contain explicit relations which connect the same relational arguments. The analysis of this subset then allows us to identify those instances that could be labeled with high confidence, i.e. where back-translations from all three languages allow us to infer the same coherence label. Note that it is not necessarily the case that all ba"
W19-2703,W15-2703,0,0.0218277,"I cannot, for the Council of Europe and the European Court of Human Rights, which have issued a decision, which I can understand, in Parliament raises some issues. Czech back-translation: I repeat that, when it comes to the European Union, in this case we are dealing with the domestic legal system of the Member States. in addition, (Expansion.Conjunction) I cannot answer for the Council of Europe or for the European Court of Human Rights , which has issued a decision that I understand may cause in Parliament some doubts. Case 4: Implicit relations can co-occur with marked discourse relations (Rohde et al., 2015), and multiple translations help discover these instances, for example: Original English: We all understand that nobody can return Russia to the path of freedom and democracy, (implicit: but) what is more, the situation in our country is not as straight6 forward as it might appear to the superficial observer. French back-translation: we all understand that nobody We would like to thank all the anonymous reviewers for their careful reading and insightful comments. This research was funded by the German Research Foundation (DFG) as part of SFB 1102 “Information Density and Linguistic Encoding”."
W19-2703,D09-1036,0,0.344787,"Missing"
W19-2703,E17-1027,1,0.845317,"m Europarl (Koehn, 2005), it contains about 2.05M English-French, 1.96M EnglishGerman and 0.65M English-Czech pairs. After preprocessing, we got about 0.53M parallel sentence pairs in all these four languages. The Penn Discourse Treebank (PDTB): PDTB (Prasad et al., 2008) is the largest available manually annotated corpus of discourse relations from Wall Street Journal. Each discourse relation has been annotated in three hierarchy levels. In this paper, we follow the previous conventional settings and focus on the second-level 11-ways classification (Lin et al., 2009; Ji and Eisenstein, 2015; Rutherford et al., 2017; Shi et al., 2017), after removing the relations with few instances. 4.2 Figure 3: Bi-LSTM network for implicit discoure relation classification. bidirectional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network. A LSTM recurrent neural network processes a variable-length sequence x = (x1 , x2 , ..., xn ). At time step t, the state of memory cell ct and hidden ht are calculated with the Equations 1:    it σ  ft   σ   o  =  σ  W · [ht−1 , xt ] t cˆt tanh  Implicit discourse relation classification To evaluate whether the extracted data is helpful to this task,"
W19-2703,E14-1068,0,0.0589394,"relation, as one of the most important and challenging part of discourse parser system, has drawn a lot of attention in recent years after the release of PDTB (Prasad et al., 2008), the largest available corpus with annotated implicit examples, including two shared task in CoNLL-2015 and CoNLL-2016 (Xue et al., 2015, 2016). Early attempts focused on statistical machine learning solutions with sparse linguistic features and linear models. They used several linguistically informed features like polarity tags, Levin verb classes and brown cluster etc. (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014). Recent methods for discourse relation classification have increasingly relied on neural network architectures (Ji et al., 2016; Qin et al., 2016, 2017; Shi and Demberg, 2018). However, with the high number of parameters to be trained in more and more complicated deep neural network architectures, the demand for more reliable annotated data has become even more urgent. Data extension has been a longstanding goal in implicit discourse relation classification. Wang et al. (2012) proposed to differentiate typical and atypical examples for each relation and augment training data for implicit only"
W19-2703,N15-1081,0,0.0848685,"classifying implicit discourse relations stems from the lack of strong indicative cues. Early work has already shown that implicit relations cannot be learned from explicit ones by just removing the discourse markers, which may lead to a meaning shift in the examples (Sporleder and Lascarides, 2008), making human-annotated relations currently the only reliable source for training implicit discourse relation classification. Due to the limited size of available training data, several approaches have been proposed for acquiring additional training data using automatic methods (Wang et al., 2012; Rutherford and Xue, 2015). The most promising approach so far, Shi et al. (2017), exploits the fact that human translators sometimes insert a connective in their translation even when a relation was implicit in the original text. Using a back-translation method, Shi et al. showed that such instances can be used for acquiring additional labeled text. Shi et al. (2017) however only used a single target langauge (French), and had no control over the quality of the labels extracted from backtranslated connectives. In this paper, we therefore systematically compare the contribution of three target translation languages fro"
W19-2703,W12-1614,0,0.082119,"ing implicit discourse relation, as one of the most important and challenging part of discourse parser system, has drawn a lot of attention in recent years after the release of PDTB (Prasad et al., 2008), the largest available corpus with annotated implicit examples, including two shared task in CoNLL-2015 and CoNLL-2016 (Xue et al., 2015, 2016). Early attempts focused on statistical machine learning solutions with sparse linguistic features and linear models. They used several linguistically informed features like polarity tags, Levin verb classes and brown cluster etc. (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014). Recent methods for discourse relation classification have increasingly relied on neural network architectures (Ji et al., 2016; Qin et al., 2016, 2017; Shi and Demberg, 2018). However, with the high number of parameters to be trained in more and more complicated deep neural network architectures, the demand for more reliable annotated data has become even more urgent. Data extension has been a longstanding goal in implicit discourse relation classification. Wang et al. (2012) proposed to differentiate typical and atypical examples for each relation and augment trai"
W19-2703,E17-2024,1,0.850839,"arning rate of 0.01 as the optimization algorithm and set the dropout layers after embedding and output layer with drop rates of 0.5 and 0.2 respectively. The word vectors are pre-trained word embeddings from Word2Vec3 . Settings: We follow the previous works and evaluate our data on second-level 11-ways classification on PDTB with 3 settings: Lin et al. (2009) (denotes as PDTB-Lin) uses sections 2-21, 22 and 23 as train, dev and test set; Ji and Eisenstein (2015) uses sections 2-20, 0-1 and 21-22 as train, dev and test set; Moreover, we also use 10-folds cross validation among sections 0-23 (Shi and Demberg, 2017). For each experiment, the additional data is only added into the training set. Temp.Asynchronous Temp.Synchrony Cont.Cause Cont.Prag.Cause Cont.Condition Cont.Prag.Condition Comp.Contrast Comp.Prag.Contrast Comp.Concession Comp.Prag.Concession Exp.Conjunction Exp.Instantiation Exp.Restatement Exp.Alternative Exp.Exception Exp.List Results Explicitation in French, German and Czech PDTB En-Fr En-De En-Cz Figure 5 shows that the filtering by majority votes (including only two cases where at least two back-translations agree with one another vs. where all three agree) does again not change the di"
W19-2703,I17-1049,1,0.394054,"and Computer Science, Saarland University Saarland Informatic Campus, 66123 Saarbr¨ucken, Germany {w.shi, frances, vera}@coli.uni-saarland.de Abstract 1. [No one has worked out the players’ average age.]Arg1 But [most appear to be in their late 30s.]Arg2 — Explicit, Comparison.Contrast Implicit discourse relation classification is one of the most challenging and important tasks in discourse parsing, due to the lack of connectives as strong linguistic cues. A principle bottleneck to further improvement is the shortage of training data (ca. 18k instances in the Penn Discourse Treebank (PDTB)). Shi et al. (2017) proposed to acquire additional data by exploiting connectives in translation: human translators mark discourse relations which are implicit in the source language explicitly in the translation. Using back-translations of such explicitated connectives improves discourse relation parsing performance. This paper addresses the open question of whether the choice of the translation language matters, and whether multiple translations into different languages can be effectively used to improve the quality of the additional data. 1 2. [I want to add one more truck.]Arg1 (Implicit = Because) [I sense"
W19-2703,C12-1168,0,0.0532637,"Missing"
W19-2703,K15-2001,0,0.0514039,"o English, explicit relations can be easily identified by discourse parser and then original English sentences would be labeled accordingly. We follow the pipeline proposed in Shi et al. (2017), as illustrated in Figure 1, with the following differences: Related Work Recognizing implicit discourse relation, as one of the most important and challenging part of discourse parser system, has drawn a lot of attention in recent years after the release of PDTB (Prasad et al., 2008), the largest available corpus with annotated implicit examples, including two shared task in CoNLL-2015 and CoNLL-2016 (Xue et al., 2015, 2016). Early attempts focused on statistical machine learning solutions with sparse linguistic features and linear models. They used several linguistically informed features like polarity tags, Levin verb classes and brown cluster etc. (Pitler et al., 2009; Park and Cardie, 2012; Rutherford and Xue, 2014). Recent methods for discourse relation classification have increasingly relied on neural network architectures (Ji et al., 2016; Qin et al., 2016, 2017; Shi and Demberg, 2018). However, with the high number of parameters to be trained in more and more complicated deep neural network archite"
W19-2703,K16-2001,0,0.300377,"Missing"
W19-2703,D14-1196,0,0.0965883,"systematically compare the contribution of three target translation languages from different language families: French (a Romance language), German (from the Germanic language family) and Czech (a Slavic language). As all three of these languages are part of the EuroParl corpus, this also allows us to directly test whether higher quality Introduction Discourse relations connect two sentences/clauses to each other. The identification of discourse relations is an important step in natural language understanding and is beneficial to various downstream NLP applications such as text summarization (Yoshida et al., 2014; Gerani et al., 2014), question answering (Verberne et al., 2007; Jansen et al., 2014), machine translation (Guzm´an et al., 2014; Meyer et al., 2015), and so on. Discourse relations can be marked explicitly using a discourse connective or discourse adverbial such as “because”, “but”, “however”, see example 1. Explicitly marked relations are relatively easy to classify automatically (Pitler et al., 2008). In example 2, the causal relation is not marked explicitly, and can only be inferred from the texts. This second type of case is empirically even more common than explicitly marked relations"
W19-2915,N16-3012,1,0.802217,"Missing"
W19-3404,W98-1425,0,0.335701,"ious baselines by a wide margin and generates stories which are fluent as well as globally coherent. 1 Introduction Automatic story generation is the task of automatically determining the content and utilizing proper language to craft stories. One of the most important aspects of these stories is their coherence. The scope of global coherence includes arranging the contents in a plausible order, staying on topic, and creating cohesion through anaphoric expressions, etc. Traditionally, story generation is performed with symbolic planning systems (see, e.g., Meehan, 1976; Riedl and Young, 2010; Busemann and Horacek, 1998). These systems often follow a hierarchical pipeline: higher level modules perform text planning, determine discourse relations and contents of each sentence; lower level modules account for surface realization accordingly. Although capable of producing impressive, coherent stories, these systems rely heavily on manual 34 Proceedings of the Second Storytelling Workshop, pages 34–45 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics • We illustrate the possibility of guiding the direction of story generation by conditioning the generation on a latent intention var"
W19-3404,Q17-1003,1,0.715176,"he design reduces the demand on training data. 35 3 Data kens) excessive repetitions in the generated story. More precisely, when a word or phrase is repeated at least three times, the third repetition would be deleted. e.g., ‘i like the tree very very very much’ becomes ‘i like the tree very very much’. The generation terminates when the agenda is exhausted and a sentence-terminating punctuation is generated. Our work is based on the InScript corpus. We mainly utilized its event annotations and the temporal script graphs extracted from the corpus. 3.1 The InScript Corpus The InScript corpus (Modi et al., 2017a) was designed for the investigation of script knowledge. The corpus includes around 100 stories for each of 10 common daily scenarios. These stories are annotated with event types as well as participant types. For this paper, we only exploit the event type annotations. An example is shown in Figure 1. The average story length is approximately 240 tokens; the corpus includes 238k tokens in total. We use the corpus to train the neural surface realization component. 3.2 4.1 Given a scenario, the agenda generator goes through the temporal script graph and samples a path through it. For the examp"
W19-3404,W18-1505,0,0.0339549,"and Lewis, 2017; Kiddon et al., 2016). In neural text generation, some domain-specific categories of words are also informative for the progress of generation. Kiddon et al. (2016) developed an end-to-end neural text generation model, which keeps track of the usage of the keywords (e.g. recipe ingredients) with attention mechanism, and conditions surface realization on the usage of these words. NLG with Explicit Text Planning Noting that RNN based language models could only account for local coherence, attempts have been made to perform text planning on a higher level (e.g. Jain et al., 2017; Peng et al., 2018). Puduppully et al. (2018) performs content selection and planning with attention based neural networks before surface realization, to generate specifically structured NBA game summaries. Martin et al. (2018) uses sequence to sequence neural network (event2event) to generate events (represented by a verb and its most important arguments) corresponding to consecutive sentences. A second sequence to sequence model (event2sentence) generates a sentence based on the event. Our method differs from that of Martin et al. (2018), mainly in that (1) we seek to implement text coherence on a document lev"
W19-3404,N16-3012,1,0.79849,"Missing"
W19-3404,P15-1152,0,0.0840056,"Missing"
W19-3404,W17-0901,0,0.137138,"ecutive events. The outcome of this process is an agenda, a plausible sequence of events. Due to its symbolic nature, the agenda generator demands no extra training data, which is crucial for reducing the demand of data. Moreover, as the agenda generation is fully transparent and interpretable, we gain fair controllability over the content. For example, dropping a specific event from the agenda would cause the generation to skip it. Actually, it is also possible to use the surface realization module independently and generate a story from an event sequence as input. The Temporal Script Graphs Wanzare et al. (2017) compiled the InScript event annotations into temporal script graphs (see Figure 2). These directed graphs contain information on the typical temporal order of events in a script scenario, which is a crucial aspect of script knowledge. In our method, temporal script graphs are used to generate plausible sequences of events for building the agenda. 4 The Agenda Generator Our Model Overview Our model consists of three modules. Firstly, a symbolic agenda generator, which is responsible for performing text planning. Given a specific scenario (e.g., baking a cake), it produces an agenda according i"
W19-3404,W15-4639,0,0.063381,"Missing"
W19-3404,C16-1131,0,0.0306389,"t al., 2017). The nodes are the event clusters whereas the dashed boxes include some possible utterances that correspond to these clusters. ht = GRU ([xt−1 ; ept−1 ; eft−1 ]) As a direct consequence, we need to train dense vector representations of all words in the vocabulary. This is quite ambitious, as the corpus is fairly small-scale (about 238k tokens). To alleviate this data sparsity issue, we initialize our word embeddings with Google’s pre-trained word2vec vectors1 (see, e.g., Mikolov et al., 2013). The effectiveness of this domain-adaptation method in language modelling is observed in Zhang et al. (2016). As a side effect, our word embedding dimensionality is fixed at 300. To determine whether the forthcoming event has been instantiated, i.e. whether the model is ready to move onwards, we integrate a binary classifier into the architecture: Figure 3: An illustration of the surface realization module. It produces two outputs: a distribution over the vocabulary that predicts the successive word, and a boolean-valued variable that indicates whether the generation should move to the next event. For the first functionality (see Figure 4 for the model architecture), we condition the prediction of t"
W19-4003,C14-1027,0,0.128914,"coherence relations are connections between text segments that are not overtly marked. Annotating implicit coherence relations using crowd-sourcing is methodologically challenging, because assigning coherence relation labels as used in popular discourse frameworks like the Penn Discourse Treebank style (PDTB, Prasad et al., 2008, 2018) or the Rhetorical Structure Theory (RST, Mann and Thompson, 1988; Carlson et al., 2003) requires linguistic knowledge and substantial training. It is thus not possible to obtain high quality annotations of coherence relation labels from untrained crowd workers (Kawahara et al., 2014; Kishimoto et al., 2018). A more promising method for obtaining discourse annotations through crowd-sourcing is to 16 Proceedings of the 13th Linguistic Annotation Workshop, pages 16–25 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics connectives. In this work, we therefore propose a new annotation procedure which builds on the method of Scholman and Demberg (2017a). Our contributions in this paper consist of: et al. (2018) later re-annotated a portion of the relations by trained annotators, and found that the quality of the annotation from crowd-sourcing was"
W19-4003,W17-0803,1,0.09428,"identifying cases where several coherence relations may hold between two segments. They provided participants with relations that were already marked with a discourse adverbial, and asked them to additionally insert a conjunction out of a list of six highly frequent connectives (and, but, so, because, before, or). Highly frequent connectives are often ambiguous, for instance, the insertion of but does not allow us to infer whether the relation is a contrast or a concession relation. When we want to do finegrained relation annotation, providing only general connectives is thus not sufficient. Scholman and Demberg (2017a) addressed this problem by restricting the types of relations that could occur in their experiment. They selected six types of coherence relations from the overlapping part of the PDTB2.0 and RST-DT corpora, and re-annotated them using crowd-sourced annotators. Workers in this study could choose from a list of connectives which distinguish unambiguously between the six relation types of interest. For example, instead of the connective but, they provided a choice between nevertheless and by contrast. However, for annotating text more generally, we need to provide connectives that can capture"
W19-4003,L18-1637,0,0.0197435,"connections between text segments that are not overtly marked. Annotating implicit coherence relations using crowd-sourcing is methodologically challenging, because assigning coherence relation labels as used in popular discourse frameworks like the Penn Discourse Treebank style (PDTB, Prasad et al., 2008, 2018) or the Rhetorical Structure Theory (RST, Mann and Thompson, 1988; Carlson et al., 2003) requires linguistic knowledge and substantial training. It is thus not possible to obtain high quality annotations of coherence relation labels from untrained crowd workers (Kawahara et al., 2014; Kishimoto et al., 2018). A more promising method for obtaining discourse annotations through crowd-sourcing is to 16 Proceedings of the 13th Linguistic Annotation Workshop, pages 16–25 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics connectives. In this work, we therefore propose a new annotation procedure which builds on the method of Scholman and Demberg (2017a). Our contributions in this paper consist of: et al. (2018) later re-annotated a portion of the relations by trained annotators, and found that the quality of the annotation from crowd-sourcing was not satisfactory. They ar"
W19-4003,Q14-1025,0,0.0302626,"as biased to the set of options available to them and whether contexts were necessary for workers to infer the relations. The overall distributions of the annotated senses under different annotation conditions are shown in Figure 1. It can be seen that the relative distribution of the senses was maintained across different approaches, suggesting that the 2-step setup successfully replicates the results obtained from the forcechoice method. However, the distributions were statistically different across the two methods be9 We also tried aggregation by an annotation model (Dawid and Skene, 1979; Passonneau and Carpenter, 2014), but the predicted labels were mostly the same as the majority label. 21 Figure 2: (Experiment 1 results) Distribution of majority sense of the items annotated by the 2 steps approach in comparison with the forced choice approach under the context condition. Results are grouped by the original PDTB relation (titles of subgraphs). The item count of each group of relations are bracketed. the flexibility and potential of the two-step design and showed that it can be used to obtain similarly reliable annotation as in the oracle forced-choice setting. We conducted another experiment to evaluate th"
W19-4003,prasad-etal-2008-penn,0,0.162186,"abels are inferred from connectives that workers insert into the text. We show that the proposed method leads to replicable coherence annotations, and analyse the agreement between the obtained relation labels and annotations from PDTB and RSTDT on the same texts. 1 Introduction Implicit coherence relations are connections between text segments that are not overtly marked. Annotating implicit coherence relations using crowd-sourcing is methodologically challenging, because assigning coherence relation labels as used in popular discourse frameworks like the Penn Discourse Treebank style (PDTB, Prasad et al., 2008, 2018) or the Rhetorical Structure Theory (RST, Mann and Thompson, 1988; Carlson et al., 2003) requires linguistic knowledge and substantial training. It is thus not possible to obtain high quality annotations of coherence relation labels from untrained crowd workers (Kawahara et al., 2014; Kishimoto et al., 2018). A more promising method for obtaining discourse annotations through crowd-sourcing is to 16 Proceedings of the 13th Linguistic Annotation Workshop, pages 16–25 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics connectives. In this work, we therefore"
W19-4003,W18-4710,0,0.0764612,"Missing"
W19-4003,N16-3012,1,0.818791,"Missing"
W19-4003,W16-1707,0,0.0932464,"ertion Task Frances Yung† , Merel C.J. Scholman† and Vera Demberg†,‡ † Dept. of Language Science and Technology ‡ Dept. of Mathematics and Computer Science, Saarland University Saarland Informatic Campus, 66123 Saarbr¨ucken, Germany [frances|m.c.j.scholman|vera]@coli.uni-saarland.de Abstract ask workers to insert discourse connectives (Rohde et al., 2016; Scholman and Demberg, 2017a). However, this method so far has only been used in settings where it was sufficient to give workers a small set of connectives to choose from, and not in broad-coverage coherence relation annotation. For example, Rohde et al. (2016) focused on identifying cases where several coherence relations may hold between two segments. They provided participants with relations that were already marked with a discourse adverbial, and asked them to additionally insert a conjunction out of a list of six highly frequent connectives (and, but, so, because, before, or). Highly frequent connectives are often ambiguous, for instance, the insertion of but does not allow us to infer whether the relation is a contrast or a concession relation. When we want to do finegrained relation annotation, providing only general connectives is thus not s"
