2020.acl-demos.12,Q19-1038,0,0.0435449,"Missing"
2020.acl-demos.12,D14-1181,0,0.100216,"aware embeddings have a high dot product similarity score with the questions they answer. This allows for retrieval of indexed candidates using efficient nearest neighbor search.3 Introduction We introduce three new multilingual members in the universal sentence encoder (USE) (Cer et al., 2018) family of sentence embedding models. The models target performance on tasks that involve multilingual semantic similarity and achieve a new state-of-the-art in performance on monolingual and cross-lingual semantic retrieval (SR). One model targets efficient resource usage with a CNN model architecture (Kim, 2014). Another targets accuracy using the Transformer architecture (Vaswani et al., 2017). The third model provides an alternative interface to our multilingual Transformer model for use in retrieval question answering (ReQA). The 16 languages supported by our multilingual models are given in Table 1.1 3 3.1 Encoder Architecture Multi-task Dual Encoder Training Similar to Cer et al. (2018) and Chidambaram et al. (2018), we target broad coverage using a † Corresponding authors: {yinfeiy, cer}@google.com 1 Language coverage was selected based, in part, on the ease of obtaining data for the tasks used"
2020.acl-demos.12,D15-1075,0,0.132976,"Missing"
2020.acl-demos.12,D18-2012,0,0.0425165,"Missing"
2020.acl-demos.12,N16-1153,0,0.0496994,"Missing"
2020.acl-demos.12,D18-2029,1,0.918532,"Missing"
2020.acl-demos.12,D16-1264,0,0.0174513,"rt. USETrans and USECNN perform comparably on Quora. However, USETrans performs notably better than USECNN on AskUbuntu, suggesting the AskUbuntu data could be more challenging. 5.2 SQuAD Dev SQuAD Train Paragraph Retrieval USEQA Trans+Cxt BM25 (baseline) 63.5 61.6 53.3 52.4 Sentence Retrieval USEQA Trans+Cxt USETrans 53.2 47.1 43.3 37.2 Table 6: P@1 for SQuAD ReQA. Models are not trained on SQuAD. Dev and Train only refer to the respective sections of the SQuAD dataset. 5.3 Retrieval Question Answering (ReQA) Similar to the data set construction used for the SR tasks, the SQuAD v1.0 dataset (Rajpurkar et al., 2016) is transformed into a retrieval question answering (ReQA) task.14 We first break all documents in the dataset into sentences using the sentence splitter distributed with the ReQA evaluation suite.15 Each question of the (question, answer spans) tuples in the dataset is treated as a query. The task is to retrieve the sentence designated by the tuple answer span. Search is performed on a retrieval corpus consisting of all of the sentences within the corpus. We contrast sentence and paragraph-level retrieval using our models, with the later allowing for comparison against a BM25 baseline (Jones"
2020.acl-demos.12,P17-1171,0,0.0183926,"ted Nation (UN) Parallel Corpus (Ziemski et al., 2016), containing 86,000 bilingual document pairs matching English (en) documents with with their translations in five other languages: French (fr), 13 Performance is degraded from Yang et al. (2019) due to using a single sentencepiece vocabulary to cover 16 languages. Languages like Chinese, Korean, Japanese have much more characters. To ensure the vocab coverage, sentencepiece tends to split the text of these languages into single characters, which increases the difficulty of the task. 14 The retrieval question answering task was suggested by Chen et al. (2017) and then recently explored further by Cakaloglu et al. (2018). However, Cakaloglu et al. (2018)’s use of sampling makes it difficult to directly compare with their results and we provide our own baseline based on BM25. 15 https://github.com/google/ retrieval-qa-eval 16 BM25 is a strong baseline for text retrieval tasks. Paragraph-level experiments use the BM25 implementa11 The task is related to paraphrase identification (Dolan et al., 2004) and Semantic Textual Similarity (STS) (Cer et al., 2017), but with the identification of meaning similarity being assessed in the context of a retrieval"
2020.acl-demos.12,C10-1124,0,0.032039,"stion-answer pairs,7 mined translation pairs,8 and the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015).9 SNLI only contains English data. The number of mined questions-answer pairs also varies across languages with a bias toward a handful of top tier languages. To balance training across languages, we use Google’s translation system to translate SNLI to the other 15 languages. 5 7 QA pairs are mined from online forums and QA websites, including Reddit, StackOverflow, and YahooAnswers. 8 The translation pairs are mined using a system similar to the approach described in Uszkoreit et al. (2010). 9 MultiNLI (Williams et al., 2018), a more extensive corpus, contains examples from multiple sources but with different licences. Employing SNLI avoids navigating the licensing complexity of using MultiNLI to training public models. Experiments on Retrieval Tasks In this section we evaluate our multilingual encoding models on semantic retrieval, bitext and 10 While USEQA Trans+Cxt uses the same underlying shared encoder as USETrans but with additional task specific layers, we anticipate that the models could diverge in the future. 89 Model en-es en-fr en-ru en-zh USETrans 86.1 85.8 89.0 83.3"
2020.acl-demos.12,L18-1269,0,0.207529,"retrieval experiments are extended to explore cross-lingual semantic retrieval (cl-SR) and cross-lingual retrieval question answering (clReQA). SR queries and ReQA questions are machine translated into other languages, while keeping the retrieval candidates in English.18 Table 7 provides our cross-lingual retrieval results for our transformer and CNN multilingual sentence encoding models. We compare against the state-ofthe-art LASER multilingual sentence embedding 6 Experiments on Transfer Tasks For comparison with prior USE models, English task transfer performance is evaluated on SentEval (Conneau and Kiela, 2018). For sentence classification transfer tasks, the output of the sentence encoders are provided to a task specific DNN. For the pairwise semantic similarity task, the similarity of sentence ? and ? is assessed using ) ( embeddings ?? − arccos ||? |? ||, following Yang et al. (2018). In table 8, our multilingual models show competitive transfer performance when compared to stateof-the-art sentence embedding models. USETrans performs better than USECNN on all tasks. Our new tion: https://github.com/nhirakawa/BM25, with default parameters. We exclude sentence-level BM25, as BM25 generally perfor"
2020.acl-demos.12,D17-1070,0,0.0723721,"Missing"
2020.acl-demos.12,N18-1101,0,0.0835085,"Missing"
2020.acl-demos.12,C04-1051,0,0.210371,"the text of these languages into single characters, which increases the difficulty of the task. 14 The retrieval question answering task was suggested by Chen et al. (2017) and then recently explored further by Cakaloglu et al. (2018). However, Cakaloglu et al. (2018)’s use of sampling makes it difficult to directly compare with their results and we provide our own baseline based on BM25. 15 https://github.com/google/ retrieval-qa-eval 16 BM25 is a strong baseline for text retrieval tasks. Paragraph-level experiments use the BM25 implementa11 The task is related to paraphrase identification (Dolan et al., 2004) and Semantic Textual Similarity (STS) (Cer et al., 2017), but with the identification of meaning similarity being assessed in the context of a retrieval task. 12 The model for Quora is trained on Paralex (http: //knowitall.cs.washington.edu/paralex) and AskUbuntu data. The model for AskUbuntu is trained on Paralex and Quora. 90 Model Quora USETrans USECNN LASER AskUbuntu USETrans USECNN LASER Average USETrans USECNN LASER SQuAD train USEQA Trans+Cxt en ar de es fr it ja ko nl pt Cross-lingual Semantic Retrieval (cl-SR) pl ru th tr zh / zh-t 89.1 89.2 83.1 85.5 86.3 86.7 86.8 85.1 82.5 83.8 86"
2020.acl-demos.12,W18-3022,1,0.940596,"specialization for each type of task. The model training architecture is illustrated at figure 1. Transformer The transformer encoding model embeds sentences using the encoder component of the transformer architecture (Vaswani et al., 2017). Bi-directional self-attention is used to compute context-aware representations of tokens in a sentence, taking into account both the ordering and the identity of the tokens. The context-aware token representations are then averaged together to obtain a sentence-level embedding. 4 Question-answer prediction is similar to conversationalresponse prediction (Yang et al., 2018). We treat the question as the conversational input and the answer as the response. For improved answer selection, we provide a bag-of-words (BoW) context feature as an additional input to the answer encoder. For our models, we use the entire paragraph containing the answer as context. The context feature is encoded using a separate DAN encoder. CNN The CNN sentence encoding model feeds the input token sequence embeddings into a con5 https://github.com/google/ sentencepiece 6 Out-of-vocabulary characters map to an <UNK> token. 88 Task Name Retrieval Question-Answering (ReQA) Translation Rankin"
2020.acl-demos.12,L16-1561,0,0.0143952,"s using the sentence splitter distributed with the ReQA evaluation suite.15 Each question of the (question, answer spans) tuples in the dataset is treated as a query. The task is to retrieve the sentence designated by the tuple answer span. Search is performed on a retrieval corpus consisting of all of the sentences within the corpus. We contrast sentence and paragraph-level retrieval using our models, with the later allowing for comparison against a BM25 baseline (Jones et al., 2000).16 Bitext Retrieval (BR) Bitext retrieval performance is evaluated on the United Nation (UN) Parallel Corpus (Ziemski et al., 2016), containing 86,000 bilingual document pairs matching English (en) documents with with their translations in five other languages: French (fr), 13 Performance is degraded from Yang et al. (2019) due to using a single sentencepiece vocabulary to cover 16 languages. Languages like Chinese, Korean, Japanese have much more characters. To ensure the vocab coverage, sentencepiece tends to split the text of these languages into single characters, which increases the difficulty of the task. 14 The retrieval question answering task was suggested by Chen et al. (2017) and then recently explored further"
2021.acl-short.35,D19-5819,1,0.812536,"N (P@N) and Mean Reciprocal Rank (MRR). 1 For machine reading, early fusion using crossattention introduces an inductive bias to compare fine grained text spans within questions and answers. This inductive bias is missing from the single dot-product scoring operation of dual encoder retrieval models. Thus, late fusion is expected to require more training data to learn the necessary representations for fine grained comparisons. Introduction Open domain question answering (QA) involves finding answers to questions from an open corpus (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Ahmad et al., 2019). The task has led to a growing interest in scalable end-to-end retrieval systems for question answering. When QA is formulated as a reading comprehension task, cross-attention models like BERT (Devlin et al., 2019) have achieved better-than-human performance on benchmarks such as the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). Cross-attention models are especially well suited for problems involving comparisons between paired textual inputs, as they provide early fusion of fine-grained information within the pair. This encourages careful comparison and integration of"
2021.acl-short.35,P17-1171,0,0.0180574,"ns on Precision at N (P@N) and Mean Reciprocal Rank (MRR). 1 For machine reading, early fusion using crossattention introduces an inductive bias to compare fine grained text spans within questions and answers. This inductive bias is missing from the single dot-product scoring operation of dual encoder retrieval models. Thus, late fusion is expected to require more training data to learn the necessary representations for fine grained comparisons. Introduction Open domain question answering (QA) involves finding answers to questions from an open corpus (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Ahmad et al., 2019). The task has led to a growing interest in scalable end-to-end retrieval systems for question answering. When QA is formulated as a reading comprehension task, cross-attention models like BERT (Devlin et al., 2019) have achieved better-than-human performance on benchmarks such as the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). Cross-attention models are especially well suited for problems involving comparisons between paired textual inputs, as they provide early fusion of fine-grained information within the pair. This encourages careful compariso"
2021.acl-short.35,N19-1423,0,0.190729,"missing from the single dot-product scoring operation of dual encoder retrieval models. Thus, late fusion is expected to require more training data to learn the necessary representations for fine grained comparisons. Introduction Open domain question answering (QA) involves finding answers to questions from an open corpus (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Ahmad et al., 2019). The task has led to a growing interest in scalable end-to-end retrieval systems for question answering. When QA is formulated as a reading comprehension task, cross-attention models like BERT (Devlin et al., 2019) have achieved better-than-human performance on benchmarks such as the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). Cross-attention models are especially well suited for problems involving comparisons between paired textual inputs, as they provide early fusion of fine-grained information within the pair. This encourages careful comparison and integration of details across and within the two texts. However, early fusion across questions and answers is a poor fit for retrieval, since it prevents precomputation of the answer representations. Rather, To support learning im"
2021.acl-short.35,W18-6317,1,0.880636,"Missing"
2021.acl-short.35,2020.emnlp-main.550,0,0.0110768,"comprehension model (Jurafsky and Martin, 2018; Kratzwald and Feuerriegel, 2018; Yang et al., 2019a). Prior work has focused on the answer span annotation task and has even achieved super human performance on some datasets. However, the evaluations implicitly assume the trivial availability of passages for each question that are likely to contain the correct answer. While the retrieval task can be approached using traditional keyword based retrieval methods such as BM25, there is a growing interest in developing more sophisticated neural retrieval methods (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020). 3 Dataset Training Pairs NQ SQuAD 106,521 87,133 Questions 4,131 10,485 Test Candidates 22,118 10,642 Table 1: Statistics of MutiReQA NQ and SQuAD tasks: # of training pairs, # of questions, # of candidates. Methodology In this section we describe the proposed approach using a neural retrieval model augmented with su1 https://github.com/ google-research-datasets/MultiReQA Ques 1, Ans 1, S1 Ques 2, Ans 2, S2 … Ques n, Ans n, Sn Answer Candidate Corpus Nerual Retrieval Model Model Training Figure 1: Use of a cross-attention model for the supervised mining of additional QA pairs. Our accurate c"
2021.acl-short.35,D18-1055,0,0.0353467,"Missing"
2021.acl-short.35,P19-1612,0,0.0344025,"Missing"
2021.acl-short.35,D16-1264,0,0.037391,"aining data to learn the necessary representations for fine grained comparisons. Introduction Open domain question answering (QA) involves finding answers to questions from an open corpus (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Ahmad et al., 2019). The task has led to a growing interest in scalable end-to-end retrieval systems for question answering. When QA is formulated as a reading comprehension task, cross-attention models like BERT (Devlin et al., 2019) have achieved better-than-human performance on benchmarks such as the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). Cross-attention models are especially well suited for problems involving comparisons between paired textual inputs, as they provide early fusion of fine-grained information within the pair. This encourages careful comparison and integration of details across and within the two texts. However, early fusion across questions and answers is a poor fit for retrieval, since it prevents precomputation of the answer representations. Rather, To support learning improved representations for retrieval, we explore a supervised data augmentation approach leveraging a complex classification model with cro"
2021.acl-short.35,2020.emnlp-main.477,1,0.838667,"l. After training the cross-attention model, we retrieve additional potential answers to questions using an off-the-shelf retrieval system2 . The predicted scores from our classifier with cross-attention are then used to weight and filter the retrieved candidates with positive examples serving as additional training data for the dual encoder based retrieval model. Retrieval Question-Answering (ReQA) Ahmad et al. (2019) introduced Retrieval QuestionAnswering (ReQA), a task that has been rapidly adopted by the community (Guo et al., 2020; Chang et al., 2020; Ma et al., 2020; Zhao and Lee, 2020; Roy et al., 2020). Given a question, the task is to retrieve the answer sentence from a corpus of candidates. ReQA provides direct evaluation of retrieval, independent of span annotation. Compare to Open Domain QA, ReQA focuses on evaluating the retrieval component and, by construction, avoids the need for span annotation. We explore the proposed approach on MultiReQA-NQ and MultiReQA-SQuAD (Guo et al., 2020).1 MultiReQA (Guo et al., 2020) established standardized training / dev / test splits. Statistics for each tasks are listed in Table 1. 4 Question Text Pretrained Retrieval Module 4.1 BERT Classification M"
2021.acl-short.35,P08-1082,0,0.0780697,"odels directly trained with gold annotations on Precision at N (P@N) and Mean Reciprocal Rank (MRR). 1 For machine reading, early fusion using crossattention introduces an inductive bias to compare fine grained text spans within questions and answers. This inductive bias is missing from the single dot-product scoring operation of dual encoder retrieval models. Thus, late fusion is expected to require more training data to learn the necessary representations for fine grained comparisons. Introduction Open domain question answering (QA) involves finding answers to questions from an open corpus (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Ahmad et al., 2019). The task has led to a growing interest in scalable end-to-end retrieval systems for question answering. When QA is formulated as a reading comprehension task, cross-attention models like BERT (Devlin et al., 2019) have achieved better-than-human performance on benchmarks such as the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). Cross-attention models are especially well suited for problems involving comparisons between paired textual inputs, as they provide early fusion of fine-grained information within the p"
2021.acl-short.35,W19-4108,0,0.15099,"ional Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 263–268 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Neural Passage Retrieval for Open Domain Question Answering Cross-attention Teacher Model Ques 1, Ans 1 Ques 2, Ans 2 … Ques n, Ans n Open domain question answering systems usually follow a two-step approach: first retrieve question relevant passages, and then scan the returned text to identify the answer span using a reading comprehension model (Jurafsky and Martin, 2018; Kratzwald and Feuerriegel, 2018; Yang et al., 2019a). Prior work has focused on the answer span annotation task and has even achieved super human performance on some datasets. However, the evaluations implicitly assume the trivial availability of passages for each question that are likely to contain the correct answer. While the retrieval task can be approached using traditional keyword based retrieval methods such as BM25, there is a growing interest in developing more sophisticated neural retrieval methods (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020). 3 Dataset Training Pairs NQ SQuAD 106,521 87,133 Questions 4,131 10,485 Te"
2021.acl-short.35,D15-1237,0,0.0903257,"Missing"
2021.acl-short.35,2020.acl-demos.12,1,0.829817,"Missing"
2021.acl-short.35,2020.acl-demos.5,0,0.0110809,"ing a retrieval model. After training the cross-attention model, we retrieve additional potential answers to questions using an off-the-shelf retrieval system2 . The predicted scores from our classifier with cross-attention are then used to weight and filter the retrieved candidates with positive examples serving as additional training data for the dual encoder based retrieval model. Retrieval Question-Answering (ReQA) Ahmad et al. (2019) introduced Retrieval QuestionAnswering (ReQA), a task that has been rapidly adopted by the community (Guo et al., 2020; Chang et al., 2020; Ma et al., 2020; Zhao and Lee, 2020; Roy et al., 2020). Given a question, the task is to retrieve the answer sentence from a corpus of candidates. ReQA provides direct evaluation of retrieval, independent of span annotation. Compare to Open Domain QA, ReQA focuses on evaluating the retrieval component and, by construction, avoids the need for span annotation. We explore the proposed approach on MultiReQA-NQ and MultiReQA-SQuAD (Guo et al., 2020).1 MultiReQA (Guo et al., 2020) established standardized training / dev / test splits. Statistics for each tasks are listed in Table 1. 4 Question Text Pretrained Retrieval Module 4.1 BE"
2021.adaptnlp-1.10,P17-1171,0,0.247151,"or “reading comprehension” which aims to extract a short answer span from a given passage. Rather than just identifying answers within a short preselected passage that is provided to the model effectively by an oracle, retrieving sentence-level answers from a large pool of candidates directly addresses the realworld problem of searching for answers within a corpus. Sentences retrieved as answers in this manner can be used directly to answer questions. Alternatively, retrieved sentences, as well as possibly the passages that contains them, can be provided to a traditional Open Domain QA model (Chen et al., 2017; Karpukhin et al., 2020). Recent research has shown promising results on developing neural models for retrieval tasks including ReQA, MS MARCO, and the retrieval part of open domain question QA (Roy et al., 2020; Karpukhin et al., 2020; Xiong et al., 2020; Luan et al., 2020). One challenge of employing neural models is that it usually requires a large amount of training data. While it is possible to get such data from a general domain, it may hard to get similar data for specialized domains, which is a common Introduction Retrieval-based question answering (QA) investigates the problem of fin"
2021.adaptnlp-1.10,N19-1423,0,0.229329,"a general domain model can perform on domain specific QA tasks or even the extent of transfer possible across different specialized domains. In order to further investigate these questions within the context of the ReQA task, we propose a new common evaluation suite consisting of eight new datasets extracted from existing QA datasets. Five in-domain tasks include training and test data, while three out-of-domain tasks contain only test data. We provide cross domain baselines for neural and non-neural retrieval methods. Our baseline experiments use two competitive neural models, based on BERT (Devlin et al., 2019) and USEQA (Yang et al., 2019), respectively, and BM25, a strong information retrieval baseline. BM25 performs surprisingly well on many retrieval question answering tasks, achieving the best performance on two of five in-domain tasks and all three out-ofdomain tasks. Neural models achieve the highest performance on three of five in-domain tasks, outperforming BM25 by a wide margin on tasks with less token overlap between question and answer. Comparing general models trained on a mixture of QA training sets to specialized in-domain models trained on a single QA task reveals that models trained"
2021.adaptnlp-1.10,D19-5801,0,0.0687428,"f questions, and ranki is the rank of the first correct answer for the ith question. 4 Retrieval models are often measured by P@N (N=1,3,5,10). However, as our main concern is whether the question is correctly answered, we focus on P@1. 95 3 Multi-domain ReQA (MultiReQA) TextbookQA Multi-modal question-answer pairs taken from middle school science curricula (Kembhavi et al., 2017). In this paper, we only consider the text aspect of this task as defined by the original MRQA shared task. The multi-domain ReQA (MultiReQA) test suite is composed of select datasets drawn from the MRQA shared task (Fisch et al., 2019a).5 We follow the training, in-domain test, out-of-domain test splits defined in MRQA. The individual datasets are described below: Table 2 provides example question-answer sentence pairs. Datasets are converted from a span identification task to sentence-level retrieval. The questions from the original data are used without modification. Supporting documents are split into sentences using NLTK. 6 All resulting sentences become retrieval candidates. Answer spans identify sentences containing the correct answers. Spans covering multiple sentence are excluded.7 SearchQA Jeopardy question-answer"
2021.adaptnlp-1.10,W18-6317,1,0.810493,"trieval, with the “Best Matching 25” (BM25) family of ranking functions providing a well established baseline (Robertson and Zaragoza, 2009). In previous work on open domain question answering, BM25 has been used to retrieve eviBERT BERT dual encoders are used for retrieval tasks like translation retrieval (Feng et al., 2020) and QA passage retrieval (Roy et al., 2020; Karpukhin et al., 2020). We explore a BERT dual encoder as our first neural baseline, using the BERTBASE model,8 due to memory constraints.9 details of dual encoder training with negative sampling, see Gillick et al. (2018) and Guo et al. (2018). 10 Note that we switch the final activation layer of the BERT CLS token from tanh to gelu. 11 https://tfhub.dev/google/universal-sentence-encodermultilingual-qa/1 12 USE-QA uses a 6 layer transformer with 8 attention heads, a hidden size of 512 and a filter size of 2048. The context DAN encoder uses hidden sizes [320, 320, 512, 512] with residual connections. The feed-forward networks for question and answer both use hidden sizes [320, 512], so the final dimension of the encodings is 512. 8 The BERTBASE model uses 12 transformer layers with 12 attention heads, a hidden size of 768 and a filt"
2021.adaptnlp-1.10,P15-1162,0,0.0477048,"Missing"
2021.adaptnlp-1.10,P17-1147,0,0.36628,"identification task to sentence-level retrieval. The questions from the original data are used without modification. Supporting documents are split into sentences using NLTK. 6 All resulting sentences become retrieval candidates. Answer spans identify sentences containing the correct answers. Spans covering multiple sentence are excluded.7 SearchQA Jeopardy question-answer pairs augmented with text snippets retrieved by Google (Dunn et al., 2017). TriviaQA Trivia enthusiasts authored questionanswer pairs. Answers are drawn from Wikipedia and Bing web search results, excluding trivia websites (Joshi et al., 2017b). Table 3 provides statistics on the number of training set pairs and the number of questions, candidates and average answers per question in the evaluation data. Table 4 shows the average length of word tokens and degree of token overlap. SearchQA and HotpotQA have supporting documents split by [DOC]/[PAR] tags, so they have comparatively shorter context length. TriviaQA has much longer context length because all supporting documents were tokenized as one due to lack of clear division among special tags in the dataset. NaturalQuestions contain lists and tables that bring up the average answ"
2021.adaptnlp-1.10,Q19-1026,0,0.014614,"2018). The annotators generate the questions knowing the answers and the supporting contexts. SQuAD 1.1 Wikipedia question-answer pairs (Rajpurkar et al., 2016a). Given the supporting contexts from Wikipedia, the annotators were asked to write questions such that the answers could be found in the contexts. Moreover, many of the questions are directly formed from parts of the supporting contexts. NaturalQuestions (NQ) Questions are real queries issued by multiple users to Google search that retrieve a Wikipedia page in the top five search results. Answer text is drawn from the search results (Kwiatkowski et al., 2019). We removed the duplicate question-answer pairs in the in-domain test split, since during the original dataset construction, multiple raters were asked to select answers from the paragraphs. Unlike ReQA (Ahmad et al., 2019), we did not limit the questions and candidates to be only within the HTML paragraph block, and the candidates could contain lists and tables. 6 As the datasets SearchQA, TriviaQA and HotpotQA contain special tags [DOC], [PAR], [SEP], and [TLE], we perform dataset-specific pre-processing to handle context splitting and tag removal. TriviaQA has [DOC] [TLE] [PAR] tags, but w"
2021.adaptnlp-1.10,P19-1612,0,0.0689292,"eval tasks including ReQA, MS MARCO, and the retrieval part of open domain question QA (Roy et al., 2020; Karpukhin et al., 2020; Xiong et al., 2020; Luan et al., 2020). One challenge of employing neural models is that it usually requires a large amount of training data. While it is possible to get such data from a general domain, it may hard to get similar data for specialized domains, which is a common Introduction Retrieval-based question answering (QA) investigates the problem of finding answers to questions from an open corpus (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Lee et al., 2019; Ahmad et al., 2019; Chang et al., 2020; Ma et al., 2020). There is a growing interest in building scalable end-to-end question answering systems for large scale retrieval (Ahmad et al., 2019; Roy et al., 2020). Retrieval question answering (ReQA) (Ahmad et al., 2019), illustrated in Table 1, defines the task as directly retrieving an answer sentence from a corpus.2 Motivated by real applications ∗ Corresponding authors: {xyguo, yinfeiy}@google.com Work done during an internship at Google Research. 1 We released the sentence boundary annotation of MultiReQA: https://github.com/ google-researc"
2021.adaptnlp-1.10,radev-etal-2002-evaluating,0,0.0318601,"ins. While a general neural model covering all domains is achievable, the best performing neural model is often obtained by training exclusively on in-domain data. 2 Retrieval QA (ReQA) ReQA formalizes the retrieval-based QA task as the identification of a sentence in-context that answers a provided question (Ahmad et al., 2019). Retrieval QA models are evaluated using Precision at 1 (P@1) and Mean Reciprocal Rank (MRR). The P@1 score tests whether the true answer sentence appears as the top-ranked candidate4 . MRR, introduced for the evaluation of retrieval based QA systems (Voorhees, 2001;P Radev et al., 2002), is 1 calculated as MRR = N1 N i=1 ranki , where N is the total number of questions, and ranki is the rank of the first correct answer for the ith question. 4 Retrieval models are often measured by P@N (N=1,3,5,10). However, as our main concern is whether the question is correctly answered, we focus on P@1. 95 3 Multi-domain ReQA (MultiReQA) TextbookQA Multi-modal question-answer pairs taken from middle school science curricula (Kembhavi et al., 2017). In this paper, we only consider the text aspect of this task as defined by the original MRQA shared task. The multi-domain ReQA (MultiReQA) te"
2021.adaptnlp-1.10,D16-1264,0,0.258178,"e, and SQuAD questions are written with advance knowledge of the answers and supporting contexts. However, even though HotpotQA questions are also written with the knowledge of the answers and contexts, the degree of overlap is quite low likely due to the inclusion of multi-document inference. HotpotQA Wikipedia question-answer pairs. This dataset differs from the others in that the questions require reasoning over multiple supporting documents (Yang et al., 2018). The annotators generate the questions knowing the answers and the supporting contexts. SQuAD 1.1 Wikipedia question-answer pairs (Rajpurkar et al., 2016a). Given the supporting contexts from Wikipedia, the annotators were asked to write questions such that the answers could be found in the contexts. Moreover, many of the questions are directly formed from parts of the supporting contexts. NaturalQuestions (NQ) Questions are real queries issued by multiple users to Google search that retrieve a Wikipedia page in the top five search results. Answer text is drawn from the search results (Kwiatkowski et al., 2019). We removed the duplicate question-answer pairs in the in-domain test split, since during the original dataset construction, multiple"
2021.adaptnlp-1.10,2020.emnlp-main.477,1,0.921498,"acle, retrieving sentence-level answers from a large pool of candidates directly addresses the realworld problem of searching for answers within a corpus. Sentences retrieved as answers in this manner can be used directly to answer questions. Alternatively, retrieved sentences, as well as possibly the passages that contains them, can be provided to a traditional Open Domain QA model (Chen et al., 2017; Karpukhin et al., 2020). Recent research has shown promising results on developing neural models for retrieval tasks including ReQA, MS MARCO, and the retrieval part of open domain question QA (Roy et al., 2020; Karpukhin et al., 2020; Xiong et al., 2020; Luan et al., 2020). One challenge of employing neural models is that it usually requires a large amount of training data. While it is possible to get such data from a general domain, it may hard to get similar data for specialized domains, which is a common Introduction Retrieval-based question answering (QA) investigates the problem of finding answers to questions from an open corpus (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Lee et al., 2019; Ahmad et al., 2019; Chang et al., 2020; Ma et al., 2020). There is a growing interest"
2021.adaptnlp-1.10,D18-1052,0,0.0125436,"5’s token overlap heuristic is effective over large spans of text, while the neural model obtains a “deeper” semantic understanding and thus extracts more signal out of a single sentence. 20 Error Analysis Related Work Open domain QA involves finding answers to questions within large document collections (Voorhees and Tice, 2000). The ground-truth answer for many evaluations is a span often containing a word or a short phrase (i.a., Kwiatkowski et al. (2019); Chen et al. (2017); Rajpurkar et al. (2016b)). Karpukhin et al. (2020) and Xiong et al. (2020) explored passage level retrieval for QA. Seo et al. (2018) constructs a phrase-indexed QA challenge benchmark retrieving phrases, allowing for a direct F1 and exact-match evaluation on SQuAD. (Seo et al., 2019) demonstrates phrase-indexed QA systems can be built using a combination of dense (neural) and sparse (term-frequency based) 21 Note that even if the models retrieve different answers, both answers could still be correct. We report P@1 here, but observed similar trends in MRR. 100 Example 1 (from NQ): what kind of fish live in the salton sea Correct Answer: [...] Due to the high salinity , very few fish species can tolerate living in the Salton"
2021.adaptnlp-1.10,P19-1436,0,0.0112695,"re signal out of a single sentence. 20 Error Analysis Related Work Open domain QA involves finding answers to questions within large document collections (Voorhees and Tice, 2000). The ground-truth answer for many evaluations is a span often containing a word or a short phrase (i.a., Kwiatkowski et al. (2019); Chen et al. (2017); Rajpurkar et al. (2016b)). Karpukhin et al. (2020) and Xiong et al. (2020) explored passage level retrieval for QA. Seo et al. (2018) constructs a phrase-indexed QA challenge benchmark retrieving phrases, allowing for a direct F1 and exact-match evaluation on SQuAD. (Seo et al., 2019) demonstrates phrase-indexed QA systems can be built using a combination of dense (neural) and sparse (term-frequency based) 21 Note that even if the models retrieve different answers, both answers could still be correct. We report P@1 here, but observed similar trends in MRR. 100 Example 1 (from NQ): what kind of fish live in the salton sea Correct Answer: [...] Due to the high salinity , very few fish species can tolerate living in the Salton Sea . Introduced tilapia are the main fish that can tolerate the high salinity levels and pollution . Other freshwater fish species live in the rivers"
2021.adaptnlp-1.10,P08-1082,0,0.0743642,"shown promising results on developing neural models for retrieval tasks including ReQA, MS MARCO, and the retrieval part of open domain question QA (Roy et al., 2020; Karpukhin et al., 2020; Xiong et al., 2020; Luan et al., 2020). One challenge of employing neural models is that it usually requires a large amount of training data. While it is possible to get such data from a general domain, it may hard to get similar data for specialized domains, which is a common Introduction Retrieval-based question answering (QA) investigates the problem of finding answers to questions from an open corpus (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Lee et al., 2019; Ahmad et al., 2019; Chang et al., 2020; Ma et al., 2020). There is a growing interest in building scalable end-to-end question answering systems for large scale retrieval (Ahmad et al., 2019; Roy et al., 2020). Retrieval question answering (ReQA) (Ahmad et al., 2019), illustrated in Table 1, defines the task as directly retrieving an answer sentence from a corpus.2 Motivated by real applications ∗ Corresponding authors: {xyguo, yinfeiy}@google.com Work done during an internship at Google Research. 1 We released the sentence boundary ann"
2021.adaptnlp-1.10,D15-1237,0,0.0932553,"Missing"
2021.adaptnlp-1.10,D18-1259,0,0.0193569,"swer length. SearchQA and SQuAD have high degree of question/answer overlap because the supporting documents in SearchQA are retrieved by search engine, and SQuAD questions are written with advance knowledge of the answers and supporting contexts. However, even though HotpotQA questions are also written with the knowledge of the answers and contexts, the degree of overlap is quite low likely due to the inclusion of multi-document inference. HotpotQA Wikipedia question-answer pairs. This dataset differs from the others in that the questions require reasoning over multiple supporting documents (Yang et al., 2018). The annotators generate the questions knowing the answers and the supporting contexts. SQuAD 1.1 Wikipedia question-answer pairs (Rajpurkar et al., 2016a). Given the supporting contexts from Wikipedia, the annotators were asked to write questions such that the answers could be found in the contexts. Moreover, many of the questions are directly formed from parts of the supporting contexts. NaturalQuestions (NQ) Questions are real queries issued by multiple users to Google search that retrieve a Wikipedia page in the top five search results. Answer text is drawn from the search results (Kwiatk"
2021.eacl-main.249,S17-2001,1,0.926212,"rating. Caption-caption and image-image candidates are referred to as C2C and I2I, respectively. I2I pairs are selected with the above other-modality method. For C2C pairs, we sample half the pairs using the other-modality method and half from within cocaptions. The latter introduces (mostly) positive associations between caption pairs describing the same image. This gives a balanced set of caption pairs describing same and different images. Pairs in C2C and I2I are scored by in-house raters using a continuous scale between 0 and 5. We adopt the widely used Semantic Textual Similarity (STS) (Cer et al., 2017) for text pairs and extend it to images to define Semantic Image Similarity (SIS). To recognize that this is a graded (rather than discrete) judgment, we encouraged raters to select scores like 1.3 and obtain the final score for a pair as the average of five individual ratings. Intermodality We select caption-image candidates C2I based on human ratings for I2I and C2C pairs. We mainly seek new positive matches like those identified by annotators in Ilharco et al. (2019). For each I2I pair (ij , ik ), a C2I pair (ck , ij ) is generated, where ck is a MS-COCO caption for ik . We generate pairs f"
2021.eacl-main.249,D14-1162,0,0.0885562,"vn ) and C (c1 ...cn ) (the latter representing cocaption groups of five captions each). Each item is encoded with an off-the-shelf unimodal model. Cosine similarity between items defines two symmetric matrices: S C (pairwise caption similarities) and S V (pairwise image similarities). The diagonals are set to zero to not sample identical items. We encode images with Graph-RISE (486) and construct S I , the image-based similarity for pairs of co-caption groups. We encode captions with Universal Sentence Encoder (USE) (Cer et al., 2018) and average bag of words (BoW) based on GloVe embeddings (Pennington et al., 2014). Co-caption representations are averaged to create a single representation. From these, we construct S C , the caption-based similarity for images pairs. USE and BoW embeddings produce two S C matrices, but we gloss over this detail below. We use S C to select image pairs and S I for caption pairs. Because of the cross-modal semantic gap, diversity and size of the underlying data, these pairs exhibit a wide range of similarity. Selecting the five most similar items (according to modelbased S V and S C ) thus produces good representation of varying amounts of similarity as judged by people. Be"
2021.eacl-main.249,W10-0721,0,0.0606111,"nstrates language’s power to improve image representations (Juan et al., 2020). Learning representations for both vision and language jointly should be even more effective—indeed, much progress has been made on such cross-modal learning using image captioning data (Karpathy and Li, 2015; Harwath and Glass, 2017; Faghri et al., 2018; Li et al., 2019). However, it is not yet clear whether learning representations in multimodal contexts improves performance within as well as across modalities as there are no datasets ideally suited for this at present. Image captioning datasets such as Flickr8k (Rashtchian et al., 2010), Flickr30k (Young et al., 2014), Multi30k (Elliott et al., 2016), Microsoft Common Objects in COntext (MS-COCO) (Lin et al., 2014), and Conceptual Captions (Sharma et al., 2018) only capture relationships between images and textual captions created for them. They miss many valid relationships between unassociated images and captions, from captions to other captions, and from images to other images. We address this gap with Crisscrossed Captions (CxC, exemplified in Figure 1), a dataset with graded, denser annotations for relationships between and among captions and images in the MS-COCO evalu"
2021.emnlp-main.470,W19-4330,1,0.901639,"Missing"
2021.emnlp-main.470,2020.acl-main.747,0,0.043076,"elevant” item. In the case of strong alignment, for the observation that removing language inforany representation, all semantically relevant items mation is able to improve the cross-lingual are closer than all irrelevant items, regardless of transfer performance. their language. Roy et al. (2020) show sentence representations from the same language tend to cluster 1 Introduction in weak-alignment system. Similar phenomena can Recently, large-scale language modeling has ex- be observed on other pre-trained multilingual modpanded from English to the multilingual setting els like mBERT, XLM-R (Conneau et al., 2020) (i.a., Devlin et al. (2019); Conneau and Lample and CMLM (Yang et al., 2020). Roy et al. (2020) (2019); Conneau et al. (2020)). Although these provides carefully-designed training strategies for models are trained with language modeling ob- retrieval-like model to mitigate this issue in order jectives on monolingual data, i.e. without cross- to obtain language agnostic multilingual systems. lingual information, these multilingual systems exWe systematically explore a simple post-training hibit impressive zero-shot cross-lingual ability (Hu method we refer to as Language Information Reet al.,"
2021.emnlp-main.470,L18-1269,0,0.0150963,"XEVAL include Movie Reviews (Pang and Lee, 2005), binary SST (sentiment analysis, Socher et al. (2013)), MPQA (opinionpolarity, Wiebe et al. (2005)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)) and SICK (both entailment and relatedness (Marelli et al., 2014)). For this evaluation, we use mBERT as the base multilingual encoder. Still the weights of mBERT are fixed during training and only downstream neural structures are trained. The training, cross-validation and evaluation uses SentEval toolkit (Conneau and Kiela, 2018). Results are presented in Table 3. The metric is the averaging performance across 9 datasets mentioned above. Introducing LIR is beneficial on German, Spanish, French and Chinese. We also notice that for English dataset, removing principal components actually hurts the performance. This observation also echoes with findings in previous 4 Related Work & Our Novelty English sentence embedding works, e.g. Yang et al. (2019b). We speculate this is because English data Different training methods have been proposed to are dominant in mBERT training data. Therefore obtain language agnostic represent"
2021.emnlp-main.470,N19-1423,0,0.0259685,"strong alignment, for the observation that removing language inforany representation, all semantically relevant items mation is able to improve the cross-lingual are closer than all irrelevant items, regardless of transfer performance. their language. Roy et al. (2020) show sentence representations from the same language tend to cluster 1 Introduction in weak-alignment system. Similar phenomena can Recently, large-scale language modeling has ex- be observed on other pre-trained multilingual modpanded from English to the multilingual setting els like mBERT, XLM-R (Conneau et al., 2020) (i.a., Devlin et al. (2019); Conneau and Lample and CMLM (Yang et al., 2020). Roy et al. (2020) (2019); Conneau et al. (2020)). Although these provides carefully-designed training strategies for models are trained with language modeling ob- retrieval-like model to mitigate this issue in order jectives on monolingual data, i.e. without cross- to obtain language agnostic multilingual systems. lingual information, these multilingual systems exWe systematically explore a simple post-training hibit impressive zero-shot cross-lingual ability (Hu method we refer to as Language Information Reet al., 2020b). These observations r"
2021.emnlp-main.470,E14-1049,0,0.0873866,"Missing"
2021.emnlp-main.470,2020.lrec-1.297,0,0.0235931,"is section we describe Language Information Removal (LIR) to address the self language bias in cT e L multilingual embeddings (Yang et al., 2020). The (1) eL := eL − cL L keL k2 first step is to extract the language identity information for each language space. Given a multi- 3 Experiments lingual embedding system E, e.g. multilingual BERT, and a collection of multilingual texts {tiL }, In the following experiments, sentences used for extracting principle components are sampled from where tiL denotes the ith phrase in the collection for the language L. We construct a language matrix Wiki-40B (Guo et al., 2020). We use 10,000 sentences per language. We notice performance iniML ∈ Rn×d for language L, where n denotes the tially increases as more sentences are used but number of sentences in language L and d denotes then is almost unchanged after n &gt; 10, 000. We the dimension of the representation. The row i of tried different samplings of {tiL } and text resources ML is the representation of tiL computed by E. Second, we extract language identification com- other than Wiki-40B, e.g., Tatoeba (Artetxe and Schwenk, 2019). The minimal differences in perponents for each language. One observation in forman"
2021.emnlp-main.470,2020.emnlp-main.363,0,0.0417805,"Missing"
2021.emnlp-main.470,2020.acl-main.653,0,0.0809888,"Missing"
2021.emnlp-main.470,P19-1493,0,0.0522307,"Missing"
2021.emnlp-main.470,marelli-etal-2014-sick,0,0.00980066,"previous sections. In this section, we apply LIR in XEVAL, a collection of multilingual sentence representation benchmark (Yang et al., 2020). The training set and test set of XEVAL are in the same language (i.e. the evaluation is not cross-lingual). Benchmarks on XEVAL include Movie Reviews (Pang and Lee, 2005), binary SST (sentiment analysis, Socher et al. (2013)), MPQA (opinionpolarity, Wiebe et al. (2005)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)) and SICK (both entailment and relatedness (Marelli et al., 2014)). For this evaluation, we use mBERT as the base multilingual encoder. Still the weights of mBERT are fixed during training and only downstream neural structures are trained. The training, cross-validation and evaluation uses SentEval toolkit (Conneau and Kiela, 2018). Results are presented in Table 3. The metric is the averaging performance across 9 datasets mentioned above. Introducing LIR is beneficial on German, Spanish, French and Chinese. We also notice that for English dataset, removing principal components actually hurts the performance. This observation also echoes with findings in pr"
2021.emnlp-main.470,Q17-1022,0,0.0613744,"Missing"
2021.emnlp-main.470,P04-1035,0,0.00944819,"rent datasets. We have tested LIR on cross-lingual benchmarks in previous sections. In this section, we apply LIR in XEVAL, a collection of multilingual sentence representation benchmark (Yang et al., 2020). The training set and test set of XEVAL are in the same language (i.e. the evaluation is not cross-lingual). Benchmarks on XEVAL include Movie Reviews (Pang and Lee, 2005), binary SST (sentiment analysis, Socher et al. (2013)), MPQA (opinionpolarity, Wiebe et al. (2005)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)) and SICK (both entailment and relatedness (Marelli et al., 2014)). For this evaluation, we use mBERT as the base multilingual encoder. Still the weights of mBERT are fixed during training and only downstream neural structures are trained. The training, cross-validation and evaluation uses SentEval toolkit (Conneau and Kiela, 2018). Results are presented in Table 3. The metric is the averaging performance across 9 datasets mentioned above. Introducing LIR is beneficial on German, Spanish, French and Chinese. We also notice that for English dataset, removing principal components actually hurts"
2021.emnlp-main.470,2021.starsem-1.22,0,0.0686634,"Missing"
2021.emnlp-main.470,P05-1015,0,0.153952,"ment models, removing just the first component should be adequate for cross-lingual retrieval (table 1). For tasks like classification and sentiment analysis (tables 2 and 3), the optimal number of components to remove seems to vary on different datasets. We have tested LIR on cross-lingual benchmarks in previous sections. In this section, we apply LIR in XEVAL, a collection of multilingual sentence representation benchmark (Yang et al., 2020). The training set and test set of XEVAL are in the same language (i.e. the evaluation is not cross-lingual). Benchmarks on XEVAL include Movie Reviews (Pang and Lee, 2005), binary SST (sentiment analysis, Socher et al. (2013)), MPQA (opinionpolarity, Wiebe et al. (2005)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)) and SICK (both entailment and relatedness (Marelli et al., 2014)). For this evaluation, we use mBERT as the base multilingual encoder. Still the weights of mBERT are fixed during training and only downstream neural structures are trained. The training, cross-validation and evaluation uses SentEval toolkit (Conneau and Kiela, 2018). Results are presented i"
2021.emnlp-main.470,P10-1114,0,0.0613697,"Missing"
2021.emnlp-main.470,2020.emnlp-main.477,1,0.868662,"Missing"
2021.emnlp-main.470,D13-1170,0,0.00355554,"be adequate for cross-lingual retrieval (table 1). For tasks like classification and sentiment analysis (tables 2 and 3), the optimal number of components to remove seems to vary on different datasets. We have tested LIR on cross-lingual benchmarks in previous sections. In this section, we apply LIR in XEVAL, a collection of multilingual sentence representation benchmark (Yang et al., 2020). The training set and test set of XEVAL are in the same language (i.e. the evaluation is not cross-lingual). Benchmarks on XEVAL include Movie Reviews (Pang and Lee, 2005), binary SST (sentiment analysis, Socher et al. (2013)), MPQA (opinionpolarity, Wiebe et al. (2005)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)) and SICK (both entailment and relatedness (Marelli et al., 2014)). For this evaluation, we use mBERT as the base multilingual encoder. Still the weights of mBERT are fixed during training and only downstream neural structures are trained. The training, cross-validation and evaluation uses SentEval toolkit (Conneau and Kiela, 2018). Results are presented in Table 3. The metric is the averaging performance acr"
2021.emnlp-main.470,2020.acl-demos.12,1,0.841537,"Missing"
2021.emnlp-main.470,D19-1059,1,0.898727,"Missing"
2021.emnlp-main.502,Q19-1038,0,0.0745736,"hoosing such configurations, training details and potential limitations of CMLM are discussed in the appendix. 4.5 4.5.1 Evaluations XEVAL: Multilingual Benchmarks for Sentence Representations Evaluation The method to produce sentence representations for mBERT and XLM-R is chosen to be average pooling after exploring options including [CLS] representations and max pooling. The multilingual model CMLM trained on monolingual data outperform all baselines in all 15 languages. Results of models trained with cross-lingual data are presented in Table 4. Baseline models for comparison include LASER (Artetxe and Schwenk (2019), trained with parallel data) and multilingual USE ((Yang et al., 2019a), trained with crosslingual NLI. Note it only supports 16 languages). Our model (S3) outperforms LASER in all 15 languages. Notably, finetuning with NLI in the crosslingual way produces significant improvement (S3 + NLI v.s. S3). Multitask learning with CMLM and BR can also be used to increase the performance of pretrained encoders, e.g. mBERT. mBERT trained with CMLM and BR (f-mBERT) has a significant improvement upon mBERT. Evaluations in previous multilingual literature focused on the cross-lingual transfer learning abi"
2021.emnlp-main.502,D18-2029,1,0.922883,"self language bias in multilingual representations and propose a simple but effective approach to eliminate it. The pre-trained models are released at https://tfhub.dev/s?q= universal-sentence-encoder-cmlm. 2 Conditional Masked Language Modeling ing next sentence prediction with MLM training. By “conditional”, we mean the MLM task for one sentence depends on the encoded sentence level representation of the adjacent sentence. This builds on prior work on next sentence prediction that has been widely used for learning sentence level representations (Kiros et al., 2015; Logeswaran and Lee, 2018; Cer et al., 2018; Yang et al., 2019a), but has thus far produced poor quality sentence embeddings within BERT based models using MLM loss (Reimers and Gurevych, 2019). While existing MLMs like BERT include next sentence prediction tasks, they do so without any inductive bias to try to encode the meaning of a sentence within a single embedding vector. We introduce a strong inductive bias for learning sentence embeddings by structuring the task as follows. Given a pair of ordered sentences, the first sentence is fed to an encoder that produces a sentence level embedding. The embedding is then provided to an enc"
2021.emnlp-main.502,W19-4330,1,0.848165,"92.3 93.5 74.0 86.3 88.6 88.1 86.8 Table 6: Classification accuracy on the Amazon Reviews dataset. tion of [u, v, |u − v|, u ∗ v] (following works e.g. Reimers and Gurevych (2019)), is trained on the English training set and then evaluated on English, French, German and Japanese test sets (each has 6000 examples). The same multilingual encoder and classifier are used for all the evaluations. We also experiment with whether freezing the encoder weights or not during training. As presented in Table 6, CMLM alone has already outperformed baseline models, including Multi-task Dual-Encoder (MTDE, Chidambaram et al. (2019)), mBERT and XLM-R. Training with BR and cross-lingual NLI finetuning further boost the performance. 4.6 Tatoeba: Semantic Search is to find the nearest neighbor for the query sentence in the other language. The experiments is conducted on the 36 languages as in XTREME (Hu et al., 2020). The evaluation metric is retrieval accuracy. Results are presented in Table 5. Our model CMLM+BR outperforms all baseline models in 30 out of 36 languages and has the highest average performance. One interesting observation is that finetuning with NLI actually undermines the model performance on semantic searc"
2021.emnlp-main.502,L18-1269,0,0.0859207,"el agnostic sentences. The model therefore needs to learn efapproach to remove the language identifying fective sentence representations in order to perform information from the representation while still good MLM. Since CMLM is fully unsupervised, retaining sentence semantics. it can be easily extended to new languages. We explore CMLM for both English and multilingual 1 Introduction sentence embeddings for 100+ languages. Our EnSentence embeddings map sentences into a vector glish CMLM model achieves state-of-the-art perspace. The vectors capture rich semantic informa- formance on SentEval (Conneau and Kiela, 2018), tion that can be used to measure semantic textual even outperforming models learned using (semisimilarity (STS) between sentences or train classi- )supervised signals. Moreover, models training on fiers for a broad range of downstream tasks (Con- the English Amazon review data using our multineau et al., 2017; Subramanian et al., 2018; Lo- lingual vectors exhibit strong multilingual transfer geswaran and Lee, 2018; Cer et al., 2018; Reimers performance on translations of the Amazon review and Gurevych, 2019; Yang et al., 2019a,e). State- evaluation data to French, German and Japanese, of-the"
2021.emnlp-main.502,D17-1070,0,0.0258362,"coder sharing encoder weights for different The classifier for the downstream is logistic regresinputs can be also referred as “siamese encoder” 3 sion. For each task, the encoder and embeddings Representation concatenation has been used in previous work for enabling cross attention between global vectors and are fixed and only downstream neural structures local token embeddings to help the representations learning are trained. of long/structured inputs (Ainslie et al., 2020; Manzil Zaheer, 2020). The baseline sentence embedding models in6218 clude SkipThought (Kiros et al., 2015), InferSent (Conneau et al., 2017), USE (Cer et al., 2018), QuickThought (Logeswaran and Lee, 2018) and English BERT using standard pre-trained models from TensorFlow Hub website (Devlin et al., 2019), XLNet (Yang et al., 2019d), RoBERTa (Liu et al., 2019), SBert (Reimers and Gurevych, 2019). To evaluate the possible improvements coming from training data and processes, we train standard BERT models (English BERT base/large (CC)) on the same Common Crawl Corpora that CMLM is trained on. Similarly, we also train QuickThought, a competitive unsupervised sentence representations learning model, on the same Common Crawl Corpora (d"
2021.emnlp-main.502,N19-1423,0,0.179162,"{ziyi.yang,darve}@stanford.edu 2 Google Research {yinfeiy,cer,jaxlaw}@google.com Abstract 2019a). However, labeled and semi-structured data are difficult and expensive to obtain, making it This paper presents a novel training method, hard to cover many domains and languages. ConConditional Masked Language Modeling versely, recent efforts to improve language mod(CMLM), to effectively learn sentence repreels include the development of masked language sentations on large scale unlabeled corpora. model (MLM) pre-training from large scale unlaCMLM integrates sentence representation beled corpora (Devlin et al., 2019; Lan et al., 2020; learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Liu et al., 2019). While internal MLM model repOur English CMLM model achieves state-ofresentations are helpful when fine-tuning on downthe-art performance on SentEval (Conneau stream tasks, they do not directly produce good senand Kiela, 2018), even outperforming models tence representations, without further supervised learned using supervised signals. As a fully (Reimers and Gurevych, 2019) or semi-structured unsupervised learning method, CMLM can (Feng et al., 2020) fine-tuning. be c"
2021.emnlp-main.502,2021.ccl-1.108,0,0.071914,"Missing"
2021.emnlp-main.502,marelli-etal-2014-sick,0,0.0128466,"steps and linear decay afterwards. We explore two transformer configurations same as in the original BERT paper, i.e., base and large. The number of projections N is 15 by experimenting with multiple choices. 3.1 Evaluation We evaluate the sentence representations on the following tasks: (1) classification: MR (movie reviews Pang and Lee (2005)), binary SST (sentiment analysis, Socher et al. (2013)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)). (2) Entailment: SICK dataset for entailment (SICK-E, Marelli et al. (2014)). The evaluation is done using 1 One can equivalently choose other pooling methods, such SentEval (Conneau and Kiela, 2018) which is a preas max pooling or use the vector output corresponding to a special token position such as the [CLS] token. vailing evaluation toolkit for sentence embeddings. 2 The dual-encoder sharing encoder weights for different The classifier for the downstream is logistic regresinputs can be also referred as “siamese encoder” 3 sion. For each task, the encoder and embeddings Representation concatenation has been used in previous work for enabling cross attention betwe"
2021.emnlp-main.502,P04-1035,0,0.0151912,"h learning rate of 10−3 , β1 = 0.9, β2 = 0.999, warm-up in the first 10,000 steps and linear decay afterwards. We explore two transformer configurations same as in the original BERT paper, i.e., base and large. The number of projections N is 15 by experimenting with multiple choices. 3.1 Evaluation We evaluate the sentence representations on the following tasks: (1) classification: MR (movie reviews Pang and Lee (2005)), binary SST (sentiment analysis, Socher et al. (2013)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)). (2) Entailment: SICK dataset for entailment (SICK-E, Marelli et al. (2014)). The evaluation is done using 1 One can equivalently choose other pooling methods, such SentEval (Conneau and Kiela, 2018) which is a preas max pooling or use the vector output corresponding to a special token position such as the [CLS] token. vailing evaluation toolkit for sentence embeddings. 2 The dual-encoder sharing encoder weights for different The classifier for the downstream is logistic regresinputs can be also referred as “siamese encoder” 3 sion. For each task, the encoder and embeddings Representation co"
2021.emnlp-main.502,P05-1015,0,0.0141269,"the fact that in CMLM, language modeling has access to extra information from adjacent sentences. We train with batch size of 2048 for 1 million steps. The optimizer is LAMB (You et al., 2020) with learning rate of 10−3 , β1 = 0.9, β2 = 0.999, warm-up in the first 10,000 steps and linear decay afterwards. We explore two transformer configurations same as in the original BERT paper, i.e., base and large. The number of projections N is 15 by experimenting with multiple choices. 3.1 Evaluation We evaluate the sentence representations on the following tasks: (1) classification: MR (movie reviews Pang and Lee (2005)), binary SST (sentiment analysis, Socher et al. (2013)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)). (2) Entailment: SICK dataset for entailment (SICK-E, Marelli et al. (2014)). The evaluation is done using 1 One can equivalently choose other pooling methods, such SentEval (Conneau and Kiela, 2018) which is a preas max pooling or use the vector output corresponding to a special token position such as the [CLS] token. vailing evaluation toolkit for sentence embeddings. 2 The dual-encoder sharing e"
2021.emnlp-main.502,P10-1114,0,0.108205,"Missing"
2021.emnlp-main.502,D19-1410,0,0.229586,"a. model (MLM) pre-training from large scale unlaCMLM integrates sentence representation beled corpora (Devlin et al., 2019; Lan et al., 2020; learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Liu et al., 2019). While internal MLM model repOur English CMLM model achieves state-ofresentations are helpful when fine-tuning on downthe-art performance on SentEval (Conneau stream tasks, they do not directly produce good senand Kiela, 2018), even outperforming models tence representations, without further supervised learned using supervised signals. As a fully (Reimers and Gurevych, 2019) or semi-structured unsupervised learning method, CMLM can (Feng et al., 2020) fine-tuning. be conveniently extended to a broad range of languages and domains. We find that a In this paper, we explore an unsupervised apmultilingual CMLM model co-trained with proach, called Conditional Masked Language Modbitext retrieval (BR) and natural language eling (CMLM), to effectively learn sentence repinference (NLI) tasks outperforms the previresentations from large scale unlabeled corpora. ous state-of-the-art multilingual models by a The CMLM model architecture is illustrated in large margin, e.g. 10"
2021.emnlp-main.502,2020.emnlp-main.365,0,0.0330003,"Missing"
2021.emnlp-main.502,D19-1382,1,0.63591,"in multilingual representations and propose a simple but effective approach to eliminate it. The pre-trained models are released at https://tfhub.dev/s?q= universal-sentence-encoder-cmlm. 2 Conditional Masked Language Modeling ing next sentence prediction with MLM training. By “conditional”, we mean the MLM task for one sentence depends on the encoded sentence level representation of the adjacent sentence. This builds on prior work on next sentence prediction that has been widely used for learning sentence level representations (Kiros et al., 2015; Logeswaran and Lee, 2018; Cer et al., 2018; Yang et al., 2019a), but has thus far produced poor quality sentence embeddings within BERT based models using MLM loss (Reimers and Gurevych, 2019). While existing MLMs like BERT include next sentence prediction tasks, they do so without any inductive bias to try to encode the meaning of a sentence within a single embedding vector. We introduce a strong inductive bias for learning sentence embeddings by structuring the task as follows. Given a pair of ordered sentences, the first sentence is fed to an encoder that produces a sentence level embedding. The embedding is then provided to an encoder that condition"
2021.emnlp-main.502,2020.emnlp-main.477,1,0.889396,"Missing"
2021.emnlp-main.502,P19-4007,0,0.0578645,"Missing"
2021.emnlp-main.502,D13-1170,0,0.00421279,"o extra information from adjacent sentences. We train with batch size of 2048 for 1 million steps. The optimizer is LAMB (You et al., 2020) with learning rate of 10−3 , β1 = 0.9, β2 = 0.999, warm-up in the first 10,000 steps and linear decay afterwards. We explore two transformer configurations same as in the original BERT paper, i.e., base and large. The number of projections N is 15 by experimenting with multiple choices. 3.1 Evaluation We evaluate the sentence representations on the following tasks: (1) classification: MR (movie reviews Pang and Lee (2005)), binary SST (sentiment analysis, Socher et al. (2013)), TREC (questiontype, Voorhees and Tice (2000)), CR (product reviews, Hu and Liu (2004)), SUBJ (subjectivity/objectivity, Pang and Lee (2004)). (2) Entailment: SICK dataset for entailment (SICK-E, Marelli et al. (2014)). The evaluation is done using 1 One can equivalently choose other pooling methods, such SentEval (Conneau and Kiela, 2018) which is a preas max pooling or use the vector output corresponding to a special token position such as the [CLS] token. vailing evaluation toolkit for sentence embeddings. 2 The dual-encoder sharing encoder weights for different The classifier for the dow"
2021.emnlp-main.502,D19-1059,1,0.894795,"Missing"
2021.emnlp-main.502,N18-1101,0,0.0217,"sentence representations by finetuning the transformer encoder with CMLM and BR. As shown in Table 4, the improvements of f-mBERT (finetuned mBERT) upon mBERT are significant. l1 and a hypothesis sentence v in language l2 , we train a 3-way classifier on the concatenation of [u, v, |u − v|, u ∗ v]. Weights of transformer encoders are also updated in the finetuning process. Different from previous work also using multilingual NLI data (Yang et al., 2019a), the premise u and hypothesis v are in different languages. The cross-lingual NLI data are generated by translating Multi-Genre NLI Corpus (Williams et al., 2018) into 14 languages using Google Translate API. 4.4 Configurations Monolingual training data for CMLM are generated from 3 versions of Common Crawl data in 113 languages. The data cleaning and filtering is the same as the English-only ones. A new cased vocabulary is built from the all data sources using 4.3 Finetuning with Cross-lingual Natural the WordPiece vocabulary generation library from Language Inference Tensorflow Text. The language smoothing expoFinetuning with NLI data has proved to be an ef- nent from the vocab generation tool is set to 0.3, fective method to improve the quality of e"
2021.emnlp-main.502,2020.acl-demos.12,1,0.865743,"Missing"
cer-etal-2010-parsing,de-marneffe-etal-2006-generating,1,\N,Missing
cer-etal-2010-parsing,A00-2018,0,\N,Missing
cer-etal-2010-parsing,nivre-etal-2006-maltparser,0,\N,Missing
cer-etal-2010-parsing,W07-1420,0,\N,Missing
cer-etal-2010-parsing,N03-1033,0,\N,Missing
cer-etal-2010-parsing,W09-1419,0,\N,Missing
cer-etal-2010-parsing,W07-1423,0,\N,Missing
cer-etal-2010-parsing,C96-1058,0,\N,Missing
cer-etal-2010-parsing,W09-1402,0,\N,Missing
cer-etal-2010-parsing,W08-1301,1,\N,Missing
cer-etal-2010-parsing,W07-1004,0,\N,Missing
cer-etal-2010-parsing,P03-1054,0,\N,Missing
cer-etal-2010-parsing,H05-1066,0,\N,Missing
cer-etal-2010-parsing,P05-1022,0,\N,Missing
cer-etal-2010-parsing,W07-1417,0,\N,Missing
cer-etal-2010-parsing,P06-1055,0,\N,Missing
cer-etal-2010-parsing,P08-1006,0,\N,Missing
cer-etal-2010-parsing,W03-3017,0,\N,Missing
cer-etal-2010-parsing,W04-3224,0,\N,Missing
cer-etal-2010-parsing,P08-1000,0,\N,Missing
D13-1141,D10-1005,0,0.03299,"nd Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collober"
D13-1141,N10-1080,1,0.345132,"onolingual, but significantly better than Align-Init (as in Section3.2.1) on the NER task. 4.3 Vector matching alignment Translation equivalence of the bilingual embeddings is evaluated by naive word alignment to match word embeddings by cosine distance.5 The Alignment Error Rates (AER) reported in Table 3 suggest that bilingual training using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; i"
D13-1141,W09-0439,0,0.00837207,"Missing"
D13-1141,P13-1031,1,0.225345,"Missing"
D13-1141,N06-2015,0,0.261157,"dings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Review of prior work Distributed word representations are u"
D13-1141,P12-1092,1,0.809739,"s across languages. The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity"
D13-1141,S12-1049,0,0.294238,"nsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference o"
D13-1141,N03-1017,0,0.0720823,"Missing"
D13-1141,N12-1005,0,0.00587547,"ements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). Given a context window c in a document d, the optimization minimizes the following Context Objective for a word w in the"
D13-1141,N06-1014,0,0.0184131,"tions, e.g. ‘lake’ and the Chinese word ‘潭’ (deep pond), their semantic proximity could be correctly quantified. We describe in the next sub-sections the methods to intialize and train bilingual embeddings. These methods ensure that bilingual embeddings retain their translational equivalence while their distributional semantics are improved during online training with a monolingual corpus. 3.2.1 Initialization by MT alignments First, we use MT Alignment counts as weighting to initialize Chinese word embeddings. In our experiments, we use MT word alignments extracted with the Berkeley Aligner (Liang et al., 2006) 1 . Specifically, we use the following equation to compute starting word embeddings: Wt-init = S X Cts + 1 s=1 Algorithm and methods JCO = Here f is a function defined by a neural network. wr is a word chosen in a random subset VR of the r vocabulary, and cw is the context window containing word wr . This unsupervised objective function contrasts the score between when the correct word is placed in context with when a random word is placed in the same context. We incorporate the global context information as in Huang et al. (2012), shown to improve performance of word embeddings. r max(0, 1 −"
D13-1141,W13-3512,1,0.24197,"and apply word embeddings using continuous models for language. Collobert et al. (2008) learn embeddings in an unsupervised manner through a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve compe"
D13-1141,P11-1015,0,0.289194,"Missing"
D13-1141,P03-1021,0,0.0270902,"g using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; if no word is found in a phrase, a zero vector is assigned 4 Due to variations caused by online minibatch L-BFGS, we take embeddings from five random points out of last 105 minibatch iterations, and average their semantic similarity results. 1396 5 This is evaluated on 10,000 randomly selected sentence pairs from the MT training s"
D13-1141,P06-1102,0,0.021416,"Missing"
D13-1141,N10-1135,0,0.126436,"Missing"
D13-1141,N10-1013,0,0.320906,"Missing"
D13-1141,D11-1014,1,0.274729,"Missing"
D13-1141,P00-1054,0,0.281177,"a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training wi"
D13-1141,P07-1066,0,0.0208964,"icient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). G"
D13-1141,P10-1040,0,0.914221,"semantic similarities across languages. The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improve"
D13-1141,P13-1106,1,0.321038,"trained embeddings4 out-perform pruned tf-idf by 14.1 and 12.6 Spearman Correlation (×100), respectively. Further, they out-perform embeddings initialized from alignment by 7.9 and 6.4. Both our tf-idf implementation and the word embeddings have significantly higher Kendall’s Tau value compared to Prior work (Jin and Wu, 2012). We verified Tau calculations with original submissions provided by the authors. 4.2 Named Entity Recognition We perform NER experiments on OntoNotes (v4.0) (Hovy et al., 2006) to validate the quality of the Chinese word embeddings. Our experimental setup is the same as Wang et al. (2013). With embeddings, we build a naive feed-forward neural network (Collobert et al., 2008) with 2000 hidden neurons and a sliding window of five words. This naive setting, without sequence modeling or sophisticated Embeddings Align-Init Mono-trained Biling-trained Prec. 0.34 0.54 0.48 Rec. 0.52 0.62 0.55 F1 0.41 0.58 0.52 Improve 0.17 0.11 Table 3: Vector Matching Alignment AER (lower is better) Embeddings Mono-trained Biling-trained Prec. 0.27 0.37 Rec. 0.32 0.45 AER 0.71 0.59 join optimization, is not competitive with state-ofthe-art (Wang et al., 2013). Table 2 shows that the bilingual embedd"
D13-1141,P01-1067,0,0.0161864,"Missing"
D13-1141,P06-2124,0,0.00531532,"l continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). Given a context window"
D13-1141,C12-1089,0,\N,Missing
D18-2029,C02-1150,0,0.111066,"ains a single DAN encoder to support multiple downstream tasks. An advantage of the DAN encoder is that compute time is linear in the length of the input sequence. Similar to Iyyer et al. (2015), our results demonstrate that DANs achieve strong baseline performance on text classification tasks. 3.3 T RAIN D EV T EST SST STS Bench TREC MR CR SUBJ MPQA 67,349 5,749 5,452 - 872 1,500 - 1,821 1,379 500 10,662 3,775 10,000 10,606 Table 1: Transfer task evaluation sets. (MPQA) Phrase opinion polarity from news data (Wiebe et al., 2005); (TREC) Fine grained question classification sourced from TREC (Li and Roth, 2002); (SST) Binary phrase sentiment classification (Socher et al., 2013); (STS Benchmark) Semantic textual similarity (STS) between sentence pairs scored by Pearson r with human judgments (Cer et al., 2017); (WEAT) Word pairs from the psychology literature on implicit association tests (IAT) that are used to characterize model bias (Caliskan et al., 2017).8 Table 1 gives the number of samples for each transfer task. Encoder Training Data 5 Unsupervised training data are drawn from a variety of web sources. The sources are Wikipedia, web news, web question-answer pages and discussion forums. We aug"
D18-2029,D15-1075,0,0.523649,"between sentence pairs scored by Pearson r with human judgments (Cer et al., 2017); (WEAT) Word pairs from the psychology literature on implicit association tests (IAT) that are used to characterize model bias (Caliskan et al., 2017).8 Table 1 gives the number of samples for each transfer task. Encoder Training Data 5 Unsupervised training data are drawn from a variety of web sources. The sources are Wikipedia, web news, web question-answer pages and discussion forums. We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) in order to further improve our representations (Conneau et al., 2017). Since the only supervised training data is SNLI, the models can be used for a wide range of downstream supervised tasks that do not overlap with this dataset.7 4 DATASET Transfer Learning Models For sentence classification transfer tasks, the output of the sentence encoders are provided to a task specific DNN. For the pairwise semantic similarity task, the similarity of sentence u  embeddings  and v is assessed using − arccos ||u||uv||v ||.9 5.1 Baselines For each transfer task, we include baselines that only make use o"
D18-2029,S17-2001,1,0.832941,"ults demonstrate that DANs achieve strong baseline performance on text classification tasks. 3.3 T RAIN D EV T EST SST STS Bench TREC MR CR SUBJ MPQA 67,349 5,749 5,452 - 872 1,500 - 1,821 1,379 500 10,662 3,775 10,000 10,606 Table 1: Transfer task evaluation sets. (MPQA) Phrase opinion polarity from news data (Wiebe et al., 2005); (TREC) Fine grained question classification sourced from TREC (Li and Roth, 2002); (SST) Binary phrase sentiment classification (Socher et al., 2013); (STS Benchmark) Semantic textual similarity (STS) between sentence pairs scored by Pearson r with human judgments (Cer et al., 2017); (WEAT) Word pairs from the psychology literature on implicit association tests (IAT) that are used to characterize model bias (Caliskan et al., 2017).8 Table 1 gives the number of samples for each transfer task. Encoder Training Data 5 Unsupervised training data are drawn from a variety of web sources. The sources are Wikipedia, web news, web question-answer pages and discussion forums. We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) in order to further improve our representations (Conneau et a"
D18-2029,D17-1070,0,0.484316,"t al., 2017); (WEAT) Word pairs from the psychology literature on implicit association tests (IAT) that are used to characterize model bias (Caliskan et al., 2017).8 Table 1 gives the number of samples for each transfer task. Encoder Training Data 5 Unsupervised training data are drawn from a variety of web sources. The sources are Wikipedia, web news, web question-answer pages and discussion forums. We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) in order to further improve our representations (Conneau et al., 2017). Since the only supervised training data is SNLI, the models can be used for a wide range of downstream supervised tasks that do not overlap with this dataset.7 4 DATASET Transfer Learning Models For sentence classification transfer tasks, the output of the sentence encoders are provided to a task specific DNN. For the pairwise semantic similarity task, the similarity of sentence u  embeddings  and v is assessed using − arccos ||u||uv||v ||.9 5.1 Baselines For each transfer task, we include baselines that only make use of word-level transfer and baselines that make use of no transfer learni"
D18-2029,P04-1035,0,0.0241679,"The baselines that use pretrained word embeddings allow us to contrast word- vs. sentence-level transfer. Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings. For reference, we compare with InferSent (Conneau et al., 2017) and Transfer Tasks This section presents the data used for the transfer learning experiments and word embedding association tests (WEAT): (MR) Movie review sentiment on a five star scale (Pang and Lee, 2005); (CR) Sentiment of customer reviews (Hu and Liu, 2004); (SUBJ) Subjectivity of movie reviews and plot summaries (Pang and Lee, 2004); 5 The Skip-Thought like task replaces the LSTM (Hochreiter and Schmidhuber, 1997) in the original formulation with a transformer model. 6 SNLI (Bowman et al., 2015; Conneau et al., 2017) 7 For questions on downstream evaluations possibly overlapping with the encoder training data, visit the TF Hub discussion board, https://groups.google.com/a/ tensorflow.org/d/forum/hub, or e-mail the corresponding authors. 8 For MR, CR, SUBJ, SST, and TREC we use the preparation of the data provided by Conneau et al. (2017). 9 arccos converts cosine similarity into an angular distance that obeys the triangl"
D18-2029,P05-1015,0,0.354446,"pretrained word embeddings are included as input to two model types: a convolutional neural network model (CNN) (Kim, 2014); a DAN. The baselines that use pretrained word embeddings allow us to contrast word- vs. sentence-level transfer. Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings. For reference, we compare with InferSent (Conneau et al., 2017) and Transfer Tasks This section presents the data used for the transfer learning experiments and word embedding association tests (WEAT): (MR) Movie review sentiment on a five star scale (Pang and Lee, 2005); (CR) Sentiment of customer reviews (Hu and Liu, 2004); (SUBJ) Subjectivity of movie reviews and plot summaries (Pang and Lee, 2004); 5 The Skip-Thought like task replaces the LSTM (Hochreiter and Schmidhuber, 1997) in the original formulation with a transformer model. 6 SNLI (Bowman et al., 2015; Conneau et al., 2017) 7 For questions on downstream evaluations possibly overlapping with the encoder training data, visit the TF Hub discussion board, https://groups.google.com/a/ tensorflow.org/d/forum/hub, or e-mail the corresponding authors. 8 For MR, CR, SUBJ, SST, and TREC we use the preparati"
D18-2029,S17-2016,0,0.0925536,"Hub discussion board, https://groups.google.com/a/ tensorflow.org/d/forum/hub, or e-mail the corresponding authors. 8 For MR, CR, SUBJ, SST, and TREC we use the preparation of the data provided by Conneau et al. (2017). 9 arccos converts cosine similarity into an angular distance that obeys the triangle inequality. We find that angular distance performs better on STS than cosine similarity. 170 M ODEL Skip-Thought with layer normalization (Ba et al., 2016) on sentence-classification tasks. On the STS Benchmark, we compare with InferSent and the state-of-the-art neural STS systems CNN (HCTI) (Shao, 2017) and gConv (Yang et al., 2018). 5.2 Combined Transfer Models We explore combining the sentence and wordlevel transfer models by concatenating their representations prior to the classification layers. For completeness, we report results providing the classification layers with the concatenating of the sentence-level embeddings and the representations produced by baseline models that do not make use of word-level transfer learning. 6 Experiments Experiments use our most recent transformer and DAN encoding models.10 Transfer task model hyperparamaters are tuned using a combination of Vizier (Golo"
D18-2029,D13-1170,0,0.0300453,"advantage of the DAN encoder is that compute time is linear in the length of the input sequence. Similar to Iyyer et al. (2015), our results demonstrate that DANs achieve strong baseline performance on text classification tasks. 3.3 T RAIN D EV T EST SST STS Bench TREC MR CR SUBJ MPQA 67,349 5,749 5,452 - 872 1,500 - 1,821 1,379 500 10,662 3,775 10,000 10,606 Table 1: Transfer task evaluation sets. (MPQA) Phrase opinion polarity from news data (Wiebe et al., 2005); (TREC) Fine grained question classification sourced from TREC (Li and Roth, 2002); (SST) Binary phrase sentiment classification (Socher et al., 2013); (STS Benchmark) Semantic textual similarity (STS) between sentence pairs scored by Pearson r with human judgments (Cer et al., 2017); (WEAT) Word pairs from the psychology literature on implicit association tests (IAT) that are used to characterize model bias (Caliskan et al., 2017).8 Table 1 gives the number of samples for each transfer task. Encoder Training Data 5 Unsupervised training data are drawn from a variety of web sources. The sources are Wikipedia, web news, web question-answer pages and discussion forums. We augment unsupervised learning with training on supervised data from the"
D18-2029,P15-1162,0,0.0756035,"Missing"
D18-2029,W18-3022,1,0.817199,"rporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias. 1 c Model Toolkit Models are implemented in TensorFlow (Abadi et al., 2016) and are made publicly available on TensorFlow Hub.2 Listing 1 provides an example 3 † Corresponding authors: {cer, yinfeiy}@google.com 1 We describe our publicly released models. See Yang et al. (2018) and Henderson et al. (2017) for additional architectural details of models similar to those presented here. 2 https://www.tensorflow.org/hub/, Apache 2.0 license, with models available as saved TF graphs. Basic text preprocessing and white-space tokenization is performed internally. Preprocessing lowercases the text and removes punctuation. OOV items are handled using string hashing to index into 400,000 OOV embeddings. 4 Visit https://colab.research.google.com/ to try the code snippet in Listing 1. Example code and documentation is available on the TF Hub website. 169 Proceedings of the 2018"
D18-2029,D14-1181,0,0.0460215,"ers are provided to a task specific DNN. For the pairwise semantic similarity task, the similarity of sentence u  embeddings  and v is assessed using − arccos ||u||uv||v ||.9 5.1 Baselines For each transfer task, we include baselines that only make use of word-level transfer and baselines that make use of no transfer learning at all. For word-level transfer, we incorporate word embeddings from a word2vec skip-gram model trained on a corpus of news data (Mikolov et al., 2013). The pretrained word embeddings are included as input to two model types: a convolutional neural network model (CNN) (Kim, 2014); a DAN. The baselines that use pretrained word embeddings allow us to contrast word- vs. sentence-level transfer. Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings. For reference, we compare with InferSent (Conneau et al., 2017) and Transfer Tasks This section presents the data used for the transfer learning experiments and word embedding association tests (WEAT): (MR) Movie review sentiment on a five star scale (Pang and Lee, 2005); (CR) Sentiment of customer reviews (Hu and Liu, 2004); (SUBJ) Subjectivity of movie reviews and plot su"
D19-5819,P17-1171,0,0.20439,"ts (Voorhees and Tice, 2000). Successful systems usually follow a two-step approach to answer a given question: first retrieve relevant articles or blocks, and then scan the returned text to identify the answer using a reading comprehension model (Jurafsky and Martin, 2018; Kratzwald and Feuerriegel, 2018; Yang et al., 2019a; Lee et al., 2019). While the reading comprehension step has been widely studied with many existing datasets (Rajpurkar et al., 2016; Nguyen et al., 2016; Dunn et al., 2017; Kwiatkowski et al., 2019), machine reading at scale is still a challenging task for the community. Chen et al. (2017) recently proposed DrQA, treating Wikipedia as a knowledge base over which to answer factoid questions from SQuAD (Rajpurkar et al., 2016), Curatˇ edTREC (Baudiˇs and Sediv´ y, 2015) and other sources. The task measures how well a system can successfully extract the answer span given a question, but it still relies on a document retrieval step. The ReQA eval differs from DrQA task by skipping the intermediate step and retrieving the answer sentence directly. There is also a growing interest in answer selection at scale. Surdeanu et al. (2008) constructs a dataset with 142,627 question-answer p"
D19-5819,Q19-1026,0,0.197279,"question makes sense in the original context of the Wikipedia article on Fr´ed´eric Chopin, but is underspecified when asked in isolation, and could reasonably have other answers. One possible resolution would be to include the context title as part of the question context. However this is unrealistic from the point of view of end systems where the user doesn’t have a specific document in mind. This concern can be avoided by switching from “back-written” datasets to “web-search based” datasets. These include MS MARCO (Nguyen et al., 2016), TriviaQA (Joshi et al., 2017) and Natural Questions (Kwiatkowski et al., 2019). For these sets, questions are taken from natural sources, and a search engine is used in the process of constructing QA pairs. However, there is an important caveat to mention when using web-search data to build ReQA tasks. In these datasets, the answers are derived from web documents retrieved by a search engine, where the question is used as the search query. This introduces a bias toward answers that are already retrievable through traditional search methods. By comparison, answers in SQuAD 1.1 may be found in “off-topic” documents, and it is valuable for an evaluation to measure the abil"
D19-5819,W18-6317,1,0.890977,"Missing"
D19-5819,P19-1612,0,0.126196,"Missing"
D19-5819,N18-1202,0,0.106872,"Missing"
D19-5819,D16-1264,0,0.412984,"d hope to promote further research by offering the Retrieval Question-Answering (ReQA) benchmark, which tests a model’s ability to retrieve relevant answers efficiently from a large set of documents. Our code is available at https://github.com/ google/retrieval-qa-eval. The remainder of the paper is organized as follows. In Section 2, we define our goals in developing large-scale answer retrieval models. Section 3 describes our method for transforming withindocument reading comprehension tasks into Retrieval Question-Answering (ReQA) tasks, and deIntroduction Popular QA benchmarks like SQuAD (Rajpurkar et al., 2016) have driven impressive progress on the task of identifying spans of text within a specific passage that answer a posed question. Recent models using BERT pretraining (Devlin et al., 2019) have already surpassed human performance on SQuAD 1.1 and 2.0. While impressive, these systems are not yet sufficient for the end task of answering user questions at scale, since in general, we don’t know which documents are likely to contain an answer. On the one hand, typical document retrieval solutions fall short here, since they aren’t trained to directly model the connection between questions and answe"
D19-5819,P17-1147,0,0.0573682,"Missing"
D19-5819,D18-1052,0,0.074621,"entence level retrieval and by providing strong sentence-level and paragraph-level baselines over a replicable construction of a retrieval evaluation set from the SQuAD data. Further, while Cakaloglu et al. (2018) trained their model on data drawn from SQuAD, we would like to highlight that our own strong baselines do not make use of any training data from SQuAD. We advocate for future work to attempt a similar approach of using sources of model training and evaluation data that are distinct as possible in order to provide a better picture of how well models generally perform a task. Finally, Seo et al. (2018) construct a phraseindexed question answering challenge that is similar to ReQA in requiring the question and the answer be encoded separately of one another. However, while ReQA focuses on sentence-based retrieval, their benchmark retrieves phrases, allowing for a direct F1 and exact-match evaluation on SQuAD. Seo et al. (2019) demonstrate an implementation of a phrase-indexed question answering system using a combination of dense (neural) and sparse (term-frequency based) indices. We believe that ReQA can help guide development of such systems by providing a point of evaluation between SQuAD"
D19-5819,D18-1055,0,0.0824985,"Missing"
D19-5819,P19-1436,0,0.700082,"g the answer is not “on topic” for the question. QA models with strong performance on reading comprehension can’t be used directly for largescale retrieval. This is because competitive QA models use interactions between the question and candidate answer in the early stage of modeling (e.g. through cross-attention) making it infeasible to score a large set of candidates at inference time. There is growing interest in training end-to-end retrieval systems that can efficiently surface relevant results without an intermediate document retrieval phase (Gillick et al., 2018; Cakaloglu et al., 2018; Seo et al., 2019; Henderson et al., 2019). We are excited by this direction, and hope to promote further research by offering the Retrieval Question-Answering (ReQA) benchmark, which tests a model’s ability to retrieve relevant answers efficiently from a large set of documents. Our code is available at https://github.com/ google/retrieval-qa-eval. The remainder of the paper is organized as follows. In Section 2, we define our goals in developing large-scale answer retrieval models. Section 3 describes our method for transforming withindocument reading comprehension tasks into Retrieval Question-Answering (ReQ"
D19-5819,D18-2012,0,0.0334514,"well as the lower degree of lexical overlap between questions and answers. Table 5 illustrates the tradeoff between model accuracy and resource usage. Figure 3: A schematic dual encoder for questionanswer retrieval. As our primary neural baseline, we take the recently released universal sentence encoder QA (USE-QA) model from Yang et al. (2019c)10 . This is a multilingual QA retrieval model that co-trains a question-answer dual encoder along with secondary tasks of translation ranking and natural language inference. The model uses sub-word tokenization, with a 128k “sentencepiece” vocabulary (Kudo and Richardson, 2018). Question and answer text are encoded independently using a 6layer transformer encoder (Vaswani et al., 2017), and then reduced to a fixed-length vector through average pooling. The final encoding dimensionality is 512. The training corpus contains over a billion question-answer pairs from popular online forums and QA websites like Reddit and StackOverflow. As a second neural baseline, we include an internal QA model (QALite ) designed for use on mobile devices. Like USE-QA, this model is trained over online forum data, and uses a transformerbased text encoder. The core differences are reduct"
D19-5819,P08-1082,0,0.124903,"at scale is still a challenging task for the community. Chen et al. (2017) recently proposed DrQA, treating Wikipedia as a knowledge base over which to answer factoid questions from SQuAD (Rajpurkar et al., 2016), Curatˇ edTREC (Baudiˇs and Sediv´ y, 2015) and other sources. The task measures how well a system can successfully extract the answer span given a question, but it still relies on a document retrieval step. The ReQA eval differs from DrQA task by skipping the intermediate step and retrieving the answer sentence directly. There is also a growing interest in answer selection at scale. Surdeanu et al. (2008) constructs a dataset with 142,627 question-answer pairs from Yahoo! Answers, with the goal of retrieving the right answer from all answers given a question. However, the dataset is limited to “how to” questions, which simplifies the problem by restricting it to a specific domain. Additionally the underlying data is not as broadly accessible as SQuAD and other more recent QA datasets, due to more restrictive terms of use. WikiQA (Yang et al., 2015) is another task involving large-scale sentence-level answer selection. The candidate sentences are, however, limited to a small set of documents re"
D19-5819,N19-4013,0,0.116731,"Missing"
D19-5819,D15-1237,0,0.120229,"Missing"
D19-5819,W18-3022,1,0.887509,"Missing"
D19-5819,P18-2124,0,\N,Missing
D19-5819,N19-1423,0,\N,Missing
N06-1006,H05-1079,0,0.0893309,"the core of weighted abduction theorem proving consider matching an individual node of the hypothesis (e.g. rose(e1)) with something from the text (e.g. fell(e1)), just as in the graph-matching approach. The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al. (2005). Then the theorem prover may generate intermediate forms in the proof, but, nevertheless, individual terms are resolved locally without reference to global context. Finally, a few efforts (Akhmatova, 2005; Fowler et al., 2005; Bos and Markert, 2005) have tried to translate sentences into formulas of first-order logic, in order to test logical entailment with a theorem prover. While in principle this approach does not suffer from the limitations we describe below, in practice it has not borne much fruit. Because few problem sentences can be accurately translated to logical form, and because logical entailment is a strict standard, recall tends to be poor. The simple graph matching formulation of the problem belies three important issues. First, the above systems assume a form of upward monotonicity: if a good match is found with a part of"
N06-1006,de-marneffe-etal-2006-generating,1,0.369258,"Missing"
N06-1006,H05-1049,1,0.379173,"e. The simplest approach is to base the entailment prediction on the degree of semantic overlap between the text and hypothesis using models based on bags of words, bags of n-grams, TF-IDF scores, or something similar (Jijkoun and de Rijke, 2005). Such models have serious limitations: semantic overlap is typically a symmetric relation, whereas entailment is clearly not, and, because overlap models do not account for syntactic or semantic structure, they are easily fooled by examples like ID 2081. A more structured approach is to formulate the entailment prediction as a graph matching problem (Haghighi et al., 2005; de Salvo Braz et al., 2005). In this formulation, sentences are represented as normalized syntactic dependency graphs (like the one shown in figure 1) and entailment is approximated with an alignment between the graph representing the hypothesis and a portion of the corresponding graph(s) representing the text. Each possible alignment of the graphs has an associated score, and the score of the best alignment is used as an approximation to the strength of the entailment: a betteraligned hypothesis is assumed to be more likely to be entailed. To enable incremental search, alignment scores are"
N06-1006,P88-1012,0,0.0610457,"variety of approximate search techniques. Haghighi et al. (2005) 42 divide the search into two steps: in the first step they consider node scores only, which relaxes the problem to a weighted bipartite graph matching that can be solved in polynomial time, and in the second step they add the edges scores and hillclimb the alignment via an approximate local search. A third approach, exemplified by Moldovan et al. (2003) and Raina et al. (2005), is to translate dependency parses into neo-Davidsonian-style quasilogical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al., 1988). Unless supplemented with a knowledge base, this approach is actually isomorphic to the graph matching approach. For example, the graph in figure 1 might generate the quasi-LF rose(e1), nsubj(e1, x1), sales(x1), nn(x1, x2), Mitsubishi(x2), dobj(e1, x3), percent(x3), num(x3, x4), 46(x4). There is a term corresponding to each node and arc, and the resolution steps at the core of weighted abduction theorem proving consider matching an individual node of the hypothesis (e.g. rose(e1)) with something from the text (e.g. fell(e1)), just as in the graph-matching approach. The two models become disti"
N06-1006,P03-1054,1,0.0200203,"ble about their semantic content. We use typed dependency graphs, which contain a node for each word and labeled edges representing the grammatical relations between words. Figure 1 gives the typed dependency graph for ID 971. This representation contains much of the information about words and relations between them, and is relatively easy to compute from a syntactic parse. However many semantic phenomena are not represented properly; particularly egregious is the inability to represent quantification and modality. We parse input sentences to phrase structure trees using the Stanford parser (Klein and Manning, 2003), a statistical syntactic parser trained on the Penn TreeBank. To ensure correct parsing, we preprocess the sentences to collapse named entities into new dedicated tokens. Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modi"
N06-1006,levy-andrew-2006-tregex,0,0.0121484,"Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modified version of the Collins head rules that favor semantic heads (such as lexical verbs rather than auxiliaries), and dependents of heads are typed using tregex patterns (Levy and Andrew, 2006), an extension of the tgrep pattern language. The nodes in the final graph are then annotated with their associated word, part-of-speech (given by the parser), lemma (given by a finite-state transducer described by Minnen et al. (2001)) and named-entity tag. 3.2 Alignment graphs representing the hypothesis and the text. An alignment consists of a mapping from each node (word) in the hypothesis graph to a single node in the text graph, or to null.3 Figure 1 gives the alignment for ID 971. The space of alignments is large: there are O((m + 1)n ) possible alignments for a hypothesis graph with n"
N06-1006,W05-1201,0,0.108372,"of the text, assuming that we allow an alignment with “loose” arc correspondence.2 Under this candidate alignment, the lexical alignments are perfect, and the only imperfect alignment is the subject arc of were is mismatched in the two. A robust inference guesser will still likely conclude that there is entailment. We propose that all three problems can be resolved in a two-stage architecture, where the alignment phase is followed by a separate phase of entailment determination. Although developed independently, the same division between alignment and classification has also been proposed by Marsi and Krahmer (2005), whose textual system is developed and evaluated on parallel translations into Dutch. Their classification phase features an output space of five semantic relations, and performs well at distinguishing entailing sentence pairs. Finding aligned content can be done by any search procedure. Compared to previous work, we emphasize structural alignment, and seek to ignore issues like polarity and quantity, which can be left to a subsequent entailment decision. For example, the scoring function is designed to encourage antonym matches, and ignore the negation of verb predicates. The ideas clearly g"
N06-1006,W03-0430,0,0.00852042,"ut words and relations between them, and is relatively easy to compute from a syntactic parse. However many semantic phenomena are not represented properly; particularly egregious is the inability to represent quantification and modality. We parse input sentences to phrase structure trees using the Stanford parser (Klein and Manning, 2003), a statistical syntactic parser trained on the Penn TreeBank. To ensure correct parsing, we preprocess the sentences to collapse named entities into new dedicated tokens. Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modified version of the Collins head rules that favor semantic heads (such as lexical verbs rather than auxiliaries), and dependents of heads are typed using tregex patterns (Levy and Andrew, 2006), an extension of the tgrep pattern language. The nodes in the final graph are then anno"
N06-1006,N03-1022,0,0.1205,"e extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems. 1 Introduction During the last five years there has been a surge in work which aims to provide robust textual inference in arbitrary domains about which the system has no expertise. The best-known such work has occurred within the field of question answering (Pasca and Harabagiu, 2001; Moldovan et al., 2003); more recently, such work has continued with greater focus in addressing the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005) and within the U.S. Government AQUAINT program. Substantive progress on this task is key to many text and natural language applications. If one could tell that Protestors chanted slogans opposing a free trade agreement was a match for people demonstrating against free trade, then one could offer a form of semantic search not available with current keywordbased search. Even greater benefits would flow to richer and more semantically complex NLP"
N06-1006,H05-1047,0,0.0223103,"with a factored alignment score. The last issue arising in the graph matching approaches is the inherent confounding of alignment and entailment determination. The way to show that one graph element does not follow from another is to make the cost of aligning them high. However, since we are embedded in a search for the lowest cost alignment, this will just cause the system to choose an alternate alignment rather than recognizing a non-entailment. In ID 152, we would like the hypothesis to align with the first part of the text, to 1 This is the same problem labeled and addressed as context in Tatu and Moldovan (2005). 43 be able to prove that civilians are not members of law enforcement agencies and conclude that the hypothesis does not follow from the text. But a graphmatching system will to try to get non-entailment by making the matching cost between civilians and members of law enforcement agencies be very high. However, the likely result of that is that the final part of the hypothesis will align with were civilians at the end of the text, assuming that we allow an alignment with “loose” arc correspondence.2 Under this candidate alignment, the lexical alignments are perfect, and the only imperfect al"
N10-1080,W08-0312,0,0.027162,"nfigurations of TERp, WER, several configurations of METEOR, as well as additive combinations of these metrics. The TERp configurations include the default configuration of TERp and TERpA: the configuration of TERp that was trained to match human judgments for NIST Metrics MATR (Matthew Snover and Schwartz, 2008; Przybocki et al., 2008). For METEOR, we used the standard METEOR English parameters (α = 0.8, β = 2.5, γ = 0.4), and the English parameters for the ranking METEOR (α = 0.95, β = 0.5, γ = 0.5),4 which was tuned to maximize the metric’s correlation with WMT-07 human ranking judgements (Agarwal and Lavie, 2008). The default METEOR parameters favor longer translations than the other metrics, since high α values place much more weight on unigram recall than precision. Since this may put models tuned to METEOR at a disadvantage when being evaluated by the other metrics, we also use a variant of the standard English model and of ranking METEOR with α set to 0.5, as this weights both recall and precision equally. For each iteration of MERT, 20 random restarts were used in addition to the best performing point discovered during earlier iterations of training.5 4 Agarwal and Lavie (2008) report γ = 0.45, h"
N10-1080,E06-1032,0,0.137662,"c. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006)."
N10-1080,D09-1030,0,0.210252,".0 NIST vs. BLEU:4 51.5 WER vs. TERp 51.5 METR:0.5 vs METR 51.5 TERp vs. BLEU:4 51.0 BLEU:4 vs. METR R:0.5 50.5 p-value 0.0028 0.02 0.089 0.089 0.11 0.11 0.11 0.22 0.26 0.26 0.31 0.31 0.36 0.36 0.47 < 0.001 0.052 0.069 0.11 0.14 0.36 0.36 0.36 0.42 0.47 Table 5: Select pairwise preference for models trained to different evaluation metrics. For A vs. B, preferred indicates how often A was preferred to B. We bold the better training metric for statistically significant differences. duced by experts by having multiple workers complete each HIT and then combining their answers (Snow et al., 2008; Callison-Burch, 2009). We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07). The HITs consist of a pair of machine translated sentences and a single human generated reference translation. The reference is chosen at random from those available for each sentence. Capitalization of the translated sentences is restored using an HMM based truecaser (Lita et al., 2003). Turkers are instructed to “. . . select the machine translation generated sentence that is easiest to read and best conveys what i"
N10-1080,N10-2003,1,0.719283,"hat restart points are provided, we use the same series of random restart points for each model. During each iteration of MERT, the random seed is based on the MERT iteration number. Thus, while a different set of random points is selected during each MERT iteration, on any given iteration all models use the same set of points. This prevents models from doing better or worse just because they received different starting points. However, it is still possible that certain random starting points are better for some evaluation metrics than others. 4 Experiments Experiments were run using Phrasal (Cer et al., 2010), a left-to-right beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) on a variety of data sets. During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the"
N10-1080,W08-0336,1,0.371777,"-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008"
N10-1080,D08-1024,0,0.0433274,"e language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a num"
N10-1080,N09-1025,0,0.0142183,"rtance the metric assigns to precision and recall. The fact that the WER, TER and TERp models perform very similarly suggests that current phrasebased translation systems lack either the features or the model structure to take advantage of swap edit operations. The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitiv"
N10-1080,D08-1089,1,0.909049,"Missing"
N10-1080,P06-1121,0,0.0202158,"o METEOR can be made more robust by setting α to 0.5, which 562 balances the importance the metric assigns to precision and recall. The fact that the WER, TER and TERp models perform very similarly suggests that current phrasebased translation systems lack either the features or the model structure to take advantage of swap edit operations. The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here"
N10-1080,N03-1017,0,0.0603839,"ned using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2"
N10-1080,P07-2045,0,0.0214612,"ng each iteration of MERT, the random seed is based on the MERT iteration number. Thus, while a different set of random points is selected during each MERT iteration, on any given iteration all models use the same set of points. This prevents models from doing better or worse just because they received different starting points. However, it is still possible that certain random starting points are better for some evaluation metrics than others. 4 Experiments Experiments were run using Phrasal (Cer et al., 2010), a left-to-right beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) on a variety of data sets. During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based"
N10-1080,N06-1014,0,0.0140937,"and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram languag"
N10-1080,P03-1020,0,0.00807775,"differences. duced by experts by having multiple workers complete each HIT and then combining their answers (Snow et al., 2008; Callison-Burch, 2009). We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07). The HITs consist of a pair of machine translated sentences and a single human generated reference translation. The reference is chosen at random from those available for each sentence. Capitalization of the translated sentences is restored using an HMM based truecaser (Lita et al., 2003). Turkers are instructed to “. . . select the machine translation generated sentence that is easiest to read and best conveys what is stated in the reference”. Differences between the two machine translations are emphasized by being underlined and bold faced.9 The resulting HITs are made available only to workers in the United States, as pilot experiments indicated this results in more consistent preference judgments. Three preference judgments are obtained for each pair of translations and are combined using weighted majority vote. As shown in table 5, in many cases the quality of the transla"
N10-1080,niessen-etal-2000-evaluation,0,0.0382506,"to tune the metric to human judgments on a specific language and variation of the evaluation task (e.g., ranking candidate translations vs. reproducing judgments of translations adequacy and fluency). 2.3 Translation Edit Rate TER (Snover et al., 2006) searches for the shortest sequence of edit operations needed to turn a candidate translation into one of the reference translations. The allowable edits are the insertion, deletion, and substitution of individual words and swaps of adjacent sequences of words. The swap operation differentiates TER from the simpler word error rate (WER) metric (Nießen et al., 2000), which only makes use of insertions, deletions, and substitutions. Swaps prevent phrase reorderings from being excessively penalized. Once the shortest sequence of operations is found,3 TER is calculated simply as the number of required edits divided by the reference translation length, or average reference translation length when multiple are available (4). TER = min edits avg ref length (4) TER-Plus (TERp) (Snover et al., 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric’s agreement with human judgments. TERp also introduces three new edit"
N10-1080,J03-1002,0,0.00225018,"ing toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic (Koehn et al., 2003). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Koehn et al., 2007). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. The resulting decoding model has 14 features to be trained. 5 Results As seen in tables 1 and 2, the evaluation metric we use during training has a substantia"
N10-1080,P03-1021,0,0.771788,"based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgment"
N10-1080,P02-1040,0,0.0968379,"ole in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate"
N10-1080,2006.amta-papers.25,0,0.790114,"tively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met1 MET"
N10-1080,W09-0441,0,0.314159,"g translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met1 METEOR: Metric for Evalua"
N10-1080,D08-1027,1,0.129613,"Missing"
N10-1080,D07-1080,0,0.0464296,"depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However"
N10-1080,D09-1006,0,0.0246061,"ture of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitive to how the mechanics of the metric interact with the structure and feature set of the decoding model being used. BLEU and NIST’s strong showing in both the machine and human evaluation results indicates that they are still the best general choice for training model parameters. We emphasize that improved metric correlations with human judgments do not imply that models"
N10-1080,I05-3027,1,\N,Missing
N10-2003,W09-2307,1,0.558961,"ensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Ra"
N10-2003,D08-1089,1,0.894263,"construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/sof"
N10-2003,P09-1087,1,0.142109,"rget language do a poor job of modeling long distance syntactic relationships. For example, if there are a number of intervening words between a verb and its subject, n-gram language models will often not be of much help in selecting the verb form that agrees with the subject. The target side dependency language model feature captures these long distance relationships by providing a dependency score for the target translations produced by the decoder. This is done using an efficient quadratic time algorithm that operates within the main decoding loop rather than in a separate reranking stage (Galley and Manning, 2009). Discriminative Distortion The standard distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically sign"
N10-2003,N10-1129,1,0.124195,"d distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models"
N10-2003,N03-1017,0,0.00907943,"ases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license"
N10-2003,P07-2045,0,0.0488706,"Missing"
N10-2003,koen-2004-pharaoh,0,0.121277,"f) (model_name) Running this command will first create word level alignments for the sentences in source.txt and target.txt using the Berkeley cross-EM aligner 1 http://www.itl.nist.gov/iad/mig/tests /mt/2009/ResultsRelease/currentArabic.html 9 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9–12, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 3 Decoder Decoding Engines The package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Manning, 2010). Both engines use features written to a common but extensible feature API, which allows features to be written once and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading"
N10-2003,N06-1014,0,0.141897,"ematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by"
N10-2003,niessen-etal-2000-evaluation,0,0.0200338,"ature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assist in the development of new"
N10-2003,J03-1002,0,0.00159764,"e and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to m"
N10-2003,P03-1021,0,0.0139183,"and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. tranlations per minute Figure 1: Chinese-to-English translation using discontinuous phrases. 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared betwe"
N10-2003,P02-1040,0,0.102389,"ord-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. tranlations per minute Figure 1: Chinese-to-English translation using discontinuous phrases. 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared between the threads is starting to overwhelm the value provided b"
N10-2003,W09-0441,0,0.0154617,"ese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assi"
P13-1031,2007.mtsummit-papers.3,0,0.172644,"Missing"
P13-1031,P12-1016,1,0.0338413,"d the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using"
P13-1031,W08-0304,1,0.824052,"crimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses"
P13-1031,N10-2003,1,0.951564,"). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Adaptive Online Algorithms 2.2 Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions"
P13-1031,W08-0336,1,0.0716331,"es baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) impleme"
P13-1031,N12-1047,0,0.33688,"sed in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Lin"
P13-1031,D08-1024,0,0.396709,"in MT where good sparse features may fire very infrequently. We would instead like to take larger steps for sparse features and smaller steps for dense features. 2.1 1/2 wt = wt−1 − ηΣt ∇`t (wt−1 ) (2) −1 &gt; Σ−1 t = Σt−1 + ∇`t (wt−1 )∇`t (wt−1 ) = t X i=1 ∇`i (wi−1 )∇`i (wi−1 )&gt; (3) A diagonal approximation to Σ can be used for a high-dimensional vector wt . In this case, AdaGrad is simple to implement and computationally cheap. Consider a single dimension j, and P let scalars vt = wt,j , gt = ∇j `t (wt−1 ), Gt = ti=1 gi2 , then the update rule is −1/2 vt = vt−1 − η Gt Gt = Gt−1 + gt gt2 MIRA Chiang et al. (2008) described an adaption of MIRA (Crammer et al., 2006) to MT. MIRA makes the following update: wt = arg min w We specify the loss function for MT in section 3.1. 1 kw − wt−1 k22 + `t (w) 2η (6) The first term expresses conservativity: the weight should change as little as possible based on a single example, ensuring that it is never beneficial to overshoot the minimum. The relationship to SGD can be seen by linearizing the loss function `t (w) ≈ `t (wt−1 ) + (w − wt−1 )&gt; ∇`t (wt−1 ) and taking the derivative of (6). The result is exactly (1). AROW Chiang (2012) adapted AROW (Crammer et al., 200"
P13-1031,N09-1025,0,0.1009,"mes a mini-batch of examples. 3.2 Updating and Regularization Algorithm 1 lines 9–11 compute the adaptive learning rate, update the weights, and apply regularization. Section 2.1 explained the AdaGrad learning rate computation. To update and regularize the weights we apply the Forward-Backward Splitting (FOBOS) (Duchi and Singer, 2009) framework, which separates the two operations. The two-step FOBOS update is wt− 1 = wt−1 − ηt−1 ∇`t−1 (wt−1 ) 2 (13) search. Some of the features generalize, but many do not. This was well understood in previous work, so heuristic filtering was usually applied (Chiang et al., 2009, inter alia). In contrast, we need only select an appropriate regularization strength λ. Specifically, when r(w) = λkwk1 , the closedform solution to (14) is h i wt = sign(wt− 1 ) |wt− 1 |− ηt−1 λ (15) 2 where [x]+ = max(x, 0) is the clipping function that in this case sets a weight to 0 when it falls below the threshold ηt−1 λ. It is straightforward to adapt this to AdaGrad with diagonal Σ by setting 1 2 each dimension of ηt−1,j = ηΣt,jj and by taking element-wise products. We find that ∇`t−1 (wt−1 ) only involves several hundred active features for the current example (or mini-batch). Howev"
P13-1031,D08-1089,1,0.124908,"ier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Al"
P13-1031,W12-3154,0,0.0117732,"by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation. 1 To learn good weights for the sparse features, most algorithms—including ours—benefit from more tuning data, and the natural source is the training bitext. However, the bitext presents two problems. First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues. Introduction Sparse, overlapping features such as words and ngram contexts improve many NLP systems such as parsers and taggers. Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an acti"
P13-1031,W11-2130,0,0.13411,"Missing"
P13-1031,2012.iwslt-papers.17,0,0.195332,"ing features such as words and ngram contexts improve many NLP systems such as parsers and taggers. Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from t"
P13-1031,P12-1031,0,0.0207883,"g set, we improve significantly over all other models. PRO learns a smaller model with the PT+AL+LO feature set which is surprising given that it applies L2 regularization (AdaGrad uses L1 ). We speculate that this may be an consequence of stochastic learning. Our algorithm decodes each example with a new weight vector, thus exploring more of the search space for the same tuning set. 4.4 Bitext Tuning Experiment Tables 2 and 3 show that adding tuning examples improves translation quality. Nevertheless, even the larger tuning set is small relative to the bitext from which rules were extracted. He and Deng (2012) and Simianer et al. (2012) showed significant translation quality gains by tuning on the bitext. However, their bitexts matched the genre of their test sets. Our bitexts, like those of most large-scale systems, do not. Domain mismatch matters for the dense feature set (Haddow and Koehn, 2012). We show that it also matters for feature-rich MT. Before aligning each bitext, we randomly sampled and sequestered 5k and 15k sentence tuning sets, and a 5k test set. We prevented overlap beDA MT04 MT04 MT04 MT04 5ktest 5ktest DB MT06 MT568 bitext5k bitext15k bitext5k bitext15k |A| 70k 70k 70k 70k 82k 8"
P13-1031,D11-1125,0,0.173043,"lly means choosing a hinge loss. On the other hand, AdaGrad/linearized AROW only requires that the gradient of the loss function can be computed efficiently. Adaptive Online MT Algorithm 1 shows the full algorithm introduced in this paper. AdaGrad (lines 9–10) is a crucial piece, but the loss function, regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 3.1 Pairwise Logistic Loss Function Algorithm 1 lines 5–8 describe the gradient computation. We cast MT tuning as pairwise ranking (Herbrich et al., 1999, inter alia), which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1 (Lin and Och, 2004). Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Let M (d) = w · φ(d) be the model score. For any derivation d+ that is better than d− under G, we desire pairwise agreement such"
P13-1031,N07-1008,0,0.0932284,"Missing"
P13-1031,P03-1054,1,0.008716,"w-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to B"
P13-1031,P07-2045,0,0.0116504,"some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, c Sofia, Bul"
P13-1031,N09-1069,0,0.198652,"oolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Adaptive Online Algorithms 2.2 Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). Recall that stochastic gradient descent (SGD), a fundamental online method, updates weights w according to wt = wt−1 − η∇`t (wt−1 ) (1) with loss function1 `t (w) of the tth example, (sub)gradient of the loss with respect to the parameters ∇`t (wt−1 ), and learning rate η. SGD is sensitive to the learning rate η, which is difficult to set in an MT system that mixes frequent “dense” features (like the language model) with sparse features (e.g., for translation rules). Furthermore, η applies to each coordinate in the gradient, an undesirable property in"
P13-1031,P06-1096,0,0.115406,"ight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4 4.2 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized"
P13-1031,N06-1014,0,0.0250986,"ight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4 4.2 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized"
P13-1031,N12-1023,0,0.0600191,"statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Ou"
P13-1031,C04-1072,0,0.037745,"regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 3.1 Pairwise Logistic Loss Function Algorithm 1 lines 5–8 describe the gradient computation. We cast MT tuning as pairwise ranking (Herbrich et al., 1999, inter alia), which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1 (Lin and Och, 2004). Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Let M (d) = w · φ(d) be the model score. For any derivation d+ that is better than d− under G, we desire pairwise agreement such that     G e(d+ ), e1:k &gt; G e(d− ), e1:k 313 ⇐⇒ M (d+ ) &gt; M (d− ) 2 According to experiments not reported in this paper. Ensuring pairwise agreement is the same as ensuring w · [φ(d+ ) − φ(d− )] &gt; 0. For learning, we need to select derivation pairs (d+ , d− ) to compute difference vectors x+ ="
P13-1031,W10-2925,0,0.0155326,"gradient” method of Langford et al. (2009) (Algorithm 2). A fixed threadpool of workers computes gradients in parallel and sends them to a master thread, which updates a central weight vector. Crucially, the weight updates need not be applied in order, so synchronization is unnecessary; the workers only idle at the end of an epoch. The consequence is that the update in line 8 of Algorithm 2 is with respect to gradient gt0 with t0 ≤ t. Langford et al. (2009) gave convergence results for 314 stale updating, but the bounds do not apply to our setting since we use L1 regularization. Nevertheless, Gimpel et al. (2010) applied this framework to other non-convex objectives and obtained good empirical results. Bilingual Ar-En Zh-En Monolingual Sentences Tokens Tokens 6.6M 9.3M 375M 538M 990M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). Our asynchronous, stochastic method has practical appeal for MT. During a tuning run, the online method decodes the tuning set under many more weight vectors than a MERT-style batch method. This characteristic may result in broader exploration of the"
P13-1031,maamouri-etal-2008-enhancing,0,0.0478018,"ature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second"
P13-1031,J93-2004,0,0.0522379,"to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional basel"
P13-1031,N10-1069,0,0.0335846,"dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on:"
P13-1031,P02-1038,0,0.168102,"Missing"
P13-1031,P12-1002,0,0.684228,"nslation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in t"
P13-1031,P06-1091,0,0.110701,"Missing"
P13-1031,D07-1080,0,0.155024,"ithms. 6 Related Work Our work relates most closely to that of Hasler et al. (2012b), who tuned models containing both sparse and dense features with Moses. A discriminative phrase table helped them improve slightly over a dense, online MIRA baseline, but their best results required initialization with MERT-tuned weights and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized"
P13-1031,J04-4002,0,0.248212,"table exist in shared memory, obviating the need for remote queries. tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative reordering (LO): indicators for eight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4 4.2 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19"
P13-1031,N12-1026,0,0.119789,"Missing"
P13-1031,P03-1021,0,0.245203,"t half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational"
P13-1031,P11-2074,0,0.15203,"Missing"
P13-1031,P02-1040,0,0.113953,"ing to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. We ran their code with standard settings. Moses5 also contains the discriminative phrase table implementatio"
P13-1031,W05-0908,0,0.152488,"Missing"
P13-1031,2012.iwslt-evaluation.4,0,\N,Missing
P13-1031,D08-1076,0,\N,Missing
S12-1051,W07-0718,0,0.194874,"Missing"
S12-1051,W08-0309,0,0.0142237,"Missing"
S12-1051,P11-1020,0,0.0767679,"Missing"
S12-1051,C04-1051,0,0.824876,"Missing"
S12-1051,N06-2015,0,0.158425,"Missing"
S12-1096,P02-1001,0,0.0220005,"he systems described in this paper. We start off by framing the problem of semantic textual similarity in terms of weighted edit distance calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains and probabilistic finite state transducers all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our FSM accepts is an edit sequence that transforms one side of the sentence pair (denoted as s1 ) into the other side (s2 ). Our pFSM has a unique start and stop state, and one state per edit operation (i.e., Insert, Delete, Substitution). The probability of an edit sequence e is generated by the model is the product of the state transition probabilities in the pFSM, formally de648 First Joint Confer"
S12-1096,knight-al-onaizan-1998-translation,0,0.0674666,"development or training of the systems described in this paper. We start off by framing the problem of semantic textual similarity in terms of weighted edit distance calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains and probabilistic finite state transducers all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our FSM accepts is an edit sequence that transforms one side of the sentence pair (denoted as s1 ) into the other side (s2 ). Our pFSM has a unique start and stop state, and one state per edit operation (i.e., Insert, Delete, Substitution). The probability of an edit sequence e is generated by the model is the product of the state transition probabilities in the pFSM, formally de648 Firs"
S12-1096,N03-1019,0,0.019888,"cribed in this paper. We start off by framing the problem of semantic textual similarity in terms of weighted edit distance calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains and probabilistic finite state transducers all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our FSM accepts is an edit sequence that transforms one side of the sentence pair (denoted as s1 ) into the other side (s2 ). Our pFSM has a unique start and stop state, and one state per edit operation (i.e., Insert, Delete, Substitution). The probability of an edit sequence e is generated by the model is the product of the state transition probabilities in the pFSM, formally de648 First Joint Conference on Lexical and Com"
S12-1096,W12-3107,1,0.782976,"interpolated into the official submitted runs ranking, it would be placed at the 22nd place among 89 runs. Among the three official runs submitted to the shared task (pPDAAll, pFSMIndi and Entailment), pFSMIndi performs the best, placed at 653 38th place among 89 runs. Since our metrics were originally designed for statistical machine translation (MT) evaluation, we found that on the unseen SMTNews test set, which consists of news conversation sentence pairs from the MT domain, our pPDA model placed at a much higher position (13 among 89 runs). In comparison to results on MT evaluation task (Wang and Manning, 2012), we found that the pPDA and pFSM models work less well on STS. Whereas in MT evaluation it is common to have access to thousands of training examples, there is an order of magnitude less available training data in STS. Therefore, learning hundreds of feature parameters in our models from such few examples are likely to be ill-posed. Overall, the RTE system did not perform as well as the regression based models except for MSRvid domain , which has the shortest overall sentence length. Our qualitative evaluation suggests that MSRvid domain seems to exhibit the least degree of lexical divergence"
S12-1096,W07-1401,0,\N,Missing
S13-1004,S12-1051,1,0.661393,"t together. • (1) The two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar. • (0) The two sentences are on different topics. John went horse back riding at dawn with a whole group of friends. Sunrise at dawn is a magnificent view to take in if you wake up early enough for it. Figure 1: Annotation values with explanations and examples for the core STS task. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held a DARPA sponsored workshop at Columbia University1 . In 2013, STS was selected as the official Shared Task of the *SEM 2013 conference. Accordingly, in STS 2013, we set up two tasks: The core task CORE, which is similar to the 2012 task; and a pilot task on typed-similarity TYPED between semi-structured records. For CORE, we provided all the STS 2012 data as training data, and the test data was drawn from related but different datasets. This is in contrast to the STS 2012 task where the train/test data were drawn from the same datasets. The 2012 datasets comprised the fo"
S13-1004,P98-1013,0,0.04578,"(OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the authors. The rest, 291 pairs, are sampled from unmapped sense pairs with a string similarity ranging from 0.5 to 0.9. The FnWN subset has 189 manually mapped pairs of senses from FrameNet 1.5 (Baker et al., 1998) to WordNet 3.1. They are ran35 domly selected from 426 mapped pairs. In combination, both datasets comprise 750 pairs of glosses. 2.2 Typed-similarity TYPED task This task is devised in the context of the PATHS project,3 which aims to assist users in accessing digital libraries looking for items. The project tests methods that offer suggestions about items that might be useful to recommend, to assist in the interpretation of the items, and to support the user in the discovery and exploration of the collections. Hence the task is about comparing pairs of items. The pairs are generated in the E"
S13-1004,S12-1059,0,0.0590663,"_coefficient# Calculating_a_weighted_correlation a one-tailed parametric test based on Fisher’s ztransformation (Press et al., 2002, equation 14.5.10). 4.2 The Baseline Systems For the CORE dataset, we produce scores using a simple word overlap baseline system. We tokenize the input sentences splitting at white spaces, and then represent each sentence as a vector in the multidimensional token space. Each dimension has 1 if the token is present in the sentence, 0 otherwise. Vector similarity is computed using the cosine similarity metric. We also run two freely available sysˇ c et tems, DKPro (Bar et al., 2012) and TakeLab (Sari´ 5 al., 2012) from STS 2012, and evaluate them on the CORE dataset. They serve as two strong contenders since they ranked 1st (DKPro) and 2nd (TakeLab) in last year’s STS task. For the TYPED dataset, we first produce XML files for each of the items, using the fields as provided to participants. Then we run named entity recognition and classification (NERC) and date detection using Stanford CoreNLP. This is followed by calculating the similarity score for each of the types as follows. • General: cosine similarity of TF-IDF vectors of tokens from all fields. • Author: cosine s"
S13-1004,N12-1017,0,0.00854843,"ies on several (1-4) reference translations. HYTER, on the other hand, leverages millions of translations. The HTER set comprises 150 pairs, where one sentence is machine translation output and the corresponding sentence is a human post-edited translation. We sample the data from the dataset used in the DARPA GALE project with an HTER score ranging from 0 to 120. The HYTER set has 600 pairs from 3 subsets (each subset contains 200 pairs): a. reference Figure 3: Annotation instructions for TYPED task vs. machine translation. b. reference vs. Finite State Transducer (FST) generated translation (Dreyer and Marcu, 2012). c. machine translation vs. FST generated translation. The HYTER data set is used in (Dreyer and Marcu, 2012). The OnWN/FnWN dataset contains gloss pairs from two sources: OntoNotes-WordNet (OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the au"
S13-1004,N06-2015,0,0.104937,"ation instructions for TYPED task vs. machine translation. b. reference vs. Finite State Transducer (FST) generated translation (Dreyer and Marcu, 2012). c. machine translation vs. FST generated translation. The HYTER data set is used in (Dreyer and Marcu, 2012). The OnWN/FnWN dataset contains gloss pairs from two sources: OntoNotes-WordNet (OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the authors. The rest, 291 pairs, are sampled from unmapped sense pairs with a string similarity ranging from 0.5 to 0.9. The FnWN subset has 189 manually mapped pairs of senses from FrameNet 1.5 (Baker et al., 1998) to WordNet 3.1. They are ran35 domly selected from 426 mapped pairs. In combination, both datasets comprise 750 pairs of glosses. 2.2 Typed-similarity TYPED task This task is devised in the context of the PATHS project,3 which aims to assist users in acce"
S13-1004,2006.amta-papers.25,0,0.0305115,"the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sample another 375 pairs from the different EMM cluster in the same manner. The SMT dataset comprises pairs of sentences used in machine translation evaluation. We have two different sets based on the evaluation metric used: an HTER set, and a HYTER set. Both metrics use the TER metric (Snover et al., 2006) to measure the similarity of pairs. HTER typically relies on several (1-4) reference translations. HYTER, on the other hand, leverages millions of translations. The HTER set comprises 150 pairs, where one sentence is machine translation output and the corresponding sentence is a human post-edited translation. We sample the data from the dataset used in the DARPA GALE project with an HTER score ranging from 0 to 120. The HYTER set has 600 pairs from 3 subsets (each subset contains 200 pairs): a. reference Figure 3: Annotation instructions for TYPED task vs. machine translation. b. reference vs"
S13-1004,S12-1060,0,0.328958,"Missing"
S13-1004,C98-1013,0,\N,Missing
S14-2010,S14-2085,0,0.0392393,"Missing"
S14-2010,S14-2069,0,0.0326403,"Missing"
S14-2010,S14-2128,0,0.0328229,"Missing"
S14-2010,P13-1024,1,0.0618966,"data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the news title, while the other one represents a Twitter comment on that particular news. They are evenly sampled from string similarity values between 0.5 and 1. Table 1 shows the explanations and values associated with each score between 5 and 0. As in prior years, we used Amazon Mechanical Turk (AMT)3 to crowdsource the annotation of the English pairs.4 Annotators are presented with the Table 2: English subtask: Summary of train (2012 and 2013) and test (2014) datasets. a DARPA sponsored workshop at Columbia University.1 In 2013, STS wa"
S14-2010,S14-2112,0,0.0317369,"Missing"
S14-2010,N06-2015,0,0.0715746,"ferent similarity ranges, hence we built two sets of headline pairs: (i) a set where the pairs come from the same EMM cluster, (ii) and another set where the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs"
S14-2010,S12-1051,1,0.623306,"r as both tasks have been defined to date in the literature) in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for a myriad of NLP tasks such as MT evaluation, information extraction, question answering, summarization, etc. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline map"
S14-2010,S14-2131,0,0.0336314,"Missing"
S14-2010,S14-2072,0,0.0817436,"Missing"
S14-2010,Q14-1018,0,0.0731,"Missing"
S14-2010,S14-2039,0,0.0993932,"Missing"
S14-2010,S14-2078,0,0.101731,"Missing"
S14-2010,S14-2022,0,0.0306687,"Missing"
S14-2010,S14-2046,0,0.022257,"Missing"
S14-2010,D13-1179,0,0.0175763,"Missing"
S14-2010,S12-1060,0,0.0199826,"Missing"
S14-2010,S14-2093,0,0.0281526,"Missing"
S14-2010,W10-0721,0,0.050451,"d 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the new"
S14-2010,W10-0707,0,\N,Missing
S14-2010,P94-1019,0,\N,Missing
S14-2010,Q14-1017,0,\N,Missing
S14-2010,S14-2138,0,\N,Missing
S15-2045,agerri-etal-2014-ixa,1,0.57453,"onal and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple regular expressions. Additionally, this step collects chunk regions coming either from gold standard or from the chunking done by ixa-pipes-chunk (Agerri et al., 2014). This is followed by a lowercased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus ena"
S15-2045,S12-1051,1,0.519741,"TS also differs from both TE and paraphrasing (in as far as both tasks have been defined to date in the literature) in that rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for many NLP tasks such as MT evaluation, information extraction, question answering, summarization. In 2012, we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success (Agirre et al., 2012). In addition, we 252 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity"
S15-2045,S14-2010,1,0.800447,"f the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity in a new language, namely Spanish (Agirre et al., 2014). This year we presented three subtasks: the English subtask, the Spanish subtask and the interpretable pilot subtask. The English subtask comprised pairs from headlines and image descriptions, and it also introduced new genres, including answer pairs from a tutorial dialogue system and from Q&A websites, and pairs from a dataset tagged with committed belief annotations. For the Spanish subtask, additional pairs from news and Wikipedia articles were selected. The annotations for both tasks leveraged crowdsourcing. Finally, with the interpretable STS pilot subtask, we wanted to start exploring"
S15-2045,P14-1023,0,0.0115737,"7070 0.7251 0.7311 0.7250 0.7422 0.6364 0.7775 0.7032 0.7130 0.7189 0.4616 0.7533 0.6111 0.5379 0.5424 0.5672 0.6558 0.4919 0.5912 0.6964 0.7114 0.6364 Table 3: Task 2a: English evaluation results in terms of Pearson correlation. 259 Rank 61 42 29 63 56 57 70 69 71 62 44 49 43 74 72 73 34 28 26 1 3 5 19 18 16 8 9 2 12 13 23 41 59 20 47 22 11 15 33 45 55 24 10 17 50 51 52 4 7 6 46 36 27 39 31 30 32 25 53 14 40 37 35 68 21 58 66 65 64 48 67 60 42 38 54 approach for the top three participants (DLS@CU, ExBThemis, Samsung). They use WordNet (Miller, 1995), Mikolov Embeddings (Mikolov et al., 2013; Baroni et al., 2014) and PPDB (Ganitkevitch et al., 2013). In general, generic NLP tools such as lemmatization, PoS tagging, distributional word embeddings, distributional and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple"
S15-2045,N13-1092,0,0.0796576,"the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,P13-2080,0,0.0179598,"Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,W10-0721,0,0.0157602,"ns student answers Q&A forum answers commited belief Table 2: English subtask: Summary of train (2012, 2013, 2014) and test (2015) datasets. lines come from a different EMM cluster. Then, we computed the string similarity between those pairs. Accordingly, we sampled 1000 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity as a metric. We sampled another 1000 pairs from the different EMM cluster in the same manner. The Images dataset is a subset of the PASCAL VOC-2008 dataset (Rashtchian et al., 2010), which consists of 1000 images with around 10 descriptions each, and has been used by a number of image description systems. It was also sampled using string similarity, discarding those that had been used in previous years. We organized two bins with 1000 pairs each: one with pairs of descriptions from the same image, and the other one with pairs of descriptions from different images. The source of the Answers-student pairs is the BEETLE corpus (Dzikovska et al., 2010), which is a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system."
S15-2045,W00-0726,0,0.313421,"Missing"
S15-2045,S12-1060,0,0.211502,"Missing"
S15-2045,W10-0707,0,\N,Missing
S16-1081,S16-1103,0,0.0432732,"n of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The nex"
S16-1081,S12-1051,1,0.454212,"for replicating human judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translat"
S16-1081,S13-1004,1,0.536348,"n judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techni"
S16-1081,S14-2010,1,0.564132,"g the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fl"
S16-1081,S16-1101,1,0.859309,"Missing"
S16-1081,S16-1086,0,0.0343004,"Missing"
S16-1081,P98-1013,0,0.0704238,"E498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translation"
S16-1081,S16-1117,0,0.0317832,"Missing"
S16-1081,S16-1089,0,0.463664,"STS. The overall winner, Samsung Poland NLP Team, proposes a textual similarity model that is a novel hybrid of recursive auto-encoders from deep learning with penalty and reward signals extracted from WordNet (Rychalska et al., 2016). To obtain even better performance, this model is combined in an ensemble with a number of other similarity models including a version of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structure"
S16-1081,D15-1181,0,0.0283646,"nik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased approach for textual similarity that incorporates word alignment information (Itoh, 2016). 6"
S16-1081,S16-1170,0,0.023392,"s such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased"
S16-1081,N06-2015,0,0.0113871,"ual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translations. We also introduce an evaluat"
S16-1081,S16-1106,0,0.0956972,"ween the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time systems without corrections are: ALL 0.68923; plagiarism 0.78949; answer-answer 0.48018; postediting 0.81241; headlines 0.76439; question-question 0.57140. Team Run ALL Ans.-Ans. HDL Plagiarism Postediting Ques.-Ques. Samsung Poland NLP Team UWB MayoNLPTeam Samsung Poland NLP Team NaCTeM ECNU UMD-TTIC"
S16-1081,P14-2124,0,0.0245172,"roximately 0.25 drop in correlation on the news data as compare to the multi-source setting; 2) systems performing evenly on both data sets. 6.5.1 Methods In terms of approaches, most runs rely on a monolingual framework. They automatically translate the Spanish member of a sentence pair into English and then compute monolingual semantic similarity using a system developed for English. In contrast, the CNRC team (Lo et al., 2016) provides a true crosslingual system that makes use of embedding space phrase similarity, the score from XMEANT, a crosslingual machine translation evaluation metric (Lo et al., 2014), and precision and recall features for material filling aligned cross-lingual semantic roles (e.g., action, agent, patient). The FBK HLT team (Ataman et al., 2016) proposes a model combining cross-lingual word embeddings with features from QuEst (Specia et al., 2013), a tool for machine translation quality estimation. The RTM system (Bic¸ici, 2016) also builds on methods developed for machine translation quality estimation and is applicable to both cross-lingual and monolingual similarity. The GWU NLP team (Aldarmaki and Diab, 2016) uses a shared cross-lingual vector space to directly assess"
S16-1081,S16-1102,0,0.0363344,"Missing"
S16-1081,P14-5010,0,0.0120217,"data sources we use for the evaluation sets. 3.1.1 Selection Heuristics Unless otherwise noted, pairs are heuristically selected using a combination of lexical surface form and word embedding similarity between a candidate pair of text snippets. The heuristics are used to find pairs sharing some minimal level of either surface or embedding space similarity. An approximately equal number of candidate sentence pairs are produced using our lexical surface form and word embedding selection heuristics. Both heuristics make use of a Penn Treebank style tokenization of the text provided by CoreNLP (Manning et al., 2014). 500 year 2016 2016 2016 dataset Trial News Multi-source pairs 103 301 294 source Sampled ≤ 2015 STS en-es news articles en news headlines, short-answer plag., MT postedits, Q&A forum answers, Q&A forum questions Table 3: Spanish-English subtask: Trial and test data sets. Surface Lexical Similarity Our surface form selection heuristic uses an information theoretic measure based on unigram overlap (Lin, 1998). As shown in equation (1), surface level lexical similarity between two snippets s1 and s2 is computed as a log probability weighted sum of the words common to both snippets divided by a"
S16-1081,D14-1162,0,0.109685,"Missing"
S16-1081,S16-1093,0,0.0144173,"ased model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engin"
S16-1081,S16-1091,0,0.0291873,"representations of the two snippets. 6.4 English Subtask The rankings for the English STS subtask are given in Tables 4 and 5. The baseline system ranked 100th. Table 6 provides the best and median scores for each of the individual evaluation sets as well as overall.15 The table also provides the difference between the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time sy"
S16-1081,P13-4014,0,0.0272301,"Missing"
S16-1081,2011.eamt-1.12,0,0.00782041,"swers. This corpus provides a collection of short answers to computer science questions that exhibit varying degrees of plagiarism from related Wikipedia articles.4 The short answers include text that was constructed by each of the following four strategies: 1) copying and pasting individual sentences from Wikipedia; 2) light revision of material copied from Wikipedia; 3) heavy revision of material from Wikipedia; 4) non-plagiarised answers produced without even looking at Wikipedia. This corpus is segmented into individual sentences using CoreNLP (Manning et al., 2014). 3.1.4 Postediting The Specia (2011) EAMT 2011 corpus provides machine translations of French news data using the Moses machine translation system (Koehn et al., 2007) paired with postedited corrections of those translations.5 The corrections were provided by human translators instructed to perform the minimum useful for finding semantically similar text snippets that differ in surface form. 4 Questions: A. What is inheritance in object orientated programming?, B. Explain the PageRank algorithm that is used by the Google search engine, C. Explain the Vector Space Model that is used for Information Retrieval., D. Explain Bayes Th"
S16-1081,S15-2027,0,0.0111195,"r to be significantly worse than the monolingual submissions even though the systems are being asked to perform the more challenging problem of evaluating crosslingual sentence pairs. While the correlations are not directly comparable, they do seem to motivate a more direct comparison between cross-lingual and monolingual STS systems. In terms of performance on the manually culled news data set, the highest overall rank is achieved by an unsupervised system submitted by team UWB (Brychcin and Svoboda, 2016). The unsupervised UWB system builds on the word alignment based STS method proposed by Sultan et al. (2015). However, when calculating the final similarity score, it weights both the aligned and unaligned words by their inverse document frequency. This system is able to attain a 0.912 correlation on the news data, while ranking second on the multi-source data set. For the multi-source test set, the highest scoring submission is a supervised system from the UWB team that combines multiple signals originating from lexical, syntactic and semantic similarity approaches in a regression-based model, achieving a 0.819 correlation. This is modestly better than the second place unsupervised approach that ac"
S16-1081,S16-1094,0,0.00995156,"th a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without"
S16-1081,C98-1013,0,\N,Missing
S16-1081,P07-2045,0,\N,Missing
S17-2001,S17-2013,0,0.019824,"Missing"
S17-2001,S17-2031,0,0.0137096,"Missing"
S17-2001,P98-1013,0,0.169413,"Missing"
S17-2001,S15-2045,1,0.888131,"elated but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversati"
S17-2001,L16-1662,0,0.0083775,"ubmission uses sentence IC exclusively. Another ensembles IC with Sultan et al. (2015)’s alignment method, while a third ensembles IC with cosine similarity of summed word embeddings with an IDF weighting scheme. Sentence IC in isolation outperforms all systems except those from ECNU. Combining sentence IC with word embedding similarity performs best. CompiLIG (Ferrero et al., 2017) The best Spanish-English performance on SNLI sentences was achieved by CompiLIG using features including: cross-lingual conceptual similarity using DBNary (Serasset, 2015), cross-language MultiVec word embeddings (Berard et al., 2016), and Brychcin and Svoboda (2016)’s improvements to Sultan et al. (2015)’s method. LIM-LIG (Nagoudi et al., 2017) Using only weighted word embeddings, LIM-LIG took second place on Arabic.17 Arabic word embeddings are summed into sentence embeddings using uniform, POS and IDF weighting schemes. Sentence similarity is computed by cosine similarity. POS and IDF outperform uniform weighting. Combining the IDF and POS weights by multiplication is reported by LIM-LIG to achieve r 0.7667, higher than all submitted Arabic (track 1) systems. HCTI (Shao, 2017) Third place overall is obtained by HCTI wit"
S17-2001,S17-2030,0,0.0302147,"Missing"
S17-2001,S17-2021,0,0.0336317,"Missing"
S17-2001,S16-1081,1,0.903281,"n an English sentence and its Arabic machine translation5 where they perform post-editing to correct errors. Spanish translation is completed by a University of Sheffield graduate student who is a native Spanish speaker and fluent in English. Turkish translations are obtained from SDL.6 3.4 Crowdsourced Annotations Crowdsourced annotation is performed on Amazon Mechanical Turk.8 Annotators examine the STS pairings of English SNLI sentences. STS labels are then transferred to the translated pairs for crosslingual and non-English tracks. The annotation instructions and template are identical to Agirre et al. (2016). Labels are collected in batches of 20 pairs with annotators paid $1 USD per batch. Five annotations are collected per pair. The MTurk master9 qualification is required to perform the task. Gold scores average the five individual annotations. This section describes the preparation of the evaluation data. For SNLI data, this includes the selection of sentence pairs, annotation of pairs with STS labels and the translation of the original English sentences. WMT quality estimation data is directly annotated with STS labels. 3.3 Annotation 4.1 Table 2 summarizes the evaluation data by track. The s"
S17-2001,W14-3302,1,0.742822,"Missing"
S17-2001,S12-1051,1,0.784858,"nd paraphrase detection in that it captures gradations of meaning overlap rather than making binary classifications of particular relationships. While semantic relatedness expresses a graded semantic relationship as well, it is non-specific about the nature of the relationship with contradictory material still being a candidate for a high score (e.g., “night” and “day” are highly related but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep lear"
S17-2001,S17-2012,0,0.0153726,"Missing"
S17-2001,S17-2032,0,0.0142311,"Missing"
S17-2001,D15-1075,0,0.136451,"s/missing. John said he is considered a witness but not a suspect. “He is not a suspect anymore.” John said. The two sentences are not equivalent, but share some details. They flew out of the nest in groups. They flew into the nest together. The two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar. The two sentences are completely dissimilar. The black dog is running through the snow. A race car driver is driving his car through the mud. Evaluation Data The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) is the primary evaluation data source with the exception that one of the 3 Previous years of the STS shared task include more data sources. This year the task draws from two data sources and includes a diverse set of languages and language-pairs. 4 HTER is the minimal number of edits required for correction of a translation divided by its length after correction. pilot track on cross-lingual Spanish-English STS. The English tracks attracted the most participation and have the largest use of the evaluation data in ongoing research. 2 Track 1 2 3 4a 4b 5 6 Language(s) Arabic (ar-ar) Arabic-Engl"
S17-2001,S16-1089,0,0.0248981,"Missing"
S17-2001,S17-2015,0,0.0563979,"fication of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow and Peskov, 2017) NLPProxem UMDeep (Barrow and Peskov, 2017) Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* NLPProxem RTM (Bic¸ici"
S17-2001,D15-1181,0,0.0245131,"Missing"
S17-2001,N16-1108,0,0.0260604,"Missing"
S17-2001,S16-1170,0,0.0147665,"Missing"
S17-2001,D17-1070,0,0.281621,"Missing"
S17-2001,N16-1162,0,0.0181244,"Missing"
S17-2001,C04-1051,0,0.847641,"Missing"
S17-2001,N06-2015,0,0.0472258,"Missing"
S17-2001,S17-2024,0,0.0294688,"Missing"
S17-2001,S17-2019,0,0.0294083,"Missing"
S17-2001,P15-1162,0,0.0488426,"Missing"
S17-2001,S12-1061,0,0.675418,"ationship with contradictory material still being a candidate for a high score (e.g., “night” and “day” are highly related but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarizat"
S17-2001,marelli-etal-2014-sick,0,0.0431263,"from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006).4 Participants are not allowed to use the gold qualit"
S17-2001,P16-1089,0,0.0231566,"Missing"
S17-2001,S17-2025,0,0.0295389,"Missing"
S17-2001,H92-1116,0,0.16712,"Missing"
S17-2001,W16-1609,0,0.0245746,"Arabic, Spanish and Turkish. The primary evaluation criteria combines performance on all of the different language conditions except English-Turkish, which was run as a surprise language track. Even with this departure from prior years, the task attracted 31 teams producing 84 submissions. STS shared task data sets have been used extensively for research on sentence level similarity and semantic representations (i.a., Arora et al. (2017); Conneau et al. (2017); Mu et al. (2017); Pagliardini et al. (2017); Wieting and Gimpel (2017); He and Lin (2016); Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016b,a); He et al. (2015); Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment"
S17-2001,P17-2099,0,0.0191097,"Missing"
S17-2001,S17-2029,0,0.0284965,"Missing"
S17-2001,S17-2017,0,0.0940842,"jerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) hjpwhu hjpwhu compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) FCICU (Hassan et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* L2F/INESC-ID (Fialho et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) MatrusriIndia NRC* NRC OkadaNaoya ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) QLUT (Meng et al., 2017)* QLUT (Meng et al., 2017) QLUT (Meng et al., 2017)* SIGMA SIGMA SIGMA SIGMA PKU 2 SIGMA PKU 2 SIGMA PKU 2 STS-UHH (Kohail et al., 2017) UCSC-NLP UdL (Al-Natsheh et al., 2017) UdL (Al-Natsheh et al., 2017)* UdL (Al-Natsheh et al., 2017) Primary 73.16 70.44 69.40 67.89 67.03 66.62 65.98 65.90"
S17-2001,P10-1023,0,0.039078,"2017) Fourth place overall is MITRE that, like ECNU, takes an ambitious feature engineering approach complemented by deep learning. Ensembled components inˇ c clude: alignment similarity; TakeLab STS (Sari´ et al., 2012b); string similarity measures such as matching n-grams, summarization and MT metrics (BLEU, WER, PER, ROUGE); a RNN and recurrent convolutional neural networks (RCNN) over word alignments; and a BiLSTM that is state-ofthe-art for textual entailment (Chen et al., 2016). FCICU (Hassan et al., 2017) Fifth place overall is FCICU that computes a sense-base alignment using BabelNet (Navigli and Ponzetto, 2010). BabelNet synsets are multilingual allowing non-English and cross-lingual pairs to be processed similarly to English pairs. Alignment similarity scores are used with two runs: one that combines the scores within a string kernel and another that uses them with a weighted variant of Sultan et al. (2015)’s method. Both runs average the Babelnet based scores with soft-cardinality (Jimenez et al., 2012b). BIT (Wu et al., 2017) Second place overall is achieved by BIT primarily using sentence information content (IC) informed by WordNet and BNC word frequencies. One submission uses sentence IC exclu"
S17-2001,S17-2022,0,0.0368865,"Missing"
S17-2001,N18-1049,0,0.148448,"Missing"
S17-2001,S17-2014,0,0.250083,"ow and Peskov, 2017) Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* NLPProxem RTM (Bic¸ici, 2017)* UMDeep (Barrow and Peskov, 2017) RTM (Bic¸ici, 2017)* RTM (Bic¸ici, 2017)* ¨ ResSim (Bjerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) hjpwhu hjpwhu compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) FCICU (Hassan et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* L2F/INESC-ID (Fialho et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) MatrusriIndia NRC* NRC OkadaNaoya ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017)"
S17-2001,P14-5010,0,0.0119011,"Missing"
S17-2001,D14-1162,0,0.109043,"Missing"
S17-2001,P15-1094,0,0.0153938,"Missing"
S17-2001,S12-1060,0,0.0229108,"Missing"
S17-2001,C16-1009,0,0.0254292,"lect methods are highlighted below. As directed by the SemEval workshop organizers, the CodaLab research platform hosts the task.11 6.4 Rankings Baseline The baseline is the cosine of binary sentence vectors with each dimension representing whether an individual word appears in a sentence.12 For crosslingual pairs, non-English sentences are translated into English using state-of-the-art machine translation.13 The baseline achieves an average correlation of 53.7 with human judgment on tracks 1-5 and would rank 23rd overall out the 44 system submissions that participated in all tracks. 14 e.g., Reimers et al. (2016) report success using STS labels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://transla"
S17-2001,S16-1091,0,0.00933941,"2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 tea"
S17-2001,Q15-1025,0,0.02311,"Missing"
S17-2001,D16-1157,0,0.0328236,"Missing"
S17-2001,P16-2068,0,0.0331584,"Missing"
S17-2001,P17-1190,0,0.0172847,"Missing"
S17-2001,S17-2007,0,0.0512581,"ormalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow and Peskov, 2017) NLPProxem UMDeep (Barrow and Peskov, 2017) Lump (Espa˜n"
S17-2001,S17-2016,0,0.204045,"Missing"
S17-2001,S15-2001,0,0.0578438,"elated but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversati"
S17-2001,Q14-1006,0,0.0320307,"Lin (2016); Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016b,a); He et al. (2015); Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences f"
S17-2001,2006.amta-papers.25,0,0.0602904,"e entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006).4 Participants are not allowed to use the gold quality estimation annotations to inform STS scores. Task Overview STS is the assessment of pairs of sentences according to their degree of semantic similarity. The task involves producing real-valued similarity scores for sentence pairs. Performance is measured by the Pearson correlation of machine scores with human judgments. The ordinal scale in Table 1 guides human annotation, ranging from 0 for no meaning overlap to 5 for meaning equivalence. Intermediate values reflect interpretable levels of partial overlap in meaning. The annotation scale"
S17-2001,S17-2023,0,0.0172414,"Missing"
S17-2001,S17-2018,0,0.0148193,"Missing"
S17-2001,S15-2027,0,0.00981403,"Missing"
S17-2001,S17-2028,0,0.624183,"abels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow a"
S17-2001,S14-2010,1,\N,Missing
S17-2001,S17-2027,0,\N,Missing
S17-2001,W17-4759,0,\N,Missing
W07-1427,W02-1001,0,0.0111527,"Missing"
W07-1427,de-marneffe-etal-2006-generating,1,0.0621763,"Missing"
W07-1427,levy-andrew-2006-tregex,0,0.0313803,"age A core part of an entailment system is the ability to find semantically equivalent patterns in text. Previously, we wrote tedious graph traversal code by hand for each desired pattern. As a remedy, we wrote Semgrex, a pattern language for dependency graphs. We use Semgrex atop the typed dependencies from the Stanford Parser (de Marneffe et al., 2006b), as aligned in the alignment phase, to identify both semantic patterns in a single text and over two aligned pieces of text. The syntax of the language was modeled after tgrep/Tregex, query languages used to find syntactic patterns in trees (Levy and Andrew, 2006). This speeds up the process of graph search and reduces errors that occur in complicated traversal code. 5.1 Semgrex Features Rather than providing regular expression matching of atomic tree labels, as in most tree pattern languages, Semgrex represents nodes as a (nonrecursive) attribute-value matrix. It then uses regular expressions for subsets of attribute values. For example, {word:run;tag:/ˆNN/} refers to any node that has a value run for the attribute word and a tag that starts with NN, while {} refers to any node in the graph. However, the most important part of Semgrex is that it allow"
W07-1427,W07-1431,1,0.380995,"Missing"
W07-1427,N06-1006,1,\N,Missing
W08-0304,P07-2045,0,0.0546783,"h all other candidate translations that have yet to be selected as the 1-best. And, for each of the n n-best lists, this may have to be done up to m − 1 times. 2.2 Search Strategies In this section, we review two search strategies that, in conjunction with the line search just described, can be used to drive MERT. The first, Powell’s method, was advocated by Och (2003) when MERT was first introduced for statistical machine translation. The second, which we call Koehn-coordinate descent (KCD)6 , is used by the MERT utility packaged with the popular Moses statistical machine translation system (Koehn et al., 2007). 6 Moses uses David Chiang’s CMERT package. Within the source file mert.c, the function that implements the overall search strategy, optimize koehn(), is based on Philipp Koehn’s Perl script for MERT optimization that was distributed with Pharaoh. 2.2.1 Powell’s Method 3.1 Powell’s method (Press et al., 2007) attempts to efficiently search the objective by constructing a set of mutually non-interfering search directions. The basic procedure is as follows: (i) A collection of search directions is initialized to be the coordinates of the space being searched; (ii) The objective is minimized by"
W08-0304,N03-1017,0,0.0204375,"hich the results are influenced by some runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly o"
W08-0304,P03-1021,0,0.778611,"Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powell’s method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant"
W08-0304,P02-1038,0,0.0634091,"ariant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant and thus cannot be optimized using gradient techniques, Och (2003) provides an approach that performs such training efficiently. In this paper we explore a number of variations on MERT. Fir"
W08-0304,J03-1002,0,0.00325925,"runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test"
W08-0304,P02-1040,0,0.106415,"of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant and thus cannot be optimized using gradient techniques, Och (2003) provides an approach that performs such training efficiently. In this paper we explore a number of variations on MERT. First, it is shown that performance gains can be had by making use of a stochastic search strategy as compare to that obtained by Powell’s method and Let F be a co"
W08-0304,W05-0908,0,0.0266415,"20 30.191 29.529 29.963 30.674 Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximum loss of adjacent plateaus, right. The none entry for each search strategy represents the baseline where no regularization is used. Statistically significant test set gains, p &lt; 0.01, over the respective baselines are in bold face. of Powell’s method, diagonal search, with coordinate descent’s robustness to the sudden jumps between regions that result from global line minimization. Using an approximate randomization test for statistical significance (Riezler & Maxwell, 2005), and with KCD as a baseline, the gains obtained by stochastic search on MT03 are statistically significant (p = 0.002), as are the gains on MT05 (p = 0.005). Table 4 indicates that performing regularization by either averaging or taking the maximum of adjacent plateaus during the line search leads to gains for both Powell’s method and KCD. However, no reliable additional gains appear to be had when stochastic search is combined with regularization. It may seem surprising that the regularization gains for Powell & KCD are seen not only in the test sets but on the dev set as well. That is, in t"
W08-0304,P06-2101,0,0.284978,"tion to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. One candidate for performing such regularization is the continuous approximation of the MERT objective, O = Epw (`). Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O = `. However, Zens et al. (2007) found that O = Epw (`) achieved substantially better test set performance than O = `, even though it performs slightly worse on the data used to train the parameters. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8 However, we speculate that similar results could be obtained using a uniform distribution over (−1, 1) 31 ature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. However, the most straightforward implementation of such methods requires a loss that can be applied at the sentence level. If the evaluation metric of interest does not have this property (e.g. BLEU), the loss must be approximated using s"
W08-0304,N04-4026,0,0.0132182,"es obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test set while Powell modestly outperforms coordinate descent on the MT05 test set. Moreover, the fact that Powell’s algorithm did not perform better than KCD on the training data10 , and in fact actually performed modestly worse, suggests that"
W08-0304,I05-3027,1,0.564376,"izable number of random restarts should be used in order to minimize the degree to which the results are influenced by some runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powe"
W08-0304,D07-1055,0,0.249878,"be 39.1 while all surrounding plateaus have a BLEU score that is &lt; 10. Intuitively, such a minima would be a very bad solution, as the resulting parameters would likely exhibit very poor generalization to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. One candidate for performing such regularization is the continuous approximation of the MERT objective, O = Epw (`). Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O = `. However, Zens et al. (2007) found that O = Epw (`) achieved substantially better test set performance than O = `, even though it performs slightly worse on the data used to train the parameters. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8 However, we speculate that similar results could be obtained using a uniform distribution over (−1, 1) 31 ature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. However, the most straightforward imp"
W13-2217,N10-2003,1,0.849554,"ity improvements over MERT (Och, 2003) and PRO (Hopkins and May, 2011) for two languages in a research setting. The purpose of our submission to the 2013 Workshop on Statistical Machine Translation (WMT) Shared Task is to compare the algorithm to more established methods in an evaluation. We submitted English-French (En-Fr) and EnglishGerman (En-De) systems, each with over 100k features tuned on 10k sentences. This paper describes the systems and also includes new feature sets and practical extensions to the original algorithm. Translation Model Our machine translation (MT) system is Phrasal (Cer et al., 2010), a phrase-based system based on alignment templates (Och and Ney, 2004). Like many MT systems, Phrasal models the predictive translation distribution p(e|f ; w) directly as p(e|f ; w) = h i 1 exp w&gt; φ(e, f ) Z(f ) Online, Adaptive Tuning Algorithm ⇐⇒ M (d+ ) &gt; M (d− ) Ensuring pairwise agreement is the same as ensuring w · [φ(d+ ) − φ(d− )] &gt; 0. For learning, we need to select derivation pairs (d+ , d− ) to compute difference vectors x+ = φ(d+ ) − φ(d− ). Then we have a 1-class separation problem trying to ensure w · x+ &gt; 0. The derivation pairs are sampled with the algorithm of Hopkins and M"
W13-2217,P10-4002,0,0.0144529,"the phrase table size by excluding less relevant training examples. 150 4.2 Tokenization We tokenized the English (source) data according to the Penn Treebank standard (Marcus et al., 1993) with Stanford CoreNLP. The French data was tokenized with packages from the Stanford French Parser (Green et al., 2013a), which implements a scheme similar to that used in the French Treebank (Abeillé et al., 2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German LM included all of the monol"
W13-2217,D11-1125,0,0.204584,"eal-world translation quality for high-dimensional feature maps and associated weight vectors. That case requires a more scalable tuning algorithm. We describe the Stanford University NLP Group submission to the 2013 Workshop on Statistical Machine Translation Shared Task. We demonstrate the effectiveness of a new adaptive, online tuning algorithm that scales to large feature and tuning sets. For both English-French and English-German, the algorithm produces feature-rich models that improve over a dense baseline and compare favorably to models tuned with established methods. 1 2.1 2 Following Hopkins and May (2011) we cast MT tuning as pairwise ranking. Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Define the linear model score M (d) = w · φ(d). For any derivation d+ that is better than d− under a gold metric G, we desire pairwise agreement such that     G e(d+ ), e1:k &gt; G e(d− ), e1:k Introduction Green et al. (2013b) describe an online, adaptive tuning algorithm for feature-rich translation models. They showed considerable translation quality improvements over MERT (Och, 2003"
W13-2217,P07-2045,0,0.00689866,"at this extension yielded a consistent +0.2 BLEU improvement at test time for both languages. Subsequent experiments on the data sets of Green et al. (2013b) showed that standard BLEU+1 works best for multiple references. Extensions to (Green et al., 2013b) Sentence-Level Metric We previously used the gold metric BLEU+1 (Lin and Och, 2004), which smoothes bigram precisions and above. This metric worked well with multiple references, but we found that it is less effective in a single-reference setting 3 3.1 Feature Sets Dense Features The baseline “dense” model has 19 features: the nine Moses (Koehn et al., 2007) baseline features, a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), the (log) bitext count of each translation rule, and an indicator for unique rules. The final dense feature sets for each language differ slightly. The En-Fr system incorporates a second language model. The En-De system adds a future cost component to the linear distortion model (Green et al., 2010).The future cost estimate allows the distortion limit to be raised without a decrease in translation quality. 149 3.2 Sparse Features Bilingual Sparse features do not necessarily fire on each hypothesis exte"
W13-2217,N06-1014,0,0.0551513,"Stanford CoreNLP. The French data was tokenized with packages from the Stanford French Parser (Green et al., 2013a), which implements a scheme similar to that used in the French Treebank (Abeillé et al., 2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German LM included all of the monolingual data plus the target side of the En-De bitext. We built an analogous model for French. In addition, we estimated a separate French LM from the Gigaword data.1 4.5 French Agreement Correctio"
W13-2217,C04-1072,0,0.0649442,"elize the weight updates. Green et al. (2013b) describe the parallelization technique that is implemented in Phrasal. 2.2 like WMT. To make the metric more robust, Nakov et al. (2012) extended BLEU+1 by smoothing both the unigram precision and the reference length. We found that this extension yielded a consistent +0.2 BLEU improvement at test time for both languages. Subsequent experiments on the data sets of Green et al. (2013b) showed that standard BLEU+1 works best for multiple references. Extensions to (Green et al., 2013b) Sentence-Level Metric We previously used the gold metric BLEU+1 (Lin and Och, 2004), which smoothes bigram precisions and above. This metric worked well with multiple references, but we found that it is less effective in a single-reference setting 3 3.1 Feature Sets Dense Features The baseline “dense” model has 19 features: the nine Moses (Koehn et al., 2007) baseline features, a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), the (log) bitext count of each translation rule, and an indicator for unique rules. The final dense feature sets for each language differ slightly. The En-Fr system incorporates a second language model. The En-De system adds a fu"
W13-2217,P03-1020,0,0.0538024,"Missing"
W13-2217,J93-2004,0,0.0458196,"f the French monolingual data, but sampled a 5M-sentence bitext from the approximately 40M available En-Fr parallel sentences. To select the sentences we first created a “target” corpus by concatenating the tuning and test sets (newstest2008–2013). Then we ran the feature decay algorithm (FDA) (Biçici and Yuret, 2011), which samples sentences that most closely resemble the target corpus. FDA is a principled method for reducing the phrase table size by excluding less relevant training examples. 150 4.2 Tokenization We tokenized the English (source) data according to the Penn Treebank standard (Marcus et al., 1993) with Stanford CoreNLP. The French data was tokenized with packages from the Stanford French Parser (Green et al., 2013a), which implements a scheme similar to that used in the French Treebank (Abeillé et al., 2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Alig"
W13-2217,N09-1046,0,0.0230455,"d method for reducing the phrase table size by excluding less relevant training examples. 150 4.2 Tokenization We tokenized the English (source) data according to the Penn Treebank standard (Marcus et al., 1993) with Stanford CoreNLP. The French data was tokenized with packages from the Stanford French Parser (Green et al., 2013a), which implements a scheme similar to that used in the French Treebank (Abeillé et al., 2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German"
W13-2217,D08-1089,1,0.870502,"nguages. Subsequent experiments on the data sets of Green et al. (2013b) showed that standard BLEU+1 works best for multiple references. Extensions to (Green et al., 2013b) Sentence-Level Metric We previously used the gold metric BLEU+1 (Lin and Och, 2004), which smoothes bigram precisions and above. This metric worked well with multiple references, but we found that it is less effective in a single-reference setting 3 3.1 Feature Sets Dense Features The baseline “dense” model has 19 features: the nine Moses (Koehn et al., 2007) baseline features, a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), the (log) bitext count of each translation rule, and an indicator for unique rules. The final dense feature sets for each language differ slightly. The En-Fr system incorporates a second language model. The En-De system adds a future cost component to the linear distortion model (Green et al., 2010).The future cost estimate allows the distortion limit to be raised without a decrease in translation quality. 149 3.2 Sparse Features Bilingual Sparse features do not necessarily fire on each hypothesis extension. Unlike prior work on sparse MT features, our feature extractors do not filter featur"
W13-2217,N10-1129,1,0.850586,"etric worked well with multiple references, but we found that it is less effective in a single-reference setting 3 3.1 Feature Sets Dense Features The baseline “dense” model has 19 features: the nine Moses (Koehn et al., 2007) baseline features, a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), the (log) bitext count of each translation rule, and an indicator for unique rules. The final dense feature sets for each language differ slightly. The En-Fr system incorporates a second language model. The En-De system adds a future cost component to the linear distortion model (Green et al., 2010).The future cost estimate allows the distortion limit to be raised without a decrease in translation quality. 149 3.2 Sparse Features Bilingual Sparse features do not necessarily fire on each hypothesis extension. Unlike prior work on sparse MT features, our feature extractors do not filter features based on tuning set counts. We instead rely on the regularizer to select informative features. Several of the feature extractors depend on source-side part of speech (POS) sequences and dependency parses. We created those annotations with the Stanford CoreNLP pipeline. Discriminative Phrase Table A"
W13-2217,J13-1009,1,0.880191,"Missing"
W13-2217,P13-1031,1,0.41077,"the algorithm produces feature-rich models that improve over a dense baseline and compare favorably to models tuned with established methods. 1 2.1 2 Following Hopkins and May (2011) we cast MT tuning as pairwise ranking. Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Define the linear model score M (d) = w · φ(d). For any derivation d+ that is better than d− under a gold metric G, we desire pairwise agreement such that     G e(d+ ), e1:k &gt; G e(d− ), e1:k Introduction Green et al. (2013b) describe an online, adaptive tuning algorithm for feature-rich translation models. They showed considerable translation quality improvements over MERT (Och, 2003) and PRO (Hopkins and May, 2011) for two languages in a research setting. The purpose of our submission to the 2013 Workshop on Statistical Machine Translation (WMT) Shared Task is to compare the algorithm to more established methods in an evaluation. We submitted English-French (En-Fr) and EnglishGerman (En-De) systems, each with over 100k features tuned on 10k sentences. This paper describes the systems and also includes new feat"
W13-2217,P13-2121,0,0.0659945,"2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German LM included all of the monolingual data plus the target side of the En-De bitext. We built an analogous model for French. In addition, we estimated a separate French LM from the Gigaword data.1 4.5 French Agreement Correction In French verbs must agree in number and person with their subjects, and adjectives (and some past participles) must agree in number and gender with the nouns they modify. On their own, phrasal alignment an"
W13-2217,W11-2123,0,0.0216746,"compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German LM included all of the monolingual data plus the target side of the En-De bitext. We built an analogous model for French. In addition, we estimated a separate French LM from the Gigaword data.1 4.5 French Agreement Correction In French verbs must agree in number and person with their subjects, and adjectives (and some past participles) must agree in number and gender with the nouns they modify. On their own, phrasal alignment and target side language modeling yield correc"
W13-2217,C12-1121,0,0.123388,"han the conventional method. Whereas we previously stopped the algorithm after four iterations, we now select the model according to held-out accuracy. + where [x]+ = max(x, 0) is the clipping function that in this case sets a weight to 0 when it falls below the threshold ηt−1 λ. Online algorithms are inherently sequential; this algorithm is no exception. If we want to scale the algorithm to large tuning sets, then we need to parallelize the weight updates. Green et al. (2013b) describe the parallelization technique that is implemented in Phrasal. 2.2 like WMT. To make the metric more robust, Nakov et al. (2012) extended BLEU+1 by smoothing both the unigram precision and the reference length. We found that this extension yielded a consistent +0.2 BLEU improvement at test time for both languages. Subsequent experiments on the data sets of Green et al. (2013b) showed that standard BLEU+1 works best for multiple references. Extensions to (Green et al., 2013b) Sentence-Level Metric We previously used the gold metric BLEU+1 (Lin and Och, 2004), which smoothes bigram precisions and above. This metric worked well with multiple references, but we found that it is less effective in a single-reference setting"
W13-2217,J04-4002,0,0.209079,"for two languages in a research setting. The purpose of our submission to the 2013 Workshop on Statistical Machine Translation (WMT) Shared Task is to compare the algorithm to more established methods in an evaluation. We submitted English-French (En-Fr) and EnglishGerman (En-De) systems, each with over 100k features tuned on 10k sentences. This paper describes the systems and also includes new feature sets and practical extensions to the original algorithm. Translation Model Our machine translation (MT) system is Phrasal (Cer et al., 2010), a phrase-based system based on alignment templates (Och and Ney, 2004). Like many MT systems, Phrasal models the predictive translation distribution p(e|f ; w) directly as p(e|f ; w) = h i 1 exp w&gt; φ(e, f ) Z(f ) Online, Adaptive Tuning Algorithm ⇐⇒ M (d+ ) &gt; M (d− ) Ensuring pairwise agreement is the same as ensuring w · [φ(d+ ) − φ(d− )] &gt; 0. For learning, we need to select derivation pairs (d+ , d− ) to compute difference vectors x+ = φ(d+ ) − φ(d− ). Then we have a 1-class separation problem trying to ensure w · x+ &gt; 0. The derivation pairs are sampled with the algorithm of Hopkins and May (2011). Suppose that we sample s pairs for source sentence ft to comp"
W13-2217,P03-1021,0,0.0396715,"ay (2011) we cast MT tuning as pairwise ranking. Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Define the linear model score M (d) = w · φ(d). For any derivation d+ that is better than d− under a gold metric G, we desire pairwise agreement such that     G e(d+ ), e1:k &gt; G e(d− ), e1:k Introduction Green et al. (2013b) describe an online, adaptive tuning algorithm for feature-rich translation models. They showed considerable translation quality improvements over MERT (Och, 2003) and PRO (Hopkins and May, 2011) for two languages in a research setting. The purpose of our submission to the 2013 Workshop on Statistical Machine Translation (WMT) Shared Task is to compare the algorithm to more established methods in an evaluation. We submitted English-French (En-Fr) and EnglishGerman (En-De) systems, each with over 100k features tuned on 10k sentences. This paper describes the systems and also includes new feature sets and practical extensions to the original algorithm. Translation Model Our machine translation (MT) system is Phrasal (Cer et al., 2010), a phrase-based syst"
W13-2217,P06-1055,0,0.0641403,"ction most of the time. For verbs, we find that the inflections are often accurate: number is encoded in the English verb and subject, and 3rd person is generally correct in the absence of a 1st or 2nd person pronoun. However, since English does not generally encode gender, adjective inflection must rely on language modeling, which is often insufficient. To address this problem we apply an automatic inflection correction post-processing step. First, we generate dependency parses of our system’s output using BONSAI (Candito and Crabbé, 2009), a French-specific extension to the Berkeley Parser (Petrov et al., 2006). Based on these dependencies, we match adjectives with the nouns they modify and past participles with their subjects. Then we use Lefff (Sagot, 2010), a machine-readable French lexicon, to determine the gender and number of the noun and to choose the correct inflection for the adjective or participle. Applied to our 3,000 sentence development set, this correction scheme produced 200 corrections with perfect accuracy. It produces a slight (−0.014) drop in BLEU score. This arises from cases where the reference translation uses a synonymous but differently gendered noun, and consequently has di"
W13-2217,sagot-2010-lefff,0,0.0129899,"lly correct in the absence of a 1st or 2nd person pronoun. However, since English does not generally encode gender, adjective inflection must rely on language modeling, which is often insufficient. To address this problem we apply an automatic inflection correction post-processing step. First, we generate dependency parses of our system’s output using BONSAI (Candito and Crabbé, 2009), a French-specific extension to the Berkeley Parser (Petrov et al., 2006). Based on these dependencies, we match adjectives with the nouns they modify and past participles with their subjects. Then we use Lefff (Sagot, 2010), a machine-readable French lexicon, to determine the gender and number of the noun and to choose the correct inflection for the adjective or participle. Applied to our 3,000 sentence development set, this correction scheme produced 200 corrections with perfect accuracy. It produces a slight (−0.014) drop in BLEU score. This arises from cases where the reference translation uses a synonymous but differently gendered noun, and consequently has different adjective inflection. 4.6 German De-compounding Split German compounds must be merged after translation. This process often requires inserting"
W13-2217,W09-3821,0,\N,Missing
W13-2217,D08-1076,0,\N,Missing
W13-2217,W11-2131,0,\N,Missing
W13-2239,P96-1041,0,0.0262624,"16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the Positive Diversity criteria. 7 equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built"
W13-2239,N12-1047,0,0.0167,") (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009),"
W13-2239,D08-1024,0,0.0206523,"orrectness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system"
W13-2239,N10-1141,0,0.110527,"uires the construction of multiple systems that are simultaneously diverse and well-performing. If the systems are not distinct enough, they will bring very little value during system combination. However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination. Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; DeNero et al., 2010; Xiao et al., 2013). However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations. Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems. It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, given the characterist"
W13-2239,W11-2107,0,0.0279539,"erates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system. This mean"
W13-2239,W00-0405,0,0.0118247,"tion with other systems. They constructed chains of systems whereby the output of one decoder is feed as input to the next decoder in the pipeline. The downstream systems are built and tuned to correct errors produced by the preceding system. In this approach, the downstream decoder acts as a machine learning based post editing system. Related Work While the idea of encouraging diversity in individual systems that will be used for system combination has been proven effective in speech recognition and document summarization (Hinton, 2002; Breslin and Gales, 2007; Carbonell and Goldstein, 1998; Goldstein et al., 2000), there has only been a modest amount of prior work exploring such approaches for machine translation. Prior work within machine translation has investigated adapting machine learning techniques for building ensembles of classifiers to translation system tuning, encouraging diversity by varying both the hyperparameters and the data used to build the individual systems, and chaining together individual translation systems. Xiao et al. (2013) explores using boosting to train an ensemble of machine translation systems. Following the standard Adaboost algorithm, each system was trained in sequence"
W13-2239,P13-1031,1,0.839248,"6.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the Positive Diversity criteria. 7 equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built during prior iterations. For clarity of presentation, these diversity scores are reported as 1.0−BLEU."
W13-2239,2009.mtsummit-posters.1,0,0.1431,"(Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] ← tune(systems [i"
W13-2239,W10-1744,0,0.0728713,"are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie,"
W13-2239,2010.amta-papers.34,0,0.519175,"are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie,"
W13-2239,D11-1125,0,0.174735,"ness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their"
W13-2239,N10-2003,1,0.839823,"n be a heterogeneous collection of substantially different systems (e.g., phrase-based, hierarchical, syntactic, or tunable hybrid systems) or even multiple copies of a single machine translation system. In all cases, systems later in the list will be trained to produce translations that both fit the references and are encouraged to be distinct from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4"
W13-2239,D07-1029,0,0.019307,"er combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] ← tune(systems [i], source, PDα,i ()) // Save translations from tuned modeli for use during // the diversity co"
W13-2239,C08-1145,0,0.0181633,"oughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA,"
W13-2239,P08-2021,0,0.016728,"uce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He,"
W13-2239,N03-1017,0,0.00817328,"rd phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the"
W13-2239,2003.mtsummit-papers.32,0,0.0342342,"ranslation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each"
W13-2239,N06-1014,0,0.0127583,"from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using gro"
W13-2239,D07-1105,0,0.616982,"l system combination requires the construction of multiple systems that are simultaneously diverse and well-performing. If the systems are not distinct enough, they will bring very little value during system combination. However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination. Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; DeNero et al., 2010; Xiao et al., 2013). However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations. Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems. It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, g"
W13-2239,E06-1005,0,0.0255359,"ould not only obtain good translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the in"
W13-2239,P04-1063,0,0.028282,"s results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g."
W13-2239,P03-1021,0,0.0583526,"ing parameter values Θ that produce translations sysΘ that in turn achieve a high score on some correctness measure: arg max Correctness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Fo"
W13-2239,P02-1040,0,0.106954,"when combining a good system with poor performing systems even if the systems col3 System Combination Tuning Individual Translation Systems Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references. As shown in equation (1), this can be written as finding parameter values Θ that produce translations sysΘ that in turn achieve a high score on some correctness measure: arg max Correctness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization obje"
W13-2239,N07-1029,0,0.0983508,"ood translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system tran"
W13-2239,P07-1040,0,0.021501,"ood translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system tran"
W13-2239,W09-0441,0,0.0194723,"bination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine t"
W13-2239,P13-1081,0,0.0124211,"translations that both fit the references and are encouraged to be distinct from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics w"
W13-2239,N09-2052,0,0.01801,"et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system. This means that little or no gains will typically be seen when combining a good system with poor performing systems even if the systems col3 System Combination Tuning Individual Translation Systems Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references"
W13-2239,D08-1089,1,\N,Missing
W13-2239,2005.iwslt-1.5,0,\N,Missing
W13-2239,N12-1023,0,\N,Missing
W14-3311,W13-2205,0,0.0953026,"Missing"
W14-3311,P13-2121,0,0.0527971,"Missing"
W14-3311,W11-2123,0,0.0545202,"Missing"
W14-3311,D11-1125,0,0.0352263,"elihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. Invoked by prefixing the LM path with the “kenlm:”. 115 2 For simplicity, we assume one reference, but the multireference case is analogous. Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Wei"
W14-3311,P07-1019,0,0.0616148,"), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (Huang and Chiang, 2007). The standard multi-stack beam search (Och and Ney, 2004) is also an option. Either procedure can be configured in one of several recombination modes. The “Pharaoh” mode only considers linear distortion, source coverage, and target LM history. The “Exact” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics availabl"
W14-3311,N03-1017,0,0.0105437,"cience Department, Stanford University {spenceg,danielcer,manning}@stanford.edu Abstract We present a new version of Phrasal, an open-source toolkit for statistical phrasebased machine translation. This revision includes features that support emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. T"
W14-3311,P07-2045,0,0.0234988,"emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems"
W14-3311,W08-0304,1,0.816084,"specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. Invoked by prefixing the LM path with the “kenlm:”. 115 2 For simplicity, we assume one reference, but the multireference case is analogous. Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are performed after each tuning example is decoded, and"
W14-3311,W04-3250,0,0.0231682,"act” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics available for tuning can also be invoked for evaluation. For significance testing, the toolkit includes an implementation of the permutation test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r∈R d : w(d) r : s(r,w) d0 : s(d0 ,w) axiom r∈ / cov(d) |cov(d) |= |s| item goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d0 is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM estimated from cased, tokenized text. A subsequent detokenization step is thus necessary. A more conv"
W14-3311,N10-2003,1,0.842411,"ase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems. Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext. Third, Phrasal supplies the key ingredients for web-based, interactive MT: an asynchronous RESTful JSON web service implemented as a J2EE servlet, integrated pre- and post"
W14-3311,N12-1047,0,0.182935,"includes native support for the class-based language models that have become popular in recent evaluations (Wuebker et al., 2012; Ammar et al., 2013; Durrani et al., 2013). 2.2 Tuning Once a language model has been estimated and a phrase table has been extracted, the next step is to estimate model weights. Phrasal supports tuning over n-best lists, which permits rapid experimentation with different error metrics and loss functions. Lattice-based tuning, while in principle more powerful, requires metrics and losses that factor over lattices, and in practice works no better than n-best tuning (Cherry and Foster, 2012). Tuning requires a parallel set {(ft , et )}Tt=1 of source sentences ft and target references et .2 Phrasal follows the log-linear approach to phrasebased translation (Och and Ney, 2004) in which the predictive translation distribution p(e|f ; w) is modeled directly as p(e|f ; w) = Rule Extraction The next step in the pipeline is extraction of a phrase table. Phrasal includes a multi-threaded version of the rule extraction algorithm of Och and Ney (2004). Phrase tables can be filtered to a specific data set—as is common in research environments. When filtering, the rule extractor lowers memor"
W14-3311,W11-2107,0,0.0199706,"a reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. Invoked by prefixing the LM path with the"
W14-3311,W13-2212,0,0.0302619,"e. Static rule features are useful in two cases. First, if a feature value depends on bitext statistics, which are not accessible during tuning or decoding, then that feature should be stored in the phrase table. Examples are the standard phrase translation probabilities, and the dense rule count and rule uniqueness indicators described by Green et al. (2013). Second, if a feature depends only on the rule and is unlikely to change, then it may be more efficient to store that feature value in the phrase table. An example is a feature template that indicates inclusion in a specific data domain (Durrani et al., 2013). Rule extractor feature templates must implement the FeatureExtractor interface and are loaded via reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BL"
W14-3311,P10-4002,0,0.0153594,"ing time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems. Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext. Third, Phrasal supplies the key ingredients for web-based, interactive MT: an asynchronous RESTful JSON we"
W14-3311,D08-1089,1,0.768952,"Missing"
W14-3311,N06-1014,0,0.0536844,"p://nlp.stanford.edu/software/phrasal/ 2 Standard System Pipeline This section describes the steps required to build a phrase-based MT system from raw text. Each step is implemented as a stand-alone executable. For convenience, the Phrasal distribution includes a script that coordinates the steps. 2.1 Prerequisites Phrasal assumes offline preparation of word alignments and at least one target-side language model. Word Alignment The rule extractor can accommodate either unsymmetrized or symmetrized alignments. Unsymmetrized alignments can be produced with either GIZA++ or the Berkeley Aligner (Liang et al., 2006). Phrasal then applies symmetrization on-the-fly using heuristics such as grow-diag or grow-diag-final. If the alignments are symmetrized separately, then Phrasal accepts align114 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 114–121, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics ments in the i-j Pharaoh format, which indicates that source token i is aligned to target token j. The rule extractor can also create lexicalized reordering tables. The standard phrase orientation model (Tillmann, 2004) and the hierarchical mo"
W14-3311,C04-1072,0,0.0374434,"and n-best lists are not accumulated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2 , efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (H"
W14-3311,P03-1020,0,0.0652213,"ion test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r∈R d : w(d) r : s(r,w) d0 : s(d0 ,w) axiom r∈ / cov(d) |cov(d) |= |s| item goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d0 is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM estimated from cased, tokenized text. A subsequent detokenization step is thus necessary. A more convenient alternative is the CRF-based postprocessor that can be trained to invert an arbitrary pre-processor. This post-processor can perform truecasing and detokenization in one pass. 3 Feature API Phrasal supports dynamic feature extraction during tuning and decoding. In the API, feature templates are called featurizers. There are two types with associated interfaces: RuleFeaturizer and DerivationFeaturizer. One way to illustrate these two featurizers is to consider phrasebased"
W14-3311,C12-1121,0,0.0776569,"ated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2 , efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (Huang and Chiang, 2007). The stand"
W14-3311,W14-3316,1,0.779167,"core code has fewer dependencies in terms of software and expertise. Second, Phrasal makes extensive use of Java interfaces and reflection. This is especially helpful in the feature API. A feature function can be added to the system by simply implementing an interface and specifying the class name on the decoder command line. There is no need to modify or recompile anything other than the new feature function. This paper presents a direct comparison of Phrasal and Moses that shows favorable results in terms of decoding speed and tuning time. An indirect comparison via the WMT2014 shared task (Neidert et al., 2014) showed that Phrasal compares favorably to Moses in an evaluation setting. The source code is freely available at: http://nlp.stanford.edu/software/phrasal/ 2 Standard System Pipeline This section describes the steps required to build a phrase-based MT system from raw text. Each step is implemented as a stand-alone executable. For convenience, the Phrasal distribution includes a script that coordinates the steps. 2.1 Prerequisites Phrasal assumes offline preparation of word alignments and at least one target-side language model. Word Alignment The rule extractor can accommodate either unsymmet"
W14-3311,J04-4002,0,0.927809,"anguage model has been estimated and a phrase table has been extracted, the next step is to estimate model weights. Phrasal supports tuning over n-best lists, which permits rapid experimentation with different error metrics and loss functions. Lattice-based tuning, while in principle more powerful, requires metrics and losses that factor over lattices, and in practice works no better than n-best tuning (Cherry and Foster, 2012). Tuning requires a parallel set {(ft , et )}Tt=1 of source sentences ft and target references et .2 Phrasal follows the log-linear approach to phrasebased translation (Och and Ney, 2004) in which the predictive translation distribution p(e|f ; w) is modeled directly as p(e|f ; w) = Rule Extraction The next step in the pipeline is extraction of a phrase table. Phrasal includes a multi-threaded version of the rule extraction algorithm of Och and Ney (2004). Phrase tables can be filtered to a specific data set—as is common in research environments. When filtering, the rule extractor lowers memory utilization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature"
W14-3311,P03-1021,0,0.0122406,"n to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. Invoked by prefixing the LM path with the “kenlm:”. 115 2 For simplicity, we assume one reference, but the multireference case is analogous. Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT"
W14-3311,P02-1040,0,0.0957625,"le extractor feature templates must implement the FeatureExtractor interface and are loaded via reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batc"
W14-3311,P13-1031,1,0.894343,"ization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature API. This allows for storage of static rule feature values in the phrase table. Static rule features are useful in two cases. First, if a feature value depends on bitext statistics, which are not accessible during tuning or decoding, then that feature should be stored in the phrase table. Examples are the standard phrase translation probabilities, and the dense rule count and rule uniqueness indicators described by Green et al. (2013). Second, if a feature depends only on the rule and is unlikely to change, then it may be more efficient to store that feature value in the phrase table. An example is a feature template that indicates inclusion in a specific data domain (Durrani et al., 2013). Rule extractor feature templates must implement the FeatureExtractor interface and are loaded via reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is no"
W14-3311,W05-0908,0,0.0676322,"ion modes. The “Pharaoh” mode only considers linear distortion, source coverage, and target LM history. The “Exact” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics available for tuning can also be invoked for evaluation. For significance testing, the toolkit includes an implementation of the permutation test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r∈R d : w(d) r : s(r,w) d0 : s(d0 ,w) axiom r∈ / cov(d) |cov(d) |= |s| item goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d0 is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM esti"
W14-3311,W14-3360,1,0.824384,"source of inefficiency. Second, we observe that the Java parallel garbage collector (GC) runs up to seven threads, which become increasingly active as the number of decoder threads increases. These and other Java overhead threads must be scheduled, limiting gains as the number of decoding threads approaches the number of physical cores. Finally, Figure 3 shows tuning BLEU as a function of wallclock time. For Moses we chose the batch MIRA implementation of Cherry and Foster (2012), which is popular for tuning feature-rich systems. Phrasal uses the online tuner with the expected BLEU objective (Green et al., 2014). Moses achieves a maximum BLEU score of 47.63 after 143 minutes of tuning, while Phrasal reaches this level after just 17 minutes, later reaching a maximum BLEU of 47.75 after 42 minutes. Much of the speedup can be attributed to phrase table and LM loading time: the Phrasal tuner loads these data structures just once, while the Moses tuner loads them every epoch. Of course, this loading time becomes more significant with larger-scale systems. 6 8 ● Conclusion We presented a revised version of Phrasal, an opensource, phrase-based MT toolkit. The revisions support new directions in MT research"
W14-3311,2006.amta-papers.25,0,0.0277386,"actor interface and are loaded via reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. In"
W14-3311,N04-4026,0,0.03882,"Missing"
W14-3311,D08-1076,0,\N,Missing
W14-3311,W13-2226,0,\N,Missing
W14-3360,W13-2205,0,0.314914,"from frequency filtering on the tuning data; see section 6.1). The feature is local. Class-based rule indicator Word classes abstract over lexical items. For each rule r, a prototype that abstracts over many rules can be built by concatenating {ϕ(w) : w ∈ f (r)} with {ϕ(w) : w ∈ e(r)}. For example, suppose that Arabic class 492 consists primarily of Arabic present tense verbs and class 59 contains English auxiliaries. Then the model might penalize a rule prototype like 492&gt;59_59, which drops the verb. This template fires an indicator for each rule prototype and is local. Target unigram class (Ammar et al., 2013) Target lexical items with similar syntactic and semantic properties may have very different frequencies in the training data. These frequencies will influence the dense features. For example, in one of our English class mappings the following words map to the same class: word surface-to-surface air-to-air ground-to-air class 0 0 0 freq. 269 98 63 another LM. This template fires a separate indicator for each class {ϕ(w) : w ∈ e(r)} and is local. 3.2 Word Alignments Word alignment features allow the model to recognize fine-grained phrase-internal information that is largely opaque in the dense"
W14-3360,W11-2131,0,0.037778,"sted defaults: Brown, default; Clark, 10 iterations, frequency cutoff τ = 5; Och, 10 iterations. Our implementation: PredictiveFull, 30 iterations, τ = 0; Predictive, 30 iterations, τ = 5. labels. The in-domain rule sets need not be disjoint since some rules might be useful across domains. This paper explores the following approach: we choose one of the M domains as the default. Next, we collect some source sentences for each of the M − 1 remaining domains. Using these examples we then identify in-domain sentence pairs in the bitext via data selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable an"
W14-3360,W08-0336,1,0.807457,"hm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7 System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8 Other learning settings: 16 threads, mini-batch size of 20; L1 regularization strength λ = 0.001; learning rate η0 = 0.02; initialization of LM to 0.5, word penalty to"
W14-3360,N12-1047,0,0.245585,"e now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the s"
W14-3360,W13-2212,0,0.162749,"s, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input domain. This template fires a generic in-domain indicator and a domain-specific indicator (e.g., the features might be indomain and indomain-nw). The feature is local. Adjacent Rule Indicator Indicators for adjacent in-domain rules. This template also fires both generic and domain-specific features. The feature is non-local and the state is a boolean indicating if the last rule in a partial derivation is in-domain. 5 Experiments We evaluate and analyze our feature set under a variety of large-scale experimental conditions including multiple do"
W14-3360,N09-1068,1,0.910726,"Missing"
W14-3360,N13-1048,0,0.187988,"meter or an index indicating a categorical value like an n-gram context. For each language, the extended feature templates require unigram counts and a word-to-class mapping ϕ : w 7→ c for word w ∈ V and class c ∈ C. These can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2 Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not 1 http://nlp.stanford.edu/software/phrasal the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ. @ ⇒ reasons -0.022 H. AJ. @ ⇒ reasons for 0.002 H. AJ. @ ⇒ the reasons for 0.016 These translations are all correct depending on con text. When the plural nou"
W14-3360,N12-1023,0,0.17572,"ve algorithm design for machine translation (MT) has lately been a booming enterprise. There are now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that"
W14-3360,W13-2217,1,0.90216,"Missing"
W14-3360,P13-1031,1,0.939443,"re a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit and can be extracted effi"
W14-3360,W14-3311,1,0.825114,"orpora. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). training corpora5 come from several Linguistic Data Consortium (LDC) sources from 2012 and earlier (Table 2). The test, development, and tuning corpora6 come from the NIST OpenMT and MetricsMATR evaluations (Table 3). Extended features benefit from more tuning data, so we concatenated five NIST data sets to build one large tuning set. Observe that all test data come from later epochs than the tuning and development data. From these data we built phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results W"
W14-3360,N13-1003,0,0.534621,"nslation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit an"
W14-3360,N09-1025,0,0.0514297,"ptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS)"
W14-3360,P11-2080,0,0.0123227,"small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM topic information. 8 Conclusion This paper makes four major contributions. First, we introduced extended features for phrase-based MT that exceeded both dense and feature-rich baselines. Second, we specialized the features to source domains, further extending the gains. Third, we showed that online expected BLEU is faster and more stable than online PRO for extended fe"
W14-3360,D10-1056,0,0.060428,"Missing"
W14-3360,2012.amta-papers.4,0,0.0472446,"es. Yu et al. (2013) used word classes as backoff features to reduce overfitting. Wuebker et al. (2013) replaced all lexical items in the bitext and monolingual data with classes, and estimated the dense feature set. 11 Space limitations preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classif"
W14-3360,P07-1033,0,0.108877,"Missing"
W14-3360,N12-1017,0,0.0217904,"●● ● ● ● ●●● ●● ● ● ● ●● ● ● ●●● ● ● ● ●● ●●● ● ● ● ●● ● ● ●●●●●● ● ● ● ● ●●● ● ● ● ● ●●●● ●● ● ● ● ● ●● ●● ● ● ● ●● ● ● ● ●● ● ●● ● ● ● ●● ● ●● ●●● ● ● ● ● ●● ● ● ●● ● ● ●●● ● ● ●● ●● ● ● ●● ●● ● ●● ● ●● ● ● ●● ● ●● ● ●● ● ● ●● ●●●● ●●● ●● ● ● ● ● ● ●●● ●●●● ● ● ●●● ●● ● ● ●●● ● ●● ●●● ● ●● ●● ● ●● ● ●● ● ● ● ●● ●● ● ● ● ●● ● ● ● ● ● ● ● ●● ● ● ●● References Few MT data sets supply multiple references. Even when they do, those references are but a sample from a larger pool of possible translations. This observation has motivated attempts at generating lattices of translations for evaluation (Dreyer and Marcu, 2012; Bojar et al., 2013). But evaluation is only part of the problem. Table 6c shows that the Dense model, which has only a few features to describe the data, is little affected by the elimination of references. In contrast, the feature-rich model degrades significantly. This may account for the underperformance of features in single-reference settings like WMT (Durrani et al., 2013; Green et al., 2013a). The next section explores the impact of references further. Reference Variance We took the Dense Ar-En output for the dev data, which has four references, and computed the sentence-level BLEU+1"
W14-3360,2012.iwslt-papers.17,0,0.0218206,"tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM topic information. 8 Conclusion This paper makes four major contributions. First, we introduced extended features for phrase-based MT that exceeded both dense and feature-rich baselines. Second, we specialized the features to source domains, further extending the gains. Third, we showed that online expected BLEU is faster and more stable than online PRO for extended features. Finally, we released fast, scalable, languageindependent tools for implementing the feature set"
W14-3360,P13-2121,0,0.147076,"ed five NIST data sets to build one large tuning set. Observe that all test data come from later epochs than the tuning and development data. From these data we built phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and t"
W14-3360,D11-1125,0,0.626613,"ng, and release fast, scalable, and language-independent tools for implementing the features. 1 Introduction Scalable discriminative algorithm design for machine translation (MT) has lately been a booming enterprise. There are now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this p"
W14-3360,Q13-1035,0,0.0707732,"ta selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input domain. This template fires a generic in-domain indicator and"
W14-3360,N03-1017,0,0.0213201,"ther a real-valued parameter or an index indicating a categorical value like an n-gram context. For each language, the extended feature templates require unigram counts and a word-to-class mapping ϕ : w 7→ c for word w ∈ V and class c ∈ C. These can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2 Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not 1 http://nlp.stanford.edu/software/phrasal the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ. @ ⇒ reasons -0.022 H. AJ. @ ⇒ reasons for 0.002 H. AJ. @ ⇒ the reasons for 0.016 These translations are all correct depending on con text"
W14-3360,P07-2045,0,0.0160733,"atch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit and can be extracted efficiently during decoding. Because all feature weights are tuned on the development set, the new feature templates are amenable to feature augmentation (Daumé III, 2007), a simple domain adaptation technique that we show works surprisingly well for MT. Extended features are designed according to a principle rather than a rule: they should fire less than standard dense features, which are general, but more than so-called sparse f"
W14-3360,P06-1096,0,0.491875,"e can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2 Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not 1 http://nlp.stanford.edu/software/phrasal the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ. @ ⇒ reasons -0.022 H. AJ. @ ⇒ reasons for 0.002 H. AJ. @ ⇒ the reasons for 0.016 These translations are all correct depending on con text. When the plural noun H AJ  . . @ ‘reasons’ appears in a construct state (iDafa) the preposition for is unrealized. Moreover, depending on the context, the English translation might also require the determiner the, which is also unrealized."
W14-3360,N06-1014,0,0.490338,"e can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2 Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not 1 http://nlp.stanford.edu/software/phrasal the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ. @ ⇒ reasons -0.022 H. AJ. @ ⇒ reasons for 0.002 H. AJ. @ ⇒ the reasons for 0.016 These translations are all correct depending on con text. When the plural noun H AJ  . . @ ‘reasons’ appears in a construct state (iDafa) the preposition for is unrealized. Moreover, depending on the context, the English translation might also require the determiner the, which is also unrealized."
W14-3360,W05-0908,0,0.354367,"Missing"
W14-3360,C04-1072,0,0.106017,"online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making it slow to compute. To address these shortcomings, we explore an online variant of expected error (Och, 2003, Eq.7). Let Et = {ei }ni=1 be a scored n-best list of translations at time step t for source input ft . Let G(e) be a gold error metric that evaluates each candidate translation with respect to a set of one or more (3) To our knowledge, we are the first to experiment with the online version of this loss.2 When G(e) is sentence-level BLEU+1 (Lin and Och, 2004)—the setting in our experiments—this loss is also known as expected BLEU (Cherry and Foster, 2012). However, other metrics are possible. 3 Extended Phrase-based Features We divide our feature templates into five categories, which are well-known sources of error in phrasebased translation. The features are defined over derivations d = {ri }D i=1 , which are ordered sequences of rules r from the translation model. Define functions f (·) to be the source string of a rule or derivation and e(·) to be the target string. Local features can be extracted from individual rules and do not declare any st"
W14-3360,D12-1037,0,0.0333627,"ns preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM"
W14-3360,maamouri-etal-2008-enhancing,0,0.0189658,"lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7 System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8 Other learning settings: 16 threads, mini-batch size of 20; L1 regularization strength λ = 0.0"
W14-3360,J93-2004,0,0.04582,"lingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7 System settings: distortion limit of 5, cube pruning bea"
W14-3360,P14-2034,1,0.81695,"For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7 System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8 Other learning settings"
W14-3360,C12-1121,0,0.261662,"vity penalty, the maximum lowerbounds the multiple-reference score since BLEU aggregates n-grams across references. The multiplereference score is an “easier” target since the model has more opportunities to match n-grams. Consider again the single-reference condition and one of the pathological cases at the top of Figure 1a. Suppose that the low-scoring reference is observed in the single-reference condition. The more expressive feature-rich model has a greater capacity to fit that reference when, under another reference, it would have matched the translation exactly and incurred a low loss. Nakov et al. (2012) suggested extensions to BLEU+1 that were subsequently found to improve accuracy in the single-reference condition (Gimpel and Smith, 2012a). Repeating the min/max calculations with the most effective extensions (according to Gimpel and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended feature"
W14-3360,P12-1048,0,0.0140448,"Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM to"
W14-3360,P08-1086,0,0.259778,"Missing"
W14-3360,2012.amta-papers.18,0,0.194544,"hen identify in-domain sentence pairs in the bitext via data selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input do"
W14-3360,D07-1080,0,0.0741595,"s a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features ar"
W14-3360,P13-2003,0,0.0431401,"learn w, we follow the online procedure of Green et al. (2013b), who calculate gradient steps with AdaGrad (Duchi et al., 2011) and perform feature selection via L1 regularization in the FOBOS (Duchi and Singer, 2009) framework. This procedure accommodates any loss function for which a subgradient can be computed. Green et al. (2013b) used a PRO objective (Hopkins and May, 2011) with a logistic (surrogate) loss function. However, later results showed overfitting (Green et al., 2013a), and we found that their online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making it slow to compute. To address these shortcomings, we explore an online variant of expected error (Och, 2003, Eq.7). Let Et = {ei }ni=1 be a scored n-best list of translations at time step t for source input ft . Let G(e) be a gold error metric that evaluates each candidate translation with respect to a set of one or more (3) To our knowledge, we are the first to experiment with the online version of this loss.2 When G(e) is sentence-level BLEU+1 (Lin and Och, 2004)—the setting in our experiments—this loss is also known as expected BLEU (Cherry and Fos"
W14-3360,J03-1002,0,0.00552048,"y if alignment 1 is a common alignment error. Lexicalized alignment features allow the model to compensate for these events. This feature fires an indicator for each alignment in a rule—including multiword cliques—and is local. Class-based alignments Like the class-based rule indicator, this feature template replaces each lexical item with its word class, resulting in an alignment prototype. This feature fires an indicator for each alignment in a rule after mapping lexical items to classes. It is local. Source class deletion Phrase extraction algorithms often use a “grow” symmetrization step (Och and Ney, 2003) to add alignment points. Sometimes this procedure can produce a rule that deletes important source content words. This feature template allows the model to penalize these rules by firing an indicator for the class of each unaligned source word. The feature is local. Punctuation ratio Languages use different types and ratios of punctuation (Salton, 1958). For example, quotation marks are not commonly used in Arabic, but they are conventional in English. Furthermore, spurious alignments often contain punctuation. To control these two phenomena, this feature template returns the ratio of target"
W14-3360,D13-1138,0,0.112687,"conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS) tags to reduce sparsity. This is a natural idea that lay dormant until recently. Ammar et al. (2013) incorporated unigram and bigram target class features. Yu et al. (2013) used word classes as backoff features to reduce overfitting. Wuebker et al. (2013) replaced all lexical items in the bitext and monolingual data with classes, and estimated the dense feature set. 11 Space limitations preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels."
W14-3360,J04-4002,0,0.109594,"show that an online variant of expected error (Och, 2003) is significantly faster to compute, less prone to overfitting, and nearly as effective as a pairwise loss. We release all software—feature extractors, and fast word clustering and data selection packages—used in our experiments.1 2 references. The smooth loss function is `t (wt−1 ) = Ep(e|ft ;wt−1 ) [G(e)]   1 X = exp w&gt; φ(e0 , f ) · G(e0 ) Z 0 e ∈Et with P normalization  constant Z = exp w&gt; φ(e0 , f ) . The gradient gt for coordinate j is: Phrase-based Models and Learning e0 ∈Et The log-linear approach to phrase-based translation (Och and Ney, 2004) directly models the predictive translation distribution p(e|f ; w) = 1 Z(f ) h i exp w&gt; φ(e, f ) (2) gt = E[G(e)φj (e, ft )]− E[G(e)]E[φj (e, ft )] (1) where e is the target string, f is the source string, w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. Assume that there is also a function ρ(e, f ) ∈ Rd that produces a recombination map for the features. That is, each coordinate in ρ represents the state of the corresponding coordinate in φ. For example, suppose that φj is the log probability produced by the n-gram langua"
W14-3360,P03-1021,0,0.230152,"ecessary and even detrimental when features follow this principle. We report large-scale translation quality experiments relative to both dense and feature-rich baselines. Our best feature set, which includes domain adaptation features, yields an average +1.05 BLEU improvement for Arabic-English and +0.67 for 466 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 466–476, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Chinese-English. In addition to the extended feature set, we show that an online variant of expected error (Och, 2003) is significantly faster to compute, less prone to overfitting, and nearly as effective as a pairwise loss. We release all software—feature extractors, and fast word clustering and data selection packages—used in our experiments.1 2 references. The smooth loss function is `t (wt−1 ) = Ep(e|ft ;wt−1 ) [G(e)]   1 X = exp w&gt; φ(e0 , f ) · G(e0 ) Z 0 e ∈Et with P normalization  constant Z = exp w&gt; φ(e0 , f ) . The gradient gt for coordinate j is: Phrase-based Models and Learning e0 ∈Et The log-linear approach to phrase-based translation (Och and Ney, 2004) directly models the predictive translat"
W14-3360,P02-1040,0,0.09414,"phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568"
W14-3360,D13-1112,0,0.0509147,"work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS) tags to reduce sparsity. This is a natural idea that lay dormant until recently. Ammar et al. (2013) incor"
W14-3360,D08-1076,0,\N,Missing
W18-3022,S14-2010,1,0.773955,"Missing"
W18-3022,S16-1081,1,0.899116,"Missing"
W18-3022,S12-1051,1,0.819296,"Missing"
W18-3022,S13-1004,1,0.833795,"Missing"
W18-3022,S12-1059,0,0.0608901,"Missing"
W18-3022,D15-1075,0,0.0783501,"gressively more distant context in the higher layers. Sentence Embedding ∑ Attn. Layern Fully Connected Layer0 Attn. Layern 2.4 ∑ We anticipate that learning good semantic representations may benefit from the inclusion of multiple distinct tasks during training. Multiple tasks should improve the coverage of semantic phenomenon that are critical to one task but less essential to another. We explore multitask models that use a shared encoder for learning conversational response prediction and natural language inference (NLI). The NLI data are from the Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) corpus. The sentences are mostly non-conversational, providing a complementary learning signal. Figure 5 illustrates the multitask model with SNLI. We keep the input-response model the same, and build another two encoders for SNLI pairs, sharing parameters with the input-response encoders. Following Conneau et al. (2017), we encode a sentence pair into vectors u1 , u2 and construct a feature vector (u1 , u2 , |u1 − u2 |, u1 ∗ u2 ). The feature vector is fed into a 3-way classifier consisting of a feedforward network culminating in a softmax layer. Following prior work, we use a single 512 uni"
W18-3022,S12-1061,0,0.0611699,"Missing"
W18-3022,S17-2001,1,0.941795,"engineered and mixed systems for both tasks. 1 Figure 1: Sentences have similar meanings if they can be answered by a similar distribution of conversational responses. et al., 2016). The internal representations of such models resolve the semantics necessary to predict the correct response across a broad selection of input messages. Meaning similarity between sentences then can be obtained by comparing the sentence-level representations learned by such models. We follow this approach, and assess the quality of the resulting similarity scores on the Semantic Textual Similarity (STS) Benchmark (Cer et al., 2017) and a question similarity subtask from SemEval 2017’s Community Question Answering (CQA) evaluation. The STS benchmark scores sentence pairs based on their degree of meaning similarity. The Community Question Answering (CQA) subtask B (Nakov et al., 2017) ranks questions based on their similarity with a target question. We first assess representations learned from unsupervised conversational input-response pairs. We then explore augmenting our model with multi-task training over a combination of unsupervised conversational response prediction and supervised training on Natural Language Infere"
W18-3022,S17-2051,0,0.0284351,"considered as good questions, which should rank above “Irrelevant” ones. Similar to the STS experiments, we use cosine similarity between the original question and related questions, without considering any other interaction between the two questions.11 Given a related question Qi and its original question Q, we first encode them into vectors ui and u. Then the related questions are ranked based on the cosine similarity with respect to the original question, Figure 7: Predicted semantic similarity scores vs. ground truth on the STS Benchmark. cos(ui , u). Results are shown in table 6. SimBow (Charlet and Damnati, 2017) and KeLP (Filice et al., 2017), which are the best systems on the 2017 task, are used as baselines.12 Even without tuning on the training data provided by the task, our models show competitive performance. Reddit+SNLI outperforms SimBow-primary, which official ranked first during the 2017 shared task. 12 In the competition, each team can submit one primary run and two contrastive runs. Only the primary run is used for the official ranking. 11 Our model also excludes the use of comments and user profiles provided by CQA as optional contextual features. 170 against the ground truth labels withi"
W18-3022,D17-1070,0,0.382357,"chmark scores sentence pairs based on their degree of meaning similarity. The Community Question Answering (CQA) subtask B (Nakov et al., 2017) ranks questions based on their similarity with a target question. We first assess representations learned from unsupervised conversational input-response pairs. We then explore augmenting our model with multi-task training over a combination of unsupervised conversational response prediction and supervised training on Natural Language Inference (NLI) data, as training to NLI has been shown to independently yield useful general purpose representations (Conneau et al., 2017). Unsupervised training over conversational data yields represenIntroduction We propose a novel approach to sentence-level semantic similarity based on unsupervised learning from conversational data. We observe that semantically similar sentences have a similar distribution of potential conversational responses, and that a model trained to predict conversational responses should implicitly learn useful semantic representations. As illustrated in Figure 1, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years o"
W18-3022,P15-1094,0,0.0211459,"4 CQA Subtask B To further validate the effectiveness of sentence representations learned from conversational data, we assess the proposed models on subtask B of SemEval Community Question Answering (CQA) (Nakov et al., 2017). In this task, given an “original” question Q, and the top ten related questions from a forum (Q1 , . . . , Q10 ) as retrieved by a search engine, the goal is to rank the related questions according to their similarity with respect 8 InferSent (Conneau et al., 2017), Sent2Vec (Pagliardini et al., 2017), SIF (Arora et al., 2017), PV-DBOW (Lau and Baldwin, 2016), C-PHRASE (Kruszewski et al., 2015), ECNU (Tian et al., 2017) and BIT (Wu et al., 2017). 9 For both the STS shared task and the STS benchmark leaderboard, systems are allowed to use external datasets as long as they do not make use of supervised annotations on data that overlap with the evaluation sets. InferSent introduced the use of SNLI for STS. However, we discovered 4 out of the 1,379 pairs within the STS Benchmark dev set and 5 out of the 1,500 pairs in the STS Benchmark test set overlap with the SNLI training set. We do not believe this minimal overlap had a meaningful impact on the results presented here. 10 As summariz"
W18-3022,S17-2053,0,0.0237768,"hould rank above “Irrelevant” ones. Similar to the STS experiments, we use cosine similarity between the original question and related questions, without considering any other interaction between the two questions.11 Given a related question Qi and its original question Q, we first encode them into vectors ui and u. Then the related questions are ranked based on the cosine similarity with respect to the original question, Figure 7: Predicted semantic similarity scores vs. ground truth on the STS Benchmark. cos(ui , u). Results are shown in table 6. SimBow (Charlet and Damnati, 2017) and KeLP (Filice et al., 2017), which are the best systems on the 2017 task, are used as baselines.12 Even without tuning on the training data provided by the task, our models show competitive performance. Reddit+SNLI outperforms SimBow-primary, which official ranked first during the 2017 shared task. 12 In the competition, each team can submit one primary run and two contrastive runs. Only the primary run is used for the official ranking. 11 Our model also excludes the use of comments and user profiles provided by CQA as optional contextual features. 170 against the ground truth labels within the STS Benchmark test data."
W18-3022,W16-1609,0,0.0136077,"the state-of-the-art neural STS 4.4 CQA Subtask B To further validate the effectiveness of sentence representations learned from conversational data, we assess the proposed models on subtask B of SemEval Community Question Answering (CQA) (Nakov et al., 2017). In this task, given an “original” question Q, and the top ten related questions from a forum (Q1 , . . . , Q10 ) as retrieved by a search engine, the goal is to rank the related questions according to their similarity with respect 8 InferSent (Conneau et al., 2017), Sent2Vec (Pagliardini et al., 2017), SIF (Arora et al., 2017), PV-DBOW (Lau and Baldwin, 2016), C-PHRASE (Kruszewski et al., 2015), ECNU (Tian et al., 2017) and BIT (Wu et al., 2017). 9 For both the STS shared task and the STS benchmark leaderboard, systems are allowed to use external datasets as long as they do not make use of supervised annotations on data that overlap with the evaluation sets. InferSent introduced the use of SNLI for STS. However, we discovered 4 out of the 1,379 pairs within the STS Benchmark dev set and 5 out of the 1,500 pairs in the STS Benchmark test set overlap with the SNLI training set. We do not believe this minimal overlap had a meaningful impact on the re"
W18-3022,S17-2003,0,0.0736434,"Missing"
W18-3022,P82-1020,0,0.75225,"Missing"
W18-3022,P15-1162,0,0.126241,"Missing"
W18-3022,D14-1162,0,0.0799848,"Missing"
W18-3022,S17-2016,0,0.0207056,"ning on SNLI without multitask training on Reddit, it would be equivalent to InferSent but without the use of pretrained sentence embeddings. We do not provide results for this configuration as preliminary experiments suggested it performed poorly. 168 dev test Reddit+SNLI tuned 0.835 0.808 Reddit+SNLI 0.814 0.782 Reddit tuned 0.809 0.781 Reddit 0.762 0.731 Neural representation models CNN (HCTI) 0.834 0.784 InferSent 0.801 0.758 Sent2Vec 0.787 0.755 SIF 0.801 0.720 PV-DBOW 0.722 0.649 C-PHRASE 0.743 0.639 Feature engineered and mixed systems ECNU 0.847 0.810 BIT 0.829 0.809 model CNN (HCTI) (Shao, 2017) and other systems in Cer et al. (2017).8 The untuned Reddit model is competitive with many of the other neural representation models, demonstrating that the sentence embeddings learned on Reddit conversations do keep text with similar semantics close in embedding space. The “out-of-the-box” multitask model, Reddit+SNLI, achieves an r of 0.814 on the dev set and 0.782 on test. Using a transformation matrix to adapt the Reddit model trained without SNLI to STS, we achieve Pearson’s r of 0.809 on dev and 0.781 on test. This surpasses InferSent and is close to the performance of the best neural r"
W18-3022,P15-1150,0,0.0730311,"Missing"
W18-3022,S17-2028,0,0.0255635,"ate the effectiveness of sentence representations learned from conversational data, we assess the proposed models on subtask B of SemEval Community Question Answering (CQA) (Nakov et al., 2017). In this task, given an “original” question Q, and the top ten related questions from a forum (Q1 , . . . , Q10 ) as retrieved by a search engine, the goal is to rank the related questions according to their similarity with respect 8 InferSent (Conneau et al., 2017), Sent2Vec (Pagliardini et al., 2017), SIF (Arora et al., 2017), PV-DBOW (Lau and Baldwin, 2016), C-PHRASE (Kruszewski et al., 2015), ECNU (Tian et al., 2017) and BIT (Wu et al., 2017). 9 For both the STS shared task and the STS benchmark leaderboard, systems are allowed to use external datasets as long as they do not make use of supervised annotations on data that overlap with the evaluation sets. InferSent introduced the use of SNLI for STS. However, we discovered 4 out of the 1,379 pairs within the STS Benchmark dev set and 5 out of the 1,500 pairs in the STS Benchmark test set overlap with the SNLI training set. We do not believe this minimal overlap had a meaningful impact on the results presented here. 10 As summarized by Cer et al. (2017), E"
W18-3022,S17-2007,0,0.0300463,"tence representations learned from conversational data, we assess the proposed models on subtask B of SemEval Community Question Answering (CQA) (Nakov et al., 2017). In this task, given an “original” question Q, and the top ten related questions from a forum (Q1 , . . . , Q10 ) as retrieved by a search engine, the goal is to rank the related questions according to their similarity with respect 8 InferSent (Conneau et al., 2017), Sent2Vec (Pagliardini et al., 2017), SIF (Arora et al., 2017), PV-DBOW (Lau and Baldwin, 2016), C-PHRASE (Kruszewski et al., 2015), ECNU (Tian et al., 2017) and BIT (Wu et al., 2017). 9 For both the STS shared task and the STS benchmark leaderboard, systems are allowed to use external datasets as long as they do not make use of supervised annotations on data that overlap with the evaluation sets. InferSent introduced the use of SNLI for STS. However, we discovered 4 out of the 1,379 pairs within the STS Benchmark dev set and 5 out of the 1,500 pairs in the STS Benchmark test set overlap with the SNLI training set. We do not believe this minimal overlap had a meaningful impact on the results presented here. 10 As summarized by Cer et al. (2017), ENCU makes use of a large f"
W18-3022,N18-1049,0,\N,Missing
W18-3022,P18-1224,0,\N,Missing
W18-6317,S12-1051,1,0.8695,"Missing"
W18-6317,W11-1218,0,0.108126,"Missing"
W18-6317,P02-1038,0,0.0326877,"anking task by maximizing the log likelihood of Papprox . This objective is particularly 1 Our implementation sums the word and bi-gram embeddings and then divides the result by sqrt(n), where n is the sentence length. The intuition behind dividing by sqrt(n) is as follows: We want our input embeddings to be sensitive to length. However, we also want to ensure that, for short sequences, the relative differences in the representations are not dominated by sentence length effects. (2) This formulation is similar to early work on discriminative training of log-linear translation decoding models (Och and Ney, 2002). However, 166 Source (Target) How to display and access shared files (Comment afficher et acc´eder aux fichiers partag´es) en-fr Random Hard Random The General Delegation for Armaments (La d´el´egation g´en´erale pour l’armement) Hard Random Oil and gas investments (Inversiones en petr´oleo y gas) Hard en-es In Spain, it has clearly chosen the gratuity (En Espa˜na, se ha elegido claramente la gratuidad) Random Hard Negatives Sa respiration devient laborieuse Benoit Faucon Lieu London Acc`es l’environment des fichiers partag´es Des e´ l´ements comme des fichiers de dossiers RCS 871, o`u le jug"
W18-6317,W09-0430,0,0.0197429,"Zipporah scores for ParaCrawl. 9 The sizes of WMT training set and filtered ParaCrawl are very close, so we simply mix the data together without any up sampling or down sampling. 10 For this analysis we use the same definition of extreme Zipporah scores as in section 5.2 173 Another direction, however, is to identify bitexts using only textual information, as the metadata associated with documents can often be sparse or unreliable (Uszkoreit et al., 2010). Some text-based approaches for identifying bitexts rely on methods such as n-gram scoring (Uszkoreit et al., 2010), named entity matching (Do et al., 2009), and cross-language information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005). There is active research on using embeddingbased approaches where texts are mapped to an embedding space in order to determine whether they are bitexts. Gr´egoire and Langlais (2017) use a Siamese network (Yin et al., 2015) to map source and target language sentences into the same space, then classify whether the sentences are parallel based on labelled data. Hassan et al. (2018) obtain English and Chinese sentence embeddings in a shared space by averaging encoder states from a bilingual shared en"
W18-6317,P02-1040,0,0.101319,"Missing"
W18-6317,P99-1068,0,0.794793,"Missing"
W18-6317,J03-3002,0,0.65407,"Missing"
W18-6317,P15-1162,0,0.130454,"Missing"
W18-6317,P18-2037,0,0.375566,"form nearly as well as models trained on the original data (within 1-2 BLEU). 1 Introduction Volumes of quality parallel training data are critical to neural machine translation (NMT) systems. While large distributed systems have proven useful for mining parallel documents (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), these approaches are computationally intensive and rely on heavily engineered subsystems. Recent work has approached the problem by training lightweight end-to-end models based on word and sentencelevel embeddings (Gr´egoire and Langlais, 2017; Bouamor and Sajjad, 2018; Schwenk, 2018). We propose a novel method for training bilingual sentence embeddings that proves useful for ∗ † Carnegie Mellon University Pittsburgh, PA, USA equal contribution Work done during an internship at Google AI. 165 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 165–176 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64017 level pairings we achieve a matching accuracy that is comparable to that of the much heavier weight and more computationally intensive approac"
W18-6317,P06-1062,0,0.52659,"Missing"
W18-6317,P07-2045,0,0.0065736,"rated confidence score6 with default threshold 0.5. In the second version, we perform document-level matching over the UN dataset. Within paired documents, we follow Uszkoreit et al. (2010) and employ a dynamic programming sentence alignment algorithm informed by sentence length and multilingual probabilistic dictionaries. In both versions, we drop sentence pairs where both sides are either identical or a language detector declares them to be in the wrong language. As a post-processing step, the resulting translations are resegmented using the Moses tokenizer and true-cased before evaluation (Koehn et al., 2007). Table 5 shows the results obtained from the models trained on the different variations of the parallel data. The models trained with mined pairs perform very close to the Oracle model, demonstrating the effectiveness of the proposed parallel corpus mining approach. Training on the mined sentence-level pairs even does slightly better than using the Oracle data for en-es. This is presumWMT Our data Zipporah WMT + Our data WMT + Zipporah en-fr (wmt14) 38.38 39.81 39.29 40.30 39.29 en-es (wmt13) 32.69 33.75 33.58 34.15 34.07 Table 6: BLEU scores on WMT testing sets of the NMT models trained on d"
W18-6317,C10-1124,0,0.235502,"propose a novel method for training bilingual sentence embeddings that proves useful for ∗ † Carnegie Mellon University Pittsburgh, PA, USA equal contribution Work done during an internship at Google AI. 165 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 165–176 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64017 level pairings we achieve a matching accuracy that is comparable to that of the much heavier weight and more computationally intensive approach of Uszkoreit et al. (2010). Training an NMT model using the reconstructed corpus results in models that perform nearly as well as those trained on the original parallel corpus (within 1-2 BLEU). Finally, our method has a modest degree of correlation with the pair quality scores provided by Zipporah (Xu and Koehn, 2017). However, our method has higher agreement with human judgments, and our approach to filter the ParaCrawl corpus results in NMT systems with higher BLEU scores. 2 rather than using a weighted sum of manually engineered features, we define φ to be the dotproduct of sentence embeddings for the source, u, an"
W18-6317,P03-1010,0,0.288686,"et and filtered ParaCrawl are very close, so we simply mix the data together without any up sampling or down sampling. 10 For this analysis we use the same definition of extreme Zipporah scores as in section 5.2 173 Another direction, however, is to identify bitexts using only textual information, as the metadata associated with documents can often be sparse or unreliable (Uszkoreit et al., 2010). Some text-based approaches for identifying bitexts rely on methods such as n-gram scoring (Uszkoreit et al., 2010), named entity matching (Do et al., 2009), and cross-language information retrieval (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005). There is active research on using embeddingbased approaches where texts are mapped to an embedding space in order to determine whether they are bitexts. Gr´egoire and Langlais (2017) use a Siamese network (Yin et al., 2015) to map source and target language sentences into the same space, then classify whether the sentences are parallel based on labelled data. Hassan et al. (2018) obtain English and Chinese sentence embeddings in a shared space by averaging encoder states from a bilingual shared encoder NMT system. The cosine similarity between these sentence embedd"
W18-6317,J05-4003,0,0.400228,"Missing"
W18-6317,P06-1011,0,0.25324,"Missing"
W18-6317,D17-1319,0,0.27164,"s, pages 165–176 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64017 level pairings we achieve a matching accuracy that is comparable to that of the much heavier weight and more computationally intensive approach of Uszkoreit et al. (2010). Training an NMT model using the reconstructed corpus results in models that perform nearly as well as those trained on the original parallel corpus (within 1-2 BLEU). Finally, our method has a modest degree of correlation with the pair quality scores provided by Zipporah (Xu and Koehn, 2017). However, our method has higher agreement with human judgments, and our approach to filter the ParaCrawl corpus results in NMT systems with higher BLEU scores. 2 rather than using a weighted sum of manually engineered features, we define φ to be the dotproduct of sentence embeddings for the source, u, and target, v, with φ(x, y) = u&gt; · v. A similar log-linear sentence embedding based formulation of P (y|x) has been previously used for conversation and e-mail response prediction (Henderson et al., 2017; Yang et al., 2018). 2.2 Bilingual sentence embeddings are obtained using the dual-encoder a"
W18-6317,W18-3022,1,0.902213,"Embeddings Mandy Guoa∗, Qinlan Shenb†∗, Yinfei Yanga∗, Heming Gea , Daniel Cera , Gustavo Hernandez Abregoa , Keith Stevensa , Noah Constanta , Yun-Hsuan Sunga , Brian Stropea , Ray Kurzweila Google AI Mountain View, CA, USA a b Abstract sentence-level mining of parallel data. Sentences are encoded using Deep Averaging Networks (DANs) (Iyyer et al., 2015), a simple bag of n-grams architecture that has been shown to provide surprisingly competitive performance on a number of tasks including sentence classification (Iyyer et al., 2015; Cer et al., 2018), conversation input-response prediction (Yang et al., 2018), and email response prediction (Henderson et al., 2017). Separate encoders are used for each language with candidate source and target sentences being paired based on the dot-product of their embedded representations. Training maximizes the dot-product score of sentence pairs that are translations of each other at the expense of sampled negatives. We contrast using random negatives with carefully selected hard negatives that challenge the model to distinguish between true translation pairs versus non-translation pairs that exhibit some degree of semantic similarity. The efficiency of the sent"
W18-6317,L16-1561,0,0.194487,"Missing"
W19-4330,D17-1070,0,0.56339,"nly make use of parallel data on both cross-lingual semantic textual similarity (STS) (Cer et al., 2017) and cross-lingual eigen-similarity (Søgaard et al., 2018). For European languages, the results show that the addition of monolingual data improves the embedding alignment of sentences and their translations. Further, we find that cross-lingual training with additional monolingual data leads to far better crosslingual transfer learning performance.1 Introduction Sentence embeddings are broadly useful for a diverse collection of downstream natural language processing tasks (Cer et al., 2018; Conneau et al., 2017; Kiros et al., 2015; Logeswaran and Lee, 2018; Subramanian et al., 2018). Sentence embeddings evaluated on downstream tasks in prior work have been trained on monolingual data, preventing them from being used for cross-lingual transfer learning. However, recent work on learning multilingual sentence embeddings has produced representations that capture semantic similarity even when sentences are written in different languages (Eriguchi et al., 2018; Guo et al., 2018; Schwenk and Douze, 2017; Singla et al., 2018). We explore multi-task extensions of multilingual models for cross-lingual transfe"
W19-4330,D18-1269,0,0.0773573,"Missing"
W19-4330,L18-1614,0,0.0267774,"Missing"
W19-4330,W18-6317,1,0.824496,"nce embeddings are broadly useful for a diverse collection of downstream natural language processing tasks (Cer et al., 2018; Conneau et al., 2017; Kiros et al., 2015; Logeswaran and Lee, 2018; Subramanian et al., 2018). Sentence embeddings evaluated on downstream tasks in prior work have been trained on monolingual data, preventing them from being used for cross-lingual transfer learning. However, recent work on learning multilingual sentence embeddings has produced representations that capture semantic similarity even when sentences are written in different languages (Eriguchi et al., 2018; Guo et al., 2018; Schwenk and Douze, 2017; Singla et al., 2018). We explore multi-task extensions of multilingual models for cross-lingual transfer learning. ∗ 1 Models based on this work are available at https: //tfhub.dev/ as: universal-sentence-encoder-xling/ende, universal-sentence-encoder-xling/en-fr, and universalsentence-encoder-xling/en-es. A large multilingual model is available as universal-sentence-encoder-xling/many. equal contribution 250 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 250–259 c Florence, Italy, August 2, 2019. 2019 Association for Comput"
W19-4330,D15-1075,0,0.066719,"es in the same batch as sIi . We realize g I and g R as deep neural networks that are trained to 2 Using pre-trained embeddings, did not improve performance during preliminary experiments. 251 where sIi is a sentence taken from an article and sPi and sR i are its predecessor and successor sentences, respectively. For this task, we model all three of g P (sPi ), g I (sIi ), and g R (sR i ) by g followed by separate, fully-connected feedforward layers of size 320 and 512 and using tanh activation. Natural Language Inference (NLI). We also include an English-only natural language inference task (Bowman et al., 2015). For this task, we first encode an input sentence sIi and its corresponding response hypothesis sR i into vectors u1 and u2 using g. Following Conneau et al. (2017), the vectors u1 , u2 are then used to construct a relation feature vector (u1 , u2 , |u1 − u2 |, u1 ∗ u2 ), where (·) represents concatenation and ∗ represents element-wise multiplication. The relation vector is then fed into a single feedforward layer of size 512 followed by a softmax output layer that is used to perform the 3-way NLI classification. Translation Ranking. Our translation task setup is identical to the one used by"
W19-4330,S17-2001,1,0.92008,"ions are enhanced using multitask training and unsupervised monolingual corpora. The effectiveness of our multilingual sentence embeddings are assessed on a comprehensive collection of monolingual, cross-lingual, and zeroshot/few-shot learning tasks. 1 We evaluate the learned representations on several monolingual and cross-lingual tasks, and provide a graph-based analysis of the learned representations. Multi-task training using additional monolingual tasks is found to improve performance over models that only make use of parallel data on both cross-lingual semantic textual similarity (STS) (Cer et al., 2017) and cross-lingual eigen-similarity (Søgaard et al., 2018). For European languages, the results show that the addition of monolingual data improves the embedding alignment of sentences and their translations. Further, we find that cross-lingual training with additional monolingual data leads to far better crosslingual transfer learning performance.1 Introduction Sentence embeddings are broadly useful for a diverse collection of downstream natural language processing tasks (Cer et al., 2018; Conneau et al., 2017; Kiros et al., 2015; Logeswaran and Lee, 2018; Subramanian et al., 2018). Sentence"
W19-4330,D18-2029,1,0.917739,"Missing"
W19-4330,D18-2012,0,0.0260165,"en-es 0.526 0.572 en-de 0.761 2.187 en-zh 2.366 0.393 Table 7: Average eigen-similarity values of source and target embedding subsets. to two absolute percentage points) on the dev sets for the training tasks. The notable exception is the word-embedding only English-German models tend to perform much worse on the dev sets for the training tasks involving German. This is likely due to the prevalence of compound words in German and represents an interesting difference for future exploration. We subsequently explored training versions of our cross-lingual models using a SentencePiece vocabulary (Kudo and Richardson, 2018), a set of largely sub-word tokens (characters and word chunks) that provide good coverage of an input dataset. Multilingual models for a single language pair (e.g., en-de) trained with SentencePiece performed similarly on the training dev sets to the models using character n-grams. However, when more languages are included in a single model (e.g., a single model that covers en, fr, de, es, and zh), SentencePiece tends to perform worse than using a combination of word and character n-gram embeddings. Within a larger joint model, SentencePiece is particularly problematic for languages like zh,"
W19-4330,L18-1269,0,0.0455698,"k models outperform the previous state-of-the-art by a sizable amount. We observe the en-zh translation-ranking models perform significantly better on the downstream tasks than the European language pair translationranking models. The en-zh models are possibly less capable of exploiting grammatical and other superficial similarities and are forced to rely on semantic representations. Exploring this further may present a promising direction for future research. 3.4 Cross-lingual Retrieval We first evaluate all of our cross-lingual models on several downstream English tasks taken from SentEval (Conneau and Kiela, 2018) to verify the impact of cross-lingual training. Evaluations are performed by training single hidden-layer feedforward networks on top of the 512-dimensional emWe evaluate both the multi-task and translationranking models’ efficacy in performing crosslingual retrieval by using held-out translation pair data. Following Guo et al. (2018) and Henderson et al. (2017), we use precision at N (P@N) as our evaluation metric. Performance is scored by checking if a source sentence’s target translation ranks7 in the top N scored candidates when considering K other randomly selected target sentences. We s"
W19-4330,S17-2007,0,0.0191702,"S Benchmark for all models. The first column shows the trained model performance on the original English STS Benchmark. Columns 2 to 5 provide the performance on the remaining languages. Multi-task models perform better than the translation ranking models on our multilingual STS Benchmark evaluation sets. Table 3 provides the results from the en-es models on the SemEval-2017 STS *-es tracks. The multitask models achieve 0.827 Pearson’s r for the es-es task and 0.769 for the en-es task. As a point of reference, we also list the two best performing STS systems, ECNU (Tian et al., 2017) and BIT (Wu et al., 2017), as reported in Cer et al. (2017). Our results are very close to these state-of-the-art feature engineered and mixed systems. 3.5 Multilingual STS Cross-lingual representations are evaluated on semantic textual similarity (STS) in French, Spanish, German, and Chinese. To evaluate SpanishSpanish (es-es) STS, we use data from track 3 of the SemEval-2017 STS shared task (Cer et al., 2017), containing 250 Spanish sentence pairs. We evaluate English-Spanish (en-es) STS using STS 2017 track 4(a),10 which contains 250 EnglishSpanish sentence pairs. 8 999 is smaller than the 10+ million used by Guo e"
W19-4330,P10-1114,0,0.263585,"Missing"
W19-4330,W17-2619,0,0.0311587,"broadly useful for a diverse collection of downstream natural language processing tasks (Cer et al., 2018; Conneau et al., 2017; Kiros et al., 2015; Logeswaran and Lee, 2018; Subramanian et al., 2018). Sentence embeddings evaluated on downstream tasks in prior work have been trained on monolingual data, preventing them from being used for cross-lingual transfer learning. However, recent work on learning multilingual sentence embeddings has produced representations that capture semantic similarity even when sentences are written in different languages (Eriguchi et al., 2018; Guo et al., 2018; Schwenk and Douze, 2017; Singla et al., 2018). We explore multi-task extensions of multilingual models for cross-lingual transfer learning. ∗ 1 Models based on this work are available at https: //tfhub.dev/ as: universal-sentence-encoder-xling/ende, universal-sentence-encoder-xling/en-fr, and universalsentence-encoder-xling/en-es. A large multilingual model is available as universal-sentence-encoder-xling/many. equal contribution 250 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 250–259 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics DOT D"
W19-4330,W18-3022,1,0.94263,"source-target pairs: English-French (en-fr), English-Spanish (en-es), English-German (en-de), and English-Chinese (en-zh). The resulting model structure is illustrated in Figure 1. We note that the conversational response ranking task can be seen as a special case of Contrastive Predictive Coding (CPC) (van den Oord et al., 2018) that only makes predictions one step into the future. Multi-Task Dual-Encoder Model The core of our approach is multi-task training over problems that can be modeled as ranking input-response pairs encoded via dual-encoders (Cer et al., 2018; Henderson et al., 2017; Yang et al., 2018). Cross-lingual representations are obtained by incorporating a translation bridge task (Gouws et al., 2015; Guo et al., 2018; Yang et al., 2019). For input-response ranking, we take an input sentence sIi and an associated response senR tence sR i , and we seek to rank si over all other R possible response sentences sj ∈ S R . We model I the conditional probability P (sR i |si ) as: I I P (sR i |si ) = ∑ R) eϕ(si ,si R sR j ∈S R ,sR ) j eϕ(si (1) I I ⊤ R R ϕ(sIi , sR j ) = g (si ) g (sj ) Where g I and g R are the input and response sentence encoding functions that compose the dual-encoder. Th"
W19-4330,P18-2035,0,0.0258971,"erse collection of downstream natural language processing tasks (Cer et al., 2018; Conneau et al., 2017; Kiros et al., 2015; Logeswaran and Lee, 2018; Subramanian et al., 2018). Sentence embeddings evaluated on downstream tasks in prior work have been trained on monolingual data, preventing them from being used for cross-lingual transfer learning. However, recent work on learning multilingual sentence embeddings has produced representations that capture semantic similarity even when sentences are written in different languages (Eriguchi et al., 2018; Guo et al., 2018; Schwenk and Douze, 2017; Singla et al., 2018). We explore multi-task extensions of multilingual models for cross-lingual transfer learning. ∗ 1 Models based on this work are available at https: //tfhub.dev/ as: universal-sentence-encoder-xling/ende, universal-sentence-encoder-xling/en-fr, and universalsentence-encoder-xling/en-es. A large multilingual model is available as universal-sentence-encoder-xling/many. equal contribution 250 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 250–259 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics DOT DOT DOT SOFTMAX DOT FC"
W19-4330,P18-1072,0,0.051108,"Missing"
W19-4330,S17-2028,0,0.0173464,"shows Pearson’s r on the STS Benchmark for all models. The first column shows the trained model performance on the original English STS Benchmark. Columns 2 to 5 provide the performance on the remaining languages. Multi-task models perform better than the translation ranking models on our multilingual STS Benchmark evaluation sets. Table 3 provides the results from the en-es models on the SemEval-2017 STS *-es tracks. The multitask models achieve 0.827 Pearson’s r for the es-es task and 0.769 for the en-es task. As a point of reference, we also list the two best performing STS systems, ECNU (Tian et al., 2017) and BIT (Wu et al., 2017), as reported in Cer et al. (2017). Our results are very close to these state-of-the-art feature engineered and mixed systems. 3.5 Multilingual STS Cross-lingual representations are evaluated on semantic textual similarity (STS) in French, Spanish, German, and Chinese. To evaluate SpanishSpanish (es-es) STS, we use data from track 3 of the SemEval-2017 STS shared task (Cer et al., 2017), containing 250 Spanish sentence pairs. We evaluate English-Spanish (en-es) STS using STS 2017 track 4(a),10 which contains 250 EnglishSpanish sentence pairs. 8 999 is smaller than the"
W19-4330,N18-1101,0,0.125641,"Missing"
W19-5207,W18-6317,1,0.509258,"Yun-Hsuan Sung, Brian Strope, Ray Kurzweil Google AI 1600 Amphitheatre Parkway Mountain View, CA, USA {xyguo, yinfeiy, kstevens, cer, hemingge, yhsung, raykurzweil}@google.com Abstract Obtaining a high-quality parallel training corpus is one of the most critical issues in machine translation. Previous work on parallel document mining using large distributed systems has proven effective (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), but these systems are often heavily engineered and computationally intensive. Recent work on parallel data mining has focused on sentence-level embeddings (Guo et al., 2018; Artetxe and Schwenk, 2018; Yang et al., 2019). However, these sentence embedding methods have had limited success when applied to documentlevel mining tasks (Guo et al., 2018). A recent study from Yang et al. (2019) shows that document embeddings obtained from averaging sentence embeddings can achieve state-of-the-art performance in document retrieval on the United Nation (UN) corpus. This simple averaging approach, however, heavily relies on high quality sentence embeddings and the cleanliness of documents in the application domain. In our work, we explore using three variants of document-l"
W19-5207,W11-1218,0,0.0448674,"Missing"
W19-5207,P15-1162,0,0.0470627,"Missing"
W19-5207,D18-1325,0,0.0277106,"aly, August 1-2, 2019. 2019 Association for Computational Linguistics Dot Product HiDE model performs well even when the underlying sentence-level model is relatively weak. We summarize our contributions as follows: Encoder • We introduce and explore different approaches for using document embeddings in parallel document mining. [x0, ..., xn] • We adapt the previous work on hierarchical networks to introduce a simple hierarchical document encoder trained on document pairs for this task. [y0, ..., ym] Figure 1: Dual encoder for parallel corpus mining, where (x, y) represents translation pairs. Miculicich et al. (2018) approached Document Classification and Neural Machine Translation using Hierarchical Attention Networks, and Wang et al. (2017) proposed using a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context. However, the amount of work applying document embeddings to the translation pair mining task has been limited. Yang et al. (2019) recently showed strong parallel document retrieval results using document embeddings obtained by averaging sentence embeddings. Our paper extends this work to explore different variants of document-level embeddings for parallel document"
W19-5207,D18-1482,0,0.0263577,"l., 2018; Schwenk, 2018). Guo et al. (2018) has studied documentlevel mining from sentence embeddings using a hyperparameter tuned similarity function, but had limited success compared to the heavily engineered system proposed by Uszkoreit et al. (2010). An extensive amount of work has also been done on learning document embeddings. Le and Mikolov (2014); Li et al. (2015); Dai et al. (2015) explored Paragraph Vector with various lengths (sentence, paragraph, document) trained on next word/n-gram prediction given context sampled from the paragraph. The work from Roy et al. (2016); Chen (2017); Wu et al. (2018) obtained document embeddings from word-level embeddings. More recent work has been focused on learning document embeddings through hierarchical training. The work from Yang et al. (2016); 3 Model This section introduces our document embedding models and training procedure. 3.1 Translation Candidate Ranking Task using a Dual Encoder All models use the dual encoder architecture in Figure 1, allowing candidate translation pairs to be scored using an efficient dot-product operation. The embeddings that feed the dot-product are trained by modeling parallel corpus mining as a translation ranking ta"
W19-5207,J05-4003,0,0.458491,"el embeddings for parallel document mining, including using an endto-end hierarchical encoder model. • Empirical results show our best document embedding model leads to state-of-the-art results on the document-level bitext retrieval task on two different datasets. The proposed hierarchical models are very robust to variations in sentence splitting and the underlying sentence embedding quality. 2 Encoder Related Work Parallel document mining has been extensively studied. One standard approach is to identify bitexts using metadata, such as document titles (Yang and Li, 2002), publication dates (Munteanu and Marcu, 2005, 2006), or document structure (Chen and Nie, 2000; Resnik and Smith, 2003; Shi et al., 2006). However, the metadata related to the documents can often be sparse or unreliable (Uszkoreit et al., 2010). More recent research has focused on embedding-based approaches, where texts are mapped to an embedding space to calculate their similarity distance and determine whether they are parallel (Gr´egoire and Langlais, 2017; Hassan et al., 2018; Schwenk, 2018). Guo et al. (2018) has studied documentlevel mining from sentence embeddings using a hyperparameter tuned similarity function, but had limited"
W19-5207,P06-1011,0,0.746641,"Missing"
W19-5207,J03-3002,0,0.767451,"rarchical encoder model. • Empirical results show our best document embedding model leads to state-of-the-art results on the document-level bitext retrieval task on two different datasets. The proposed hierarchical models are very robust to variations in sentence splitting and the underlying sentence embedding quality. 2 Encoder Related Work Parallel document mining has been extensively studied. One standard approach is to identify bitexts using metadata, such as document titles (Yang and Li, 2002), publication dates (Munteanu and Marcu, 2005, 2006), or document structure (Chen and Nie, 2000; Resnik and Smith, 2003; Shi et al., 2006). However, the metadata related to the documents can often be sparse or unreliable (Uszkoreit et al., 2010). More recent research has focused on embedding-based approaches, where texts are mapped to an embedding space to calculate their similarity distance and determine whether they are parallel (Gr´egoire and Langlais, 2017; Hassan et al., 2018; Schwenk, 2018). Guo et al. (2018) has studied documentlevel mining from sentence embeddings using a hyperparameter tuned similarity function, but had limited success compared to the heavily engineered system proposed by Uszkoreit et"
W19-5207,N16-1174,0,0.0460675,"to the heavily engineered system proposed by Uszkoreit et al. (2010). An extensive amount of work has also been done on learning document embeddings. Le and Mikolov (2014); Li et al. (2015); Dai et al. (2015) explored Paragraph Vector with various lengths (sentence, paragraph, document) trained on next word/n-gram prediction given context sampled from the paragraph. The work from Roy et al. (2016); Chen (2017); Wu et al. (2018) obtained document embeddings from word-level embeddings. More recent work has been focused on learning document embeddings through hierarchical training. The work from Yang et al. (2016); 3 Model This section introduces our document embedding models and training procedure. 3.1 Translation Candidate Ranking Task using a Dual Encoder All models use the dual encoder architecture in Figure 1, allowing candidate translation pairs to be scored using an efficient dot-product operation. The embeddings that feed the dot-product are trained by modeling parallel corpus mining as a translation ranking task (Guo et al., 2018). Given translation pair (x, y), we learn to rank true translation y over other candidates, Y. We use batch negatives, with sentence yi of the pair (xi , yi ) serving"
W19-5207,P18-2037,0,0.37018,"Ray Kurzweil Google AI 1600 Amphitheatre Parkway Mountain View, CA, USA {xyguo, yinfeiy, kstevens, cer, hemingge, yhsung, raykurzweil}@google.com Abstract Obtaining a high-quality parallel training corpus is one of the most critical issues in machine translation. Previous work on parallel document mining using large distributed systems has proven effective (Uszkoreit et al., 2010; Antonova and Misyurev, 2011), but these systems are often heavily engineered and computationally intensive. Recent work on parallel data mining has focused on sentence-level embeddings (Guo et al., 2018; Artetxe and Schwenk, 2018; Yang et al., 2019). However, these sentence embedding methods have had limited success when applied to documentlevel mining tasks (Guo et al., 2018). A recent study from Yang et al. (2019) shows that document embeddings obtained from averaging sentence embeddings can achieve state-of-the-art performance in document retrieval on the United Nation (UN) corpus. This simple averaging approach, however, heavily relies on high quality sentence embeddings and the cleanliness of documents in the application domain. In our work, we explore using three variants of document-level embeddings for paralle"
W19-5207,P06-1062,0,0.338127,"• Empirical results show our best document embedding model leads to state-of-the-art results on the document-level bitext retrieval task on two different datasets. The proposed hierarchical models are very robust to variations in sentence splitting and the underlying sentence embedding quality. 2 Encoder Related Work Parallel document mining has been extensively studied. One standard approach is to identify bitexts using metadata, such as document titles (Yang and Li, 2002), publication dates (Munteanu and Marcu, 2005, 2006), or document structure (Chen and Nie, 2000; Resnik and Smith, 2003; Shi et al., 2006). However, the metadata related to the documents can often be sparse or unreliable (Uszkoreit et al., 2010). More recent research has focused on embedding-based approaches, where texts are mapped to an embedding space to calculate their similarity distance and determine whether they are parallel (Gr´egoire and Langlais, 2017; Hassan et al., 2018; Schwenk, 2018). Guo et al. (2018) has studied documentlevel mining from sentence embeddings using a hyperparameter tuned similarity function, but had limited success compared to the heavily engineered system proposed by Uszkoreit et al. (2010). An ext"
W19-5207,L16-1561,0,0.228809,"Missing"
W19-5207,C10-1124,0,0.720915,"document-level bitext retrieval task on two different datasets. The proposed hierarchical models are very robust to variations in sentence splitting and the underlying sentence embedding quality. 2 Encoder Related Work Parallel document mining has been extensively studied. One standard approach is to identify bitexts using metadata, such as document titles (Yang and Li, 2002), publication dates (Munteanu and Marcu, 2005, 2006), or document structure (Chen and Nie, 2000; Resnik and Smith, 2003; Shi et al., 2006). However, the metadata related to the documents can often be sparse or unreliable (Uszkoreit et al., 2010). More recent research has focused on embedding-based approaches, where texts are mapped to an embedding space to calculate their similarity distance and determine whether they are parallel (Gr´egoire and Langlais, 2017; Hassan et al., 2018; Schwenk, 2018). Guo et al. (2018) has studied documentlevel mining from sentence embeddings using a hyperparameter tuned similarity function, but had limited success compared to the heavily engineered system proposed by Uszkoreit et al. (2010). An extensive amount of work has also been done on learning document embeddings. Le and Mikolov (2014); Li et al."
W19-5207,W19-4330,1,\N,Missing
