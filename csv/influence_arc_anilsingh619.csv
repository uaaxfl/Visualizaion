2013.mtsummit-papers.15,W06-3114,0,0.0170076,"on the T RACE corpus,1 a new, large corpus of French to English and English to French post-editions, which has been recently assembled using data collected from a public web portal and from datasets used in MT evaluation campaigns. The second contribution of this work is a study of the variability of post-edition, a question that the growing role of the TER score, both in MT evaluation and as a measure of the post-edition effort,2 makes more and more important. Since it has long been recognized that MT evaluation (especially at the sentence level) is plagued with a low inter-rater agreement (Koehn and Monz, 2006), it seems appropriate to raise the same issues in relationship to the QE task. Our analysis relies on a subpart of the T RACE corpus containing automatic translations that have been post-edited independently by two translators. To the best of our knowledge, this is the first time that several post-editions of the same sentences have been collected, allowing us to perform both a qualitative comparison of the differences between the post-editions of two translators as well as a quantitative analysis of the inter-rater agreement for the hTER score. The rest of the paper is organized as follows."
2013.mtsummit-papers.15,E06-1032,0,0.0322995,"o compute standard MT metrics on the automatic output. As reflected in Table 1 for the English to French direction, the metric values are much higher than what is usually observed in MT evaluation campaigns. This shows that the post-edited references are indeed much closer to the translations than the references used in these campaigns. For instance, when S YS S TAT is evaluated against the references of the WMT campaign, its TER score is 56.27, nearly twice as worse as when evaluated using post-edited translations as reference. It should also be noted that, as mentioned in many past studies (Callison-Burch et al., 2006), rule-based systems are highly disfavored by automatic metrics. 3 Failure Analysis of MT systems We show, in this section, how comparing translation hypotheses with their post-editions can help identify and analyze failures of MT systems. For space reasons, only results for the English to French direction are presented. 3.1 Error Patterns By computing the edit distance at the word-level between translation hypotheses and their postedition, it is possible to automatically detect the modifications required to make MT output both fluent and adequate. The careful analysis of the most frequent cor"
2013.mtsummit-papers.15,W12-3102,0,0.572737,"in order to correct the translations in terms of fluency and adequacy, is becoming more and more popular both to produce human-quality translations at a reduced cost (Garcia, 2011) or to evaluate the quality of MT systems. Indeed, the hTER score (Snover et al., 2006), which depends on the number of editions required to transform a MT hypothesis into a correct (post-edited) translation has proved to be a good indicator of the quality of a MT system. With the development of post-edition, more and more datasets of post-edited translations are being collected and distributed (Potet et al., 2012; Callison-Burch et al., 2012). These corpora have been accumulated in the context of MT evaluation campaigns and have mainly been used to estimate translation quality. They can also serve several other purposes: our first contribution is to show how they can be used to identify and analyze the limits of a MT system and to train a quality estimation (QE) system. For these tasks we present results achieved on the T RACE corpus,1 a new, large corpus of French to English and English to French post-editions, which has been recently assembled using data collected from a public web portal and from datasets used in MT evaluation"
2013.mtsummit-papers.15,2012.eamt-1.60,0,0.0137318,"ish to French direction. For the two directions, 1, 000 additional sentences that have been post-edited independently by two translators have also been prepared. These corpora can be freely downloaded from the T RACE website. Half of the source sentences have been collected through a public web portal which serves each month several millions of translation requests between French and English. These requests cover a wide variety of genres and domains. The other half of the corpus is made of parts of the datasets provided by MT evaluation campaigns (WMT3 (Callison-Burch et al., 2012) and IWSLT (Cettolo et al., 2012)) and by Word Sense Disambiguation campaigns (Lefever and Hoste, 2010). Examples from this part of the corpus are accompanied by additional information provided by the campaigns organizers such as reference translations or semantic annotations. These sentences have been translated by two MT systems: the first one, denoted by S YS RULE, is a commercial rule-based system; the second, denoted S YS S TAT, a state-of-the-art phrase-based statistical MT system developed for the WMT’12 evaluation campaign (Le et al., 2012). Precise guidelines were given to the translators to ensure that the correctio"
2013.mtsummit-papers.15,2004.tmi-1.8,0,0.0127131,"le a a` dans que en un des Deletion 799 335 329 278 277 256 242 215 212 167 de a` la le que les en et des pour Table 3: Most frequent editions. 3.2 Differences between Automatic Translations and their Post-Edition To characterize the differences between automatic translations and their post-edition, we propose to learn a classifier that could distinguish between these two kinds of translations. We hope that finding which features are relevant for making this distinction will provide us some insight about the limits of MT systems. This approach is directly inspired by earlier work in QE like (Kulesza and Shieber, 2004), where the authors try to learn the difference between a good and a bad translation. In the experiments described in this section, each translation is represented by 336 numerical features, most of which are inspired by works in QE for MT (Callison-Burch et al., 2012).4 These features can be classified into four categories: • Association Features: Measures of the quality of the ‘association’ between the source and the target sentences like, for instance, features derived from the IBM 1 model scores; • Fluency Features: Measures of the ‘fluency’ or the ‘grammaticality’ of the target and source"
2013.mtsummit-papers.15,S10-1003,0,0.0430789,"sentences that have been post-edited independently by two translators have also been prepared. These corpora can be freely downloaded from the T RACE website. Half of the source sentences have been collected through a public web portal which serves each month several millions of translation requests between French and English. These requests cover a wide variety of genres and domains. The other half of the corpus is made of parts of the datasets provided by MT evaluation campaigns (WMT3 (Callison-Burch et al., 2012) and IWSLT (Cettolo et al., 2012)) and by Word Sense Disambiguation campaigns (Lefever and Hoste, 2010). Examples from this part of the corpus are accompanied by additional information provided by the campaigns organizers such as reference translations or semantic annotations. These sentences have been translated by two MT systems: the first one, denoted by S YS RULE, is a commercial rule-based system; the second, denoted S YS S TAT, a state-of-the-art phrase-based statistical MT system developed for the WMT’12 evaluation campaign (Le et al., 2012). Precise guidelines were given to the translators to ensure that the corrections of the automatic translations were minimal: they were asked to prod"
2013.mtsummit-papers.15,D12-1077,0,0.0204608,"Missing"
2013.mtsummit-papers.15,potet-etal-2012-collection,0,0.0767283,"Missing"
2013.mtsummit-papers.15,2006.amta-papers.25,0,0.0359803,"these data, notably the development of an automatic Quality Estimation (QE) system and the detection of frequent errors in automatic translations. Both applications require a careful assessment of the variability in post-editions, that we study here. 1 Introduction Post-editing, the process of editing the outputs of a Machine Translation (MT) system in order to correct the translations in terms of fluency and adequacy, is becoming more and more popular both to produce human-quality translations at a reduced cost (Garcia, 2011) or to evaluate the quality of MT systems. Indeed, the hTER score (Snover et al., 2006), which depends on the number of editions required to transform a MT hypothesis into a correct (post-edited) translation has proved to be a good indicator of the quality of a MT system. With the development of post-edition, more and more datasets of post-edited translations are being collected and distributed (Potet et al., 2012; Callison-Burch et al., 2012). These corpora have been accumulated in the context of MT evaluation campaigns and have mainly been used to estimate translation quality. They can also serve several other purposes: our first contribution is to show how they can be used to"
2013.mtsummit-papers.15,specia-etal-2010-dataset,0,0.0962939,"Missing"
2013.mtsummit-papers.15,2013.tc-1.10,0,0.0399314,"Missing"
2020.aacl-srw.24,P06-1032,0,0.0907706,"Since this corpus is extracted from a relatively natural source, it can be useful for evaluating GEC systems. We also analyze this corpus using an extended version of the ERRANT toolkit. Introduction Grammatical Error Correction (GEC) involves automatically correcting errors in written text, whether relating to orthography, syntax or fluency. Today, most approaches for solving this problem highlight statistical and deep learning methods as opposed to rulebased methods. These methods treat GEC as a translation task, from an ungram matical to a grammatically correct form of the same language (Brockett et al., 2006). This re quires a considerable amount of supervised data in the form of ‘edits’, which are pairs of incor rect and correct sentences. Researchers have re cently done remarkable work on English and a few other resourcerich languages and have released many datasets to evaluate state of the art meth ods. Comparatively less attention has been given 3. We evaluate a few well studied approaches for languages like English on these datasets, and thus produce the initial GEC results for the Hindi language. The code and data to reproduce our experiments are available at http://github.com/s-ankur/h"
2020.aacl-srw.24,W19-4406,0,0.0114265,"b.com/s-ankur/hindi_ grammar_correction. 2 Related Work The most common GEC datasets come from correctionannotated language learner essays. The English learner corpora include those from shared 165 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 165–171 c December 4 - 7, 2020. 2020 Association for Computational Linguistics tasks such as Helping Our Own (Dale et al., 2012), CoNLL2014 (Ng et al., 2014) and recently, BEA2019 (Bryant et al., 2019). Similar learner corpora exist for the Russian (Rozovskaya and Roth, 2019) and the Czech (Náplava and Straka, 2019) languages. However, the problem with such manually annotated corpora is that they are not readily available for low resource languages, and their creation will be resource and timeintensive. Another popular method has been the deliberate injection of errors into grammatically correct sen tences, whether by a rulebased system or by strate gies like roundtrip translation (Lichtarge et al., 2019). The former approach has been essential for languages with limited training data."
2020.aacl-srw.24,P17-1074,0,0.0230918,"fy this tool for Hindi and extract edits from a Wikipedia revision dump dated October 1, 2020 to create our dataset, which we term as HiWikEd. For filtering the edits, we constrain extracted sentence length to between 6 and 27 tokens and consider only substitution operations with a tokenbased Levenstein edit distance of less than 0.3. Addition ally, we discard edits containing only a difference in punctuation or numbers and corrections involv ing extremely rare tokens or HTML markups. Ed its relating to vandalism are also discarded. 5 Error Analysis The ERRor ANotation Toolkit (ERRANT3 ) (Bryant et al., 2017; Felice et al., 2016) is a tool that uses morphological and dependency information to analyze, merge and categorize errors using a rulebased system. Initially created for English, it has since been extended to German (Boyd, 2018). We use a similar method to extend the toolkit to Hindi and use it to classify the errors in HiWikEd (See Table 2 for examples). Although the classification criteria consider many exceptional cases, the basic reasoning used by us is as follows: 1. POS tags and lemma for the tokens are ob tained using the StanfordNLP tagger (Qi et al., 2018). By comparing POS tags f"
2020.aacl-srw.24,N12-1067,0,0.0458486,"Missing"
2020.aacl-srw.24,W12-2006,0,0.0212166,"ode and data to reproduce our experiments are available at http://github.com/s-ankur/hindi_ grammar_correction. 2 Related Work The most common GEC datasets come from correctionannotated language learner essays. The English learner corpora include those from shared 165 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 165–171 c December 4 - 7, 2020. 2020 Association for Computational Linguistics tasks such as Helping Our Own (Dale et al., 2012), CoNLL2014 (Ng et al., 2014) and recently, BEA2019 (Bryant et al., 2019). Similar learner corpora exist for the Russian (Rozovskaya and Roth, 2019) and the Czech (Náplava and Straka, 2019) languages. However, the problem with such manually annotated corpora is that they are not readily available for low resource languages, and their creation will be resource and timeintensive. Another popular method has been the deliberate injection of errors into grammatically correct sen tences, whether by a rulebased system or by strate gies like roundtrip translation (Lichtarge et al., 2019). The for"
2020.aacl-srw.24,P18-3021,0,0.0448682,"Missing"
2020.aacl-srw.24,D18-1028,0,0.0254741,"ial for languages with limited training data. This was the case for English early on (Izumi et al., 2004; Foster and Andersen, 2009), and is still the case for low resource languages such as Indonesian (Ir mawati et al., 2017). Provided that the artificial errors closely resemble realworld mistakes, this method can be applied to obtain large volumes of training data reliably. A third approach involves mining edits from websites, such as language learner websites (Mizu moto et al., 2011) or from websites with public revision histories like Wikipedia1 (Grundkiewicz and JunczysDowmunt, 2014; Faruqui et al., 2018; Boyd, 2018) or GitHub (Hagiwara and Mita, 2020). While this has the potential to yield natural datasets of considerable size, there are several issues with edits obtained by this method, as not all corrections made in the text are of a grammatical nature; and many simply add more information or are semantic improvements to the text. As the edits lack any hu man curation, this method results in a more noisy corpus. 3 Hindi Grammar Hindi is a fusional language that expresses gram matical features like case, gender, number, tense, etc. via morphological changes. In particular, all verbs and s"
2020.aacl-srw.24,C16-1079,0,0.014728,"i and extract edits from a Wikipedia revision dump dated October 1, 2020 to create our dataset, which we term as HiWikEd. For filtering the edits, we constrain extracted sentence length to between 6 and 27 tokens and consider only substitution operations with a tokenbased Levenstein edit distance of less than 0.3. Addition ally, we discard edits containing only a difference in punctuation or numbers and corrections involv ing extremely rare tokens or HTML markups. Ed its relating to vandalism are also discarded. 5 Error Analysis The ERRor ANotation Toolkit (ERRANT3 ) (Bryant et al., 2017; Felice et al., 2016) is a tool that uses morphological and dependency information to analyze, merge and categorize errors using a rulebased system. Initially created for English, it has since been extended to German (Boyd, 2018). We use a similar method to extend the toolkit to Hindi and use it to classify the errors in HiWikEd (See Table 2 for examples). Although the classification criteria consider many exceptional cases, the basic reasoning used by us is as follows: 1. POS tags and lemma for the tokens are ob tained using the StanfordNLP tagger (Qi et al., 2018). By comparing POS tags for the edit, the error"
2020.aacl-srw.24,W09-2112,0,0.0416705,"nd Roth, 2019) and the Czech (Náplava and Straka, 2019) languages. However, the problem with such manually annotated corpora is that they are not readily available for low resource languages, and their creation will be resource and timeintensive. Another popular method has been the deliberate injection of errors into grammatically correct sen tences, whether by a rulebased system or by strate gies like roundtrip translation (Lichtarge et al., 2019). The former approach has been essential for languages with limited training data. This was the case for English early on (Izumi et al., 2004; Foster and Andersen, 2009), and is still the case for low resource languages such as Indonesian (Ir mawati et al., 2017). Provided that the artificial errors closely resemble realworld mistakes, this method can be applied to obtain large volumes of training data reliably. A third approach involves mining edits from websites, such as language learner websites (Mizu moto et al., 2011) or from websites with public revision histories like Wikipedia1 (Grundkiewicz and JunczysDowmunt, 2014; Faruqui et al., 2018; Boyd, 2018) or GitHub (Hagiwara and Mita, 2020). While this has the potential to yield natural datasets of con"
2020.aacl-srw.24,2020.lrec-1.835,0,0.0349438,"This was the case for English early on (Izumi et al., 2004; Foster and Andersen, 2009), and is still the case for low resource languages such as Indonesian (Ir mawati et al., 2017). Provided that the artificial errors closely resemble realworld mistakes, this method can be applied to obtain large volumes of training data reliably. A third approach involves mining edits from websites, such as language learner websites (Mizu moto et al., 2011) or from websites with public revision histories like Wikipedia1 (Grundkiewicz and JunczysDowmunt, 2014; Faruqui et al., 2018; Boyd, 2018) or GitHub (Hagiwara and Mita, 2020). While this has the potential to yield natural datasets of considerable size, there are several issues with edits obtained by this method, as not all corrections made in the text are of a grammatical nature; and many simply add more information or are semantic improvements to the text. As the edits lack any hu man curation, this method results in a more noisy corpus. 3 Hindi Grammar Hindi is a fusional language that expresses gram matical features like case, gender, number, tense, etc. via morphological changes. In particular, all verbs and some adjectives are inflected to agree with the nu"
2020.aacl-srw.24,Q19-1001,0,0.0202445,"on GEC datasets come from correctionannotated language learner essays. The English learner corpora include those from shared 165 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 165–171 c December 4 - 7, 2020. 2020 Association for Computational Linguistics tasks such as Helping Our Own (Dale et al., 2012), CoNLL2014 (Ng et al., 2014) and recently, BEA2019 (Bryant et al., 2019). Similar learner corpora exist for the Russian (Rozovskaya and Roth, 2019) and the Czech (Náplava and Straka, 2019) languages. However, the problem with such manually annotated corpora is that they are not readily available for low resource languages, and their creation will be resource and timeintensive. Another popular method has been the deliberate injection of errors into grammatically correct sen tences, whether by a rulebased system or by strate gies like roundtrip translation (Lichtarge et al., 2019). The former approach has been essential for languages with limited training data. This was the case for English early on (Izumi et al., 2004; Foster and And"
2020.aacl-srw.24,N19-1333,0,0.0124042,"ping Our Own (Dale et al., 2012), CoNLL2014 (Ng et al., 2014) and recently, BEA2019 (Bryant et al., 2019). Similar learner corpora exist for the Russian (Rozovskaya and Roth, 2019) and the Czech (Náplava and Straka, 2019) languages. However, the problem with such manually annotated corpora is that they are not readily available for low resource languages, and their creation will be resource and timeintensive. Another popular method has been the deliberate injection of errors into grammatically correct sen tences, whether by a rulebased system or by strate gies like roundtrip translation (Lichtarge et al., 2019). The former approach has been essential for languages with limited training data. This was the case for English early on (Izumi et al., 2004; Foster and Andersen, 2009), and is still the case for low resource languages such as Indonesian (Ir mawati et al., 2017). Provided that the artificial errors closely resemble realworld mistakes, this method can be applied to obtain large volumes of training data reliably. A third approach involves mining edits from websites, such as language learner websites (Mizu moto et al., 2011) or from websites with public revision histories like Wikipedia1 (Gru"
2020.aacl-srw.24,I11-1017,0,0.0842063,"Missing"
2020.aacl-srw.24,D19-5545,0,0.0455835,"ated language learner essays. The English learner corpora include those from shared 165 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 165–171 c December 4 - 7, 2020. 2020 Association for Computational Linguistics tasks such as Helping Our Own (Dale et al., 2012), CoNLL2014 (Ng et al., 2014) and recently, BEA2019 (Bryant et al., 2019). Similar learner corpora exist for the Russian (Rozovskaya and Roth, 2019) and the Czech (Náplava and Straka, 2019) languages. However, the problem with such manually annotated corpora is that they are not readily available for low resource languages, and their creation will be resource and timeintensive. Another popular method has been the deliberate injection of errors into grammatically correct sen tences, whether by a rulebased system or by strate gies like roundtrip translation (Lichtarge et al., 2019). The former approach has been essential for languages with limited training data. This was the case for English early on (Izumi et al., 2004; Foster and Andersen, 2009), and is still the case for l"
2020.aacl-srw.24,P15-2097,0,0.0337579,"Missing"
2020.aacl-srw.24,W14-1701,0,0.0138357,"xperiments are available at http://github.com/s-ankur/hindi_ grammar_correction. 2 Related Work The most common GEC datasets come from correctionannotated language learner essays. The English learner corpora include those from shared 165 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 165–171 c December 4 - 7, 2020. 2020 Association for Computational Linguistics tasks such as Helping Our Own (Dale et al., 2012), CoNLL2014 (Ng et al., 2014) and recently, BEA2019 (Bryant et al., 2019). Similar learner corpora exist for the Russian (Rozovskaya and Roth, 2019) and the Czech (Náplava and Straka, 2019) languages. However, the problem with such manually annotated corpora is that they are not readily available for low resource languages, and their creation will be resource and timeintensive. Another popular method has been the deliberate injection of errors into grammatically correct sen tences, whether by a rulebased system or by strate gies like roundtrip translation (Lichtarge et al., 2019). The former approach has been essenti"
2020.aacl-srw.24,K18-2016,0,0.0222034,"n Toolkit (ERRANT3 ) (Bryant et al., 2017; Felice et al., 2016) is a tool that uses morphological and dependency information to analyze, merge and categorize errors using a rulebased system. Initially created for English, it has since been extended to German (Boyd, 2018). We use a similar method to extend the toolkit to Hindi and use it to classify the errors in HiWikEd (See Table 2 for examples). Although the classification criteria consider many exceptional cases, the basic reasoning used by us is as follows: 1. POS tags and lemma for the tokens are ob tained using the StanfordNLP tagger (Qi et al., 2018). By comparing POS tags for the edit, the error category is decided as follows. 2 3 via http://dumps.wikimedia.org/ 166 http://github.com/snukky/wikiedits http://github.com/chrisjbryant/errant Figure 1: Frequencies of various error types in the HiWikEd dataset. Figure 2: Example of error insertion. In the first sentence, the verb जाता (jātā, “used to”) agrees with the noun टेिनस (tenis, “tennis”) . We change the inflection of the verb and thus introduce disagreement into the sentence. The same is the case for the adjective सूखा (sūkhā, “dry”) in the next sentence. 2. Edits with the same lemma"
2020.aacl-srw.24,W18-1819,0,0.0412885,"Missing"
2020.aacl-srw.24,N19-1014,0,0.0393879,"Missing"
2020.icon-main.31,E17-4007,0,0.0599607,"Missing"
2020.loresmt-1.6,C18-1111,0,0.0636696,"Missing"
2020.loresmt-1.6,N19-4009,0,0.0167909,"guages We used Nepali–Hindi language pairs to train the model in all translation directions. The motive behind using Nepali–Hindi language pairs is the relatedness between this language pair with Bhojpuri– Hindi and Magahi–Hindi language pairs. Here, we use language pairs as one domain and evaluate it on another domain. So, we use directly trained model on Bhojpuri–Hindi and Magahi–Hindi language pairs to evaluate the model. 3.2 Value 5 512 512 2 2 0.4 0.2 Adam inverse sqrt 1e-3 1e-9 (0.9, 0.98) 100 Two-iteration back-translation using monolingual data 5 Experimental Settings We used fairseq (Ott et al., 2019) sequence modelling toolkit for training the transformer-based NMT model. The hyper-parameter settings used in the paper are described in table 1. For Bhojpuri to Hindi, we used Hindi→Nepali trained model to translate the monolingual Hindi data to Nepali. Then we append this predicted parallel data with Nepali→Hindi and trained the Bhojpuri→Hindi model and evaluate the model with Bhojpuri→Hindi development and test sets provided by Organizer. For Hindi to Bhojpuri, we used Nepali→Hindi trained model to translate the monolingual Bhojpuri data to Hindi. Then we append this predicted parallel dat"
2020.loresmt-1.6,W18-2703,0,0.0637498,"Missing"
2020.loresmt-1.6,P02-1040,0,0.12365,"inguistics and target domains are two different tasks that are related to each other in some relevant way. We apply domain adaptation to similar languages. Hindi, Nepali, Bhojpuri and Magahi are similar languages and they are also orthographically similar 1 (Mundotiya et al., 2020). All these languages use the Devanagari script and they share a lot of cognates and loan words among each other. This is the main motivation behind using unsupervised domain adaptation on similar languages in our paper. We use back-translation to increase the parallel corpus that helps in improving the BLEU points (Papineni et al., 2002) for the translation task. Parameters Encoder and decoder layers Encoder embedding dimension Decoder embedding dimension Encoder attention heads Decoder attention heads Dropout Attention dropout Optimizer Learning rate scheduler Learning rate Minimum learning rate Adam-betas Number of epochs 3 Table 1: Hyperparameters used in our experiment Methodology We used an unsupervised hybrid approach to train the model for zero parallel training data. 3.1 4 Corpus Description We used the Nepali–Hindi language pairs collected from WMT 20192 similar language translation task, as well as from Opus3 and TD"
2020.loresmt-1.6,D10-1092,0,0.13056,"i and trained the Bhojpuri→Hindi model and evaluate the model with Bhojpuri→Hindi development and test sets provided by Organizer. For Hindi to Bhojpuri, we used Nepali→Hindi trained model to translate the monolingual Bhojpuri data to Hindi. Then we append this predicted parallel data with Hindi→Nepali and trained the Hindi→Bhojpuri model and evaluated the model with Hindi→Bhojpuri development and test set provided by the shared task organizers. Similarly, we repeat the above steps for Magahi– Hindi language pairs. 6 Results The shared task organizers used BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), Precision, Recall and F-measure metrics to evaluate the performance of the system which is shown in figure 1 and table 2. We use unsupervised domain adaptation approach followed by Back-translation for Bhojpuri → Hindi and Magahi → Hindi language pairs, whereas for Hindi → Bhojpuri and Hindi 2 http://www.statmt.org/wmt19/ http://opus.nlpl.eu/ 4 https://www.tdil-dc.in/index.php? lang=en 3 1 https://nepalgo.de/post/121994474120/ guffgaff-nepali-vs-hindi 44 Figure 1: Bar diagram represents Precision, Recall, and F-measure System Bhojpuri → Hindi Magahi → Hindi Hindi → Bhojpuri Hindi → Magahi BL"
2020.loresmt-1.6,D19-5222,1,0.829856,"nd Rajesh Kumar Mundotiya and Anil Kumar Singh IIT (BHU), Varanasi {amitkumar.rs.cse17, rajeshkm.rs.cse16, aksingh.cse}@iitbhu.ac.in Abstract tools of high resource languages for creating the resources and tools for low resource languages. In this paper, we use the available Nepali–Hindi low resource data for domain adaptation (Chu and Wang, 2018), followed by a back-translation (Hoang et al., 2018) approach for creating the synthetic dataset and NMT tools for Bhojpuri–Hindi and Magahi–Hindi zero-shot languages. We evaluate our method using a Transformerbased NMT system (Vaswani et al., 2017; Kumar and Singh, 2019) for domain adaptation and backtranslation of monolingual data. Our approach gives new state-of-the-art results of 19.5, 13.71, 2.54, and 3.16 BLEU points for Bhojpuri → Hindi, Magahi → Hindi, Hindi → Bhojpuri and Hindi → Magahi language pairs, respectively. This paper reports a Machine Translation (MT) system submitted by the NLPRL team for the Bhojpuri–Hindi and Magahi–Hindi language pairs at LoResMT 2020 shared task. We used an unsupervised domain adaptation approach that gives promising results for zero or extremely low resource languages. Task organizers provide the development and the te"
2020.loresmt-1.6,2020.tacl-1.47,0,0.0180532,"source instance, and Y is a labelled instance, and DT = {Z} is the target domain, where Z is an unlabeled instance. Then, the goal of unsupervised domain adaptation is to improve the accuracy of the target domain with the help of the source domain, where source Introduction For the past few years, Neural Machine Translation (NMT) (Sutskever et al., 2014) has been the favoured approach for both high resource and low resource languages translation. A large number of variations of the Transformer (Vaswani et al., 2017) model, such as BERT (Devlin et al., 2018), BART (Lewis et al., 2019), MBART (Liu et al., 2020) have been developed, giving state-of-the-art result for translation. However, successful NMT is only possible if a large parallel corpus is available to train the model. However, not so much parallel corpus is available for extremely low resource languages and there are still many languages in which no parallel corpora are available. Creating data, even parallel corpus, for extremely low resource languages is a time and labour consuming process. Instead of creating the data from scratch for such low resource languages, there is a possibility to use the available resources and 43 Proceedings o"
2020.loresmt-1.6,N19-1039,0,0.0280408,"vely promising results, with a wide range, of 19.5, 13.71, 2.54, and 3.16 BLEU points for Bhojpuri to Hindi, Magahi to Hindi, Hindi to Bhojpuri and Hindi to Magahi language pairs, respectively. 1 2 Background Ojha (2019) describe the resources created for Bhojpuri languages and it also covers some results on Bhojpuri–English SMT systems. In 2019, LoResMT (Karakanta et al., 2019) organized the shared task for Bhojpuri–English and Magahi– English language pairs. However, previous works contains some parallel sentences. The work in this paper is primarily based on unsupervised domain adaptation (Miller, 2019), followed by backtranslation of monolingual data (Guzm´an et al., 2019). Unsupervised domain adaptation is the task of training a model on labelled data from a source domain to achieve better performance on data from a target domain, with access to only unlabeled data in the target domain. Suppose DS = {X, Y} is domain of source task, where X is a source instance, and Y is a labelled instance, and DT = {Z} is the target domain, where Z is an unlabeled instance. Then, the goal of unsupervised domain adaptation is to improve the accuracy of the target domain with the help of the source domain,"
2020.paclic-1.54,W06-1615,0,0.370567,"Missing"
2020.paclic-1.54,D16-1053,0,0.0830367,"Missing"
2020.paclic-1.54,2020.acl-srw.22,0,0.0723364,"Missing"
2020.paclic-1.54,P19-1229,0,0.0463103,"Missing"
2020.paclic-1.54,P16-1101,0,0.0955398,"Missing"
2020.paclic-1.54,L18-1446,0,0.0506686,"Missing"
2020.paclic-1.54,W17-7507,0,0.0645905,"Missing"
2020.paclic-1.54,P16-2067,0,0.0302473,"Missing"
2020.paclic-1.54,W18-3503,0,0.0485636,"Missing"
2020.paclic-1.54,W15-2201,0,0.0605506,"Missing"
2020.paclic-1.54,E14-1062,0,0.0405344,"Missing"
2020.wmt-1.126,2020.coling-tutorials.3,0,0.117428,"s continually declining (Dołowy-Rybi´nska, 2018). To counter this, attempts are being made to increase the number of Sorbian speakers through bilingual educational scenarios and MT1 . 1 https://minorityrights.org/ minorities/sorbs/ Background NMT is an end-to-end learning system (Bahdanau et al., 2015), based on the data-driven approach of machine translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning (Zheng et al. (2019)), unsupervised learning (Sun et al. (2020)), data augmentation (Siddhant et al. (2020)), transfer learning (Aji (2020)), meta-learning (Li et al. (2020)), pivot-based (Kim et al. (2019)), and multilingual machine translation (Dabre et al. (2020)). A model-agnostic meta-learning algorithm (Finn et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters (Chung et al.,"
2020.wmt-1.126,ahmadnia-etal-2017-persian,0,0.0259924,"ajesh Kumar Mundotiya, Amit Kumar, Anil Kumar Singh Department of Computer Science & Engineering Indian Institute of Technology (BHU) Varanasi, India {rupjyotibaruah.rs.cse18, rajeshkm.rs.cse16}@iitbhu.ac.in {amitkumar.rs.cse17, aksingh.cse}@iitbhu.ac.in Abstract Low resource MT was being attempted even before Neural Machine Translation (NMT) became the state-of-the-art. Several methods are used to improve the accuracy and quality of the lowresource SMT systems by using comparable corpora (Irvine and Callison-Burch, 2013; Babych et al., 2007), pivot language (English or nonEnglish) technique (Ahmadnia et al., 2017; Paul et al., 2013), and using related resource-rich language (Nakov and Ng, 2012). We use a byte-level version of Byte Pair Encoding based model with a Transformer for our experiments. The main motivation was to try out this model for the shared task and see how it works under a shared task setting. This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try"
2020.wmt-1.126,2020.acl-main.688,0,0.0204934,"pts are being made to increase the number of Sorbian speakers through bilingual educational scenarios and MT1 . 1 https://minorityrights.org/ minorities/sorbs/ Background NMT is an end-to-end learning system (Bahdanau et al., 2015), based on the data-driven approach of machine translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning (Zheng et al. (2019)), unsupervised learning (Sun et al. (2020)), data augmentation (Siddhant et al. (2020)), transfer learning (Aji (2020)), meta-learning (Li et al. (2020)), pivot-based (Kim et al. (2019)), and multilingual machine translation (Dabre et al. (2020)). A model-agnostic meta-learning algorithm (Finn et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters (Chung et al., 2016), mixed word/characters (Luong and Man1075 Proceedings of the 5th Conf"
2020.wmt-1.126,2007.mtsummit-papers.5,0,0.101311,"for Very Low Resource Supervised Machine Translation Rupjyoti Baruah, Rajesh Kumar Mundotiya, Amit Kumar, Anil Kumar Singh Department of Computer Science & Engineering Indian Institute of Technology (BHU) Varanasi, India {rupjyotibaruah.rs.cse18, rajeshkm.rs.cse16}@iitbhu.ac.in {amitkumar.rs.cse17, aksingh.cse}@iitbhu.ac.in Abstract Low resource MT was being attempted even before Neural Machine Translation (NMT) became the state-of-the-art. Several methods are used to improve the accuracy and quality of the lowresource SMT systems by using comparable corpora (Irvine and Callison-Burch, 2013; Babych et al., 2007), pivot language (English or nonEnglish) technique (Ahmadnia et al., 2017; Paul et al., 2013), and using related resource-rich language (Nakov and Ng, 2012). We use a byte-level version of Byte Pair Encoding based model with a Transformer for our experiments. The main motivation was to try out this model for the shared task and see how it works under a shared task setting. This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 2"
2020.wmt-1.126,N18-1032,0,0.0189727,"translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning (Zheng et al. (2019)), unsupervised learning (Sun et al. (2020)), data augmentation (Siddhant et al. (2020)), transfer learning (Aji (2020)), meta-learning (Li et al. (2020)), pivot-based (Kim et al. (2019)), and multilingual machine translation (Dabre et al. (2020)). A model-agnostic meta-learning algorithm (Finn et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters (Chung et al., 2016), mixed word/characters (Luong and Man1075 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1075–1078 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics ning, 2016), or more intelligent sub-words (Sennrich et al., 2016). It is claimed that an NMT model using such an approach is capable of openvocabulary"
2020.wmt-1.126,D18-1398,0,0.0234644,"translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning (Zheng et al. (2019)), unsupervised learning (Sun et al. (2020)), data augmentation (Siddhant et al. (2020)), transfer learning (Aji (2020)), meta-learning (Li et al. (2020)), pivot-based (Kim et al. (2019)), and multilingual machine translation (Dabre et al. (2020)). A model-agnostic meta-learning algorithm (Finn et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters (Chung et al., 2016), mixed word/characters (Luong and Man1075 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1075–1078 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics ning, 2016), or more intelligent sub-words (Sennrich et al., 2016). It is claimed that an NMT model using such an approach is capable of openvocabulary"
2020.wmt-1.126,W13-2233,0,0.0691305,"Missing"
2020.wmt-1.126,D19-1080,0,0.0233541,"s through bilingual educational scenarios and MT1 . 1 https://minorityrights.org/ minorities/sorbs/ Background NMT is an end-to-end learning system (Bahdanau et al., 2015), based on the data-driven approach of machine translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning (Zheng et al. (2019)), unsupervised learning (Sun et al. (2020)), data augmentation (Siddhant et al. (2020)), transfer learning (Aji (2020)), meta-learning (Li et al. (2020)), pivot-based (Kim et al. (2019)), and multilingual machine translation (Dabre et al. (2020)). A model-agnostic meta-learning algorithm (Finn et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters (Chung et al., 2016), mixed word/characters (Luong and Man1075 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1075–1078 c Online, Nove"
2020.wmt-1.126,P16-1100,0,0.057785,"Missing"
2020.wmt-1.126,N19-4009,0,0.0224293,"of tokens. BPE tries to find a balance between characterand word-level hybrid representations, enabling the encoding of any rare words in the vocabulary with appropriate subword tokens without introducing any “unknown” tokens. These segmented byte sequences are encoded into variable-length tokens, i.e., n-grams, which leads to the generation of the BPE vocabulary with byte n-grams. Before being fed to the Transformer model, the learned BBPE passes through bidirectional GRU, which enables to retain contextualization between byte representation of BPE. 4 Experimental Setup We use the Fairseq2 (Ott et al., 2019) library to train the Transformer with the same learning rate as in the original paper. 4.1 Dataset and Preprocessing Our models were trained on the data provided by the Workshop on Machine Translation (WMT) 2020. The statistics about the training, validation and test sets are 60000, 2000 and 2000, respectively for both directional pairs (HSB - GER). We obtained 1727916 and 1710293 tokens of the GER and HSB, respectively, from the train set for 2 https://github.com/pytorch/fairseq Training Details We trained the Transformer model with Bi-GRU embedding, in which contextualization using the numb"
2020.wmt-1.126,P02-1040,0,0.110126,"s://github.com/pytorch/fairseq Training Details We trained the Transformer model with Bi-GRU embedding, in which contextualization using the number of encoder and decoder layers are 2 with the dropout value 0.3. We trained our model with a batch size of 100, with the aid of Adam optimizer at 0.0005 learning rate. The learning rate has warmup update by 4000 to label smoothed cross-entropy loss function with label-smoothing value 0.1. 5 Results and Analysis The BBPE based Transformer model was evaluated on the blind test set at five different metrics provided by the task organizer, namely BLEU (Papineni et al., 2002), BLEU-cased, TER (Snover et al., 2006), BEER2.0 (Stanojevi´c and Sima’an, 2014), and CharacTER (Wang et al., 2016). The obtained metrics score for each pair to each experiment is specified in Table 1. The prediction of the test set was generated by performing the best validation checkpoint. However, while comparing the BLEU score of the valid set with the test set, we obtained a difference of +3.21 for HSB→GER and +0.15 for GER→HSB pairs. Before submitting the predictions of the test set, the BLEU scores of best and last checkpoints were almost equal, as shown in Table 2. Moreover, the vocabu"
2020.wmt-1.126,P16-1162,0,0.0498636,"et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters (Chung et al., 2016), mixed word/characters (Luong and Man1075 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1075–1078 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics ning, 2016), or more intelligent sub-words (Sennrich et al., 2016). It is claimed that an NMT model using such an approach is capable of openvocabulary translation by encoding rare and unknown words as sequences of sub-word units. The purpose of our experiments was to try out a supervised NMT system for the low resource language like HSB to GER and vice-versa for the WMT20 shared task. preprocessing. The BPE vocabulary, Byte vocabulary and Character vocabulary are 16384, 2048 and 4096, respectively, for generating binary dataset by using fairseq-preprocess. The BBPE used as a subword BPE tokenizer, where preprocessing was performed using lowercasing only. Th"
2020.wmt-1.126,2020.acl-main.252,0,0.0161061,"owy-Rybi´nska, 2018). To counter this, attempts are being made to increase the number of Sorbian speakers through bilingual educational scenarios and MT1 . 1 https://minorityrights.org/ minorities/sorbs/ Background NMT is an end-to-end learning system (Bahdanau et al., 2015), based on the data-driven approach of machine translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning (Zheng et al. (2019)), unsupervised learning (Sun et al. (2020)), data augmentation (Siddhant et al. (2020)), transfer learning (Aji (2020)), meta-learning (Li et al. (2020)), pivot-based (Kim et al. (2019)), and multilingual machine translation (Dabre et al. (2020)). A model-agnostic meta-learning algorithm (Finn et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters (Chung et al., 2016), mixed word/characters (Luong and Man"
2020.wmt-1.126,2006.amta-papers.25,0,0.021267,"etails We trained the Transformer model with Bi-GRU embedding, in which contextualization using the number of encoder and decoder layers are 2 with the dropout value 0.3. We trained our model with a batch size of 100, with the aid of Adam optimizer at 0.0005 learning rate. The learning rate has warmup update by 4000 to label smoothed cross-entropy loss function with label-smoothing value 0.1. 5 Results and Analysis The BBPE based Transformer model was evaluated on the blind test set at five different metrics provided by the task organizer, namely BLEU (Papineni et al., 2002), BLEU-cased, TER (Snover et al., 2006), BEER2.0 (Stanojevi´c and Sima’an, 2014), and CharacTER (Wang et al., 2016). The obtained metrics score for each pair to each experiment is specified in Table 1. The prediction of the test set was generated by performing the best validation checkpoint. However, while comparing the BLEU score of the valid set with the test set, we obtained a difference of +3.21 for HSB→GER and +0.15 for GER→HSB pairs. Before submitting the predictions of the test set, the BLEU scores of best and last checkpoints were almost equal, as shown in Table 2. Moreover, the vocabulary size plays a crucial role in data-"
2020.wmt-1.126,W14-3354,0,0.0673114,"Missing"
2020.wmt-1.126,2020.acl-main.324,0,0.0249276,"is number is continually declining (Dołowy-Rybi´nska, 2018). To counter this, attempts are being made to increase the number of Sorbian speakers through bilingual educational scenarios and MT1 . 1 https://minorityrights.org/ minorities/sorbs/ Background NMT is an end-to-end learning system (Bahdanau et al., 2015), based on the data-driven approach of machine translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning (Zheng et al. (2019)), unsupervised learning (Sun et al. (2020)), data augmentation (Siddhant et al. (2020)), transfer learning (Aji (2020)), meta-learning (Li et al. (2020)), pivot-based (Kim et al. (2019)), and multilingual machine translation (Dabre et al. (2020)). A model-agnostic meta-learning algorithm (Finn et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters (Chung et al.,"
2020.wmt-1.126,W16-2342,0,0.0140728,"tualization using the number of encoder and decoder layers are 2 with the dropout value 0.3. We trained our model with a batch size of 100, with the aid of Adam optimizer at 0.0005 learning rate. The learning rate has warmup update by 4000 to label smoothed cross-entropy loss function with label-smoothing value 0.1. 5 Results and Analysis The BBPE based Transformer model was evaluated on the blind test set at five different metrics provided by the task organizer, namely BLEU (Papineni et al., 2002), BLEU-cased, TER (Snover et al., 2006), BEER2.0 (Stanojevi´c and Sima’an, 2014), and CharacTER (Wang et al., 2016). The obtained metrics score for each pair to each experiment is specified in Table 1. The prediction of the test set was generated by performing the best validation checkpoint. However, while comparing the BLEU score of the valid set with the test set, we obtained a difference of +3.21 for HSB→GER and +0.15 for GER→HSB pairs. Before submitting the predictions of the test set, the BLEU scores of best and last checkpoints were almost equal, as shown in Table 2. Moreover, the vocabulary size plays a crucial role in data-driven approaches of MTs as well. Hence, we have increased the vocabulary si"
2020.wmt-1.126,W19-5427,0,0.0213199,"to 15, 000 speakers (Elle, 2010), although this number is continually declining (Dołowy-Rybi´nska, 2018). To counter this, attempts are being made to increase the number of Sorbian speakers through bilingual educational scenarios and MT1 . 1 https://minorityrights.org/ minorities/sorbs/ Background NMT is an end-to-end learning system (Bahdanau et al., 2015), based on the data-driven approach of machine translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning (Zheng et al. (2019)), unsupervised learning (Sun et al. (2020)), data augmentation (Siddhant et al. (2020)), transfer learning (Aji (2020)), meta-learning (Li et al. (2020)), pivot-based (Kim et al. (2019)), and multilingual machine translation (Dabre et al. (2020)). A model-agnostic meta-learning algorithm (Finn et al., 2017) for low-resource NMT exploits the multilingual high-resource language tasks (Gu et al., 2018b). Gu et al. (2018a) achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segment"
2020.wmt-1.44,W18-2202,0,0.0222221,"ubmitted for Marathi to Hindi and 21 systems submitted for Hindi to Marathi in the shared task. Out of these, our submission ranked 6th and 9th, respectively. 1 2 Introduction In the last decade and a half, neural machine translation (NMT) (Sutskever et al., 2014) has achieved great success in automatically translating human language text, outperforming statistical machine translation (SMT) (Koehn et al., 2003). Both the system require very large corpus sizes to train and evaluate the results. They, however, don’t work very well for low resource data (He et al., 2016; Koehn and Knowles, 2017; Dowling et al., 2018). Translation from or to low resource languages is the major challenges faced by today’s NMT systems. Different methods have been proposed to overcome the data sparsity problem for low resource languages by researchers around the world. These include using monolingual data (Wu et al., 2019), fine-tuning (Miceli Barone et al., 2017) the high resource monolingual and parallel data on low resource data, back translation (Hoang et al., 2018), etc. They succeed up to some extent, but the success is limited, as the reported results show when compared to those for resource rich languages. In this pap"
2020.wmt-1.44,W18-2703,0,0.0171904,"y large corpus sizes to train and evaluate the results. They, however, don’t work very well for low resource data (He et al., 2016; Koehn and Knowles, 2017; Dowling et al., 2018). Translation from or to low resource languages is the major challenges faced by today’s NMT systems. Different methods have been proposed to overcome the data sparsity problem for low resource languages by researchers around the world. These include using monolingual data (Wu et al., 2019), fine-tuning (Miceli Barone et al., 2017) the high resource monolingual and parallel data on low resource data, back translation (Hoang et al., 2018), etc. They succeed up to some extent, but the success is limited, as the reported results show when compared to those for resource rich languages. In this paper, we use the Transformer networkbased NMT system (Vaswani et al., 2017) because it is among the state of the art models for machine Similar Languages Two languages are considered similar or closely related if they are close relatives in terms of the linguistic family of the linguistic family tree (or forest), or if the speakers of the two languages are in close contact over a long period of time. Contact over a long period leads to the"
2020.wmt-1.44,D10-1092,0,0.0535377,"Missing"
2020.wmt-1.44,W17-3204,0,0.0200672,"e a total of 23 systems submitted for Marathi to Hindi and 21 systems submitted for Hindi to Marathi in the shared task. Out of these, our submission ranked 6th and 9th, respectively. 1 2 Introduction In the last decade and a half, neural machine translation (NMT) (Sutskever et al., 2014) has achieved great success in automatically translating human language text, outperforming statistical machine translation (SMT) (Koehn et al., 2003). Both the system require very large corpus sizes to train and evaluate the results. They, however, don’t work very well for low resource data (He et al., 2016; Koehn and Knowles, 2017; Dowling et al., 2018). Translation from or to low resource languages is the major challenges faced by today’s NMT systems. Different methods have been proposed to overcome the data sparsity problem for low resource languages by researchers around the world. These include using monolingual data (Wu et al., 2019), fine-tuning (Miceli Barone et al., 2017) the high resource monolingual and parallel data on low resource data, back translation (Hoang et al., 2018), etc. They succeed up to some extent, but the success is limited, as the reported results show when compared to those for resource rich"
2020.wmt-1.44,N03-1017,0,0.213777,"Missing"
2020.wmt-1.44,D19-5222,1,0.742486,"Missing"
2020.wmt-1.44,D17-1156,0,0.0125711,"e text, outperforming statistical machine translation (SMT) (Koehn et al., 2003). Both the system require very large corpus sizes to train and evaluate the results. They, however, don’t work very well for low resource data (He et al., 2016; Koehn and Knowles, 2017; Dowling et al., 2018). Translation from or to low resource languages is the major challenges faced by today’s NMT systems. Different methods have been proposed to overcome the data sparsity problem for low resource languages by researchers around the world. These include using monolingual data (Wu et al., 2019), fine-tuning (Miceli Barone et al., 2017) the high resource monolingual and parallel data on low resource data, back translation (Hoang et al., 2018), etc. They succeed up to some extent, but the success is limited, as the reported results show when compared to those for resource rich languages. In this paper, we use the Transformer networkbased NMT system (Vaswani et al., 2017) because it is among the state of the art models for machine Similar Languages Two languages are considered similar or closely related if they are close relatives in terms of the linguistic family of the linguistic family tree (or forest), or if the speakers o"
2020.wmt-1.44,P02-1040,0,0.106413,"Missing"
2020.wmt-1.44,2006.amta-papers.25,0,0.132977,"Missing"
2020.wmt-1.44,D19-1430,0,0.0210859,"utomatically translating human language text, outperforming statistical machine translation (SMT) (Koehn et al., 2003). Both the system require very large corpus sizes to train and evaluate the results. They, however, don’t work very well for low resource data (He et al., 2016; Koehn and Knowles, 2017; Dowling et al., 2018). Translation from or to low resource languages is the major challenges faced by today’s NMT systems. Different methods have been proposed to overcome the data sparsity problem for low resource languages by researchers around the world. These include using monolingual data (Wu et al., 2019), fine-tuning (Miceli Barone et al., 2017) the high resource monolingual and parallel data on low resource data, back translation (Hoang et al., 2018), etc. They succeed up to some extent, but the success is limited, as the reported results show when compared to those for resource rich languages. In this paper, we use the Transformer networkbased NMT system (Vaswani et al., 2017) because it is among the state of the art models for machine Similar Languages Two languages are considered similar or closely related if they are close relatives in terms of the linguistic family of the linguistic fam"
2020.wnut-1.60,C18-1139,0,0.0225142,"al. (2020) proposed deep learning models to predict the number of COVID-19 positive cases in 32 states of India and its Union Territories. Using RNN based LSTM cells and its variants such as deep LSTM, convolutional LSTM, and bi-directional LSTM as predictive models led to the conclusion that at present Bi-directional LSTM gives the best results, and convolutional LSTM gives the lowest results based on prediction errors. Contextual embeddings are known to provide better results on the basic NLP problems such as Sentiment analysis (Wang et al., 2020; M¨uller et al., 2020; Kruspe et al., 2020; Akbik et al., 2018). In this paper, we made an effort to automatically identify whether a COVID-19 English tweet is informative or not, using a model based on a contextual embedding called ELMo (Peters et al., 2018). The dataset of 10K COVID-19 English tweets was provided by the organizers of the shared task. 3 Problem Statement The goal of WNUT-2020 Task 2, named as the “Identification of informative COVID-19 English tweets”, is to classify COVID-19 tweets into informative and uninformative categories. This is a binary classification task to learn F : X → Y where X = {X1 , X2 , X3 . . . , Xm } is a tweet of len"
2020.wnut-1.60,N19-4010,0,0.0392214,"Missing"
2020.wnut-1.60,W14-4012,0,0.0832117,"Missing"
2020.wnut-1.60,2020.nlpcovid19-acl.14,0,0.0269226,"Missing"
2020.wnut-1.60,N18-1202,0,0.0240692,"p LSTM, convolutional LSTM, and bi-directional LSTM as predictive models led to the conclusion that at present Bi-directional LSTM gives the best results, and convolutional LSTM gives the lowest results based on prediction errors. Contextual embeddings are known to provide better results on the basic NLP problems such as Sentiment analysis (Wang et al., 2020; M¨uller et al., 2020; Kruspe et al., 2020; Akbik et al., 2018). In this paper, we made an effort to automatically identify whether a COVID-19 English tweet is informative or not, using a model based on a contextual embedding called ELMo (Peters et al., 2018). The dataset of 10K COVID-19 English tweets was provided by the organizers of the shared task. 3 Problem Statement The goal of WNUT-2020 Task 2, named as the “Identification of informative COVID-19 English tweets”, is to classify COVID-19 tweets into informative and uninformative categories. This is a binary classification task to learn F : X → Y where X = {X1 , X2 , X3 . . . , Xm } is a tweet of length m and Y ∈ {inf ormative, uninf ormative}. 4 System Description Word embeddings or distributed representations of words use dense, real-valued vectors to represent vocabulary words. Word embedd"
agarwal-etal-2012-gui,ambati-etal-2010-high,1,\N,Missing
agarwal-etal-2012-gui,singh-ambati-2010-integrated,1,\N,Missing
agarwal-etal-2012-gui,E03-1068,0,\N,Missing
agarwal-etal-2012-gui,W09-3036,0,\N,Missing
agarwal-etal-2012-gui,P05-1040,0,\N,Missing
agarwal-etal-2012-gui,W00-1907,0,\N,Missing
agarwal-etal-2012-gui,P11-2060,0,\N,Missing
agarwal-etal-2012-gui,W11-3405,1,\N,Missing
agarwal-etal-2012-gui,I08-2141,1,\N,Missing
agarwal-etal-2012-gui,singh-2012-concise,1,\N,Missing
C18-1247,W17-5229,0,0.0169348,"able. We choose it over other pre-trained models like Google word2vec (Mikolov et al., 2013) and the GloVe (Pennington et al., 2014) Twitter model. This was backed by cross validation results (not shown for brevity). Preprocessing: Before forming word vector based embeddings of the Twitter tweets, we employed some preprocessing steps including removal of URLs and user mentions, stripping punctuations from word boundaries, hashtag segmentation (‘#wearethebest’ to ‘we are the best’ using the Word Segment 4 module in Python) and elongation removal (‘goooooood’ to ‘good’) (inspired by the work in Akhtar et al. (2017)). Such preprocessing helped in improving cross-validation performance. Network Hyper-Parameters and Architecture Settings: We show the final hyper-parameter values for the LE-PC-DNN (section 2), and the multi-task models LE-PC-DMTL and LE-PC-DMTL-EI (section 3) in Table 2. These values were chosen on the basis of 7-fold cross validation results on the combined training and validation sets. We changed various network hyper-parameters like number of layers, output dimensionality, the type of pooling for CNNs (max versus average) and dropout value. 4 https://github.com/grantjenks/wordsegment 291"
C18-1247,H05-1073,0,0.119819,"show how the various components within our models contribute, to help guide the design of future systems for our task as well as other, related tasks. d) Experiments to gauge the correlation between different pairs of emotion and an effort to link our observations with past findings in linguistic and psychological experiments. e) Error analysis to assess reasons for erroneous intensity prediction in tweets. The code for all our approaches is publicly available 1 . Most datasets and systems focus on emotion classification or detecting the presence or absence of an emotion (Brooks et al., 2013; Alm et al., 2005; Aman and Szpakowicz, 2007; Bollen et al., 2011; Wang et al., 2016). To the best of our knowledge, apart from the dataset used for conducting the EmoInt shared task, there is only one resource annotated for the intensity of emotion in text, which was released by Strapparava and Mihalcea (2007) as part of SemEval 2007. Annotators gave a score between 0 and 100 for the degree of emotion in newspaper headlines using a slide bar in a web interface. We perform our experiments on a Twitter dataset (Mohammad and Bravo-Marquez, 2017a) (Table 4). Users from many backgrounds express emotions via tweets"
C18-1247,W15-4319,0,0.0671581,"Missing"
C18-1247,W15-2915,0,0.0667958,"Missing"
C18-1247,D17-1169,0,0.116161,"istic information in our network, and use transfer learning by incorporating ∗ The two first authors have equal contributions to the paper This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2914 Proceedings of the 27th International Conference on Computational Linguistics, pages 2914–2926 Santa Fe, New Mexico, USA, August 20-26, 2018. activations of a pre-trained convolutional neural network which uses emojis to learn representations for related tasks of sentiment and emotion detection in tweets (Felbo et al., 2017). Our model achieves state of the art performance on the EmoInt dataset. b) Two multi-task neural network based models which successfully handle all emotions jointly, as opposed to having a separate network for each individual emotion. Our best multi-task learning based approach performs relatively well when compared to other approaches and previous systems while benefiting from better generalization, speed and lesser trainable network parameters. c) A comprehensive set of ablation tests to show how the various components within our models contribute, to help guide the design of future systems"
C18-1247,S15-2080,0,0.0236421,"xt Outside the tweet: Tweet #3 requires common sense or world knowledge to know that an important meeting drawing near is generating anxiety with high intensity. This fear or anxiety is not obvious from the text itself which might explain the predicted intensity of 0.291 when the actual intensity is 0.667. Tweet #4 suffers from a similar problem. World knowledge is required to know the relationship between ‘Ibiza blues’ and sadness. • Metaphors: Metaphorical expressions, like ‘crying with laughter’ in Tweet #1 or ‘Ibiza blues’ in Tweet #4, pose a significant challenge to computational models (Ghosh et al., 2015). We carried out the above analysis after concluding all the experiments. The number of tweets where the predicted intensity was higher than actual and vice-versa were almost equal for any emotion. Hence, our system is not biased towards predicting higher or lower intensities than the annotated ones. 8 Conclusion & Future Work We proposed various deep neural architectures for the task of emotion intensity prediction on microblogging data, specifically, Twitter data. This problem was recently brought to light by the EmoInt shared task (Mohammad and Bravo-Marquez, 2017b). Combining fully-connect"
C18-1247,W15-4322,0,0.0513319,"Missing"
C18-1247,W17-5207,1,0.90619,"s as PC-DNN (Parallel Combination of Deep Neural Networks). As it (2) (2) incorporates Linguistic features (layer lc ) and Emoji-based pretrained CNN activations (layer ld ) as well, we will refer to the network discussed in this section as LE-PC-DNN (Figure 1). 3 Deep Multi-Task Learning (DMTL): Handling all Emotions in a Unified Architecture Multi-task Learning (Caruna, 1993; Caruana, 1998) has resulted in successful systems for various NLP tasks (Collobert and Weston, 2008), especially in cross-lingual settings (Huang et al., 2013). MTL was first applied to Emotion Intensity prediction in (Goel et al., 2017). In their neural architecture, the initial network layers are shared across multiple emotions upto a certain point, after which the architecture gets diverged for each emotion. The objective is to jointly train on different emotion datasets such that initial layers increase generalization and the individual final layers can learn task specific features. The network input features are the concatenation of average word embeddings across the length of the tweet and linguistic features (using Tweet2LexiconFeatureExtractor), and the neural layers were fully-connected. 3.1 LE-PC-DMTL for Emotion In"
C18-1247,W17-5206,0,0.0494169,"Missing"
C18-1247,N16-1062,0,0.0142294,", l(6) ) (3) (3) (2) (d) The layer (l(4) ) is obtained by concatenating the feature activations of layers la , lb , lc , &ld . l(4) is connected to l(5) , which is a fully-connected layer. Additionally, a linear hidden layer (l(6) ) with lesser number of neurons is introduced on top of l(5) . 2.4 Output Layer (ˆ y) Finally, we use a sigmoid neuron after l(6) to compute the intensity score yˆ of between 0 and 1. It is quite common to have fully connected layers or small feedforward neural networks connected in a sequential or hierarchical manner to sequence-to-sequence models like CNNs/LSTMs (Lee and Dernoncourt, 2016; Plahl et al., 2013). The feedforward networks in this case take in the pooled output of CNNs/LSTMs as input. Our architecture (Figure 1), however, combines feedforward, convolutional (3) layers and manually extracted features in a non-hierarchical manner (layer la ) as well as in hierarchical (3) fashion (layer lb ). Such a non-sequential combination of fully-connected and convolutional networks is novel for NLP tasks, to the best of our knowledge (though this has been used by the vision community (Antol et al., 2015; He et al., 2017; Vijayanarasimhan et al., 2017)). We call this neural fram"
C18-1247,W17-5205,0,0.0729315,"y or happiness. The intensity of anger expressed in a grievance can be used to automatically decide the priority of addressing complaints. A possible example scenario: based on the tweet ‘X box one controller jockey update is the fucking worst! #fuming’ expressing a higher intensity of anger than the tweet ‘New madden bundle code for x box one is stupid, takes forever to download lol’, may point to the need of prioritizing addressing issues with the new jockey stick before looking into the madden bundle of the same product (an Xbox One). The WASSA’17 workshop conducted the EmoInt shared task (Mohammad and Bravo-Marquez, 2017b) where given a tweet and the emotion it exhibits, systems had to predict the intensity of this given emotion as a real valued score between 0 and 1. We use the shared task setting and the dataset they provide to develop and evaluate all our approaches. The main contributions of our work are: a) A neural architecture for emotion intensity detection which combines convolutional layers with fully connected layers in a non-sequential or ‘parallel’ fashion. Such a combination of these neural models or layers has been explored in computer vision (Antol et al., 2015; Vijayanarasimhan et al., 2017)"
C18-1247,D14-1162,0,0.0810155,"ensity. Details regarding creation and annotation can be found in Mohammad and Bravo-Marquez (2017a). 4.2 Implementation Details Word Embeddings: We use publicly available pre-trained word embeddings called the Twitter word2vec model (Godin et al., 2015). These were trained on 400 million tweets for the ACL-WNUT 2015 shared task (Baldwin et al., 2015) using the word2vec approach (Mikolov et al., 2013). The large number of tweets in training data makes it a better choice than others available. We choose it over other pre-trained models like Google word2vec (Mikolov et al., 2013) and the GloVe (Pennington et al., 2014) Twitter model. This was backed by cross validation results (not shown for brevity). Preprocessing: Before forming word vector based embeddings of the Twitter tweets, we employed some preprocessing steps including removal of URLs and user mentions, stripping punctuations from word boundaries, hashtag segmentation (‘#wearethebest’ to ‘we are the best’ using the Word Segment 4 module in Python) and elongation removal (‘goooooood’ to ‘good’) (inspired by the work in Akhtar et al. (2017)). Such preprocessing helped in improving cross-validation performance. Network Hyper-Parameters and Architectur"
C18-1247,S07-1013,0,0.0540016,"ngs in linguistic and psychological experiments. e) Error analysis to assess reasons for erroneous intensity prediction in tweets. The code for all our approaches is publicly available 1 . Most datasets and systems focus on emotion classification or detecting the presence or absence of an emotion (Brooks et al., 2013; Alm et al., 2005; Aman and Szpakowicz, 2007; Bollen et al., 2011; Wang et al., 2016). To the best of our knowledge, apart from the dataset used for conducting the EmoInt shared task, there is only one resource annotated for the intensity of emotion in text, which was released by Strapparava and Mihalcea (2007) as part of SemEval 2007. Annotators gave a score between 0 and 100 for the degree of emotion in newspaper headlines using a slide bar in a web interface. We perform our experiments on a Twitter dataset (Mohammad and Bravo-Marquez, 2017a) (Table 4). Users from many backgrounds express emotions via tweets. Increasing activity of bots on Twitter (Gilani et al., 2017) and companies aiming to accurately respond to tweets about their products or services makes Twitter an attractive domain for sentiment analysis (Fan and Gordon, 2014; Nakov et al., 2016). Twenty two systems participated in the EmoIn"
D19-5222,W13-3520,0,0.0480271,"Missing"
D19-5222,W18-6459,0,0.0200664,"Proceedings of the 6th Workshop on Asian Translation, pages 171–174 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics nation of language model as well as a translation model with decoding algorithms. On the the other hand, EBMT uses available translated examples to perform translation based on analogies. This is executed by detecting examples that coincide with the input. Then the alignment is performed to locate those parts of the translation that can be reused. Neural Machine Translation (NMT) (Sutskever et al., 2014) came into the prominence around 2014. (Choudhary et al., 2018) train an NMT model using pre-trained word-embedding (Al-Rfou’ et al., 2013) along with subword units using Byte-Pair-Encoding (BPE) (Sennrich et al., 2015). Several models have been trained on various datasets and have given promising results. Hybrid-based MT (Simov et al., 2016) is the combination of rule-based methods and any of the data-driven approaches. Our paper describes experiments on using the transformer architecture (Vaswani et al., 2017) that we tried with English and Tamil language pair and it achieves a better result than the shared task baseline. 3 due to complex morphology, bu"
D19-5222,W16-0604,0,0.0177079,"ted examples to perform translation based on analogies. This is executed by detecting examples that coincide with the input. Then the alignment is performed to locate those parts of the translation that can be reused. Neural Machine Translation (NMT) (Sutskever et al., 2014) came into the prominence around 2014. (Choudhary et al., 2018) train an NMT model using pre-trained word-embedding (Al-Rfou’ et al., 2013) along with subword units using Byte-Pair-Encoding (BPE) (Sennrich et al., 2015). Several models have been trained on various datasets and have given promising results. Hybrid-based MT (Simov et al., 2016) is the combination of rule-based methods and any of the data-driven approaches. Our paper describes experiments on using the transformer architecture (Vaswani et al., 2017) that we tried with English and Tamil language pair and it achieves a better result than the shared task baseline. 3 due to complex morphology, but size of the training data is limited. Hence, to deal with Indic corpora, we decided to use a vocabulary size of 5,000 symbols for source and target byte-pair encoding, respectively. 3.3 We trained two models, namely, Tamil – English and English – Tamil. For training the model, W"
D19-5222,D10-1092,0,0.0605206,"ing space. The embedding dimensions of encoder and decoder in the feed-forward network are set to 2048. The number of encoder and decoder attention heads are set to 2. The models are regularized with dropout, label smoothing and weight decay, with the corresponding hyper-parameters being set to 0.4, 0.2 and 0.0001, respectively. Models are optimized with Adam using β1 = 0.9 and β2 = 0.98. We perform the experiments on an Nvidia Titan Xp GPU. System Description This section covers the dataset, preprocessing, and the experimental setup required for our systems. 3.1 4 Results and Analysis RIBES (Isozaki et al., 2010), BLEU (Papineni et al., 2002), and AM-FM (Banchs et al., 2015) scores of our submitted systems are shown in Table 1, Table 2, and Table 3 resepectively. WAT 2019 organizers evaluate all the submitted system using Adequacy, BLEU, RIBES, and AM-FM scores, as shown in Figure 1 and Figure 2. It is known that Tamil and English follow different word orders, therefore we have to focus on word order for translation. On considering word order, our system performs well on RIBES metric, as shown in Figure 2. If we go through AM-FM score in Figure 2, our system still works well, keeping in view the prese"
D19-5222,P02-1040,0,0.108113,"nsions of encoder and decoder in the feed-forward network are set to 2048. The number of encoder and decoder attention heads are set to 2. The models are regularized with dropout, label smoothing and weight decay, with the corresponding hyper-parameters being set to 0.4, 0.2 and 0.0001, respectively. Models are optimized with Adam using β1 = 0.9 and β2 = 0.98. We perform the experiments on an Nvidia Titan Xp GPU. System Description This section covers the dataset, preprocessing, and the experimental setup required for our systems. 3.1 4 Results and Analysis RIBES (Isozaki et al., 2010), BLEU (Papineni et al., 2002), and AM-FM (Banchs et al., 2015) scores of our submitted systems are shown in Table 1, Table 2, and Table 3 resepectively. WAT 2019 organizers evaluate all the submitted system using Adequacy, BLEU, RIBES, and AM-FM scores, as shown in Figure 1 and Figure 2. It is known that Tamil and English follow different word orders, therefore we have to focus on word order for translation. On considering word order, our system performs well on RIBES metric, as shown in Figure 2. If we go through AM-FM score in Figure 2, our system still works well, keeping in view the preservation of semantic meaning an"
D19-5222,W12-5611,0,0.49077,"Missing"
F13-2028,abekawa-etal-2010-community,0,0.0475848,"Missing"
F13-2028,W12-3102,0,0.076723,"Missing"
F13-2028,2012.eamt-1.60,0,0.0681271,"Missing"
F13-2028,2004.tmi-1.8,0,0.0596356,"Missing"
F13-2028,S10-1003,0,0.100773,"Missing"
F13-2028,potet-etal-2012-collection,0,0.042695,"Missing"
F13-2028,2006.amta-papers.25,0,0.1378,"Missing"
F13-2028,W12-3120,1,0.885309,"Missing"
I08-1009,W06-1109,1,0.820931,"ing input mechanisms, instead of just for transliteration of NEs etc. for MT or CLIR. One reason for very high variation in the latter case is that unlike Romaji for Japanese (which is taught in 722,000 19,800 10,500 5,490 665 133 102 Table 1: Variations of a Hindi Word nOkarI (job). The numbers are pages returned when searching on Google. 1 2 www.google.co.in/press/pressrel/news transliteration.html www.quillpad.com 65 rest are pruned. Now we have two probability distributions which can be compared by a measure of distributional similarity. The measure used is symmetric cross entropy or SCE (Singh, 2006a). Since the accuracy of identification is low if test data is very low, which is true in our case because we are trying to identify the class of a single word, we had to extend the method used by Singh. One major extension was that we add word beginning and ending markers to all the words in training as well as test data. This is because n-grams at beginning, middle and end of words should be treated differently if we want to identify the ‘language’ (or class) of the word. For every given word, we get a probability about its origin based on SCE. Based on this probability measure, translitera"
I08-1009,W05-0808,0,0.0946774,"factor in achieving higher accuracy in transliteration. 1 Introduction Transliteration is a crucial factor in Cross Lingual Information Retrieval (CLIR). It is also important for Machine Translation (MT), especially when the languages do not use the same scripts. It is the process of transforming a word written in a source language into a word in a target language without the aid of a resource like a bilingual dictionary. Word pronunciation is usually preserved or is modified according to the way the word should be pronounced in the target language. In simple terms, it means 64 Aswani et. al (Aswani and Gaizauskas, 2005) have used a transliteration similarity mechanism to align English-Hindi parallel texts. They used character based direct correspondences between Hindi and English to produce possible transliterations. Then they apply edit distance based similarity to select the most probable transliteration in the English text. However, such method can only be appropriate for aligning parallel texts as the number of possible candidates is quite small. The paper is structured as follows. In Section2, we discuss the problem of a high degree of variation in Indian words, especially when written in Latin script."
I08-1009,I08-2141,1,0.868294,"Missing"
I08-1009,P07-1015,0,0.134087,"babilities of all trigrams to belong to a particular language as an measure to disambiguate word origins. We use a more sophisticated method that has been successfully used for language and encoding identification (Singh, 2006a). We first prepare letter based 5-gram models from the lists of two kinds of words (Indian and foreign). Then we combine n-grams of all orders and rank them according to their probability in descending order. Only the top N n-grams are retained and the 66 ture) in ILs. Similar observations have been made about Hindi by Su-Youn Yoon, Kyoung-Young Kim and Richard Sproat (Yoon et al., 2007). We have prepared our own mappings with help from native speakers of the languages concerned, which is relatively quite a simple task since the letters in Indic scripts correspond closely with phonemes. 6 Transliteration of Indian Words These words include (mainly Indian) named entities of (e.g. Taj Mahal, Manmohan Singh) and common vocabulary words (common nouns, verbs) which need to be transliterated. They also include words which are spelled similar to the way Indian words are spelled when written in Latin (e.g. Baghdad, Husain). As stated earlier, this class of words are much more relevan"
I08-1009,C02-1099,0,\N,Missing
I08-1009,P06-1143,0,\N,Missing
I08-1009,Y06-1050,0,\N,Missing
I08-2141,W06-1109,1,0.917806,"), when the user views the list of fonts. Of course, we can also give the user the option to see all the fonts installed on the system. 2.2 Language-Encoding Identification Another important element of the design is a language-encoding identification tool that is integrated into the language-encoding support module so that if the user opens a file and does not know the language or encoding of the text, the tool can automatically identify the language-encoding of the text. The language-encoding identification tool is based on byte based n-gram models using a distributional similarity measures (Singh, 2006a). This tools is computationally quite a light one as the amount of 959 data required for training is very small and it has been found to be one of the most accurate languageencoding systems currently available. The user can make it even faster by removing those languageencodings which she may not be interested in. This will require only a change in the relevant properties file. 2.3 Encoding Conversion There is also a wrapper module for calling any installed or built in encoding converter for languages which use more than one encodings. The user can easily convert the encoding of the text dep"
I08-2141,I08-1009,1,0.826182,"ns Interfaces The editor is built into Sanchay in such a way that it is possible to open different views of a document, depending on the annotation format. For example, if the currently opened document is in SSF format, then the same document can be opened in the Sanchay Syntactic Annotation Interface just by clicking on a button or a context menu item. The opposite is also possible, i.e., if a document is open in the Syntactic Annotation Interface, then it can be directly opened into the Sanchay Editor as a simple text file. 3.5 to be added is a more discerning mechanism for transliteration (Surana and Singh, 2008). The first important idea in this mechanism is to use different methods for transliteration based on the word origin (identified using a modified version of the languageencoding tool). The second major idea is to use fuzzy text matching for selecting the best match. This method also has outperformed other methods. There is a plan to extend the editor to allow direct annotation. We will begin by providing support for discourse annotation and other similar annotations. 5 Conclusions Some Other Facilities Apart from the above mentioned facilities, Sanchay Editor also has the usual facilities ava"
I08-3004,I08-3017,0,0.0494557,"Missing"
I08-3004,I08-3011,0,0.0419366,"Missing"
I08-3004,I08-3014,0,0.0610757,"Missing"
I08-3004,I08-3006,0,0.0362205,"Missing"
I08-3004,I08-3001,0,0.0306247,"of papers submitted (and selected) was pleasantly surprising. The workshop includes paper on topics as diverse as Machine Translation (MT) from text to sign language (an L-language on which very few people have worked) to MT from speech to speech. And from segmentation and stemming to parser adaptation. Also, from input methods, text editor and interfaces to part of speech (POS) tagger. The variety is also remarkable in terms of the languages covered and research locations. In addition, the workshop includes three invited talks: the first on building language resources by resource adaptation (David and Maxwell, 2008); the second on cross-language resource sharing (Sornlertlamvanich, 2008b); and the third on breaking the Zipfian barrier in NLP (Choudhury, 2008). It can be said that the workshop has been a moderate success. We hope it will stimulate further work in this direction. 7 An Overview of the Papers We noted above that resource adaptation needs a lot more study. In one of the papers at the workshop, Zeman and Resnik presented their work on cross-language parser adaptation between related languages, which can be highly relevant for the L-languages in ‘linguistic areas’ (Emeneau, 1956; Emeneau, 1980)"
I08-3004,I08-3020,0,0.030503,"Missing"
I08-3004,I08-3009,0,0.0363013,"Missing"
I08-3004,I08-3010,0,0.0590803,"Missing"
I08-3004,I08-3007,0,0.0292871,"Missing"
I08-3004,I08-2136,0,0.029537,"rity Languages, 1998, Spain. • Projects supported by ELRA on the Basic Language Resource Kit (BLARK) that targets the specifications of a minimal kits for each language to support NLP tools development3 . • There is also a corresponding project at LDC (the Less Commonly Taught Languages4 ). • The IJCNLP Workshop on Named Entity Recognition for South and South Asian Languages5 . This list is, of course, not exhaustive. There are many papers relevant to the theme of this workshop at the IJCNLP 2008 main conference6 , as at some previous major conferences. There is also a very relevant tutorial (Mihalcea, 2008) at the IJCNLP 2008 conference about building resources and tools for languages with scarce resources. Even the industry is realizing the importance of providing computing support for some of the Llanguages. In the last few years there have been many announcements about the addition of some 3 http://www.elda.org/blark http://projects.ldc.upenn.edu/LCTL 5 http://ltrc.iiit.ac.in/ner-ssea-08/ 6 http://ijcnlp2008.org 4 such language to a product or a service and also of the addition of better facilities (input methods, transliteration, search) in an existing product or service for some L-language."
I08-3004,I08-3013,0,0.0522531,"Missing"
I08-3004,I08-3019,0,0.042869,"Missing"
I08-3004,I08-3018,0,0.0575329,"Missing"
I08-3004,I08-3012,0,0.0278334,"Missing"
I08-3004,I08-3015,0,0.0663965,"Missing"
I08-3004,I08-3016,0,0.0548948,"Missing"
I08-3004,I08-3008,0,0.0302469,"Missing"
I08-3004,I08-3003,0,\N,Missing
I08-3004,I08-3005,0,\N,Missing
I08-5003,W03-0423,0,0.0305681,"roblem for SSEA languages. However, for comparing the results of these different methods, we will need a reasonably good baseline. A mature system tuned for English but trained on SSEA language data can become such a baseline. We will describe such a baseline in a later section. This baseline system has been tested on the data provided for the shared task. We present the results for all five languages under the settings required for the shared task. 2 Related Work Various techniques have been used for solving the NER problem (Mikheev et al., 1999; Borthwick, 1999; Cucerzan and Yarowsky, 1999; Chieu and Ng, 2003; Klein et al., 2003; Kim and Woodland, 2000) ranging from naively using gazetteers to rules based techniques to purely statistical techniques, even hybrid approaches. Several workshops consisting of shared tasks (Sang, 2002; Sang and De Meulder, 2003) have been held with specific focus on this problem. In this section we will mention some of techniques used previously. Most of the approaches can be classified based on the features they use, whether they are rule based or machine learning based or hybrid approaches. Some of the commonly used features are: • Word form and part of speech (POS) t"
I08-5003,W99-0612,0,0.224023,"methods for solving the NER problem for SSEA languages. However, for comparing the results of these different methods, we will need a reasonably good baseline. A mature system tuned for English but trained on SSEA language data can become such a baseline. We will describe such a baseline in a later section. This baseline system has been tested on the data provided for the shared task. We present the results for all five languages under the settings required for the shared task. 2 Related Work Various techniques have been used for solving the NER problem (Mikheev et al., 1999; Borthwick, 1999; Cucerzan and Yarowsky, 1999; Chieu and Ng, 2003; Klein et al., 2003; Kim and Woodland, 2000) ranging from naively using gazetteers to rules based techniques to purely statistical techniques, even hybrid approaches. Several workshops consisting of shared tasks (Sang, 2002; Sang and De Meulder, 2003) have been held with specific focus on this problem. In this section we will mention some of techniques used previously. Most of the approaches can be classified based on the features they use, whether they are rule based or machine learning based or hybrid approaches. Some of the commonly used features are: • Word form and pa"
I08-5003,I08-5008,0,0.0661306,"ndhi International Hindi University’. This would be translated in Hindi as mahaatmaa gaandhii antarraashtriya hindii vishvavidyaalaya. Only ‘International’ and ‘University’ are to be translated, while the other words are to be transliterated. The nested named entities in this case are: ‘Mahatma’ (NETO), ‘Gandhi’ (NEP), ‘Mahatma Gandhi’ (NEP), and ‘Mahatma Gandhi International Hindi University’ (NEO). 6 Named Entity Annotated Corpus For Hindi, Oriya and Telugu, all the annotation was performed at IIIT, Hyderabad. For Bengali, the corpus was developed at IIIT, Hyderabad and Jadavpur University (Ekbal and Bandyopadhyay, 2008b), Calcutta. For Urdu, annotation was performed at CRULP, Lahore (Hussain, 2008) and IIIT, Allahabd. Even though all the annotation was done by native speakers of respective languages, named entity annotation was a new task for everyone involved. This was because of practical constraints as explained in an earlier section. The corpus was divided into two parts, one for training and one for testing. The testing corpus was annotated with nested named entities, while the training corpus was only annotated with ‘maximal’ named entities. Since different teams were working on different languages, i"
I08-5003,I08-7001,0,0.0289792,"ndhi International Hindi University’. This would be translated in Hindi as mahaatmaa gaandhii antarraashtriya hindii vishvavidyaalaya. Only ‘International’ and ‘University’ are to be translated, while the other words are to be transliterated. The nested named entities in this case are: ‘Mahatma’ (NETO), ‘Gandhi’ (NEP), ‘Mahatma Gandhi’ (NEP), and ‘Mahatma Gandhi International Hindi University’ (NEO). 6 Named Entity Annotated Corpus For Hindi, Oriya and Telugu, all the annotation was performed at IIIT, Hyderabad. For Bengali, the corpus was developed at IIIT, Hyderabad and Jadavpur University (Ekbal and Bandyopadhyay, 2008b), Calcutta. For Urdu, annotation was performed at CRULP, Lahore (Hussain, 2008) and IIIT, Allahabd. Even though all the annotation was done by native speakers of respective languages, named entity annotation was a new task for everyone involved. This was because of practical constraints as explained in an earlier section. The corpus was divided into two parts, one for training and one for testing. The testing corpus was annotated with nested named entities, while the training corpus was only annotated with ‘maximal’ named entities. Since different teams were working on different languages, i"
I08-5003,I08-5006,0,0.126726,"Missing"
I08-5003,I08-5002,0,0.0268188,"national Hindi University’. This would be translated in Hindi as mahaatmaa gaandhii antarraashtriya hindii vishvavidyaalaya. Only ‘International’ and ‘University’ are to be translated, while the other words are to be transliterated. The nested named entities in this case are: ‘Mahatma’ (NETO), ‘Gandhi’ (NEP), ‘Mahatma Gandhi’ (NEP), and ‘Mahatma Gandhi International Hindi University’ (NEO). 6 Named Entity Annotated Corpus For Hindi, Oriya and Telugu, all the annotation was performed at IIIT, Hyderabad. For Bengali, the corpus was developed at IIIT, Hyderabad and Jadavpur University (Ekbal and Bandyopadhyay, 2008b), Calcutta. For Urdu, annotation was performed at CRULP, Lahore (Hussain, 2008) and IIIT, Allahabd. Even though all the annotation was done by native speakers of respective languages, named entity annotation was a new task for everyone involved. This was because of practical constraints as explained in an earlier section. The corpus was divided into two parts, one for training and one for testing. The testing corpus was annotated with nested named entities, while the training corpus was only annotated with ‘maximal’ named entities. Since different teams were working on different languages, i"
I08-5003,W98-1118,0,0.0444451,"Othello New Delhi, Paris 3rd September, 1991 (ambiguous) 3.14, 4,500 Rs. 4,500, 5 kg Maximum Entropy, Archeology Table 1: The named entity tagset used for the shared task • Affixes like Hyderabad, Mehdipatnam, Lingampally Rampur, • Gazetteer features: class in the gazetteer • Left and right context • Token length, e.g. the number of letters in a word • Previous history in the document or the corpus • Classes of preceding NEs The machine learning techniques tried for NER include the following: • Hidden Markov Models or HMM (Zhou and Su, 2001) • Decision Trees (Isozaki, 2001) • Maximum Entropy (Borthwick et al., 1998) • Support Vector Machines or SVM (Takeuchi and Collier, 2002) • Conditional Random Fields or CRF (Settles, 2004) Different ways of classifying named entities have been used, i.e., there are more than one tagsets for NER. For example, the CoNLL 2003 shared task2 had only four tags: persons, locations, organizations 2 and miscellaneous. On the other hand, MUC-63 has a near ontology for information extraction purposes. In this (MUC-6) tagset, there are three4 main kinds of NEs: ENAMEX (persons, locations and organizations), TIMES (time expressions) and NUMEX (number expresssions). There has been"
I08-5003,I08-5011,0,0.0197175,"Missing"
I08-5003,I08-5013,0,0.27941,"Missing"
I08-5003,I08-7017,0,0.0132725,"tarraashtriya hindii vishvavidyaalaya. Only ‘International’ and ‘University’ are to be translated, while the other words are to be transliterated. The nested named entities in this case are: ‘Mahatma’ (NETO), ‘Gandhi’ (NEP), ‘Mahatma Gandhi’ (NEP), and ‘Mahatma Gandhi International Hindi University’ (NEO). 6 Named Entity Annotated Corpus For Hindi, Oriya and Telugu, all the annotation was performed at IIIT, Hyderabad. For Bengali, the corpus was developed at IIIT, Hyderabad and Jadavpur University (Ekbal and Bandyopadhyay, 2008b), Calcutta. For Urdu, annotation was performed at CRULP, Lahore (Hussain, 2008) and IIIT, Allahabd. Even though all the annotation was done by native speakers of respective languages, named entity annotation was a new task for everyone involved. This was because of practical constraints as explained in an earlier section. The corpus was divided into two parts, one for training and one for testing. The testing corpus was annotated with nested named entities, while the training corpus was only annotated with ‘maximal’ named entities. Since different teams were working on different languages, in some cases even the same language, and also because most of the corpus was crea"
I08-5003,P01-1041,0,0.0114075,"ma, Dr., Mr. Pride and Prejudice, Othello New Delhi, Paris 3rd September, 1991 (ambiguous) 3.14, 4,500 Rs. 4,500, 5 kg Maximum Entropy, Archeology Table 1: The named entity tagset used for the shared task • Affixes like Hyderabad, Mehdipatnam, Lingampally Rampur, • Gazetteer features: class in the gazetteer • Left and right context • Token length, e.g. the number of letters in a word • Previous history in the document or the corpus • Classes of preceding NEs The machine learning techniques tried for NER include the following: • Hidden Markov Models or HMM (Zhou and Su, 2001) • Decision Trees (Isozaki, 2001) • Maximum Entropy (Borthwick et al., 1998) • Support Vector Machines or SVM (Takeuchi and Collier, 2002) • Conditional Random Fields or CRF (Settles, 2004) Different ways of classifying named entities have been used, i.e., there are more than one tagsets for NER. For example, the CoNLL 2003 shared task2 had only four tags: persons, locations, organizations 2 and miscellaneous. On the other hand, MUC-63 has a near ontology for information extraction purposes. In this (MUC-6) tagset, there are three4 main kinds of NEs: ENAMEX (persons, locations and organizations), TIMES (time expressions) and"
I08-5003,C00-2102,0,0.0282833,"ed with named entities. In this paper we describe such a corpus developed for five South Asian languages. These languages are Hindi, Bengali, Oriya, Telugu and Urdu. This paper also presents an overview of the work done for the IJCNLP workshop on NER for SSEA languages. The workshop included two tracks. The first track was for regular research papers, while the second was organized on the lines of a shared task. Fairly mature named entity recognition systems are now available for European languages (Sang, 2002; Sang and De Meulder, 2003), especially English, and even for East Asian languages (Sassano and Utsuro, 2000). However, for South and South East Asian languages, the problem of NER is still far from being solved. Even though we can gain much insight from the methods used for English, there are many issues which make the nature of the problem different for SSEA languages. For example, these languages do not have capitalization, which is a major feature used by NER systems for European languages. Another characteristic of these languages is that most of them use scripts of Brahmi origin, which have highly phonetic characteristics that could be utilized for multilingual NER. For some languages, there ar"
I08-5003,W03-0428,0,0.0206456,"uages. However, for comparing the results of these different methods, we will need a reasonably good baseline. A mature system tuned for English but trained on SSEA language data can become such a baseline. We will describe such a baseline in a later section. This baseline system has been tested on the data provided for the shared task. We present the results for all five languages under the settings required for the shared task. 2 Related Work Various techniques have been used for solving the NER problem (Mikheev et al., 1999; Borthwick, 1999; Cucerzan and Yarowsky, 1999; Chieu and Ng, 2003; Klein et al., 2003; Kim and Woodland, 2000) ranging from naively using gazetteers to rules based techniques to purely statistical techniques, even hybrid approaches. Several workshops consisting of shared tasks (Sang, 2002; Sang and De Meulder, 2003) have been held with specific focus on this problem. In this section we will mention some of techniques used previously. Most of the approaches can be classified based on the features they use, whether they are rule based or machine learning based or hybrid approaches. Some of the commonly used features are: • Word form and part of speech (POS) tags • Orthographic f"
I08-5003,W04-1221,0,0.0196192,"Missing"
I08-5003,I08-5001,0,0.031547,"Missing"
I08-5003,I08-5015,0,0.048528,"Missing"
I08-5003,W03-0430,0,0.00813305,"Collier, 2002) • Conditional Random Fields or CRF (Settles, 2004) Different ways of classifying named entities have been used, i.e., there are more than one tagsets for NER. For example, the CoNLL 2003 shared task2 had only four tags: persons, locations, organizations 2 and miscellaneous. On the other hand, MUC-63 has a near ontology for information extraction purposes. In this (MUC-6) tagset, there are three4 main kinds of NEs: ENAMEX (persons, locations and organizations), TIMES (time expressions) and NUMEX (number expresssions). There has been some previous work on NER for SSEA languages (McCallum and Li, 2003; Cucerzan and Yarowsky, 1999), but most of the time such work was an offshoot of the work done for European languages. Even including the current workshop, the work on NER for SSEA languages is still in the initial stages as the results reported by papers in this workshop clearly show. 3 A New Named Entity Tagset The tagset being used for the NERSSEAL-08 shared task consists of more tags than the four tags used for the CoNLL 2003 shared task. The reason we opted for these tags was that we needed a slightly finer tagset for machine translation (MT). The initial aim was to improve the performan"
I08-5003,E99-1001,0,0.0541807,"bove show that we might need different methods for solving the NER problem for SSEA languages. However, for comparing the results of these different methods, we will need a reasonably good baseline. A mature system tuned for English but trained on SSEA language data can become such a baseline. We will describe such a baseline in a later section. This baseline system has been tested on the data provided for the shared task. We present the results for all five languages under the settings required for the shared task. 2 Related Work Various techniques have been used for solving the NER problem (Mikheev et al., 1999; Borthwick, 1999; Cucerzan and Yarowsky, 1999; Chieu and Ng, 2003; Klein et al., 2003; Kim and Woodland, 2000) ranging from naively using gazetteers to rules based techniques to purely statistical techniques, even hybrid approaches. Several workshops consisting of shared tasks (Sang, 2002; Sang and De Meulder, 2003) have been held with specific focus on this problem. In this section we will mention some of techniques used previously. Most of the approaches can be classified based on the features they use, whether they are rule based or machine learning based or hybrid approaches. Some of the"
I08-5003,I08-5014,0,0.120428,"Missing"
I08-5003,I08-5004,0,0.190868,"Missing"
I08-5003,I08-5009,0,0.0397312,"Missing"
I08-5003,W03-0419,0,0.289121,"Missing"
I08-5003,W02-2024,0,0.288084,"in identifying them. In order to use these machine learning techniques, we need corpus annotated with named entities. In this paper we describe such a corpus developed for five South Asian languages. These languages are Hindi, Bengali, Oriya, Telugu and Urdu. This paper also presents an overview of the work done for the IJCNLP workshop on NER for SSEA languages. The workshop included two tracks. The first track was for regular research papers, while the second was organized on the lines of a shared task. Fairly mature named entity recognition systems are now available for European languages (Sang, 2002; Sang and De Meulder, 2003), especially English, and even for East Asian languages (Sassano and Utsuro, 2000). However, for South and South East Asian languages, the problem of NER is still far from being solved. Even though we can gain much insight from the methods used for English, there are many issues which make the nature of the problem different for SSEA languages. For example, these languages do not have capitalization, which is a major feature used by NER systems for European languages. Another characteristic of these languages is that most of them use scripts of Brahmi origin, which"
I08-5003,I08-5010,0,0.0346488,"Missing"
I08-5003,I08-5007,0,0.0281783,"Missing"
I08-5003,W02-2029,0,0.0333728,".14, 4,500 Rs. 4,500, 5 kg Maximum Entropy, Archeology Table 1: The named entity tagset used for the shared task • Affixes like Hyderabad, Mehdipatnam, Lingampally Rampur, • Gazetteer features: class in the gazetteer • Left and right context • Token length, e.g. the number of letters in a word • Previous history in the document or the corpus • Classes of preceding NEs The machine learning techniques tried for NER include the following: • Hidden Markov Models or HMM (Zhou and Su, 2001) • Decision Trees (Isozaki, 2001) • Maximum Entropy (Borthwick et al., 1998) • Support Vector Machines or SVM (Takeuchi and Collier, 2002) • Conditional Random Fields or CRF (Settles, 2004) Different ways of classifying named entities have been used, i.e., there are more than one tagsets for NER. For example, the CoNLL 2003 shared task2 had only four tags: persons, locations, organizations 2 and miscellaneous. On the other hand, MUC-63 has a near ontology for information extraction purposes. In this (MUC-6) tagset, there are three4 main kinds of NEs: ENAMEX (persons, locations and organizations), TIMES (time expressions) and NUMEX (number expresssions). There has been some previous work on NER for SSEA languages (McCallum and Li"
I08-5003,P02-1060,0,\N,Missing
I08-5003,I08-5005,0,\N,Missing
I08-5003,I08-5012,0,\N,Missing
I17-4003,I17-4023,0,0.0654236,"Missing"
I17-4003,I17-4008,0,0.0647491,"Missing"
I17-4003,S14-2004,0,0.0428713,"there were 9 and then 3 more products were added in subsequent years (Ding et al., 2008) (Liu et al., 2015). These reviews were sentence-wise annotated with the following: Related Work Many researchers have undertaken the study of opinion diversity, but most exhibit limited scope owing to the absence of a standard dataset among the community. The Blog Track Opinion Finding Task (TREC 6-8) has a favourable corpus, and was initially meant to judge systems on their Reranking approach on web search results, based on opinion diversity. The Aspect Based Sentiment Analysis task at SemEval 2014-2016 (Pontiki et al., 2014) was an initiative towards the objective evaluation of sentiment expressed in product reviews. In a wide enough corpora of 39 datasets, ranging across 7 domains and 8 languages, the task was to identify target entity and pick the attribute commented upon (from a list of attributes already provided to annotators). Our aim differs slightly in that we reward systems which ultimately produce an opinion diversified (and representative) ranking of a subset of the review corpora. The motivation for this statement bases itself on two targeted benefits: 1. feature on which opinion is expressed, if any"
K15-2009,D14-1008,0,0.0365767,"Missing"
K15-2009,D09-1036,0,0.073065,"Missing"
K15-2009,miltsakaki-etal-2004-penn,0,0.119544,"Missing"
K15-2009,P09-2004,0,0.0762533,"Missing"
K15-2009,K15-2001,0,0.0607183,"Missing"
K15-2009,W15-1841,0,\N,Missing
K15-2009,I11-1120,0,\N,Missing
K16-2015,kipper-etal-2006-extending,0,0.0342705,"otherwise it is positive or negative. stitute about a third of the non-explicit data (215 EntRel relations and 522 Implicit relations in development data) and not all remaining sentences contain an implicit relation. In our implicit argument span detector, we treat the first sentence in adjacent sentence pair as Arg1 and the second sentence as Arg2. Next, we focus on the Implicit Sense Classification task. 3.1 VerbNet Classes: VerbNet is a verb lexicon with mappings to WordNet and FrameNet. VerbNet is organized into classes (with subclasses) on the basis of syntactic and semantic similarity (Kipper et al., 2006). We have created verbNetClassArg1 and verbNetClassArg2 features, which contain the VerbNet class of the lemmatized forms of the main verbs of the respective arguments (Zhou, 2015). VerbNet classes are important features and this was verified by analyzing the most informative features of this classification task. We find that many VerbNet classes are more informative than even baseline features. Implicit Sense Classification This task is considered as the bottleneck of SDP systems and is especially challenging due to lack to connective based features. We create a different set of baseline feat"
K16-2015,D14-1008,0,0.0718793,"POS + connectivePOS, nextWord, nextPOSTag, nextPOS + connectivePOS, root2Leaf, root2LeafCompressed, leftSibling, rightSibling, parentCategory. These features have been borrowed from previous work of (Wang and Lan, 2015). 2.2 2.2.2 Argument Span Extractor This stage of the pipeline extracts the span of the arguments from the sentence or sentences containing the discourse relation. To extract arguments, we first break the sentence into clauses. Two methods have been proposed in literature to carry out this task: Lin’s tree subtract method (Lin et al., 2014) and Kong’s constituency based method (Kong et al., 2014). According to (Kong et al., 2014), Kong’s constituency based approach outperforms Lin’s tree subtraction algorithm. However, since Kong’s method is based on using the connective node in the parse tree as the base node for recursion, we can only use this method for those sentences which contain the connective. Hence, we use Kong’s extraction method for Same Sentence Argument Extraction. SS Argument Extractor: Kong’s constituency-based approach is a recursion in which the connective’s lowest tree node is chosen as the target node, and its left and right siblings are chosen as candidates for arg"
K16-2015,prasad-etal-2008-penn,0,0.436341,"them (Xue et al., 2016). The discourse relations can be explicit, in which relations are expressed by certain words or phrases, or implicit, where words are not directly used to convey the relation, but instead, the meaning is implied. These words or phrases which convey the existence of a discourse relation directly are called connectives. The lexical units between which relation exists, could be a pair of clauses, a pair of sentences or even multiple sentences which can be adjacent or non-adjacent. These are called arguments. A discourse treebank called the Penn Discourse TreeBank or PDTB (Prasad et al., 2008) serves as the gold standard for this task and is used as training data. The output of our system follows the same format as PDTB. Development data is also provided to perform experiments on the system. Phrase structure and dependency parses of 2 Explicit SDP Identification of explicit discourse relations consists of several stages. First stage is the detection of discourse connectives in the text. This connective binds the arguments syntactically and semantically (Prasad et al., 2008) which is helpful in feature creation for the following tasks of argument position detection and argument span"
K16-2015,K15-2002,0,0.297509,"ot sufficient for this task and we therefore use Maximum Entropy Classifier to identify whether a potential connective keyword actually forms a discourse relation or not. This task has been sufficiently mastered and high F1 scores have been reported by previous teams. Mostly syntactic features have been used for this classification task such as Connective, connectivePOS, PrevWord, PrevPOSTag, PrevPOS + connectivePOS, nextWord, nextPOSTag, nextPOS + connectivePOS, root2Leaf, root2LeafCompressed, leftSibling, rightSibling, parentCategory. These features have been borrowed from previous work of (Wang and Lan, 2015). 2.2 2.2.2 Argument Span Extractor This stage of the pipeline extracts the span of the arguments from the sentence or sentences containing the discourse relation. To extract arguments, we first break the sentence into clauses. Two methods have been proposed in literature to carry out this task: Lin’s tree subtract method (Lin et al., 2014) and Kong’s constituency based method (Kong et al., 2014). According to (Kong et al., 2014), Kong’s constituency based approach outperforms Lin’s tree subtraction algorithm. However, since Kong’s method is based on using the connective node in the parse tree"
K16-2015,K16-2001,0,0.0145572,"ted in the paper. We choose a data driven approach for each task and put a special focus on utilizing the resources allowed by the organizers for creating novel features. We also give details of various experiments with the dataset and the lexicon provided for the task. 1 Introduction Shallow Discourse Parsing (SDP) is a linguistic task that identifies semantic relations between a pair of lexical units in a piece of discourse. Discourse relation is defined by three entities: a connective, a pair of lexical units between which the relation exists and the type or sense of relation between them (Xue et al., 2016). The discourse relations can be explicit, in which relations are expressed by certain words or phrases, or implicit, where words are not directly used to convey the relation, but instead, the meaning is implied. These words or phrases which convey the existence of a discourse relation directly are called connectives. The lexical units between which relation exists, could be a pair of clauses, a pair of sentences or even multiple sentences which can be adjacent or non-adjacent. These are called arguments. A discourse treebank called the Penn Discourse TreeBank or PDTB (Prasad et al., 2008) ser"
K16-2015,P13-1171,0,0.0322898,"tire training corpus were extracted. This created around 12,489 constituency parses and 89 dependency parses. However, the number of parse features was too high and unnecessary to work with. Hence, we put a frequency cap of 5 on the feature set which brought the features down to 2,515. We also used NLTK’s stop-words to filter out dependency parses created by common words as these parse rules are highly recurring over the distribution of the entire corpus. 3.1.2 Words with similar meaning are expected to have vectors in close proximity in the vector space (Mikolov et al., 2013). Inspired from (Yih et al., 2013), a work on Question-Answering system, we represent the entire Arg1 and Arg2 as vectors. We take each argument, drop the stopwords and then take a weighted sum over the vector representations of remaining words of the argument. Even after removing stop words, there is a difference in importance and relevance of the remaining words. This is why we choose to take a weighted sum of the word vectors. We chose TF-IDF (Term Frequency-Inverse Document Frequency) scores as weights. The TF-IDF value increases proportionally with the number of times a word appears in the document and decreases with the"
K16-2015,K15-2004,0,0.358708,"ontain an implicit relation. In our implicit argument span detector, we treat the first sentence in adjacent sentence pair as Arg1 and the second sentence as Arg2. Next, we focus on the Implicit Sense Classification task. 3.1 VerbNet Classes: VerbNet is a verb lexicon with mappings to WordNet and FrameNet. VerbNet is organized into classes (with subclasses) on the basis of syntactic and semantic similarity (Kipper et al., 2006). We have created verbNetClassArg1 and verbNetClassArg2 features, which contain the VerbNet class of the lemmatized forms of the main verbs of the respective arguments (Zhou, 2015). VerbNet classes are important features and this was verified by analyzing the most informative features of this classification task. We find that many VerbNet classes are more informative than even baseline features. Implicit Sense Classification This task is considered as the bottleneck of SDP systems and is especially challenging due to lack to connective based features. We create a different set of baseline features as borrowed from (Lin et al., 2014). We describe semantic features used for this task in detail in this section. 3.1.1 Word2Vec: Subjectivity strength and VerbNet classes only"
K17-2007,D14-1179,0,0.0169113,"Missing"
K17-2007,K17-2001,0,0.0598311,"Missing"
K17-2007,N13-1138,0,\N,Missing
K17-2007,E14-1060,0,\N,Missing
kolachina-etal-2010-grammar,J93-2004,0,\N,Missing
kolachina-etal-2010-grammar,W09-3812,1,\N,Missing
kolachina-etal-2010-grammar,W00-1307,0,\N,Missing
kolachina-etal-2010-grammar,W09-3036,0,\N,Missing
kolachina-etal-2010-grammar,P07-1025,0,\N,Missing
kolachina-etal-2010-grammar,P98-1013,0,\N,Missing
kolachina-etal-2010-grammar,C98-1013,0,\N,Missing
kolachina-etal-2010-grammar,J02-3001,0,\N,Missing
kolachina-etal-2010-grammar,J01-3003,0,\N,Missing
kolachina-etal-2010-grammar,I08-2099,1,\N,Missing
kolachina-etal-2010-grammar,begum-etal-2008-developing,1,\N,Missing
N09-3016,P08-1065,0,0.0199511,"Missing"
N09-3016,W02-1001,0,0.013261,"Then a local phoneme predictor is used to guess the phonemes for every letter in a word. The size of the letter chunk could be either one or two. Only one candidate for every word is allowed. The best phoneme sequence is obtained by using Viterbi search. An online model MIRA (Crammer and Singer, 2003) which updates parameters is used for the L2P task by Jiampojamarn et al (2008). The authors unify the steps of letter segmentation, phoneme prediction and sequence modeling into a single module. The phoneme prediction and sequence modeling are considered as tagging problems and a Perceptron HMM (Collins, 2002) is used to model it. The letter segmenter module is replaced by a monotone phrasal decoder (Zens and Ney, 2004) to search for the possible substrings in a word and output the n-best list for updating MIRA. Bisani and Ney (2002) take the joint multigrams of graphemes and phonemes as features for alignment and language modeling for phonetic transcription probabilities. A hybrid approach similar to this is by (van den Bosch and Canisius, 2006). In the next section we model the problem as a Statistical Machine Translation (SMT) task. 3 Modeling the Problem Assume that given a word, represented as"
N09-3016,N07-1047,0,0.611059,"er-to-phoneme conversion task, a single letter can map to multiple phonemes [x → ks] and multiple letters can generate a single phoneme. A letter can also map to a null phoneme [e → ϕ] and vice-versa. These examples give a glimpse of why the task is so complex and a single machine learning technique may not be enough to solve the problem. A overview of the literature supports this claim. In older approaches, the alignment between the letters and phonemes was taken to be one-toone (Black et al., 1998) and the phoneme was predicted for every single letter. But recent work (Bisani and Ney, 2002; Jiampojamarn et al., 2007) shows that multiple letter-to-phoneme alignments perform better than single letter to phoneme alignments. The problem can be either viewed as a multi-class classiﬁer problem or a structure prediction problem. In structure prediction, the algorithm takes the previous decisions as the features which inﬂuence the current decision. In the classiﬁer approach, only the letter and its context are taken as features. Then, either multiclass decision trees (Daelemans and van den Bosch, 1997) or instance based learning as in (van den Bosch and Daelemans, 1998) is used to predict the class, which in this"
N09-3016,P08-1103,0,0.183749,"class decision trees (Daelemans and van den Bosch, 1997) or instance based learning as in (van den Bosch and Daelemans, 1998) is used to predict the class, which in this case is a phoneme. Some of these methods (Black et al., 1998) are not completely automatic and need an initial handcrafted seeding to begin the classiﬁcation. Structure prediction is like a tagging problem where HMMs (Taylor, 2005) are used to model the problem. Taylor claims that except for a preprocessing step, it is completely automatic. The whole process is performed in a single step. The results are poor, as reasoned in (Jiampojamarn et al., 2008) due to the emission probabilities not being informed by the previous letter’s emission probabilities. Pronunciation by Analogy (PbA) is a datadriven method (Marchand and Damper, 2000) for letter-to-phoneme conversion which is used again by Damper et al (2004). They simply use an 91 Expectation-Maximisation (EM) like algorithm for aligning the letter-phoneme pairs in a speech dictionary. They claim that by integrating the alignments induced by the algorithm into the PbA system, they were able to improve the accuracy of the pronunciation signiﬁcantly. We also use the many-to-many alignment appr"
N09-3016,N03-1017,0,0.0422559,"ontext of SMT, say English-Spanish, the parallel corpus is aligned bidirectionally to obtain the two alignments. The IBM models give only one-to-one alignments between words in a sentence pair. So, GIZA++ uses some heuristics to reﬁne the alignments (Och and Ney, 2003). In our input data, the source side consists of grapheme (or letter) sequences and the target side consists of phoneme sequences. Every letter or grapheme is treated as a single ‘word’ for the GIZA++ input. The transcription probabilities can then be easily learnt from the alignments induced by GIZA++, using a scoring function (Koehn et al., 2003). Figure 1 shows the alignments induced by GIZA++ for the example words which are mentioned by Jiampojamarn et al (2007). In this ﬁgure, we only show the alignments from graphemes to phonemes. Figure 1: Example Alignments from GIZA++ 5 Evaluation We evaluated our models on the English CMUDict, French Brulex, German Celex and Dutch Celex speech dictionaries. These dictionaries are available for download on the website of PROANALSYL1 Letter-to-Phoneme Conversion Challenge. Table 1 shows the number of words for each language. The datasets available at the website were divided into 10 folds. In th"
N09-3016,P07-2045,0,0.00772322,"emaining 8 sets for training. We report our results in word accuracy rate, based on 10-fold cross validation, with mean and standard deviation. Language English French German Dutch Datasets CMUDict Brulex Celex Celex Number of Words 112241 27473 49421 116252 Table 1: Number of words in each Dataset 5.2 System Comparison We removed the one-to-one alignments from the corpora and induced our own alignments using GIZA++. We used minimum error rate training (Och, 2003) and the A* beam search decoder implemented by Koehn (Koehn et al., 2003). All the above tools are available as parts of the MOSES (Koehn et al., 2007) toolkit. 5.1 Exploring the Parameters The parameters which have a major inﬂuence on the performance of a phrase-based SMT model are the alignment heuristics, the maximum phrase length (MPR) and the order of the language model (Koehn et al., 2003). In the context of letter to phoneme conversion, phrase means a sequence of letters or phonemes mapped to each other with some probability (i.e., the hypothesis) and stored in a phrase table. The maximum phrase length corresponds to the maximum number of letters or phonemes that a hypothesis can contain. Higher phrase length corresponds a larger phra"
N09-3016,N06-1030,0,0.0627483,"Missing"
N09-3016,J00-2003,0,0.0216496,"eme. Some of these methods (Black et al., 1998) are not completely automatic and need an initial handcrafted seeding to begin the classiﬁcation. Structure prediction is like a tagging problem where HMMs (Taylor, 2005) are used to model the problem. Taylor claims that except for a preprocessing step, it is completely automatic. The whole process is performed in a single step. The results are poor, as reasoned in (Jiampojamarn et al., 2008) due to the emission probabilities not being informed by the previous letter’s emission probabilities. Pronunciation by Analogy (PbA) is a datadriven method (Marchand and Damper, 2000) for letter-to-phoneme conversion which is used again by Damper et al (2004). They simply use an 91 Expectation-Maximisation (EM) like algorithm for aligning the letter-phoneme pairs in a speech dictionary. They claim that by integrating the alignments induced by the algorithm into the PbA system, they were able to improve the accuracy of the pronunciation signiﬁcantly. We also use the many-to-many alignment approach but in a different way and obtained from a different source. The recent work of Jiampojamarn et al (2007) combines both of the above approaches in a very interesting manner. It us"
N09-3016,J03-1002,0,0.00172066,"pλM (f |l) 1 (5) 92 =� � exp ΣM m=1 λm hm (f, l) f´1I � � ´I exp ΣM m=1 λm hm (f1 , l) � (6) with the denominator, a normalization factor that can be ignored in the maximization process. The above modeling entails ﬁnding the suitable model parameters or weights which reﬂect the properties of our task. We adopt the criterion followed in (Och, 2003) for optimising the parameters of the model. The details of the solution and proof for the convergence are given in Och (2003). The models’ weights, used for the L2P task, are obtained from this training. 4 Letter-to-Phoneme Alignment We used GIZA++ (Och and Ney, 2003), an open source toolkit, for aligning the letters with the phonemes in the training data sets. In the context of SMT, say English-Spanish, the parallel corpus is aligned bidirectionally to obtain the two alignments. The IBM models give only one-to-one alignments between words in a sentence pair. So, GIZA++ uses some heuristics to reﬁne the alignments (Och and Ney, 2003). In our input data, the source side consists of grapheme (or letter) sequences and the target side consists of phoneme sequences. Every letter or grapheme is treated as a single ‘word’ for the GIZA++ input. The transcription p"
N09-3016,P03-1021,0,0.100249,"lso be directly modeled using a log-linear model. In this model, we have a set of M feature functions hm (f, l), m = 1...M . For each feature function there exists a weight or model parameter λm , m = 1...M . Thus the posterior probability becomes: Pr (f |l) = pλM (f |l) 1 (5) 92 =� � exp ΣM m=1 λm hm (f, l) f´1I � � ´I exp ΣM m=1 λm hm (f1 , l) � (6) with the denominator, a normalization factor that can be ignored in the maximization process. The above modeling entails ﬁnding the suitable model parameters or weights which reﬂect the properties of our task. We adopt the criterion followed in (Och, 2003) for optimising the parameters of the model. The details of the solution and proof for the convergence are given in Och (2003). The models’ weights, used for the L2P task, are obtained from this training. 4 Letter-to-Phoneme Alignment We used GIZA++ (Och and Ney, 2003), an open source toolkit, for aligning the letters with the phonemes in the training data sets. In the context of SMT, say English-Spanish, the parallel corpus is aligned bidirectionally to obtain the two alignments. The IBM models give only one-to-one alignments between words in a sentence pair. So, GIZA++ uses some heuristics t"
N09-3016,P07-1119,0,0.051025,"Missing"
N09-3016,P02-1019,0,0.21392,"Missing"
N09-3016,W06-3206,0,0.0280128,"Missing"
N09-3016,W98-1224,0,0.0810779,"Missing"
N09-3016,N04-1033,0,0.0152754,"tter chunk could be either one or two. Only one candidate for every word is allowed. The best phoneme sequence is obtained by using Viterbi search. An online model MIRA (Crammer and Singer, 2003) which updates parameters is used for the L2P task by Jiampojamarn et al (2008). The authors unify the steps of letter segmentation, phoneme prediction and sequence modeling into a single module. The phoneme prediction and sequence modeling are considered as tagging problems and a Perceptron HMM (Collins, 2002) is used to model it. The letter segmenter module is replaced by a monotone phrasal decoder (Zens and Ney, 2004) to search for the possible substrings in a word and output the n-best list for updating MIRA. Bisani and Ney (2002) take the joint multigrams of graphemes and phonemes as features for alignment and language modeling for phonetic transcription probabilities. A hybrid approach similar to this is by (van den Bosch and Canisius, 2006). In the next section we model the problem as a Statistical Machine Translation (SMT) task. 3 Modeling the Problem Assume that given a word, represented as a sequence of letters l = l1J = l1 ...lj ...lJ , needs to be transcribed as a sequence of phonemes, represented"
N09-3016,C04-1030,0,\N,Missing
R09-1064,P06-1035,0,0.428435,"genetic trees 2 1 In recent years, the methods developed in computational biology [13, 15, 11, 19] have been successfully adapted in computational linguistics for constructing the phylogeny3 . All these methods are character based or distance based methods. The major disadvantage of these approaches is that they require handcrafted lists. Moreover, the methods inspired from glottochronology take a boolean matrix as input, which denotes the change in the state of the ‘characters’ (the ‘characters’ can be lexical, morphological or phonological) to infer the phylogenetic trees. Ellison and Kirby [9] discuss establishing a probability distribution for every language through intralexical comparison using confusion probabilities. They use normalized edit distance to calculate the probabilities. Then the distance between every language pair is estimated as a distance between the probability distributions formed for individual languages. The distances (between languages) are estimated using KLdivergence and Rao’s distance. The same measures are also used to find the level of cognacy between the words. The experiments are conducted on Dyen’s [8] classical Indo-European dataset. The estimated d"
R09-1064,W06-1109,1,0.795213,"apes so that to a lay person the scripts seem very different. In fact, there is a ‘super encoding’ or ‘meta encoding’ called ISCII that can be used to represent this common alphabet. The letters of this common alphbet can be approximately treated like phonemes for computational purposes. For languages which do not use such scripts, we will first have to convert the text into a phonetic notation to be able to use the methods described below, except perhaps the first one. 3.1 Symmetric Cross Entropy (SCE) The first measure is purely a letter n-gram based measure similar to the one used by Singh [17] for language 4 Potential cognates are words of different languages which are similar in form and therefore are likely to be cognates. They might include some ‘false friends’, i.e., words which are not etymologically inherited. It is worthwhile to experiment (using statistical techniques) on potential cognates, even without removing the ‘false friends’ because a large percentage of them are actually cognates in the linguistic sense. 356 and encoding identification. Note that since letters in Brahmi origin scripts can almost be treated like phonemes, we could call this method a phoneme ngram ba"
R09-1064,W07-1306,1,0.940289,"unity. Most of the previous work is focused on reconstruction of phylogenetic trees for a particular language family using handcrafted word lists [12, 3, 2, 14] or using synthetic data [4]. In this paper we pose the following questions. What happens when we try to construct phylogenetic trees using inter-language distances in the context of a linguistic area 1 ? Can the phylogenetic trees be used for evaluating the robustness of the inter-language distance measures and the meaningfulness of the distances? To our knowledge these questions have not been addressed previously. As Singh and Surana [18] showed, corpus based measures can be successfully used for comparative study of languages. Can these distances, estimated from a noisy corpus2 , meaningfully be used to construct phylogenetic trees? Can the information represented by the tree give meaningful interpretations about the languages involved? In this paper, we try to answer these questions. By using meaningful measures for estimating the distance between languages, we try to establish that the answers 1 2 The term linguistic area or Sprachbund [10] refers to a group of languages that have become similar in some way as a result of p"
S18-1104,S18-1005,0,0.0491574,"Missing"
S18-1104,D16-1104,0,0.0161247,"g to Automated Readability Index (Senter and Smith, 1967), the standard deviation, the average and the median of the word length serve as indicators of the complexity of the text (Rajadesingan et al., 2015). Incongruity of Context: Ironic similes are common in literature (e.g. as clear as mud in which both clear and mud are sentiment neutral words.). Due to this neutrality, the lexicon based methods are unable to capture the incongruity present. Therefore, maximum and minimum GloVe (Pennington et al., 2014) cosine similarity between any two words in a tweet are used as features in our system (Joshi et al., 2016). Repetition-based Features: Users often change their writing style to depict sarcasm and irony, which is analogous to the change of tone in speech while expressing sarcasm, e.g. Loooovvveeeeeee when my phone gets wiped. We use the count of of words with repetitive characters and the count of ‘senti words’ (sentiment score ≥ 2 and sentiment score ≤ -2) with repetitive characters as our features (Rajadesingan et al., 2015). Punctuation-based Features: Punctuation counts can sometimes serve as an indicator of ironic texts (Kreuz and Caucci, 2007). We use the counts of characters like hashtag (#)"
S18-1104,P15-1100,0,0.0731037,"Missing"
S18-1104,W07-0101,0,0.368352,"words in a tweet are used as features in our system (Joshi et al., 2016). Repetition-based Features: Users often change their writing style to depict sarcasm and irony, which is analogous to the change of tone in speech while expressing sarcasm, e.g. Loooovvveeeeeee when my phone gets wiped. We use the count of of words with repetitive characters and the count of ‘senti words’ (sentiment score ≥ 2 and sentiment score ≤ -2) with repetitive characters as our features (Rajadesingan et al., 2015). Punctuation-based Features: Punctuation counts can sometimes serve as an indicator of ironic texts (Kreuz and Caucci, 2007). We use the counts of characters like hashtag (#), ellipsis (...), exclamation mark (!), question mark (?), colon (:), quote (”) and apostrophe (’) in a tweet as features. Presence of Markers: Discourse markers are certain words that help in expressing ideas and performing specific functions (Far´ıas et al., 2016). Our system uses a curated list of discourse markers. Similar to the list of discourse markers, we also use a list of intensifiers (e.g. heck ), laughter words (e.g. lmao, lol etc.), interjections (e.g. oops) and swear words (e.g. shit) as their appearance in a tweet indicates the p"
S18-1104,D14-1162,0,0.0838728,"al number of syllables in the tweet, along with number of words that contain polysyllables as features. According to Automated Readability Index (Senter and Smith, 1967), the standard deviation, the average and the median of the word length serve as indicators of the complexity of the text (Rajadesingan et al., 2015). Incongruity of Context: Ironic similes are common in literature (e.g. as clear as mud in which both clear and mud are sentiment neutral words.). Due to this neutrality, the lexicon based methods are unable to capture the incongruity present. Therefore, maximum and minimum GloVe (Pennington et al., 2014) cosine similarity between any two words in a tweet are used as features in our system (Joshi et al., 2016). Repetition-based Features: Users often change their writing style to depict sarcasm and irony, which is analogous to the change of tone in speech while expressing sarcasm, e.g. Loooovvveeeeeee when my phone gets wiped. We use the count of of words with repetitive characters and the count of ‘senti words’ (sentiment score ≥ 2 and sentiment score ≤ -2) with repetitive characters as our features (Rajadesingan et al., 2015). Punctuation-based Features: Punctuation counts can sometimes serve"
singh-2012-concise,singh-ambati-2010-integrated,1,\N,Missing
singh-2012-concise,W00-1324,0,\N,Missing
singh-2012-concise,bird-etal-2000-towards,0,\N,Missing
singh-2012-concise,P05-3009,0,\N,Missing
singh-2012-concise,agarwal-etal-2012-gui,1,\N,Missing
singh-2012-concise,W06-2716,0,\N,Missing
singh-2012-concise,I08-2141,1,\N,Missing
singh-2012-concise,I08-2099,0,\N,Missing
singh-2012-concise,U04-1019,0,\N,Missing
singh-ambati-2010-integrated,nilsson-nivre-2008-malteval,0,\N,Missing
singh-ambati-2010-integrated,bel-etal-2000-simple,0,\N,Missing
singh-ambati-2010-integrated,N07-4006,0,\N,Missing
singh-ambati-2010-integrated,W00-1324,0,\N,Missing
singh-ambati-2010-integrated,I08-2141,1,\N,Missing
singh-ambati-2010-integrated,havasi-etal-2006-bulb,0,\N,Missing
singh-ambati-2010-integrated,cunningham-etal-2000-software,0,\N,Missing
singh-etal-2008-estimating,W07-1306,1,\N,Missing
singh-etal-2008-estimating,P06-1035,0,\N,Missing
singh-etal-2008-estimating,W97-1102,0,\N,Missing
W05-0816,P93-1001,0,0.103174,"Missing"
W05-0816,P91-1023,0,0.347435,"Missing"
W05-0816,W96-0201,0,0.428984,"Missing"
W05-0816,moore-2002-fast,0,0.355003,"Missing"
W05-0816,J03-1002,0,0.0139245,"Missing"
W05-0816,1992.tmi-1.7,0,0.448509,"Missing"
W05-0816,P91-1022,0,0.518921,"Missing"
W05-0816,P94-1012,0,0.176799,"Missing"
W05-0816,P93-1002,0,0.568674,"Missing"
W05-0816,J93-1004,0,\N,Missing
W05-0816,J93-2003,0,\N,Missing
W05-0816,J90-2002,0,\N,Missing
W05-0816,J93-1006,0,\N,Missing
W05-0816,P98-1117,0,\N,Missing
W05-0816,C98-1113,0,\N,Missing
W06-1109,W97-0907,0,0.688193,"’s (Beesley, 1988) automatic language identifier for online texts was based on mathematical language models developed for breaking ciphers. These models basically had characteristic letter sequences and frequencies (‘orthographical features’) for each language, making them similar to n-grams models. The insights on which they are based, as Beesley points out, have been known at least since the time of Ibn ad-Duraihim who lived in the 14th century. Beesley’s method needed 6-64 K of training data and 10-12 words of test data. It treats language and encoding pair as one entity. Adams and Resnik (Adams and Resnik, 1997) describe a client-server system using Dunning’s n-grams based algorithm (Dunning, 1994) for a variety of tradeoffs available to NLP applications like between the labelling accuracy and the size and completeness of language models. Their system dynamically adds language models. The system uses other tools to identify the text encoding. They use 5-grams with add-k smoothing. Training size was 1-50 K and test size above 50 characters. Some pruning is done, like for frequencies up to 3. Some methods for language identification use techniques similar to n-gram based text categorization (Cavnar and"
W06-1109,O97-1002,0,0.0852005,"ntain any unique words. Cavnar’s method, combined with some heuristics, was used by Kikui (Kikui, 1996) to identify languages as well as encodings for a multilingual text. He relied on known mappings between languages and encodings and treated East Asian languages differently from West European languages. Kranig (Muthusamy et al., 1994) and (Simon, 2005) have reviewed and evaluated some of the well known language identification methods. Martins and Silva (Martins and Silva, 2005) describe a method similar to Cavnar’s but which uses a different similarity measure proposed by Jiang and Conrath (Jiang and Conrath, 1997). Some heuristics are also employed. Poutsma’s (Poutsma, 2001) method is based on Monte Carlo sampling of n-grams from the beginning of the document instead of building a complete model of the whole document. Sibun and Reynar (Sibun and Reynar, 1996) use mutual information statistics or relative entropy, also called Kullback-Leibler distance for language identification. Souter et al.(Souter et al., 1994) compared unique character string, common word and ’trigraph’ based approaches and found the last to be the best. Compression based approaches have also been used for language identification. O"
W06-1109,C96-2110,0,0.043906,"upon grammatically correct words instead of the most common words. He also used the knowledge about the alphabet and the word morphology via syllabation. Giguet tried this method for tagging sentences in a document with the language name, i.e., dealing with multilingual documents. Another method (Stephen, 1993) was based on ‘common words’ which are characteristic of each language. This methods assumes unique words for each language. One major problem with this method was that the test string might not contain any unique words. Cavnar’s method, combined with some heuristics, was used by Kikui (Kikui, 1996) to identify languages as well as encodings for a multilingual text. He relied on known mappings between languages and encodings and treated East Asian languages differently from West European languages. Kranig (Muthusamy et al., 1994) and (Simon, 2005) have reviewed and evaluated some of the well known language identification methods. Martins and Silva (Martins and Silva, 2005) describe a method similar to Cavnar’s but which uses a different similarity measure proposed by Jiang and Conrath (Jiang and Conrath, 1997). Some heuristics are also employed. Poutsma’s (Poutsma, 2001) method is based"
W07-1306,W06-1107,0,0.0187908,"show that even noisy corpora can be used for comparative study of languages. Different measures can give different kinds of insights. 2 Related Work Typology or history of languages can be studied using spoken data or text. There has been work on the former (Remmel, 1980; Kondrak, 2002), but we 40 Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40–47, c Prague, June 2007. 2007 Association for Computational Linguistics will focus only on text. An example of a major work on text based similarity is the paper by Kondrak and Sherif (Kondrak and Sherif, 2006). They have evaluated various phonetic similarity algorithms for aligning cognates. They found that learning based algorithms outperform manually constructed schemes, but only when large training data is used. A recent work on applications of such techniques for linguistic study is by Heeringa et al. (Heeringa et al., 2006). They performed a study on different variations of string distance algorithms for dialectology and concluded that order sensitivity is important while scaling with length is not. It may be noted that Ellison and Kirby (Ellison and Kirby, 2006) have shown that scaling by dis"
W07-1306,W97-1102,0,0.753382,"ult, semi-automatic crosslingual comparison of languages is also becoming a fruitful area of study. Among the fundamental tools for crosslingual comparison are measures of inter-language distances. In linguistics, the study of inter-language distances, especially for language classification, has a long history (Swadesh, 1952; Ellison and Kirby, 2006). Basically, the work on this problem has been along linguistic, archaeological and computational streams. Like in other disciplines, computational methods are increasingly being combined with other more conventional approaches (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ellison and Kirby, 2006). The work being presented in this paper belongs to the computational stream. Even in the computational stream, most of the previous work on inter-language distances had a strong linguistic dimension. For example, most of the quantitative measures of inter-language distance have been applied on handcrafted word lists (Swadesh, 1952; Dyen et al., 1992). However, with increasing use of computational techniques and the availability of electronic data, a natural question arises: Can languages be linguistically compared based on word lists extracted from cor"
W07-1306,W97-0907,0,0.071448,"Missing"
W07-1306,P06-1035,0,0.580053,"historical knowledge. We argue that the answer is yes and that such studies can provide or validate linguistic and computational insights. 1 Introduction Crosslingual and multilingual processing is acquiring importance in the computational linguistics community. As a result, semi-automatic crosslingual comparison of languages is also becoming a fruitful area of study. Among the fundamental tools for crosslingual comparison are measures of inter-language distances. In linguistics, the study of inter-language distances, especially for language classification, has a long history (Swadesh, 1952; Ellison and Kirby, 2006). Basically, the work on this problem has been along linguistic, archaeological and computational streams. Like in other disciplines, computational methods are increasingly being combined with other more conventional approaches (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ellison and Kirby, 2006). The work being presented in this paper belongs to the computational stream. Even in the computational stream, most of the previous work on inter-language distances had a strong linguistic dimension. For example, most of the quantitative measures of inter-language distance have been"
W07-1306,W06-1108,0,0.25056,"of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40–47, c Prague, June 2007. 2007 Association for Computational Linguistics will focus only on text. An example of a major work on text based similarity is the paper by Kondrak and Sherif (Kondrak and Sherif, 2006). They have evaluated various phonetic similarity algorithms for aligning cognates. They found that learning based algorithms outperform manually constructed schemes, but only when large training data is used. A recent work on applications of such techniques for linguistic study is by Heeringa et al. (Heeringa et al., 2006). They performed a study on different variations of string distance algorithms for dialectology and concluded that order sensitivity is important while scaling with length is not. It may be noted that Ellison and Kirby (Ellison and Kirby, 2006) have shown that scaling by distance does give significantly better results. Nakleh et al. (Nakleh et al., 2005) have written about using phylogenetic techniques in historical linguistics as mentioned by Nerbonne (Nerbonne, 2005) in the review of the book titled ‘Language Classification by Numbers’ by McMahon and McMahon (McMahon and McMahon, 2005). All"
W07-1306,W06-1109,1,0.878234,"section-7. This category of measures have to incorporate more linguistic information if they are to provide good results. Designing such measures can be a challenging problem as we will be mainly relying on the corpus for our information. Knowledge about similarities and differences of writing systems can play an important role here. The two cognate based measures described in sections 9 and 10 are an attempt at this. But first we describe a simple n-gram based measure. 7 Symmetric Cross Entropy (SCE) The first measure is purely a letter n-gram based measure similar to the one used by Singh (Singh, 2006b) for language and encoding identification. To calculate the distance, we first prepare letter 5-gram models from the corpora of the languages to be compared. Then we combine n-grams of all orders and rank them according to their probability in descending order. Only the top N n-grams are retained and the rest are pruned. 1 Now we have two probability distributions which can be compared by a measure of distributional similarity. We have used symmetric cross entropy as such a measure: dsce = X (p(gl ) log q(gm ) + q(gm ) log p(gl )) gl =gm (1) where p and q are the probability distributions fo"
W13-2250,W12-3102,0,0.14645,"Missing"
W13-2250,1993.eamt-1.1,0,0.511454,"Missing"
W13-2250,P03-1021,0,0.0272251,"search space E (generally represented as a lattice); δu (E) has the value 1 if u occurs in the translation hypothesis E and 0 otherwise and P (E, A|F ) is the probability that the source sentence F is translated by the hypothesis E using a derivation A. Following (Gispert et al., 2013), this probability is estimated by applying a soft-max function to the score of the decoder: exp (α × H(E, A, F )) 0 0 (A0 ,E 0 )∈E exp (H(E , A , F )) 0.00 P (A, E|F ) = P where the decoder score H(E, A, F ) is typically a linear combination of a handful of features, the weights of which are estimated by MERT (Och, 2003). n-gram posteriors therefore aggregate two pieces of information: first, the number of paths in the lattice (i.e. the number of translation hypotheses of the search path) the n-gram appears in; second, the decoder scores of these paths that can be roughly interpreted as a quality of the path. Computing P (u|E) requires to enumerate all ngram contained in E and to count the number of paths in which this n-gram appears at least once. An efficient method to perform this computation in a single traversal of the lattice is described in (Gispert et al., 2013). This algorithm has been reimplemented1"
W13-2250,P10-1063,0,0.0272722,"tion (SMT) systems in the professional translation industry is still limited by the lack of reliability of SMT outputs, the quality of which varies to a great extent. In this context, a critical piece of information would be for MT systems to assess their output translations with automatically derived quality measures. This problem is the focus of a shared task, the aim of which is to predict the quality of a translation without knowing any human reference(s). To the best of our knowledge, all approaches so far have tackled quality estimation as a supervised learning problem (He et al., 2010; Soricut and Echihabi, 2010; Specia et al., 2010; Specia, 2011). A wide variety of features have been proposed, most of which can be described as loosely ‘linguistic’ features that describe the source sentence, the target sentence and the association between them (Callison-Burch et al., 2012). Surprisingly enough, information used by the decoder to choose the best translation in the search space, such as its internal scores, have hardly been considered and never proved to be useful. Indeed, it is well-known that these scores are hard to interpret and to compare across hypotheses. Furthermore, mapping scores of a linear"
W13-2250,2011.eamt-1.12,0,0.180488,"n industry is still limited by the lack of reliability of SMT outputs, the quality of which varies to a great extent. In this context, a critical piece of information would be for MT systems to assess their output translations with automatically derived quality measures. This problem is the focus of a shared task, the aim of which is to predict the quality of a translation without knowing any human reference(s). To the best of our knowledge, all approaches so far have tackled quality estimation as a supervised learning problem (He et al., 2010; Soricut and Echihabi, 2010; Specia et al., 2010; Specia, 2011). A wide variety of features have been proposed, most of which can be described as loosely ‘linguistic’ features that describe the source sentence, the target sentence and the association between them (Callison-Burch et al., 2012). Surprisingly enough, information used by the decoder to choose the best translation in the search space, such as its internal scores, have hardly been considered and never proved to be useful. Indeed, it is well-known that these scores are hard to interpret and to compare across hypotheses. Furthermore, mapping scores of a linear classifier (such as the scores estim"
W13-2250,2013.tc-1.10,0,0.038752,"Description LIMSI has participated to the tasks 1-1 (prediction of the hTER) and 1-3 (prediction of the postedition time). Similar features and learning algorithms have been considered for the two tasks. We will first quickly describe them before discussing the specific development made for task 1-3. 1 Our implementation can be downloaded from http:// perso.limsi.fr/Individu/wisniews/. 399 3.1 Features 3.2 In addition to the features described in the previous section, 176 ‘standard’ features for quality estimation have been considered. The full list of features we have considered is given in (Wisniewski et al., 2013) and the features set can be downloaded from our website.2 These features can be classified into four broad categories: Learning Methods The main focus of this work is to study the relevance of features for quality estimation; therefore, only very standard learning methods were used in our work. For this year submission both random forests (Breiman, 2001) and elastic net regression (Zou and Hastie, 2005) have been used. The capacity of random forests to take into account complex interactions between features has proved to be a key element in the results achieved in our experiments with last ye"
W13-2250,W12-3120,1,0.636027,"can be downloaded from our website.2 These features can be classified into four broad categories: Learning Methods The main focus of this work is to study the relevance of features for quality estimation; therefore, only very standard learning methods were used in our work. For this year submission both random forests (Breiman, 2001) and elastic net regression (Zou and Hastie, 2005) have been used. The capacity of random forests to take into account complex interactions between features has proved to be a key element in the results achieved in our experiments with last year campaign datasets (Zhuang et al., 2012). As we are considering a larger features set this year and the number of examples is comparatively quite small, we also considered elastic regression, a linear model trained with L1 and L2 priors as regularizers, hoping that training a sparse model would reduce the risk of overfitting. In this study, we have used the implementation provided by scikit-learn (Pedregosa et al., 2011). As detailed in Section 4.1, cross-validation has been used to choose the hyper-parameters of all regressors, namely the number of estimators, the maximal depth of a tree and the minimum number of examples in a leaf"
W13-2250,P10-1064,0,\N,Missing
W14-5208,agarwal-etal-2012-gui,1,0.829058,"annotation, even going back to an earlier stage, if necessary, to correct mistakes. It allows all the annotation information to be situated in one contiguous place. The interface uses the Java API for SSF, which is perhaps the most developed among the different APIs for SSF. The API (a part of Sanchay) again allows transparency for the programmer as far as manipulating the data is concerned. It also ensures that there are fewer bugs when new programmers work on any part of the system where SSF data is being used. One recent addition to the interface was a GUI to correct mistakes in treebanks (Agarwal et al., 2012). The syntactic annotation interface is not the only interface in Sanchay that uses SSF. Some other interfaces do that too. For example, there are sentence alignment and word alignment interfaces, which also use the same format for similar reasons. Thus, it is even possible to build parallel treebanks in SSF 74 using the Sanchay interfaces. Then there are other tools in Sanchay such as the integrated tool for accessing language resources (Singh and Ambati, 2010). This tool allows various kinds of language resources, including those in SSF, to be accessed, searched and manipulated through the i"
W14-5208,I08-2099,1,0.812263,"r every module. Readability of SSF helps in development and debugging because the input and output of any module can be easily seen and read by humans, whether linguists or programmers. Even if a module fails, SSF helps to run the modules without any effect on normal operation of system. In such a case, the output SSF would have unfilled value of an attribute and downstream modules continue to operate on the data stream. 6.2 Annotation Interfaces and Other Tools Sanchay, mentioned above, has a syntactic annotation interface that has been used for development of treebanks for Indian languages (Begum et al., 2008). These treebanks have been one of the primary sources of information for the development the Sampark machine translation systems, among other things. This syntactic annotation interface provides facilities for everything that is required to be done to transform the selected data in the raw text format to the final annotated treebank. The usual stages of annotation include POS tagging, morphological annotation, chunking and dependency annotation. This interface has evolved over a period of several years based on the feedback received from the annotators and other users. There are plans to use"
W14-5208,J95-3006,0,0.42936,"Kannada and 4 bi-directional systems between Tamil and Malayalam / Telugu. Out of these, 8 pairs have been exposed via a web interface. A REST API is also available to acess the machine translation system over the Internet. 6 http://sanchay.co.in There are 22 constitutionally recognized languages in India, and many more which are not recognized. Hindi, Bengali, Telugu, Marathi, Tamil and Urdu are among the major languages of the world in terms of number of speakers, summing up to a total of 850 million. 8 73 http://sampark.org.in 7 The Sampark system uses Computational Paninian Grammar (CPG) (Bharati et al., 1995), in combination with machine learning. Thus, it is a hybrid system using both rule-based and statistical approaches. There are 13 major modules that together form a hybrid system. The machine translation system is based on the analyze-transfer-generate paradigm. It starts with an analysis of the source language sentence. Then a transfer of structure and vocabulary to target language is carried out. Finally the target language is generated. One of the benefits of this approach is that the language analyzer for a particular language can be developed once and then be combined with generators for"
W14-5208,W02-0109,0,0.181672,"riously threaten the sustainability of the system. This may apply to all large software systems, but the complexities associated with humans languages (both within and across languages) only add to the problem. To make it possible to build various components of an infrastructure that scales within and across languages for a wide variety of purposes, and to be able to do it by re-using the representation(s) and the code, deserves to be considered an achievement. GATE1 (Cunningham et al., 2011; Li et al., 2009), UIMA2 (Ferrucci and Lally, 2004; Bari et al., 2013; Noh and Pad´o, 2013) and NLTK3 (Bird, 2002) are well known achievements of this kind. This paper is about one other such effort that has proved to be successful over the last decade or more. 2 Related Work GATE is designed to be an architecture, a framework and a development environment, quite like UIMA, although the two differ in their realization of this goal. It enables users to develop and deploy robust language engineering components and resources. It also comes bundled with several commonly used baseline Natural Language Processing (NLP) applications. It makes strict distinction between data, algorithms, and ways of visualising t"
W14-5208,N10-1093,1,0.804836,"is also a text editor in Sanchay that is specially tailored for Indian languages and it can validate SSF (Singh, 2008). The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can be used for data in SSF is another big facilitator for anyone who wants to build new tools for language processing and wants to operate on linguistic data. Apart from these, a number of research projects have used SSF (the representation or the analyzer) directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharati et al., 2009; Gadde et al., 2010; Husain et al., 2011). 7 Conclusion We described a readable representation scheme called Shakti Standard Format (SSF). We showed how this scheme (an instance of the blackboard architectural model), which is based on certain organizational principles such as modularity, simplicity, robustness and transparency, can be used to create not only a linguistic analysis engine (Shakti Natural Language Analyzer), but can be used for arbitrary other purposes wherever linguistic analysis is one of the tasks. We briefly described the machine translation systems (Shakti and Sampark) which use this scheme a"
W14-5208,I11-1143,1,0.813905,"r in Sanchay that is specially tailored for Indian languages and it can validate SSF (Singh, 2008). The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can be used for data in SSF is another big facilitator for anyone who wants to build new tools for language processing and wants to operate on linguistic data. Apart from these, a number of research projects have used SSF (the representation or the analyzer) directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharati et al., 2009; Gadde et al., 2010; Husain et al., 2011). 7 Conclusion We described a readable representation scheme called Shakti Standard Format (SSF). We showed how this scheme (an instance of the blackboard architectural model), which is based on certain organizational principles such as modularity, simplicity, robustness and transparency, can be used to create not only a linguistic analysis engine (Shakti Natural Language Analyzer), but can be used for arbitrary other purposes wherever linguistic analysis is one of the tasks. We briefly described the machine translation systems (Shakti and Sampark) which use this scheme at their core level. Si"
W14-5208,singh-ambati-2010-integrated,1,0.859138,"n any part of the system where SSF data is being used. One recent addition to the interface was a GUI to correct mistakes in treebanks (Agarwal et al., 2012). The syntactic annotation interface is not the only interface in Sanchay that uses SSF. Some other interfaces do that too. For example, there are sentence alignment and word alignment interfaces, which also use the same format for similar reasons. Thus, it is even possible to build parallel treebanks in SSF 74 using the Sanchay interfaces. Then there are other tools in Sanchay such as the integrated tool for accessing language resources (Singh and Ambati, 2010). This tool allows various kinds of language resources, including those in SSF, to be accessed, searched and manipulated through the inter-connected annotation interfaces and the SSF API. There is also a text editor in Sanchay that is specially tailored for Indian languages and it can validate SSF (Singh, 2008). The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can be used for data in SSF is another big facilitator for anyone who wants to build new tools for language processing and wants to operate on linguistic data. Apart from these, a number o"
W14-5208,I08-2141,1,0.649924,"and word alignment interfaces, which also use the same format for similar reasons. Thus, it is even possible to build parallel treebanks in SSF 74 using the Sanchay interfaces. Then there are other tools in Sanchay such as the integrated tool for accessing language resources (Singh and Ambati, 2010). This tool allows various kinds of language resources, including those in SSF, to be accessed, searched and manipulated through the inter-connected annotation interfaces and the SSF API. There is also a text editor in Sanchay that is specially tailored for Indian languages and it can validate SSF (Singh, 2008). The availability of a corpus query language (section-5.2) that is implemented in Sanchay and that can be used for data in SSF is another big facilitator for anyone who wants to build new tools for language processing and wants to operate on linguistic data. Apart from these, a number of research projects have used SSF (the representation or the analyzer) directly or indirectly, that is, either for theoretical frameworks or as part of the implementation (Bharati et al., 2009; Gadde et al., 2010; Husain et al., 2011). 7 Conclusion We described a readable representation scheme called Shakti Sta"
W14-5208,singh-2012-concise,1,0.790337,"ure tree would not allow. This overlaying of constrained links over the core trees allows multiple layers and/or types of annotation to be stored in the same structure. With a little more improvisation, we can even have links across sentences, i.e., at the discourse level (see section-3.3). It is possible, for example, to have a phrase structure tree (the core tree) overlaid with a dependency tree (via constrained links or ‘threads’), just as it is possible to have POS tagged and chunked data to be overlaid with named entities and discourse relations. The Sanchay Corpus Query Language (SCQL) (Singh, 2012) is a query language designed for threaded trees. It so turns out that SSF is also a representation that can be viewed as threaded trees. Thus, the SCQL can work over data in SSF. This language has a simple, intuitive and concise syntax and high expressive power. It allows not only to search for complicated patterns with short queries but also allows data manipulation and specification of arbitrary return values. Many of the commonly used tasks that otherwise require writing programs, can be performed with one or more queries. 6 Applications 6.1 Sampark Machine Translation Architecture Overcom"
W17-0912,K15-2001,0,0.0273905,"training set) could perfectly link the context and the ending. We modified all the stories such that a manually introduced symbol (like ‘CCC’: not in the vocabulary) separates the first four sentences from the fifth sentence, and its representation is learned by training a word2vec model on the data. On the test and validation set, the hypothesis whose representation is the closest to the sum of the vectors of the context and the connective symbol is chosen as the prediction. The intuition comes from the implicit connective sense classification task for the Shallow Discourse Parsing problem (Xue et al., 2015). Context window size 100 and dimensionality 300 were found to be the optimal hyperparameters in our experiments. 3. Gensim Doc2Vec: Distributed representation of documents and sentences extends the concept of word vectors to larger textual units (Le and Mikolov, 2014). A host of variations were tried (as provided by Python’s ˇ uˇrek and Sojka, Gensim functionality (Reh˚ 2010)). The distributed bag of words model (dbow) along with a context window of three words was found to give the best results for this approach (Table 1). This approach is again trying to model semantic similarity via senten"
W17-0912,P14-5010,0,0.0046206,"ong Short-Term Memory (LSTM) network (Mueller and Thyagarajan, 2016). The model is implemented as in the paper - using the SICK training set and Google word2vec, with the weights optimized as per the SemEval 2014 task on semantic similarity of sentences (Marelli et al., 2014). This is one of the current state of the art models for capturing semantic similarity. 5. Sentiment: In this approach, we choose the hypothesis that matches the average sentiment of the context. We use NLTK VADER Sentiment Analyzer (Hutto and Gilbert, 2014) instead of the Stanford Core NLP tool for sentiment analysis by (Manning et al., 2014) as used in (Mostafazadeh et al., 2016) due to notably better results (Table 1). In our experiments on the validation set, matching sentiment of the full context instead of just the last one/two/three sentence(s) gives the best performance for this approach. This approach does not use semantic similarity. Combining the above three – called word2vec (combined) approach – through simple voting produced slightly better results than with any individual variation (as can be seen in Table 1), and thus we used this 3 The Ensemble Model We tried various ways of combining the power of the different app"
W17-0912,S14-2001,0,0.0363938,"context window of three words was found to give the best results for this approach (Table 1). This approach is again trying to model semantic similarity via sentence embedding. 4. Siamese LSTM: We also implement a deep neural network model for assessing the semantic similarity between a pair of sentences. It uses a Siamese adaptation of the Long Short-Term Memory (LSTM) network (Mueller and Thyagarajan, 2016). The model is implemented as in the paper - using the SICK training set and Google word2vec, with the weights optimized as per the SemEval 2014 task on semantic similarity of sentences (Marelli et al., 2014). This is one of the current state of the art models for capturing semantic similarity. 5. Sentiment: In this approach, we choose the hypothesis that matches the average sentiment of the context. We use NLTK VADER Sentiment Analyzer (Hutto and Gilbert, 2014) instead of the Stanford Core NLP tool for sentiment analysis by (Manning et al., 2014) as used in (Mostafazadeh et al., 2016) due to notably better results (Table 1). In our experiments on the validation set, matching sentiment of the full context instead of just the last one/two/three sentence(s) gives the best performance for this approa"
W17-0912,N16-1098,0,0.4846,"Shared Task 2017 - the Story Cloze Test. The main conclusion from our results is that an approach based on semantic similarity alone may not be enough for this task. We test various approaches and compare them with two ensemble systems. One is based on voting and the other on logistic regression based classifier. Our final system is able to outperform the previous state of the art for the Story Cloze test. Another very interesting observation is the performance of sentiment based approach which works almost as well on its own as our final ensemble system. 1 Introduction The Story Cloze Test (Mostafazadeh et al., 2016) is a recently introduced framework to evaluate story understanding and script learning. Representation of commonsense knowledge is major theme in Natural Language Processing and is also important for this task. The organizers provide a training corpus called the ROCStories dataset (we will refer to it as the Story Cloze corpus or dataset). It consists of very simple 98161 everyday life stories (combining the spring and winter training sets). All stories consist of five sentences which capture ‘causal and temporal common sense relations between daily events’. The validation and test sets conta"
W17-0912,D13-1170,0,0.0160699,"Missing"
W17-4007,D07-1091,0,0.0664239,"l text chunks (called ‘phrases’) without the utilization of any explicit linguistic information (such as morphological, syntactic, or semantic). That is, it considers only the surface form of words to create a phrase table. Such additional information can be incorporated in the form of ‘factors’, along with the words or characters (in case of transliteration) to improve the accuracy of standard SMT. • We also show that proposed phoneme chunk based method for word transduction performs better than the standard Statistical Machine Translation as well as factored Statistical Machine Translation (Koehn and Hoang, 2007) when applied on the same dataset using the Moses decoder (Koehn et al., 2007). 2 Related Work In a parallel corpus of a language and its dialect (or closely related language), words can be categorized into two categories based on their pronunciation or orthographic form: Surface form, along with these factors, creates factored representation of each word (Koehn et al., 2007; Koehn and Hoang, 2007). For factored SMT, we augmented each letter of Devanagari with their phonetic features (described in the next section) to create its factored representation. This factored Statistical Machine Transl"
W17-4007,P07-2045,0,0.0825759,"ic information (such as morphological, syntactic, or semantic). That is, it considers only the surface form of words to create a phrase table. Such additional information can be incorporated in the form of ‘factors’, along with the words or characters (in case of transliteration) to improve the accuracy of standard SMT. • We also show that proposed phoneme chunk based method for word transduction performs better than the standard Statistical Machine Translation as well as factored Statistical Machine Translation (Koehn and Hoang, 2007) when applied on the same dataset using the Moses decoder (Koehn et al., 2007). 2 Related Work In a parallel corpus of a language and its dialect (or closely related language), words can be categorized into two categories based on their pronunciation or orthographic form: Surface form, along with these factors, creates factored representation of each word (Koehn et al., 2007; Koehn and Hoang, 2007). For factored SMT, we augmented each letter of Devanagari with their phonetic features (described in the next section) to create its factored representation. This factored Statistical Machine Translation at character level performed better than our baseline SMT system. • Word"
W17-4007,J03-1002,0,0.00944712,"word. Statistical Machine Translation For this work, a bilingual Hindi-Bhojpuri Machine Translation Model has been used as the baseline by using the Moses decoder. Moses requires a parallel corpus (e.g. Hindi and Bhojpuri) that is used for training the system. For this task, each letter from the parallel corpus (parallel word list) of Hindi-Bhojpuri language pair was treated as if it was a word of a sentence and each word was treated as a single sentence. In other words, machine translation was performed at the character level. The publicly available tool GIZA++ was used to align the letters (Och and Ney, 2003). IRSTLM (Federico et al., 2008) was used to create the language model, which computes the probability of target language sentences (words in our case). Language model was prepared using the 19532 words, compiled from a Bhojpuri newspaWA = N umber of correct translation T otal number of transduced words N P D(T, B) = P D(T, B) − P Dmin P Dmax − P Dmin where P D(T, B) is phonetic distance between transduced output T , and correct transduction B, computed using phonetic model as described by Singh (2006), P Dmin , P Dmax are minimum and maximum phonetic distance between transduced 5 61 http://ta"
W17-4007,P02-1040,0,0.105722,"ures as the factors performs better than standard SMT. However our phoneme chunk based method performs better than the other methods. This could be because we do not have enough data for SMT, which is the common scenario for resource-scarce languages. 4.6 1. Hindi-Bhojpuri word pairs of the test corpus (shown by the curve Dataset in Figure 4) 2. Generated output by the SMT technique and its correct Bhojpuri output (shown by the curve SM T in Figure 4) 3. Generated output by the FSMT1 technique and its correct Bhojpuri output (shown by the curve F SM T 1 in Figure 4) BLEU Score The BLEU score (Papineni et al., 2002) (which is one of the most popular measures for machine translation) for all the methods is summarised in Table 5. Here also our method significantly outperforms other methods for our language-pair. Method BLEU Score SMT FSMT1 FSMT2 Proposed Method 75.05 75.92 76.18 79.82 4. Generated output by the FSMT2 technique and its correct Bhojpuri output (shown by the curve F SM T 2 in Figure 4) 5. Generated output by the proposed technique and its correct Bhojpuri output (shown by the curve P roposed M ethod in Figure 4) From the comparison we can conclude that proposed method has better performance t"
W17-4007,W09-3510,0,0.0328608,"ord transduction’ approach, which can show noticeable improvement in inter-dialectal translation by transducing OOV words. We define the term word transduction as the conversion of words from one source language to another closely related target language such that pronunciation and meaning are similar. This can have two aspects. One is cognate generation, while the other is adapting borrowed words from the source language to the target language such that it matches the phonology of the target language. In other words, word transduction can be seen as transliteration (Denoual and Lepage, 2006; Finch and Sumita, 2009) in the same script to incorporate phonological changes between a pair of closely related languages. • We propose a phoneme chunk based method for word transduction which transduces words of Hindi to its closely related language Bhojpuri. Using the method described in this paper, we try to predict Bhojpuri pronunciation from its corresponding Hindi word. This method is adapted from extensively reported earlier work on similar problems. Since the problem of machine transliteration can also be viewed as the process of machine translation at the character level, we have used the popular phrase-ba"
W17-4007,P95-1002,0,0.318569,"Missing"
W17-4007,A00-1002,0,\N,Missing
W17-7504,baccianella-etal-2010-sentiwordnet,0,0.0297308,"of punctuation characters to the total count of characters in the reference sentence. 6. Count of long words∗∗ : The count of words in the reference sentence exceeding six letters in length. 7. Average word Length∗∗ : The ratio of count of total characters in a word to the count of words in the reference sentence. 8. Count of named entities: The number of named entities in the reference sentence. 9. Average sentiment score∗∗ : The overall positive and negative sentiment score of the reference sentence averaged over all the words, based on the SentiWordNet 3.0 lexical resource as described by Baccianella et al. (2010). 3.2.1 Boosting ensemble algorithms work by creating a sequence of models that attempt to correct the mistakes of the models used before them in the sequence. Therefore, these offer the added benefit of combining outputs from weak learners (those whose performance is at least better than random chance) to create a strong learner with improved prediction performance, by paying higher focus on instances that have been misclassified or have higher errors by preceding weak rules. This is assisted by a majority vote of the weak learner’s predictions weighted by their individual accuracy. Figure 1"
W17-7504,W16-1511,0,0.0678317,"Missing"
W17-7504,D14-1181,0,0.00405957,"itance and the reference sentences. All sentences other than the top-k are not included in the final output text span, even though the model might have labelled them as positive. base classifiers in a GBC are regression trees. Since our task is a binary classification, only one regression tree is used as a special case. 3.2.2 Convolutional Neural Network Convolutional Neural Networks, as described in Schmidhuber (2015), have the ability to extract features of high-level abstraction with minimum pre-processing of data. They have been widely used for sentence classification problems, such as by Kim (2014). Recently, Ngoc Giang et al. (2016) also used CNNs for a sequence classification problem involving classification of DNA sequences by considering these sequences as text data. Given the success of CNNs on these, we explore their use in our task. However, in our case, a class-imbalance problem occurs due to the number of positive reference-citance instances being far too low (495). These are too few examples for any deep learning model to extract meaningful features from the original text. Using the original sentences, and modeling it directly as a sequence classification on pairs of sentences"
W17-7504,W16-1514,0,0.104935,"he binary classification model, with L2-SVM performing the best. Moraes et al. (2016) used SVM with subset tree kernel, a type of convolution kernel. They computed similarities between three tree representations of the citance and reference text. Li et al. (2016) used an SVM classifier with a topical lexicon to identify the best matching reference spans for a citance, using IFD similarity, Jaccard similarity and context similarity. The PolyU system by Cao et al. (2016), for Task 1a, used SVM-rank with lexical and document structural features to rank reference text sentences for every citance. Klampfl et al. (2016) applied a modified version of an unsupervised summarization technique (TextSentenceRank) to the reference document. Nomoto (2016) treated the problem as a ranking problem, learning one component of the similarity through a neural network and using TF-IDF scores for the other component. Aggarwal and Sharma (2016a) employed lexical and syntactic dependency24 cues in writing rules to extract text spans 3 Method The structure of the dataset is described in Section 4.1. Citances and their actual reference texts are extracted from gold-standard annotations. Citances in CPs are paired with each sent"
W17-7504,J02-4002,0,0.148257,"s, etc. They then used the facet information to match each sentence to a citance having the same facet in the CP. ing this feature set. Feature representations extracted (as described later), are used to train three binary classifiers - an Adaptive Boosting Classifier (ABC), a Gradient Boosting Classifier (GBC) and a CNN classifier. The datasets provided for this year’s as well as last year’s shared task have been used. 2 Related Work There has been a large amount of work on the task of summarizing scientific documents. However, as is clear from review surveys and papers such as Jones (2007), Teufel and Moens (2002) and Nenkova (2011), just using citances of a paper does not taken into account the context of a user or place the paper in a larger perspective of related work. Most of the related work on the task of identifying text spans in the RP that correspond to a particular citance, have been presented at the shared task mentioned in the previous section. We highlight some relevant work and various methods used for this task. Yeh et al. (2017) also used a binary classification approach for Task 1A, as we do. They used five classification algorithms to learn the binary classification model, with L2-SVM"
W17-7504,W16-1518,0,0.0178972,"entifying text spans in the RP that correspond to a particular citance, have been presented at the shared task mentioned in the previous section. We highlight some relevant work and various methods used for this task. Yeh et al. (2017) also used a binary classification approach for Task 1A, as we do. They used five classification algorithms to learn the binary classification model, with L2-SVM performing the best. Moraes et al. (2016) used SVM with subset tree kernel, a type of convolution kernel. They computed similarities between three tree representations of the citance and reference text. Li et al. (2016) used an SVM classifier with a topical lexicon to identify the best matching reference spans for a citance, using IFD similarity, Jaccard similarity and context similarity. The PolyU system by Cao et al. (2016), for Task 1a, used SVM-rank with lexical and document structural features to rank reference text sentences for every citance. Klampfl et al. (2016) applied a modified version of an unsupervised summarization technique (TextSentenceRank) to the reference document. Nomoto (2016) treated the problem as a ranking problem, learning one component of the similarity through a neural network and"
W17-7504,D15-1166,0,0.137896,"Missing"
W17-7504,W16-1517,0,0.0211853,"context in which the RP is written, key ideas behind the RP and a concise synopsis of it. All of this is impor• We explore various measures for evaluattant for a task like scientific paper summaing similarity between texts while buildrization, which not only requires the content23 1 of a paper but also meta-information http://wing.comp.nus.edu.sg/ cl-scisumm2017/ S Bandyopadhyay, D S Sharma and R Sangal. Proc. of about the 14th Intl. Conference on Natural Language Processing, pages 23–32, c Kolkata, India. December 2017. 2016 NLP Association of India (NLPAI) in the RP for a given CP citance. Malenfant and Lapalme (2016) presented a novel approach to solve this task. They first performed another task of identifying the facet of each sentence of the RP. These facets belonged to a predefined set of facets, such as introduction, abstract, results, etc. They then used the facet information to match each sentence to a citance having the same facet in the CP. ing this feature set. Feature representations extracted (as described later), are used to train three binary classifiers - an Adaptive Boosting Classifier (ABC), a Gradient Boosting Classifier (GBC) and a CNN classifier. The datasets provided for this year’s a"
W17-7504,H92-1116,0,0.568598,"orporate several modified and newly added features. The features marked by an asterisk (∗ ) are the ones that are borrowed, but modified. The features marked by two asterisks (∗∗ ) are the newly added features in this work. For features that have been borrowed from Yeh et al. (2017), more elaborate details about them can be seen in their work. 3.1.1 3.1.2 Knowledge-based features 1. WordNet-based semantic similarity∗ : The best semantic similarity score between words in the citance and the reference sentence out of all the sets of cognitive synonyms (synsets) present in the WordNet, following Miller (1992) and Pedersen et al. (2004). Lexical features This class deals with the features representing similarity measure of words for each pair of citance and reference sentence. As suggested by the results of Kenter et al. (2016) for short text similarity tasks, the overall sentence similarity measure based on each feature is calculated by averaging the similarities over all the words in the sentences. 3.1.3 Corpus-based feature 1. Word2Vec-based semantic similarity∗∗ : The word-to-word semantic similarity between the citance and the reference sentence is obtained based on the pre-trained embedding v"
W17-7504,W16-1513,0,0.0273887,"ust using citances of a paper does not taken into account the context of a user or place the paper in a larger perspective of related work. Most of the related work on the task of identifying text spans in the RP that correspond to a particular citance, have been presented at the shared task mentioned in the previous section. We highlight some relevant work and various methods used for this task. Yeh et al. (2017) also used a binary classification approach for Task 1A, as we do. They used five classification algorithms to learn the binary classification model, with L2-SVM performing the best. Moraes et al. (2016) used SVM with subset tree kernel, a type of convolution kernel. They computed similarities between three tree representations of the citance and reference text. Li et al. (2016) used an SVM classifier with a topical lexicon to identify the best matching reference spans for a citance, using IFD similarity, Jaccard similarity and context similarity. The PolyU system by Cao et al. (2016), for Task 1a, used SVM-rank with lexical and document structural features to rank reference text sentences for every citance. Klampfl et al. (2016) applied a modified version of an unsupervised summarization tec"
W17-7504,W16-1519,0,0.0175431,"ion kernel. They computed similarities between three tree representations of the citance and reference text. Li et al. (2016) used an SVM classifier with a topical lexicon to identify the best matching reference spans for a citance, using IFD similarity, Jaccard similarity and context similarity. The PolyU system by Cao et al. (2016), for Task 1a, used SVM-rank with lexical and document structural features to rank reference text sentences for every citance. Klampfl et al. (2016) applied a modified version of an unsupervised summarization technique (TextSentenceRank) to the reference document. Nomoto (2016) treated the problem as a ranking problem, learning one component of the similarity through a neural network and using TF-IDF scores for the other component. Aggarwal and Sharma (2016a) employed lexical and syntactic dependency24 cues in writing rules to extract text spans 3 Method The structure of the dataset is described in Section 4.1. Citances and their actual reference texts are extracted from gold-standard annotations. Citances in CPs are paired with each sentence in the RPs, along with a binary label indicating their actual reference relations - 0 if the citance actually refers to the R"
W17-7559,R13-1071,0,0.0273551,"arlsson et al. (1995), Brill (1992)) and required a set of hand-crafted rules learnt from a tagged corpus. More recently, Kessikbayeva and Cicekli (2016) present a morphological disambiguation system using rules based on disambiguations of context words. Statistical approaches are also used for morphological disambiguation. Hakkani-Tür et al. (2000) propose a model based on joint conditional probabilities of the root and tags. Sak et al. (2007) use a perceptron model, while other statistical models use decision trees as by Görgün and Yildiz (2011). Hybrid approaches have also been tried, with Orosz and Novák (2013) using an approach combining rule-based and statistical approaches, to prune grammar-violating parses. The use of deep learning for morphological disambiguation, has been explored. Straka and Straková (2017) build a neural system for tasks such as sentence segmentation, tokeniza486 tion and POS tagging. Plank et al. (2016) 3 Neural Models We present four models for morphological disambiguation. Some aspects are common among them. They all use a deep neural network, which, given the current word in con2 tures of all previous and following words in the window sideration and one of the candidate"
W17-7559,P16-2067,0,0.0168845,"akkani-Tür et al. (2000) propose a model based on joint conditional probabilities of the root and tags. Sak et al. (2007) use a perceptron model, while other statistical models use decision trees as by Görgün and Yildiz (2011). Hybrid approaches have also been tried, with Orosz and Novák (2013) using an approach combining rule-based and statistical approaches, to prune grammar-violating parses. The use of deep learning for morphological disambiguation, has been explored. Straka and Straková (2017) build a neural system for tasks such as sentence segmentation, tokeniza486 tion and POS tagging. Plank et al. (2016) 3 Neural Models We present four models for morphological disambiguation. Some aspects are common among them. They all use a deep neural network, which, given the current word in con2 tures of all previous and following words in the window sideration and one of the candidate morphological analyses of the word, acts as a binary true/false classifier. A final softmax layer outputs probabilities for correct and incorrect, based on whether the candidate analysis is correct or not. An ideal classifier would predict the probability of correct as 1 and incorrect as 0 for the correct morphological ana"
W17-7559,H92-1022,0,0.127982,"outperforms this state-of-the-art system. 2 Related Work There is very little directly corresponding previous work on morphological disambiguation and it cannot be formulated in the same way as POS tagging. This is because the number of classes is fixed in POS tagging, whereas it is variable in our problem. Still, since part of speech (POS) tagging is a closely related task, the work on POS tagging can also provide useful insights. However, morphological disambiguation is a harder task to perform than POS tagging. The earliest approaches to POS tagging were rule-based (Karlsson et al. (1995), Brill (1992)) and required a set of hand-crafted rules learnt from a tagged corpus. More recently, Kessikbayeva and Cicekli (2016) present a morphological disambiguation system using rules based on disambiguations of context words. Statistical approaches are also used for morphological disambiguation. Hakkani-Tür et al. (2000) propose a model based on joint conditional probabilities of the root and tags. Sak et al. (2007) use a perceptron model, while other statistical models use decision trees as by Görgün and Yildiz (2011). Hybrid approaches have also been tried, with Orosz and Novák (2013) using an app"
W17-7559,C16-1018,0,0.0183691,"m works on tasks such as POS tagging, we bear in mind that POS tagging and morphological disambiguation are significantly different. Morphological disambiguation is more complex because it works with multiple categories and not just partof-speech. This introduces sparseness in the model, as well as considerations of whether the different categories are dependent on each other, on how to combine classifiers for each category, etc. The number of analyses for a word also varies. Yildiz et al. (2016) propose a convolutional neural net architecture, which takes context disambiguation into account. Shen et al. (2016) use a deep neural model with character-level and well as tag-level LSTMs to embed analyses. Our work shares certain aspects in common with theirs but is different in many ways. We experiment on Hindi (which has significantly different morphological properties from the three languages they explore), use different neural structures, show the effect of language specific phonological features and study the impact of unsupervised pre-training of embeddings under different settings. Further, as mentioned earlier, we consider their results to be state-of-the-art because theirs is a language-agnostic"
W17-7559,N16-1077,0,0.0258981,"ogether make up a morphological analysis for a word. We use the term ‘candidate analysis’ to refer to each of the morphological analyses generated by the analyzer for a given word. 3.2 Broad Basis for the Architectures We first establish an intuitive and statistical foundation to justify our decision choices in building the deep neural network. We extract dependencies between roots and features from the work by Hakkani-Tür et al. (2000), noting that the assumptions used for Turkish by the authors hold good for Hindi too. We also obtain surface-information-related dependencies from the work by Faruqui et al. (2016). The following is the full set of extended dependencies: Figure 1: Architecture for the word surface vector. ‘i’ indicates the ith input word. 3.3 Word Input Word inputs to the network are embedded at two levels. A word embedding vector is generated using the word as a whole. Each character in the word is also embedded in a character embedding vector and these character embeddings are fed, in sequence, to a bidirectional GRU. The output vector of the GRU and the • Dependency #1: The root of a word 487 depends on the roots as well as the fea3 word embedding vector are concatenated together to"
W17-7559,K17-3009,0,0.0212358,"ng rules based on disambiguations of context words. Statistical approaches are also used for morphological disambiguation. Hakkani-Tür et al. (2000) propose a model based on joint conditional probabilities of the root and tags. Sak et al. (2007) use a perceptron model, while other statistical models use decision trees as by Görgün and Yildiz (2011). Hybrid approaches have also been tried, with Orosz and Novák (2013) using an approach combining rule-based and statistical approaches, to prune grammar-violating parses. The use of deep learning for morphological disambiguation, has been explored. Straka and Straková (2017) build a neural system for tasks such as sentence segmentation, tokeniza486 tion and POS tagging. Plank et al. (2016) 3 Neural Models We present four models for morphological disambiguation. Some aspects are common among them. They all use a deep neural network, which, given the current word in con2 tures of all previous and following words in the window sideration and one of the candidate morphological analyses of the word, acts as a binary true/false classifier. A final softmax layer outputs probabilities for correct and incorrect, based on whether the candidate analysis is correct or not. A"
W17-7559,C00-1042,0,0.234779,"blem. Still, since part of speech (POS) tagging is a closely related task, the work on POS tagging can also provide useful insights. However, morphological disambiguation is a harder task to perform than POS tagging. The earliest approaches to POS tagging were rule-based (Karlsson et al. (1995), Brill (1992)) and required a set of hand-crafted rules learnt from a tagged corpus. More recently, Kessikbayeva and Cicekli (2016) present a morphological disambiguation system using rules based on disambiguations of context words. Statistical approaches are also used for morphological disambiguation. Hakkani-Tür et al. (2000) propose a model based on joint conditional probabilities of the root and tags. Sak et al. (2007) use a perceptron model, while other statistical models use decision trees as by Görgün and Yildiz (2011). Hybrid approaches have also been tried, with Orosz and Novák (2013) using an approach combining rule-based and statistical approaches, to prune grammar-violating parses. The use of deep learning for morphological disambiguation, has been explored. Straka and Straková (2017) build a neural system for tasks such as sentence segmentation, tokeniza486 tion and POS tagging. Plank et al. (2016) 3 Ne"
W18-0914,W18-0907,0,0.315079,"ntactic, conceptual, affective, and contextual (word embeddings) features. Beigman Klebanov et al. (2016) experimented with unigrams, WordNet (Miller, 1995) and VerbNet (Schuler, 2006) based features for detection of verb metaphors. Introduction Lakoff (1993) defines a metaphorical expression as a linguistic expression which is the surface realization of a cross-domain mapping in a conceptual system. On one hand, metaphors play a significant role in making a language more creative. On the other, they also make language understanding difficult for artificial systems. Metaphor Shared Task 2018 (Leong et al., 2018) aims to explore various approaches for word-level metaphor detection in sentences. The task is to predict whether the target word in the given sentence is metaphoric or not. There are two categories for this shared task. The first one, All POS, tests the models for content words from all types of POS among nouns, adjectives, adverbs and verbs, while the second category, Verbs, tests the models only for verbs. 2 3 Data The dataset provided for this task is VU Amsterdam Metaphor Corpus (VUAMC). VUAMC is extracted from the British National Corpus (BNC Baby) and is annotated using MIPVU Procedure"
W18-0914,W14-2302,0,0.0735464,"detecting its metaphoricity. In this paper, we present a deep neural architecture for metaphor detection which exploits this contrast. Additionally, we also use cost-sensitive learning by re-weighting examples, and baseline features like concreteness ratings, POS and WordNet-based features. The best performing system of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018. 1 Other attempts which employ supervised learning approaches for metaphor detection on VUAMC corpus involve the use of logistic classifier (Beigman Klebanov et al., 2014) on a set of features, which include unigrams, topic models, POS, and concreteness features. Later, Beigman Klebanov et al. (2015) showed a significant improvement by re-weighting examples for cost sensitive learning and experimenting with concreteness information. Gargett and Barnden (2015) focused on utilizing the interactions between concreteness, imageability, and affective meaning for metaphor detection. Rai et al. (2016) explored Conditional Random Fields with syntactic, conceptual, affective, and contextual (word embeddings) features. Beigman Klebanov et al. (2016) experimented with uni"
W18-0914,D14-1162,0,0.0816414,"×k) W(2) ∈ IR , W(3) ∈ IR , W(4) ∈ (2×m) IR are the corresponding weight matrices, b(2) ∈ IRm , b(3) ∈ IR2 , b(4) ∈ IR2 are the corresponding biases, gbaseline ∈ IRk is the baseline feature vector and α is a trainable variable which determines the weights to be given to the baseline features and the contrast features. 5 Value 300 200 0.15 0.3 30 2 Implementation Details 6 We split the provided training data in 90:10 ratio as training set and development set. We use this development set to tune our hyperparameters for the different variations of our model. We use 300-dimensional GloVe vectors (Pennington et al., 2014) trained on 6B Common Crawl corpus as word embeddings, setting the embeddings of outof-vocabulary words to zero. To prevent overfitting on the training set, we use dropout regularization (Srivastava et al., 2014) and early stopping (Yao et al., 2007). We set the minibatch size to 50 examples and we zero pad the A and B split sets (as defined in section 4.1 ). More details on the hyperparameter settings can be found in the table 4. Experiments and Evaluation In this section, we present evaluation results for our model. Table 3 shows their comparison on the test set using F1 score as the metric"
W18-0914,W15-1402,0,0.118099,"rast. Additionally, we also use cost-sensitive learning by re-weighting examples, and baseline features like concreteness ratings, POS and WordNet-based features. The best performing system of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018. 1 Other attempts which employ supervised learning approaches for metaphor detection on VUAMC corpus involve the use of logistic classifier (Beigman Klebanov et al., 2014) on a set of features, which include unigrams, topic models, POS, and concreteness features. Later, Beigman Klebanov et al. (2015) showed a significant improvement by re-weighting examples for cost sensitive learning and experimenting with concreteness information. Gargett and Barnden (2015) focused on utilizing the interactions between concreteness, imageability, and affective meaning for metaphor detection. Rai et al. (2016) explored Conditional Random Fields with syntactic, conceptual, affective, and contextual (word embeddings) features. Beigman Klebanov et al. (2016) experimented with unigrams, WordNet (Miller, 1995) and VerbNet (Schuler, 2006) based features for detection of verb metaphors. Introduction Lakoff (199"
W18-0914,W16-1103,0,0.0350068,"Missing"
W18-0914,P16-2017,0,0.239412,"c classifier (Beigman Klebanov et al., 2014) on a set of features, which include unigrams, topic models, POS, and concreteness features. Later, Beigman Klebanov et al. (2015) showed a significant improvement by re-weighting examples for cost sensitive learning and experimenting with concreteness information. Gargett and Barnden (2015) focused on utilizing the interactions between concreteness, imageability, and affective meaning for metaphor detection. Rai et al. (2016) explored Conditional Random Fields with syntactic, conceptual, affective, and contextual (word embeddings) features. Beigman Klebanov et al. (2016) experimented with unigrams, WordNet (Miller, 1995) and VerbNet (Schuler, 2006) based features for detection of verb metaphors. Introduction Lakoff (1993) defines a metaphorical expression as a linguistic expression which is the surface realization of a cross-domain mapping in a conceptual system. On one hand, metaphors play a significant role in making a language more creative. On the other, they also make language understanding difficult for artificial systems. Metaphor Shared Task 2018 (Leong et al., 2018) aims to explore various approaches for word-level metaphor detection in sentences. Th"
W18-0914,D17-1162,0,0.426329,"Missing"
W18-0914,S16-2003,0,0.237659,"Missing"
W18-0914,W16-1104,0,0.613615,"Missing"
W18-0914,W15-1403,0,0.0467642,"res. The best performing system of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018. 1 Other attempts which employ supervised learning approaches for metaphor detection on VUAMC corpus involve the use of logistic classifier (Beigman Klebanov et al., 2014) on a set of features, which include unigrams, topic models, POS, and concreteness features. Later, Beigman Klebanov et al. (2015) showed a significant improvement by re-weighting examples for cost sensitive learning and experimenting with concreteness information. Gargett and Barnden (2015) focused on utilizing the interactions between concreteness, imageability, and affective meaning for metaphor detection. Rai et al. (2016) explored Conditional Random Fields with syntactic, conceptual, affective, and contextual (word embeddings) features. Beigman Klebanov et al. (2016) experimented with unigrams, WordNet (Miller, 1995) and VerbNet (Schuler, 2006) based features for detection of verb metaphors. Introduction Lakoff (1993) defines a metaphorical expression as a linguistic expression which is the surface realization of a cross-domain mapping in a conceptual system. On one hand, me"
W18-0914,P14-1024,0,0.394379,"Missing"
W18-3220,W15-4322,0,0.0531338,"Missing"
W18-3220,W18-3219,0,0.0800136,"Missing"
W18-3220,W17-2606,0,0.04559,"Missing"
W18-3220,W17-4419,0,0.277058,"trainable weight vector, bg is the bias and σ(·) is the sigmoid function. The result of this layer X is then concatenated with the gazetteer embeddings of the token. Fully Connected Network: We use two fully connected networks after the concatenation of the gate network output and gazetteer embeddings. The number of dense units is kept fixed to 100 each. The activation function is varied according to Table 1 for producing different models. Multitask Learning: Multitask learning has been shown as a good way to regularize models (Baxter, 2000; Collobert and Weston, 2008). Following the work of Aguilar et al. (2017), we split the task into Named Entity (NE) categorization (classifying a token into one of the NE classes) and NE segmentation (classifying token as NE or Not-NE). We passed the dense layer’s output as input to these final classification layers. A softmax layer with 19 classes is used for the categorization task and a single sigmoid neuron is used for the segmentation task as depicted in Figure 1. The cross-entropy losses for these tasks are added to yield total loss for the model. BiLSTM for Word Representation: We use Bidirectional LSTM (Dyer et al., 2015) in the model to learn the contextua"
W18-3220,W17-4421,0,0.0450925,"NER to extract this useful information makes it an essential part of the Information Extraction pipeline. The social media platforms like Twitter, Reddit etc. have become a massive source of information due to their growth in the recent past. Performing NER on social texts can be challenging due to the unstructured and colloquial nature of social texts. Various attempts have been made in the past to solve the problem of NER on social texts (Derczynski et al., 2017; Strauss et al., 2016). However, most of the previous systems were developed to work with monolingual texts (Ritter et al., 2011; Lin et al., 2017), ignoring the phenomena of codeswitching (i.e., switching between different lan∗ 2 Proposed Approach This section describes feature representations, model description and the ensembling technique in detail. 2.1 Feature Representation The following representations are used to capture overall information for each token: Word, Character and Lexical representations. Word Representation: Word representations are created using concatenation of two separate representations, one based on the pre-trained word vecThese authors have equal contribution to the paper 148 Proceedings of The Third Workshop o"
W18-3220,P16-1101,0,0.0482739,"on is used for the segmentation task as depicted in Figure 1. The cross-entropy losses for these tasks are added to yield total loss for the model. BiLSTM for Word Representation: We use Bidirectional LSTM (Dyer et al., 2015) in the model to learn the contextual relationship between the words. Word representations described earlier are used as input to this layer. The BiLSTM layer consists of two LSTM layers having 3 units each. With one layer connected in the forward direction and the other layer connected in the backward direction, this captures the information from the past and the future (Ma and Hovy, 2016). The outputs of both forward and backward LSTM are then concatenated to produce a final single embedding for the input token. We vary recurrent dropouts (Gal and Ghahramani, 2016), input dropouts and output dropouts as shown in the Table 1, across three different models. The gate layer is fed with the output of this layer (Xw ). Convolution Network for Character Representation: We use a CNN-architecture to learn the character based representation of a word. The character embeddings of a token, denoted as Rd×l , where d is the dimension of a single character’s embedding and l is the max length"
W18-3220,W16-3927,0,0.026736,"of word representations and POS tag embeddings is used as input to a sigmoid dense layer. The value of the sigmoid output controls the relative contribution of the character and word representation in the final representation of the token. Following the work of Miyamoto and Cho (2016), the output of this layer g is used to take the weighted average of Bi-LSTM network output (Xg ) and the convolutional network output (Xch ): responding to each character. Each token is either truncated or post-padded to generate a token of 20 characters. Lexical Representation: We use the gazetteer provided by Mishra and Diesner (2016) and some Spanish gazetteers of our own to provide world knowledge to our model. Top 1000 celebrity Twitter handles from this list1 are also added. We represent gazetteer input for a token as a 19 dimensional vector, one binary value corresponding to each class. The binary bit represents the presence (1) or absence (0) of the token in the gazetteer (i.e. word list) of the respective class. g = σ(vgT Xg + bg ) X = (1 − g)Xch + gXg 2.2 Model Description where vg is the trainable weight vector, bg is the bias and σ(·) is the sigmoid function. The result of this layer X is then concatenated with t"
W18-3220,N13-1039,0,0.021261,"Missing"
W18-3220,D11-1141,0,0.229744,"Missing"
W18-3220,W16-3919,0,0.0319707,"Missing"
W18-3921,D14-1162,0,0.0844277,"in the sentences and are much better suited for our task. The overall description of our system is given in the following section. Input The input of the model is a sentence of words of an unknown language to be identified. Output The output will be a language class (the language name) corresponding to the given sentence. We will get a probability vector of shape(1, 5) that we pass through an argmax layer to extract the index of the most likely language label. Steps Involved 1. The first step is to convert an input sentence into a word vector representation. 2. We train 50-dimensional GloVe (Pennington et al., 2014) word embedding during the training phase of LSTMs. 3. Finally, we feed GloVe representation of each word in a given sentence of unknown language label into the LSTM hidden layers and predict the most appropriate language label for the sentence. 186 Mini-batching We are using Keras (Chollet and others, 2015) as the framework for implementing LSTM for our task. In our task, we will train LSTMs using mini-batches (Ioffe and Szegedy, 2015). The most basic requirement of a Deep Learning framework is that all sequences in the same mini-batch have the same length. If we had a five words sentence and"
W18-3921,W18-3901,0,0.0567236,"Missing"
W18-6116,D13-1084,0,0.061731,"Missing"
W18-6116,W14-5152,0,0.038234,"Missing"
W18-6116,W16-5803,0,0.0487313,"Missing"
W18-6116,P17-1180,0,0.0122455,"nsemble model for Hindi-English code-mixed LID. The first classifier used modified edit distance, word frequency and character n-grams as features. The second classifier used the output of the first classifier for the current word, along with language tag and POS tag of neighboring to give the final tag. Piergallini et al. 116 Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 116–120 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics 5 (2016) made a word level model taking char ngrams and capitalization as feature. Rijhwani et al. (2017) presented a generalized language tagger for arbitrary large set of languages which is fully unsupervised. Choudhury et al. (2017) used a model which concatenated word embeddings and character embeddings to predict the target language tag. Mandal et al. (2018a) used character embeddings along with phonetic embeddings to build an ensemble model for language tagging. 3 Inspired from the recent deep neural architectures developed for image classification tasks, especially the Inception architecture (Szegedy et al., 2015), we decided to use a very similar concept for learning language at word leve"
W18-6116,Y14-1041,0,0.0177059,"stly due to the problem of over-fitting. 2 Related Work In the recent past, a lot of work has been done in the field of code-mixing data, especially language tagging. King and Abney (2013) used weakly semi-supervised methods for building a world level language identifier. Linear chain CRFs with context information limited to bigrams was employed by Nguyen and Do˘gru¨oz (2013). Logistic regression along with a module which gives code-switching probability was used by Vyas et al. (2014). Multiple features like word context, dictionary, n-gram, edit distance were used by Das and Gamb¨ack (2014). Jhamtani et al. (2014) combined two classifiers into an ensemble model for Hindi-English code-mixed LID. The first classifier used modified edit distance, word frequency and character n-grams as features. The second classifier used the output of the first classifier for the current word, along with language tag and POS tag of neighboring to give the final tag. Piergallini et al. 116 Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 116–120 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics 5 (2016) made a word level model taking char ngr"
W18-6116,D14-1181,0,0.00992494,"Missing"
W18-6116,D14-1105,0,0.0181261,"ed data. Since most of the tasks require supervised models, the bottleneck of data crisis affects the performance quite a lot, mostly due to the problem of over-fitting. 2 Related Work In the recent past, a lot of work has been done in the field of code-mixing data, especially language tagging. King and Abney (2013) used weakly semi-supervised methods for building a world level language identifier. Linear chain CRFs with context information limited to bigrams was employed by Nguyen and Do˘gru¨oz (2013). Logistic regression along with a module which gives code-switching probability was used by Vyas et al. (2014). Multiple features like word context, dictionary, n-gram, edit distance were used by Das and Gamb¨ack (2014). Jhamtani et al. (2014) combined two classifiers into an ensemble model for Hindi-English code-mixed LID. The first classifier used modified edit distance, word frequency and character n-grams as features. The second classifier used the output of the first classifier for the current word, along with language tag and POS tag of neighboring to give the final tag. Piergallini et al. 116 Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 116–"
W18-6116,N13-1131,0,0.0144588,"a, context capture is extremely hard as well. Proper context capture can help in solving problems like ambiguity, that is word forms which are common to both the languages, but for which, the correct tag can be easily understood by knowing the context. An additional issue is a lack of available code-mixed data. Since most of the tasks require supervised models, the bottleneck of data crisis affects the performance quite a lot, mostly due to the problem of over-fitting. 2 Related Work In the recent past, a lot of work has been done in the field of code-mixing data, especially language tagging. King and Abney (2013) used weakly semi-supervised methods for building a world level language identifier. Linear chain CRFs with context information limited to bigrams was employed by Nguyen and Do˘gru¨oz (2013). Logistic regression along with a module which gives code-switching probability was used by Vyas et al. (2014). Multiple features like word context, dictionary, n-gram, edit distance were used by Das and Gamb¨ack (2014). Jhamtani et al. (2014) combined two classifiers into an ensemble model for Hindi-English code-mixed LID. The first classifier used modified edit distance, word frequency and character n-gr"
W18-6116,C18-1327,0,0.0447595,"Missing"
W18-6116,P18-4013,0,0.0451455,"Missing"
W18-6116,P16-1101,0,0.110006,"Missing"
W19-2006,N09-1003,0,0.0734228,"embeddings, and 43 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 43–51 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics sic from extrinsic evaluations is the lack of any trainable parameters in the former. Schnabel et al. (2015) discuss word relatedness, analogy, selective preference, and categorization as types of intrinsic tasks. Our proposed task is most similar to the word relatedness/similarity tasks, several of which have already been proposed in literature: WS-3533 (Finkelstein et al., 2002), WS-SIM and WSREL (Agirre et al., 2009), RG-65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991), MTurk-2875 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN7 (Bruni et al., 2012), YP-130 (Yang and Powers, 2006), Rare Words (Luong et al., 2013), etc. We list the ones above specifically since those are the ones we compare our proposed task to, using the online resource wordvectors.org (Faruqui and Dyer, 2014), whose code remains available on GitHub 1 . Association of Computational Linguistics2 and Vecto AI3 also maintain benchmark pages for word similarity. Likewise, VecEval (Nayak et al., 2016) and Mu"
W19-2006,W16-2506,0,0.0807232,"and Dyer, 2014), whose code remains available on GitHub 1 . Association of Computational Linguistics2 and Vecto AI3 also maintain benchmark pages for word similarity. Likewise, VecEval (Nayak et al., 2016) and Multilingual-embeddings-eval-portal (Ammar et al., 2016) are GitHub repositories for Extrinsic Evaluation of word embeddings. 4,5 . Another direction of work has been towards critiquing intrinsic evaluation, in a bid to understand its shortcomings and potential workarounds (Schnabel et al., 2015; Zhai et al., 2016). One of the key shortcomings is the Absence of Statistical Significance (Faruqui et al., 2016), which we aim to tackle through this proposal. We believe that a massive dataset of word associations can be used to circumvent issues related to confidence intervals of scores reported. We put this belief to test in later sections of this paper. a downstream task and measure changes in performance metrics specific to that task, Intrinsic evaluations directly test for syntactic or semantic relationships between words (Schnabel et al., 2015). For example, the word similarity task asks word embeddings to predict how similar are the meanings of two prompt words. The closer this estimate is to hu"
W19-2006,P14-1023,0,0.11374,"Missing"
W19-2006,Q17-1010,0,0.0706469,"cue tea, the most frequent response could be black. We need to bear this in mind when using this dataset to evaluate word embeddings intrinsically, since usually intrinsic datasets give out a single value for a word pair. This also does not fit well with the traditional measure of similarity/relatedness between two words, i.e., cosine distance, which is a symmetric metric. 4 of embeddings, including the best and most popular ones: 1. Word2Vec Skip Gram (Mikolov et al., 2013b,a) trained on Google News.8 2. GloVe (Pennington et al., 2014) trained on Wikipedia 2014 and Gigaword 5.9 3. FastText (Bojanowski et al., 2017) trained with subword information on Common Crawl (600B tokens).10 4. ConceptNet Numberbatch (Speer et al., 2017) trained on a big knowledge graph and some text corpora.11 5. Baroni and Lenci’s (2014) count-based embeddings, which are the result of dimensionality reduction on a large count matrix.12 6. Random Baseline: a baseline developed by randomly allotting 300 floating numbers to each word in the common vocabulary of the above five embeddings. We used intrinsic evaluations in the form of 13 word similarity tasks, provided by wordvectors.org (Faruqui and Dyer, 2014). For our proposed task"
W19-2006,P12-1015,0,0.0846907,"omputational Linguistics sic from extrinsic evaluations is the lack of any trainable parameters in the former. Schnabel et al. (2015) discuss word relatedness, analogy, selective preference, and categorization as types of intrinsic tasks. Our proposed task is most similar to the word relatedness/similarity tasks, several of which have already been proposed in literature: WS-3533 (Finkelstein et al., 2002), WS-SIM and WSREL (Agirre et al., 2009), RG-65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991), MTurk-2875 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN7 (Bruni et al., 2012), YP-130 (Yang and Powers, 2006), Rare Words (Luong et al., 2013), etc. We list the ones above specifically since those are the ones we compare our proposed task to, using the online resource wordvectors.org (Faruqui and Dyer, 2014), whose code remains available on GitHub 1 . Association of Computational Linguistics2 and Vecto AI3 also maintain benchmark pages for word similarity. Likewise, VecEval (Nayak et al., 2016) and Multilingual-embeddings-eval-portal (Ammar et al., 2016) are GitHub repositories for Extrinsic Evaluation of word embeddings. 4,5 . Another direction of work has been toward"
W19-2006,W10-0604,0,0.0312908,"Missing"
W19-2006,C16-1175,0,0.348365,"Missing"
W19-2006,W13-3512,0,0.0937152,"ck of any trainable parameters in the former. Schnabel et al. (2015) discuss word relatedness, analogy, selective preference, and categorization as types of intrinsic tasks. Our proposed task is most similar to the word relatedness/similarity tasks, several of which have already been proposed in literature: WS-3533 (Finkelstein et al., 2002), WS-SIM and WSREL (Agirre et al., 2009), RG-65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991), MTurk-2875 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN7 (Bruni et al., 2012), YP-130 (Yang and Powers, 2006), Rare Words (Luong et al., 2013), etc. We list the ones above specifically since those are the ones we compare our proposed task to, using the online resource wordvectors.org (Faruqui and Dyer, 2014), whose code remains available on GitHub 1 . Association of Computational Linguistics2 and Vecto AI3 also maintain benchmark pages for word similarity. Likewise, VecEval (Nayak et al., 2016) and Multilingual-embeddings-eval-portal (Ammar et al., 2016) are GitHub repositories for Extrinsic Evaluation of word embeddings. 4,5 . Another direction of work has been towards critiquing intrinsic evaluation, in a bid to understand its sho"
W19-2006,W16-2504,0,0.262454,"SREL (Agirre et al., 2009), RG-65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991), MTurk-2875 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN7 (Bruni et al., 2012), YP-130 (Yang and Powers, 2006), Rare Words (Luong et al., 2013), etc. We list the ones above specifically since those are the ones we compare our proposed task to, using the online resource wordvectors.org (Faruqui and Dyer, 2014), whose code remains available on GitHub 1 . Association of Computational Linguistics2 and Vecto AI3 also maintain benchmark pages for word similarity. Likewise, VecEval (Nayak et al., 2016) and Multilingual-embeddings-eval-portal (Ammar et al., 2016) are GitHub repositories for Extrinsic Evaluation of word embeddings. 4,5 . Another direction of work has been towards critiquing intrinsic evaluation, in a bid to understand its shortcomings and potential workarounds (Schnabel et al., 2015; Zhai et al., 2016). One of the key shortcomings is the Absence of Statistical Significance (Faruqui et al., 2016), which we aim to tackle through this proposal. We believe that a massive dataset of word associations can be used to circumvent issues related to confidence intervals of scores report"
W19-2006,D14-1162,0,0.103723,", word embeddings have become the basic building block of several state-of-the-art models spanning multiple problems across Natural Language Processing and Information Retrieval. Word embeddings are essentially non-sparse representations of words in the form of one (relatively) small dimensional vector of real numbers for every word, and all of these vectors lie in the same continuous space. Despite the clear benefits of these distributed representations, it is not obvious how to come up with apt word embeddings for a given NLP task. Approaches such as word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014), etc. have been shown to perform well on downstream tasks such as text classification, sequence labelling, question answering, text summarization, and machine translation. Typically, word vectors are used in NLP models in two ways: fixed pretrained embeddings, and 43 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 43–51 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics sic from extrinsic evaluations is the lack of any trainable parameters in the former. Schnabel et al. (2015) discuss word relatedness, analogy, selective"
W19-2006,2018.jeptalnrecital-court.37,0,0.0612866,"Missing"
W19-2006,D15-1036,0,0.21099,"as word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014), etc. have been shown to perform well on downstream tasks such as text classification, sequence labelling, question answering, text summarization, and machine translation. Typically, word vectors are used in NLP models in two ways: fixed pretrained embeddings, and 43 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 43–51 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics sic from extrinsic evaluations is the lack of any trainable parameters in the former. Schnabel et al. (2015) discuss word relatedness, analogy, selective preference, and categorization as types of intrinsic tasks. Our proposed task is most similar to the word relatedness/similarity tasks, several of which have already been proposed in literature: WS-3533 (Finkelstein et al., 2002), WS-SIM and WSREL (Agirre et al., 2009), RG-65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991), MTurk-2875 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN7 (Bruni et al., 2012), YP-130 (Yang and Powers, 2006), Rare Words (Luong et al., 2013), etc. We list the ones above specifically since"
