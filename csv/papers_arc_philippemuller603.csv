2021.jeptalnrecital-taln.6,Plongements Interpr{\\'e}tables pour la D{\\'e}tection de Biais Cach{\\'e}s (Interpretable Embeddings for Hidden Biases Detection),2021,-1,-1,2,0,5595,tom bourgeade,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"De nombreuses t{\^a}ches s{\'e}mantiques en TAL font usage de donn{\'e}es collect{\'e}es de mani{\`e}re semiautomatique, ce qui est souvent source d{'}artefacts ind{\'e}sirables qui peuvent affecter n{\'e}gativement les mod{\`e}les entra{\^\i}n{\'e}s sur celles-ci. Avec l{'}{\'e}volution plus r{\'e}cente vers des mod{\`e}les {\`a} usage g{\'e}n{\'e}rique pr{\'e}-entra{\^\i}n{\'e}s plus complexes, et moins interpr{\'e}tables, ces biais peuvent conduire {\`a} l{'}int{\'e}gration de corr{\'e}lations ind{\'e}sirables dans des applications utilisateurs. R{\'e}cemment, quelques m{\'e}thodes ont {\'e}t{\'e} propos{\'e}es pour entra{\^\i}ner des plongements de mots avec une meilleure interpr{\'e}tabilit{\'e}. Nous proposons une m{\'e}thode simple qui exploite ces repr{\'e}sentations pour d{\'e}tecter de mani{\`e}re pr{\'e}ventive des corr{\'e}lations lexicales faciles {\`a} apprendre, dans divers jeux de donn{\'e}es. Nous {\'e}valuons {\`a} cette fin quelques mod{\`e}les de plongements interpr{\'e}tables populaires pour l{'}anglais, en utilisant {\`a} la fois une {\'e}valuation intrins{\`e}que, et un ensemble de t{\^a}ches s{\'e}mantiques en aval, et nous utilisons la qualit{\'e} interpr{\'e}table des plongements afin de diagnostiquer des biais potentiels dans les jeux de donn{\'e}es associ{\'e}s."
2021.emnlp-main.104,Weakly supervised discourse segmentation for multiparty oral conversations,2021,-1,-1,3,0,8847,lila gravellier,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Discourse segmentation, the first step of discourse analysis, has been shown to improve results for text summarization, translation and other NLP tasks. While segmentation models for written text tend to perform well, they are not directly applicable to spontaneous, oral conversation, which has linguistic features foreign to written text. Segmentation is less studied for this type of language, where annotated data is scarce, and existing corpora more heterogeneous. We develop a weak supervision approach to adapt, using minimal annotation, a state of the art discourse segmenter trained on written text to French conversation transcripts. Supervision is given by a latent model bootstrapped by manually defined heuristic rules that use linguistic and acoustic information. The resulting model improves the original segmenter, especially in contexts where information on speaker turns is lacking or noisy, gaining up to 13{\%} in F-score. Evaluation is performed on data like those used to define our heuristic rules, but also on transcripts from two other corpora."
2021.disrpt-1.1,"The {DISRPT} 2021 Shared Task on Elementary Discourse Unit Segmentation, Connective Detection, and Relation Classification",2021,-1,-1,4,0,795,amir zeldes,Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021),0,"In 2021, we organized the second iteration of a shared task dedicated to the underlying units used in discourse parsing across formalisms: the DISRPT Shared Task (Discourse Relation Parsing and Treebanking). Adding to the 2019 tasks on Elementary Discourse Unit Segmentation and Connective Detection, this iteration of the Shared Task included for the first time a track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data."
2021.disrpt-1.3,Multi-lingual Discourse Segmentation and Connective Identification: {MELODI} at Disrpt2021,2021,-1,-1,2,0,11202,morteza ezzabady,Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021),0,"We present an approach for discourse segmentation and discourse connective identification, both at the sentence and document level, within the Disrpt 2021 shared task, a multi-lingual and multi-formalism evaluation campaign. Building on the most successful architecture from the 2019 similar shared task, we leverage datasets in the same or similar languages to augment training data and improve on the best systems from the previous campaign on 3 out of 4 subtasks, with a mean improvement on all 16 datasets of 0.85{\%}. Within the Disrpt 21 campaign the system ranks 3rd overall, very close to the 2nd system, but with a significant gap with respect to the best system, which uses a rich set of additional features. The system is nonetheless the best on languages that benefited from crosslingual training on sentence internal segmentation (German and Spanish)."
2020.lrec-1.125,{D}isc{S}ense: Automated Semantic Analysis of Discourse Markers,2020,-1,-1,4,1,2009,damien sileo,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Using a model trained to predict discourse markers between sentence pairs, we predict plausible markers between sentence pairs with a known semantic relation (provided by existing classification datasets). These predictions allow us to study the link between discourse markers and the semantic relations annotated in classification datasets. Handcrafted mappings have been proposed between markers and discourse relations on a limited set of markers and a limited set of categories, but there exists hundreds of discourse markers expressing a wide variety of relations, and there is no consensus on the taxonomy of relations between competing discourse theories (which are largely built in a top-down fashion). By using an automatic prediction method over existing semantically annotated datasets, we provide a bottom-up characterization of discourse markers in English. The resulting dataset, named DiscSense, is publicly available."
W19-5950,Which aspects of discourse relations are hard to learn? Primitive decomposition for discourse relation classification,2019,0,0,3,0,23786,charlotte roze,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"Discourse relation classification has proven to be a hard task, with rather low performance on several corpora that notably differ on the relation set they use. We propose to decompose the task into smaller, mostly binary tasks corresponding to various primitive concepts encoded into the discourse relation definitions. More precisely, we translate the discourse relations into a set of values for attributes based on distinctions used in the mappings between discourse frameworks proposed by Sanders et al. (2018). This arguably allows for a more robust representation of discourse relations, and enables us to address usually ignored aspects of discourse relation prediction, namely multiple labels and underspecified annotations. We show experimentally which of the conceptual primitives are harder to learn from the Penn Discourse Treebank English corpus, and propose a correspondence to predict the original labels, with preliminary empirical comparisons with a direct model."
W19-2715,{T}o{N}y: Contextual embeddings for accurate multilingual discourse segmentation of full documents,2019,0,1,1,1,5596,philippe muller,Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019,0,"Segmentation is the first step in building practical discourse parsers, and is often neglected in discourse parsing studies. The goal is to identify the minimal spans of text to be linked by discourse relations, or to isolate explicit marking of discourse relations. Existing systems on English report F1 scores as high as 95{\%}, but they generally assume gold sentence boundaries and are restricted to English newswire texts annotated within the RST framework. This article presents a generic approach and a system, ToNy, a discourse segmenter developed for the DisRPT shared task where multiple discourse representation schemes, languages and domains are represented. In our experiments, we found that a straightforward sequence prediction architecture with pretrained contextual embeddings is sufficient to reach performance levels comparable to existing systems, when separately trained on each corpus. We report performance between 81{\%} and 96{\%} in F1 score. We also observed that discourse segmentation models only display a moderate generalization capability, even within the same language and discourse representation scheme."
S19-1004,Composition of Sentence Embeddings: Lessons from Statistical Relational Learning,2019,35,0,4,1,2009,damien sileo,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Various NLP problems {--} such as the prediction of sentence similarity, entailment, and discourse relations {--} are all instances of the same general task: the modeling of semantic relations between a pair of textual elements. A popular model for such problems is to embed sentences into fixed size vectors, and use composition functions (e.g. concatenation or sum) of those vectors as features for the prediction. At the same time, composition of embeddings has been a main focus within the field of Statistical Relational Learning (SRL) whose goal is to predict relations between entities (typically from knowledge base triples). In this article, we show that previous work on relation prediction between texts implicitly uses compositions from baseline SRL models. We show that such compositions are not expressive enough for several tasks (e.g. natural language inference). We build on recent SRL models to address textual relational problems, showing that they are more expressive, and can alleviate issues from simpler compositions. The resulting models significantly improve the state of the art in both transferable sentence representation learning and relation prediction."
N19-1351,Mining Discourse Markers for Unsupervised Sentence Representation Learning,2019,0,1,4,1,2009,damien sileo,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Current state of the art systems in NLP heavily rely on manually annotated datasets, which are expensive to construct. Very little work adequately exploits unannotated data {--} such as discourse markers between sentences {--} mainly because of data sparseness and ineffective extraction methods. In the present work, we propose a method to automatically discover sentence pairs with relevant discourse markers, and apply it to massive amounts of data. Our resulting dataset contains 174 discourse markers with at least 10k examples each, even for rare markers such as {``}coincidentally{''} or {``}amazingly{''}. We use the resulting data as supervision for learning transferable sentence embeddings. In addition, we show that even though sentence representation learning through prediction of discourse marker yields state of the art results across different transfer tasks, it{'}s not clear that our models made use of the semantic relation between sentences, thus leaving room for further improvements."
2019.jeptalnrecital-deft.8,Aprentissage non-supervis{\\'e} pour l{'}appariement et l{'}{\\'e}tiquetage de cas cliniques en fran{\\c{c}}ais - {DEFT}2019 (Unsupervised learning for matching and labelling of french clincal cases - {DEFT}2019 ),2019,-1,-1,3,1,2009,damien sileo,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. D{\\'e}fi Fouille de Textes (atelier TALN-RECITAL),0,"Nous pr{\'e}sentons le syst{\`e}me utilis{\'e} par l{'}{\'e}quipe Synapse/IRIT dans la comp{\'e}tition DEFT2019 portant sur deux t{\^a}ches li{\'e}es {\`a} des cas cliniques r{\'e}dig{\'e}s en fran{\c{c}}ais : l{'}une d{'}appariement entre des cas cliniques et des discussions, l{'}autre d{'}extraction de mots-clefs. Une des particularit{\'e} est l{'}emploi d{'}apprentissage non-supervis{\'e} sur les deux t{\^a}ches, sur un corpus construit sp{\'e}cifiquement pour le domaine m{\'e}dical en fran{\c{c}}ais"
2019.jeptalnrecital-court.1,Analyse faiblement supervis{\\'e}e de conversation en actes de dialogue (Weakly supervised dialog act analysis),2019,-1,-1,3,0,27340,catherine thompson,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Nous nous int{\'e}ressons ici {\`a} l{'}analyse de conversation par chat dans un contexte orient{\'e}-t{\^a}che avec un conseiller technique s{'}adressant {\`a} un client, o{\`u} l{'}objectif est d{'}{\'e}tiqueter les {\'e}nonc{\'e}s en actes de dialogue, pour alimenter des analyses des conversations en aval. Nous proposons une m{\'e}thode l{\'e}g{\`e}rement supervis{\'e}e {\`a} partir d{'}heuristiques simples, de quelques annotations de d{\'e}veloppement, et une m{\'e}thode d{'}ensemble sur ces r{\`e}gles qui sert {\`a} annoter automatiquement un corpus plus large de fa{\c{c}}on bruit{\'e}e qui peut servir d{'}entrainement {\`a} un mod{\`e}le supervis{\'e}. Nous comparons cette approche {\`a} une approche supervis{\'e}e classique et montrons qu{'}elle atteint des r{\'e}sultats tr{\`e}s proches, {\`a} un co{\^u}t moindre et tout en {\'e}tant plus facile {\`a} adapter {\`a} de nouvelles donn{\'e}es."
2019.jeptalnrecital-court.26,Repr{\\'e}sentation s{\\'e}mantique distributionnelle et alignement de conversations par chat (Distributional semantic representation and alignment of online chat conversations ),2019,-1,-1,2,0,5595,tom bourgeade,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Les mesures de similarit{\'e} textuelle ont une place importante en TAL, du fait de leurs nombreuses applications, en recherche d{'}information et en classification notamment. En revanche, le dialogue fait moins l{'}objet d{'}attention sur cette question. Nous nous int{\'e}ressons ici {\`a} la production d{'}une similarit{\'e} dans le contexte d{'}un corpus de conversations par chat {\`a} l{'}aide de m{\'e}thodes non-supervis{\'e}es, exploitant {\`a} diff{\'e}rents niveaux la notion de s{\'e}mantique distributionnelle, sous forme d{'}embeddings. Dans un m{\^e}me temps, pour enrichir la mesure, et permettre une meilleure interpr{\'e}tation des r{\'e}sultats, nous {\'e}tablissons des alignements explicites des tours de parole dans les conversations, en exploitant la distance de Wasserstein, qui permet de prendre en compte leur dimension structurelle. Enfin, nous {\'e}valuons notre approche {\`a} l{'}aide d{'}une t{\^a}che externe sur la petite partie annot{\'e}e du corpus, et observons qu{'}elle donne de meilleurs r{\'e}sultats qu{'}une variante plus na{\""\i}ve {\`a} base de moyennes."
J18-2001,A Dependency Perspective on {RST} Discourse Parsing and Evaluation,2018,28,9,2,1,24638,mathieu morey,Computational Linguistics,0,"Computational text-level discourse analysis mostly happens within Rhetorical Structure Theory (RST), whose structures have classically been presented as constituency trees, and relies on data from the RST Discourse Treebank (RST-DT); as a result, the RST discourse parsing community has largely borrowed from the syntactic constituency parsing community. The standard evaluation procedure for RST discourse parsers is thus a simplified variant of PARSEVAL, and most RST discourse parsers use techniques that originated in syntactic constituency parsing. In this article, we isolate a number of conceptual and computational problems with the constituency hypothesis. We then examine the consequences, for the implementation and evaluation of RST discourse parsers, of adopting a dependency perspective on RST structures, a view advocated so far only by a few approaches to discourse parsing. While doing that, we show the importance of the notion of headedness of RST structures. We analyze RST discourse parsing as dependency parsing by adapting to RST a recent proposal in syntactic parsing that relies on head-ordered dependency trees, a representation isomorphic to headed constituency trees. We show how to convert the original trees from the RST corpus, RST-DT, and their binarized versions used by all existing RST parsers to head-ordered dependency trees. We also propose a way to convert existing simple dependency parser output to constituent trees. This allows us to evaluate and to compare approaches from both constituent-based and dependency-based perspectives in a unified framework, using constituency and dependency metrics. We thus propose an evaluation framework to compare extant approaches easily and uniformly, something the RST parsing community has lacked up to now. We can also compare parsers{'} predictions to each other across frameworks. This allows us to characterize families of parsing strategies across the different frameworks, in particular with respect to the notion of headedness. Our experiments provide evidence for the conceptual similarities between dependency parsers and shift-reduce constituency parsers, and confirm that dependency parsing constitutes a viable approach to RST discourse parsing."
2018.jeptalnrecital-deft.5,"Concat{\\'e}nation de r{\\'e}seaux de neurones pour la classification de tweets, {DEFT}2018 (Concatenation of neural networks for tweets classification, {DEFT}2018 )",2018,-1,-1,3,1,2009,damien sileo,"Actes de la Conf{\\'e}rence TALN. Volume 2 - D{\\'e}monstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT",0,Nous pr{\'e}sentons le syst{\`e}me utilis{\'e} par l{'}{\'e}quipe Melodi/Synapse D{\'e}veloppement dans la comp{\'e}tition DEFT2018 portant sur la classification de th{\'e}matique ou de sentiments de tweets en fran{\c{c}}ais. On propose un syst{\`e}me unique pour les deux approches qui combine concat{\'e}nativement deux m{\'e}thodes d{'}embedding et trois mod{\`e}les de repr{\'e}sentation s{\'e}quence. Le syst{\`e}me se classe 1/13 en analyse de sentiments et 4/13 en classification th{\'e}matique.
D17-1136,How much progress have we made on {RST} discourse parsing? A replication study of recent results on the {RST}-{DT},2017,0,13,2,1,24638,mathieu morey,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"This article evaluates purported progress over the past years in RST discourse parsing. Several studies report a relative error reduction of 24 to 51{\%} on all metrics that authors attribute to the introduction of distributed representations of discourse units. We replicate the standard evaluation of 9 parsers, 5 of which use distributed representations, from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT. Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure. We evaluate all these parsers with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings. Under this more stringent procedure, the gains attributable to distributed representations represent at most a 16{\%} relative error reduction on fully-labelled structures."
2017.jeptalnrecital-court.13,Changement stylistique de phrases par apprentissage faiblement supervis{\\'e} (Textual Style Transfer using Weakly Supervised Learning),2017,-1,-1,3,1,2009,damien sileo,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Plusieurs t{\^a}ches en traitement du langage naturel impliquent de modifier des phrases en conservant au mieux leur sens, comme la reformulation, la compression, la simplification, chacune avec leurs propres donn{\'e}es et mod{\`e}les. Nous introduisons ici une m{\'e}thode g{\'e}n{\'e}rale s{'}adressant {\`a} tous ces probl{\`e}mes, utilisant des donn{\'e}es plus simples {\`a} obtenir : un ensemble de phrases munies d{'}indicateurs sur leur style, comme des phrases et le type de sentiment qu{'}elles expriment. Cette m{\'e}thode repose sur un mod{\`e}le d{'}apprentissage de repr{\'e}sentations non supervis{\'e} (un auto-encodeur variationnel), puis sur le changement des repr{\'e}sentations apprises pour correspondre {\`a} un style donn{\'e}. Le r{\'e}sultat est {\'e}valu{\'e} qualitativement, puis quantitativement sur le jeu de donn{\'e}es de compression de phrases Microsoft, avec des r{\'e}sultats encourageants."
L16-1601,Corpus Annotation within the {F}rench {F}rame{N}et: a Domain-by-domain Methodology,2016,13,2,3,0,35314,marianne djemaa,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper reports on the development of a French FrameNet, within the ASFALDA project. While the first phase of the project focused on the development of a French set of frames and corresponding lexicon (Candito et al., 2014), this paper concentrates on the subsequent corpus annotation phase, which focused on four notional domains (commercial transactions, cognitive stances, causality and verbal communication). Given full coverage is not reachable for a relatively {``}new{''} FrameNet project, we advocate that focusing on specific notional domains allowed us to obtain full lexical coverage for the frames of these domains, while partially reflecting word sense ambiguities. Furthermore, as frames and roles were annotated on two French Treebanks (the French Treebank (Abeill{\'e} and Barrier, 2004) and the Sequoia Treebank (Candito and Seddah, 2012), we were able to extract a syntactico-semantic lexicon from the annotated frames. In the resource{'}s current status, there are 98 frames, 662 frame evoking words, 872 senses, and about 13000 annotated frames, with their semantic roles assigned to portions of text. The French FrameNet is freely available at alpage.inria.fr/asfalda."
L16-1603,A General Framework for the Annotation of Causality Based on {F}rame{N}et,2016,31,4,2,0,35315,laure vieu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present here a general set of semantic frames to annotate causal expressions, with a rich lexicon in French and an annotated corpus of about 5000 instances of causal lexical items with their corresponding semantic frames. The aim of our project is to have both the largest possible coverage of causal phenomena in French, across all parts of speech, and have it linked to a general semantic framework such as FN, to benefit in particular from the relations between other semantic frames, e.g., temporal ones or intentional ones, and the underlying upper lexical ontology that enable some forms of reasoning. This is part of the larger ASFALDA French FrameNet project, which focuses on a few different notional domains which are interesting in their own right (Djemma et al., 2016), including cognitive positions and communication frames. In the process of building the French lexicon and preparing the annotation of the corpus, we had to remodel some of the frames proposed in FN based on English data, with hopefully more precise frame definitions to facilitate human annotation. This includes semantic clarifications of frames and frame elements, redundancy elimination, and added coverage. The result is arguably a significant improvement of the treatment of causality in FN itself."
C16-1334,A Supervised Approach for Enriching the Relational Structure of Frame Semantics in {F}rame{N}et,2016,23,1,2,0,16836,shafqat virk,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Frame semantics is a theory of linguistic meanings, and is considered to be a useful framework for shallow semantic analysis of natural language. FrameNet, which is based on frame semantics, is a popular lexical semantic resource. In addition to providing a set of core semantic frames and their frame elements, FrameNet also provides relations between those frames (hence providing a network of frames i.e. FrameNet). We address here the limited coverage of the network of conceptual relations between frames in FrameNet, which has previously been pointed out by others. We present a supervised model using rich features from three different sources: structural features from the existing FrameNet network, information from the WordNet relations between synsets projected into semantic frames, and corpus-collected lexical associations. We show large improvements over baselines consisting of each of the three groups of features in isolation. We then use this model to select frame pairs as candidate relations, and perform evaluation on a sample with good precision."
W14-6601,Presentation of the {S}em{D}is 2014 workshop: distributional semantics for two tasks - lexical substitution and exploration of specialized corpora (Pr{\\'e}sentation de l{'}atelier {S}em{D}is 2014 : s{\\'e}mantique distributionnelle pour la substitution lexicale et l{'}exploration de corpus sp{\\'e}cialis{\\'e}s) [in {F}rench],2014,0,1,5,0,11476,cecile fabre,{TALN}-{RECITAL} 2014 Workshop {S}em{D}is 2014 : Enjeux actuels de la s{\\'e}mantique distributionnelle ({S}em{D}is 2014: Current Challenges in Distributional Semantics),0,None
P14-1045,Predicting the relevance of distributional semantic similarity with contextual information,2014,24,2,1,1,5596,philippe muller,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP. The resulting relations are often noisy or difficult to interpret in general. This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains, but instead of considering it in abstracto, we focus on pairs of words in context. In a discourse, we are interested in knowing if the semantic link between two items is a byproduct of textual coherence or is irrelevant. We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity, and to assess the prevalence of actual semantic relations between word tokens. We then built an experiment to automatically predict this relevance, evaluated on the reliable reference data set which was the outcome of the first annotation. We show that in-document information greatly improve the prediction made by the similarity level alone."
candito-etal-2014-developing,Developing a {F}rench {F}rame{N}et: Methodology and First results,2014,23,6,10,0.314676,16504,marie candito,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The Asfalda project aims to develop a French corpus with frame-based semantic annotations and automatic tools for shallow semantic analysis. We present the first part of the project: focusing on a set of notional domains, we delimited a subset of English frames, adapted them to French data when necessary, and developed the corresponding French lexicon. We believe that working domain by domain helped us to enforce the coherence of the resulting resource, and also has the advantage that, though the number of frames is limited (around a hundred), we obtain full coverage within a given domain."
F14-1022,Unsupervised extraction of semantic relations (Extraction non supervis{\\'e}e de relations s{\\'e}mantiques lexicales) [in {F}rench],2014,0,0,4,0,35871,juliette conrath,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
C14-1206,Unsupervised extraction of semantic relations using discourse cues,2014,31,7,4,0,35871,juliette conrath,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper presents a knowledge base containing triples involving pairs of verbs associated with semantic or discourse relations. The relations in these triples are marked by discourse connectors between two adjacent instances of the verbs in the triple in the large French corpus, frWaC. We detail several measures that evaluate the relevance of the triples and the strength of their association. We use manual annotations to evaluate our method, and also study the coverage of our resource with respect to the discourse annotated corpus Annodis. Our positive results show the potential impact of our resource for discourse analysis tasks as well as other semantically oriented tasks like temporal and causal information extraction"
W13-4002,Expressivity and comparison of models of discourse structure,2013,25,9,3,0,5809,antoine venant,Proceedings of the {SIGDIAL} 2013 Conference,0,"Several discourse annotated corpora now exist for NLP exploitation. Nevertheless, it is not clear how these annotations compare: are they incompatible, incomparable, or do they share some inter- pretations? In this paper, we relate three types of discourse annotation as found in: (i) the RST Tree Bank corpus, (ii) SDRT corpora DISCOR and ANNODIS, and (iii) dependency tree structures. The latter have not yet been used in actual annotations, but represent elementary substructures which are interesting for automated parsing. Specifically, we discuss two ways of interpreting RST trees by taking discourse relations as semantics operators, one is fully specified, the other one underspecified. We also provide an underspecified semantic interpretation of dependency trees. We define trans- lations between RST and DT that preserve these underspecified interpretations. On this basis, we design similarity measures that quantify the loss of information implied by these translations. Over- all, these translations and metrics provide a unified framework that will hopefully enable us to take advantage of the various existing discourse annotation data that are available for automated tasks."
S13-2017,{MELODI}: Semantic Similarity of Words and Compositional Phrases using Latent Vector Weighting,2013,16,2,3,0,5597,tim cruys,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"In this paper we present our system for the SemEval 2013 Task 5a on semantic similar- ity of words and compositional phrases. Our system uses a dependency-based vector space model, in combination with a technique called latent vector weighting. The system computes the similarity between a particular noun in- stance and the head noun of a particular noun phrase, which was weighted according to the semantics of the modifier. The system is en- tirely unsupervised; one single parameter, the similarity threshold, was tuned using the train- ing data."
S13-2026,{MELODI}: A Supervised Distributional Approach for Free Paraphrasing of Noun Compounds,2013,12,5,3,0,5597,tim cruys,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"This paper describes the system submitted by the MELODI team for the SemEval-2013 Task 4 : Free Paraphrases of Noun Compounds (Hendrickx et al., 2013). Our approach combines the strength of an unsupervised distributional word space model with a supervised maximum-entropy classification model; the distributional model yields a feature representation for a particular compound noun, which is subsequently used by the classifier to induce a number of appropriate paraphrases."
afantenos-etal-2012-empirical,An empirical resource for discovering cognitive principles of discourse organisation: the {ANNODIS} corpus,2012,35,36,8,1,17571,stergos afantenos,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the ANNODIS resource, a discourse-level annotated corpus for French. The corpus combines two perspectives on discourse: a bottom-up approach and a top-down approach. The bottom-up view incrementally builds a structure from elementary discourse units, while the top-down view focuses on the selective annotation of multi-level discourse structures. The corpus is composed of texts that are diversified with respect to genre, length and type of discursive organisation. The methodology followed here involves an iterative design of annotation guidelines in order to reach satisfactory inter-annotator agreement levels. This allows us to raise a few issues relevant for the comparison of such complex objects as discourse structures. The corpus also serves as a source of empirical evidence for discourse theories. We present here two first analyses taking advantage of this new annotated corpus --one that tested hypotheses on constraints governing discourse structure, and another that studied the variations in composition and signalling of multi-level discourse structures."
C12-1115,Constrained Decoding for Text-Level Discourse Parsing,2012,32,30,1,1,5596,philippe muller,Proceedings of {COLING} 2012,0,"This paper presents a novel approach to document-based discourse analysis by performing a global A* search over the space of possible structures while optimizing a global criterion over the set of potential coherence relations. Existing approaches to discourse analysis have so far relied on greedy search strategies or restricted themselves to sentence-level discourse parsing. Another advantage of our approach, over other global alternatives (like Maximum Spanning Tree decoding algorithms), is its flexibility in being able to integrate constraints (including linguistically motivated ones like the Right Frontier Constraint). Finally, our paper provides the first discourse parsing system for French; our evaluation is carried out on the Annodis corpus. While using a lot less training data than earlier approaches than previous work on English, our system manages to achieve state-of-the-art results, with F1-scores of 66.2 and 46.8 when compared to unlabeled and labeled reference structures."
2011.jeptalnrecital-long.15,Comparaison d{'}une approche miroir et d{'}une approche distributionnelle pour l{'}extraction de mots s{\\'e}mantiquement reli{\\'e}s (Comparing a mirror approach and a distributional approach for extracting semantically related words),2011,-1,-1,1,1,5596,philippe muller,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans (Muller {\&} Langlais, 2010), nous avons compar{\'e} une approche distributionnelle et une variante de l{'}approche miroir propos{\'e}e par Dyvik (2002) sur une t{\^a}che d{'}extraction de synonymes {\`a} partir d{'}un corpus en fran{\c{c}}ais. Nous pr{\'e}sentons ici une analyse plus fine des relations extraites automatiquement en nous int{\'e}ressant cette fois-ci {\`a} la langue anglaise pour laquelle de plus amples ressources sont disponibles. Diff{\'e}rentes fa{\c{c}}ons d{'}{\'e}valuer notre approche corroborent le fait que l{'}approche miroir se comporte globalement mieux que l{'}approche distributionnelle d{\'e}crite dans (Lin, 1998), une approche de r{\'e}f{\'e}rence dans le domaine."
afantenos-etal-2010-learning,Learning Recursive Segments for Discourse Parsing,2010,4,12,3,1,17571,stergos afantenos,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Automatically detecting discourse segments is an important preliminary step towards full discourse parsing. Previous research on discourse segmentation have relied on the assumption that elementary discourse units (EDUs) in a document always form a linear sequence (i.e., they can never be nested). Unfortunately, this assumption turns out to be too strong, for some theories of discourse, like the ''``Segmented Discourse Representation Theory'''' or SDRT, allow for nested discourse units. In this paper, we present a simple approach to discourse segmentation that is able to produce nested EDUs. Our approach builds on standard multi-class classification techniques making use of a regularized maximum entropy model, combined with a simple repairing heuristic that enforces global coherence. Our system was developed and evaluated on the first round of annotations provided by the French Annodis project (an ongoing effort to create a discourse bank for French). Cross-validated on only 47 documents (1,445 EDUs), our system achieves encouraging performance results with an F-score of 73{\%} for finding EDUs."
C10-1029,Comparison of different algebras for inducing the temporal structure of texts,2010,19,6,2,0,11461,pascal denis,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper investigates the impact of using different temporal algebras for learning temporal relations between events. Specifically, we compare three interval-based algebras: Allen (1983) algebra, Bruce (1972) algebra, and the algebra derived from the TempEval-07 campaign. These algebras encode different granularities of relations and have different inferential properties. They in turn behave differently when used to enforce global consistency constraints on the building of a temporal representation. Through various experiments on the TimeBank/AQUAINT corpus, we show that although the TempEval relation set leads to the best classification accuracy performance, it is too vague to be used for enforcing consistency. By contrast, the other two relation sets are similarly harder to learn, but more useful when global consistency is important. Overall, the Bruce algebra is shown to give the best compromise between learnability and expressive power."
2010.jeptalnrecital-long.23,Une {\\'e}valuation de l{'}impact des types de textes sur la t{\\^a}che de segmentation th{\\'e}matique,2010,-1,-1,2,0,39176,clementine adam,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cette {\'e}tude a pour but de contribuer {\`a} la d{\'e}finition des objectifs de la segmentation th{\'e}matique (ST), en incitant {\`a} prendre en consid{\'e}ration le param{\`e}tre du type de textes dans cette t{\^a}che. Notre hypoth{\`e}se est que, si la ST est certes pertinente pour traiter certains textes dont l{'}organisation est bien th{\'e}matique, elle n{'}est pas adapt{\'e}e {\`a} la prise en compte d{'}autres modes d{'}organisation (temporelle, rh{\'e}torique), et ne peut pas {\^e}tre appliqu{\'e}e sans pr{\'e}caution {\`a} des textes tout-venants. En comparant les performances d{'}un syst{\`e}me de ST sur deux corpus, {\`a} organisation th{\'e}matique {``}forte{''} et {``}faible{''}, nous montrons que cette t{\^a}che est effectivement sensible {\`a} la nature des textes."
2010.jeptalnrecital-court.21,Comparaison de ressources lexicales pour l{'}extraction de synonymes,2010,-1,-1,1,1,5596,philippe muller,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,
2009.jeptalnrecital-court.5,{ANNODIS}: une approche outill{\\'e}e de l{'}annotation de structures discursives,2009,-1,-1,11,0,43228,mariepaule perywoodley,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Le projet ANNODIS vise la construction d{'}un corpus de textes annot{\'e}s au niveau discursif ainsi que le d{\'e}veloppement d{'}outils pour l{'}annotation et l{'}exploitation de corpus. Les annotations adoptent deux points de vue compl{\'e}mentaires : une perspective ascendante part d{'}unit{\'e}s de discours minimales pour construire des structures complexes via un jeu de relations de discours ; une perspective descendante aborde le texte dans son entier et se base sur des indices pr{\'e}-identifi{\'e}s pour d{\'e}tecter des structures discursives de haut niveau. La construction du corpus est associ{\'e}e {\`a} la cr{\'e}ation de deux interfaces : la premi{\`e}re assiste l{'}annotation manuelle des relations et structures discursives en permettant une visualisation du marquage issu des pr{\'e}traitements ; une seconde sera destin{\'e}e {\`a} l{'}exploitation des annotations. Nous pr{\'e}sentons les mod{\`e}les et protocoles d{'}annotation {\'e}labor{\'e}s pour mettre en oeuvre, au travers de l{'}interface d{\'e}di{\'e}e, la campagne d{'}annotation."
tannier-muller-2008-evaluation,Evaluation Metrics for Automatic Temporal Annotation of Texts,2008,9,10,2,0,5686,xavier tannier,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Recent years have seen increasing attention in temporal processing of texts as well as a lot of standardization effort of temporal information in natural language. A central part of this information lies in the temporal relations between events described in a text, when their precise times or dates are not known. Reliable human annotation of such information is difficult, and automatic comparisons must follow procedures beyond mere precision-recall of local pieces of information, since a coherent picture can only be considered at a global level. We address the problem of evaluation metrics of such information, aiming at fair comparisons between systems, by proposing some measures taking into account the globality of a text."
2008.jeptalnrecital-long.4,Annotation d{'}expressions temporelles et d{'}{\\'e}v{\\'e}nements en fran{\\c{c}}ais,2008,-1,-1,3,0,44225,gabriel parent,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous proposons une m{\'e}thode pour identifier, dans un texte en fran{\c{c}}ais, l{'}ensemble des expressions adverbiales de localisation temporelle, ainsi que tous les verbes, noms et adjectifs d{\'e}notant une {\'e}ventualit{\'e} ({\'e}v{\'e}nement ou {\'e}tat). Cette m{\'e}thode, en plus d{'}identifier ces expressions, extrait certaines informations s{\'e}mantiques : la valeur de la localisation temporelle selon la norme TimeML et le type des {\'e}ventualit{\'e}s. Pour les expressions adverbiales de localisation temporelle, nous utilisons une cascade d{'}automates, alors que pour l{'}identification des {\'e}v{\'e}nements et {\'e}tats nous avons recours {\`a} une analyse compl{\`e}te de la phrase. Nos r{\'e}sultats sont proches de travaux comparables sur l{'}anglais, en l{'}absence d{'}{\'e}valuation quantitative similaire sur le fran{\c{c}}ais."
W06-3811,Synonym Extraction Using a Semantic Distance on a Dictionary,2006,20,29,1,1,5596,philippe muller,Proceedings of {T}ext{G}raphs: the First Workshop on Graph Based Methods for Natural Language Processing,0,"Synonyms extraction is a difficult task to achieve and evaluate. Some studies have tried to exploit general dictionaries for that purpose, seeing them as graphs where words are related by the definition they appear in, in a complex network of an arguably semantic nature. The advantage of using a general dictionary lies in the coverage, and the availability of such resources, in general and also in specialised domains. We present here a method exploiting such a graph structure to compute a distance between words. This distance is used to isolate candidate synonyms for a given word. We present an evaluation of the relevance of the candidates on a sample of the lexicon."
C04-1008,Annotating and measuring temporal relations in texts,2004,13,24,1,1,5596,philippe muller,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper focuses on the automated processing of temporal information in written texts, more specifically on relations between events introduced by verbs in finite clauses. While this problem has been largely studied from a theoretical point of view, it has very rarely been applied to real texts, if ever, with quantified results. The methodology required is still to be defined, even though there have been proposals in the strictly human annotation case. We propose here both a procedure to achieve this task and a way of measuring the results. We have been testing the feasibility of this on newswire articles, with promising results."
C04-1173,Word Sense Disambiguation using a dictionary for sense similarity measure,2004,12,9,3,0,18789,bruno gaume,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper presents a disambiguation method in which word senses are determined using a dictionary. We use a semantic proximity measure between words in the dictionary, taking into account the whole topology of the dictionary, seen as a graph on its entries. We have tested the method on the problem of disambiguation of the dictionary entries themselves, with promising results considering we do not use any prior annotated data."
2004.jeptalnrecital-long.6,"D{\\'e}sambigu{\\\\\i}sation par proximit{\\'e} structurelle""",2004,-1,-1,3,0,18789,bruno gaume,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}article pr{\'e}sente une m{\'e}thode de d{\'e}sambigu{\""\i}sation dans laquelle le sens est d{\'e}termin{\'e} en utilisant un dictionnaire. La m{\'e}thode est bas{\'e}e sur un algorithme qui calcule une distance Â« s{\'e}mantique Â» entre les mots du dictionnaire en prenant en compte la topologie compl{\`e}te du dictionnaire, vu comme un graphe sur ses entr{\'e}es. Nous l{'}avons test{\'e}e sur la d{\'e}sambigu{\""\i}sation des d{\'e}finitions du dictionnaire elles-m{\^e}mes. L{'}article pr{\'e}sente des r{\'e}sultats pr{\'e}liminaires, qui sont tr{\`e}s encourageants pour une m{\'e}thode ne n{\'e}cessitant pas de corpus annot{\'e}."
2004.jeptalnrecital-long.29,Une m{\\'e}thode pour l{'}annotation de relations temporelles dans des textes et son {\\'e}valuation,2004,-1,-1,1,1,5596,philippe muller,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article traite de l{'}annotation automatique d{'}informations temporelles dans des textes et vise plus particuli{\`e}rement les relations entre {\'e}v{\'e}nements introduits par les verbes dans chaque clause. Si ce probl{\`e}me a mobilis{\'e} beaucoup de chercheurs sur le plan th{\'e}orique, il reste en friche pour ce qui est de l{'}annotation automatique syst{\'e}matique (et son {\'e}valuation), m{\^e}me s{'}il existe des d{\'e}buts de m{\'e}thodologie pour faire r{\'e}aliser la t{\^a}che par des humains. Nous proposons ici {\`a} la fois une m{\'e}thode pour r{\'e}aliser la t{\^a}che automatiquement et une mani{\`e}re de mesurer {\`a} quel degr{\'e} l{'}objectif est atteint. Nous avons test{\'e} la faisabilit{\'e} de ceci sur des d{\'e}p{\^e}ches d{'}agence avec des premiers r{\'e}sultats encourageants."
