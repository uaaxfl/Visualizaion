2020.lrec-1.97,Books of Hours. the First Liturgical Data Set for Text Segmentation.,2020,-1,-1,1,1,16811,amir hazem,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The Book of Hours was the bestseller of the late Middle Ages and Renaissance. It is a historical invaluable treasure, documenting the devotional practices of Christians in the late Middle Ages. Up to now, its textual content has been scarcely studied because of its manuscript nature, its length and its complex content. At first glance, it looks too standardized. However, the study of book of hours raises important challenges: (i) in image analysis, its often lavish ornamentation (illegible painted initials, line-fillers, etc.), abbreviated words, multilingualism are difficult to address in Handwritten Text Recognition (HTR); (ii) its hierarchical entangled structure offers a new field of investigation for text segmentation; (iii) in digital humanities, its textual content gives opportunities for historical analysis. In this paper, we provide the first corpus of books of hours, which consists of Latin transcriptions of 300 books of hours generated by Handwritten Text Recognition (HTR) - that is like Optical Character Recognition (OCR) but for handwritten and not printed texts. We designed a structural scheme of the book of hours and annotated manually two books of hours according to this scheme. Lastly, we performed a systematic evaluation of the main state of the art text segmentation approaches."
2020.computerm-1.9,Towards Automatic Thesaurus Construction and Enrichment.,2020,-1,-1,1,1,16811,amir hazem,Proceedings of the 6th International Workshop on Computational Terminology,0,"Thesaurus construction with minimum human efforts often relies on automatic methods to discover terms and their relations. Hence, the quality of a thesaurus heavily depends on the chosen methodologies for: (i) building its content (terminology extraction task) and (ii) designing its structure (semantic similarity task). The performance of the existing methods on automatic thesaurus construction is still less accurate than the handcrafted ones of which is important to highlight the drawbacks to let new strategies build more accurate thesauri models. In this paper, we will provide a systematic analysis of existing methods for both tasks and discuss their feasibility based on an Italian Cybersecurity corpus. In particular, we will provide a detailed analysis on how the semantic relationships network of a thesaurus can be automatically built, and investigate the ways to enrich the terminological scope of a thesaurus by taking into account the information contained in external domain-oriented semantic sets."
2020.computerm-1.13,{T}erm{E}val 2020: {TALN}-{LS}2{N} System for Automatic Term Extraction,2020,-1,-1,1,1,16811,amir hazem,Proceedings of the 6th International Workshop on Computational Terminology,0,"Automatic terminology extraction is a notoriously difficult task aiming to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. The main ways that addressed this task can be ranged in four main categories: (i) rule-based approaches, (ii) feature-based approaches, (iii) context-based approaches, and (iv) hybrid approaches. For this first TermEval shared task, we explore a feature-based approach, and a deep neural network multitask approach -BERT- that we fine-tune for term extraction. We show that BERT models (RoBERTa for English and CamemBERT for French) outperform other systems for French and English languages."
2020.coling-main.527,Data Selection for Bilingual Lexicon Induction from Specialized Comparable Corpora,2020,-1,-1,2,0,21642,martin laville,Proceedings of the 28th International Conference on Computational Linguistics,0,"Narrow specialized comparable corpora are often small in size. This particularity makes it difficult to build efficient models to acquire translation equivalents, especially for less frequent and rare words. One way to overcome this issue is to enrich the specialized corpora with out-of-domain resources. Although some recent studies have shown improvements using data augmentation, the enrichment method was roughly conducted by adding out-of-domain data with no particular attention given to how to enrich words and how to do it optimally. In this paper, we contrast several data selection techniques to improve bilingual lexicon induction from specialized comparable corpora. We first apply two well-established data selection techniques often used in machine translation that is: Tf-Idf and cross entropy. Then, we propose to exploit BERT for data selection. Overall, all the proposed techniques improve the quality of the extracted bilingual lexicons by a large margin. The best performing model is the cross entropy, obtaining a gain of about 4 points in MAP while decreasing computation time by a factor of 10."
2020.coling-main.549,Hierarchical Text Segmentation for Medieval Manuscripts,2020,-1,-1,1,1,16811,amir hazem,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we address the segmentation of books of hours, Latin devotional manuscripts of the late Middle Ages, that exhibit challenging issues: a complex hierarchical entangled structure, variable content, noisy transcriptions with no sentence markers, and strong correlations between sections for which topical information is no longer sufficient to draw segmentation boundaries. We show that the main state-of-the-art segmentation methods are either inefficient or inapplicable for books of hours and propose a bottom-up greedy approach that considerably enhances the segmentation results. We stress the importance of such hierarchical segmentation of books of hours for historians to explore their overarching differences underlying conception about Church."
2020.bucc-1.9,{TALN}/{LS}2{N} Participation at the {BUCC} Shared Task: Bilingual Dictionary Induction from Comparable Corpora,2020,-1,-1,2,0,21642,martin laville,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,0,"This paper describes the TALN/LS2N system participation at the Building and Using Comparable Corpora (BUCC) shared task. We first introduce three strategies: (i) a word embedding approach based on fastText embeddings; (ii) a concatenation approach using both character Skip-Gram and character CBOW models, and finally (iii) a cognates matching approach based on an exact match string similarity. Then, we present the applied strategy for the shared task which consists in the combination of the embeddings concatenation and the cognates matching approaches. The covered languages are French, English, German, Russian and Spanish. Overall, our system mixing embeddings concatenation and perfect cognates matching obtained the best results while compared to individual strategies, except for English-Russian and Russian-English language pairs for which the concatenation approach was preferred."
W19-4730,Towards Automatic Variant Analysis of Ancient Devotional Texts,2019,-1,-1,1,1,16811,amir hazem,Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,0,"We address in this paper the issue of text reuse in liturgical manuscripts of the middle ages. More specifically, we study variant readings of the Obsecro Te prayer, part of the devotional Books of Hours often used by Christians as guidance for their daily prayers. We aim at automatically extracting and categorising pairs of words and expressions that exhibit variant relations. For this purpose, we adopt a linguistic classification that allows to better characterize the variants than edit operations. Then, we study the evolution of Obsecro Te texts from a temporal and geographical axis. Finally, we contrast several unsupervised state-of-the-art approaches for the automatic extraction of Obsecro Te variants. Based on the manual observation of 772 Obsecro Te copies which show more than 21,000 variants, we show that the proposed methodology is helpful for an automatic study of variants and may serve as basis to analyze and to depict useful information from devotional texts."
R19-1054,Tweaks and Tricks for Word Embedding Disruptions,2019,0,0,1,1,16811,amir hazem,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Word embeddings are established as very effective models used in several NLP applications. If they differ in their architecture and training process, they often exhibit similar properties and remain vector space models with continuously-valued dimensions describing the observed data. The complexity resides in the developed strategies for learning the values within each dimensional space. In this paper, we introduce the concept of disruption which we define as a side effect of the training process of embedding models. Disruptions are viewed as a set of embedding values that are more likely to be noise than effective descriptive features. We show that dealing with disruption phenomenon is of a great benefit to bottom-up sentence embedding representation. By contrasting several in-domain and pre-trained embedding models, we propose two simple but very effective tweaking techniques that yield strong empirical improvements on textual similarity task."
R19-1055,Meta-Embedding Sentence Representation for Textual Similarity,2019,0,0,1,1,16811,amir hazem,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Word embedding models are now widely used in most NLP applications. Despite their effectiveness, there is no clear evidence about the choice of the most appropriate model. It often depends on the nature of the task and on the quality and size of the used data sets. This remains true for bottom-up sentence embedding models. However, no straightforward investigation has been conducted so far. In this paper, we propose a systematic study of the impact of the main word embedding models on sentence representation. By contrasting in-domain and pre-trained embedding models, we show under which conditions they can be jointly used for bottom-up sentence embeddings. Finally, we propose the first bottom-up meta-embedding representation at the sentence level for textual similarity. Significant improvements are observed in several tasks including question-to-question similarity, paraphrasing and next utterance ranking."
2019.jeptalnrecital-court.28,R{\\'e}utilisation de Textes dans les Manuscrits Anciens (Text Reuse in Ancient Manuscripts),2019,-1,-1,1,1,16811,amir hazem,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Nous nous int{\'e}ressons dans cet article {\`a} la probl{\'e}matique de r{\'e}utilisation de textes dans les livres liturgiques du Moyen {\^A}ge. Plus particuli{\`e}rement, nous {\'e}tudions les variations textuelles de la pri{\`e}re Obsecro Te souvent pr{\'e}sente dans les livres d{'}heures. L{'}observation manuelle de 772 copies de l{'}Obsecro Te a montr{\'e} l{'}existence de plus de 21 000 variantes textuelles. Dans le but de pouvoir les extraire automatiquement et les cat{\'e}goriser, nous proposons dans un premier temps une classification lexico-s{\'e}mantique au niveau n-grammes de mots pour ensuite rendre compte des performances de plusieurs approches {\'e}tat-de-l{'}art d{'}appariement automatique de variantes textuelles de l{'}Obsecro Te."
L18-1045,Word Embedding Approach for Synonym Extraction of Multi-Word Terms,2018,0,1,1,1,16811,amir hazem,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1330,"{P}y{RATA}, Python Rule-based fe{A}ture s{T}ructure Analysis",2018,0,0,2,0,17729,nicolas hernandez,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1432,A Multi-Domain Framework for Textual Similarity. A Case Study on Question-to-Question and Question-Answering Similarity Tasks,2018,0,0,1,1,16811,amir hazem,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1080,Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora,2018,0,1,1,1,16811,amir hazem,Proceedings of the 27th International Conference on Computational Linguistics,0,"Recent evaluations on bilingual lexicon extraction from specialized comparable corpora have shown contrasted performance while using word embedding models. This can be partially explained by the lack of large specialized comparable corpora to build efficient representations. Within this context, we try to answer the following questions: First, (i) among the state-of-the-art embedding models, whether trained on specialized corpora or pre-trained on large general data sets, which one is the most appropriate model for bilingual terminology extraction? Second (ii) is it worth it to combine multiple embeddings trained on different data sets? For that purpose, we propose the first systematic evaluation of different word embedding models for bilingual terminology extraction from specialized comparable corpora. We emphasize how the character-based embedding model outperforms other models on the quality of the extracted bilingual lexicons. Further more, we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets. Our approach leads to higher performance than the best individual embedding model."
hazem-etal-2017-mappsent,{M}app{S}ent: a Textual Mapping Approach for Question-to-Question Similarity,2017,0,2,1,1,16811,amir hazem,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Since the advent of word embedding methods, the representation of longer pieces of texts such as sentences and paragraphs is gaining more and more interest, especially for textual similarity tasks. Mikolov et al. (2013) have demonstrated that words and phrases exhibit linear structures that allow to meaningfully combine words by an element-wise addition of their vector representations. Recently, Arora et al. (2017) have shown that removing the projections of the weighted average sum of word embedding vectors on their first principal components, outperforms sophisticated supervised methods including RNN{'}s and LSTM{'}s. Inspired by Mikolov et al. (2013) and Arora et al. (2017) findings and by a bilingual word mapping technique presented in Artetxe et al. (2016), we introduce MappSent, a novel approach for textual similarity. Based on a linear sentence embedding representation, its principle is to build a matrix that maps sentences in a joint-subspace where similar sets of sentences are pushed closer. We evaluate our approach on the SemEval 2016/2017 question-to-question similarity task and show that overall MappSent achieves competitive results and outperforms in most cases state-of-art methods."
I17-4034,{M}app{S}ent at {IJCNLP}-2017 Task 5: A Textual Similarity Approach Applied to Multi-choice Question Answering in Examinations,2017,0,0,1,1,16811,amir hazem,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"In this paper we present MappSent, a textual similarity approach that we applied to the multi-choice question answering in exams shared task. MappSent has initially been proposed for question-to-question similarity hazem2017. In this work, we present the results of two adaptations of MappSent for the question answering task on the English dataset."
I17-1069,Bilingual Word Embeddings for Bilingual Terminology Extraction from Specialized Comparable Corpora,2017,0,2,1,1,16811,amir hazem,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Bilingual lexicon extraction from comparable corpora is constrained by the small amount of available data when dealing with specialized domains. This aspect penalizes the performance of distributional-based approaches, which is closely related to the reliability of word{'}s cooccurrence counts extracted from comparable corpora. A solution to avoid this limitation is to associate external resources with the comparable corpus. Since bilingual word embeddings have recently shown efficient models for learning bilingual distributed representation of words, we explore different word embedding models and show how a general-domain comparable corpus can enrich a specialized comparable corpus via neural networks"
L16-1496,Bilingual Lexicon Extraction at the Morpheme Level Using Distributional Analysis,2016,23,0,1,1,16811,amir hazem,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Bilingual lexicon extraction from comparable corpora is usually based on distributional methods when dealing with single word terms (SWT). These methods often treat SWT as single tokens without considering their compositional property. However, many SWT are compositional (composed of roots and affixes) and this information, if taken into account can be very useful to match translational pairs, especially for infrequent terms where distributional methods often fail. For instance, the English compound \textit{xenograft} which is composed of the root \textit{xeno} and the lexeme \textit{graft} can be translated into French compositionally by aligning each of its elements (\textit{xeno} with \textit{x{\'e}no} and \textit{graft} with \textit{greffe}) resulting in the translation: \textit{x{\'e}nogreffe}. In this paper, we experiment several distributional modellings at the morpheme level that we apply to perform compositional translation to a subset of French and English compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level."
L16-1661,Improving Bilingual Terminology Extraction from Comparable Corpora via Multiple Word-Space Models,2016,24,1,1,1,16811,amir hazem,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"There is a rich flora of word space models that have proven their efficiency in many different applications including information retrieval (Dumais, 1988), word sense disambiguation (Schutze, 1992), various semantic knowledge tests (Lund et al., 1995; Karlgren, 2001), and text categorization (Sahlgren, 2005). Based on the assumption that each model captures some aspects of word meanings and provides its own empirical evidence, we present in this paper a systematic exploration of the principal corpus-based word space models for bilingual terminology extraction from comparable corpora. We find that, once we have identified the best procedures, a very simple combination approach leads to significant improvements compared to individual models."
C16-1321,Efficient Data Selection for Bilingual Terminology Extraction from Comparable Corpora,2016,0,5,1,1,16811,amir hazem,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Comparable corpora are the main alternative to the use of parallel corpora to extract bilingual lexicons. Although it is easier to build comparable corpora, specialized comparable corpora are often of modest size in comparison with corpora issued from the general domain. Consequently, the observations of word co-occurrences which are the basis of context-based methods are unreliable. We propose in this article to improve word co-occurrences of specialized comparable corpora and thus context representation by using general-domain data. This idea, which has been already used in machine translation task for more than a decade, is not straightforward for the task of bilingual lexicon extraction from specific-domain comparable corpora. We go against the mainstream of this task where many studies support the idea that adding out-of-domain documents decreases the quality of lexicons. Our empirical evaluation shows the advantages of this approach which induces a significant gain in the accuracy of extracted lexicons."
W15-3413,{LINA}: Identifying Comparable Documents from {W}ikipedia,2015,11,6,2,0,17779,emmanuel morin,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"This paper describes the LINA system for the BUCC 2015 shared track. Following (Enright and Kondrak, 2007), our system identify comparable documents by collecting counts of hapax words. We extend this method by filtering out document pairs sharing target documents using pigeonhole reasoning and cross-lingual information ."
N15-1103,Continuous Adaptation to User Feedback for Statistical Machine Translation,2015,11,0,3,0,3257,frederic blain,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper gives a detailed experiment feedback of different approaches to adapt a statistical machine translation system towards a targeted translation project, using only small amounts of parallel in-domain data. The experiments were performed by professional translators under realistic conditions of work using a computer assisted translation tool. We analyze the influence of these adaptations on the translator productivity and on the overall post-editing effort. We show that significant improvements can be obtained by using the presented adaptation techniques."
P14-1121,Looking at Unbalanced Specialized Comparable Corpora for Bilingual Lexicon Extraction,2014,29,11,2,0,17779,emmanuel morin,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the historical contextbased projection method dedicated to this task is relatively insensitive to the sizes of each part of the comparable corpus. Within this context, we have carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons."
daille-hazem-2014-semi,Semi-compositional Method for Synonym Extraction of Multi-Word Terms,2014,24,2,2,0,5603,beatrice daille,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Automatic synonyms and semantically related word extraction is a challenging task, useful in many NLP applications such as question answering, search query expansion, text summarization, etc. While different studies addressed the task of word synonym extraction, only a few investigations tackled the problem of acquiring synonyms of multi-word terms (MWT) from specialized corpora. To extract pairs of synonyms of multi-word terms, we propose in this paper an unsupervised semi-compositional method that makes use of distributional semantics and exploit the compositional property shared by most MWT. We show that our method outperforms significantly the state-of-the-art."
W13-2504,A Comparison of Smoothing Techniques for Bilingual Lexicon Extraction from Comparable Corpora,2013,32,4,1,1,16811,amir hazem,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"Smoothing is a central issue in language modeling and a prior step in different natural language processing (NLP) tasks. However, less attention has been given to it for bilingual lexicon extraction from comparable corpora. If a first work to improve the extraction of low frequency words showed significant improvement while using distance-based averaging (Pekar et al., 2006), no investigation of the many smoothing techniques has been carried out so far. In this paper, we present a study of some widelyused smoothing algorithms for language n-gram modeling (Laplace, Good-Turing, Kneser-Ney...). Our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach (Fung, 1998) traditionally used for bilingual lexicon extraction. We show that using smoothing as a preprocessing step of the standard approach increases its performance significantly."
I13-1196,Word Co-occurrence Counts Prediction for Bilingual Terminology Extraction from Comparable Corpora,2013,23,4,1,1,16811,amir hazem,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Methods dealing with bilingual lexicon extraction from comparable corpora are often based on word co-occurrence observation and are by essence more effective when using large corpora. In most cases, specialized comparable corpora are of small size, and this particularity has a direct impact on bilingual terminology extraction results. In order to overcome insufficient data coverage and to make word co-occurrence statistics more reliable, we propose building a predictive model of word co-occurrence counts. We compare different predicting models with the traditional Standard Approach and show that once we have identified the best procedures, our method increases significantly the performance of extracting word translations from comparable corpora."
F13-1018,Bilingual Lexicon Extraction from Comparable Corpora by Combining Contextual Representations (Extraction de lexiques bilingues {\\`a} partir de corpus comparables par combinaison de repr{\\'e}sentations contextuelles) [in {F}rench],2013,0,0,1,1,16811,amir hazem,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
W12-1107,Participation du {LINA} {\\`a} {DEFT}2012 ({LINA} at {DEFT}2012) [in {F}rench],2012,-1,-1,2,0,4245,florian boudin,"JEP-TALN-RECITAL 2012, Workshop DEFT 2012: D{\\'E}fi Fouille de Textes (DEFT 2012 Workshop: Text Mining Challenge)",0,None
hazem-morin-2012-adaptive,Adaptive Dictionary for Bilingual Lexicon Extraction from Comparable Corpora,2012,19,14,1,1,16811,amir hazem,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"One of the main resources used for the task of bilingual lexicon extraction from comparable corpora is : the bilingual dictionary, which is considered as a bridge between two languages. However, no particular attention has been given to this lexicon, except its coverage, and the fact that it can be issued from the general language, the specialised one, or a mix of both. In this paper, we want to highlight the idea that a better consideration of the bilingual dictionary by studying its entries and filtering the non-useful ones, leads to a better lexicon extraction and thus, reach a higher precision. The experiments are conducted on a medical domain corpora. The French-English specialised corpus 'breast cancer' of 1 million words. We show that the empirical results obtained with our filtering process improve the standard approach traditionally dedicated to this task and are promising for future work."
W11-1206,Bilingual Lexicon Extraction from Comparable Corpora as Metasearch,2011,23,8,1,1,16811,amir hazem,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"In this article we present a novel way of looking at the problem of automatic acquisition of pairs of translationally equivalent words from comparable corpora. We first present the standard and extended approaches traditionally dedicated to this task. We then reinterpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. The empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach."
2011.jeptalnrecital-long.13,"Degr{\\'e} de comparabilit{\\'e}, extraction lexicale bilingue et recherche d{'}information interlingue (Degree of comparability, bilingual lexical extraction and cross-language information retrieval)",2011,-1,-1,4,0,9573,bo li,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous {\'e}tudions dans cet article le probl{\`e}me de la comparabilit{\'e} des documents composant un corpus comparable afin d{'}am{\'e}liorer la qualit{\'e} des lexiques bilingues extraits et les performances des syst{\`e}mes de recherche d{'}information interlingue. Nous proposons une nouvelle approche qui permet de garantir un certain degr{\'e} de comparabilit{\'e} et d{'}homog{\'e}n{\'e}it{\'e} du corpus tout en pr{\'e}servant une grande part du vocabulaire du corpus d{'}origine. Nos exp{\'e}riences montrent que les lexiques bilingues que nous obtenons sont d{'}une meilleure qualit{\'e} que ceux obtenus avec les approches pr{\'e}c{\'e}dentes, et qu{'}ils peuvent {\^e}tre utilis{\'e}s pour am{\'e}liorer significativement les syst{\`e}mes de recherche d{'}information interlingue."
2011.jeptalnrecital-long.19,M{\\'e}tarecherche pour l{'}extraction lexicale bilingue {\\`a} partir de corpus comparables (Metasearch for bilingual lexical extraction from comparable corpora),2011,-1,-1,1,1,16811,amir hazem,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous pr{\'e}sentons dans cet article une nouvelle mani{\`e}re d{'}aborder le probl{\`e}me de l{'}acquisition automatique de paires de mots en relation de traduction {\`a} partir de corpus comparables. Nous d{\'e}crivons tout d{'}abord les approches standard et par similarit{\'e} interlangue traditionnellement d{\'e}di{\'e}es {\`a} cette t{\^a}che. Nous r{\'e}interpr{\'e}tons ensuite la m{\'e}thode par similarit{\'e} interlangue et motivons un nouveau mod{\`e}le pour reformuler cette approche inspir{\'e}e par les m{\'e}tamoteurs de recherche d{'}information. Les r{\'e}sultats empiriques que nous obtenons montrent que les performances de notre mod{\`e}le sont toujours sup{\'e}rieures {\`a} celles obtenues avec l{'}approche par similarit{\'e} interlangue, mais aussi comme {\'e}tant comp{\'e}titives par rapport {\`a} l{'}approche standard."
