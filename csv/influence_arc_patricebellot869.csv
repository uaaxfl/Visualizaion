2003.jeptalnrecital-poster.11,P00-1011,0,0.0196462,"du « focus » (les informations qui le qualifient) ou encore un ensemble de mots clés à trouver dans le voisinage de la réponse. De même, le prototype Q/R du LIA affecte à chaque question une ou plusieurs étiquettes (classées par ordre de probabilité parmi une hiérarchie d’étiquettes déduites des questions de TREC-9 et 10) ou même « unknown » en cas d’indécision. L’étiquetage de la question se fait de préférence à partir d’un étiqueteur à base de règles (reposant sur des motifs constitués par des mots de la question ou leur étiquette syntaxique), et sinon à partir d’un étiqueteur probabiliste (Béchet et al., 2000). Recherche des documents susceptibles de contenir la réponse : le prototype Q/R du LIA ne contient pas de tel module mais dépend de la liste des 1000 premiers documents proposés par les organisateurs de la campagne en considérant la question brute comme requête. Il est à noter qu’il n’y a aucune garantie que la réponse à la question soit effectivement comprise dans le jeu de documents fournis même si elle est comprise dans le reste du corpus de référence. Reconnaissance des entités nommées : ce travail a été effectué par la société Sinequa (http://www.sinequa.com). Les entités sont détectées"
2003.jeptalnrecital-poster.11,P02-1006,0,0.0488816,"ortée était d’ailleurs d’éviter dans la mesure du possible un système Q/R complètement architecturé sur ce principe. Cependant, dans un tel cas, il est tout de même intéressant de se demander, notamment au niveau applicatif, quel est l’intérêt de chercher à nouveau une réponse (ou une autre formulation d&apos;une réponse) qui est déjà connue. À plus long terme, l&apos;objectif visé est de disposer d’une source de connaissances de qualité pour servir d&apos;amorces à des itérations dont le but est d&apos;extraire des connaissances similaires (motifs d&apos;extraction, motifs de réponses ; Soubbotin & Soubbotin, 2001 ; Ravichandran & Hovy, 2002) soit à l’intérieur du corpus, soit à partir d’Internet ; mais surtout d’évaluer et comparer la crédibilité de différentes réponses candidates (notamment au travers des relations permettant de l&apos;établir ou liant ses différents constituants). 3.1.1 Construction des BC Les « connaissances » ont été extraites de tableaux et de listes provenant de différents sites Internet de nature encyclopédique. En fait, l’un des principaux critères était que ces informations soient présentées sous une forme fortement structurée pour en faciliter l’extraction (manuelle) et qu’elles soient en nombre suffisant. E"
2005.jeptalnrecital-court.21,P03-1071,0,0.0529727,"Missing"
2005.jeptalnrecital-court.21,J02-1002,0,0.0573078,"Missing"
2005.jeptalnrecital-court.21,P01-1064,0,0.0500489,"Missing"
2006.jeptalnrecital-long.31,sekine-etal-2002-extended,0,0.0265427,"Missing"
2007.jeptalnrecital-poster.26,ayache-etal-2006-equer,0,0.0379865,"Missing"
2007.jeptalnrecital-poster.26,P00-1037,0,0.12319,"Missing"
2007.jeptalnrecital-poster.26,P02-1019,0,0.0414782,"Missing"
2007.jeptalnrecital-poster.8,ayache-etal-2006-equer,0,0.0563122,"Missing"
2007.jeptalnrecital-poster.8,P03-1001,0,0.044696,"Missing"
2007.jeptalnrecital-poster.8,H01-1006,0,0.0470176,"Missing"
2007.jeptalnrecital-poster.8,magnini-etal-2006-multilingual,0,0.0611991,"Missing"
2008.jeptalnrecital-court.15,H05-1045,0,0.294419,"Missing"
2008.jeptalnrecital-court.15,J02-1002,0,0.0720188,"Missing"
2008.jeptalnrecital-court.15,2008.jeptalnrecital-recital.11,1,0.873769,"Missing"
2008.jeptalnrecital-court.15,W06-0804,0,0.0612141,"Missing"
2008.jeptalnrecital-court.15,W06-0302,0,0.0361102,"Missing"
2008.jeptalnrecital-court.15,W06-1613,0,0.229327,"Missing"
2008.jeptalnrecital-court.15,H05-2018,0,0.042126,"Missing"
2011.jeptalnrecital-court.29,N03-1002,0,0.016579,"Missing"
2011.jeptalnrecital-court.29,A97-1029,0,0.0836565,"Missing"
2011.jeptalnrecital-court.29,J01-1005,0,0.0430585,"Missing"
2011.jeptalnrecital-court.29,C92-2082,0,0.0997957,"Missing"
2011.jeptalnrecital-court.29,W03-0430,0,0.0251729,"Missing"
2011.jeptalnrecital-court.29,sekine-nobata-2004-definition,0,0.0355356,"Missing"
2011.jeptalnrecital-court.29,sekine-etal-2002-extended,0,0.0316589,"Missing"
2011.jeptalnrecital-court.29,P10-1149,0,0.041779,"Missing"
2017.jeptalnrecital-demo.7,E17-2017,0,0.0613853,"Missing"
2018.jeptalnrecital-court.38,E17-2017,0,0.0243788,"Missing"
2018.jeptalnrecital-court.38,L16-1626,0,0.0544262,"Missing"
2018.jeptalnrecital-court.38,W16-6208,0,0.0457277,"Missing"
2018.jeptalnrecital-court.38,D17-1169,0,0.0426451,"Missing"
2018.jeptalnrecital-court.38,J86-2003,0,0.134549,"Missing"
2018.jeptalnrecital-court.38,P15-1107,0,0.0189618,"Missing"
2018.jeptalnrecital-court.38,D07-1043,0,0.0187212,"Missing"
2018.jeptalnrecital-deft.1,2018.jeptalnrecital-deft.12,0,0.102356,"(negative, neutral, positive, mixed), to identify clues of sentiment and target, and to annotate each tweet in terms of source and target concerning all expressed sentiments. Twelve teams participated, mainly on the two first tasks. On the identification of tweets about public transportation, micro F-measure values range from 0.827 to 0.908. On the identification of the global polarity, micro F-measure values range from 0.381 to 0.823. M OTS - CLÉS : Classification automatique, Analyse de sentiments, Fouille de texte. K EYWORDS: Automatic Classification, Sentiment Analysis, Text Mining. c ATALA 2018 219 1 Introduction Dans la continuité de l’édition 2015 (Hamon et al., 2015), la treizième édition du DÉfi Fouille de Textes (DEFT) porte sur l’extraction d’information et l’analyse de sentiments dans des tweets rédigés en français sur la thématique des transports. La campagne s’est déroulée sur une période limitée avec une ouverture des inscriptions le 31 janvier, la diffusion des données d’entraînement à partir du 19 février, et le déroulement de la phase de test entre le 28 mars et le 5 avril, sur une durée de trois jours fixée par chacun des participants. Quinze équipes se sont inscrites,"
benkoussas-etal-2014-collection,kim-etal-2012-annotated,1,\N,Missing
F12-2043,P08-1017,0,0.0596655,"Missing"
F14-2021,C02-1025,0,0.0242794,"Missing"
F14-2021,H05-1062,1,0.723119,"Missing"
F14-2021,galliano-etal-2006-corpus,0,0.0225259,"trois corpus, ` a savoir, les noms de personnes, les toponymes et les noms d’organisations. Les entit´es structur´ees de Quaero ont ´et´e ”aplaties” en ne gardant que la structuration de plus haut niveau (Ex : <pers.ind> <name.first> Jacques </name.first> <name.last> Chirac </name.last> </pers.ind> : <pers> Jacques Chirac </pers>). Deux corpus ont ´et´e utilis´es : 1. Corpus Quaero d’´emissions t´el´e-radio-diffus´ees annot´e en entit´es nomm´ees (QO) Le corpus QO consiste en l’annotation manuelle du corpus ESTER2 et du corpus d’´evaluation de syst`emes de reconnaissance de la parole Quaero (Galliano et al., 2006). Ce corpus comporte la particularit´e de n’ˆetre 4. http ://www.elra.info/ELRA.html 512 Impact de la nature et de la taille des corpus d’apprentissage sur les performances dans la [P-Et2.2] ´tection automatique des entite ´s nomme ´es de Books Plus tard, <pers> R. Janko </pers> s’est inspir´ e des remarques de <pers> T. Weischadle </pers>. ` A propos de l’introduction, il rel` eve plusieurs exceptions aux remarques formul´ ees par <pers> T. Weischadle </pers>, parmi lesquelles la pr´ esence du nom du dieu au vocatif. Revues <pers> Michael Spens </pers>, (<pers> M. Spens </pers>, Paysages cont"
F14-2021,W01-0521,0,0.179702,"Missing"
F14-2021,C96-1079,0,0.305374,"Missing"
F14-2021,M98-1002,0,0.0927634,"Missing"
F14-2021,2010.jeptalnrecital-court.34,0,0.0643462,"Missing"
F14-2021,E03-1015,0,0.0119931,"rminer quelle est la meilleure strat´egie pour adapter un syst`eme existant ` a un nouveau cadre d’utilisation, tout en minimisant l’effort d’adaptation. 1. http ://www.itl.nist.gov/iad/mig/tests/ace/ 2. http ://duc.nist.gov/ 3. http ://www.openedition.org/ 511 ´bastien, Bellot Patrice, Be ´chet Fre ´de ´ric [P-Et2.2] Ollagnier Ana¨ıs, Fournier Se 2 D´ etecteur d’entit´ es nomm´ ees et corpus d’apprentissage Les travaux en REN sont class´es principalement selon trois types d’approches : les approches symboliques, les approches statistiques et les approches hybrides. Les approches symboliques (Poibeau, 2003) se basent sur des r`egles ´ecrites ` a la main, ces approches ont l’avantage de privil´egier la pr´ecision des d´etections mais pr´esentent l’inconv´enient d’ˆetre tr`es d´ependantes du domaine d’application et d’ˆetre coˆ uteuses `a mettre en place si l’on veut obtenir une couverture satisfaisante (Nadeau & Sekine, 2007). Les approches statistiques (Burger et al., 2002) se basent sur un m´ecanisme d’apprentissage `a partir d’un corpus pr´e-´etiquet´e. De mani`ere g´en´erale, ce type d’approche privil´egie le rappel plutˆ ot que la pr´ecision. Cependant, comme pr´ecis´e pr´ec´edemment, ces ap"
F14-2021,W12-3606,0,0.0137225,"d’apprentissage. 2. Corpus Quaero de presse ancienne ´etendue en entit´es nomm´ees (QP) Le corpus QP consiste en l’oc´erisation de 76 num´eros de journaux, publi´es entre 1890-1891, fournis par la Biblioth`eque Nationale de France. Trois publications sont utilis´ees (Le Temps, La Croix et Le Figaro) pour un total de 295 pages. Ce corpus pr´esente plusieurs caract´eristiques. Premi`erement, il est enti`erement constitu´e de documents OCR-is´es dont le taux de qualit´e a ´et´e estim´e bon par rapport `a l’´etat de l’art dans le domaine (Character Error Rate de 5,09 et Word Error Rate de 36,59) (Rosset et al., 2012). Quelques erreurs r´esiduelles persistent notamment dans la reconnaissance de certain caract`ere comme le « e » souvent ´ecrit « o ». Deuxi`emement, ce corpus r´ef`ere `a une p´eriode assez ancienne dont les informations diff`erent des actuelles. Et troisi`emement, la particularit´e des journaux OCR-is´es bas´es sur des ´editions papiers se retrouve ´egalement dans leurs structures en colonnes. De ce fait, les textes conservent de nombreux sauts de ligne ainsi que de nombreuses c´esures. Dans l’article de Rosset (Rosset et al., 2012), trois syst`emes de REN ` a base de m´ethodes statistiques"
F14-2021,A97-1015,0,0.298822,"Missing"
kim-etal-2012-annotated,councill-etal-2008-parscit,0,\N,Missing
L16-1576,kim-etal-2012-annotated,1,0.812728,"Missing"
P13-2027,D12-1087,0,0.383925,"ated “concepts”. This approach mostly relies on pseudorelevance feedback, where these so-called “concepts” are the most frequent words occurring in the top-ranked documents retrieved by the retrieval system (Lavrenko and Croft, 2001). From that perspective, topic models seem attractive in the sense that they can provide a descriptive and intuitive representation of concepts. But how can we quantify the usefulness of these topics with respect to an IR system? Recently, researchers developed measures which evaluate the semantic coherence of topic models (Newman et al., 2010; Mimno et al., 2011; Stevens et al., 2012). We adopt their view of semantic coherence and apply one of these measures to query-oriented topics. The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results sh"
P13-2027,N07-1017,0,0.0305082,"e stemmed with the well-known light Krovetz stemmer and stopwords were removed using the standard English stoplist embedded with Indri (417 words). Measuring the coherence of query-oriented topics TDRM relies on two important parameters: the number of topics K that we want to learn, and the number of feedback documents N from which LDA learns the topics. Varying these two parameters can help to capture more information and to model finer topics, but how about their global semantic coherence? Term similarities measured in restricted domains was the first step for evaluating semantic coherence (Gliozzo et al., 2007), and was a first basis for the development of several topic coherence evaluation measures (Newman et al., 2010). Computing the Pointwise Mutual Information (PMI) of all word pairs over Wikipedia was found to be an effective metric using news and books corpora. Recently, Stevens et al. (2012) used (among others) an aggregate version of this metric to evaluate large amounts of topic models. We use this method to evaluate the coherence of query-oriented topics. Specifically, the coherence 3.2 Semantic coherence evaluation Most coherent topics are composed of rare words that do not often occur in"
P13-2027,D11-1024,0,\N,Missing
P13-2027,N10-1012,0,\N,Missing
S13-2075,baccianella-etal-2010-sentiwordnet,0,0.0957417,"Missing"
S13-2075,C10-2005,0,0.042988,"ated tweets by using Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) (Go, Bhayani et al. 2009). Go et al. (2009) reported that SVM outperforms other classifiers. They tried a unigram and a bigram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decline the results. N-gram with lexicon features and microbloging features were useful but POS features were not (Kouloumpis, Wilson et al. 2011). In contrast, Pak & Paroubek (2010) reported that POS and bigrams both help. Barbosa & Feng (2010) proposed the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words, Agarwal et al. (2011) extended their approach by using real valued prior polarity and by combining prior polarity with POS. They build models for classifying tweets into positive, negative and neutral sentiment classes and three models were proposed: a unigram model, a feature based model and a tree kernel based model which presented a new tree representation for tweets. Both combining unigrams with their feat"
S13-2075,P11-4022,0,0.0362534,"Missing"
S13-2075,pak-paroubek-2010-twitter,0,0.0345866,"l. (2012). Machine learning approaches were employed from annotated tweets by using Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) (Go, Bhayani et al. 2009). Go et al. (2009) reported that SVM outperforms other classifiers. They tried a unigram and a bigram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decline the results. N-gram with lexicon features and microbloging features were useful but POS features were not (Kouloumpis, Wilson et al. 2011). In contrast, Pak & Paroubek (2010) reported that POS and bigrams both help. Barbosa & Feng (2010) proposed the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words, Agarwal et al. (2011) extended their approach by using real valued prior polarity and by combining prior polarity with POS. They build models for classifying tweets into positive, negative and neutral sentiment classes and three models were proposed: a unigram model, a feature based model and a tree kernel based model which presented a new tree rep"
S13-2075,W02-1011,0,0.0167289,"Missing"
S13-2075,W11-2207,0,0.138483,"Missing"
S13-2075,S13-2052,0,0.0683576,"Missing"
S13-2075,W11-0705,0,\N,Missing
S13-2075,W11-2200,0,\N,Missing
S14-2104,P97-1023,0,0.915874,"Missing"
S14-2104,D10-1101,0,0.197463,"ect Sentiment model (HASM) was proposed by Kim et al (Kim, Zhang et al. 2013) to discover a hierarchical structure of aspect-based sentiments from unlabelled online reviews. Supervised methods uses normally a CRF or HMM models. Jin and Ho (Jin and Ho 2009) applied a lexicalized HMM model to extract aspects using the words and their part-of-speech tags in order to learn a model, then unsupervised algorithm for determining the aspect sentiment using the nearest opinion word to the aspect and taking into account the polarity reversal words (such as not). CRF model was used by Jakob and Gurevych (Jakob and Gurevych 2010) with these features: tokens, POS tags, syntactic dependency (if the aspect has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the sentence containing an opinionated expression is labelled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspect-expression from the training set. Our method for aspect extraction is closed to"
S14-2104,S14-2004,0,0.0616281,"Missing"
S14-2104,P10-1042,0,0.743821,"t works did not take this case into account. Therefore, we could define the opinion by the quintuple (Liu 2012) (ei, aij, sijkl, hk, tl) where ei is the entity i, aij are the aspects of the entity i, sijkl is the expressed sentiment on the aspect at the time tl, hk the holder which created the document or the text. This definition does not take into account that the entity has aspects that could have also other aspects which leads to an aspect hierarchy, in order to avoid this information loss, few works have handled this issue, they proposed to represent the aspect as a tree of aspect terms (Wei and Gulla 2010; Kim, Zhang et al. 2013). Supervised and unsupervised methods have been used for handling this task, in this paper, we propose supervised methods and test them over two datasets related to laptop reviews and restaurant reviews provided by the ABSA task of SemEval2014 (Pontiki, Galanis et al. 2014). We tackle four subtasks: 1. Aspect term extraction: CRF model is proposed. 2. Aspect Term Polarity Detection: Multinomial Naive-Bayes classifier with some features such as Z-score, POS and prior polarity extracted from Subjectivity 596 Proceedings of the 8th International Workshop on Semantic Evalu"
S14-2104,H05-1044,0,0.136164,"Missing"
S14-2104,N10-1122,0,\N,Missing
S14-2104,S13-2075,1,\N,Missing
S14-2113,pak-paroubek-2010-twitter,0,0.0390112,"s were employed from annotated tweets by using Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM). The authors (Go, Bhayani et al. 2009) reported that SVM outperforms other classifiers. They tried a unigram and a bigram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the quality of results. The authors in (Kouloumpis, Wilson et al. 2011) found that N-gram with lexicon features and micro-blogging features are useful but POS features are not. In contrast, in (Pak and Paroubek 2010) they reported that POS and bigrams both help. In (Barbosa and Feng 2010) the authors proposed the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS tags, in (Agarwal, Xie et al. 2011) this approach was extended by using real valued prior polarity and by combining prior polarity with POS. Authors in (Saif, He et al. 2012) proposed to use the semantic features, therefore they extracted the named entities in the tweets. Authors in (Hamdan, Béchet et al. 2013) used the concepts extract"
S14-2113,W02-1011,0,0.016803,"Missing"
S14-2113,W11-2207,0,0.209768,"Missing"
S14-2113,H05-1044,0,0.0489275,"Missing"
S14-2113,W11-0705,0,\N,Missing
S14-2113,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S14-2113,C10-2005,0,\N,Missing
S14-2113,S14-2009,0,\N,Missing
S14-2113,S13-2075,1,\N,Missing
S14-2113,P11-4022,0,\N,Missing
S14-2113,S13-2052,0,\N,Missing
S15-2095,baccianella-etal-2010-sentiwordnet,0,0.329518,"on 7. used; they reported that social theories such as Sentiment Consistency and Emotional Contagion could be helpful for sentiment analysis. 2 Related Work 3 Three main approaches for sentiment analysis can be identified in Twitter. The lexicon based approach which depends on sentiment lexicons containing positive, negative and neutral words or expressions; the polarity is computed according to the number of common opinionated words between the lexicons and the text. Many dictionaries have been created manually such as MPQA Lexicon (Wilson et al., 2005) or automatically such as SentiWordNet (Baccianella et al., 2010). Machine learning approach adapts different classifiers and features. Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) were adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia co"
S15-2095,S13-2075,1,0.853834,"Missing"
S15-2095,S14-2113,1,0.775034,"iment Lexicons The system extracts four features from the manual constructed lexicons and six features from the automatic ones. For each sentence the number of positive words, the number of negative ones, the number of positive words divided by the number of negative ones and the polarity of the last word are extracted from manual constructed lexicons. In addition to the sum of the positive scores and the sum of the negative scores from the automatic constructed lexicons. 4.5 Z score Z score can distinguish the importance of each term in each class, their performances have been 570 proved in (Hamdan et al., 2014). We assume as in the mentioned work that the term frequencies are following a multi-nomial distribution. Thus, Z score can be seen as a standardization of the term frequency using multi-nomial distribution. We compute the Z score for each term ti in a class Cj (tij ) by calculating its term relative frequency tf rij in a particular class Cj , as well as the mean (meani ) which is the term probability over the whole corpus multiplied by the number of terms in the class Cj , and standard deviation (sdi ) of term ti according to the underlying corpus. Like in (Hamdan et al., 4 29) we tested diff"
S15-2095,S15-2095,1,0.106861,"noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia concepts are useful with Nave-Bayes classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervise"
S15-2095,S14-2111,0,0.109778,"Missing"
S15-2095,S12-1033,0,0.1551,"Missing"
S15-2095,S13-2053,0,0.544432,"adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia and the adjectives from WordNet, they reported that the DBpedia concepts are useful with Nave-Bayes classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classificatio"
S15-2095,S13-2052,0,0.125799,"y wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervised sentiment classification is 3.1 569 Data and Resources Labeled Data We used the data set provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downloaded these tweets using the given script. We obtained 9646 tweets, the whole training data set is used for training, the provided development set containing 1654 tweets is used for tuning the machine learner. The test data set 2015 contains about 2390 tweets (Rosenthal et al., 5 06). Table 1 shows the distribution of each label in each data set. Twitter all neg. pos. neut. train 9684 1458 3640 4586 dev 1654 340 739 575 test-2015 2390 365 1038 987 Table1. Sentiment labels distribution in"
S15-2095,S15-2078,0,0.143618,"Missing"
S15-2095,W12-3716,0,0.0495006,"ded to feature space where each feature represents the number of words in the text mapped to each cluster. The 1000 clusters is provided in Twitter Word Clusters of CMU ARK group. 1000 clusters were constructed from approximately 56 million tweets. 4.6.2 Topic features Latent dirichlet association or topic modeling is used to extract 10 features. Lda-c is configured with 10 topics and the training data is used for training the model, then for each sentence in the test set, the trained model estimates the number of words assigned to each topic. 4.6.3 Semantic Role Labeling Features Authors in (Ruppenhofer and Rehbein, 2012) encode semantic role labeling features in SVM classifier. Our system also extract two types of features, the names: the whole term which represents an argument of the predicate and the tags: the type of each argument in the text (A0 represents the subject of predicate, A1 the object, AM-TMP the time, AMADV the situation, AM-loc the location). These encodings are defined by the tool which we used (Senna). We think that the predicate arguments can constitute a multi-word expression which may be helpful in Sentiment Classification. 5 5.1 Experiments Experiment Setup We trained the L1-regularized"
S15-2095,W11-2207,0,0.0366967,"es classifier but less useful with SVM. Many features were used with SVM including the lexicon-based features in (Mohammad et al., 2013) which seem to get the most gain in performance. Another work has also proved the importance of lexicon-based features with logistic regression classifier (Miura et al., 4 08; Hamdan et al., 2015a; Hamdan et al., 2015b). The third main approach takes into account the influence of users on their followers and the relation between the users and the tweets they wrote. It assumes that using the Twitter follower graph might improve the polarity classification. In (Speriosu et al., 2011) authors demonstrated that using label propagation with Twitter follower graph improves the polarity classification. In (Tan et al., 2011) authors employed social relation for user-level sentiment analysis. In (Hu et al., 2013) a Sociological Approach to handling the Noisy and short Text (SANT) for supervised sentiment classification is 3.1 569 Data and Resources Labeled Data We used the data set provided in SemEval 2013 for subtask B of sentiment analysis in Twitter (Nakov et al., 2013). The participants have been provided with training tweets annotated positive, negative or neutral. We downl"
S15-2095,H05-2018,0,0.862824,"described in section 6 and future work is presented in section 7. used; they reported that social theories such as Sentiment Consistency and Emotional Contagion could be helpful for sentiment analysis. 2 Related Work 3 Three main approaches for sentiment analysis can be identified in Twitter. The lexicon based approach which depends on sentiment lexicons containing positive, negative and neutral words or expressions; the polarity is computed according to the number of common opinionated words between the lexicons and the text. Many dictionaries have been created manually such as MPQA Lexicon (Wilson et al., 2005) or automatically such as SentiWordNet (Baccianella et al., 2010). Machine learning approach adapts different classifiers and features. Naive Bayes, Maximum Entropy MaxEnt and Support Vector Machines (SVM) were adapted in (Go et al., 2009) in which the authors reported that SVM outperforms other classifiers. They tried a unigram and a bi-gram model in conjunction with parts-of-speech (POS) features; they noted that the unigram model outperforms all other models when using SVM and that POS features decrease the results. Authors in (Hamdan et al., 4 29) used the concepts extracted from DBPedia a"
S15-2128,baccianella-etal-2010-sentiwordnet,0,0.00680142,"egative labels with the following features: - Word n-grams Features Unigrams and bigrams are extracted for each word in the context without any stemming or stop-word removing, all terms with occurrence less than 3 are removed from the feature space. - Sentiment Lexicon-based Features The system extracts four features from the manual constructed lexicons (Bing Liu Lexicon (Hu and Liu, 2004) and MPQA subjectivity Lexicon (Wilson et al., 2005)) and six features from the automatic ones (NRC Hashtag Sentiment Lexicon (Mohammad, 6 07), Sentiment140 Lexicon (Mohammad et al., 2013), and SentiWordNet (Baccianella et al., 2010)). For each context the number of positive words, the number of negative ones, the number of positive words divided by the number of negative ones and the polarity of the last word are extracted from manual constructed lexicons. In addition to the sum of the positive scores and the sum of the negative scores from the automatic constructed 756 lexicons. - Negation Features The rule-based algorithm presented in Christopher Potts Sentiment Symposium Tutorial is implemented. This algorithm appends a negation suffix to all words that appear within a negation scope which is determined by the negatio"
S15-2128,S13-2075,1,0.918282,"Missing"
S15-2128,S14-2113,1,0.925633,", 2010) with the following features: tokens, POS tags, syntactic dependency (if the aspect has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the 754 sentence containing an opinionated expression is labeled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspectexpression from the training set. A CRF model is also used by (Hamdan et al., 2014b) with lexical and POS features. Unsupervised methods based on LDA (Latent Dirichlet allocation) have been proposed. Brody and Elhadad (Brody and Elhadad, 2010) used LDA to figure ou the aspects, determined the number of topics by applying a clustering method, then they used a similar method proposed by Hatzivassiloglou and McKeown (Hatzivassiloglou and McKeown, 1997) to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain, seed sets were used and assigned scores, these scores were propagated using propagation method through the aspect-sentiment gra"
S15-2128,S14-2104,1,0.927942,", 2010) with the following features: tokens, POS tags, syntactic dependency (if the aspect has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the 754 sentence containing an opinionated expression is labeled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspectexpression from the training set. A CRF model is also used by (Hamdan et al., 2014b) with lexical and POS features. Unsupervised methods based on LDA (Latent Dirichlet allocation) have been proposed. Brody and Elhadad (Brody and Elhadad, 2010) used LDA to figure ou the aspects, determined the number of topics by applying a clustering method, then they used a similar method proposed by Hatzivassiloglou and McKeown (Hatzivassiloglou and McKeown, 1997) to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain, seed sets were used and assigned scores, these scores were propagated using propagation method through the aspect-sentiment gra"
S15-2128,S15-2095,1,0.741347,"oduct attributes and tackled the problem of sentiment analysis as a hierarchical classification problem. Unsupervised hierarchical aspect Sentiment model (HASM) was proposed by Kim et al (Kim et al., 3 07) to discover a hierarchical structure of aspect-based sentiments from unlabeled online reviews. Aspect term polarity detection can be seen as a sentence level sentiment analysis. Therefore, many papers can be mentioned. Supervised methods have been widely exploited for this purpose, a classification algorithms with a wise feature extraction could achieve good results (Mohammad et al., 2013) (Hamdan et al., 2015a) (Hamdan et al., 4 29). 3 Opinion Target Expression (OTE) An opinion target expression (OTE) is an expression used in the given text to refer to an aspect or an aspect term related to the reviewed entity. The objective of OTE slot is to extract all opinion target expressions in a restaurant review, OTE could be a word or multiple words. For this purpose, we have used CRF (Conditional Random Field) which have proved its performance in information extraction. We choose the IOB notation for representing each sentence in the review. Therefore, we distinguish the terms at the Beginning, the Insid"
S15-2128,P97-1023,0,0.0380864,"e input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspectexpression from the training set. A CRF model is also used by (Hamdan et al., 2014b) with lexical and POS features. Unsupervised methods based on LDA (Latent Dirichlet allocation) have been proposed. Brody and Elhadad (Brody and Elhadad, 2010) used LDA to figure ou the aspects, determined the number of topics by applying a clustering method, then they used a similar method proposed by Hatzivassiloglou and McKeown (Hatzivassiloglou and McKeown, 1997) to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain, seed sets were used and assigned scores, these scores were propagated using propagation method through the aspect-sentiment graph building from the pairs of aspect and related adjectives. Lin and He (Lin et al., 2012) proposed Joint model of Sentiment and Topic (JST) which extends the state-of-the-art topic model (LDA) by adding a sentiment layer, this model is fully unsupervised and it can detect sentiment and topic simultaneously. Wei and Gulla (Wei and Gulla, 2010) modeled the hierarchical"
S15-2128,D10-1101,0,0.0371309,"rithm to extract the frequent aspects. KNN algorithm is applied to estimate the aspect rating scaling from 1 to 5 stands for (Excellent, Good, Average, Poor, Terrible). Supervised methods uses normally the CRF or HMM models. Jin and Ho (Jin and Ho, 2009) applied a lexicalized HMM model to extract aspects using the words and their part-of-speech tags in order to learn a model, then unsupervised algorithm for determining the aspect sentiment using the nearest opinion word to the aspect and taking into account the polarity reversal words (such as not). A CRF model was used by Jakob and Gurevych (Jakob and Gurevych, 2010) with the following features: tokens, POS tags, syntactic dependency (if the aspect has a relation with the opinionated word), word distance (the distance between the word in the closest noun phrase and the opinionated word), and opinion sentences (each token in the 754 sentence containing an opinionated expression is labeled by this feature), the input of this method is also the opinionated expressions, they use these expressions for predicting the aspect sentiment using the dependency parsing for retrieving the pair aspectexpression from the training set. A CRF model is also used by (Hamdan"
S15-2128,S12-1033,0,0.0642798,"Missing"
S15-2128,S13-2053,0,0.185212,"l relationships among product attributes and tackled the problem of sentiment analysis as a hierarchical classification problem. Unsupervised hierarchical aspect Sentiment model (HASM) was proposed by Kim et al (Kim et al., 3 07) to discover a hierarchical structure of aspect-based sentiments from unlabeled online reviews. Aspect term polarity detection can be seen as a sentence level sentiment analysis. Therefore, many papers can be mentioned. Supervised methods have been widely exploited for this purpose, a classification algorithms with a wise feature extraction could achieve good results (Mohammad et al., 2013) (Hamdan et al., 2015a) (Hamdan et al., 4 29). 3 Opinion Target Expression (OTE) An opinion target expression (OTE) is an expression used in the given text to refer to an aspect or an aspect term related to the reviewed entity. The objective of OTE slot is to extract all opinion target expressions in a restaurant review, OTE could be a word or multiple words. For this purpose, we have used CRF (Conditional Random Field) which have proved its performance in information extraction. We choose the IOB notation for representing each sentence in the review. Therefore, we distinguish the terms at the"
S15-2128,S15-2082,0,0.0675407,"pects of the entity i, sijkl is the expressed sentiment on the aspect at the time tl , hk the holder which created the document or the text. This definition does not take into account that the entity has aspects that could have also other aspects which leads to an aspect hierarchy, in order to avoid this information loss, few work has handled this issue, they proposed to represent the aspect as a tree of aspect terms. In this paper, we focus on Opinion Target Extraction (OTE) and Sentiment Polarity towards a target or a category. The description of each subtask is provided by ABSA organizers (Pontiki et al., 2015). For OTE or aspect term extraction, a CRF model is proposed with IOB annotation and several groups of features including syntactic, lexical, semantic, sentiment lexicon features. For aspect term polarity detection, a logistic regression classifier is trained with weighting schema for positive and negative labels and several groups of features are extracted includ753 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 753–758, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics ing lexical, syntactic, semantic, lexicon and Z"
S15-2128,P10-1042,0,0.0891609,"d McKeown (Hatzivassiloglou and McKeown, 1997) to extract the conjunctive adjectives, but not the disjunctive due to the specificity of the domain, seed sets were used and assigned scores, these scores were propagated using propagation method through the aspect-sentiment graph building from the pairs of aspect and related adjectives. Lin and He (Lin et al., 2012) proposed Joint model of Sentiment and Topic (JST) which extends the state-of-the-art topic model (LDA) by adding a sentiment layer, this model is fully unsupervised and it can detect sentiment and topic simultaneously. Wei and Gulla (Wei and Gulla, 2010) modeled the hierarchical relation between product aspects. They defined Sentiment Ontology Tree (SOT) to formulate the knowledge of hierarchical relationships among product attributes and tackled the problem of sentiment analysis as a hierarchical classification problem. Unsupervised hierarchical aspect Sentiment model (HASM) was proposed by Kim et al (Kim et al., 3 07) to discover a hierarchical structure of aspect-based sentiments from unlabeled online reviews. Aspect term polarity detection can be seen as a sentence level sentiment analysis. Therefore, many papers can be mentioned. Supervi"
S15-2128,H05-2018,0,0.707121,"3 previous and subsequent words, respectively. -word lemma using WordNet. -word POS using NLTK parser. -word shape: the shape of each character in the word (capital letter, small letter, digit, punctuation, other symbol) -word type: the type of the word (uppercase, digit, symbol, combination ) -Named entity: the IOB annotation for the named entity extracted from the review using Senna (Collobert, 2011). -chunk: the chunk of the word (NP, VP, PP) extracted using Senna. -polarity: the sum of word polarity score calculated using Bing Liu Lexicon (Hu and Liu, 2004) and MPQA subjectivity Lexicon (Wilson et al., 2005). -Prefixes (all prefixes having length between one to four ). -Suffixes (all suffixes having length between one to four). -Stop word: if the word is a stop word or not. -if the initial letter is uppercase, if all letters are uppercase, All letters lowercase, All letters digit, Contains a uppercase letter, Contains a lowercase letter, Contains a digit, Contains a alphabet letter, Contains a symbol. We also extract the value of each two successive features in the the range -2,2 (the previous and subsequent two words of actual word) for the following features: word surface, word POS, word chunk,"
S15-2128,N10-1122,0,\N,Missing
S16-1076,baccianella-etal-2010-sentiwordnet,0,0.0413398,"(Kiritchenko et al., 2014). P M I(w, negative) is calculated similarly. For the English language, we have done our testing using the below manual constructed sentiment lexicons: 1. Bing Liu Lexicon of Negative and postive words (Hu and Liu, 2004). 2. MPQA Subjectivity Lexicon, it is a MultiPerspective Question Answering Subjectivity Lexicon (Wilson et al., 2005). And the below automatic constructed sentiment lexicons: 3. Sentiment140 corpora containing tweets with positive or negative emoticons (Go et al., 2009). 4. NRC Hashtag Sentiment Lexicon (Mohammad and Turney, 2013). 5. SentiWordNet 2 (Baccianella et al., 2010), it is the result of automatically annotating all WordNet synsets according to their degrees of positivity, negativity, and neutrality. 6. Sentiment words from the MPQA word list (Riloff et al., 2003; Wilson et al., 2005). We used the positive, negative words only. 7. And we also use the test file’s data of Semeval2013 Task2 (subtaskA) 3 with positive and negative annotated tweets. And based on the test results, we used the sentiment lexicons 1,3,4,5 and 7 of the ones previously mentioned, which gave the best results. For the Arabic language, we are using the below manual constructed sentimen"
S16-1076,P98-2127,0,0.240242,"to express opinions. The syntactic patterns are formed based on part-of-speech (POS) tags, then the sentiment orientation (SO) of the patterns is calculated using the pointwise mutual information (PMI) measure. We renounced the use of syntactic patterns due to the majority of one word phrases in the competition files. Turney and Littman (2003) created two sets of prototypical polar words, one containing positive and another containing negative example words. To compute a new term’s polarity, they used the point-wise mutual information (PMI) between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), for the purpose of creating a large scale Twitter sentiment lexicons. The work of Turney and Littman (2003) is the base of our approach, we use several available sentiment lexicons, and also we use web search engine for each phrase not included in those sentiment lexicons. 3 Difficulties Comparison in this task with languages: English and Arabic Although the same method is applied for both languages: English and Arabic, the level of difficulty is different when treating both of them. On the resources level, for the English language, we c"
S16-1076,W03-1014,0,0.102039,"ive and postive words (Hu and Liu, 2004). 2. MPQA Subjectivity Lexicon, it is a MultiPerspective Question Answering Subjectivity Lexicon (Wilson et al., 2005). And the below automatic constructed sentiment lexicons: 3. Sentiment140 corpora containing tweets with positive or negative emoticons (Go et al., 2009). 4. NRC Hashtag Sentiment Lexicon (Mohammad and Turney, 2013). 5. SentiWordNet 2 (Baccianella et al., 2010), it is the result of automatically annotating all WordNet synsets according to their degrees of positivity, negativity, and neutrality. 6. Sentiment words from the MPQA word list (Riloff et al., 2003; Wilson et al., 2005). We used the positive, negative words only. 7. And we also use the test file’s data of Semeval2013 Task2 (subtaskA) 3 with positive and negative annotated tweets. And based on the test results, we used the sentiment lexicons 1,3,4,5 and 7 of the ones previously mentioned, which gave the best results. For the Arabic language, we are using the below manual constructed sentiment lexicons: 1. Arabic Sentiment Tweets Dataset4 , a set of Arabic tweets containing over 10,000 entries. 2. Twitter data-set for Arabic Sentiment Analysis5 , 1000 positive tweets and 1000 negative one"
S16-1076,P02-1053,0,0.069268,"web search engine to find probability of positivity for the phrase based on its co-occurrence near the word ”excellent” and near the word ”poor”. 2 Related work The sentiment words are the main factor for sentiment classification, by consequence sentiment words and phrases can be used for sentiment classification in an unsupervised method, a method that 469 Proceedings of SemEval-2016, pages 469–473, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics can solve the problem of domain dependency and reduce the need for annotated training data. The method of Turney (2002) is such a technique. It performs classification based on fixed syntactic patterns that are usually used to express opinions. The syntactic patterns are formed based on part-of-speech (POS) tags, then the sentiment orientation (SO) of the patterns is calculated using the pointwise mutual information (PMI) measure. We renounced the use of syntactic patterns due to the majority of one word phrases in the competition files. Turney and Littman (2003) created two sets of prototypical polar words, one containing positive and another containing negative example words. To compute a new term’s polarity"
S16-1076,H05-1044,0,0.512625,"itive or in a positive tweet, f req(w) is the total frequency of term w in sentiment lexicons and labeled tweets, f req(pos) is the total number of positive terms in sentiment lexicons and labeled tweets, and N is the total number of terms in the data-set (Kiritchenko et al., 2014). P M I(w, negative) is calculated similarly. For the English language, we have done our testing using the below manual constructed sentiment lexicons: 1. Bing Liu Lexicon of Negative and postive words (Hu and Liu, 2004). 2. MPQA Subjectivity Lexicon, it is a MultiPerspective Question Answering Subjectivity Lexicon (Wilson et al., 2005). And the below automatic constructed sentiment lexicons: 3. Sentiment140 corpora containing tweets with positive or negative emoticons (Go et al., 2009). 4. NRC Hashtag Sentiment Lexicon (Mohammad and Turney, 2013). 5. SentiWordNet 2 (Baccianella et al., 2010), it is the result of automatically annotating all WordNet synsets according to their degrees of positivity, negativity, and neutrality. 6. Sentiment words from the MPQA word list (Riloff et al., 2003; Wilson et al., 2005). We used the positive, negative words only. 7. And we also use the test file’s data of Semeval2013 Task2 (subtaskA)"
S16-1076,C98-2122,0,\N,Missing
S17-2120,D15-1304,0,0.0311616,"challenges than the English language, since words can have transitional meanings depending on position within a sentence and the type of sentence (verbal or nominal) (Farra et al., 2010), we can still find some interesting experiments in lexical-based sentiment analysis: El-Beltagy and Ali (2013) built a sentiment lexicon based on a manually constructed seed sentiment lexicon of 380 words. Using this lexicon, with assigned sentiment intensity score for each value, they were able to calculate the sentiment orientation for a set of tweets in Arabic language (Egyptian dialect). Another paper by Eskander and Rambow (2015) presented a large list of sentiment lexicon for Arabic language 1 called SLSA where each value is associated with a sentiment intensity score. The scores were assigned due to a link created between the English annotation of each Arabic entry to a synset from SentiWordNet (Cambria et al., 2010). For our system in Arabic language, we are following the same method as the system in English language. But since there is no previously created list of adapted for tweets seed words, we create the list following the same method in (Htait et al., 2017), and then use it with cosine similarity measure of"
S17-2120,W15-4322,0,0.0617458,"Missing"
S17-2120,W06-1642,0,0.147985,"Missing"
S17-2120,P11-1015,0,0.0656585,"isupervised method for sentiment classification that aims to train a classifier with a small number of labeled data (called seed data). Some other experiments used the concept with unsupervied methods which reduces the need of annotated training data. For example Turney (2002; 2003), which used statistical measures to calculate the similarities between words and a list of 14 seed words (Table 1), such as point wise mutual information (PMI). But we should note that Turney’s seed words were manually selected based on restaurant reviews, which have different nature than tweets. Also we find that Maas et al. (2011) used the concept as ”bag of words” but with cosine similarity measure on word embedding. Our previous work (Htait et al., 2017) was on sentiment intensity prediction of tweets segments using SemEval2016 Task71 data. We extracted new seed words as more adapted for tweets seed words. We retrieved the most frequent words in Sentiment140 (Go et al., 2009) and then manually filtered the list to eliminate the neutral words. Our tests in (Htait et al., 2017) showed the efficiency of the new seed words over Turney’s 14 seed words. Also, they showed that using cosine similarity measure of word embeddi"
S17-2120,L16-1006,0,0.0519781,"Missing"
S17-2120,P02-1053,0,0.0322504,"entiment analysis experiments, some used the concept with supervised or semi-supervised methods. 718 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 718–722, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics For example, Ju et al. (2012) worked on a semisupervised method for sentiment classification that aims to train a classifier with a small number of labeled data (called seed data). Some other experiments used the concept with unsupervied methods which reduces the need of annotated training data. For example Turney (2002; 2003), which used statistical measures to calculate the similarities between words and a list of 14 seed words (Table 1), such as point wise mutual information (PMI). But we should note that Turney’s seed words were manually selected based on restaurant reviews, which have different nature than tweets. Also we find that Maas et al. (2011) used the concept as ”bag of words” but with cosine similarity measure on word embedding. Our previous work (Htait et al., 2017) was on sentiment intensity prediction of tweets segments using SemEval2016 Task71 data. We extracted new seed words as more adapt"
S18-1081,E17-2017,0,0.180724,"Missing"
S18-1081,S18-1003,0,0.0195595,"wever, emojis are not used the same way as emoticons in messaging applications. They can convey further information, even more when combined. The advantage of emojis is that they are becoming more standardized, even though existing emojis are still growing quickly2 . This is why emoji prediction is a relatively new task. It can be considered as a composite task mixing emotion prediction for face emojis, aspect/subject detection for object emojis, and other metadata prediction for more abstract emojis, representing ideas for instance. This year, SemEval started the first emoji prediction task (Barbieri et al., 2018). It consists of a multiclass classification task for a total of 20 possible classes, i.e. emojis. This task is interesting in several ways. Firstly, it is a relatively new task that only a few studies did focus on. Secondly, it is 2 Related Work Several research studies focus on emoji prediction. Most of them use word embeddings in order to do a multiclass emoji prediction. At the beginning, images were used instead of text as the source of emoji prediction (Cappallo et al., 2015). Eisner (Eisner et al., 2016) used embeddings based on emoji description in the Unicode3 list, such as smiling fa"
S18-1081,W16-6208,0,0.1499,"Missing"
S18-1081,D17-1169,0,0.0721367,"Missing"
S18-1081,pak-paroubek-2010-twitter,0,0.0388296,"ingual Emoji Prediction. Our system approaches both languages as being equal by first; considering word embeddings associated to automatically computed features of different types, then by applying bagging algorithm RandomForest to predict the emoji of a tweet. 1 Introduction Emojis were first used to emphasize conversations before becoming representations of specific emotions, objects or ideas. They are now used in almost every social medium and conversation devices, such as messenging applications or even emails1 . Tweets and their emoticons were used as labels to predict polarity at first (Pak and Paroubek, 2010). However, emojis are not used the same way as emoticons in messaging applications. They can convey further information, even more when combined. The advantage of emojis is that they are becoming more standardized, even though existing emojis are still growing quickly2 . This is why emoji prediction is a relatively new task. It can be considered as a composite task mixing emotion prediction for face emojis, aspect/subject detection for object emojis, and other metadata prediction for more abstract emojis, representing ideas for instance. This year, SemEval started the first emoji prediction ta"
sitbon-bellot-2006-tools,W97-0304,0,\N,Missing
sitbon-bellot-2006-tools,J97-1003,0,\N,Missing
sitbon-etal-2008-evaluating,sekine-etal-2002-extended,0,\N,Missing
sitbon-etal-2008-evaluating,C02-1109,0,\N,Missing
sitbon-etal-2008-evaluation,J90-1003,0,\N,Missing
sitbon-etal-2008-evaluation,P06-1036,0,\N,Missing
sitbon-etal-2008-evaluation,W04-2117,0,\N,Missing
Y18-1006,W17-1322,0,0.0184974,"s. Therefore, the performance of these systems is limited by the difficulty of the language processing and the considerable lack of effective NLP tools that take into consideration the Arabic language [Alagha & Abu-Taha, 2015], [Al-Khalifa & Al-Wabil, 2007]. Arabic is an inflected and derivational Semitic language where from one root, several forms could be derived. This language poses challenges for the tasks of the NLP, as opposed to other languages, because of its morphological richness, relatively free words order and diglossic nature (where standard and dialects mix in most genres data) [Almarwani and Diab, 2017]. Moreover, with the absence of the capitalization in the Arabic language, we cannot distinguish the verbs from the nouns. The absence of voyellation in the Arabic texts is also a great source of ambiguity for morphological, syntactic and semantic analysis. Thus, the Arabic language still lacks large-scale computing resources that are very useful for English as the WordNet [Miller, 1995], or a resource, such as, VerbOcean [Chklovski and Pantel, 2004]. In this paper, we focus on a few dimensions whose development seems more particularly to the automatic understanding of Arabic texts. Specifica"
Y18-1006,W14-3607,0,0.0468667,"Missing"
Y18-1006,W04-3205,0,0.0442724,"ges, because of its morphological richness, relatively free words order and diglossic nature (where standard and dialects mix in most genres data) [Almarwani and Diab, 2017]. Moreover, with the absence of the capitalization in the Arabic language, we cannot distinguish the verbs from the nouns. The absence of voyellation in the Arabic texts is also a great source of ambiguity for morphological, syntactic and semantic analysis. Thus, the Arabic language still lacks large-scale computing resources that are very useful for English as the WordNet [Miller, 1995], or a resource, such as, VerbOcean [Chklovski and Pantel, 2004]. In this paper, we focus on a few dimensions whose development seems more particularly to the automatic understanding of Arabic texts. Specifically, we introduce a real analysis of text passages which may contain an answer to a question. Indeed, the automatic understanding of a text is one of the basic components of such a question-answering system to choose and obtain a precise answer. Consequently, the approach presented in this paper makes it possible to construct a semantic and logical representation of texts via the conceptual graphs. As a result, text comprehension can be defined as a"
