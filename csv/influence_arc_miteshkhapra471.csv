2020.acl-main.387,D18-1216,0,0.0260729,"work in many ways can be seen as a continuation to the recent studies (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) on the subject of interpretability of attention. Several other works (Shao et al., 2019; Martins and Astudillo, 2016; Malaviya et al., 2018; Niculae and Blondel, 2017; Maruf et al., 2019; Peters et al., 2018) focus on improving the interpretability of attention distributions by inducing sparsity. However, the extent to which sparse attention distributions actually offer faithful and plausible explanations haven’t been studied in detail. Few works (Bao et al., 2018) map attention distributions to human annotated rationales. Our work on the other hand does not require any additional supervision. Work by (Guo et al., 2019) focus on developing interpretable LSTMs specifically for multivariate time series analysis. Several other works (Clark et al., 2019; Vig and Belinkov, 2019; Tenney et al., 2019; Michel et al., 2019; Jawahar et al., 2019; Tsai et al., 2019) analyze attention distributions and attention heads learned by transformer language models. The idea of orthogonalizing representations in an LSTM have been used by (Nema et al., 2017) but they use a d"
2020.acl-main.387,D15-1075,0,0.0151938,"ther Text Classification: We use the Twitter ADR (Nikfarjam et al., 2015) dataset with 8K tweets where the task is to detect if a tweet describes an adverse drug reaction or not. We use a subset of the 20 Newsgroups dataset (Jain and Wallace, 2019) to classify news articles into baseball vs hockey sports categories. From MIMIC ICD9 (Johnson et al., 2016), we use 2 datasets: Anemia, to determine the type of Anemia (Chronic vs Acute) a patient is diagnosed with and Diabetes, to predict whether a patient is diagnosed with Diabetes or not. Natural Language Inference: We consider the SNLI dataset (Bowman et al., 2015) for recogniz4207 ing textual entailment within sentence pairs. The SNLI dataset has three possible classification labels, viz entailment, contradiction and neutral. Paraphrase Detection: We utilize the Quora Question Paraphrase (QQP) dataset (part of the GLUE benchmark (Wang et al., 2018)) with pairs of questions labeled as paraphrased or not. We split the training set into 90 : 10 training and validation; and use the original dev set as our test set. Question Answering: We made use of all three QA tasks from the bAbI dataset (Weston et al., 2015). The tasks consist of answering questions tha"
2020.acl-main.387,P18-1012,0,0.0698195,"Missing"
2020.acl-main.387,W19-4828,0,0.0244419,"Niculae and Blondel, 2017; Maruf et al., 2019; Peters et al., 2018) focus on improving the interpretability of attention distributions by inducing sparsity. However, the extent to which sparse attention distributions actually offer faithful and plausible explanations haven’t been studied in detail. Few works (Bao et al., 2018) map attention distributions to human annotated rationales. Our work on the other hand does not require any additional supervision. Work by (Guo et al., 2019) focus on developing interpretable LSTMs specifically for multivariate time series analysis. Several other works (Clark et al., 2019; Vig and Belinkov, 2019; Tenney et al., 2019; Michel et al., 2019; Jawahar et al., 2019; Tsai et al., 2019) analyze attention distributions and attention heads learned by transformer language models. The idea of orthogonalizing representations in an LSTM have been used by (Nema et al., 2017) but they use a different diversity model in the context of improving performance of Natural Language Generation models 7 Conclusion & Future work In this work, we have analyzed why existing attention distributions can neither provide a faithful nor a plausible explanation for the model’s predictions. We s"
2020.acl-main.387,N19-1357,0,0.689572,"en Table 1: Samples of Attention distributions from Vanilla and Diversity LSTM models on the Quora Question Paraphrase (QQP) & Babi 1 datasets. . Attention mechanisms (Bahdanau et al., 2014; Vaswani et al., 2017) play a very important role in neural network-based models for various Natural Language Processing (NLP) tasks. They not only improve the performance of the model but are also often used to provide insights into the working of a model. Recently, there is a growing debate on whether attention mechanisms can offer transparency to a model or not. For example, Serrano and Smith (2019) and Jain and Wallace (2019) show that high attention weights need not necessarily correspond to a higher impact on the model’s predictions and hence they do not provide a faithful explanation for the model’s predictions. On the other hand, Wiegreffe and Pinter (2019) argues that there is still a possibility that attention distributions may provide a plausible explanation for the predictions. In other words, they might provide 4206 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4206–4216 c July 5 - 10, 2020. 2020 Association for Computational Linguistics a plausible reconst"
2020.acl-main.387,P19-1356,0,0.0138537,"the interpretability of attention distributions by inducing sparsity. However, the extent to which sparse attention distributions actually offer faithful and plausible explanations haven’t been studied in detail. Few works (Bao et al., 2018) map attention distributions to human annotated rationales. Our work on the other hand does not require any additional supervision. Work by (Guo et al., 2019) focus on developing interpretable LSTMs specifically for multivariate time series analysis. Several other works (Clark et al., 2019; Vig and Belinkov, 2019; Tenney et al., 2019; Michel et al., 2019; Jawahar et al., 2019; Tsai et al., 2019) analyze attention distributions and attention heads learned by transformer language models. The idea of orthogonalizing representations in an LSTM have been used by (Nema et al., 2017) but they use a different diversity model in the context of improving performance of Natural Language Generation models 7 Conclusion & Future work In this work, we have analyzed why existing attention distributions can neither provide a faithful nor a plausible explanation for the model’s predictions. We showed that hidden representations learned by LSTM encoders tend to be highly similar acr"
2020.acl-main.387,D16-1011,0,0.165629,"Missing"
2020.acl-main.387,P11-1015,0,0.0733289,"ets spanning different tasks; here, we introduce these datasets and tasks and provide a brief recap of the standard LSTM+attention model used for these tasks. We consider the tasks of Binary Text classification, Natural Language Inference, Paraphrase Detection, and Question Answering. We use a total of 12 datasets, most of them being the same as the ones used in (Jain and Wallace, 2019). We divide Text classification into Sentiment Analysis and Other Text classification for convenience. Sentiment Analysis: We use the Stanford Sentiment Treebank (SST) (Socher et al., 2013), IMDB Movie Reviews (Maas et al., 2011), Yelp and Amazon for sentiment analysis. All these datasets use binary target variable (positive /negative). Other Text Classification: We use the Twitter ADR (Nikfarjam et al., 2015) dataset with 8K tweets where the task is to detect if a tweet describes an adverse drug reaction or not. We use a subset of the 20 Newsgroups dataset (Jain and Wallace, 2019) to classify news articles into baseball vs hockey sports categories. From MIMIC ICD9 (Johnson et al., 2016), we use 2 datasets: Anemia, to determine the type of Anemia (Chronic vs Acute) a patient is diagnosed with and Diabetes, to predict"
2020.acl-main.387,P18-2059,0,0.0681077,"Missing"
2020.acl-main.387,N19-1313,0,0.0271992,"rt the percentage preference given to the vanilla and Diversity LSTM models on the Yelp, SNLI, QQP, and bAbI 1 datasets; the attention distributions from Diversity LSTM significantly outperforms the attention from vanilla LSTM across all the datasets and criteria. 6 Related work Our work in many ways can be seen as a continuation to the recent studies (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) on the subject of interpretability of attention. Several other works (Shao et al., 2019; Martins and Astudillo, 2016; Malaviya et al., 2018; Niculae and Blondel, 2017; Maruf et al., 2019; Peters et al., 2018) focus on improving the interpretability of attention distributions by inducing sparsity. However, the extent to which sparse attention distributions actually offer faithful and plausible explanations haven’t been studied in detail. Few works (Bao et al., 2018) map attention distributions to human annotated rationales. Our work on the other hand does not require any additional supervision. Work by (Guo et al., 2019) focus on developing interpretable LSTMs specifically for multivariate time series analysis. Several other works (Clark et al., 2019; Vig and Belinkov, 2019; T"
2020.acl-main.387,A94-1016,0,0.167444,"Missing"
2020.acl-main.387,L18-1008,0,0.0204878,") the attention distributions are more explainable and align better with a human’s interpretation of the model’s prediction (Secs. 5.6, 5.7). Throughout this section we will compare the following three models: 1. Vanilla LSTM: The model described in section 2.1 which uses the vanilla LSTM. 2. Diversity LSTM: The model described in section 2.1 with the vanilla LSTM but trained with the diversity objective described in section 4.2. 3. Orthogonal LSTM: The model described in Implementation Details For all datasets except bAbi, we either use pretrained Glove (Pennington et al., 2014) or fastText (Mikolov et al., 2018) word embeddings with 300 dimensions. For the bAbi dataset, we learn 50 dimensional word embeddings from scratch during training. We use a 1-layered LSTM as the encoder with hidden size of 128 for bAbi and 256 for the other datasets. For the diversity weight λ, we use a value of 0.1 for SNLI, 0.2 for CNN, and 0.5 for the remaining datasets. We use Adam optimizer with a learning rate of 0.001 and select the best model based on accuracy on the validation split. All the subsequent analysis are performed on the test split. 5.2 Empirical evaluation Our main goal is to show that our proposed models"
2020.acl-main.387,P17-1098,1,0.798913,"etail. Few works (Bao et al., 2018) map attention distributions to human annotated rationales. Our work on the other hand does not require any additional supervision. Work by (Guo et al., 2019) focus on developing interpretable LSTMs specifically for multivariate time series analysis. Several other works (Clark et al., 2019; Vig and Belinkov, 2019; Tenney et al., 2019; Michel et al., 2019; Jawahar et al., 2019; Tsai et al., 2019) analyze attention distributions and attention heads learned by transformer language models. The idea of orthogonalizing representations in an LSTM have been used by (Nema et al., 2017) but they use a different diversity model in the context of improving performance of Natural Language Generation models 7 Conclusion & Future work In this work, we have analyzed why existing attention distributions can neither provide a faithful nor a plausible explanation for the model’s predictions. We showed that hidden representations learned by LSTM encoders tend to be highly similar across different timesteps, thereby affecting the interpretability of attention weights. We proposed two techniques to effectively overcome this shortcoming and showed that attention distributions in the resu"
2020.acl-main.387,D14-1162,0,0.0891206,"Missing"
2020.acl-main.387,W18-5450,0,0.0212138,"eference given to the vanilla and Diversity LSTM models on the Yelp, SNLI, QQP, and bAbI 1 datasets; the attention distributions from Diversity LSTM significantly outperforms the attention from vanilla LSTM across all the datasets and criteria. 6 Related work Our work in many ways can be seen as a continuation to the recent studies (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) on the subject of interpretability of attention. Several other works (Shao et al., 2019; Martins and Astudillo, 2016; Malaviya et al., 2018; Niculae and Blondel, 2017; Maruf et al., 2019; Peters et al., 2018) focus on improving the interpretability of attention distributions by inducing sparsity. However, the extent to which sparse attention distributions actually offer faithful and plausible explanations haven’t been studied in detail. Few works (Bao et al., 2018) map attention distributions to human annotated rationales. Our work on the other hand does not require any additional supervision. Work by (Guo et al., 2019) focus on developing interpretable LSTMs specifically for multivariate time series analysis. Several other works (Clark et al., 2019; Vig and Belinkov, 2019; Tenney et al., 2019; Mi"
2020.acl-main.387,P19-1282,0,0.0760907,"den . Daniel went to the garden Table 1: Samples of Attention distributions from Vanilla and Diversity LSTM models on the Quora Question Paraphrase (QQP) & Babi 1 datasets. . Attention mechanisms (Bahdanau et al., 2014; Vaswani et al., 2017) play a very important role in neural network-based models for various Natural Language Processing (NLP) tasks. They not only improve the performance of the model but are also often used to provide insights into the working of a model. Recently, there is a growing debate on whether attention mechanisms can offer transparency to a model or not. For example, Serrano and Smith (2019) and Jain and Wallace (2019) show that high attention weights need not necessarily correspond to a higher impact on the model’s predictions and hence they do not provide a faithful explanation for the model’s predictions. On the other hand, Wiegreffe and Pinter (2019) argues that there is still a possibility that attention distributions may provide a plausible explanation for the predictions. In other words, they might provide 4206 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4206–4216 c July 5 - 10, 2020. 2020 Association for Computational Lin"
2020.acl-main.387,D13-1170,0,0.0098752,"tions. We experiment on a variety of datasets spanning different tasks; here, we introduce these datasets and tasks and provide a brief recap of the standard LSTM+attention model used for these tasks. We consider the tasks of Binary Text classification, Natural Language Inference, Paraphrase Detection, and Question Answering. We use a total of 12 datasets, most of them being the same as the ones used in (Jain and Wallace, 2019). We divide Text classification into Sentiment Analysis and Other Text classification for convenience. Sentiment Analysis: We use the Stanford Sentiment Treebank (SST) (Socher et al., 2013), IMDB Movie Reviews (Maas et al., 2011), Yelp and Amazon for sentiment analysis. All these datasets use binary target variable (positive /negative). Other Text Classification: We use the Twitter ADR (Nikfarjam et al., 2015) dataset with 8K tweets where the task is to detect if a tweet describes an adverse drug reaction or not. We use a subset of the 20 Newsgroups dataset (Jain and Wallace, 2019) to classify news articles into baseball vs hockey sports categories. From MIMIC ICD9 (Johnson et al., 2016), we use 2 datasets: Anemia, to determine the type of Anemia (Chronic vs Acute) a patient is"
2020.acl-main.387,P19-1452,0,0.0295048,"9; Peters et al., 2018) focus on improving the interpretability of attention distributions by inducing sparsity. However, the extent to which sparse attention distributions actually offer faithful and plausible explanations haven’t been studied in detail. Few works (Bao et al., 2018) map attention distributions to human annotated rationales. Our work on the other hand does not require any additional supervision. Work by (Guo et al., 2019) focus on developing interpretable LSTMs specifically for multivariate time series analysis. Several other works (Clark et al., 2019; Vig and Belinkov, 2019; Tenney et al., 2019; Michel et al., 2019; Jawahar et al., 2019; Tsai et al., 2019) analyze attention distributions and attention heads learned by transformer language models. The idea of orthogonalizing representations in an LSTM have been used by (Nema et al., 2017) but they use a different diversity model in the context of improving performance of Natural Language Generation models 7 Conclusion & Future work In this work, we have analyzed why existing attention distributions can neither provide a faithful nor a plausible explanation for the model’s predictions. We showed that hidden representations learned by"
2020.acl-main.387,W19-4808,0,0.0475592,"2017; Maruf et al., 2019; Peters et al., 2018) focus on improving the interpretability of attention distributions by inducing sparsity. However, the extent to which sparse attention distributions actually offer faithful and plausible explanations haven’t been studied in detail. Few works (Bao et al., 2018) map attention distributions to human annotated rationales. Our work on the other hand does not require any additional supervision. Work by (Guo et al., 2019) focus on developing interpretable LSTMs specifically for multivariate time series analysis. Several other works (Clark et al., 2019; Vig and Belinkov, 2019; Tenney et al., 2019; Michel et al., 2019; Jawahar et al., 2019; Tsai et al., 2019) analyze attention distributions and attention heads learned by transformer language models. The idea of orthogonalizing representations in an LSTM have been used by (Nema et al., 2017) but they use a different diversity model in the context of improving performance of Natural Language Generation models 7 Conclusion & Future work In this work, we have analyzed why existing attention distributions can neither provide a faithful nor a plausible explanation for the model’s predictions. We showed that hidden repres"
2020.acl-main.387,W18-5446,0,0.0240797,"hockey sports categories. From MIMIC ICD9 (Johnson et al., 2016), we use 2 datasets: Anemia, to determine the type of Anemia (Chronic vs Acute) a patient is diagnosed with and Diabetes, to predict whether a patient is diagnosed with Diabetes or not. Natural Language Inference: We consider the SNLI dataset (Bowman et al., 2015) for recogniz4207 ing textual entailment within sentence pairs. The SNLI dataset has three possible classification labels, viz entailment, contradiction and neutral. Paraphrase Detection: We utilize the Quora Question Paraphrase (QQP) dataset (part of the GLUE benchmark (Wang et al., 2018)) with pairs of questions labeled as paraphrased or not. We split the training set into 90 : 10 training and validation; and use the original dev set as our test set. Question Answering: We made use of all three QA tasks from the bAbI dataset (Weston et al., 2015). The tasks consist of answering questions that would require one, two or three supporting statements from the context. The answers are a span in the context. We then use the CNN News Articles dataset (Hermann et al., 2015) consisting of 90k articles with an average of three questions per article along with their corresponding answers"
2020.acl-main.387,D19-1002,0,0.286151,"ole in neural network-based models for various Natural Language Processing (NLP) tasks. They not only improve the performance of the model but are also often used to provide insights into the working of a model. Recently, there is a growing debate on whether attention mechanisms can offer transparency to a model or not. For example, Serrano and Smith (2019) and Jain and Wallace (2019) show that high attention weights need not necessarily correspond to a higher impact on the model’s predictions and hence they do not provide a faithful explanation for the model’s predictions. On the other hand, Wiegreffe and Pinter (2019) argues that there is still a possibility that attention distributions may provide a plausible explanation for the predictions. In other words, they might provide 4206 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4206–4216 c July 5 - 10, 2020. 2020 Association for Computational Linguistics a plausible reconstruction of the model’s decision making which can be understood by a human even if it is not faithful to how the model works. In this work, we begin by analyzing why attention distributions may not faithfully explain the model’s predictions."
2020.acl-main.387,petrov-etal-2012-universal,0,\N,Missing
2020.emnlp-main.260,P17-1080,0,0.0697984,"Missing"
2020.emnlp-main.260,W19-4828,0,0.044668,"Missing"
2020.emnlp-main.260,P84-1044,0,0.329738,"Missing"
2020.emnlp-main.260,2020.repl4nlp-1.18,0,0.0290675,"layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning. 1 In the other major direction, these large Transformer-based models have been down-sized to be more time and space efficient. Different methods for down-sizing have been studied such as pruning (McCarley, 2019; Gordon et al., 2020; Sajjad et al., 2020), distillation (Sanh et al., 2019; Liu et al., 2019; Jiao et al., 2019), weight quantization (Zafrir et al., 2019; Shen et al., 2019), and weight factorization and parameter sharing (Lan et al., 2019). Pruning techniques have been particularly successful in reinforcing the folk-lore that these models are highly over-parameterized. These pruning methods prune parameters based on magnitude (Gordon et al., 2020), importance (McCarley, 2019) or layer-wise (Sajjad et al., 2020). Introduction The acclaimed success of Transformer-based models across NLP tasks has been followed b"
2020.emnlp-main.260,D19-1448,0,0.0370467,"Missing"
2020.emnlp-main.260,P19-1580,0,0.0640426,"ce (McCarley, 2019) or layer-wise (Sajjad et al., 2020). Introduction The acclaimed success of Transformer-based models across NLP tasks has been followed by two important directions of research. In the first direction, interpretability studies aim to understand how these models work. Given that multi-headed attention is an important feature of these models, researchers have focused on attention heads as the units of interpretation. These studies comment on the role of each attention head and the relation between a head’s position and its significance (Clark et al., 2019; Michel et al., 2019; Voita et al., 2019b,a; Liu et al., 2019; Belinkov et al., 2017). These studies show that certain heads are more important based In this paper, we straddle these two directions of work by asking the following question: Can we randomly prune heads, thus completely ignoring any notion of importance of heads? To answer this, we systematically study the effect of randomly pruning specific subsets of attention heads on the accuracy on different tasks. Across experiments, we modify the random sampling to vary the percentage of heads pruned and their location in the network (components and layers). We evaluate these ex"
2020.emnlp-main.260,D19-1445,0,0.0362725,"Missing"
2020.emnlp-main.260,W18-5446,0,0.0268924,"Missing"
2020.emnlp-main.261,2021.ccl-1.108,0,0.219929,"Missing"
2020.findings-emnlp.445,C16-1047,0,0.384219,"as IndicCorp, contains a total of 8.8 billion tokens across 11 major Indian languages and English. The articles in IndicCorp are primarily sourced from news crawls. Using IndicCorp, we first train and evaluate word embeddings for each of the 11 languages. Given the morphological richness of Indian languages we train FastText word embeddings which are known to be more effective for such languages. To evaluate these embeddings we curate a benchmark comprising of word similarity and analogy tasks (Akhtar et al., 2017; Grave et al., 2018), text classification tasks, sentence classification tasks (Akhtar et al., 2016; Mukku and Mamidi, 2017), and bilingual lexicon induction tasks. On most tasks, the word embeddings trained on our IndicCorp outperform similar embeddings trained on existing corpora for Indian languages. Next, we train multilingual language models for these 11 languages using the ALBERT model (Lan et al., 2020). We chose ALBERT as the base model as it is very compact and hence easier to use in downstream tasks. To evaluate these pretrained language models, we create an NLU benchmark comprising of the following tasks: article genre classification, headline prediction, named entity recognition"
2020.findings-emnlp.445,W17-0811,0,0.382274,"uation benchmark comprising of various NLU tasks. Our monolingual corpora, collectively referred to as IndicCorp, contains a total of 8.8 billion tokens across 11 major Indian languages and English. The articles in IndicCorp are primarily sourced from news crawls. Using IndicCorp, we first train and evaluate word embeddings for each of the 11 languages. Given the morphological richness of Indian languages we train FastText word embeddings which are known to be more effective for such languages. To evaluate these embeddings we curate a benchmark comprising of word similarity and analogy tasks (Akhtar et al., 2017; Grave et al., 2018), text classification tasks, sentence classification tasks (Akhtar et al., 2016; Mukku and Mamidi, 2017), and bilingual lexicon induction tasks. On most tasks, the word embeddings trained on our IndicCorp outperform similar embeddings trained on existing corpora for Indian languages. Next, we train multilingual language models for these 11 languages using the ALBERT model (Lan et al., 2020). We chose ALBERT as the base model as it is very compact and hence easier to use in downstream tasks. To evaluate these pretrained language models, we create an NLU benchmark comprising"
2020.findings-emnlp.445,W13-3520,0,0.0267876,"ommonCrawl, also contains much less data for most Indian languages than our crawls. The CC4949 1 https://commoncrawl.org Net (Wenzek et al., 2019) and C4 (Raffel et al., 2019) projects also provide tools to process common crawl, but the extracted corpora are not provided and require a large amount of processing power. Our monolingual corpora is about 4 times larger than the corresponding OSCAR corpus and two times larger than the corresponding CC-100 corpus (Conneau et al., 2020). Word Embeddings. Word embeddings have been trained for many Indian languages using limited corpora. The Polyglot (Al-Rfou et al., 2013) and FastText (Bojanowski et al., 2017) projects provide embeddings trained on Wikipedia. FastText also provides embeddings trained on Wikipedia + CommonCrawl corpora. We show that on most evaluation tasks IndicFT outperforms existing FastText based embeddings. Pretrained Transformers. Pre-trained transformers serve as general language understanding models that can be used in a wide variety of downstream NLP tasks (Radford et al., 2019). Several transformer-based language models such as GPT (Radford, 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), etc."
2020.findings-emnlp.445,Q17-1010,0,0.291047,"data for most Indian languages than our crawls. The CC4949 1 https://commoncrawl.org Net (Wenzek et al., 2019) and C4 (Raffel et al., 2019) projects also provide tools to process common crawl, but the extracted corpora are not provided and require a large amount of processing power. Our monolingual corpora is about 4 times larger than the corresponding OSCAR corpus and two times larger than the corresponding CC-100 corpus (Conneau et al., 2020). Word Embeddings. Word embeddings have been trained for many Indian languages using limited corpora. The Polyglot (Al-Rfou et al., 2013) and FastText (Bojanowski et al., 2017) projects provide embeddings trained on Wikipedia. FastText also provides embeddings trained on Wikipedia + CommonCrawl corpora. We show that on most evaluation tasks IndicFT outperforms existing FastText based embeddings. Pretrained Transformers. Pre-trained transformers serve as general language understanding models that can be used in a wide variety of downstream NLP tasks (Radford et al., 2019). Several transformer-based language models such as GPT (Radford, 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), etc. have been proposed. All these models r"
2020.findings-emnlp.445,bojar-etal-2014-hindencorp,0,0.0913868,"h sources). 2 Related Work Text Corpora. Few organized sources of monolingual corpora exist for most Indian languages. The EMILLE/CIIL corpus (McEnery et al., 2000) was an early effort to build corpora for South Asian languages, spanning 14 languages with a total of 92 million words. Wikipedia for Indian languages is small (the largest one, Hindi, has just 40 million words). The Leipzig corpus (Goldhahn et al., 2012) contains small collections of upto 1 million sentences for news and web crawls (average 300K sentences). In addition, there are some language specific corpora for Hindi and Urdu (Bojar et al., 2014; Jawaid et al., 2014). In particular, the HindMonoCorp (Bojar et al., 2014) is one of the few larger Indian language collections (787M tokens). The CommonCrawl 1 project crawls webpages in many languages by sampling various websites. Our analysis of a processed crawl for the years 2013-2016 (Buck et al., 2014) for Indian languages revealed that most Indian languages, with the exception of Hindi, Tamil and Malayalam, have few good sentences (≥10 words) - in the order of around 50 million words. The OSCAR project (Ortiz Suarez et al., 2019), a recent processing of CommonCrawl, also contains muc"
2020.findings-emnlp.445,buck-etal-2014-n,0,0.0803568,"Missing"
2020.findings-emnlp.445,2020.tacl-1.30,0,0.0641196,"ng et al., 2019), CLUE (Chinese) (Xu et al., 2020), and FLUE (French) (Le et al., 2020) are important for tracking the efficacy of NLP models across languages. Such a benchmark is missing for Indic languages and the goal of this work is to fill this void. Datasets are available for some tasks for a few languages. The following are some of the prominent publicly available datasets2 : word similarity (Akhtar et al., 2017), word analogy (Grave et al., 2018), text classification, sentiment analysis (Akhtar et al., 2016; Mukku and Mamidi, 2017), paraphrase detection (Anand Kumar et al., 2016), QA (Clark et al., 2020; Gupta et al., 2018), discourse mode classification (Dhanwal et al., 2020), etc.. We also create datasets for some tasks, most of which span all major Indian languages. We bun2 A comprehensive list of resources Indian language NLP can be found https://github.com/AI4Bharat/indicnlp_catalog for here: Language #S #T #V I/O Punjabi (pa) 29.2 773 3.0 22 Hindi (hi) 63.1 1,860 6.5 2 Bengali (bn) 39.9 836 6.6 2 Odia (or) 6.94 107 1.4 9 Assamese (as) 1.39 32.6 0.8 8 Gujarati (gu) 41.1 719 5.7 14 Marathi (mr) 34.0 551 5.8 7 Kannada (kn) 53.3 713 11.9 14 Telugu (te) 47.9 674 9.4 8 Malayalam (ml) 50.2 72"
2020.findings-emnlp.445,2020.acl-main.747,0,0.374404,"d it is a collection of (i) existing Indian language datasets for some tasks, (ii) manual translations of some English datasets into Indian languages done as a part of this work, and (iii) new datasets that were created semi-automatically for all major Indian languages as a part of this work. These new datasets were created using external metadata (such as website/Wikipedia structure) resulting in more complex NLU tasks. Across all these tasks, we show that our embeddings are competitive or better than existing pre-trained multilingual embeddings such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020). We hope that these embeddings and evaluations benchmarks will not only be useful in driving NLP research on Indic languages, but will also help in evaluating advances in NLP over a more diverse set of languages. In summary, this paper introduces IndicNLPSuite containing the following resources for Indic NLP which will be made publicly available: • IndicCorp: Large sentence-level monolingual corpora for 11 languages from two language families (Indo-Aryan branch and Dravidian) and Indian English with an average 9-fold increase in size over OSCAR. • IndicGLUE: An evaluation benchmark containing"
2020.findings-emnlp.445,2020.lrec-1.302,0,0.0348021,"ed language models such as GPT (Radford, 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), etc. have been proposed. All these models require large amounts of monolingual corpora for training. For Indic languages, two such multilingual models are available: XLM-R (Conneau et al., 2020) and multilingual BERT (Devlin et al., 2019). However, they are trained across ~100 languages and smaller Indic language corpora. NLU Benchmarks. Benchmarks such as GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), CLUE (Chinese) (Xu et al., 2020), and FLUE (French) (Le et al., 2020) are important for tracking the efficacy of NLP models across languages. Such a benchmark is missing for Indic languages and the goal of this work is to fill this void. Datasets are available for some tasks for a few languages. The following are some of the prominent publicly available datasets2 : word similarity (Akhtar et al., 2017), word analogy (Grave et al., 2018), text classification, sentiment analysis (Akhtar et al., 2016; Mukku and Mamidi, 2017), paraphrase detection (Anand Kumar et al., 2016), QA (Clark et al., 2020; Gupta et al., 2018), discourse mode classification (Dhanwal et al.,"
2020.findings-emnlp.445,P19-1493,0,0.158456,"oss entropy loss. Cloze-style Multiple-choice QA. We feed the masked text segment as input to the model and at the output we have a softmax layer which predicts a probability distribution over the given candidates. We fine-tune the model using cross entropy loss with the target label as 1 for the correct candidate and 0 for the incorrect candidates. Cross-lingual Sentence Retrieval. No finetuning is required for this task. We compute the representation of every sentence by mean-pooling the outputs in the last hidden layer and then using cosine distance to compute similarity between sentences (Libovický et al., 2019). Additionally, we also center the sentence vectors across each language to remove language-specific bias in the vectors (Reimers and Gurevych, 2019). Winograd NLI, COPA, Paraphrase Detection: We input the sentence pair into the model as segment A and segment B. The [CLS] representation from the last layer is fed into an output layer for classification into one of the categories. News Category Classification, Discourse Mode Classification, Sentiment Analysis. We feed the representation of the [CLS] token from the last layer to a linear classifier with a softmax layer to predict a probability d"
2020.findings-emnlp.445,2021.ccl-1.108,0,0.160203,"Missing"
2020.findings-emnlp.445,2000.bcs-1.11,0,0.431026,"nce-level monolingual corpora for 11 languages from two language families (Indo-Aryan branch and Dravidian) and Indian English with an average 9-fold increase in size over OSCAR. • IndicGLUE: An evaluation benchmark containing a variety of NLU tasks. • IndicFT and IndicBERT: FastText-based word embeddings (11 languages) and ALBERT-based language models (12 languages) trained on IndicCorp. The IndicBERT embeddings are multilingual (includes Indian English sources). 2 Related Work Text Corpora. Few organized sources of monolingual corpora exist for most Indian languages. The EMILLE/CIIL corpus (McEnery et al., 2000) was an early effort to build corpora for South Asian languages, spanning 14 languages with a total of 92 million words. Wikipedia for Indian languages is small (the largest one, Hindi, has just 40 million words). The Leipzig corpus (Goldhahn et al., 2012) contains small collections of upto 1 million sentences for news and web crawls (average 300K sentences). In addition, there are some language specific corpora for Hindi and Urdu (Bojar et al., 2014; Jawaid et al., 2014). In particular, the HindMonoCorp (Bojar et al., 2014) is one of the few larger Indian language collections (787M tokens). T"
2020.findings-emnlp.445,W17-5408,0,0.314831,"s a total of 8.8 billion tokens across 11 major Indian languages and English. The articles in IndicCorp are primarily sourced from news crawls. Using IndicCorp, we first train and evaluate word embeddings for each of the 11 languages. Given the morphological richness of Indian languages we train FastText word embeddings which are known to be more effective for such languages. To evaluate these embeddings we curate a benchmark comprising of word similarity and analogy tasks (Akhtar et al., 2017; Grave et al., 2018), text classification tasks, sentence classification tasks (Akhtar et al., 2016; Mukku and Mamidi, 2017), and bilingual lexicon induction tasks. On most tasks, the word embeddings trained on our IndicCorp outperform similar embeddings trained on existing corpora for Indian languages. Next, we train multilingual language models for these 11 languages using the ALBERT model (Lan et al., 2020). We chose ALBERT as the base model as it is very compact and hence easier to use in downstream tasks. To evaluate these pretrained language models, we create an NLU benchmark comprising of the following tasks: article genre classification, headline prediction, named entity recognition, Wikipedia section-title"
2020.findings-emnlp.445,P17-1178,0,0.0232335,"lingual Sentence Retrieval (#English to Indian language parallel sentences) 5,169 5,522 752 6,463 5,760 5,049 4,886 5,637 39,238 Table 2: IndicGLUE Datasets’ Statistics. The first four datasets have been created as part of this project. correct candidates from entities that occur in the same article and have the same type as the correct entity. The type of an entity is taken from Wikidata. This task is similar to the one proposed by Petroni et al. (2019) for English, and aims to check if language models can be used as knowledge bases. Named Entity Recognition. We use the WikiAnn NER dataset7 (Pan et al., 2017) which contains NER data for 282 languages. This dataset is created from Wikipedia by utilizing cross language links to propagate English named entity labels to other languages. We consider the following coarsegrained labels in this dataset: Person (PER), Organisation (ORG) and Location (LOC). Cross-lingual Sentence Retrieval. Given a sentence in English, the task is to retrieve its translation from a set of candidate sentences in an Indian language. We use the CVIT-Mann Ki Baat dataset8 (Siripragrada et al., 2020) for this task. Winograd NLI (WNLI). The WNLI task (Levesque et al., 2011) is pa"
2020.findings-emnlp.445,D14-1162,0,0.0892077,"guage on IndicCorp using FastText (Bojanowski et al., 2017). Since Indian languages are morphologically rich, we chose FastText, which is capable of integrating subword information by using character n-gram embeddings during training. We train skipgram models for 10 epochs with a window size of 5, minimum token count of 5 and 10 negative examples sampled for each instance. We chose these hyper-parameters based on suggestions by Grave et al. (2018). Based on previously published results, we expect FastText to be better than word-level algorithms like word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014) for morphologically rich languages. 5.2 hi BBC Articles IITP+ Movie IITP Product bn gu ml mr ta te FT-W FT-WC IndicFT 72.29 41.61 58.32 67.44 44.52 57.17 77.02 45.81 61.57 Soham Articles 62.79 64.78 71.82 iNLTK Headlines 81.94 86.35 83.06 90.88 84.07 83.65 81.65 89.09 90.74 95.87 91.40 95.37 ACTSA 46.03 42.51 52.58 Average 69.25 68.32 75.80 Table 4: Text classification accuracy on public datasets. On average, IndicFT embeddings outperform the baseline embeddings. We train FastText word embeddings for each language using IndicCorp, and evaluate their quality on: (a) word similarity, (b) word a"
2020.findings-emnlp.445,N18-1202,0,0.0844237,"Missing"
2020.findings-emnlp.445,D19-1250,0,0.0184964,"431 109,508 8,687 6,295 39,708 108,579 28,854 te 24,000 ml ta total 6,000 11,700 125,630 63,415 100,000 74,767 880,311 41,338 81,627 138,888 186,423 787,462 Cross-lingual Sentence Retrieval (#English to Indian language parallel sentences) 5,169 5,522 752 6,463 5,760 5,049 4,886 5,637 39,238 Table 2: IndicGLUE Datasets’ Statistics. The first four datasets have been created as part of this project. correct candidates from entities that occur in the same article and have the same type as the correct entity. The type of an entity is taken from Wikidata. This task is similar to the one proposed by Petroni et al. (2019) for English, and aims to check if language models can be used as knowledge bases. Named Entity Recognition. We use the WikiAnn NER dataset7 (Pan et al., 2017) which contains NER data for 282 languages. This dataset is created from Wikipedia by utilizing cross language links to propagate English named entity labels to other languages. We consider the following coarsegrained labels in this dataset: Person (PER), Organisation (ORG) and Location (LOC). Cross-lingual Sentence Retrieval. Given a sentence in English, the task is to retrieve its translation from a set of candidate sentences in an Ind"
2020.findings-emnlp.445,D19-1410,0,0.0206159,"which predicts a probability distribution over the given candidates. We fine-tune the model using cross entropy loss with the target label as 1 for the correct candidate and 0 for the incorrect candidates. Cross-lingual Sentence Retrieval. No finetuning is required for this task. We compute the representation of every sentence by mean-pooling the outputs in the last hidden layer and then using cosine distance to compute similarity between sentences (Libovický et al., 2019). Additionally, we also center the sentence vectors across each language to remove language-specific bias in the vectors (Reimers and Gurevych, 2019). Winograd NLI, COPA, Paraphrase Detection: We input the sentence pair into the model as segment A and segment B. The [CLS] representation from the last layer is fed into an output layer for classification into one of the categories. News Category Classification, Discourse Mode Classification, Sentiment Analysis. We feed the representation of the [CLS] token from the last layer to a linear classifier with a softmax layer to predict a probability distribution over the categories. We fine-tune the model using multi-class cross entropy loss. 6.3 Evaluation We summarize the main observations from"
2020.findings-emnlp.445,2020.lrec-1.462,0,0.101151,"n be used as knowledge bases. Named Entity Recognition. We use the WikiAnn NER dataset7 (Pan et al., 2017) which contains NER data for 282 languages. This dataset is created from Wikipedia by utilizing cross language links to propagate English named entity labels to other languages. We consider the following coarsegrained labels in this dataset: Person (PER), Organisation (ORG) and Location (LOC). Cross-lingual Sentence Retrieval. Given a sentence in English, the task is to retrieve its translation from a set of candidate sentences in an Indian language. We use the CVIT-Mann Ki Baat dataset8 (Siripragrada et al., 2020) for this task. Winograd NLI (WNLI). The WNLI task (Levesque et al., 2011) is part of the GLUE benchmark. Each example in the dataset consists of a pair of sentences where the second sentence is constructed from the first sentence by replacing an ambiguous pronoun with a possible referent within the sentence. The task is to predict if the second sentence is entailed by the original sentence. We manually translated this dataset to 3 Indic languages (hi, mr, gu) with the help of skilled bilingual speakers. The annotators were paid 3 cents per word and the translations were then verified by an ex"
2020.findings-emnlp.445,W18-5446,0,0.0673258,"Missing"
2020.nlp4convai-1.2,N18-1202,0,0.0841387,"erent responses (Shao et al., 2017). The primary reason for this is that, unlike humans, such systems do not have any access to background knowledge about the topic of conversation. For example, while chatting about movies, we use our background knowledge about the movie in the form of plot details, reviews, and comments that we might have read. To enrich such neural conversation systems, some recent works (Moghe et al., 2018; Dinan et al., 2019; Zhou et al., 2018) incorporate external knowledge in the form of documents which are relevant to the current conversation. For example, Moghe et al. (2018) released a dataset containing conversations about movies where every alternate utterance is extracted from a background document about the movie. This background document contains plot details, reviews, and Reddit comments about the movie. The focus thus shifts from sequence generation to identifying relevant snippets from the background document and modifying them suitably to form an appropriate response given the current conversational context. Intuitively, any model for this task should exploit semantic, structural and sequential information from the conversation context and the background"
2020.nlp4convai-1.2,P19-1356,0,0.0403832,"dependency graphs to incorporate structural information for semantic role labelling (Marcheggiani and Titov, 2017), neural machine translation (Bastings et al., 2017) and entity relation information in question answering (De Cao et al., 2019) and temporal information for neural dating of documents (Vashishth et al., 2018). There have been advances in learning deep contextualized word representations (Peters et al., 2018b; Devlin et al., 2019) with a hope that such representations will implicitly learn structural and relational information with the interaction between words at multiple layers (Jawahar et al., 2019; Peu∈N (v) (1) where σ is the activation function, g(u,v) ∈ R is the predicted importance of the edge (u, v) and hv ∈ Rm is node, v’s embedding. Wdir(u,v) ∈ {Win , Wout , Wself } depending on the direction dir(u, v) and Win , Wself and Wout ∈ Rm∗m . The importance of an edge g(u,v) is determined by an edge gating mechanism w.r.t. the node of interest, u as given below:  g(u,v) = sigmoid hu . Wdir(u,v) + bL(u,v) (2) In summary, a GCN computes new representation of a node u by aggregating information from it’s neighborhood N (v). When k=0, the aggregation happens only from immediate neighbors,"
2020.nlp4convai-1.2,D18-1179,0,0.0486374,"ly a few of the edges for the background knowledge at the bottom. The edge in blue corresponds to the co-reference edge, the edges in green are dependency edges and the edge in red is the entity edge. 4. Given the recent claims that BERT captures syntactic information, does it help to explicitly enrich it with syntactic information using GCNs? To systematically investigate such questions we propose a simple plug-and-play SemanticsSequences-Structures (SSS) framework which allows us to combine different semantic representations (GloVe (Pennington et al., 2014), BERT(Devlin et al., 2018), ELMo (Peters et al., 2018a)) with different structural priors (dependency graphs, co-reference graphs, etc.). It also allows us to use different ways of combining structural and sequential information, e.g., LSTM first followed by GCN or vice versa, or both in parallel. Using this framework we perform a series of experiments on the Holl-E dataset and make some interesting observations. First, we observe that the conventional adaptation of GCNs for NLP tasks, where contextualized embeddings obtained through LSTMs are fed as input to a GCN, exhibits poor performance. To overcome this, we propose some simple alternatives"
2020.nlp4convai-1.2,W19-4302,0,0.0393348,"Missing"
2020.nlp4convai-1.2,N18-2078,0,0.0445897,"ompetition (Ram et al., 2017) have benefited by using several knowledge resources. This external knowledge can be in the form of knowledge graphs or unstructured texts such as documents. Many NLP systems including conversation systems use RNNs as their basic building block which typically captures n-gram or sequential information. Adding structural information through treebased structures (Tai et al., 2015) or graph-based structures (Marcheggiani and Titov, 2017) on top of this has shown improved results on several tasks. For example, GCNs have been used to improve neural machine translation (Marcheggiani et al., 2018) by exploiting the semantic structure of the source sentence. Similarly, GCNs have been used with dependency graphs to incorporate structural information for semantic role labelling (Marcheggiani and Titov, 2017), neural machine translation (Bastings et al., 2017) and entity relation information in question answering (De Cao et al., 2019) and temporal information for neural dating of documents (Vashishth et al., 2018). There have been advances in learning deep contextualized word representations (Peters et al., 2018b; Devlin et al., 2019) with a hope that such representations will implicitly l"
2020.nlp4convai-1.2,D17-1159,0,0.0791649,"at conversations (Lowe et al., 2015; Ghazvininejad et al., 2018; Moghe et al., 2018; Dinan et al., 2019). Even the teams participating in the annual Alexa Prize competition (Ram et al., 2017) have benefited by using several knowledge resources. This external knowledge can be in the form of knowledge graphs or unstructured texts such as documents. Many NLP systems including conversation systems use RNNs as their basic building block which typically captures n-gram or sequential information. Adding structural information through treebased structures (Tai et al., 2015) or graph-based structures (Marcheggiani and Titov, 2017) on top of this has shown improved results on several tasks. For example, GCNs have been used to improve neural machine translation (Marcheggiani et al., 2018) by exploiting the semantic structure of the source sentence. Similarly, GCNs have been used with dependency graphs to incorporate structural information for semantic role labelling (Marcheggiani and Titov, 2017), neural machine translation (Bastings et al., 2017) and entity relation information in question answering (De Cao et al., 2019) and temporal information for neural dating of documents (Vashishth et al., 2018). There have been ad"
2020.nlp4convai-1.2,P17-1099,0,0.142599,"sks, we model the above probability using a neural network comprising of an encoder, a decoder, an attention mechanism, and a copy mechanism. The copy mechanism essentially helps to directly copy words from the document D instead of predicting them from the vocabulary. Our main contribution is in improving the document encoder where we use a plug-and-play framework to combine semantic, structural, and sequential information from different sources. This enriched document encoder could be coupled with any existing model. In this work, we couple it with the popular Get To The Point (GTTP) model (See et al., 2017) as used by the authors of the Holl-E dataset. In other words, we use the same attention mechanism, decoder, and copy mechanism as GTTP but augment it with an enriched document encoder. Below, we first describe the document encoder and then very briefly describe the other components of the model. We also refer the reader to the supplementary material for more details. 4.1 (k+1) hi (k) (k) = ReLU (hi Wself + X  g conv(N ) N ∈G (3) This formulation is advantageous over having |R| different GCNs as it can extract information from multi-hop pathways and can use information across different graphs"
2020.nlp4convai-1.2,D18-1255,1,0.863778,"core over the baseline. Introduction Neural conversation systems that treat dialogue response generation as a sequence generation task (Vinyals and Le, 2015) often produce generic and incoherent responses (Shao et al., 2017). The primary reason for this is that, unlike humans, such systems do not have any access to background knowledge about the topic of conversation. For example, while chatting about movies, we use our background knowledge about the movie in the form of plot details, reviews, and comments that we might have read. To enrich such neural conversation systems, some recent works (Moghe et al., 2018; Dinan et al., 2019; Zhou et al., 2018) incorporate external knowledge in the form of documents which are relevant to the current conversation. For example, Moghe et al. (2018) released a dataset containing conversations about movies where every alternate utterance is extracted from a background document about the movie. This background document contains plot details, reviews, and Reddit comments about the movie. The focus thus shifts from sequence generation to identifying relevant snippets from the background document and modifying them suitably to form an appropriate response given the cur"
2020.nlp4convai-1.2,D14-1162,0,0.0868913,"Missing"
2020.nlp4convai-1.2,D17-1235,0,0.0633348,"Missing"
2020.nlp4convai-1.2,P18-1149,0,0.0316079,"ly a few of the edges for the background knowledge at the bottom. The edge in blue corresponds to the co-reference edge, the edges in green are dependency edges and the edge in red is the entity edge. 4. Given the recent claims that BERT captures syntactic information, does it help to explicitly enrich it with syntactic information using GCNs? To systematically investigate such questions we propose a simple plug-and-play SemanticsSequences-Structures (SSS) framework which allows us to combine different semantic representations (GloVe (Pennington et al., 2014), BERT(Devlin et al., 2018), ELMo (Peters et al., 2018a)) with different structural priors (dependency graphs, co-reference graphs, etc.). It also allows us to use different ways of combining structural and sequential information, e.g., LSTM first followed by GCN or vice versa, or both in parallel. Using this framework we perform a series of experiments on the Holl-E dataset and make some interesting observations. First, we observe that the conventional adaptation of GCNs for NLP tasks, where contextualized embeddings obtained through LSTMs are fed as input to a GCN, exhibits poor performance. To overcome this, we propose some simple alternatives"
2020.nlp4convai-1.2,D18-1076,0,0.0649626,"Missing"
2020.tacl-1.52,N19-1423,0,0.0242128,"latent variable hierarchical recurrent encoder decoder (VHRED) model (Serban et al., 2017). The weight matrices, M, N, are later finetuned for the task of dialogue response evaluation. RUBER: (Tao et al., 2018) introduced an unreferenced evaluation model consisting of GRU encoders (Chung et al., 2014) to measure the relatedness between the dialogue context and a given response. The authors train the model on Chinese dialogue data with the hinge loss objective. In the last two years, considerable success in NLP has been driven by large pretrained transformerbased models (Radford et al., 2019; Devlin et al., 2019; Zhang et al., 2019). These models are typically trained with a language model objective and leverage large amounts of unlabeled data. However, none of the trained metrics discussed in the previous section leverage pretraining on largescale dialogue corpora. With the hope that such pretraining should help dialog evaluation models also, we introduce DEB (Dialog Evaluation using BERT) which is trained using a masked language model objective (similar to BERT) and a modified next response prediction objective. We set up the the task of next response prediction as one of identifying whether the gi"
2020.tacl-1.52,P16-2044,0,0.0272535,"Missing"
2020.tacl-1.52,W05-0909,0,0.157173,"words per utterance # of contexts with 5 relevant responses # of contexts with 5 adv. irrelevant responses Avg. # of words per relevant response Avg. # of words per irrelevant response 19,071 3.31 45.32 13.55 19,071 11,429 10.13 13.8 3.1 Untrained Metrics Untrained metrics can be further sub-classified into (i) n-gram based, (ii) word embedding based, and (iii) contextualized embedding based metrics. N -gram Based: N -gram based metrics score a candidate response based on the amount of n-gram overlap it has with a given reference. BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) are among the most commonly adopted n-gram based metrics to evaluate dialogue systems. BLEU is calculated using n-gram precision scores between the candidate response and the reference. ROUGE-L (Lin, 2004) is based on the F-measure of the longest common subsequence between the candidate and reference responses. METEOR (Banerjee and Lavie, 2005) relaxes the exact match criteria by including word stems, synonyms, and paraphrases. More recently, Galley et al. (2015) proposed deltaBLEU, which takes in multiple references and rewards n-gram matches with positive references and penalizes the matche"
2020.tacl-1.52,P18-1012,0,0.0608451,"Missing"
2020.tacl-1.52,D17-1230,0,0.0465326,"Missing"
2020.tacl-1.52,N19-1125,0,0.14732,"erated by sampling random utterances from other contexts, but such examples typically do not have any overlap with the context and hence are easier for the model to distinguish from relevant responses (as we will show in our results later). We refer to the randomly sampled responses as random negatives. Some efforts have been made to build dialog datasets with multiple relevant responses (i.e., multiple references), but these datasets are either very small (1000 contexts) (Moghe et al., 2018; Gupta et al., 2019) or automatically constructed from Reddit conversations, hence, potentially noisy (Gao et al., 2019). Further, these datasets do not have any carefully crafted adversarial irrelevant responses. We define an adversarial irrelevant response as one that has a significant word overlap with the context but is still an irrelevant response (hence harder to identify than randomly selected irrelevant examples, which may not have any relation to the context). To overcome this limitation of existing datasets, we propose a large scale multi-reference dataset, DailyDialog++, which is an extension of the DailyDialog dataset. In particular, for each of the 19K contexts derived from DailyDialog, we collect"
2020.tacl-1.52,W04-1013,0,0.0316883,"s per context Avg. # of words per utterance # of contexts with 5 relevant responses # of contexts with 5 adv. irrelevant responses Avg. # of words per relevant response Avg. # of words per irrelevant response 19,071 3.31 45.32 13.55 19,071 11,429 10.13 13.8 3.1 Untrained Metrics Untrained metrics can be further sub-classified into (i) n-gram based, (ii) word embedding based, and (iii) contextualized embedding based metrics. N -gram Based: N -gram based metrics score a candidate response based on the amount of n-gram overlap it has with a given reference. BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) are among the most commonly adopted n-gram based metrics to evaluate dialogue systems. BLEU is calculated using n-gram precision scores between the candidate response and the reference. ROUGE-L (Lin, 2004) is based on the F-measure of the longest common subsequence between the candidate and reference responses. METEOR (Banerjee and Lavie, 2005) relaxes the exact match criteria by including word stems, synonyms, and paraphrases. More recently, Galley et al. (2015) proposed deltaBLEU, which takes in multiple references and rewards n-gram matches with posit"
2020.tacl-1.52,W19-2310,0,0.484741,"uthors worked equally towards the project. 810 Transactions of the Association for Computational Linguistics, vol. 8, pp. 810–827, 2020. https://doi.org/10.1162/tacl a 00347 Action Editor: Xiaojun Wan. Submission batch: 6/2020; Revision batch: 8/2020; Published 12/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  Given the shortcomings of context-agnostic n-gram and embedding based metrics, the focus has now shifted to building neural network-based, trainable dialogue evaluation models (Lowe et al., 2017; Tao et al., 2018; Shimanaka et al., 2019; Ghazarian et al., 2019). Such models are trained to identify whether a given response can be considered as a valid continuation of the given context or not. In other words, the model should (i) assign a high score to all relevant responses no matter how diverse they are and (ii) assign a low score to all irrelevant responses, preferably with a clear margin of separation from relevant responses. Although there exist several opendomain dialogue datasets (Forsythand and Martell, 2007; Tiedemann, 2012; Ritter et al., 2010; Li et al., 2017b) that are used for training dialogue response generation systems, they are not su"
2020.tacl-1.52,W19-5944,0,0.0507698,"Missing"
2020.tacl-1.52,D18-1255,1,0.894528,"Missing"
2020.tacl-1.52,N18-1162,0,0.0178409,"ely lower on it for all the models. The other evaluation models and metrics cannot be compared on PersonaChat and Twitter without additional reference responses, since the available single reference in these datasets is being evaluated. On the multi-reference test set of Holl-E, however, we find that their performance is lower than the three unreferenced models. VHRED: Latent Variable HRED (VHRED) (Serban et al., 2017) includes a latent variable at the decoder, and is trained by maximizing a variational lower-bound on the log-likelihood. VHCR: Variational Hierarchical Conversation RNN (VHCR) (Park et al., 2018) further extends VHRED by drawing a prior encoding for each conversation. DialoGPT small: Zhang et al. (2020b) pretrained GPT-2-like (Radford et al., 2019) transformer models on 147M conversations extracted from Reddit comments. The small version contains 12 layers and 768 hidden dimensions. DialoGPT medium: The medium version of DialogGPT contains 24 layers and 1024 hidden dimensions. For the RNN-based models (HRED, VHRED, VHCR), we use a single-layer bidirectional encoder and single-layer decoder each with a hidden size of 1024. We pretrain the RNN-based 822 Model Pearson Spearman Response l"
2020.tacl-1.52,N10-1020,0,0.139036,"Missing"
2020.tacl-1.52,W12-2018,0,0.554784,"its performance again drops substantially when 1 Introduction Open-domain conversational systems are increasingly in demand for several applications ranging from personal digital assistants to entertainers for recreation. While several automated dialogue agents such as Siri, Alexa, Cortana, and Google Assistant have been built and deployed, there is no good automatic evaluation metric to measure the quality of their conversations. Researchers have usually adopted n-gram based metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004) or embedding based metrics (Forgues et al., 2014; Rus and Lintean, 2012; Zhang et al., 2020a) to compare the model’s response with a single reference. These metrics assume that a valid response should be semantically or lexically similar to the reference without taking the context of the conversation into consideration. However, in open domain conversations, a given context can have a wide range of possible responses that may be lexically and semantically very different from each other. For example, the context, ‘‘I like dancing and swimming, what about you?’’ can be responded to with ‘‘I paint in my free time’’ or ‘‘I do not have time for hobbies right now’’, bo"
2020.tacl-1.52,P18-1205,0,0.214897,"hat even large-scale pretrained models are not robust to adversarial examples. 2 Proposed Dataset Our goal was to build a dataset with manually created multiple relevant and adversarial irrelevant responses. For this, we wanted to start with an existing base dataset that already has one 811 relevant response for every context, and then extend it to include multiple responses. For the base dataset, we considered several popular datasets such as Twitter (Ritter et al., 2010), Reddit (Henderson et al., 2019), Open Subtitles (Tiedemann, 2012), NPS Chat (Forsythand and Martell, 2007), PersonaChat (Zhang et al., 2018), and DailyDialog (Li et al., 2017b). Of these, Twitter and Reddit are generally considered noisy, so we chose not to use either of them as the base dataset. Similarly, Open Subtitles and NPS Chat did not have speaker-aligned utterances, and hence were not suitable for our purposes. We found that the DailyDiaog dataset was clean, humanwritten, readily available, and covered a diverse set of generic topics such as ordinary life, school life, tourism, attitude & emotion, relationship, health, work, politics, culture & education, and finance. It contains a total of 13K conversations with an avera"
2020.tacl-1.52,2020.acl-demos.30,0,0.148982,"Missing"
2020.tacl-1.52,P19-1139,0,0.057524,"Missing"
2021.emnlp-main.575,W07-0718,0,0.238121,"Missing"
2021.emnlp-main.575,E06-1032,0,0.110048,"Some of the related work, particularly the relevant datasets, human evaluation criteria, and automatic metrics were already discussed earlier and hence not covered again here. We refer the readers to two recent surveys (Sai et al., 2020b; Çelikyilmaz et al., 2020) for a detailed overview of automatic evaluation metrics as well as related work on criticising the use of automatic evaluation metrics. We mention a few such important works here. BLEU is one of the most widely analysed metric with several studies showing that it does not correlate well with human judgements for machine translation (Callison-Burch et al., 2006). This issue of poor correlations of metrics with human judge- Acknowledgements ments has been reported on not just BLEU, but also on various other metrics, across several NLG We thank the annotators for helping us evaluate and tasks including Question Generation (Nema and annotate the perturbations. We thank the anonyKhapra, 2018), Data-to-Text generation (Dhingra mous EMNLP-21 reviewers whose comments and et al., 2019), Dialogue generation (Liu et al., 2016), feedback helped enhance the paper. We thank the and Summarisation (Kryscinski et al., 2019). Apart Google India Ph.D. Fellowship Progr"
2021.emnlp-main.575,W19-5356,0,0.118859,"e of the metric Further, let fe (p) be the score assigned by a given should not drop. The invariant and fluency-based 7222 Automatic Evaluation Metrics Task-specific (Context Dependent metrics) Task-agnostic (Context Free metrics) Word Based BLEU METEOR (Banerjee and Lavie, 2005) ROUGE (Lin, 2004) TER (Snover et al., 2006) CIDEr (Vedantam et al., 2015) Character Based chrF++ (Popovic, 2017) Pretrained (Static) Embeddings BERT Embeddings Greedy Matching (Rus and Lintean, 2012) Embedding Average (Landauer and Dumais, 1997) Vector Extrema (Forgues and Pineau, 2014) SMS (Clark et al., 2019) WMDo (Chow et al., 2019) MoverScore (Zhao et al., 2019) GRUEN (Zhu and Bhat, 2020) BERTScore (Zhang et al., 2020) End-to-End Trained BLEURT (Sellam et al., 2020) AS SUPERT (Gao et al., 2020) BLANC (Vasilyev et al., 2020) IC SPICE (Anderson et al., 2016) TIGEr (Jiang et al., 2019) VilBERTScore (Lee et al., 2020) QG Q-metrics (Nema and Khapra, 2018) DG ADEM (Lowe et al., 2017) RUBER (Tao et al., 2018) DEB (Sai et al., 2020a) D2T PARENT (Dhingra et al., 2019) Figure 2: Metrics analysed in this study templates are common for all the tasks considered in this work. Table 4 shows sample perturbations generated by each of th"
2021.emnlp-main.575,P19-1264,0,0.133072,"variant templates the score of the metric Further, let fe (p) be the score assigned by a given should not drop. The invariant and fluency-based 7222 Automatic Evaluation Metrics Task-specific (Context Dependent metrics) Task-agnostic (Context Free metrics) Word Based BLEU METEOR (Banerjee and Lavie, 2005) ROUGE (Lin, 2004) TER (Snover et al., 2006) CIDEr (Vedantam et al., 2015) Character Based chrF++ (Popovic, 2017) Pretrained (Static) Embeddings BERT Embeddings Greedy Matching (Rus and Lintean, 2012) Embedding Average (Landauer and Dumais, 1997) Vector Extrema (Forgues and Pineau, 2014) SMS (Clark et al., 2019) WMDo (Chow et al., 2019) MoverScore (Zhao et al., 2019) GRUEN (Zhu and Bhat, 2020) BERTScore (Zhang et al., 2020) End-to-End Trained BLEURT (Sellam et al., 2020) AS SUPERT (Gao et al., 2020) BLANC (Vasilyev et al., 2020) IC SPICE (Anderson et al., 2016) TIGEr (Jiang et al., 2019) VilBERTScore (Lee et al., 2020) QG Q-metrics (Nema and Khapra, 2018) DG ADEM (Lowe et al., 2017) RUBER (Tao et al., 2018) DEB (Sai et al., 2020a) D2T PARENT (Dhingra et al., 2019) Figure 2: Metrics analysed in this study templates are common for all the tasks considered in this work. Table 4 shows sample perturbation"
2021.emnlp-main.575,D18-1429,1,0.912356,"provided by Fabbri et al. (2020). are not robust to such perturbations (i.e., they do 3 Note that all of the datasets mentioned in Table 3 were not really evaluate the desired criteria). Overall, we collected using well established methods to ensure that the believe that the proposed templates provide a bet- annotations were of high quality. Some of these datasets do not explicitly report the Inter Annotator Agreement (IAA) ter framework for a more fine-grained evaluation scores whereas others (Fabbri et al., 2020; Castro Ferreira of automatic evaluation metrics which goes much et al., 2020; Nema and Khapra, 2018) report a good IAA score beyond computing correlations with human scores. ranging from 0.63-0.71. 7220 Task Machine Translation Question Generation Abstractive Summarization Dialogue Generation Image Captioning Data to Text Generation All above tasks Criteria Adequacy: The generated translation should adequately represent all the information present in the reference. Relevance: Is the question related to the source material they are based upon. Answerability: Is the generated question answerable given the context. Informativeness: The summary should convey the key points of the text. Non-redun"
2021.emnlp-main.575,E17-2008,0,0.0134512,"etup Datasets. For the coarse-grained evaluation, we use the datasets containing human judgements as described earlier in Table 3 in Section 2. For the fine-grained evaluation, we use datasets containing multiple ground truth references which can then be perturbed using our templates. For MT, we use the expanded version of newstest2017 Chinese to English dataset (Hassan et al., 2018) which contains two references for each sentence. For QG, we use the SQuAD dataset (Rajpurkar et al., 2016) which contains multiple questions for each passage. For AS, we use the curated personal narrative corpus (Ouyang et al., 2017). For DG, we use DailyDialog++ (Sai et al., 2020a) which contains two-speaker conversations on generic topics. For IC, we use the COCO component of the Composite dataset containing 5 reference captions for each image (Aditya et al., 2015). Lastly, for D2T, we use the Triples-to-Text data of the WebNLG 2020 challenge dataset (Castro Ferreira et al., 2020). We first do a coarse grained evaluation of several Applying perturbations. We take the reference metrics by computing their correlations with the scores assigned by humans for multiple criteria. sentences from the above task-specific datasets"
2021.emnlp-main.575,P02-1040,0,0.119211,"niversity 4 AI4Bharat {ananya,devsheth,miteshk}@cse.iitm.ac.in, dixittanay@gmail.com , sm7582@nyu.edu Abstract 2018; Sai et al., 2019). One reliable way of evaluating NLG systems is to collect human judgements. Natural Language Generation (NLG) evaluaHowever, this is a time consuming and expensive tion is a multifaceted task requiring assessprocess (Freitag et al., 2021; Deriu et al., 2019; ment of multiple desirable criteria, e.g., fluHowcroft et al., 2020). Hence, automatic evaluaency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing tion metrics such as BLEU (Papineni et al., 2002) datasets for 6 NLG tasks, we observe that the which are quicker to compute have become popular, human evaluation scores on these multiple cridespite being less reliable (Callison-Burch et al., teria are often not correlated. For example, 2006; Reiter, 2018). there is a very low correlation between human The survey by Sai et al. (2020b) shows that more scores on fluency and data coverage for the than 35 automatic evaluation metrics have been task of structured data to text generation. This suggests that the current recipe of proposing proposed for NLG since 2014, however, there is no new autom"
2021.emnlp-main.575,W17-4770,0,0.0446998,"criteria although they modoriginal output and pˆtc be the output obtained by ap- ify the sentences. For perturbations resulting from plying the perturbation template t for the criteria c. such invariant templates the score of the metric Further, let fe (p) be the score assigned by a given should not drop. The invariant and fluency-based 7222 Automatic Evaluation Metrics Task-specific (Context Dependent metrics) Task-agnostic (Context Free metrics) Word Based BLEU METEOR (Banerjee and Lavie, 2005) ROUGE (Lin, 2004) TER (Snover et al., 2006) CIDEr (Vedantam et al., 2015) Character Based chrF++ (Popovic, 2017) Pretrained (Static) Embeddings BERT Embeddings Greedy Matching (Rus and Lintean, 2012) Embedding Average (Landauer and Dumais, 1997) Vector Extrema (Forgues and Pineau, 2014) SMS (Clark et al., 2019) WMDo (Chow et al., 2019) MoverScore (Zhao et al., 2019) GRUEN (Zhu and Bhat, 2020) BERTScore (Zhang et al., 2020) End-to-End Trained BLEURT (Sellam et al., 2020) AS SUPERT (Gao et al., 2020) BLANC (Vasilyev et al., 2020) IC SPICE (Anderson et al., 2016) TIGEr (Jiang et al., 2019) VilBERTScore (Lee et al., 2020) QG Q-metrics (Nema and Khapra, 2018) DG ADEM (Lowe et al., 2017) RUBER (Tao et al., 20"
2021.emnlp-main.575,J18-3002,0,0.015203,"a time consuming and expensive tion is a multifaceted task requiring assessprocess (Freitag et al., 2021; Deriu et al., 2019; ment of multiple desirable criteria, e.g., fluHowcroft et al., 2020). Hence, automatic evaluaency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing tion metrics such as BLEU (Papineni et al., 2002) datasets for 6 NLG tasks, we observe that the which are quicker to compute have become popular, human evaluation scores on these multiple cridespite being less reliable (Callison-Burch et al., teria are often not correlated. For example, 2006; Reiter, 2018). there is a very low correlation between human The survey by Sai et al. (2020b) shows that more scores on fluency and data coverage for the than 35 automatic evaluation metrics have been task of structured data to text generation. This suggests that the current recipe of proposing proposed for NLG since 2014, however, there is no new automatic evaluation metrics for NLG by careful evaluation of the ability of such metrics to showing that they correlate well with scores asassess the quality of the output of an NLG system signed by humans for a single criteria (overon multiple desired criteria."
2021.emnlp-main.575,2020.acl-main.442,0,0.316265,"e clubbed together and evaluated using a single score assigned by an automatic evaluation metric. Second, none of the automatic evaluation metrics have a high correlation with human scores for any of the desired criteria for a given task. The above results highlight a lacunae in the evaluation of automatic evaluation metrics wherein their ability to assess the output on multiple criteria is not evaluated. In this work, we propose a flexible framework which allows a systematic evaluation of the capabilities of an automatic evaluation metric. In particular, we propose CheckList-style templates (Ribeiro et al., 2020) which evaluate the robustness of the metrics to certain perturbations targeting specific criteria. We illustrate this idea 2.1 Correlations between different criteria with an example in Table 1. In row 2 of Table 1 We use existing publicly available datasets containa gold standard output is perturbed by changing named entities, thereby affecting its factual correct- ing human judgement scores on multiple criteria ness which is important for data-to-text generation. for each of the 6 tasks described earlier. For example, (Castro Ferreira et al., 2020) contains 3025 If an automatic evaluation m"
2021.emnlp-main.575,W12-2018,0,0.200169,"- ify the sentences. For perturbations resulting from plying the perturbation template t for the criteria c. such invariant templates the score of the metric Further, let fe (p) be the score assigned by a given should not drop. The invariant and fluency-based 7222 Automatic Evaluation Metrics Task-specific (Context Dependent metrics) Task-agnostic (Context Free metrics) Word Based BLEU METEOR (Banerjee and Lavie, 2005) ROUGE (Lin, 2004) TER (Snover et al., 2006) CIDEr (Vedantam et al., 2015) Character Based chrF++ (Popovic, 2017) Pretrained (Static) Embeddings BERT Embeddings Greedy Matching (Rus and Lintean, 2012) Embedding Average (Landauer and Dumais, 1997) Vector Extrema (Forgues and Pineau, 2014) SMS (Clark et al., 2019) WMDo (Chow et al., 2019) MoverScore (Zhao et al., 2019) GRUEN (Zhu and Bhat, 2020) BERTScore (Zhang et al., 2020) End-to-End Trained BLEURT (Sellam et al., 2020) AS SUPERT (Gao et al., 2020) BLANC (Vasilyev et al., 2020) IC SPICE (Anderson et al., 2016) TIGEr (Jiang et al., 2019) VilBERTScore (Lee et al., 2020) QG Q-metrics (Nema and Khapra, 2018) DG ADEM (Lowe et al., 2017) RUBER (Tao et al., 2018) DEB (Sai et al., 2020a) D2T PARENT (Dhingra et al., 2019) Figure 2: Metrics analyse"
2021.emnlp-main.575,2020.tacl-1.52,1,0.196573,"sprocess (Freitag et al., 2021; Deriu et al., 2019; ment of multiple desirable criteria, e.g., fluHowcroft et al., 2020). Hence, automatic evaluaency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing tion metrics such as BLEU (Papineni et al., 2002) datasets for 6 NLG tasks, we observe that the which are quicker to compute have become popular, human evaluation scores on these multiple cridespite being less reliable (Callison-Burch et al., teria are often not correlated. For example, 2006; Reiter, 2018). there is a very low correlation between human The survey by Sai et al. (2020b) shows that more scores on fluency and data coverage for the than 35 automatic evaluation metrics have been task of structured data to text generation. This suggests that the current recipe of proposing proposed for NLG since 2014, however, there is no new automatic evaluation metrics for NLG by careful evaluation of the ability of such metrics to showing that they correlate well with scores asassess the quality of the output of an NLG system signed by humans for a single criteria (overon multiple desired criteria. For example, conall quality) alone is inadequate. Indeed, our sider the task"
2021.emnlp-main.575,N19-1170,0,0.0458025,"Missing"
2021.emnlp-main.575,2020.acl-main.704,0,0.260963,"a. For example, conall quality) alone is inadequate. Indeed, our sider the task of dialog evaluation, where humans extensive study involving 25 automatic evaluare asked to score the output on multiple criteria ation metrics across 6 different tasks and 18 such as fluency, adequacy, coherence, informativedifferent evaluation criteria shows that there is no single metric which correlates well with huness, engagingness, consistency, etc. Contrast this man scores on all desirable criteria, for most with automatic evaluation metrics such as BLEU, NLG tasks. Given this situation, we propose BLEURT (Sellam et al., 2020), DEB (Sai et al., CheckLists for better design and evaluation of 2020a), ADEM (Lowe et al., 2017), etc., which automatic metrics. We design templates which assign a single score to the output. What does this target a specific criteria (e.g., coverage) and score indicate? More specifically, does a low DEB perturb the output such that the quality gets score indicate that the output is not fluent or does it affected only along this specific criteria (e.g., indicate that the output is fluent but not coherent or the coverage drops). We show that existing evaluation metrics are not robust against e"
2021.emnlp-main.575,2020.acl-main.220,0,0.0116345,"e ersc o BLE re URT Remove punctuation Spelling mistake/typos Drop stopwords Subject-verb disagreement Jumbling words Negations Change numeric values Change names Drop named entities Retain only stop words Drop adjectives Drop words Antonyms Hyponyms Repeat Phrases Add extra text Contractions Expansions Num2words Synonyms Alternative references (f) Image Captioning Figure 4: Heatmap of deviation of metric scores from human averages. The darker the cell, the more the absolute deviation from human scores. proposed metrics such as DEB (Sai et al., 2020a), BLANC (Vasilyev et al., 2020) and MaUde (Sinha et al., 2020) do not correlate well with human judgements on other criteria. This is despite the fact that these are task-specific metrics which use the modern machinery of pre-trained BERT-based models and are fine-tuned on human judgements for overall quality. This vindicates our stand that simply tuning for overall quality does not lead to good correlations with other criteria. We do observe a few decently correlated metrics for some of the tasks along a few criteria. Specifically, the moderate correlations are found (i) in D2T for BLEURT along the dimensions of fluency, correctness and text structure,"
2021.emnlp-main.575,D17-1239,0,0.0606597,"Missing"
2021.naacl-tutorials.4,D18-1429,1,0.886242,"settled for automatic evaluation metrics to track scientific progress in this field. Automatic Evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004) have been around for several years and are still predominantly used. They have also been readily adopted for newer tasks in NLG such as Question Generation, Image Captioning, etc, due to the lack of any other relevant metrics. However, there has been heavy criticism for such an adoption of metrics across tasks, corroborated by their poor correlations with human judgements (Liu et al., 2016; Nema and Khapra, 2018; Dhingra et al., 2019). Several new metrics are being proposed to address the shortcomings of the existing ones (Sai et al., 2020b). The emerging metrics also explore the idea of using the context provided for the task (such as a document, image, passage, or tabular data, and so on), unlike BLEU, METEOR, ROUGE, etc. This has lead to the development of ‘context-dependent metrics’ alongside the ‘context-free metrics’. Both the context-free and context-dependent metrics can be categorized based on their underlying technique into trained metrics and untrained (i.e., rule-based/ heuristic-based) m"
2021.naacl-tutorials.4,P02-1040,0,0.11911,"nguage Generation Mitesh M. Khapra and Ananya B. Sai Robert-Bosch Centre for Data Science & Artificial Intelligence Indian Institute of Technology Madras India {miteshk,ananya}@cse.iitm.ac.in Abstract understand the scientific progress made, these NLG systems need to be evaluated carefully. The ideal way to do so would be to employ expert human evaluators. However, this option would be very time-consuming and expensive, and is thus infeasible. Hence the community has settled for automatic evaluation metrics to track scientific progress in this field. Automatic Evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004) have been around for several years and are still predominantly used. They have also been readily adopted for newer tasks in NLG such as Question Generation, Image Captioning, etc, due to the lack of any other relevant metrics. However, there has been heavy criticism for such an adoption of metrics across tasks, corroborated by their poor correlations with human judgements (Liu et al., 2016; Nema and Khapra, 2018; Dhingra et al., 2019). Several new metrics are being proposed to address the shortcomings of the existing ones (Sai et al., 2020"
2021.naacl-tutorials.4,W15-3049,0,0.0171922,"nt, image, passage, or tabular data, and so on), unlike BLEU, METEOR, ROUGE, etc. This has lead to the development of ‘context-dependent metrics’ alongside the ‘context-free metrics’. Both the context-free and context-dependent metrics can be categorized based on their underlying technique into trained metrics and untrained (i.e., rule-based/ heuristic-based) metrics. Untrained metrics can be further classified depending on whether they are word-based (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004; Snover et al., 2006; Druck and Pang, 2012; Dhingra et al., 2019), character-based (Popovic, 2015; Wang et al., 2016), or embedding-based (Rus and Lintean, 2012; Forgues et al., 2014; Kusner et al., 2015; Mathur et al., 2019; Zhang et al., 2019). Similarly, trained metrics are sub-categorized depending on There has been a massive surge of Natural Language Generation (NLG) models in the recent years, accelerated by deep learning and the availability of large-scale datasets. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community h"
2021.naacl-tutorials.4,W04-1013,0,0.156832,"Centre for Data Science & Artificial Intelligence Indian Institute of Technology Madras India {miteshk,ananya}@cse.iitm.ac.in Abstract understand the scientific progress made, these NLG systems need to be evaluated carefully. The ideal way to do so would be to employ expert human evaluators. However, this option would be very time-consuming and expensive, and is thus infeasible. Hence the community has settled for automatic evaluation metrics to track scientific progress in this field. Automatic Evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004) have been around for several years and are still predominantly used. They have also been readily adopted for newer tasks in NLG such as Question Generation, Image Captioning, etc, due to the lack of any other relevant metrics. However, there has been heavy criticism for such an adoption of metrics across tasks, corroborated by their poor correlations with human judgements (Liu et al., 2016; Nema and Khapra, 2018; Dhingra et al., 2019). Several new metrics are being proposed to address the shortcomings of the existing ones (Sai et al., 2020b). The emerging metrics also explore the idea of usin"
2021.naacl-tutorials.4,W12-2018,0,0.0291534,"BLEU, METEOR, ROUGE, etc. This has lead to the development of ‘context-dependent metrics’ alongside the ‘context-free metrics’. Both the context-free and context-dependent metrics can be categorized based on their underlying technique into trained metrics and untrained (i.e., rule-based/ heuristic-based) metrics. Untrained metrics can be further classified depending on whether they are word-based (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004; Snover et al., 2006; Druck and Pang, 2012; Dhingra et al., 2019), character-based (Popovic, 2015; Wang et al., 2016), or embedding-based (Rus and Lintean, 2012; Forgues et al., 2014; Kusner et al., 2015; Mathur et al., 2019; Zhang et al., 2019). Similarly, trained metrics are sub-categorized depending on There has been a massive surge of Natural Language Generation (NLG) models in the recent years, accelerated by deep learning and the availability of large-scale datasets. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation met"
2021.naacl-tutorials.4,D16-1230,0,0.0269813,"the community has settled for automatic evaluation metrics to track scientific progress in this field. Automatic Evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004) have been around for several years and are still predominantly used. They have also been readily adopted for newer tasks in NLG such as Question Generation, Image Captioning, etc, due to the lack of any other relevant metrics. However, there has been heavy criticism for such an adoption of metrics across tasks, corroborated by their poor correlations with human judgements (Liu et al., 2016; Nema and Khapra, 2018; Dhingra et al., 2019). Several new metrics are being proposed to address the shortcomings of the existing ones (Sai et al., 2020b). The emerging metrics also explore the idea of using the context provided for the task (such as a document, image, passage, or tabular data, and so on), unlike BLEU, METEOR, ROUGE, etc. This has lead to the development of ‘context-dependent metrics’ alongside the ‘context-free metrics’. Both the context-free and context-dependent metrics can be categorized based on their underlying technique into trained metrics and untrained (i.e., rule-ba"
2021.naacl-tutorials.4,P19-1427,0,0.0243935,"computing resources has led to the wide-spread adoption of these techniques and rapid developments in the field. To track the developments and 15 Proceedings of NAACL-HLT 2021: Tutorials, pages 15–19 June 6–11, 2021. ©2021 Association for Computational Linguistics whether they need input features (such as precision, recall, number of words in a sentence, etc,) (Stanojevic and Sima’an, 2014; Ma et al., 2017; Nema and Khapra, 2018) or whether they extract the features from the input sentences in an end-toend manner (Lowe et al., 2017; Tao et al., 2018; Cui et al., 2018; Shimanaka et al., 2018; Wieting et al., 2019; Sellam et al., 2020; Sai et al., 2020a). In this tutorial, we provide an overview of these different techniques that have been used to formulate automatic evaluation metrics. We also discuss the studies that analyze/inspect these metrics and report their shortcomings. The major criticisms on the metrics include the uninterpretability of the scores (Zhang et al., 2004; Callison-Burch et al., 2006), bias towards specific models (Dusek et al., 2020) or scores (Sai et al., 2019), and their inability to capture all the nuances in a task (Ananthakrishnan et al., 2006). We conclude by presenting th"
2021.naacl-tutorials.4,2020.tacl-1.52,1,0.905158,"ni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004) have been around for several years and are still predominantly used. They have also been readily adopted for newer tasks in NLG such as Question Generation, Image Captioning, etc, due to the lack of any other relevant metrics. However, there has been heavy criticism for such an adoption of metrics across tasks, corroborated by their poor correlations with human judgements (Liu et al., 2016; Nema and Khapra, 2018; Dhingra et al., 2019). Several new metrics are being proposed to address the shortcomings of the existing ones (Sai et al., 2020b). The emerging metrics also explore the idea of using the context provided for the task (such as a document, image, passage, or tabular data, and so on), unlike BLEU, METEOR, ROUGE, etc. This has lead to the development of ‘context-dependent metrics’ alongside the ‘context-free metrics’. Both the context-free and context-dependent metrics can be categorized based on their underlying technique into trained metrics and untrained (i.e., rule-based/ heuristic-based) metrics. Untrained metrics can be further classified depending on whether they are word-based (Papineni et al., 2002; Banerjee and"
2021.naacl-tutorials.4,zhang-etal-2004-interpreting,0,0.296636,"Missing"
2021.naacl-tutorials.4,2020.acl-main.704,0,0.0282924,"as led to the wide-spread adoption of these techniques and rapid developments in the field. To track the developments and 15 Proceedings of NAACL-HLT 2021: Tutorials, pages 15–19 June 6–11, 2021. ©2021 Association for Computational Linguistics whether they need input features (such as precision, recall, number of words in a sentence, etc,) (Stanojevic and Sima’an, 2014; Ma et al., 2017; Nema and Khapra, 2018) or whether they extract the features from the input sentences in an end-toend manner (Lowe et al., 2017; Tao et al., 2018; Cui et al., 2018; Shimanaka et al., 2018; Wieting et al., 2019; Sellam et al., 2020; Sai et al., 2020a). In this tutorial, we provide an overview of these different techniques that have been used to formulate automatic evaluation metrics. We also discuss the studies that analyze/inspect these metrics and report their shortcomings. The major criticisms on the metrics include the uninterpretability of the scores (Zhang et al., 2004; Callison-Burch et al., 2006), bias towards specific models (Dusek et al., 2020) or scores (Sai et al., 2019), and their inability to capture all the nuances in a task (Ananthakrishnan et al., 2006). We conclude by presenting the possible next direc"
2021.naacl-tutorials.4,W18-6456,0,0.014001,"s and access to powerful computing resources has led to the wide-spread adoption of these techniques and rapid developments in the field. To track the developments and 15 Proceedings of NAACL-HLT 2021: Tutorials, pages 15–19 June 6–11, 2021. ©2021 Association for Computational Linguistics whether they need input features (such as precision, recall, number of words in a sentence, etc,) (Stanojevic and Sima’an, 2014; Ma et al., 2017; Nema and Khapra, 2018) or whether they extract the features from the input sentences in an end-toend manner (Lowe et al., 2017; Tao et al., 2018; Cui et al., 2018; Shimanaka et al., 2018; Wieting et al., 2019; Sellam et al., 2020; Sai et al., 2020a). In this tutorial, we provide an overview of these different techniques that have been used to formulate automatic evaluation metrics. We also discuss the studies that analyze/inspect these metrics and report their shortcomings. The major criticisms on the metrics include the uninterpretability of the scores (Zhang et al., 2004; Callison-Burch et al., 2006), bias towards specific models (Dusek et al., 2020) or scores (Sai et al., 2019), and their inability to capture all the nuances in a task (Ananthakrishnan et al., 2006). We con"
2021.naacl-tutorials.4,2006.amta-papers.25,0,0.112877,"also explore the idea of using the context provided for the task (such as a document, image, passage, or tabular data, and so on), unlike BLEU, METEOR, ROUGE, etc. This has lead to the development of ‘context-dependent metrics’ alongside the ‘context-free metrics’. Both the context-free and context-dependent metrics can be categorized based on their underlying technique into trained metrics and untrained (i.e., rule-based/ heuristic-based) metrics. Untrained metrics can be further classified depending on whether they are word-based (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004; Snover et al., 2006; Druck and Pang, 2012; Dhingra et al., 2019), character-based (Popovic, 2015; Wang et al., 2016), or embedding-based (Rus and Lintean, 2012; Forgues et al., 2014; Kusner et al., 2015; Mathur et al., 2019; Zhang et al., 2019). Similarly, trained metrics are sub-categorized depending on There has been a massive surge of Natural Language Generation (NLG) models in the recent years, accelerated by deep learning and the availability of large-scale datasets. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement"
2021.naacl-tutorials.4,D14-1025,0,0.0642411,"Missing"
2021.naacl-tutorials.4,W16-2342,0,0.012891,"age, or tabular data, and so on), unlike BLEU, METEOR, ROUGE, etc. This has lead to the development of ‘context-dependent metrics’ alongside the ‘context-free metrics’. Both the context-free and context-dependent metrics can be categorized based on their underlying technique into trained metrics and untrained (i.e., rule-based/ heuristic-based) metrics. Untrained metrics can be further classified depending on whether they are word-based (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004; Snover et al., 2006; Druck and Pang, 2012; Dhingra et al., 2019), character-based (Popovic, 2015; Wang et al., 2016), or embedding-based (Rus and Lintean, 2012; Forgues et al., 2014; Kusner et al., 2015; Mathur et al., 2019; Zhang et al., 2019). Similarly, trained metrics are sub-categorized depending on There has been a massive surge of Natural Language Generation (NLG) models in the recent years, accelerated by deep learning and the availability of large-scale datasets. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued"
C10-1063,E09-1006,0,0.0556241,"Missing"
C10-1063,C96-1005,0,0.55873,"Missing"
C10-1063,P07-1005,0,0.101757,"Missing"
C10-1063,D09-1048,1,0.570253,"curacy. 555 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 555–563, Beijing, August 2010 cost of annotation. Finally, we propose a measure for cost-benefit analysis which identifies the optimal point of balance between these three related entities, viz., cross-linking, sense annotation and accuracy of disambiguation. The remainder of this paper is organized as follows. In section 2 we present related work. In section 3 we describe the Synset based multilingual dictionary which enables parameter projection. In section 4 we discuss the work of Khapra et al. (2009) on parameter projection for multilingual WSD. Section 5 is on the economics of multilingual WSD. In section 6 we propose a probabilistic model for representing the cross-linkage of words within synsets. In section 7 we present a strategy for injecting hard-to-disambiguate cases from the target language using selective sampling. In section 8 we introduce a measure for cost-benefit analysis for calculating the value for money in terms of accuracy, annotation effort and lexicon building effort. In section 9 we describe the experimental setup. In section 10 we present the results followed by disc"
C10-1063,W04-0834,0,0.784073,"er’s intuition. We extend their work by addressing the following question on the economics of annotation, lexicon building and performance: • Is there an optimal point of balance between the annotation effort and the lexicon building (i.e. manual cross-linking) effort at which one can be assured of best value for money in terms of accuracy? Word Sense Disambiguation (WSD) is one of the most widely investigated problems of Natural Language Processing (NLP). Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora (Ng and Lee, 1996; Lee et al., 2004) outperform unsupervised (Veronis, 2004) and knowledge based approaches (Mihalcea, 2005). HowTo address the above question we first propose a probabilistic cross linking model to eliminate the effort of manually cross linking words within the source and target language synsets and calibrate the resultant trade-off in accuracy. Next, we show that by injecting examples for most frequent hard-to-disambiguate words from the target domain one can achieve higher accuracies at optimal Sense annotation and lexicon building are costly affairs demanding prudent investment of resources. Recent work on mu"
C10-1063,H05-1052,0,0.289278,"of annotation, lexicon building and performance: • Is there an optimal point of balance between the annotation effort and the lexicon building (i.e. manual cross-linking) effort at which one can be assured of best value for money in terms of accuracy? Word Sense Disambiguation (WSD) is one of the most widely investigated problems of Natural Language Processing (NLP). Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora (Ng and Lee, 1996; Lee et al., 2004) outperform unsupervised (Veronis, 2004) and knowledge based approaches (Mihalcea, 2005). HowTo address the above question we first propose a probabilistic cross linking model to eliminate the effort of manually cross linking words within the source and target language synsets and calibrate the resultant trade-off in accuracy. Next, we show that by injecting examples for most frequent hard-to-disambiguate words from the target domain one can achieve higher accuracies at optimal Sense annotation and lexicon building are costly affairs demanding prudent investment of resources. Recent work on multilingual WSD has shown that it is possible to leverage the annotation work done for WS"
C10-1063,P96-1006,0,\N,Missing
C10-2040,A00-1031,0,0.187331,"Missing"
C10-2040,J95-4004,0,0.280771,"Missing"
C10-2040,W99-0633,0,0.0524016,"Missing"
C10-2040,A94-1024,0,0.180209,"Missing"
C10-2040,W96-0213,0,0.293228,"Missing"
C10-2040,P06-2100,1,0.863614,"Missing"
C10-2040,A92-1018,0,\N,Missing
C10-2040,H92-1023,0,\N,Missing
C12-3031,S07-1068,0,0.187128,"ides only a single algorithm and does not enable the user to compare the performance of different algorithms. Our system is a one-stop-shop for comparing several algorithms (including UKB, IMS and SenseRelate) with minimum computational and manual overhead for the user. We would like to mention that internally our system uses the implementations provided by UKB, IMS and SenseRelate and hence it would provide the same results as obtained by independently downloading and using these systems. Apart from UKB, IMS and SenseRelate, our system also provides an implementation for McCarthy’s approach (Koeling and McCarthy, 2007) and IWSD (Khapra et al., 2010). 3 System Details Figure 1 shows the main interface of our system. We first provide an overview of the system introducing the inputs which it expects followed by explaining the online output viewer, which is an interesting feature of our system. We also provide details about the mechanism with which new algorithms can be easily added to the system. Kindly refer to the figure while reading this section. 3.1 Interface Design To support various web browsers on different operating systems, we have designed the web interface using standard open technologies. The inte"
C12-3031,P05-3014,0,0.390396,"his system demonstration, we explain the system which powers our interface. The paper is organized as follows: We describe the existing, publicly available systems in section 2. In section 3 we provide technical details about our system. Section 4 summarizes the evaluation results on 3 standard datasets. Section 5 concludes the paper presenting the salient points of our system and some future enhancements for our system. 2 Related Work There are a few algorithms for which the implementation is publicly available. These include UKB (Agirre et al., 2009), IMS (Zhong and Ng, 2010), SenseLearner (Mihalcea and Csomai, 2005) and SenseRelate (Patwardhan et al., 2005). However, most of these have one or more of the overheads listed above. For example, UKB is currently available only for linux platforms. Further, the user needs to download and install these systems separately and run them on her/his machine which increases the computational cost. SenseLearner has an online interface, but in contrast to our system, it provides only a single algorithm and does not enable the user to compare the performance of different algorithms. Our system is a one-stop-shop for comparing several algorithms (including UKB, IMS and S"
C12-3031,P05-3019,0,0.785467,"-the-art algorithms. There is no overhead for the user, all (s)he needs is a web browser and the input file which may be sense tagged. Further, we also make provision for the developers of the new algorithms to integrate their algorithm in our system. This can be done by implementing a java interface exposed by us and upload the class file on our web-page. Some of the important aspects of our system are as follows: 1. Collection of several approaches - Users can obtain results for state-of-the-art approaches like IMS (Zhong and Ng, 2010), PPR (Agirre et al., 2009), knowledge based approaches (Patwardhan et al., 2005) etc, for an easy comparison of all approaches on a single dataset. 2. Parallel execution of several algorithms - The user can choose to run multiple algorithms in parallel, over the same dataset. The associated overhead of scheduling jobs and managing system resources is handled by the server and the user is exempted of these hassles. 3. Minimum supervision - After submitting his/her request, the end user can continue with their work without having to constantly monitor the task. Our interface notifies the user when the results are available. Once the users is notified, (s)he needs to downloa"
C12-3031,P10-4014,0,0.478735,"ve developed an online system, which allows the user to run several state-of-the-art algorithms. There is no overhead for the user, all (s)he needs is a web browser and the input file which may be sense tagged. Further, we also make provision for the developers of the new algorithms to integrate their algorithm in our system. This can be done by implementing a java interface exposed by us and upload the class file on our web-page. Some of the important aspects of our system are as follows: 1. Collection of several approaches - Users can obtain results for state-of-the-art approaches like IMS (Zhong and Ng, 2010), PPR (Agirre et al., 2009), knowledge based approaches (Patwardhan et al., 2005) etc, for an easy comparison of all approaches on a single dataset. 2. Parallel execution of several algorithms - The user can choose to run multiple algorithms in parallel, over the same dataset. The associated overhead of scheduling jobs and managing system resources is handled by the server and the user is exempted of these hassles. 3. Minimum supervision - After submitting his/her request, the end user can continue with their work without having to constantly monitor the task. Our interface notifies the user w"
C14-2002,W14-2109,1,0.791505,"Missing"
C14-2002,C14-1141,1,0.840609,"cussed in more detail elsewhere. Given a Topic, the Topic Analysis engine starts with initial semantic analysis of the Topic, aiming to identify the main concepts mentioned in this Topic and the sentiment towards each concept. Next, the CDC Oriented Article Retrieval engine employs IR and opinion mining techniques in order to retrieve Wikipedia articles that with high probability contain CDCs. Next, the CDC Detection engine relies on a combination of NLP and ML techniques to zoom-in within the retrieved articles and detect candidate CDCs. A detailed description of this engine can be found in (Levy et al 2014). Next, the CDC Pro/Con engine aims to automatically determine the polarity of the candidate CDC with respect to the given Topic by analyzing and contrasting the sentiment towards key concepts mentioned in the Topic and within the candidate CDC. Next, the CDC Equivalence engine uses techniques reminiscent of automatic paraphrase detection to identify whether two candidate CDCs are semantically equivalent, so to avoid redundancy in the generated output. Finally, the CDC Refinement engine aims to improve the precision of the generated output, based on the results collected thus far; e.g., using"
C16-1011,W12-3102,0,0.0122747,"set using crowdsourcing. CrowdFlower (https://make.crowdflower. com) was used as the crowdsourcing platform and they solicited one French and one German translation for each of the 5000 captions using native speakers. Note that (Rajendran et al., 2015) report results for cross modal search and do not address the problem of crosslingual image captioning. In our model, for D1 we use the same train(118K), validation (1K) and test sets (1K) as defined in (Rajendran et al., 2015) and explained above. Choosing D2 was a bit more tricky. Initially we considered the corpus released as part of WMT’12 (Callison-Burch et al., 2012) which contains roughly 44M English-French parallel sentences from various sources including News, parliamentary proceedings, etc. However, our initial small scale experiments showed that this does not work well because there is a clear mismatch between the vocabulary of this corpus and the vocabulary that we need for generating captions. Also the vocabulary is much larger (at least an order higher than what we need for image captioning) and it thus hampers training. Further, the average length and structure of these sentences is also very different from captions. Domain shift in MT is itself"
C16-1011,P05-1033,0,0.105885,"idea is not just limited to translation but could be applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliterati"
C16-1011,D14-1179,1,0.0874101,"Missing"
C16-1011,W15-3909,0,0.0154288,"applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 2012) cater to < 20 languages. Similarly,"
C16-1011,N16-1101,1,0.92347,"ng two independent models using the datasets between XZ and ZY , the model jointly learns from the two datasets. The resulting common representation learned for X and Z can be viewed as a vectorial analogue of the linguistic representation sought by interlingua based approaches. Of course, by no means do we suggest that this vectorial representation is a substitute for the rich linguistic representation but its easier to learn from parallel data (as opposed to a linguistic representation which requires hand crafted resources). Note that our work should not be confused with the recent work of (Firat et al., 2016), (Zoph and Knight, 2016) and (Elliott et al., 2015). The last two works in fact require 3-way parallel data between X, Z and Y and learn to decode sequences in Y given both X and Z. For example, at test time, (Elliott et al., 2015) generate captions in German, given both (i) the image and (ii) its corresponding English caption. This is indeed very different from the problem addressed in this paper. Similarly, even though (Firat et al., 2016) learn a single encoder per language and a single decoder per language they do not learn shared representations for multiple languages (only the attention"
C16-1011,P14-1006,0,0.042996,"ly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015). For example, Andrew et al. (2013) propose Deep CCA for learning a common representation for two views. (Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al. (2015) propose bridge correlational networks for multilingual multimodal representation learning. From the point of view of representation learning, the work of Rajendran et al. (2015) is very similar to our work except that it focuses only on representation learning and does not consider the end goal of gen"
C16-1011,N10-1065,1,0.82703,", Firat et al. (2016) focus on multi-task learning with a shared attention mechanism and the goal is to improve the MT performance for a pair of languages for which parallel data is available. This is clearly different from the goal of this paper which is to design encoder decoder models for a pair of languages where no parallel data is available but data is available only between each of these languages and a bridge language. Of course, in general the idea of pivot/bridge/interlingua based conversion is not new and has been used previously in several non-neural network settings. For example (Khapra et al., 2010) use a bridge language or pivot language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has"
C16-1011,C12-1089,0,0.0446559,"language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015). For example, Andrew et al. (2013) propose Deep CCA for learning a common representation for two views. (Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al. (2015) propose bridge correlational networks for multilingual multimodal representation learning. From the point of view of representation learning, the work of Rajendran et al. (2015) is very similar to our work except that it focuses only on representation l"
C16-1011,N03-1017,0,0.0137786,"e believe that this idea is not just limited to translation but could be applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for"
C16-1011,P07-2045,0,0.0074932,"Missing"
C16-1011,D15-1166,0,0.16404,"st limited to translation but could be applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 20"
C16-1011,W15-3911,0,0.0204691,"volving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 2012) cater to < 20 languages. Similarly, publicly available image caption datasets"
C16-1011,W15-3908,0,0.012526,"nd of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing. It is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require n C parallel datasets to build systems to convert from any source view to any target view. Obviously, this 2 does not scale well in practice because it is hard to find parallel data between all n C2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 2012) cater to < 20 languages. Similarly, publicly available"
C16-1011,P07-1108,0,0.0328358,"improve the MT performance for a pair of languages for which parallel data is available. This is clearly different from the goal of this paper which is to design encoder decoder models for a pair of languages where no parallel data is available but data is available only between each of these languages and a bridge language. Of course, in general the idea of pivot/bridge/interlingua based conversion is not new and has been used previously in several non-neural network settings. For example (Khapra et al., 2010) use a bridge language or pivot language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2"
C16-1011,D14-1174,0,0.0188242,"ormance for a pair of languages for which parallel data is available. This is clearly different from the goal of this paper which is to design encoder decoder models for a pair of languages where no parallel data is available but data is available only between each of these languages and a bridge language. Of course, in general the idea of pivot/bridge/interlingua based conversion is not new and has been used previously in several non-neural network settings. For example (Khapra et al., 2010) use a bridge language or pivot language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT. The main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al."
C16-1011,N16-1004,0,0.0694392,"els using the datasets between XZ and ZY , the model jointly learns from the two datasets. The resulting common representation learned for X and Z can be viewed as a vectorial analogue of the linguistic representation sought by interlingua based approaches. Of course, by no means do we suggest that this vectorial representation is a substitute for the rich linguistic representation but its easier to learn from parallel data (as opposed to a linguistic representation which requires hand crafted resources). Note that our work should not be confused with the recent work of (Firat et al., 2016), (Zoph and Knight, 2016) and (Elliott et al., 2015). The last two works in fact require 3-way parallel data between X, Z and Y and learn to decode sequences in Y given both X and Z. For example, at test time, (Elliott et al., 2015) generate captions in German, given both (i) the image and (ii) its corresponding English caption. This is indeed very different from the problem addressed in this paper. Similarly, even though (Firat et al., 2016) learn a single encoder per language and a single decoder per language they do not learn shared representations for multiple languages (only the attention mechanism is shared). Fu"
C16-1011,H94-1093,0,\N,Missing
C16-1011,W12-4402,0,\N,Missing
C18-1319,P13-2037,0,0.0255762,"hes for code-mixed content. This interest is largely triggered by the abundance of code-mixed content found in chats, emails, social media platforms, etc. In the context of such code-mixed content, existing works have looked at the problems of language identification (Nguyen and Dogru¨oz, 2013; Solorio et al., 2014; Barman et al., 2014; Molina et al., 2016), part-of-speech tagging (Barman et al., 2016; Ghosh et al., 2016; AlGhamdi et al., 2016), user profiling (Khapra et al., 2013), topic modeling (Rosner and Farrugia, 2007), information retrieval (Chakma and Das, 2016) and language modeling (Adel et al., 2013a; Adel et al., 2013b; Adel et al., 2015). However, to the best of our knowledge, ours is the first work on developing code-mixed conversation systems for goal-oriented dialogs. 3768 # of Utterances # of Unique utterances Average # of utterances per dialog Average # of words per utterance Average # of words per dialog Average # of KB triples per dialog # of Train Dialogs # of Validation Dialogs # of Test Dialogs Vocabulary size 49167 6733 15.19 7.71 120.33 38.24 1168 500 1117 1229 Table 3: Statistics of the English version of DSTC2 dataset 3 Background: DSTC2 Restaurant Reservation Dataset We"
C18-1319,W16-5812,0,0.0695245,"e dataset developed as a part of this work is a small step in that direction. In general, the research community has been interested in developing datasets, tools and approaches for code-mixed content. This interest is largely triggered by the abundance of code-mixed content found in chats, emails, social media platforms, etc. In the context of such code-mixed content, existing works have looked at the problems of language identification (Nguyen and Dogru¨oz, 2013; Solorio et al., 2014; Barman et al., 2014; Molina et al., 2016), part-of-speech tagging (Barman et al., 2016; Ghosh et al., 2016; AlGhamdi et al., 2016), user profiling (Khapra et al., 2013), topic modeling (Rosner and Farrugia, 2007), information retrieval (Chakma and Das, 2016) and language modeling (Adel et al., 2013a; Adel et al., 2013b; Adel et al., 2015). However, to the best of our knowledge, ours is the first work on developing code-mixed conversation systems for goal-oriented dialogs. 3768 # of Utterances # of Unique utterances Average # of utterances per dialog Average # of words per utterance Average # of words per dialog Average # of KB triples per dialog # of Train Dialogs # of Validation Dialogs # of Test Dialogs Vocabulary size"
C18-1319,P12-2040,0,0.0339623,"ons. As the name suggests, human-bot conversation datasets contain conversations between humans and an existing conversation system (typically a domain-specific goal-oriented bot) (Williams et al., 2013; Henderson et al., 2014a; Henderson et al., 2014b). Human-human conversations, on the other hand, can contain spontaneous conversations between humans, as are typically observed in discussion forums (Walker et al., 2012), chat rooms (Forsythand and Martell, 2007), SMS messages (Chen and Kan, 2013) and so on. Human-human conversations can also contain scripted dialogs such as scripts of movies (Banchs, 2012), TV shows (Roy et al., 2014), etc. It is surprising that of the 63 conversation datasets developed in the past (Serban et al., 2015), none contain multilingual conversations. In particular, none of them contain code-mixed conversations from multilingual regions of the world. There is clearly a need to fill this gap and we believe that the dataset developed as a part of this work is a small step in that direction. In general, the research community has been interested in developing datasets, tools and approaches for code-mixed content. This interest is largely triggered by the abundance of cod"
C18-1319,W14-3902,0,0.0253367,"ions from multilingual regions of the world. There is clearly a need to fill this gap and we believe that the dataset developed as a part of this work is a small step in that direction. In general, the research community has been interested in developing datasets, tools and approaches for code-mixed content. This interest is largely triggered by the abundance of code-mixed content found in chats, emails, social media platforms, etc. In the context of such code-mixed content, existing works have looked at the problems of language identification (Nguyen and Dogru¨oz, 2013; Solorio et al., 2014; Barman et al., 2014; Molina et al., 2016), part-of-speech tagging (Barman et al., 2016; Ghosh et al., 2016; AlGhamdi et al., 2016), user profiling (Khapra et al., 2013), topic modeling (Rosner and Farrugia, 2007), information retrieval (Chakma and Das, 2016) and language modeling (Adel et al., 2013a; Adel et al., 2013b; Adel et al., 2015). However, to the best of our knowledge, ours is the first work on developing code-mixed conversation systems for goal-oriented dialogs. 3768 # of Utterances # of Unique utterances Average # of utterances per dialog Average # of words per utterance Average # of words per dialog"
C18-1319,W16-5804,0,0.0139966,"d to fill this gap and we believe that the dataset developed as a part of this work is a small step in that direction. In general, the research community has been interested in developing datasets, tools and approaches for code-mixed content. This interest is largely triggered by the abundance of code-mixed content found in chats, emails, social media platforms, etc. In the context of such code-mixed content, existing works have looked at the problems of language identification (Nguyen and Dogru¨oz, 2013; Solorio et al., 2014; Barman et al., 2014; Molina et al., 2016), part-of-speech tagging (Barman et al., 2016; Ghosh et al., 2016; AlGhamdi et al., 2016), user profiling (Khapra et al., 2013), topic modeling (Rosner and Farrugia, 2007), information retrieval (Chakma and Das, 2016) and language modeling (Adel et al., 2013a; Adel et al., 2013b; Adel et al., 2015). However, to the best of our knowledge, ours is the first work on developing code-mixed conversation systems for goal-oriented dialogs. 3768 # of Utterances # of Unique utterances Average # of utterances per dialog Average # of words per utterance Average # of words per dialog Average # of KB triples per dialog # of Train Dialogs # of Validati"
C18-1319,D14-1179,0,0.0192951,"Missing"
C18-1319,E17-2075,0,0.0419404,"alog states associated with the utterances). In addition, the authors also created API calls which can be issued to an underlying Knowledge Base (KB) and appended the resultant KB triples to each dialog. Table 2 shows one small sample dialog from this adapted dataset along with the API calls. Notice that the API call uses the information of all the constraints specified by the user so far and then receives all triples from the restaurant KB which match the user’s requirements. This dataset facilitated the development of models (Bordes and Weston, 2017; Seo et al., 2017; Williams et al., 2017; Eric and Manning, 2017) which just predict the bot utterances and API calls without explicitly tracking the slots. Table 3 reports the statistics of this dataset. In this work, we create code-mixed versions of this dataset in 4 different languages as described below. 4 Code-Mixed Dialog Dataset In this section, we describe the process used for creating a new dataset containing code-mixed conversations. Specifically, we describe (i) the process used for extracting unique utterance templates from the original DSTC2 dataset, (ii) the process of creating code-mixed translations of these utterances with the help of in-ho"
C18-1319,L16-1292,0,0.0490306,"Missing"
C18-1319,W16-5811,0,0.0240221,"d we believe that the dataset developed as a part of this work is a small step in that direction. In general, the research community has been interested in developing datasets, tools and approaches for code-mixed content. This interest is largely triggered by the abundance of code-mixed content found in chats, emails, social media platforms, etc. In the context of such code-mixed content, existing works have looked at the problems of language identification (Nguyen and Dogru¨oz, 2013; Solorio et al., 2014; Barman et al., 2014; Molina et al., 2016), part-of-speech tagging (Barman et al., 2016; Ghosh et al., 2016; AlGhamdi et al., 2016), user profiling (Khapra et al., 2013), topic modeling (Rosner and Farrugia, 2007), information retrieval (Chakma and Das, 2016) and language modeling (Adel et al., 2013a; Adel et al., 2013b; Adel et al., 2015). However, to the best of our knowledge, ours is the first work on developing code-mixed conversation systems for goal-oriented dialogs. 3768 # of Utterances # of Unique utterances Average # of utterances per dialog Average # of words per utterance Average # of words per dialog Average # of KB triples per dialog # of Train Dialogs # of Validation Dialogs # of Test"
C18-1319,W16-5802,0,0.0444729,"Missing"
C18-1319,W14-4337,0,0.340341,"checking movie show timings, finding directions for navigation, etc. Apart from these commercial systems, there has also been significant academic research to advance the state of the art in conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Yao et al., 2015; Li et al., 2016a; Li et al., 2016b; Serban et al., 2017). Most of this academic research is driven by publicly available datasets such as Twitter conversation dataset (Ritter et al., 2010), Ubuntu dialog dataset (Lowe et al., 2015), Movie subtitles dataset (Lison and Tiedemann, 2016) and DSTC2 restaurant reservation dataset (Henderson et al., 2014a). In this work, we focus on goal-oriented conversations such as the ones contained in the DSTC2 dataset. Most of the datasets and state of the art systems mentioned above are monolingual. Specifically, all the utterances and responses in the conversations are in one language (typically, English) and there are no multilingual and/or code-mixed utterances/responses. However, in several multilingual regions of the world, such as India, it is natural for speakers to produce utterances and responses which are multilingual and code-mixed. For example, Table 1 shows real examples of how bilingual s"
C18-1319,N16-1014,0,0.0336438,"care, e-commerce, etc. To cater to this demand, several commercial conversation systems such as Siri, Cortana, Allo have been developed. While these systems are still far from general purpose open domain chat, they perform reasonably well for certain goal-oriented tasks such as setting alarms/reminders, booking appointments, checking movie show timings, finding directions for navigation, etc. Apart from these commercial systems, there has also been significant academic research to advance the state of the art in conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Yao et al., 2015; Li et al., 2016a; Li et al., 2016b; Serban et al., 2017). Most of this academic research is driven by publicly available datasets such as Twitter conversation dataset (Ritter et al., 2010), Ubuntu dialog dataset (Lowe et al., 2015), Movie subtitles dataset (Lison and Tiedemann, 2016) and DSTC2 restaurant reservation dataset (Henderson et al., 2014a). In this work, we focus on goal-oriented conversations such as the ones contained in the DSTC2 dataset. Most of the datasets and state of the art systems mentioned above are monolingual. Specifically, all the utterances and responses in the conversations are in o"
C18-1319,P16-1094,0,0.319774,"terances per dialog in Table 3. We also calculated the number of utterances which contain k nonnative words and then plotted a histogram (Figure 1) where the x-axis shows the number of k non-native words and the y-axis shows the number of utterances which had k non-native words. These histograms show a similar trend across all the languages. Apart from such intra-utterance code-mixing, we also noticed some intra-word code-mixing in mostly Bengali (restauranter, towner) and Tamil (addressum, numberum, addressah) versions of the dataset. 4.5 Quantitative Measures of Code-Mixing Gamb¨ack and Das (2016) introduced a measure to quantify the amount of code-mixing in a sentence as: Cu (x) =   N (x)− max {tLi } Li ∈L N (x)  0 : N (x) &gt; 0 : N (x) = 0 (1) Here, L is the set of all languages in the corpus, tLi is the number of tokens of language Li in the given sentence x, max{tLi } is the maximum number of tokens of a language Li in the sentence x and N (x) is Li ∈L the number of language-specific tokens in the sentence (N (x) does not include named entities as they are language agnostic). The authors make a crucial assumption that arg max{tLi } is the Matrix language Li ∈L and hence the numera"
C18-1319,W04-1013,0,0.0295652,"Missing"
C18-1319,L16-1147,0,0.0225953,"iented tasks such as setting alarms/reminders, booking appointments, checking movie show timings, finding directions for navigation, etc. Apart from these commercial systems, there has also been significant academic research to advance the state of the art in conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Yao et al., 2015; Li et al., 2016a; Li et al., 2016b; Serban et al., 2017). Most of this academic research is driven by publicly available datasets such as Twitter conversation dataset (Ritter et al., 2010), Ubuntu dialog dataset (Lowe et al., 2015), Movie subtitles dataset (Lison and Tiedemann, 2016) and DSTC2 restaurant reservation dataset (Henderson et al., 2014a). In this work, we focus on goal-oriented conversations such as the ones contained in the DSTC2 dataset. Most of the datasets and state of the art systems mentioned above are monolingual. Specifically, all the utterances and responses in the conversations are in one language (typically, English) and there are no multilingual and/or code-mixed utterances/responses. However, in several multilingual regions of the world, such as India, it is natural for speakers to produce utterances and responses which are multilingual and code-m"
C18-1319,W15-4640,0,0.0958115,"y perform reasonably well for certain goal-oriented tasks such as setting alarms/reminders, booking appointments, checking movie show timings, finding directions for navigation, etc. Apart from these commercial systems, there has also been significant academic research to advance the state of the art in conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Yao et al., 2015; Li et al., 2016a; Li et al., 2016b; Serban et al., 2017). Most of this academic research is driven by publicly available datasets such as Twitter conversation dataset (Ritter et al., 2010), Ubuntu dialog dataset (Lowe et al., 2015), Movie subtitles dataset (Lison and Tiedemann, 2016) and DSTC2 restaurant reservation dataset (Henderson et al., 2014a). In this work, we focus on goal-oriented conversations such as the ones contained in the DSTC2 dataset. Most of the datasets and state of the art systems mentioned above are monolingual. Specifically, all the utterances and responses in the conversations are in one language (typically, English) and there are no multilingual and/or code-mixed utterances/responses. However, in several multilingual regions of the world, such as India, it is natural for speakers to produce utter"
C18-1319,W16-5805,0,0.0286925,"l regions of the world. There is clearly a need to fill this gap and we believe that the dataset developed as a part of this work is a small step in that direction. In general, the research community has been interested in developing datasets, tools and approaches for code-mixed content. This interest is largely triggered by the abundance of code-mixed content found in chats, emails, social media platforms, etc. In the context of such code-mixed content, existing works have looked at the problems of language identification (Nguyen and Dogru¨oz, 2013; Solorio et al., 2014; Barman et al., 2014; Molina et al., 2016), part-of-speech tagging (Barman et al., 2016; Ghosh et al., 2016; AlGhamdi et al., 2016), user profiling (Khapra et al., 2013), topic modeling (Rosner and Farrugia, 2007), information retrieval (Chakma and Das, 2016) and language modeling (Adel et al., 2013a; Adel et al., 2013b; Adel et al., 2015). However, to the best of our knowledge, ours is the first work on developing code-mixed conversation systems for goal-oriented dialogs. 3768 # of Utterances # of Unique utterances Average # of utterances per dialog Average # of words per utterance Average # of words per dialog Average # of KB triple"
C18-1319,D13-1084,0,0.0785384,"Missing"
C18-1319,P02-1040,0,0.102276,"en the context and it has to generate the response. For both the models, we used Adam optimizer (Kingma and Ba, 2015) to train the network with a mini batch size of 32. We used dropouts (Srivastava et al., 2014) of 0.25 and 0.35, initial learning rate of 0.0004 and Gated Recurrent Units (GRU) (Cho et al., 2014) with hidden dimensions of size 350. We used word embeddings of size 300 with Glorot initialization (Glorot and Bengio, 2010). We also clipped the gradients at a maximum norm of 10 to avoid exploding gradients. 5.2 Evaluation We evaluate the performance of the above models using BLEU-4 (Papineni et al., 2002), ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004) which are widely used to evaluate the performance of Natural Language Generation systems. We also compute the per utterance accuracy (exact match) by comparing 3774 Metrics BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L Per response acc. Per dialog acc. English 56.6 67.2 55.9 64.8 46.0 1.4 S EQ 2 SEQ WITH ATTENTION Hindi Bengali Gujarati 54.0 56.8 53.8 62.9 67.4 64.7 52.4 57.5 54.8 61.0 65.1 62.6 48.0 50.4 47.6 1.2 1.5 1.5 Tamil 62.1 67.8 56.3 65.6 49.3 1.3 English 57.8 67.9 57.5 65.7 48.8 1.4 Hindi 54.1 63.3 52.6 61.5 47.2 1.5 HRED Bengali 56.7 67.1 56.9 64.8 47.7 1"
C18-1319,N10-1020,0,0.594531,"ar from general purpose open domain chat, they perform reasonably well for certain goal-oriented tasks such as setting alarms/reminders, booking appointments, checking movie show timings, finding directions for navigation, etc. Apart from these commercial systems, there has also been significant academic research to advance the state of the art in conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Yao et al., 2015; Li et al., 2016a; Li et al., 2016b; Serban et al., 2017). Most of this academic research is driven by publicly available datasets such as Twitter conversation dataset (Ritter et al., 2010), Ubuntu dialog dataset (Lowe et al., 2015), Movie subtitles dataset (Lison and Tiedemann, 2016) and DSTC2 restaurant reservation dataset (Henderson et al., 2014a). In this work, we focus on goal-oriented conversations such as the ones contained in the DSTC2 dataset. Most of the datasets and state of the art systems mentioned above are monolingual. Specifically, all the utterances and responses in the conversations are in one language (typically, English) and there are no multilingual and/or code-mixed utterances/responses. However, in several multilingual regions of the world, such as India,"
C18-1319,rosset-petel-2006-ritel,0,0.0547714,"number of restaurant alimentum and whats the phone number for that? is restaurant alimentum phone. okay, thank you, good bye. you are welcome. Table 2: An example chat from the English version of DSTC2 dataset (Bordes and Weston, 2017). schedules (Williams et al., 2013), collecting tourist information (Henderson et al., 2014b) and so on. Such datasets are also typically domain-specific. Open-ended conversations on the other hand involve general chat on any topic and there is no specific end task. Some popular examples of datasets containing such open-ended conversations are the Ritel Corpus (Rosset and Petel, 2006), NPS Chat Corpus (Forsythand and Martell, 2007), Twitter Corpus (Ritter et al., 2010), etc. The third dimension is whether the dataset contains human-human conversations or human-bot conversations. As the name suggests, human-bot conversation datasets contain conversations between humans and an existing conversation system (typically a domain-specific goal-oriented bot) (Williams et al., 2013; Henderson et al., 2014a; Henderson et al., 2014b). Human-human conversations, on the other hand, can contain spontaneous conversations between humans, as are typically observed in discussion forums (Wal"
C18-1319,roy-etal-2014-tvd,0,0.0311406,", human-bot conversation datasets contain conversations between humans and an existing conversation system (typically a domain-specific goal-oriented bot) (Williams et al., 2013; Henderson et al., 2014a; Henderson et al., 2014b). Human-human conversations, on the other hand, can contain spontaneous conversations between humans, as are typically observed in discussion forums (Walker et al., 2012), chat rooms (Forsythand and Martell, 2007), SMS messages (Chen and Kan, 2013) and so on. Human-human conversations can also contain scripted dialogs such as scripts of movies (Banchs, 2012), TV shows (Roy et al., 2014), etc. It is surprising that of the 63 conversation datasets developed in the past (Serban et al., 2015), none contain multilingual conversations. In particular, none of them contain code-mixed conversations from multilingual regions of the world. There is clearly a need to fill this gap and we believe that the dataset developed as a part of this work is a small step in that direction. In general, the research community has been interested in developing datasets, tools and approaches for code-mixed content. This interest is largely triggered by the abundance of code-mixed content found in chat"
C18-1319,P15-1152,0,0.0265333,"ks in several domains such as entertainment, finance, healthcare, e-commerce, etc. To cater to this demand, several commercial conversation systems such as Siri, Cortana, Allo have been developed. While these systems are still far from general purpose open domain chat, they perform reasonably well for certain goal-oriented tasks such as setting alarms/reminders, booking appointments, checking movie show timings, finding directions for navigation, etc. Apart from these commercial systems, there has also been significant academic research to advance the state of the art in conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Yao et al., 2015; Li et al., 2016a; Li et al., 2016b; Serban et al., 2017). Most of this academic research is driven by publicly available datasets such as Twitter conversation dataset (Ritter et al., 2010), Ubuntu dialog dataset (Lowe et al., 2015), Movie subtitles dataset (Lison and Tiedemann, 2016) and DSTC2 restaurant reservation dataset (Henderson et al., 2014a). In this work, we focus on goal-oriented conversations such as the ones contained in the DSTC2 dataset. Most of the datasets and state of the art systems mentioned above are monolingual. Specifically, all t"
C18-1319,W14-3907,0,0.0933342,"n code-mixed conversations from multilingual regions of the world. There is clearly a need to fill this gap and we believe that the dataset developed as a part of this work is a small step in that direction. In general, the research community has been interested in developing datasets, tools and approaches for code-mixed content. This interest is largely triggered by the abundance of code-mixed content found in chats, emails, social media platforms, etc. In the context of such code-mixed content, existing works have looked at the problems of language identification (Nguyen and Dogru¨oz, 2013; Solorio et al., 2014; Barman et al., 2014; Molina et al., 2016), part-of-speech tagging (Barman et al., 2016; Ghosh et al., 2016; AlGhamdi et al., 2016), user profiling (Khapra et al., 2013), topic modeling (Rosner and Farrugia, 2007), information retrieval (Chakma and Das, 2016) and language modeling (Adel et al., 2013a; Adel et al., 2013b; Adel et al., 2015). However, to the best of our knowledge, ours is the first work on developing code-mixed conversation systems for goal-oriented dialogs. 3768 # of Utterances # of Unique utterances Average # of utterances per dialog Average # of words per utterance Average #"
C18-1319,D14-1105,0,0.0584874,"ges in a corpus. This metric is much simpler and simply computes the number of switch points in the corpus. For example, if a corpus contains n words and there are k positions at which the language k of wordi is not the same as the language of wordj then the I-index is given by n−1 . We compute the I-index for every utterance in a dialog, then compute the average over all utterances in a dialog and finally report the average across all dialogs in the code-mixed corpus. These measures of our dataset are shown in Table 7 and are compared with that of the existing datasets (Jamatia et al., 2016; Vyas et al., 2014). Jamatia et al. (2016) collected the code-mixed text from Twitter (TW) and Facebook (FB) posts whereas Vyas et al. (2014) collected their dataset only from Facebook forums. Although the dataset of Vyas et al. (2014) show the highest inter-utterance code-mixing (δ), Hi-DSTC2 and Ta-DSTC2 show the highest level of overall code-mixing at the utterance level (Cavg ) and the corpus level (Cc ) respectively. 5 Baseline Models We establish some initial baseline results on this code-mixed dataset by evaluating two different generation based models: (i) sequence-to-sequence with attention (Bahdanau et"
C18-1319,walker-etal-2012-corpus,0,0.0263535,"06), NPS Chat Corpus (Forsythand and Martell, 2007), Twitter Corpus (Ritter et al., 2010), etc. The third dimension is whether the dataset contains human-human conversations or human-bot conversations. As the name suggests, human-bot conversation datasets contain conversations between humans and an existing conversation system (typically a domain-specific goal-oriented bot) (Williams et al., 2013; Henderson et al., 2014a; Henderson et al., 2014b). Human-human conversations, on the other hand, can contain spontaneous conversations between humans, as are typically observed in discussion forums (Walker et al., 2012), chat rooms (Forsythand and Martell, 2007), SMS messages (Chen and Kan, 2013) and so on. Human-human conversations can also contain scripted dialogs such as scripts of movies (Banchs, 2012), TV shows (Roy et al., 2014), etc. It is surprising that of the 63 conversation datasets developed in the past (Serban et al., 2015), none contain multilingual conversations. In particular, none of them contain code-mixed conversations from multilingual regions of the world. There is clearly a need to fill this gap and we believe that the dataset developed as a part of this work is a small step in that dir"
C18-1319,W13-4065,0,0.0595264,"urant alimentum phone restaurant alimentum R address restaurant alimentum address restaurant alimentum R price moderate restaurant alimentum R rating 10 restaurant alimentum is a nice restaurant <SILENCE&gt; in the south of town serving modern european food. Sure, restaurant alimentum is on can i get the address? restaurant alimentum address. The phone number of restaurant alimentum and whats the phone number for that? is restaurant alimentum phone. okay, thank you, good bye. you are welcome. Table 2: An example chat from the English version of DSTC2 dataset (Bordes and Weston, 2017). schedules (Williams et al., 2013), collecting tourist information (Henderson et al., 2014b) and so on. Such datasets are also typically domain-specific. Open-ended conversations on the other hand involve general chat on any topic and there is no specific end task. Some popular examples of datasets containing such open-ended conversations are the Ritel Corpus (Rosset and Petel, 2006), NPS Chat Corpus (Forsythand and Martell, 2007), Twitter Corpus (Ritter et al., 2010), etc. The third dimension is whether the dataset contains human-human conversations or human-bot conversations. As the name suggests, human-bot conversation data"
C18-1319,P17-1062,0,0.0931226,"without any explicit dialog states associated with the utterances). In addition, the authors also created API calls which can be issued to an underlying Knowledge Base (KB) and appended the resultant KB triples to each dialog. Table 2 shows one small sample dialog from this adapted dataset along with the API calls. Notice that the API call uses the information of all the constraints specified by the user so far and then receives all triples from the restaurant KB which match the user’s requirements. This dataset facilitated the development of models (Bordes and Weston, 2017; Seo et al., 2017; Williams et al., 2017; Eric and Manning, 2017) which just predict the bot utterances and API calls without explicitly tracking the slots. Table 3 reports the statistics of this dataset. In this work, we create code-mixed versions of this dataset in 4 different languages as described below. 4 Code-Mixed Dialog Dataset In this section, we describe the process used for creating a new dataset containing code-mixed conversations. Specifically, we describe (i) the process used for extracting unique utterance templates from the original DSTC2 dataset, (ii) the process of creating code-mixed translations of these utteranc"
D09-1048,C96-1005,0,0.775156,"Missing"
D09-1048,W04-0834,0,0.628522,"Missing"
D09-1048,P97-1009,0,0.0853609,"Missing"
D09-1048,H05-1052,0,0.122839,"nguage to another. Section 7 describes two WSD algorithms which combine various parameters for do459 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459–467, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP main-specific WSD. Experiments and results are presented in sections 8 and 9. Section 10 concludes the paper. 2 Related work Knowledge based approaches to WSD such as Lesk‟s algorithm (Michael Lesk, 1986), Walker‟s algorithm (Walker D. & Amsler R., 1986), conceptual density (Agirre Eneko & German Rigau, 1996) and random walk algorithm (Mihalcea Rada, 2005) essentially do Machine Readable Dictionary lookup. However, these are fundamentally overlap based algorithms which suffer from overlap sparsity, dictionary definitions being generally small in length. Supervised learning algorithms for WSD are mostly word specific classifiers, e.g., WSD using SVM (Lee et. al., 2004), Exemplar based WSD (Ng Hwee T. & Hian B. Lee, 1996) and decision list based algorithm (Yarowsky, 1994). The requirement of a large training corpus renders these algorithms unsuitable for resource scarce languages. Semi-supervised and unsupervised algorithms do not need large amou"
D09-1048,P96-1006,0,0.615896,"Missing"
D09-1048,W97-0209,0,0.231629,"Missing"
D09-1048,P94-1013,0,0.0429486,"Missing"
D09-1048,P95-1026,0,0.472283,"Missing"
D09-1048,J93-2003,0,\N,Missing
D15-1050,W14-2107,0,0.0693973,"Missing"
D15-1050,P12-2041,0,0.15339,"ext columns indicate the number of articles in which at least one CDE was found; the total number of CDE detected for each type; the average percent of claims for which at least one CDE was found; and for these claims, the average number of CDE found. Note that the total number of CDE is not a simple sum of the CDE per type, as CDE can be assigned with more than one type. Standard deviations of distribution across topics are given in parenthesis where relevant. guments (Mochales Palau and Moens, 2009), analyzing argument structure (Peldszus, 2014), and identifying relations between arguments (Cabrio and Villata, 2012; Ghosh et al., 2014). Other works focused on specific domains such as evidence-based legal documents (Mochales Palau and Moens, 2011; Ashley and Walker, 2013), online debates (Cabrio and Villata, ˇ 2012; Boltuˇzi´c and Snajder, 2014), and product reviews (Villalba and Saint-Dizier, 2012; Yessenalina et al., 2010). In addition, some works based on machinelearning techniques, used the same topic in training and testing (Rosenfeld and Kraus, 2015; Boltuˇzi´c and ˇ Snajder, 2014), relying on features from the topic itself in identifying arguments. In contrast, here, we focus on detecting an essen"
D15-1050,P05-1045,0,0.0257917,"ed a lexicon of words characterizing this type by looking at examples from the held-out data. This resulted with high–precision / low–recall lexicons. For example, for type Expert we used a lexicon of words describing persons and organizations that may have some relevant expertise, such as: economist, philosopher, court. In addition, we used the held-out data to automatically learn wider lexicons of words that are significantly associated with each type. All the in-house lexicons are described in detail in the supplementary material. • Named Entity Recognition (NER). We used the Stanford NER (Finkel et al., 2005) to extract named entities such as person and organization, and an in-house NER (Lally et al., 2012) to extract more fine grained categories such as ”educational organization” and ”leader”. • Patterns. We used regular expressions to represent features like: does that candidate contain a quote; does it contain a citation; does it contain numeric quantitative results. In addition, we generated complex regular expressions which combine the above lexicons with NER results to capture patterns indicative of different types. For example, the pattern [Person/organization, 0 to 10 wildcard words, an op"
D15-1050,W14-2106,0,0.0436803,"umber of articles in which at least one CDE was found; the total number of CDE detected for each type; the average percent of claims for which at least one CDE was found; and for these claims, the average number of CDE found. Note that the total number of CDE is not a simple sum of the CDE per type, as CDE can be assigned with more than one type. Standard deviations of distribution across topics are given in parenthesis where relevant. guments (Mochales Palau and Moens, 2009), analyzing argument structure (Peldszus, 2014), and identifying relations between arguments (Cabrio and Villata, 2012; Ghosh et al., 2014). Other works focused on specific domains such as evidence-based legal documents (Mochales Palau and Moens, 2011; Ashley and Walker, 2013), online debates (Cabrio and Villata, ˇ 2012; Boltuˇzi´c and Snajder, 2014), and product reviews (Villalba and Saint-Dizier, 2012; Yessenalina et al., 2010). In addition, some works based on machinelearning techniques, used the same topic in training and testing (Rosenfeld and Kraus, 2015; Boltuˇzi´c and ˇ Snajder, 2014), relying on features from the topic itself in identifying arguments. In contrast, here, we focus on detecting an essential constituent of a"
D15-1050,W14-2105,0,0.23984,"luation of each type, topics that had less than three CDE of that type. This leaves a total of 30, 37, and 22 topics for types Expert, Study, and Anecdotal, respectively. The current work is the first to report results over these CDE data, which are more than 4 times larger compared to the data released in (Aharoni et al., 2014). These data are now freely available for research purposes 4 . 4 of the relevant Evidence type; and finally, of course, it should support the claim. In addition to these observations, we note that a priori, we do not expect all claims to be supported by all CDE types (Park and Cardie, 2014). For example, opinion claims like claim B in Table 1 are expected to be less supported by Study evidence compared to factual claims, like claim A in Table 1. Moreover, as evident from Table 2, many claims do not have any associated CDE in the same article. Thus, the system performance may naturally improve if it will propose candidate CDE of a particular type, only to an automatically identified subset of the input claims. Based on these observations, we are led to suggest an architecture which approaches CDED via a pipeline of modular components. Each of these components relies upon the resu"
D15-1050,W14-2112,0,0.0117282,"etermines the number of claims considered for each type. The next columns indicate the number of articles in which at least one CDE was found; the total number of CDE detected for each type; the average percent of claims for which at least one CDE was found; and for these claims, the average number of CDE found. Note that the total number of CDE is not a simple sum of the CDE per type, as CDE can be assigned with more than one type. Standard deviations of distribution across topics are given in parenthesis where relevant. guments (Mochales Palau and Moens, 2009), analyzing argument structure (Peldszus, 2014), and identifying relations between arguments (Cabrio and Villata, 2012; Ghosh et al., 2014). Other works focused on specific domains such as evidence-based legal documents (Mochales Palau and Moens, 2011; Ashley and Walker, 2013), online debates (Cabrio and Villata, ˇ 2012; Boltuˇzi´c and Snajder, 2014), and product reviews (Villalba and Saint-Dizier, 2012; Yessenalina et al., 2010). In addition, some works based on machinelearning techniques, used the same topic in training and testing (Rosenfeld and Kraus, 2015; Boltuˇzi´c and ˇ Snajder, 2014), relying on features from the topic itself in i"
D15-1050,C14-1141,1,0.877203,"earlier defined in (Aharoni et al., 2014) and we use the same definitions here. Topic: a short phrase that frames the discussion. Claim: a general, concise statement that directly supports or contests the topic. Context Dependent Evidence (CDE): a text segment that directly supports a claim in the context of the topic. The first three rows of Table 1 show examples of a topic, a claim and CDE. For the purpose of this work, we assume that we are given a concrete topic, a relevant claim, and potentially relevant documents, provided either manually or by automatic methods (Cartright et al., 2011; Levy et al., 2014). Our task, which we term Context Dependent Evidence Detection (CDED), is to automatically pinpoint CDE within these documents. We further require that a detected CDE is reasonably well phrased, and easily understandable in the given context, so that it can be instantly and naturally used to support the claim in a discussion. Table 1 gives examples of valid CDE (V) and non-valid CDE (X) according to the definition mentioned above. It is well recognized that one can support a claim using different types of evidence (Rieke and Sillars, 2001; Seech, 2008). Furthermore, for different use cases, di"
D15-1050,P10-2062,0,0.0143555,"DE per type, as CDE can be assigned with more than one type. Standard deviations of distribution across topics are given in parenthesis where relevant. guments (Mochales Palau and Moens, 2009), analyzing argument structure (Peldszus, 2014), and identifying relations between arguments (Cabrio and Villata, 2012; Ghosh et al., 2014). Other works focused on specific domains such as evidence-based legal documents (Mochales Palau and Moens, 2011; Ashley and Walker, 2013), online debates (Cabrio and Villata, ˇ 2012; Boltuˇzi´c and Snajder, 2014), and product reviews (Villalba and Saint-Dizier, 2012; Yessenalina et al., 2010). In addition, some works based on machinelearning techniques, used the same topic in training and testing (Rosenfeld and Kraus, 2015; Boltuˇzi´c and ˇ Snajder, 2014), relying on features from the topic itself in identifying arguments. In contrast, here, we focus on detecting an essential constituent of an argument – the evidence – rather then detecting whole arguments, or detecting other argument parts like claims (Levy et al., 2014; Lippi and Torroni, 2015). In addition, we do not limit ourselves to a particular domain, nor assume that the topic of the discussion is known in advance. Finally"
D15-1050,W14-2109,1,\N,Missing
D18-1255,W17-5506,0,0.01802,"g as a sequence generation task. To make the output of these models more coherent, there is an increasing effort in integrating external background knowledge with these models. This is because human beings rely on background knowledge for conversations as well as other tasks (Schallert, 2002). There has been considerable work on incorporating background knowledge in the context of goal-oriented dialog datasets even before the advent of large-scale datasets for deep learning (Raux et al., 2005; Seneff et al., 1991) as well as in recent times (Rojas-Barahona et al., 2017; Williams et al., 2016; Eric et al., 2017) where datasets include small sized knowledge graphs as background knowledge. However, the conversations in these datasets are very templated and nowhere close to open conversations in specific domains such as the ones contained in our dataset. Even in the case of open domain conversations, there are some works which have integrated external knowledge sources. Most of the entries in 2017 Amazon Alexa Prize (Ram et al., 2017) relied on background knowledge for meaningful response generation. Milabot (Serban et al., 2017a) and even the winning entry SoundingBoard (Liu et al., 2018) used Reddit p"
D18-1255,P17-1162,0,0.013196,"e. The infusion of external knowledge in both these works is post facto (as opposed to our work where we take a bottom-up approach and explicitly create a dataset which allows exploitation of background knowledge). Additionally, existing large-scale datasets are noisy as they are extracted from online forums which are inherently noisy. In contrast, since we use crowdsourcing, the extent of noise is reduced since there are humans in the loop who were explicitly instructed to use only clean sentences from the external knowledge sources. We would also like to mention some existing works such as (He et al., 2017; Lewis et al., 2017; Krause et al., 2017) which have used crowdsourcing for creating conversation datasets. In fact, our data collection method is inspired by the work of Krause et al. (2017) where the authors use selfdialogs to collect conversation data about movies, music and sports. They are referred to as selfdialogs because the same worker plays the role of both parties in the conversation. However, our work differs from Krause et al. (2017) as we provide explicit background knowledge sources to the workers from where they can copy text with the addition of suitable prefixes and suffixes"
D18-1255,W14-4337,0,0.0351841,"Missing"
D18-1255,D17-1259,0,0.0232124,"f external knowledge in both these works is post facto (as opposed to our work where we take a bottom-up approach and explicitly create a dataset which allows exploitation of background knowledge). Additionally, existing large-scale datasets are noisy as they are extracted from online forums which are inherently noisy. In contrast, since we use crowdsourcing, the extent of noise is reduced since there are humans in the loop who were explicitly instructed to use only clean sentences from the external knowledge sources. We would also like to mention some existing works such as (He et al., 2017; Lewis et al., 2017; Krause et al., 2017) which have used crowdsourcing for creating conversation datasets. In fact, our data collection method is inspired by the work of Krause et al. (2017) where the authors use selfdialogs to collect conversation data about movies, music and sports. They are referred to as selfdialogs because the same worker plays the role of both parties in the conversation. However, our work differs from Krause et al. (2017) as we provide explicit background knowledge sources to the workers from where they can copy text with the addition of suitable prefixes and suffixes to generate appropr"
D18-1255,P16-1094,0,0.0456525,"interest in building datasets (Serban et al., 2015) for training dialog systems. Some of these datasets contain transcripts of human-bot conversations (Williams et al., 2013; Henderson et al., 2014a,b) while others are created using a fixed set of natural language patterns (Bordes and Weston, 2017; Dodge et al., 2016). The advent of deep learning created 2323 interest in the construction of large-scale dialog datasets (Lowe et al., 2015b; Ritter et al., 2010; Sordoni et al., 2015) leading to the development of several end-to-end conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2016) which treat dialog as a sequence generation task. To make the output of these models more coherent, there is an increasing effort in integrating external background knowledge with these models. This is because human beings rely on background knowledge for conversations as well as other tasks (Schallert, 2002). There has been considerable work on incorporating background knowledge in the context of goal-oriented dialog datasets even before the advent of large-scale datasets for deep learning (Raux et al., 2005; Seneff et al., 1991) as well as in recent times (Rojas-Baraho"
D18-1255,W15-4640,0,0.345733,"ion from the background knowledge when required and (iii) span prediction based models which predict the appropriate response span in the background knowledge. 1 Introduction Background knowledge plays a very important role in human conversations. For example, to have a meaningful conversation about a movie, one uses their knowledge about the plot, reviews, comments and facts about the movie. A typical conversation involves recalling important points from this background knowledge and producing them appropriately in the context of the conversation. However, most existing large scale datasets (Lowe et al., 2015b; Ritter et al., 2010; Serban et al., 2016) simply contain a sequence of utterances and responses without any explicit background knowledge associated with them. This has led to the development of models which treat conversation as a simple sequence-to-sequence generation task and often produce output which is both syntactically incorrect and incoherent (off topic). To make conversations more coherent, there is an increasing interest in integrating structured and unstructured knowledge sources with neural conversation models. While there are already some works in this direction (Rojas-Barahon"
D18-1255,N10-1020,0,0.49526,"und knowledge when required and (iii) span prediction based models which predict the appropriate response span in the background knowledge. 1 Introduction Background knowledge plays a very important role in human conversations. For example, to have a meaningful conversation about a movie, one uses their knowledge about the plot, reviews, comments and facts about the movie. A typical conversation involves recalling important points from this background knowledge and producing them appropriately in the context of the conversation. However, most existing large scale datasets (Lowe et al., 2015b; Ritter et al., 2010; Serban et al., 2016) simply contain a sequence of utterances and responses without any explicit background knowledge associated with them. This has led to the development of models which treat conversation as a simple sequence-to-sequence generation task and often produce output which is both syntactically incorrect and incoherent (off topic). To make conversations more coherent, there is an increasing interest in integrating structured and unstructured knowledge sources with neural conversation models. While there are already some works in this direction (Rojas-Barahona et al., 2017; Willia"
D18-1255,E17-1042,0,0.28817,"ning entry SoundingBoard (Liu et al., 2018) used Reddit pages, Amazon’s Evi Service, and large databases like OMDB, Google Knowledge Graph and Wikidata as external knowledge. The submission named Eigen (Guss et al., 2017) used several dialog datasets and corpora belonging to related Natural Language Processing tasks to make their responses more informative. We refer the reader to (Ram et al., 2017) for detailed analysis of these systems. In the space of academic datasets, Lowe et al. (2015a) report results on the Ubuntu dataset using manpages as external knowledge whereas Ghazvininejad et al. (2017) use Foursquare tips as external knowledge for social media conversations. However, unlike our work both these works do not create a new dataset where the responses are explicitly linked to a knowledge source. The infusion of external knowledge in both these works is post facto (as opposed to our work where we take a bottom-up approach and explicitly create a dataset which allows exploitation of background knowledge). Additionally, existing large-scale datasets are noisy as they are extracted from online forums which are inherently noisy. In contrast, since we use crowdsourcing, the extent of"
D18-1255,P17-1099,0,0.0632883,"rent Encoder Decoder model (HRED) (Serban et al., 2016) instead of its variant (Serban et al., 2017b) 2326 as the standard model performs only slightly poorly than the variant and is much easier to implement. It decomposes the context of the conversation as two level hierarchy using Recurrent Neural Networks (RNN). The lower RNN encodes individual utterances (sequence of words) which is then fed into the higher level RNN as a sequence of utterances. The decoder RNN then generates the output based on this hierarchical context representation. 4.2 Generate-or-Copy models Get To The Point (GTTP) (See et al., 2017) proposed a hybrid pointer generator network for abstractive summarization that learns to copy words from the source document when required and otherwise generates a word like any sequence-tosequence model. In the summarization task, the input is a document and the output is a summary whereas in our case the input is a {document, context} pair and the output is a response. Here, the context includes the previous two utterances and the current utterance. We modified the architecture to suit our task. We use an RNN to compute the representation of the document (like the original model) and intro"
D18-1255,P15-1152,0,0.163981,"Missing"
D18-1255,N15-1020,0,0.126787,"Missing"
D18-1255,P17-1018,0,0.0305196,"AF and GTTP will use {resource, context, response} as training data with relevant span instead of response for BiDAF. 5.3 Merging resources into a single document As stated earlier, we simply merge all the background information to create a single document which we collectively refer to as resource. For the BiDAF model, we had to restrict the length of the resource to 256 words because we found that even on a K80 GPU with 12GB RAM, this model gives an out of memory error for longer 2327 documents. We found this to be a severe limitation of this and other span based models (for example, R-Net (Wang et al., 2017)) . We experimented with three methods of creating this resource. The first method oracle uses the actual resource (plot or comments or reviews) from which the next response was generated as a resource. If that resource itself has more than 256 words then we truncate it from the beginning and the end such that the span containing the actual response is contained within the retained 256 words. The number of words that are discarded from the start or the end is chosen at random so that the correct spans do not end up in similar positions throughout the dataset. The next two methods mixed-short a"
D18-1255,W13-4065,0,0.0158245,"the copy-and-generate paradigm wherein the model tries to copy text from the given resources whenever appropriate and generate it otherwise. The third paradigm borrows from the span prediction based models which are predominantly being used for Question Answering (QA). These baseline results along with the dataset would hopefully shape future research in the area of background aware conversation models. 2 Related Work There has been an active interest in building datasets (Serban et al., 2015) for training dialog systems. Some of these datasets contain transcripts of human-bot conversations (Williams et al., 2013; Henderson et al., 2014a,b) while others are created using a fixed set of natural language patterns (Bordes and Weston, 2017; Dodge et al., 2016). The advent of deep learning created 2323 interest in the construction of large-scale dialog datasets (Lowe et al., 2015b; Ritter et al., 2010; Sordoni et al., 2015) leading to the development of several end-to-end conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2016) which treat dialog as a sequence generation task. To make the output of these models more coherent, there is an increasing effort in int"
D18-1429,D17-1219,0,0.124845,"Missing"
D18-1429,P17-1123,0,0.220317,"Missing"
D18-1429,D17-1090,0,0.136607,"Missing"
D18-1429,N10-1086,0,0.28509,"Missing"
D18-1429,C04-1046,0,0.242878,"Missing"
D18-1429,P17-1147,0,0.0805827,"Missing"
D18-1429,P15-1086,0,0.228462,"Missing"
D18-1429,D09-1030,0,0.110294,"Missing"
D18-1429,E06-1032,0,0.401796,"Missing"
D18-1429,W04-1013,0,0.159719,"Missing"
D18-1429,D16-1230,0,0.117786,"Missing"
D18-1429,W02-0109,0,0.213164,"Missing"
D18-1429,P17-1103,0,0.0535454,"Missing"
D18-1429,D16-1147,0,0.0649106,"Missing"
D18-1429,E17-1036,1,0.899988,"Missing"
D18-1429,P18-1156,1,0.89147,"Missing"
D18-1429,P16-1056,0,0.0775893,"Missing"
D18-1429,P17-1096,0,0.0649731,"Missing"
D18-1429,P02-1040,0,\N,Missing
D18-1429,D16-1264,0,\N,Missing
D18-1429,W13-2114,0,\N,Missing
D19-1326,D17-1219,0,0.0455352,"RefNet(Originality), there is an improvement in the performance where the overlap with the passage was less (as shown in blue in Figure 3).As shown in Table 8, although both questions are answerable given the passage, the question generated from Reward-RefNet(Originality) is better. 5 RewardRelated Work Early works on Question Generation were essentially rule based systems (Heilman and Smith, 2010; Mostow and Chen, 2009; Lindberg et al., 2013; Labutov et al., 2015). Current models for AQG are based on the encode-attend-decode paradigm and they either generate questions from the passage alone (Du and Cardie, 2017; Du et al., 2017; Yao et al., 2018) or from the passage and a given answer (in which case the generated question must result in the given answer). Over the past couple of years, several variants of the encodeattend-decode model have been proposed. For example, (Zhou et al., 2018) proposed a sequential copying mechanism to explicitly select a sub-span from the passage. Similarly, (Zhao et al., 2018) mainly focuses on efficiently incorporating paragraph level content by using Gated Self Attention and Maxout pointer networks. Some works (Yuan et al., 2017) even use Question Answering as a metric"
D19-1326,P17-1123,0,0.0960714,"there is an improvement in the performance where the overlap with the passage was less (as shown in blue in Figure 3).As shown in Table 8, although both questions are answerable given the passage, the question generated from Reward-RefNet(Originality) is better. 5 RewardRelated Work Early works on Question Generation were essentially rule based systems (Heilman and Smith, 2010; Mostow and Chen, 2009; Lindberg et al., 2013; Labutov et al., 2015). Current models for AQG are based on the encode-attend-decode paradigm and they either generate questions from the passage alone (Du and Cardie, 2017; Du et al., 2017; Yao et al., 2018) or from the passage and a given answer (in which case the generated question must result in the given answer). Over the past couple of years, several variants of the encodeattend-decode model have been proposed. For example, (Zhou et al., 2018) proposed a sequential copying mechanism to explicitly select a sub-span from the passage. Similarly, (Zhao et al., 2018) mainly focuses on efficiently incorporating paragraph level content by using Gated Self Attention and Maxout pointer networks. Some works (Yuan et al., 2017) even use Question Answering as a metric to evaluate the"
D19-1326,N19-1246,0,0.0556086,"Missing"
D19-1326,N10-1086,0,0.87538,"ples of generated questions from Baseline, RefNet and Reward-RefNet model on the SQuAD dataset. Answers are shown in blue . Introduction Over the past few years, there has been a growing interest in Automatic Question Generation (AQG) ∗ * The first two authors have contributed equally to this work. 1 https://github.com/PrekshaNema25/RefNet-QG from text - the task of generating a question from a passage and optionally an answer. AQG is used in curating Question Answering datasets, enhancing user experience in conversational AI systems (Shum et al., 2018) and for creating educational materials (Heilman and Smith, 2010). For the above applications, it is essential that the questions are (i) grammatically correct (ii) answerable from the passage and (iii) specific to the answer. Existing approaches focus on encoding the passage, the answer and the relationship between them using complex functions and then generate the question in one single pass. However, by carefully analysing the generated questions, we observe that these approaches tend to miss one or more of the important aspects of the question. For instance, in Table 1, the question generated by the single-pass baseline model for the first passage is gr"
D19-1326,P15-1086,0,0.172591,"y, we explicitly reward our model for having low n-gram score with the passage as compared to the initial draft. As a result we observe that with RewardRefNet(Originality), there is an improvement in the performance where the overlap with the passage was less (as shown in blue in Figure 3).As shown in Table 8, although both questions are answerable given the passage, the question generated from Reward-RefNet(Originality) is better. 5 RewardRelated Work Early works on Question Generation were essentially rule based systems (Heilman and Smith, 2010; Mostow and Chen, 2009; Lindberg et al., 2013; Labutov et al., 2015). Current models for AQG are based on the encode-attend-decode paradigm and they either generate questions from the passage alone (Du and Cardie, 2017; Du et al., 2017; Yao et al., 2018) or from the passage and a given answer (in which case the generated question must result in the given answer). Over the past couple of years, several variants of the encodeattend-decode model have been proposed. For example, (Zhou et al., 2018) proposed a sequential copying mechanism to explicitly select a sub-span from the passage. Similarly, (Zhao et al., 2018) mainly focuses on efficiently incorporating par"
D19-1326,W04-1013,0,0.0354355,"(both) respectively. We take the top 30, 000 frequent words as the vocabulary. We use Adam optimizer with a learning rate of 0.0004 and train our models for 10 epochs using cross entropy loss. For the Reward-RefNet model, we fine-tune the pretrained model with the loss function mentioned in Section 2.3 for 3 epochs. The best model is chosen based on the BLEU (Papineni et al., 2002) score on the validation split. For all the results we use beam search decoding with a beam size of 5. 3.3 Evaluation We evaluate our models, based on n-gram similarity metrics BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Lavie and Denkowski, 2009) using the package released in (Sharma et al., 2017)2 . We also quantify the answerability of our models using QBLEU43 (Nema and Khapra, 2018). 2 https://github.com/Maluuba/nlg-eval https://github.com/PrekshaNema25/ Answerability-Metric 3318 3 4 Passage: Before the freeze ended in 1952, there were only 108 existing television stations in the United States; a few major cities (such as Boston) had only two television stations, ... Questions EAD: how many television stations existed in boston ? RefNet: how many television stations did boston have in the uni"
D19-1326,W13-2114,0,0.258419,"generated on originality, we explicitly reward our model for having low n-gram score with the passage as compared to the initial draft. As a result we observe that with RewardRefNet(Originality), there is an improvement in the performance where the overlap with the passage was less (as shown in blue in Figure 3).As shown in Table 8, although both questions are answerable given the passage, the question generated from Reward-RefNet(Originality) is better. 5 RewardRelated Work Early works on Question Generation were essentially rule based systems (Heilman and Smith, 2010; Mostow and Chen, 2009; Lindberg et al., 2013; Labutov et al., 2015). Current models for AQG are based on the encode-attend-decode paradigm and they either generate questions from the passage alone (Du and Cardie, 2017; Du et al., 2017; Yao et al., 2018) or from the passage and a given answer (in which case the generated question must result in the given answer). Over the past couple of years, several variants of the encodeattend-decode model have been proposed. For example, (Zhou et al., 2018) proposed a sequential copying mechanism to explicitly select a sub-span from the passage. Similarly, (Zhao et al., 2018) mainly focuses on effici"
D19-1326,D18-1429,1,0.921845,"output of the first decoder. We do so by computing a context vector ct as where Wc0 is a weight matrix and Wo is the output matrix which is shared with the Preliminary decoder (Equation 2). Note that RefNet generates ˜ and two variants of the question : initial draft Q final draft Q. We compare these two versions of the generated questions in Section 4. 2.3 Reward-RefNet Next, we address the following question: Can the refinement decoder be explicitly rewarded for generating a question which is better than that generated by the preliminary decoder on certain desired parameters? For example, (Nema and Khapra, 2018) define fluency and answerability as desired qualities in the generated question. They evaluate fluency using BLEU score and answerability using a score which captures whether the question contains the required {named entities, important words, function words, question types} (and is thus answerable). We use these fluency and answerability scores proposed by (Nema and Khapra, 2018) as reward signals. We first compute ˜ and r(Q) for the question genthe reward r(Q) erated by the preliminary and refinement decoder respectively. We then use “REINFORCE with a baseline” algorithm (Williams, 1992) to"
D19-1326,P02-1040,0,0.104562,"hen projected to 100 dimensions. For answertagging, we use embedding size of 3. The hidden size for all the LSTMs is fixed to 512. We use 2layer, 1-layer and 2-layer stacked BiLSTM for the passage encoder, answer encoder and the decoders (both) respectively. We take the top 30, 000 frequent words as the vocabulary. We use Adam optimizer with a learning rate of 0.0004 and train our models for 10 epochs using cross entropy loss. For the Reward-RefNet model, we fine-tune the pretrained model with the loss function mentioned in Section 2.3 for 3 epochs. The best model is chosen based on the BLEU (Papineni et al., 2002) score on the validation split. For all the results we use beam search decoding with a beam size of 5. 3.3 Evaluation We evaluate our models, based on n-gram similarity metrics BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Lavie and Denkowski, 2009) using the package released in (Sharma et al., 2017)2 . We also quantify the answerability of our models using QBLEU43 (Nema and Khapra, 2018). 2 https://github.com/Maluuba/nlg-eval https://github.com/PrekshaNema25/ Answerability-Metric 3318 3 4 Passage: Before the freeze ended in 1952, there were only 108 existing television statio"
D19-1326,D14-1162,0,0.0826147,"ons are fused together at every layer. 3315 Preliminary Decoder Preliminary Decoder Encoder Attention (A1) Passage-Answer Fusion Attention on Encoder (A2) Attention on Preliminary Decoder(A3) Dual Attention Network Passage Encoder Word Embedding Answer Encoder Char Embedding Answer Tagging Output from Module Refinement Decoder Input to Module Figure 1: Our RefNet model with Preliminary and Refinement Decoder. Embedding Layer: In this layer, we compute a d-dimensional embedding for every word in the passage and the answer. This embedding is obtained by concatenating the word’s Glove embedding (Pennington et al., 2014) with its character based embedding as discussed in (Seo et al., 2016). Additionally, for passage words, we also compute a positional embedding based on the relative position of the word w.r.t. the answer span as described in (Zhao et al., 2018). For every passage word, this positional embedding is also concatenated to the word and character based embeddings. We discuss the impact of character embeddings and answer tagging in Appendix A. In the subsequent sections, we will refer to embedding of the i-th passage word wip as e(wip ) and the j-th answer word wja as e(wja ). Contextual Layer: In t"
D19-1326,E17-1036,1,0.831254,"ariants of the encodeattend-decode model have been proposed. For example, (Zhou et al., 2018) proposed a sequential copying mechanism to explicitly select a sub-span from the passage. Similarly, (Zhao et al., 2018) mainly focuses on efficiently incorporating paragraph level content by using Gated Self Attention and Maxout pointer networks. Some works (Yuan et al., 2017) even use Question Answering as a metric to evaluate the generated questions. There has also been some work on generating questions from images (Jain et al., 2017; Li et al., 2017) and from knowledge bases (Serban et al., 2016; Reddy et al., 2017). The idea of multi pass decoding which is central to our work has been used by (Xia et al., 2017) for machine translation and text summarization albeit with a different objective. Some works have also augmented seq2seq models (Rennie et al., 2017; Paulus et al., 2018; Song et al., 2017) with external reward signals using REINFORCE with baseline algorithm (Williams, 1992). The typical rewards used in these works are BLEU and ROUGE scores. Our REINFORCE loss is different from the previous ones as it uses the first decoder’s reward as the baseline instead of reward of the greedy policy. 3321 6 C"
D19-1326,D18-1259,0,0.083455,"Missing"
D19-1326,P17-1099,0,0.0604514,"ed attention weights computed by attention network A3 . The hidden state of the refinement decoder at time t is computed as follows: hdt = LSTM([e(qt−1 ); ct−1 ; gt−1 ; ha ], hdt−1 ) Finally, qt is predicted using qt = arg max(softmax(Wo [W0 c [hdt ; ct−1 ; gt−1 ])) q ˜ are the rewards obtained by where r(Q) and r(Q) comparing with the reference question Q∗ . As mentioned, this reward r(.) can be the fluency score or answerability score as defined by (Nema and Khapra, 2018). 2.4 Copy Module Along with the above-mentioned three modules, we adopt the pointer-network and coverage mechanism from (See et al., 2017). We use it to (i) handle Out-of-Vocabulary words and (ii) avoid repeating phrases in the generated questions. 3317 Dataset SQuAD (Sentence Level) SQuAD (Passage Level) HOTPOT DROP Dataset Model (Sun et al., 2018) (Zhao et al., 2018) (Kim et al., 2019) EAD RefNet (Zhao et al., 2018) EAD RefNet (Zhao et al., 2018)* EAD RefNet (Zhao et al., 2018)* EAD RefNet BLEU-1 43.02 44.51 44.74 47.27 45.07 44.61 46.41 45.29 46.00 45.45 39.56 39.21 42.81 n-gram BLEU-3 BLEU-4 20.51 15.64 21.06 15.82 16.17 22.00 16.84 23.65 18.16 21.60 16.38 21.50 16.36 22.42 16.99 24.43 19.29 24.82 19.68 26.05 21.17 22.53 18."
D19-1326,P16-1056,0,0.0527615,"Missing"
D19-1326,D18-1424,0,0.764607,"r Encoder Char Embedding Answer Tagging Output from Module Refinement Decoder Input to Module Figure 1: Our RefNet model with Preliminary and Refinement Decoder. Embedding Layer: In this layer, we compute a d-dimensional embedding for every word in the passage and the answer. This embedding is obtained by concatenating the word’s Glove embedding (Pennington et al., 2014) with its character based embedding as discussed in (Seo et al., 2016). Additionally, for passage words, we also compute a positional embedding based on the relative position of the word w.r.t. the answer span as described in (Zhao et al., 2018). For every passage word, this positional embedding is also concatenated to the word and character based embeddings. We discuss the impact of character embeddings and answer tagging in Appendix A. In the subsequent sections, we will refer to embedding of the i-th passage word wip as e(wip ) and the j-th answer word wja as e(wja ). Contextual Layer: In this layer, we compute a contextualized representation for every word in the passage by passing the word embeddings (as computed above) through a bidirectional-LSTM (Hochreiter and Schmidhuber, 1997): − → − → hpt = LSTM(e(wtp ), hp t−1 ) ∀t ∈ [1,"
D19-1326,D18-1427,0,0.437931,"Missing"
D19-1326,W17-2603,0,\N,Missing
E17-1036,2010.jeptalnrecital-court.36,0,0.0242881,"proposed in the literature along with their limitations and then discuss the work around generation of questions answer pairs from knowledge graph. A number of papers have looked at the problem of generating vocabulary questions using WordNet (Miller et al., 1990) and distributional similarity techniques (Brown et al., 2005; Heilman and Eskenazi, 2007). There are numerous works in automatic question generation from text. Many proposed methods are syntax based methods that use the parse structure of sentences, identify key phrases and apply some known transformation rules to create questions (Ali et al., 2010; Kalady et al., 2010; Varga, 2010). Mannem et al. (2010) further use semantic role labeling for transformation rules. There are also template based methods proposed where a question template is a predefined text with placeholder variables to be replaced with content from source text. Cai et al. (2006) propose an XML markup language that is used to manually create question templates. This is sensitive to the performance of syntactic and semantic parsing. Heilman and Smith (2010) use a rule based approach to transform a declarative sentence into several candidate questions and then rank them us"
E17-1036,W14-4012,0,0.0156833,"Missing"
E17-1036,P02-1040,0,0.119582,"d earlier, we also do a manual evaluation. For this, we randomly selected 700 questions from the test set. We showed the questions generated by the three methods to different human annotators and asked them to assign a score of 1 to 4 to each question (based on the guidelines described earlier). The evaluators had no knowledge about the method used to generate each question shown to them. We only consider questions with rating 4 (perfect without any errors) for each Evaluation metrics We evaluate the performance of K2Q RNN with other baselines to compare the K2Q approaches, we use BLEU score (Papineni et al., 2002) between the generated question and the reference question. BLEU score is typically used in evaluating the performance of MT systems and captures the average n-gram overlap between the generated sequence and the reference sequence. We consider n-grams upto length 4. BLEU score does not capture the true performance of the system. For example, if the trained model simply reproduces all keywords in the generated question then also the unigram overlap will be high resulting in a higher 381 Ground truth K2Q-PBSMT K2Q-RNN pitching in baseball ? difference between mergeracqs and amalgamation ? did gr"
E17-1036,D14-1116,0,0.0269293,"them. Freebase1 is one such knowledge graph that describes and organizes more than 3 billion facts in a consistent ontology. Knowledge graphs usually capture relationships between different things that can be viewed as triples (for example, CEO(Sundar Pichai, Google)). Such triples are often referred to as facts and can be used for answering factoid questions. For example, the above triple can be used to answer the question “Who is the CEO of Google ?”. It is not surprising that knowledge graphs are increasingly used for building Question Answering systems (Ferrucci, 2012; Yahya et al., 2013; He et al., 2014; Zou et al., 2014). In this paper, we focus on exploiting knowledge graphs for a related but different purpose. We propose that such triples or facts can be used for automatically generating Question Answer (QA) pairs. The generated QA pairs can then be used in certain downstream applications. For example, if some domain-specific knowledge graphs are available (such as History, Geography) then such QA pairs generated from them can be used for developing quiz systems for educational purposes. We now formally define the problem and then illustrate it with the help of an example. Consider a trip"
E17-1036,W15-4645,1,0.879178,"Missing"
E17-1036,N10-1086,0,0.538131,"at use the parse structure of sentences, identify key phrases and apply some known transformation rules to create questions (Ali et al., 2010; Kalady et al., 2010; Varga, 2010). Mannem et al. (2010) further use semantic role labeling for transformation rules. There are also template based methods proposed where a question template is a predefined text with placeholder variables to be replaced with content from source text. Cai et al. (2006) propose an XML markup language that is used to manually create question templates. This is sensitive to the performance of syntactic and semantic parsing. Heilman and Smith (2010) use a rule based approach to transform a declarative sentence into several candidate questions and then rank them using a logistic regression model. These approaches involve creating templates manually and thus require huge manual work and have low recall. A problem that has been studied recently and is similar to our problem of generating questions using knowledge graph is that of generating questions from Web queries. The motivation here is to automatically generate questions from queries for community-based question answering services such as Yahoo! Answers and WikiAnswers. The idea was fi"
E17-1036,P82-1020,0,0.818633,"Missing"
E17-1036,P14-1133,0,0.0780664,"hich interesting and meaningful questions can be constructed. For example, no interesting questions can be constructed from the triple wikipage page ID(Google, 57570) and hence we should eliminate such triples. Further, even for an interesting triple, it may be possible to use only certain subsets of keywords to construct • We train our model using 1M questions from WikiAnswers thereby ensuring that it is not tied to any specific knowledge graph. • Finally, we show that appending the automatically generated QA pairs to existing training data for training a state of the art QA system (Jonathan Berant, 2014) improves the performance of the QA system by 5.5 percent (relative). The remainder of this paper is organized as follows. In next section, we describe related work, followed by a description of our overall approach for extracting keywords from triples and generating natural language question answer pairs from them. We then describe the experiments performed to evaluate our system and then end with concluding remarks. 377 2 Related Work correct and diverse questions from a given query where the candidate questions are generated using the approach proposed by (Zhao et al., 2011). These approach"
E17-1036,N03-1033,0,0.024634,"as follows: p(qj |q&lt;j , hm ) = Θ(qj−1 , gj , hm ), Datasets For training the K2Q-RNN model we require a set of keywords and question pairs. We use a large collection of open-domain questions available from WikiAnswers dataset2 . This dataset has around 20M questions. We randomly selected 1M questions from this corpus for training and 5k questions for testing (the maximum length of a question was restricted to 50 words). We extract keywords from the selected questions by retaining only Nouns, Verbs and Adjectives in the question. The parts of speech tags were identified using Stanford Tagger (Toutanova et al., 2003). We form an ordered sequence of keywords by retaining these extracted words in the same order in which they appear in the original question. This sequence of keywords along with the original question forms one input-output sequence pair for training. where, hi ∈ &lt;n is the hidden representation at position i. hm is the final encoded hidden state vector for this sequence. We use LSTM units (Hochreiter and Schmidhuber, 1997) as Φ for our implementation based on its recent success in language processing tasks (Bahdanau et al., 2015). The function of the decoder is to compute the probability of th"
E17-1036,P07-2045,0,0.0108877,"ned the model for 10 epochs. We used the beam search with the beam size to 12 to generate the question that approximately maximizes conditional probability defined in Equation 2. K2Q-PBSMT: As mentioned earlier, we treat the problem of generating questions from keywords as a sequence to sequence translation problem. A Phrase Based Machine Translation System (PBSMT) can also be employed for this task by considering that the keyword sequences belong to a source language and the question sequences belong to a target language. We compare our approach with a standard phrase-based MT system, MOSES (Koehn et al., 2007) trained using the same 1M sequence pairs constructed from the WikiAnswers dataset. We used a 5-gram language model trained on the 1M target question sequences and tuned the parameters of the decoder using 1000 held-out sequence (these were held out from the 1M training pairs). K2Q-Template: For template based approach we use the method proposed by (Zhao et al., 2011) along with the Word2Vec (Mikolov et al., 2015) ranking as proposed by (Raghu et al., 2015). The Word2Vec ranking provides better generalization than the ranking proposed by (Zhao et al., 2011). We learn the templates using the sa"
E17-1036,I11-1104,0,0.0247631,"QA system (Jonathan Berant, 2014) improves the performance of the QA system by 5.5 percent (relative). The remainder of this paper is organized as follows. In next section, we describe related work, followed by a description of our overall approach for extracting keywords from triples and generating natural language question answer pairs from them. We then describe the experiments performed to evaluate our system and then end with concluding remarks. 377 2 Related Work correct and diverse questions from a given query where the candidate questions are generated using the approach proposed by (Zhao et al., 2011). These approaches use millions of query question pairs to learn question templates and thus have better generalization performance compared to earlier methods where templates were learnt manually. Recently, Seyler et al. (2015) proposed a method to generate natural language questions from knowledge graphs given a topic of interest. They also provide a method to estimate difficulty of generated questions. The generation of question is done by manually created template patterns and therefore is limited in application. In contrast we propose an RNN based method to learn generation of natural lan"
E17-1036,I11-1106,0,0.0297689,"didate questions and then rank them using a logistic regression model. These approaches involve creating templates manually and thus require huge manual work and have low recall. A problem that has been studied recently and is similar to our problem of generating questions using knowledge graph is that of generating questions from Web queries. The motivation here is to automatically generate questions from queries for community-based question answering services such as Yahoo! Answers and WikiAnswers. The idea was first suggested by (Lin, 2008) and further developed by (Zhao et al., 2011) and (Zheng et al., 2011). Both of these approaches are template based approaches where the templates are learnt using a huge question corpus along with query logs. Dror et al. (2013) further proposed a learning to rank based method to obtain grammatically 3 Approach In this section we propose an approach to generate Question Answer (QA) pairs for a given entity E. Let KG be the knowledge graph which contains information about various entities in the form of triples. A triple consists of a subject, a predicate and an object. Subjects and objects are nodes in the KG, which could represent a person, a place, an abstract"
E17-1036,H05-1103,0,\N,Missing
I11-1078,C10-1063,1,0.826027,"Missing"
I11-1078,C96-1005,0,0.519315,"n pairs is better than current state-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same domain (i.e., Sports) then we would expect to see more words which are manifestations of the second sense than the first sense. Thus, we can estimate the probabilities of different senses of the word ‘facility"
I11-1078,E09-1005,0,0.072917,"-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same domain (i.e., Sports) then we would expect to see more words which are manifestations of the second sense than the first sense. Thus, we can estimate the probabilities of different senses of the word ‘facility’ by looking at the counts of its tr"
I11-1078,S10-1013,0,0.0582003,"Missing"
I11-1078,apidianaki-2008-translation,0,0.01889,"presence. Further, in a bilingual setting where parameters need to be ported from one language to another, it is important to associate labels with the clusters induced from the graph partitions so that these clusters can be aligned across languages. This is a difficult proposition and does not fall under the purview of WSI. Hence, in this work we stick to dictionary defined senses as opposed to corpus induced senses. Disambiguation by Translation (Gale et al., 1992; Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Ide et al., 2001; Diab and Resnik, 2002; Ng et al., 2003; TufiS¸ et al., 2004; Apidianaki, 2008) is another paradigm which attempts at reducing the need for annotated corpora, while ensuring high accuracy. The idea is to use the different target translations of a source word as automatically acquired sense labels. A severe drawback of these algorithms is the requirement of a significant amount of parallel corpora which may be difficult to obtain for many language pairs. Li and Li (2004) proposed an approach based on bilingual bootstrapping which does not need parallel corpora and relies only on in-domain corpora Monolingual approaches to Word Sense Disambiguation are abundant ranging fro"
I11-1078,J94-4003,0,0.150752,"ion (especially in an all-words scenario) for resource constrained languages such as Hindi and Marathi which have very poor web presence. Further, in a bilingual setting where parameters need to be ported from one language to another, it is important to associate labels with the clusters induced from the graph partitions so that these clusters can be aligned across languages. This is a difficult proposition and does not fall under the purview of WSI. Hence, in this work we stick to dictionary defined senses as opposed to corpus induced senses. Disambiguation by Translation (Gale et al., 1992; Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Ide et al., 2001; Diab and Resnik, 2002; Ng et al., 2003; TufiS¸ et al., 2004; Apidianaki, 2008) is another paradigm which attempts at reducing the need for annotated corpora, while ensuring high accuracy. The idea is to use the different target translations of a source word as automatically acquired sense labels. A severe drawback of these algorithms is the requirement of a significant amount of parallel corpora which may be difficult to obtain for many language pairs. Li and Li (2004) proposed an approach based on bilingual bootstrapping which does not need paral"
I11-1078,P02-1033,0,0.413982,"ially for some of the resource deprived languages. In this context, “Disambiguation by Translation” is a popular paradigm which tries to obviate the need for sense annotated corpora without compromising on accuracy. Such algorithms rely on the frequently made observation that a word in a given source language tends to have different translations in a target language depending on its sense. Given a sentence-and-word-aligned parallel corpus, these different translations in the target language can serve as automatically acquired sense labels for the source word. Although these algorithms (e.g., (Diab and Resnik, 2002; Ng et al., 2003)) give high accuracies, the requirement of a significant amount of bilingual parallel corpora may be an unreasonable demand for many language pairs (perhaps more unreasonable than collecting sense annotated corpora itself). Recent work by Khapra et al. (2009) has shown that, within a domain, it is possible to leverage the annotation work done for WSD on one language (L2 ) for the purpose of another language (L1 ), by projecting parameters learned from wordnet and sense annotated corpus of L2 to L1 . This method does not require a parallel corpus. However, it requires sense ma"
I11-1078,C02-1058,0,0.18181,"Note that every word in the Marathi synset is considered to be a translation of the corresponding words in the Hindi synset. Thus, the Marathi words mulgaa, porgaa and por are translations of the Hindi word ladakaa and so on. These synsetspecific translations play a very important role in our work as explained in the next section. from two languages. However, their approach is semi-supervised in contrast to our approach which is unsupervised. Further, they focus on the more specific task of Word Translation Disambiguation (WTD) as opposed to our work which focuses on the broader task of WSD. Kaji and Morimoto (2002) proposed an unsupervised bilingual approach which aligns statistically significant pairs of related words in language L1 with their cross-lingual counterparts in language L2 using a bilingual dictionary. This approach is based on two assumptions (i) words which are most significantly related to a target word provide clues about the sense of the target word and (ii) translations of these related words further reinforce the sense distinctions. The translations of related words thus act as cross-lingual clues for disambiguation. This algorithm when tested on 60 polysemous words (using English as"
I11-1078,D09-1048,1,0.922468,"word in a given source language tends to have different translations in a target language depending on its sense. Given a sentence-and-word-aligned parallel corpus, these different translations in the target language can serve as automatically acquired sense labels for the source word. Although these algorithms (e.g., (Diab and Resnik, 2002; Ng et al., 2003)) give high accuracies, the requirement of a significant amount of bilingual parallel corpora may be an unreasonable demand for many language pairs (perhaps more unreasonable than collecting sense annotated corpora itself). Recent work by Khapra et al. (2009) has shown that, within a domain, it is possible to leverage the annotation work done for WSD on one language (L2 ) for the purpose of another language (L1 ), by projecting parameters learned from wordnet and sense annotated corpus of L2 to L1 . This method does not require a parallel corpus. However, it requires sense marked corpus for one of the two languages. In this work, we focus on scenarios where no sense marked corpus is available in either language. Our method requires only untagged in-domain corpora from the two languages. Given such bilingual in-domain corpora (non-parallel) the cou"
I11-1078,W04-0834,0,0.817414,"utions of words in one language are estimated based on the raw counts of the words in the aligned synset in the target language. The overall performance of our algorithm when tested on 4 language-domain pairs is better than current state-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same dom"
I11-1078,J04-1001,0,0.0261917,"to corpus induced senses. Disambiguation by Translation (Gale et al., 1992; Dagan and Itai, 1994; Resnik and Yarowsky, 1999; Ide et al., 2001; Diab and Resnik, 2002; Ng et al., 2003; TufiS¸ et al., 2004; Apidianaki, 2008) is another paradigm which attempts at reducing the need for annotated corpora, while ensuring high accuracy. The idea is to use the different target translations of a source word as automatically acquired sense labels. A severe drawback of these algorithms is the requirement of a significant amount of parallel corpora which may be difficult to obtain for many language pairs. Li and Li (2004) proposed an approach based on bilingual bootstrapping which does not need parallel corpora and relies only on in-domain corpora Monolingual approaches to Word Sense Disambiguation are abundant ranging from supervised, 696 Table 1 shows the structure of MultiDict, with one example row standing for the concept of boy. The first column is the pivot describing a concept with a unique ID. The subsequent columns show the words expressing the concept in respective languages (in the example table, English, Hindi and Marathi). The pivot language to which other languages link is Hindi. This approach of"
I11-1078,P04-1036,0,0.374066,"sed and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same domain (i.e., Sports) then we would expect to see more words which are manifestations of the second sense than the first sense. Thus, we can estimate the probabilities of different senses of the word ‘facility’ by looking at the counts of its translations in different"
I11-1078,P96-1006,0,0.834947,"the sense distributions of words in one language are estimated based on the raw counts of the words in the aligned synset in the target language. The overall performance of our algorithm when tested on 4 language-domain pairs is better than current state-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belong"
I11-1078,P03-1058,0,0.20434,"esource deprived languages. In this context, “Disambiguation by Translation” is a popular paradigm which tries to obviate the need for sense annotated corpora without compromising on accuracy. Such algorithms rely on the frequently made observation that a word in a given source language tends to have different translations in a target language depending on its sense. Given a sentence-and-word-aligned parallel corpus, these different translations in the target language can serve as automatically acquired sense labels for the source word. Although these algorithms (e.g., (Diab and Resnik, 2002; Ng et al., 2003)) give high accuracies, the requirement of a significant amount of bilingual parallel corpora may be an unreasonable demand for many language pairs (perhaps more unreasonable than collecting sense annotated corpora itself). Recent work by Khapra et al. (2009) has shown that, within a domain, it is possible to leverage the annotation work done for WSD on one language (L2 ) for the purpose of another language (L1 ), by projecting parameters learned from wordnet and sense annotated corpus of L2 to L1 . This method does not require a parallel corpus. However, it requires sense marked corpus for on"
I11-1078,H05-1052,0,0.237448,"urrent state-of-the-art knowledge based and bilingual unsupervised approaches. 1 Introduction Word Sense Disambiguation (WSD) is one of the central and most widely investigated problems in Natural Language Processing (NLP). A wide variety of approaches ranging from supervised to unsupervised algorithms have been proposed. Of these, supervised approaches (Ng and Lee, 1996; Lee et al., 2004) which rely on sense annotated corpora have proven to be more successful, and they clearly outperform knowledge based and unsupervised approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; Rada, 2005; Agirre and Soroa, 2009; McCarthy et al., 2004). However, creation of sense annotated cor695 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 695–704, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP ond sense would be more prevalent. Similarly, if we are given a corpus of another language (say, Hindi) belonging to the same domain (i.e., Sports) then we would expect to see more words which are manifestations of the second sense than the first sense. Thus, we can estimate the probabilities of different senses of the word ‘facility’ by looking"
I11-1078,C04-1192,0,0.0628159,"Missing"
K16-1027,W15-3902,0,0.0193051,"ing the ambiguity in the grapheme-to-phoneme mapping. 5 6 Experiments Data: We experimented on the following Indian language pairs representing two language families: Bengali→Hindi, Kannada→Hindi, Hindi→Kannada and Tamil→Kannada. Bengali (bn) and Hindi (hi) are Indo-Aryan languages, while Kannada (kn) and Tamil (ta) are Dravidian languages. We used 10k source language names as training corpus, which were collected from various sources. We evaluated our systems on the NEWS 2015 Indic dataset. We created this set from the English to Indian language training corpora of the NEWS 2015 shared task (Banchs et al., 2015) by mining name pairs which have English names in common. 1500 words were selected at random to create the testset. The remaining pairs are used to train and tune a skyline supervised transliteration system for comparison. The training sets are small, the number of name pairs being: 2556 (bn-hi), 4022 (knhi), 3586 (hi-kn) and 3230 (ta-kn). Bootstrapping substring-based models In the second stage, we train a discriminative, loglinear transliteration model which learns substring mappings. We use the log-linear model proposed by Och and Ney (2002) for statistical machine translation and analogous"
K16-1027,N09-1034,0,0.0209179,"SubstringFigure 1: Overview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor comparable lists. We"
K16-1027,N12-1047,0,0.0499422,"Missing"
K16-1027,2010.amta-papers.12,0,0.071539,"Missing"
K16-1027,D12-1002,0,0.455144,"Missing"
K16-1027,N10-1065,1,0.749874,"Missing"
K16-1027,P06-1103,0,0.267201,"ransliteration ambiguities. SubstringFigure 1: Overview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor"
K16-1027,P06-2065,0,0.059408,"Missing"
K16-1027,P07-2045,0,0.00456309,"Missing"
K16-1027,N15-3017,1,0.80408,"Missing"
K16-1027,P02-1038,0,0.184332,"training corpora of the NEWS 2015 shared task (Banchs et al., 2015) by mining name pairs which have English names in common. 1500 words were selected at random to create the testset. The remaining pairs are used to train and tune a skyline supervised transliteration system for comparison. The training sets are small, the number of name pairs being: 2556 (bn-hi), 4022 (knhi), 3586 (hi-kn) and 3230 (ta-kn). Bootstrapping substring-based models In the second stage, we train a discriminative, loglinear transliteration model which learns substring mappings. We use the log-linear model proposed by Och and Ney (2002) for statistical machine translation and analogous transliteration features. The features are: substring transliteration probabilities, weighted average character transliteration probabilities and character language model score. The conditional probability of the target word e given the source word f is: P (e|f) = N P ∏ i=1 P (e¯i |f¯i ) = N P ∏ i=1 exp NF ∑ λj gj (f¯i , e¯i ) j=0 (11) ¯ where, fi and e¯i are source and target substrings respectively, λj and gj are feature weight and feature function respectively for feature j, N P number of substrings and N F is number of features. We synthes"
K16-1027,N09-1005,0,0.14977,"the top-1 transliterations in the synthesized, pseudo-parallel corpus; no true parallel corpus is used. Monotone decoding was performed. We used a 5-gram character language model trained with Witten-Bell smoothing on 40k names for all target languages. We ran Stage 2 for 5 iterations. For a rule-based baseline, we used the script conversion method implemented in the Indic NLP Library2 (Kunchukuttan et al., 2015) which is based on phonemic correspondences. poorly as reported in their work too. We also experimented with re-ranking the results using a unigram word based LM - our approximation to Ravi and Knight (2009)’s use of a word based LM - and its accuracy is comparable to PC_Init. The unigram LM was trained on a corpus of 185 million and 42 million tokens for hi and kn respectively. Thus, this knowledge-lite approach cannot learn a transliteration model effectively. Rule-based transliteration (Rule) performs significantly better than PC_Init. The phonetic nature of Indic scripts makes the rule-based system a stronger baseline, yet this simple approach does not ensure high accuracy transliteration. Phonetic changes like changes in manner/place of articulation, voicing, etc. make transliteration non-tr"
K16-1027,P11-1044,0,0.0232627,"verview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor comparable lists. We need a phonemic repres"
K16-1027,P12-1049,0,0.0529545,"Missing"
K16-1027,P07-1119,0,0.0347206,"allel corpora, co-occurrence is no longer a learning signal and it is not possible to learn the character transliteration probabilities reliably. To compensate for this, we define Dirichlet priors (De ) over each character transliteration probability distributions (Θe ), which can be used to encode linguistic knowledge. This leads to our proposed EM-MAP training objective for the Mstep over the entire training set (WF ). based models, which learn substring mappings like .mba → mba, are one way to incorporate contextual information and have been shown to perform better in a supervised setting (Sherif and Kondrak, 2007). Contextual information is especially important in an unsupervised setting. 4 ∑ {∑{ ∑[ QWF (Θ) = δe,f σa,f,e ∑ + e P (e) a i=1 θfai ,ei }} + log P (e) F| ,e ) (3) j=|CF | ∀e ∈ CE , ∑ θfj ,e = 1 j=1 where, δe,f = P (e|f), σa,f,e = P (a|e, f) are conditional probabilities of the latent variables computed in the E-step. These are computed using the previous iteration’s parameter values, whose values are fixed in the current iteration. nf,e,a is the number of times characters e and f are aligned in the alignment structure a. CF and CE are the character sets of the source and target languages resp"
K16-1027,P05-1044,0,0.158297,"Missing"
K16-1027,W06-1630,0,0.0614127,"Missing"
khapra-etal-2014-transliteration,W10-2405,0,\N,Missing
khapra-etal-2014-transliteration,N10-1065,1,\N,Missing
khapra-etal-2014-transliteration,W10-2403,1,\N,Missing
khapra-etal-2014-transliteration,D08-1027,0,\N,Missing
khapra-etal-2014-transliteration,W09-3505,0,\N,Missing
khapra-etal-2014-transliteration,W10-0710,0,\N,Missing
khapra-etal-2014-transliteration,W10-0701,0,\N,Missing
khapra-etal-2014-transliteration,W10-2401,0,\N,Missing
khapra-etal-2014-transliteration,P12-1049,0,\N,Missing
khapra-etal-2014-transliteration,W10-0731,0,\N,Missing
khapra-etal-2014-transliteration,P11-1044,0,\N,Missing
khapra-etal-2014-transliteration,W09-3506,0,\N,Missing
khapra-etal-2014-transliteration,W10-0728,0,\N,Missing
khapra-etal-2014-transliteration,W10-0708,0,\N,Missing
khapra-etal-2014-transliteration,W09-3504,0,\N,Missing
kunchukuttan-etal-2012-experiences,E06-1032,0,\N,Missing
kunchukuttan-etal-2012-experiences,ambati-etal-2010-active,0,\N,Missing
kunchukuttan-etal-2012-experiences,D08-1027,0,\N,Missing
kunchukuttan-etal-2012-experiences,D08-1056,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0710,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0701,0,\N,Missing
kunchukuttan-etal-2012-experiences,D09-1030,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0734,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-1701,0,\N,Missing
N10-1065,J93-2003,0,0.018145,"001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P (Y |X; λ) = PT PK 1 · e t=1 k=1 λk fk (Yt−1 ,Yt ,X,t) N (X) (1) where, X = source word Y = target word T = length of source word K = number of f eatures λk = f eature weight N (X) = normalization constant CRF++ 1 , an open source implementation of CRF was used for training and decoding (i.e. transliterating the names). GIZA++ (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for the name pairs in the parallel names training corpora. Under this alignment, each character in the source word is aligned to zero or more characters in the corresponding target word. The following features are then generated using this character-aligned data (here ei and hi form the i-th pair of aligned characters in the source word and target word respectively): • hi and ej such that i − 2 ≤ j ≤ i + 2 • hi and source character bigrams ( {ei−1 , ei } or {ei , ei+1 }) 3 Bridge Transliteration Systems In this section, we explore the salient questio"
N10-1065,I08-6014,0,0.0230961,"Missing"
N10-1065,2003.mtsummit-papers.17,0,0.0289296,"ents and datasets used. Section 4.3 discusses the results and error analysis. Section 5 discusses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conve"
N10-1065,C00-1056,0,0.0317973,"rk Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of p"
N10-1065,kang-choi-2000-automatic,0,0.138718,"usses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grap"
N10-1065,W09-3502,1,0.870678,"Missing"
N10-1065,D09-1141,0,0.0210578,"n functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang, 2007) wherein an intermediate bridge language (say, Z) is used to fill the data void that exists between a given language pair (say, X and Y ). In fact, recently it has been shown that the accuracy of a X → Z Machine Translation system can be improved by using additional X → Y data provided Z and Y share some common vocabulary and cognates (Nakov and Ng, 2009). However, no such effort has been made in the area of Machine Transliteration. To the best of our knowledge, this work is the first attempt at providing a practical solution to the problem of transliteration in the face of resource scarcity. 3.1 CRF based transliteration engine Conditional Random Fields ((Lafferty et al., 2001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P (Y |X; λ) = PT PK 1 · e t=1 k=1 λk fk (Yt−1 ,Yt ,X,t) N (X) (1) where, X = source word"
N10-1065,J03-1002,0,0.00379825,"3.1 CRF based transliteration engine Conditional Random Fields ((Lafferty et al., 2001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P (Y |X; λ) = PT PK 1 · e t=1 k=1 λk fk (Yt−1 ,Yt ,X,t) N (X) (1) where, X = source word Y = target word T = length of source word K = number of f eatures λk = f eature weight N (X) = normalization constant CRF++ 1 , an open source implementation of CRF was used for training and decoding (i.e. transliterating the names). GIZA++ (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for the name pairs in the parallel names training corpora. Under this alignment, each character in the source word is aligned to zero or more characters in the corresponding target word. The following features are then generated using this character-aligned data (here ei and hi form the i-th pair of aligned characters in the source word and target word respectively): • hi and ej such that i − 2 ≤ j ≤ i + 2 • hi and source character bigrams ( {ei−1 , ei } or {ei , ei+1"
N10-1065,C02-1099,0,0.0547151,"d on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang"
N10-1065,W98-1005,0,0.0170358,"ocess and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available betwee"
N10-1065,I08-6006,0,0.0133337,"he results and error analysis. Section 5 discusses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid"
N10-1065,P07-1108,0,0.0606185,"Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang, 2007) wherein an intermediate bridge language (say, Z) is used to fill the data void that exists between a given language pair (say, X and Y ). In fact, recently it has been shown that the accuracy of a X → Z Machine Translation system can be improved by using additional X → Y data provided Z and Y share some common vocabulary and cognates (Nakov and Ng, 2009). However, no such effort has been made in the area of Machine Transliteration. To the best of our knowledge, this work is the first attempt at providing a practical solution to the problem of transliteration in the face of resource scarcity."
N10-1065,P97-1017,0,\N,Missing
N10-1073,H05-1120,0,0.0282426,"Hawking Stefan Hoking Suchifun Houkingu Steephan Haakimg Paul man Pol Crugmun Pooru Kuruguman Paal man Haroun al-Rashid Haroon al-Rashid Haruun aru-Rasheedo Haroon al-Rasheed Subrahmaniya Bharati Subramaniya Bharati Suburaamaniya Subrahmanya Bahaarachi Bharathi KrugUser Input Wikipedia’s Suggestion Correct Spelling Suchifun Houkingu Suchin Housing Stephen Hawking Stefan Hoking Stefan Ho king Stephen Hawking Pol Crugman Poll Krugman Paul Krugman Paal Kragaman Paul Krugman Paul Krugman Suburaamaniya Bahaarachi Subramaniya Baracchi Subrahmaniya Bharati KragaIn principle, English spell-checkers (Ahmad and Kondrak, 2005) can handle the problem of incorrect spellings in the queries formed by ESL/EFL users. But in practice, there are two difficulties. Firstly, most English spell-checkers do not have a good coverage of names which form the bulk of user queries. Secondly, spelling correction of names is difficult because spelling mistakes are markedly influenced by the native language of the user. Not surprisingly, Wikipedia’s inbuilt spell-checker suggests “Suchin Housing” as the only alternative to the query “Suchifun Houkingu” instead of the correct entity “Stephen Hawking” (See Table 3 for more examples). The"
N10-1073,P04-1067,0,0.00848769,"Missing"
N10-1073,D08-1037,0,0.0165686,"). 4. We introduce a simple and efficient algorithm for computing the similarity scores of multiword names from the single-word similarity scores (Section 3.4). 494 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarit"
N10-1073,P08-1088,0,0.00679794,"for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and allows users to query a database using either of the two views. CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al., 2004) and monolingual corpora (Haghighi et al., 2008). Nearest neighbor search is a fundamental problem where challenge is to preprocess a set of points in some metric space into a geometric data structure so that given a query point, its k-nearest neighbors in the set can be reported as fast as possible. It has applications in many areas including pattern recognition and classification, machine learning, data compression, data mining, document retrieval and statistics. The brute-force search algorithm can find the nearest neighbors in running time proportional to the product of the number of points and the dimension of the metric space. When th"
N10-1073,W09-3518,1,0.919961,"del about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and allows users to query a database using either of the two views. CCA has been u"
N10-1073,N06-1011,0,0.0133795,"nd efficient algorithm for computing the similarity scores of multiword names from the single-word similarity scores (Section 3.4). 494 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarity models are applicable for s"
N10-1073,P06-1142,0,0.0200995,"s] with fifty thousand [Russian] candidates is a large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature"
N10-1073,W09-3501,0,0.0125694,"le (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and allows users to query a database using eithe"
N10-1073,N09-1005,0,0.00638287,"large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and allows users to query a d"
N10-1073,P07-1119,0,0.0134912,"nd [Russian] candidates is a large computational hurdle (it takes our model about seven hours)”. Several algorithms for string similarity search have been proposed and applied to various problems (Jin et al., 2005). None of them are directly applicable to cross-language name search as they are based on the assumption that the query string shares the same alphabet as the database strings. Machine Transliteration has been studied extensively in the context of Machine Translation and Cross-Language Information Retrieval (Knight and Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo et al., 2006), (Sherif and Kondrak, 2007), (Ravi and Knight, 2009), (Li et al., 2009), (Khapra and Bhattacharyya, 2009). However, Machine Transliteration followed by string similarity search gives less-thansatisfactory solution for the cross-language name search problem as we will see later in Section 4. CCA was introduced by Hotelling in 1936 and has been applied to various problems including CLIR, Text Clustering, and Image Retrieval (Hardoon et al., 2004). Recently, CCA has gained importance in the Machine Learning community as a technique for multi-view learning. CCA computes a common semantic feature space for two-view data and"
N10-1073,P06-1010,0,0.0151849,"mputing the similarity scores of multiword names from the single-word similarity scores (Section 3.4). 494 5. We show experimentally that our approach significantly improves the multilingual experience of ESL/EFL users (Section 4). 2 Related Work Although approximate similarity search is wellstudied, we are not aware of any non-trivial crosslanguage name search algorithm in the literature. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al., 2006), (Udupa et al., 2009b). These techniques employ various transliteration similarity models. Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). A generative transliteration model was proposed and used along with cross-language information retrieval to mine named entity transliterations from large comparable corpora (Udupa et al., 2009b). However, none of these transliteration similarity models are applicable for searching very large nam"
N10-1073,E09-1091,1,0.862789,"crosslanguage name search which is less resource demanding than a fully functional cross-language retrieval system. There are several challenges that need to be addressed in order to enable cross-language name 493 search in Wikipedia. • Firstly, name queries are expressed by ESL/EFL users in the native languages using the orthography of those languages. Transliterating the name into Latin script using a Machine Transliteration system is an option but state-of-the-art Machine Transliteration technologies are still far away from producing the correct transliteration. Further, as pointed out by (Udupa et al., 2009a), it is not enough if a Machine Transliteration system generates a correct transliteration; it must produce the transliteration that is present in the Wikipedia title. • Secondly, there are about 6 million titles (including redirects) in English Wikipedia which rules out the naive approach of comparing the query with every one of the English Wikipedia titles for transliteration equivalence as is done typically in transliteration mining tasks. A practical cross-language name search system for Wikipedia must be able to search millions of Wikipedia titles in a fraction of a second and return th"
N10-1073,J98-4003,0,\N,Missing
N13-1032,P06-1067,0,0.593708,"uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points. 1 Introduction Handling the differences in word orders between pairs of languages is crucial in producing good machine translation. This is especially true for language pairs such as Urdu-English which have significantly different sentence structures. For example, the typical word order in Urdu is Subject Object Verb whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages)"
N13-1032,2007.mtsummit-papers.3,0,0.0442802,"d to 0 and the learning algorithm is run for N iterations. During each iteration the parameters are updated after every training instance is seen. For example, during the i-th iteration, after seeing the j-th training sentence, we update the k-th parameter θk using the following update rule: (i,j) where, (i,j−1) + φk (πjgold ) − φk (πj∗ ) θk = θk (i,j) θk = value of the k-th parameter after (3) seeing sentence j in iteration i φk = k-th feature πjgold = gold reordering for the j-th sentence T use φk (πjclosest to gold ) as this is known to be a better strategy for learning a re-ranking model (Arun and Koehn, 2007). πjclosest to gold is given by: arg max πji ∈Πj # of common bigram pairs in πji and πjgold len(πjgold ) where, Πj = set of n-best reorderings for j th sentence πjclosest to gold is thus the reordering which has the maximum overlap with πjgold in terms of the number of word pairs (wm , wn ) where wn is put next to wm . 2.5 Interpolating with MT score The approach described above aims at producing a better reordering by extracting richer features from the source sentence. Since the final aim is to improve the performance of an MT system, it would potentially be beneficial to interpolate the sco"
N13-1032,bouamor-etal-2012-identifying,0,0.0257459,"t al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) and Multiword Expressions as segments (Bouamor et al., 2012) to improve the performance of SMT but without much success. The idea of improving performance by reranking a n-best list of outputs has been used recently for the related task of parsing (Katz-Brown et al., 2011) using targeted self-training for improving the performance of reordering. However, in contrast, in our work we directly aim at improving the performance of a reordering model. 6 Related Work There are several studies which have shown that reordering the source side sentence to match the target side order leads to improvements in Machine Translation. These approaches can be broadly cl"
N13-1032,J07-2003,0,0.610271,"ith higher quality alignments from a supervised aligner (McCarley et al., 2011). The Gigaword English corpus was used for building the English language model. We report results on the NIST MT-08 evaluation set, averaging BLEU scores from the News and Web conditions to provide a single BLEU score. Table 4 compares the MT performance obtained by reordering the training and test data using the following approaches: 1. No pre-ordering: A baseline system which does not use any source side reordering as a preprocessing step 2. HIERO : A state of the art hierarchical phrase based translation system (Chiang, 2007) 3. TSP: A system which uses the 1-best reordering produced by the TSP model 4. Higher order & structural features: A system 320 Approach Unreordered TSP Higher order & structural features mBLEU 31.2 56.6 58.4 Sentence length Table 3: mBLEU scores for Urdu to English reordering using different models. Approach No pre-ordering HIERO TSP Higher order & structural features Interpolating with MT score BLEU 21.9 25.2 26.9 27.5 28.2 which reranks n-best reorderings produced by TSP using higher order and structural features 5. Interpolating with MT score : A system which interpolates the score assign"
N13-1032,P05-1066,0,0.486729,"Missing"
N13-1032,D11-1018,0,0.0493651,"anguages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) and Multiword Expressions as"
N13-1032,P06-1121,0,0.0505724,"not help (mainly due to overfitting and sparsity). 5 parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related t"
N13-1032,W12-3134,0,0.0144234,"r & structural features: A system 320 Approach Unreordered TSP Higher order & structural features mBLEU 31.2 56.6 58.4 Sentence length Table 3: mBLEU scores for Urdu to English reordering using different models. Approach No pre-ordering HIERO TSP Higher order & structural features Interpolating with MT score BLEU 21.9 25.2 26.9 27.5 28.2 which reranks n-best reorderings produced by TSP using higher order and structural features 5. Interpolating with MT score : A system which interpolates the score assigned to a reordering by our model with the score assigned by a MT system We used Joshua 4.0 (Ganitkevitch et al., 2012) which provides an open source implementation of HIERO. For training, tuning and testing HIERO we used the same experimental setup as described above. As seen in Table 4, we get an overall gain of 6.2 BLEU points with our approach as compared to a baseline system which does not use any reordering. More importantly, we outperform (i) a PBSMT system which uses the TSP model by 1.3 BLEU points and (ii) a state of the art hierarchical phrase based translation system by 3 points. Discussions We now discuss some error corrections and ablation tests. 4.1 29.7 28.2 33.4 31.2 58.7 56.8 55.8 56.6 Our ap"
N13-1032,C10-1043,0,0.088419,"y dropping one feature from each of the following pairs : i) first lex current segment, first lex next segment ii) first pos current segment, first pos next segment iii) end lex current segment, end lex next segment. This is because these pairs of features are highly dependent features. Note that similar to the pos triplet jumps feature we also tried a pos quadruplet jumps feature but it did not help (mainly due to overfitting and sparsity). 5 parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference r"
N13-1032,W10-1712,0,0.0608607,"Missing"
N13-1032,H05-1012,0,0.0794027,"Missing"
N13-1032,D11-1017,0,0.0217974,"wariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) and Multiword Expressions as segments (Bouamor et al., 2012) to improve the performance of SMT but without much success. The idea of improving performance by reranking a n-best list of outputs has been used recently for the related task of parsing (Katz-Brown et al., 2011) using targeted self-training for improving the performance of reordering. However, in contrast, in our work we directly aim at improving the performance of a reordering model. 6 Related Work There are several studies which have shown that reordering the source side sentence to match the target side order leads to improvements in Machine Translation. These approaches can be broadly classified into three types. First, approaches which reorder source sentences by applying rules to the source side 322 Conclusion In this work, we proposed a model for re-ranking the n-best reorderings produced by a"
N13-1032,N03-1017,0,0.0307343,"ed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points. 1 Introduction Handling the differences in word orders between pairs of languages is crucial in producing good machine translation. This is especially true for language pairs such as Urdu-English which have significantly different sentence structures. For example, the typical word order in Urdu is Subject Object Verb whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work on"
N13-1032,P10-1001,0,0.022393,"rce sentence induces a segmentation on the sentence. This segSince deriving a good reordering would essenmentation is based on the following heuristic: if wi tially require analyzing the syntactic structure of the and wi+1 appear next to each other in the original source sentence, the tasks of reordering and parsing sentence but do not appear next to each other in the are often considered to be related. The main motivareordered sentence then wi marks the end of a segtion for using higher order features thus comes from ment and wi+1 marks the beginning of the next sega related work on parsing (Koo and Collins, 2010) ment. To understand this better please refer to Figwhere the performance of a state of the art parser ure 1 which shows the correct reordering of an Urdu was improved by considering higher order depensentence based on its English translation and the cordencies. In our model we use trigram features (see responding segmentation induced on the Urdu senTable 2) of the following form: tence. If the correct segmentation of a sentence is φ(rui , rui+1 , rui+2 , J(rui , rui+1 ), J(rui+1 , rui+2 )) known in advance then one could use a hierarchical where rui =word at position i in the reordered model"
N13-1032,P06-1077,0,0.0787231,"to overfitting and sparsity). 5 parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tro"
N13-1032,D11-1082,0,0.0911254,"rings for the training data and (ii) using these n-best reorderings to train a perceptron model. We use the same data for training the reordering model as well as our perceptron model. This data contains 180K words of manual alignments (part of the NIST MT08 training data) and 3.9M words of automatically generated machine alignments (1.7M words from the NIST MT-08 training data1 and 2.2M words extracted from sources on the web2 ). The machine alignments were generated using a supervised maximum entropy model (Ittycheriah and Roukos, 2005) and then corrected using an improved correction model (McCarley et al., 2011). We first divide the training data into 10 folds. The n-best reorderings for each fold are then generated using a model trained on the remaining 9 folds. This division into 10 folds is done for reasons explained earlier in Section 2.1. These n-best reorderings are then used to train the perceptron model as described in Section 2.4. Note that Visweswariah et al. (2011) used only manually aligned data for training the TSP model. However, we use machine aligned data in addition to manually aligned data for training the TSP model as it leads to better performance. We used this improvised TSP mode"
N13-1032,D12-1077,0,0.0680973,"approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (Henr´ıquez Q. et al., 2010) and Multiword Expressions as segments (Bouamor et a"
N13-1032,P09-1090,1,0.933961,"whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages). Recent work (Visweswariah et al., 2011) has shown that it is possible to learn a reordering model from a relatively small number of hand aligned sentences . This eliminates the need of a source or target parser. In this work, we build upon the work of Visweswariah et al. (2011) which solves the reordering problem by treating it as an instance of the Traveling Salesman Problem (TSP). They learn a model which assigns costs to all pairs of word"
N13-1032,N04-4026,0,0.0465469,"ing by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points. 1 Introduction Handling the differences in word orders between pairs of languages is crucial in producing good machine translation. This is especially true for language pairs such as Urdu-English which have significantly different sentence structures. For example, the typical word order in Urdu is Subject Object Verb whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages). Recent work (V"
N13-1032,J03-1005,0,0.0528717,"ur MT experiments. For intrinsic evaluation we use a development set of 8017 Urdu tokens reordered manually. Table 3 compares the performance of the top-1 reordering output by our algorithm with the top-1 reordering generated by the improved TSP model in terms of mBLEU. We see a gain of 1.8 mBLEU points with our approach. Next, we see the impact of the better reorderings produced by our system on the performance of a state-of-the-art MT system. For this, we used a standard phrase based system (Al-Onaizan and Papineni, 2006) with a lexicalized distortion model with a window size of +/-4 words (Tillmann and Ney, 2003). As mentioned earlier, our training data consisted of 3.9M words including the NIST MT-08 training data. We use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al., 2011). The Gigaword English corpus was used for building the English language model. We report results on the NIST MT-08 evaluation set, averaging BLEU scores from the News and Web conditions to provide a single BLEU score. Table 4 compares the MT performance obtained by reordering the training and test data using the following approaches: 1. No pre-ordering: A baseline system which does"
N13-1032,D09-1105,0,0.0839345,"source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but performs better. The focus of our work is to use higher order and structural features (based on segmentation of the source sentence) which are not captured by their model. Some other works have used collocation based segmentation (He"
N13-1032,C10-1126,1,0.838079,"feature from each of the following pairs : i) first lex current segment, first lex next segment ii) first pos current segment, first pos next segment iii) end lex current segment, end lex next segment. This is because these pairs of features are highly dependent features. Note that similar to the pos triplet jumps feature we also tried a pos quadruplet jumps feature but it did not help (mainly due to overfitting and sparsity). 5 parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from alig"
N13-1032,D11-1045,1,0.614163,") and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages). Recent work (Visweswariah et al., 2011) has shown that it is possible to learn a reordering model from a relatively small number of hand aligned sentences . This eliminates the need of a source or target parser. In this work, we build upon the work of Visweswariah et al. (2011) which solves the reordering problem by treating it as an instance of the Traveling Salesman Problem (TSP). They learn a model which assigns costs to all pairs of words in a sentence, where the cost represents the penalty of putting a word immediately preceding another word. The best permutation is found via the chained LinKernighan heuristic for solving a TS"
N13-1032,D07-1077,0,0.126337,"ubject Object Verb whereas the typical word order in English is Subject Verb Object. Phrase based systems (Koehn et al., 2003) rely on a lexicalized distortion model (Al-Onaizan and Papineni, 2006; Tillman, 2004) and the target language model to produce output words in the correct order. This is known to be inadequate when the languages are very different in terms of word order (refer to Table 3 in Section 3). Pre-ordering source sentences while training and testing has become a popular approach in overcoming the word ordering challenge. Most techniques for pre-ordering (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) depend on a high quality source language parser, which means these methods work only if the source language has a parser (this rules out many languages). Recent work (Visweswariah et al., 2011) has shown that it is possible to learn a reordering model from a relatively small number of hand aligned sentences . This eliminates the need of a source or target parser. In this work, we build upon the work of Visweswariah et al. (2011) which solves the reordering problem by treating it as an instance of the Traveling Salesman Problem (TSP). They learn a model which assigns"
N13-1032,C04-1073,0,0.305187,"erformances obtained by dropping one feature from each of the following pairs : i) first lex current segment, first lex next segment ii) first pos current segment, first pos next segment iii) end lex current segment, end lex next segment. This is because these pairs of features are highly dependent features. Note that similar to the pos triplet jumps feature we also tried a pos quadruplet jumps feature but it did not help (mainly due to overfitting and sparsity). 5 parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model usi"
N13-1032,P02-1039,0,0.0530562,"jumps feature but it did not help (mainly due to overfitting and sparsity). 5 parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which"
N13-1032,W06-3119,0,0.0979179,"sparsity). 5 parse; the rules are either hand-written (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009) or learned from data (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). These approaches require a source side parser which is not available for many languages. The second type of approaches treat machine translation decoding as a parsing problem by using source and/or target side syntax in a Context Free Grammar framework. These include Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006). The third type of approaches, avoid the use of a parser (as required by syntax based models) and instead train a reordering model using reference reorderings derived from aligned data. These approaches (Tromble and Eisner, 2009; Visweswariah et al., 2011; DeNero and Uszkoreit, 2011; Neubig et al., 2012) have a low decode time complexity as reordering is done as a preprocessing step and not integrated with the decoder. Our work falls under the third category, as it improves upon the work of (Visweswariah et al., 2011) which is closely related to the work of (Tromble and Eisner, 2009) but perf"
N13-1032,W12-5108,0,\N,Missing
N16-1021,2012.eamt-1.60,0,0.0245826,"ch. As a side note, we would like to mention that in addition to Z1 , Z2 to ZM −1 as defined earlier, if additional parallel data is available between some of 174 the non-pivot views then the objective function can be suitably modified to use this parallel data to further improve the learning. However, this is not the focus of this work and we leave this as a possible future work. 4 Datasets In this section, we describe the two datasets that we used for our experiments. 4.1 Multlingual TED corpus Hermann and Blunsom (2014b) provide a multilingual corpus based on the TED corpus for IWSLT 2013 (Cettolo et al., 2012). It contains English transcriptions of several talks from the TED conference and their translations in multiple languages. We use the parallel data between English and other languages for training Bridge Corrnet (English, thus, acts as the pivot langauge). Hermann and Blunsom (2014b) also propose a multlingual document classification task using this corpus. The idea is to use the keywords associated with each talk (document) as class labels and then train a classifier to predict these classes. There are one or more such keywords associated with each talk but only the 15 most frequent keywords"
N16-1021,P07-1092,0,0.0349745,"mon representation for audio and video. Srivastava and Salakhutdinov (2014) extended this idea to RBMs and learned common representations for image and text. Other solutions for image/text representation learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address multimodal, multilingual learning in situations where parallel data is available only between different views and a pivot view. In the past, pivot/bridge languages have been used to facilitate MT (for example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009)), transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these works use neural networks but it is important to mention them here because they use the concept of a pivot language (view) which is central to our work. 3 Bridge Correlational Neural Network In this section, we describe Bridge CorrNet which is an extension of the CorrNet model proposed by (Chandar et al., 2016). They address the problem of"
N16-1021,P14-1006,0,0.0754634,"ments in another language by representing documents of both languages in a common subspace. Introduction The proliferation of multilingual and multimodal content online has ensured that multiple views of the same data exist. For example, it is common to find the same article published in multiple languages online in multilingual news articles, multilingual wikipedia articles, etc. Such multiple views can Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Wang et al., 2015) except (Hermann and Blunsom, 2014b) typically require parallel data between all views. However, in many realworld scenarios such parallel data may not be available. For example, while there are many publicly available datasets containing images and their corresponding English captions, it is very hard to find datasets containing images and their corresponding captions in Russian, Dutch, Hindi, Urdu, etc. In this work, we are interested in addressing such scenarios. More specifically, we consider scenarios where we have n different views but parallel data is only available between each of these views, and a pivot view. In part"
N16-1021,N10-1065,1,0.825079,"mage/text representation learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address multimodal, multilingual learning in situations where parallel data is available only between different views and a pivot view. In the past, pivot/bridge languages have been used to facilitate MT (for example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009)), transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these works use neural networks but it is important to mention them here because they use the concept of a pivot language (view) which is central to our work. 3 Bridge Correlational Neural Network In this section, we describe Bridge CorrNet which is an extension of the CorrNet model proposed by (Chandar et al., 2016). They address the problem of learning common representations between two views when parallel data is available between them. We propose an extension to their model which simultaneously learns a common repr"
N16-1021,C12-1089,0,0.0426343,"earning between views. For example, a document classifier trained on one language (view) can be used to classify documents in another language by representing documents of both languages in a common subspace. Introduction The proliferation of multilingual and multimodal content online has ensured that multiple views of the same data exist. For example, it is common to find the same article published in multiple languages online in multilingual news articles, multilingual wikipedia articles, etc. Such multiple views can Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Wang et al., 2015) except (Hermann and Blunsom, 2014b) typically require parallel data between all views. However, in many realworld scenarios such parallel data may not be available. For example, while there are many publicly available datasets containing images and their corresponding English captions, it is very hard to find datasets containing images and their corresponding captions in Russian, Dutch, Hindi, Urdu, etc. In this work, we are interested in addressing such scenarios. More specifically, we consider scenarios whe"
N16-1021,D09-1141,0,0.0143353,"and Salakhutdinov (2014) extended this idea to RBMs and learned common representations for image and text. Other solutions for image/text representation learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address multimodal, multilingual learning in situations where parallel data is available only between different views and a pivot view. In the past, pivot/bridge languages have been used to facilitate MT (for example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009)), transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these works use neural networks but it is important to mention them here because they use the concept of a pivot language (view) which is central to our work. 3 Bridge Correlational Neural Network In this section, we describe Bridge CorrNet which is an extension of the CorrNet model proposed by (Chandar et al., 2016). They address the problem of learning common representations between two vie"
N16-1021,Q14-1017,0,0.100132,"ur model addresses this issue and outperforms the model of Hermann and Blunsom (2014b). The task of cross modal access between images and text addressed in this work comes under MultiModal Representation Learning where each view belongs to a different modality. Ngiam et al. (2011) proposed an autoencoder based solution to learning common representation for audio and video. Srivastava and Salakhutdinov (2014) extended this idea to RBMs and learned common representations for image and text. Other solutions for image/text representation learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address multimodal, multilingual learning in situations where parallel data is available only between different views and a pivot view. In the past, pivot/bridge languages have been used to facilitate MT (for example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009)), transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these work"
N16-1021,N07-1061,0,0.0178932,"audio and video. Srivastava and Salakhutdinov (2014) extended this idea to RBMs and learned common representations for image and text. Other solutions for image/text representation learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address multimodal, multilingual learning in situations where parallel data is available only between different views and a pivot view. In the past, pivot/bridge languages have been used to facilitate MT (for example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009)), transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these works use neural networks but it is important to mention them here because they use the concept of a pivot language (view) which is central to our work. 3 Bridge Correlational Neural Network In this section, we describe Bridge CorrNet which is an extension of the CorrNet model proposed by (Chandar et al., 2016). They address the problem of learning common representa"
N16-1021,P07-1108,0,0.0501946,"ion to learning common representation for audio and video. Srivastava and Salakhutdinov (2014) extended this idea to RBMs and learned common representations for image and text. Other solutions for image/text representation learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address multimodal, multilingual learning in situations where parallel data is available only between different views and a pivot view. In the past, pivot/bridge languages have been used to facilitate MT (for example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009)), transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these works use neural networks but it is important to mention them here because they use the concept of a pivot language (view) which is central to our work. 3 Bridge Correlational Neural Network In this section, we describe Bridge CorrNet which is an extension of the CorrNet model proposed by (Chandar et al., 2016). They"
N16-1021,I11-1135,0,0.0170266,"eng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address multimodal, multilingual learning in situations where parallel data is available only between different views and a pivot view. In the past, pivot/bridge languages have been used to facilitate MT (for example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009)), transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these works use neural networks but it is important to mention them here because they use the concept of a pivot language (view) which is central to our work. 3 Bridge Correlational Neural Network In this section, we describe Bridge CorrNet which is an extension of the CorrNet model proposed by (Chandar et al., 2016). They address the problem of learning common representations between two views when parallel data is available between them. We propose an extension to their model which simultaneously learns a common representation for M views when parallel data is available only betwee"
N16-1021,D13-1141,0,0.013148,"xample, Multimodal Autoencoder (Ngiam et al., 2011), Deep Canonically Correlated Autoencoder (Wang et al., 2015), Deep CCA (Andrew et al., 2013) and Correlational Neural Networks (CorrNet) (Chandar et al., 2016). CorrNet performs better than most of the above mentioned methods and we build on their work as discussed in the next section. One of the tasks that we address in this work is multilingual representation learning where the aim is to learn aligned representations for words across languages. Some notable neural network based approaches here include the works of (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013; Hermann and Blunsom, 2014b; Hermann and Blunsom, 2014a; Chandar et al., 2014; Soyer et al., 2015; Gouws et al., 2015). However, except for (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b), none of these other works handle the case when parallel data is not available between all languages. Our model addresses this issue and outperforms the model of Hermann and Blunsom (2014b). The task of cross modal access between images and text addressed in this work comes under MultiModal Representation Learning where each view belongs to a different modality. Ngiam et al. (2"
N16-4006,P06-2112,0,\N,Missing
N16-4006,P07-1108,0,\N,Missing
N16-4006,W07-0705,0,\N,Missing
N16-4006,P15-2021,0,\N,Missing
N18-1139,D10-1049,0,0.0519818,"s the state of the art methods mentioned above. 2 Related work Natural Language Generation has always been of interest to the research community and has received a lot of attention in the past. The approaches for NLG range from (i) rule based approaches (e.g., (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010)) (ii) modular statistical approaches which divide the process into three phases (planning, selection and surface realization) and use data driven approaches for one or more of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid approaches which rely on a combination of handcrafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarizatio"
N18-1139,H05-1042,0,0.272092,"hese two datasets, our model outperforms the state of the art methods mentioned above. 2 Related work Natural Language Generation has always been of interest to the research community and has received a lot of attention in the past. The approaches for NLG range from (i) rule based approaches (e.g., (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010)) (ii) modular statistical approaches which divide the process into three phases (planning, selection and surface realization) and use data driven approaches for one or more of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid approaches which rely on a combination of handcrafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu"
N18-1139,W09-0603,0,0.029183,"danau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al., 2014), query based document summarization (Nema et al., 2017) and so on. Most of these models are data hungry and are trained on large amounts of data. On the other hand, NLG from structured data has largely been studied in the context of small datasets such as W EATHER G OV (Liang et al., 2009), ROBO C UP (Chen and Mooney, 2008), NFL R ECAPS (Barzilay and Lapata, 2005), P RODIGY-M ETEO (Belz and Kow, 2009) and TUNA Challenge (Gatt and Belz, 2010). Recently Mei et al. (2016) proposed RNN/LSTM based neural encoder-decoder models with attention for W EATHER G OV and ROBO C UP datasets. Unlike the datasets mentioned above, the biography dataset introduced by Lebret et al. (2016) is larger (700K {table, descriptions} pairs) and has a much larger vocabulary (400K words as opposed to around 350 or fewer words in the above datasets). Further, unlike the feed-forward neural network based model proposed by (Lebret et al., 2016) we use a sequence to sequence model and introduce components to address the p"
N18-1139,N16-1012,0,0.104002,"hat the number of fields in the infobox and the ordering of the fields within the infobox varies from person to person. Given the large size (700K examples) and heterogeneous nature of the dataset which contains biographies of people from different backgrounds (sports, politics, arts, etc.), it is hard to come up with simple rule-based templates for generating natural language descriptions from infoboxes, thereby making a case for datadriven models. Based on the recent success of data-driven neural models for various other NLG tasks (Bahdanau et al., 2014; Rush et al., 2015; Yao et al., 2015; Chopra et al., 2016; Nema et al., 2017), one simple choice is to treat the infobox as 1539 Proceedings of NAACL-HLT 2018, pages 1539–1550 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: Sample Infobox with description : V. Balakrishnan (born 1943 as Venkataraman Balakrishnan) is an Indian theoretical physicist who has worked in a number of fields of areas, including particle physics, many-body theory, the mechanical behavior of solids, dynamical systems, stochastic processes, and quantum dynamics. a sequence of {field, value} pairs and use a standard seq2seq m"
N18-1139,W07-2322,0,0.0359291,"Missing"
N18-1139,W06-1417,0,0.110799,"Missing"
N18-1139,D16-1032,0,0.071668,"bove, the biography dataset introduced by Lebret et al. (2016) is larger (700K {table, descriptions} pairs) and has a much larger vocabulary (400K words as opposed to around 350 or fewer words in the above datasets). Further, unlike the feed-forward neural network based model proposed by (Lebret et al., 2016) we use a sequence to sequence model and introduce components to address the peculiar characteristics of the task. Specifically, we introduce neural components to address the need for attention at two levels and to address the stay on and never look back behaviour required by the decoder. Kiddon et al. (2016) have explored the use of checklists to track previously visited ingredients while generating recipes from ingredients. Note that two-level attention mechanisms have also been used in the context of summarization (Nallapati et al., 2016), document classification (Yang et al., 2016), dialog systems (Serban et al., 2016), etc. However, these works deal with unstructured data (sentences at the higher level and words at a lower level) as opposed to structured data in our case. 3 Proposed model As input we are given an infobox I = {(gi , ki )}M i=1 , which is a set of pairs (gi , ki ) where gi corr"
N18-1139,C10-2062,0,0.039397,"t methods mentioned above. 2 Related work Natural Language Generation has always been of interest to the research community and has received a lot of attention in the past. The approaches for NLG range from (i) rule based approaches (e.g., (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010)) (ii) modular statistical approaches which divide the process into three phases (planning, selection and surface realization) and use data driven approaches for one or more of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid approaches which rely on a combination of handcrafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al.,"
N18-1139,D13-1157,0,0.174246,"ove. 2 Related work Natural Language Generation has always been of interest to the research community and has received a lot of attention in the past. The approaches for NLG range from (i) rule based approaches (e.g., (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010)) (ii) modular statistical approaches which divide the process into three phases (planning, selection and surface realization) and use data driven approaches for one or more of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid approaches which rely on a combination of handcrafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al., 2014), query based documen"
N18-1139,P98-1116,0,0.456153,"ot of attention in the past. The approaches for NLG range from (i) rule based approaches (e.g., (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010)) (ii) modular statistical approaches which divide the process into three phases (planning, selection and surface realization) and use data driven approaches for one or more of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid approaches which rely on a combination of handcrafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al., 2014), query based document summarization (Nema et al., 2017) and so on. Most of these models are data hungry and are trained on large amounts of dat"
N18-1139,D16-1128,0,0.466063,"com Abstract 1 Introduction Rendering natural language descriptions from structured data is required in a wide variety of commercial applications such as generating descriptions of products, hotels, furniture, etc., from a corresponding table of facts about the entity. Such a table typically contains {field, value} pairs where the field is a property of the entity (e.g., color) and the value is a set of possible assignments to this property (e.g., color = red). Another example of this is the recently introduced task of generating one line biography descriptions from a given Wikipedia infobox (Lebret et al., 2016). The Wikipedia infobox serves as a table of facts about a person and the first sentence from the corresponding article serves as a one line description of the person. Figure 1 illustrates an example input infobox which contains fields such as Born, Residence, Nationality, Fields, Institutions and Alma Mater. Each field further contains some words (e.g., particle physics, many-body theory, etc.). The corresponding description is coherent with the information contained in the infobox. In this work, we focus on the task of generating natural language descriptions from a structured table of facts"
N18-1139,P09-1011,0,0.0601136,"4). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al., 2014), query based document summarization (Nema et al., 2017) and so on. Most of these models are data hungry and are trained on large amounts of data. On the other hand, NLG from structured data has largely been studied in the context of small datasets such as W EATHER G OV (Liang et al., 2009), ROBO C UP (Chen and Mooney, 2008), NFL R ECAPS (Barzilay and Lapata, 2005), P RODIGY-M ETEO (Belz and Kow, 2009) and TUNA Challenge (Gatt and Belz, 2010). Recently Mei et al. (2016) proposed RNN/LSTM based neural encoder-decoder models with attention for W EATHER G OV and ROBO C UP datasets. Unlike the datasets mentioned above, the biography dataset introduced by Lebret et al. (2016) is larger (700K {table, descriptions} pairs) and has a much larger vocabulary (400K words as opposed to around 350 or fewer words in the above datasets). Further, unlike the feed-forward neural network based mod"
N18-1139,P15-1002,0,0.0164097,"s which influence the weights assigned to the fields. 3. Basic Seq2Seq: This is the vanilla encodeattend-decode model (Bahdanau et al., 2014). Further, to deal with the large vocabulary (∼400K words) we use a copying mechanism as a postprocessing step. Specifically, we identify the time steps at which the decoder produces unknown words (denoted by the special symbol UNK). For each such time step, we look at the attention weights on the input words and replace the UNK word by that input word which has received maximum attention at this timestep. This process is similar to the one described in (Luong et al., 2015). Even Lebret et al. (2016) have a copying mechanism tightly integrated with their model. 4.3 Results and Discussions Hyperparameter tuning We tuned the hyperparameters of all the models using a validation set. As mentioned earlier, we used a bidirectional GRU cell as the function f for computing the representation of the fields and the values (see Section 3.1). For all the models, we experimented with GRU state sizes of 128, 256 and 512. The total number of unique words in the corpus is around 400K (this includes the words in the infobox and the descriptions). Of these, we retained only the t"
N18-1139,J11-3002,0,0.0435222,"ange from (i) rule based approaches (e.g., (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010)) (ii) modular statistical approaches which divide the process into three phases (planning, selection and surface realization) and use data driven approaches for one or more of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid approaches which rely on a combination of handcrafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al., 2014), query based document summarization (Nema et al., 2017) and so on. Most of these models are data hungry and are trained on large amounts of data. On the other hand, NLG from structured data has la"
N18-1139,N16-1086,0,0.658438,"et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al., 2014), query based document summarization (Nema et al., 2017) and so on. Most of these models are data hungry and are trained on large amounts of data. On the other hand, NLG from structured data has largely been studied in the context of small datasets such as W EATHER G OV (Liang et al., 2009), ROBO C UP (Chen and Mooney, 2008), NFL R ECAPS (Barzilay and Lapata, 2005), P RODIGY-M ETEO (Belz and Kow, 2009) and TUNA Challenge (Gatt and Belz, 2010). Recently Mei et al. (2016) proposed RNN/LSTM based neural encoder-decoder models with attention for W EATHER G OV and ROBO C UP datasets. Unlike the datasets mentioned above, the biography dataset introduced by Lebret et al. (2016) is larger (700K {table, descriptions} pairs) and has a much larger vocabulary (400K words as opposed to around 350 or fewer words in the above datasets). Further, unlike the feed-forward neural network based model proposed by (Lebret et al., 2016) we use a sequence to sequence model and introduce components to address the peculiar characteristics of the task. Specifically, we introduce neura"
N18-1139,K16-1028,0,0.0801343,"pation needs attention and then decides which is the next appropriate occupation to attend to from the set of occupations (actor, director, producer, etc.). To enable this, we use a bifocal attention mechanism which computes an attention over fields at a macro level and over values at a micro level. We then fuse these attention weights such that the attention weight for a field also influences the attention over the values within it. Finally, we feed a fused context vector to the decoder which contains both field level and word level information. Note that such two-level attention mechanisms (Nallapati et al., 2016; Yang et al., 2016; Serban et al., 2016) have been used in the context of unstructured data (as opposed to structured data in our case), where at a macro level one needs to pay attention to sentences and at a micro level to words in the sentences. Next, we observe that while rendering the output, once the model pays attention to a field (say, occupation) it needs to stay on this field for a few timesteps (till all the occupations are produced in the output). We refer to this as the stay on behavior. Further, we note that once the tokens of a field are referred to, they are usually not referre"
N18-1139,P17-1098,1,0.925824,"lds in the infobox and the ordering of the fields within the infobox varies from person to person. Given the large size (700K examples) and heterogeneous nature of the dataset which contains biographies of people from different backgrounds (sports, politics, arts, etc.), it is hard to come up with simple rule-based templates for generating natural language descriptions from infoboxes, thereby making a case for datadriven models. Based on the recent success of data-driven neural models for various other NLG tasks (Bahdanau et al., 2014; Rush et al., 2015; Yao et al., 2015; Chopra et al., 2016; Nema et al., 2017), one simple choice is to treat the infobox as 1539 Proceedings of NAACL-HLT 2018, pages 1539–1550 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: Sample Infobox with description : V. Balakrishnan (born 1943 as Venkataraman Balakrishnan) is an Indian theoretical physicist who has worked in a number of fields of areas, including particle physics, many-body theory, the mechanical behavior of solids, dynamical systems, stochastic processes, and quantum dynamics. a sequence of {field, value} pairs and use a standard seq2seq model for this task."
N18-1139,D14-1162,0,0.0803607,"Missing"
N18-1139,C16-1275,0,0.019107,"e of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid approaches which rely on a combination of handcrafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al., 2014), query based document summarization (Nema et al., 2017) and so on. Most of these models are data hungry and are trained on large amounts of data. On the other hand, NLG from structured data has largely been studied in the context of small datasets such as W EATHER G OV (Liang et al., 2009), ROBO C UP (Chen and Mooney, 2008), NFL R ECAPS (Barzilay and Lapata, 2005), P RODIGY-M ETEO (Belz and Kow, 2009) and TUNA Challenge (Gatt and Belz, 2010). Recently Mei et al. (2016) proposed RNN/LSTM based neural encoder-de"
N18-1139,D15-1044,0,0.123136,"StructuredData_To_Descriptions Note that the number of fields in the infobox and the ordering of the fields within the infobox varies from person to person. Given the large size (700K examples) and heterogeneous nature of the dataset which contains biographies of people from different backgrounds (sports, politics, arts, etc.), it is hard to come up with simple rule-based templates for generating natural language descriptions from infoboxes, thereby making a case for datadriven models. Based on the recent success of data-driven neural models for various other NLG tasks (Bahdanau et al., 2014; Rush et al., 2015; Yao et al., 2015; Chopra et al., 2016; Nema et al., 2017), one simple choice is to treat the infobox as 1539 Proceedings of NAACL-HLT 2018, pages 1539–1550 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: Sample Infobox with description : V. Balakrishnan (born 1943 as Venkataraman Balakrishnan) is an Indian theoretical physicist who has worked in a number of fields of areas, including particle physics, many-body theory, the mechanical behavior of solids, dynamical systems, stochastic processes, and quantum dynamics. a sequence of {field, va"
N18-1139,P06-1139,0,0.0414598,"The approaches for NLG range from (i) rule based approaches (e.g., (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010)) (ii) modular statistical approaches which divide the process into three phases (planning, selection and surface realization) and use data driven approaches for one or more of these phases (Barzilay and Lapata, 2005; Belz, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013) (iii) hybrid approaches which rely on a combination of handcrafted rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011) and (iv) the more recent neural network based models (Bahdanau et al., 2014). 1540 Neural models for NLG have been proposed in the context of various tasks such as machine translation (Bahdanau et al., 2014), document summarization (Rush et al., 2015; Chopra et al., 2016), paraphrase generation (Prakash et al., 2016), image captioning (Xu et al., 2015), video summarization (Venugopalan et al., 2014), query based document summarization (Nema et al., 2017) and so on. Most of these models are data hungry and are trained on large amounts of data. On the other hand, NLG"
N18-1139,N16-1174,0,0.182517,"nd then decides which is the next appropriate occupation to attend to from the set of occupations (actor, director, producer, etc.). To enable this, we use a bifocal attention mechanism which computes an attention over fields at a macro level and over values at a micro level. We then fuse these attention weights such that the attention weight for a field also influences the attention over the values within it. Finally, we feed a fused context vector to the decoder which contains both field level and word level information. Note that such two-level attention mechanisms (Nallapati et al., 2016; Yang et al., 2016; Serban et al., 2016) have been used in the context of unstructured data (as opposed to structured data in our case), where at a macro level one needs to pay attention to sentences and at a micro level to words in the sentences. Next, we observe that while rendering the output, once the model pays attention to a field (say, occupation) it needs to stay on this field for a few timesteps (till all the occupations are produced in the output). We refer to this as the stay on behavior. Further, we note that once the tokens of a field are referred to, they are usually not referred to later. For exa"
N18-1139,W09-0607,0,\N,Missing
N18-1139,C98-1112,0,\N,Missing
N18-1139,N15-1173,0,\N,Missing
N18-2098,D13-1157,0,0.171258,"Missing"
N18-2098,D16-1128,0,0.316662,"Missing"
N18-2098,P09-1011,0,0.369031,"015) paradigm. In this approach, there is static attention on the attributes to compute the row representation followed by dynamic attention on the rows, which is subsequently fed to the decoder. This formulation is theoretically more efficient than the fully dynamic hierarchical attention framework followed by Nallapati et al. (2016). Also, our model does not need sophisticated sampling or sparsifying techniques like (Ling and Rush, 2017; Deng et al., 2017), thus, retaining differentiability. To demonstrate the efficacy of our approach, we transform the publicly available WEATHERGOV dataset (Liang et al., 2009) into fixed-schema tables, which is then used for our experiments. Our proposed mixed hierarchiStructured data summarization involves generation of natural language summaries from structured input data. In this work, we consider summarizing structured data occurring in the form of tables as they are prevalent across a wide variety of domains. We formulate the standard table summarization problem, which deals with tables conforming to a single predefined schema. To this end, we propose a mixed hierarchical attention based encoderdecoder model which is able to leverage the structure in addition"
N18-2098,D10-1049,0,0.0318564,"encodings, for example ‘6-21’ is encoded as ‘111100’ and ‘6-13’ is encoded as ‘110000’, the six bits corresponding to six atomic time intervals available in the dataset. Other attributes and words were encoded as one-hot vectors. Dataset and methodology: To evaluate our model we have used WEATHERGOV dataset (Liang et al., 2009) which is the standard benchmark dataset to evaluate tabular data summarization techniques. We compared the performance of our model against the state-of-the-art work of MBW (Mei et al., 2016), as well as two other baseline models KL (Konstas and Lapata, 2013) and ALK (Angeli et al., 2010). Dataset consists of a total of 29,528 tables (25000:1000:3528 ratio for train:validation:test splits) corresponding to scenarios created by collecting weather forecasts for 3,753 cities in the U.S.A over three days. There are 12 record types consisting of both numeric and categorical values. Each table contains 36 weather records (e.g., temperature, wind direction etc.) along with a corresponding natural language summary. 3.1 Training and hyperparameter tuning We used TensorFlow (Abadi et al., 2015) for our experiments. Encoder embeddings were initialized by generating the values from a unif"
N18-2098,W17-4505,0,0.121423,"Missing"
N18-2098,N16-1086,0,0.535212,"s this step where gr is the static record attention weight for rth record and q and P are weights to be learnt. We do not have any constraints on static attention vector. (3) αjr = sof tmaxj (I r ) X Br = αjr A¯rj (4) hr = GRU (B r ) (6) gr = σ(q T tanh(P cr )) (7) = v tanh(Ws st−1 + Wc cr ) (8) (5) j cr = [hr ; B r ] βtr zt = , T X r wtr (9) = sof tmaxr (βt ) gr wr , γtr = P t r g r wt (10) st = GRU (zt , st−1 ) (11) lt = W1 st + W2 zt + bl (12) pt = sof tmax(lt ) (13) γtr cr Dynamic Record attention for Decoder: Our decoder is a GRU based decoder with dynamic attention mechanism similar to (Mei et al., 2016) with modifications to modulate attention weights at each time step using static record attentions. At each time step t attention weights are calculated using 8, 9, 10, where γtr is the aggregated attention weight of record r at time step t. We use the soft attention over input encoder sequences cr to calculate the weighted average, which is passed to the GRU. GRU hidden state st is used to calculate output probabilities pt by using a softmax as described by equation 11, 12, 13, which is then used to get output word yt . Due to the static attention at attribute level, the time complexity of a"
N18-2098,W16-6626,0,0.118166,"Missing"
N18-2098,K16-1028,0,0.0758757,"summarization by leveraging the hierarchical nature of fixed-schema tables. In other words, rows consist of a fixed set of attributes and a table is defined by a set of rows. We cast this problem into a mixed hierarchical attention model following the encode-attend-decode (Cho et al., 2015) paradigm. In this approach, there is static attention on the attributes to compute the row representation followed by dynamic attention on the rows, which is subsequently fed to the decoder. This formulation is theoretically more efficient than the fully dynamic hierarchical attention framework followed by Nallapati et al. (2016). Also, our model does not need sophisticated sampling or sparsifying techniques like (Ling and Rush, 2017; Deng et al., 2017), thus, retaining differentiability. To demonstrate the efficacy of our approach, we transform the publicly available WEATHERGOV dataset (Liang et al., 2009) into fixed-schema tables, which is then used for our experiments. Our proposed mixed hierarchiStructured data summarization involves generation of natural language summaries from structured input data. In this work, we consider summarizing structured data occurring in the form of tables as they are prevalent across"
N18-2098,P02-1040,0,0.0999779,"Missing"
N18-2098,D17-1239,0,0.197015,"Missing"
N18-2098,P16-1195,0,0.0502464,"Missing"
N19-1382,C16-1053,0,0.0134654,"Moghe et al., 2018). These models typically differ in the components used for capturing interactions between query and document, capturing interactions between sentences in a document and refining the query/document representation over multiple passes (Shen et al., 2017; Dhingra et al., 2017; Sordoni et al., 2016). In particular, a co-attention network which computes the importance of every query word w.r.t. every document word and the importance of every document word w.r.t. every query word is an important component in most state of the art models (Hermann et al., 2015; Kadlec et al., 2016; Cao et al., 2016; Xiong et al., 2016; Seo et al., 2016; Gong and Bowman, 2017; Dhingra et al., 2017; Wang et al., 2017; Shen et al., 2017; Trischler et al., 2016; Group and Asia, 2017; Tan et al., 2017; Sordoni et al., 2016). Similarly, some models (Group and Asia, 2017; Seo et al., 2016; Hu et al., 2017) contain a self-attention network which computes the importance of every document word w.r.t. every other document word. In general, the most successful models (for example, BiDAF (Seo et al., 2016), QANeT (Yu et al., 2018)) use a combination of these components which capture all pairwise interactions and are"
N19-1382,P18-1012,0,0.0362136,"Missing"
N19-1382,P17-1168,0,0.0148115,"hood of the training data, the simple Related Work Over the past few years neural sequence prediction models which take a question as input and predict the corresponding answer span in a given document have evolved rapidly. Such models have also been adapted for dialog response prediction in the context of the Holl-E dataset (Moghe et al., 2018). These models typically differ in the components used for capturing interactions between query and document, capturing interactions between sentences in a document and refining the query/document representation over multiple passes (Shen et al., 2017; Dhingra et al., 2017; Sordoni et al., 2016). In particular, a co-attention network which computes the importance of every query word w.r.t. every document word and the importance of every document word w.r.t. every query word is an important component in most state of the art models (Hermann et al., 2015; Kadlec et al., 2016; Cao et al., 2016; Xiong et al., 2016; Seo et al., 2016; Gong and Bowman, 2017; Dhingra et al., 2017; Wang et al., 2017; Shen et al., 2017; Trischler et al., 2016; Group and Asia, 2017; Tan et al., 2017; Sordoni et al., 2016). Similarly, some models (Group and Asia, 2017; Seo et al., 2016; Hu"
N19-1382,D18-1232,0,0.0483072,"Missing"
N19-1382,P16-1086,0,0.0315554,"the Holl-E dataset (Moghe et al., 2018). These models typically differ in the components used for capturing interactions between query and document, capturing interactions between sentences in a document and refining the query/document representation over multiple passes (Shen et al., 2017; Dhingra et al., 2017; Sordoni et al., 2016). In particular, a co-attention network which computes the importance of every query word w.r.t. every document word and the importance of every document word w.r.t. every query word is an important component in most state of the art models (Hermann et al., 2015; Kadlec et al., 2016; Cao et al., 2016; Xiong et al., 2016; Seo et al., 2016; Gong and Bowman, 2017; Dhingra et al., 2017; Wang et al., 2017; Shen et al., 2017; Trischler et al., 2016; Group and Asia, 2017; Tan et al., 2017; Sordoni et al., 2016). Similarly, some models (Group and Asia, 2017; Seo et al., 2016; Hu et al., 2017) contain a self-attention network which computes the importance of every document word w.r.t. every other document word. In general, the most successful models (for example, BiDAF (Seo et al., 2016), QANeT (Yu et al., 2018)) use a combination of these components which capture all pairwise in"
N19-1382,D18-1255,1,0.911541,"and (iv) generating/extracting the correct answer span from the given document (Seo et al., 2016; Hu et al., 2017; Yu et al., 2018). While these models give state-of-the-art performance on a variety of datasets, they have very high space and time complexity. This is a concern, and in practice, it is often the case that one has to resort to restricting the maximum length of the input document such that the model can run with reasonable resources (say, a single 12GB Tesla K80 GPU). Such complex span prediction models are also being adapted for other NLP tasks such as dialog response prediction (Moghe et al., 2018), which is the focus of this work. In particular, we refer to the Holl-E dataset where the task is to extract the next response from a document which is relevant to the conversation (see Figure 1). This setup is very similar to QA wherein the input is {context, document} as opposed to {query, document} and the correct response span needs to be extracted from the given document. Given this similarity, it is natural to adopt existing QA models (Seo et al., 2016; Yu et al., 2018) for this task. However, the documents in Holl-E dataset are longer, and the authors specifically report that they were"
N19-1382,D14-1162,0,0.0828157,"still consumes a large amount of computational space and time during training and prediction. The QANeT model can also be modified for response prediction in a similar manner to BiDAF. Simple Attention Model for RP We now describe the simple attention model that we aim to learn. In a fashion similar to that of BiDAF and QANeT architectures, the simple model also operates in 3 distinct layers. See Figure 2 for an overview into the model. 3.3.1 Word Embedding Layer The words from the source document, the utterances by the prober and the responder are all encoded using standard GloVe embeddings (Pennington et al., 2014). 3.3.2 Contextual Embedding Layer In the next layer we encode the query (prober’s most recent utterance) using a BiGRU/BiLSTM, and encode the previous utterances of the prober and responder in a query sensitive manner. Query Encoder: Embedded query words are passed through BiGRU where final state qI ∈ R2d acts as query representation. Query Sensitive History Summariser: The history of the prober and responder are passed through a BiGRU to get context sensitive vectors hPj ∈ R2d and hRk ∈ R2d for j ∈ [J] and k ∈ [K]. These vectors are combined to get vectors hP and hR . This process of combini"
N19-1382,D16-1013,0,0.0131825,"interactions between sentences in a document and refining the query/document representation over multiple passes (Shen et al., 2017; Dhingra et al., 2017; Sordoni et al., 2016). In particular, a co-attention network which computes the importance of every query word w.r.t. every document word and the importance of every document word w.r.t. every query word is an important component in most state of the art models (Hermann et al., 2015; Kadlec et al., 2016; Cao et al., 2016; Xiong et al., 2016; Seo et al., 2016; Gong and Bowman, 2017; Dhingra et al., 2017; Wang et al., 2017; Shen et al., 2017; Trischler et al., 2016; Group and Asia, 2017; Tan et al., 2017; Sordoni et al., 2016). Similarly, some models (Group and Asia, 2017; Seo et al., 2016; Hu et al., 2017) contain a self-attention network which computes the importance of every document word w.r.t. every other document word. In general, the most successful models (for example, BiDAF (Seo et al., 2016), QANeT (Yu et al., 2018)) use a combination of these components which capture all pairwise interactions and are thus computationally very expensive. As a result, in practice, these models are not suitable for longer documents. We now quickly review existin"
N19-1382,P17-1018,0,0.0222443,"between query and document, capturing interactions between sentences in a document and refining the query/document representation over multiple passes (Shen et al., 2017; Dhingra et al., 2017; Sordoni et al., 2016). In particular, a co-attention network which computes the importance of every query word w.r.t. every document word and the importance of every document word w.r.t. every query word is an important component in most state of the art models (Hermann et al., 2015; Kadlec et al., 2016; Cao et al., 2016; Xiong et al., 2016; Seo et al., 2016; Gong and Bowman, 2017; Dhingra et al., 2017; Wang et al., 2017; Shen et al., 2017; Trischler et al., 2016; Group and Asia, 2017; Tan et al., 2017; Sordoni et al., 2016). Similarly, some models (Group and Asia, 2017; Seo et al., 2016; Hu et al., 2017) contain a self-attention network which computes the importance of every document word w.r.t. every other document word. In general, the most successful models (for example, BiDAF (Seo et al., 2016), QANeT (Yu et al., 2018)) use a combination of these components which capture all pairwise interactions and are thus computationally very expensive. As a result, in practice, these models are not suitable for long"
P10-1155,E09-1006,0,0.0747145,"Missing"
P10-1155,martinez-agirre-2004-effect,0,0.0554789,"Missing"
P10-1155,W09-2420,0,0.0692285,"Missing"
P10-1155,P07-1007,0,0.0603341,"arthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target words (Chan and Ng, 2007; Agirre and de Lacalle, 2009). To the best of our knowledge there does not exist a system that solves the combined problem of all words domain adapted WSD. We thus propose the following: In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal. Many supervised WSD systems have been built, but the effort of creating the training corpus - annotated sense marked corpora - has always been a matter of concern. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense"
P10-1155,W00-1322,0,0.0690684,"Missing"
P10-1155,O97-1002,0,0.0334337,"Missing"
P10-1155,P03-1054,0,0.00348854,"es by pairing the target word with its top-k neighbors in the thesaurus. Each target word is then disambiguated by assigning it its predominant sense – the motivation being that the predominant sense is a powerful, hard-to-beat baseline. We implemented their method using the following steps: i (1) where, i ∈ Candidate Synsets J = Set of disambiguated words θi = BelongingnessT oDominantConcept(Si) Vi = P (Si |word) 1. Obtain a domain-specific untagged corpus (we crawled a corpus of approximately 9M words from the web). 2. Extract grammatical relations from this text using a dependency parser2 (Klein and Manning, 2003). 3. Use the grammatical relations thus extracted to construct features for identifying the k nearest neighbors for each word using the distributional similarity score described in (Lin, 1998). 4. Rank the senses of each target word in the test set using a weighted sum of the distributional similarity scores of the neighbors. The weights in the sum are based on Wordnet Similarity scores (Patwardhan and Pedersen, 2003). 5. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. 3.3 Supervised approach Khapra et al. (20"
P10-1155,H05-1053,0,0.0936557,"apted WSD: Finding a Middle Ground between Supervision and Unsupervision Mitesh M. Khapra Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya Indian Institute of Technology Bombay, Mumbai - 400076, India. {miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.in Abstract of language competence, topic comprehension and domain sensitivity. This makes supervised approaches to WSD a difficult proposition (Agirre et al., 2009b; Agirre et al., 2009a; McCarthy et al., 2007). Unsupervised and knowledge based approaches have been tried with the hope of creating WSD systems with no need for sense marked corpora (Koeling et al., 2005; McCarthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target wo"
P10-1155,P98-2127,0,0.0702236,"owerful, hard-to-beat baseline. We implemented their method using the following steps: i (1) where, i ∈ Candidate Synsets J = Set of disambiguated words θi = BelongingnessT oDominantConcept(Si) Vi = P (Si |word) 1. Obtain a domain-specific untagged corpus (we crawled a corpus of approximately 9M words from the web). 2. Extract grammatical relations from this text using a dependency parser2 (Klein and Manning, 2003). 3. Use the grammatical relations thus extracted to construct features for identifying the k nearest neighbors for each word using the distributional similarity score described in (Lin, 1998). 4. Rank the senses of each target word in the test set using a weighted sum of the distributional similarity scores of the neighbors. The weights in the sum are based on Wordnet Similarity scores (Patwardhan and Pedersen, 2003). 5. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. 3.3 Supervised approach Khapra et al. (2010) proposed a supervised algorithm for domain-specific WSD and showed that it beats the most frequent corpus sense and performs on par with other state of the art algorithms like PageRank. We"
P10-1155,P04-1036,0,0.291314,"Missing"
P10-1155,J07-4005,0,0.141616,"iddle Ground between Supervision and Unsupervision Mitesh M. Khapra Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya Indian Institute of Technology Bombay, Mumbai - 400076, India. {miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.in Abstract of language competence, topic comprehension and domain sensitivity. This makes supervised approaches to WSD a difficult proposition (Agirre et al., 2009b; Agirre et al., 2009a; McCarthy et al., 2007). Unsupervised and knowledge based approaches have been tried with the hope of creating WSD systems with no need for sense marked corpora (Koeling et al., 2005; McCarthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target words (Chan and Ng, 2007;"
P10-1155,H93-1061,0,0.358441,"g a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD. a. For any target domain, create a small amount of sense annotated corpus. b. Mix it with an existing sense annotated corpus – from a mixed domain or specific domain – to train the WSD engine. 1 Introduction Amongst annotation tasks, sense marking surely takes the cake, demanding as it does high level This procedure tested on four adaptation scenarios, viz., (i) SemCor (Miller et al., 1993) to Tourism, (ii) SemCor to Health, (iii) Tourism to Health and (iv) Health to Tourism has consistently yielded good performance (to be explained in sections 6 and 7). The remainder of this paper is organized as follows. In section 2 we discuss previous work in the area of domain adaptation for WSD. In section 3 1532 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532–1541, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics we discuss three state of art supervised, unsupervised and knowledge based algorithms for WS"
P10-1155,P96-1006,0,0.288581,"Missing"
P10-1155,W04-0811,0,0.0602208,"Missing"
P10-1155,C98-2122,0,\N,Missing
P11-1057,C96-1005,0,0.525473,"Marathi (L2 ) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on"
P11-1057,D09-1048,1,0.691463,"its self corpus. This is very similar to the process of co-training (Blum and Mitchell, 1998) wherein the annotated data in the two languages can be seen as two different views of the same data. Hence, the classifier trained on one view can be improved by adding those untagged instances which are tagged with a high confidence by the classifier trained on the other view. The remainder of this paper is organized as follows. In section 2 we present related work. Section 3 describes the Synset aligned multilingual dictionary which facilitates parameter projection. Section 4 discusses the work of Khapra et al. (2009) on parameter projection. In section 5 we discuss bilingual bootstrapping which is the main focus of our work followed by a brief discussion on monolingual bootstrapping. Section 6 describes the experimental setup. In section 7 we present the results followed 562 by discussion in section 8. Section 9 concludes the paper. 2 Related Work Bootstrapping for Word Sense Disambiguation was first discussed in (Yarowsky, 1995). Starting with a very small number of seed collocations an initial decision list is created. This decisions list is then applied to untagged data and the instances which get tagg"
P11-1057,C10-1063,1,0.75332,"recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on accuracy by leveraging on the annotation work done in another language (L1 ). This is achieved with the help of a novel synsetaligned multilingual dictionary which facilitates the projection of parameters learned from the Wordnet and annotated corpus of L1 to L2 . This approach thus obviates the need for collecting large amounts of annotated corpora in multiple languages by relying on sufficient annotated corpus in one resource rich language. However, in many s"
P11-1057,W04-0834,0,0.165538,"and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1 ) and Marathi (L2 ) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amou"
P11-1057,J04-1001,0,0.0239784,"al. (2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language. They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available. However, they do not address situations where two resource deprived languages have aligned Wordnets but neither has sufficient annotated data. In such cases bilingual bootstrapping can be used so that the two languages can mutually benefit from each other’s small annotated data. Li and Li (2004) proposed a bilingual bootstrapping approach for the more specific task of Word Translation Disambiguation (WTD) as opposed to the more general task of WSD. This approach does not need parallel corpora (just like our approach) and relies only on in-domain corpora from two languages. However, their work was evaluated only on a handful of target words (9 nouns) for WTD as opposed to the broader task of WSD. Our work instead focuses on improving the performance of all words WSD for two resource deprived languages using bilingual bootstrapping. At the heart of our work lies parameter projection fa"
P11-1057,P04-1036,0,0.290564,"nguage pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on accuracy by leveraging"
P11-1057,H05-1052,0,0.141499,"tter than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on accuracy by leveraging on the annotation"
P11-1057,P96-1006,0,0.529794,"tagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1 ) and Marathi (L2 ) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost. 1 Introduction The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated da"
P11-1057,P95-1026,0,0.515652,"notated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages). To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD. Recent work by Khapra et al. (2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2 ) without compromising much on accuracy by leveraging on the annotation work done in another language (L1 ). This is achieved with the help of a novel synsetaligned multilingual dictionary which facili"
P13-1125,P06-1067,0,0.152968,"el that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. 1 Introduction Dealing with word order differences between source and target languages presents a significant challenge for machine translation systems. Failing to produce target words in the correct order results in machine translation output that is not fluent and is often very hard to understand. These problems are particularly severe when translating between languages which have very different structure. Phrase based systems (Koehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2"
P13-1125,J07-2003,0,0.0654996,"20.7 21.3 22.1 MT-08 eval News All 25.6 22.2 30.7 25.4 30.0 25.6 30.9 26.4 32.2 27.4 Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a st"
P13-1125,P05-1066,0,0.375263,"Missing"
P13-1125,D11-1018,0,0.412284,"and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) which uses manual word alignments for learning a reordering model. Specifically, we show that we can significantly improve reordering performance by using a large number of sentence pairs for which manual word alignments are not available. The mot"
P13-1125,P06-1121,0,0.0385503,"6 30.9 26.4 32.2 27.4 Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data wi"
P13-1125,W12-3134,0,0.0153685,"55.1 56.4 57.6 Table 3: mBLEU with different methods to generate reordering model training data from a machine aligned parallel corpus in addition to manual word alignments. Improvements in MT performance using the proposed models: We report results for a phrase based system with different preordering techniques. For results including a reordering model, we simply reorder the source side Urdu data both while training and at test time. In addition to 1281 phrase based systems with different preordering methods, we also report on a hierarchical phrase based system for which we used Joshua 4.0 (Ganitkevitch et al., 2012). We see a significant gain of 1.8 BLEU points in machine translation by going beyond manual word alignments using the best reordering model reported in Table 3. We also note a gain of 2.0 BLEU points over a hierarchical phrase based system. System type Baseline (no preordering) Hierarchical phrase based Reordering: Manual alignments + Machine alignments simple + machine alignments, model based Web 18.4 19.6 20.7 21.3 22.1 MT-08 eval News All 25.6 22.2 30.7 25.4 30.0 25.6 30.9 26.4 32.2 27.4 Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reor"
P13-1125,C10-1043,0,0.454711,"odels (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) whi"
P13-1125,I08-7017,0,0.0163489,"s to find source reorderings given a parallel corpus and alignments. We will see in the experimental section that this improves upon the simple heuristic for deriving reorderings described in Section 3. web4 . The parallel corpus is used for building our phrased based machine translation system and to add training data for our reordering model. For our English language model, we use the Gigaword English corpus in addition to the English side of our parallel corpus. Our Part-of-Speech tagger is a Maximum Entropy Markov model tagger trained on roughly fifty thousand words from the CRULP corpus (Hussain, 2008). For our machine translation experiments, we used a standard phrase based system (Al-Onaizan and Papineni, 2006) with a lexicalized distortion model with a window size of +/-4 words5 . To extract phrases we use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al., 2011). We report results on the (four reference) NIST MT-08 evaluation set in Table 4 for the News and Web conditions. The News and Web conditions each contain roughly 20K words in the test set, with the Web condition containing more informal text from the web. 5 Experimental setup Need for"
P13-1125,H05-1012,0,0.229253,"e apply a model (reordering or alignment) on part i that had not seen part i in training. This ensures that the alignment model does not see very optimistic reorderings and vice versa. We now describe the individual models, viz., P (a|ws , wt , π s , π t ) and C(π s |ws , a). 4.2 Modeling alignments given reordering In this section we describe how we fuse information from source and target reordering models to improve word alignments. As a base model we use the correction model for word alignments proposed by McCarley et al. (2011). This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment. The model thus needs a reasonably good initial alignment to start with for which we use the MaxEnt aligner (Ittycheriah and Roukos, 2005) as in McCarley et al. (2011). The correction model is a log-linear model: exp(λT φ(a, ws , wt )) P (a|ws , wt ) = . Z(ws , wt ) The λs are trained using the LBFGS algorithm (Liu et al., 1989) to max"
P13-1125,N03-1017,0,0.0135668,"MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. 1 Introduction Dealing with word order differences between source and target languages presents a significant challenge for machine translation systems. Failing to produce target words in the correct order results in machine translation output that is not fluent and is often very hard to understand. These problems are particularly severe when translating between languages which have very different structure. Phrase based systems (Koehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al.,"
P13-1125,P08-1068,0,0.0205333,"n reordering, our work is also more broadly related to several other efforts which we now outline. Setiawan et al. (2010) proposed the use of function word reordering to improve alignments. While this work is similar to one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). 8 Conclusion In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. We proposed a model that scores reorderings given alignments and the source sentence that we use to generate cleaner training data from noisy alignments. We also proposed a model that scores alignments given source and target sentence reorderin"
P13-1125,P06-1077,0,0.0360315,"Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or"
P13-1125,D11-1082,0,0.238648,"l word alignments and using machine alignments is the noise in the machine alignments which affects the performance of the reordering model (see Section 5). We illustrate this with the help of a motivating example. Consider the example English sentence and its translation shown in Figure 1. He went to the stadium to play vaha khelne keliye stadium ko gaya Figure 1: An example English sentence with its Urdu translation with alignment links. Red (dotted) links are incorrect links while the blue (dashed) links are the corresponding correct links. A standard word alignment algorithm that we used (McCarley et al., 2011) made the mistake of mis-aligning the Urdu ko and keliye (it switched the two). Deriving reference reorderings from these wrong alignments would give us an incorrect reordering. A reordering model trained on such incorrect reorderings would obviously perform poorly. Our task is thus two-fold (i) improve the quality of machine alignments (ii) use these less noisy alignments to derive cleaner training data for a reordering model. Before proceeding, we first point out that the two tasks, viz., reordering and word alignment are related: Having perfect reordering makes the alignment task easier whi"
P13-1125,N06-1020,0,0.00932004,"work is also more broadly related to several other efforts which we now outline. Setiawan et al. (2010) proposed the use of function word reordering to improve alignments. While this work is similar to one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). 8 Conclusion In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. We proposed a model that scores reorderings given alignments and the source sentence that we use to generate cleaner training data from noisy alignments. We also proposed a model that scores alignments given source and target sentence reorderings that improves a supe"
P13-1125,P05-1012,0,0.0279978,"inimal cost permutation by converting the problem into a symmetric Travelling Salesman Problem (TSP) and then using an implementation of the chained Lin-Kernighan heuristic (Applegate et al., 2003). The costs in the reordering model c(m, n) are parameterized by a linear model: c(m, n) = θ T Φ(w, m, n) where θ is a learned vector of weights and Φ is a vector of binary feature functions that inspect the words and POS tags of the source sentence at and around positions m and n. We use the features (Φ) described in Visweswariah et al. (2011) that were based on features used in dependency parsing (McDonald et al., 2005a). To learn the weight vector θ we require a corpus of sentences w with their desired reorderings π ∗ . Past work Visweswariah et al. (2011) used high quality manual word alignments to derive the desired reorderings π ∗ as follows. Given word aligned source and target sentences, we drop the source words that are not aligned1 . Let mi be the mean of the target word positions that the source word at index i is aligned to. We then sort the source indices in increasing order of mi (this order defines π ∗ ). If mi = mj (for example, because wi and wj are aligned to the same set of words) we keep t"
P13-1125,H05-1066,0,0.0322249,"Missing"
P13-1125,D12-1077,0,0.389674,"longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) which uses manual word alignments for learning a reordering model. Specifically, we show that we can significantly improve reordering performance by using a large number of sentence pairs for which manual word alignments are not available. The motivation for going beyond manual word alignments i"
P13-1125,C04-1073,0,0.673315,"xicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et"
P13-1125,P02-1040,0,0.0910746,"mber of sentences with manual word alignments that are used to train the reordering model. We see a roughly 3 mBLEU points drop in performance per halving of data indicating a potential for improvement by adding more data. In this section we describe the experimental setup that we used to evaluate the models proposed in this paper. All experiments were done on UrduEnglish and we evaluate reordering in two ways: Firstly, we evaluate reordering performance directly by comparing the reordered source sentence in Urdu with a reference reordering obtained from the manual word alignments using BLEU (Papineni et al., 2002) (we call this measure monolingual BLEU or mBLEU). All mBLEU results are reported on a small test set of about 400 sentences set aside from our set of sentences with manual word alignments. Additionally, we evaluate the effect of reordering on our final systems for machine translation measured using BLEU. We use about 10K sentences (180K words) of manual word alignments which were created in house using part of the NIST MT-08 training data3 to train our baseline reordering model and to train our supervised machine aligners. We use a parallel corpus of 3.9M words consisting of 1.7M words from t"
P13-1125,P02-1039,0,0.0336268,"6 22.2 30.7 25.4 30.0 25.6 30.9 26.4 32.2 27.4 Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preorderi"
P13-1125,W06-3119,0,0.024803,"rmance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is"
P13-1125,P09-1090,1,0.942154,"oehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approac"
P13-1125,D10-1052,0,0.0562855,"Missing"
P13-1125,D09-1058,0,0.0155931,"ly related to several other efforts which we now outline. Setiawan et al. (2010) proposed the use of function word reordering to improve alignments. While this work is similar to one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). 8 Conclusion In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. We proposed a model that scores reorderings given alignments and the source sentence that we use to generate cleaner training data from noisy alignments. We also proposed a model that scores alignments given source and target sentence reorderings that improves a supervised alignment model"
P13-1125,N04-4026,0,0.0419618,"lignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. 1 Introduction Dealing with word order differences between source and target languages presents a significant challenge for machine translation systems. Failing to produce target words in the correct order results in machine translation output that is not fluent and is often very hard to understand. These problems are particularly severe when translating between languages which have very different structure. Phrase based systems (Koehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently,"
P13-1125,D09-1105,0,0.187611,"ge parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an extension of Visweswariah et al. (2011) and we focus on being able to incorporate relatively noisy machine alignments to improve the reordering model. In addition to being related to work in reordering, our work is also more broadly related to seve"
P13-1125,C10-1126,1,0.944794,"zan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) which uses manual word alignmen"
P13-1125,D11-1045,1,0.706825,"red to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) which uses manual word alignments for learning a reordering model. Specifically, we show that we can significantly improve reordering performance by using a large number of sentence pairs for which manual word alignments are not available. The motivation for going beyond ma"
P13-1125,D07-1077,0,0.559736,"se based systems (Koehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper,"
P13-1125,J97-3002,0,0.0158592,"one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). 8 Conclusion In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. We proposed a model that scores reorderings given alignments and the source sentence that we use to generate cleaner training data from noisy alignments. We also proposed a model that scores alignments given source and target sentence reorderings that improves a supervised alignment model by 2.6 points in f-Measure. While the improvement in alignment performance is modest, the improvement does result in improved reordering models. Cumulatively, we see"
P17-1098,N16-1012,0,0.0498665,"rk on abstractive summarization in the context of DUC-2003 and DUC-2004 contests (Zajic et al.). We refer the reader to (Das and Martins, 2007) and (Nenkova and McKeown, 2012) for an excellent survey of 1064 the field. Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014). For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model. Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories. Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets. Hu et al. (2015) introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it. One recurring problem in encoder-decoder models for NLG is that they often repeat the same phrase/word multiple times in the summary (at the cost of both coherency and fluency). Sankaran et al. (2016) study this problem in the context of MT and propose a temporal attention model which enforces the attention weights for successive time steps to be different from each other."
P17-1098,N15-1014,0,0.0341857,"Missing"
P17-1098,D13-1155,0,0.0238544,"Missing"
P17-1098,D15-1229,0,0.00637352,"ader to (Das and Martins, 2007) and (Nenkova and McKeown, 2012) for an excellent survey of 1064 the field. Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014). For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model. Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories. Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets. Hu et al. (2015) introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it. One recurring problem in encoder-decoder models for NLG is that they often repeat the same phrase/word multiple times in the summary (at the cost of both coherency and fluency). Sankaran et al. (2016) study this problem in the context of MT and propose a temporal attention model which enforces the attention weights for successive time steps to be different from each other. Similarly, and more relevant to this work, Chen et al. (2016) propose a distraction based attention model wh"
P17-1098,P16-1094,0,0.0369852,"Missing"
P17-1098,W01-0100,0,0.232137,"Missing"
P17-1098,K16-1028,0,0.0653845,"Missing"
P17-1098,D15-1044,0,0.125581,"Missing"
P17-1098,N06-2050,0,0.0914555,"Missing"
P17-1098,P09-1062,0,0.0643599,"Missing"
P18-1156,D14-1159,0,0.0239504,"e created. For example, in SQuAD (Rajpurkar et al., 2016a), NewsQA (Trischler et al., 2016), TriviaQA (Joshi et al., 2017) and MovieQA (Tapaswi et al., 2016) the answers correspond to a span in the document. MSMARCO uses web queries as questions and the answers are synthesized by workers from documents relevant to the query. On the other hand, in most cloze-style datasets (Mostafazadeh et al., 2016; Onishi et al., 2016) the questions are created automatically by deleting a word/entity from a sentence. There are also some datasets for RC with multiple choice questions (Richardson et al., 2013; Berant et al., 2014; Lai et al., 2017) where the task is to select one among k given candidate answers. Another notable RC Dataset is NarrativeQA(s Koˇ cisk´y et al., 2018) which contains 40K QA pairs created from plot summaries of movies. It poses two tasks, where the first task involves reading the plot summaries from which the QA pairs were annotated and the second task is read the entire book or movie script (which is usually 60K words long) instead of the summary to answer the question. As acknowledged by the authors, while the first task is similar in scope to the previous datasets, the second task is at p"
P18-1156,D17-1215,0,0.049236,"ning and background knowledge) which is the case with a lot of real-world content such as story books, movies, news reports, etc. (ii) their questions possess a large lexical overlap with segments of the passage, or have a high noise level in QA pairs themselves. As demonstrated by recent work, this makes it easy for even simple keyword matching algorithms to achieve high accuracy (Weissenborn et al., 2017). In fact, these models have been shown to perform poorly in the presence of adversarially inserted sentences which have a high word overlap with the question but do not contain the answer (Jia and Liang, 2017). While this problem does not exist in TriviaQA it is admittedly noisy because of the use of distant supervision. Similarly, for cloze-style datasets, due to the automatic question generation process, it is very easy for current models to reach near human performance (Cui, 2017). This therefore limits the complexity in language understanding that a machine is required to demonstrate to do well on the RC task. Motivated by these shortcomings and to push the state-of-the-art in language understanding in RC, in this paper we propose DuoRC, which specifically presents the following challenges beyo"
P18-1156,P17-1147,0,0.270435,", Reading Comprehension (RC) systems are required to “understand” a given text passage as input and then answer questions based on it. It is therefore critical, that the dataset benchmarks established for the RC task keep progressing in complexity to reflect the challenges that arise in true language understanding, thereby enabling the development of models and techniques to solve these challenges. For RC in particular, there has been significant progress over the recent years with several benchmark datasets, the most popular of which are the SQuAD dataset (Rajpurkar et al., 2016a), TriviaQA (Joshi et al., 2017), MS MARCO (Nguyen et al., 2016), MovieQA (Tapaswi et al., 2016) and clozestyle datasets (Mostafazadeh et al., 2016; Onishi et al., 2016; Hermann et al., 2015). However, these benchmarks, owing to both the nature of the passages and the QA pairs to evaluate the RC task, have 2 primary limitations in studying language understanding: (i) Other than MovieQA, which is 1683 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1683–1693 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics a small dataset of"
P18-1156,D17-1082,0,0.0931429,"e, in SQuAD (Rajpurkar et al., 2016a), NewsQA (Trischler et al., 2016), TriviaQA (Joshi et al., 2017) and MovieQA (Tapaswi et al., 2016) the answers correspond to a span in the document. MSMARCO uses web queries as questions and the answers are synthesized by workers from documents relevant to the query. On the other hand, in most cloze-style datasets (Mostafazadeh et al., 2016; Onishi et al., 2016) the questions are created automatically by deleting a word/entity from a sentence. There are also some datasets for RC with multiple choice questions (Richardson et al., 2013; Berant et al., 2014; Lai et al., 2017) where the task is to select one among k given candidate answers. Another notable RC Dataset is NarrativeQA(s Koˇ cisk´y et al., 2018) which contains 40K QA pairs created from plot summaries of movies. It poses two tasks, where the first task involves reading the plot summaries from which the QA pairs were annotated and the second task is read the entire book or movie script (which is usually 60K words long) instead of the summary to answer the question. As acknowledged by the authors, while the first task is similar in scope to the previous datasets, the second task is at present, intractable"
P18-1156,N16-1098,0,0.0653328,"Missing"
P18-1156,P17-1098,1,0.893346,"Missing"
P18-1156,D16-1241,0,0.184656,"t is therefore critical, that the dataset benchmarks established for the RC task keep progressing in complexity to reflect the challenges that arise in true language understanding, thereby enabling the development of models and techniques to solve these challenges. For RC in particular, there has been significant progress over the recent years with several benchmark datasets, the most popular of which are the SQuAD dataset (Rajpurkar et al., 2016a), TriviaQA (Joshi et al., 2017), MS MARCO (Nguyen et al., 2016), MovieQA (Tapaswi et al., 2016) and clozestyle datasets (Mostafazadeh et al., 2016; Onishi et al., 2016; Hermann et al., 2015). However, these benchmarks, owing to both the nature of the passages and the QA pairs to evaluate the RC task, have 2 primary limitations in studying language understanding: (i) Other than MovieQA, which is 1683 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1683–1693 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics a small dataset of 15K QA pairs, all other largescale RC datasets deal only with factual descriptive passages and not narratives (involving events with ca"
P18-1156,W14-1609,0,0.0205898,"Missing"
P18-1156,D14-1162,0,0.0795801,"Missing"
P18-1156,D16-1264,0,0.700397,"r task-specific goals. In particular, Reading Comprehension (RC) systems are required to “understand” a given text passage as input and then answer questions based on it. It is therefore critical, that the dataset benchmarks established for the RC task keep progressing in complexity to reflect the challenges that arise in true language understanding, thereby enabling the development of models and techniques to solve these challenges. For RC in particular, there has been significant progress over the recent years with several benchmark datasets, the most popular of which are the SQuAD dataset (Rajpurkar et al., 2016a), TriviaQA (Joshi et al., 2017), MS MARCO (Nguyen et al., 2016), MovieQA (Tapaswi et al., 2016) and clozestyle datasets (Mostafazadeh et al., 2016; Onishi et al., 2016; Hermann et al., 2015). However, these benchmarks, owing to both the nature of the passages and the QA pairs to evaluate the RC task, have 2 primary limitations in studying language understanding: (i) Other than MovieQA, which is 1683 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1683–1693 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computation"
P18-1156,D13-1020,0,0.0588349,"questions and answers are created. For example, in SQuAD (Rajpurkar et al., 2016a), NewsQA (Trischler et al., 2016), TriviaQA (Joshi et al., 2017) and MovieQA (Tapaswi et al., 2016) the answers correspond to a span in the document. MSMARCO uses web queries as questions and the answers are synthesized by workers from documents relevant to the query. On the other hand, in most cloze-style datasets (Mostafazadeh et al., 2016; Onishi et al., 2016) the questions are created automatically by deleting a word/entity from a sentence. There are also some datasets for RC with multiple choice questions (Richardson et al., 2013; Berant et al., 2014; Lai et al., 2017) where the task is to select one among k given candidate answers. Another notable RC Dataset is NarrativeQA(s Koˇ cisk´y et al., 2018) which contains 40K QA pairs created from plot summaries of movies. It poses two tasks, where the first task involves reading the plot summaries from which the QA pairs were annotated and the second task is read the entire book or movie script (which is usually 60K words long) instead of the summary to answer the question. As acknowledged by the authors, while the first task is similar in scope to the previous datasets, th"
P18-1156,K17-1028,0,0.0294708,"Computational Linguistics a small dataset of 15K QA pairs, all other largescale RC datasets deal only with factual descriptive passages and not narratives (involving events with causality linkages that require reasoning and background knowledge) which is the case with a lot of real-world content such as story books, movies, news reports, etc. (ii) their questions possess a large lexical overlap with segments of the passage, or have a high noise level in QA pairs themselves. As demonstrated by recent work, this makes it easy for even simple keyword matching algorithms to achieve high accuracy (Weissenborn et al., 2017). In fact, these models have been shown to perform poorly in the presence of adversarially inserted sentences which have a high word overlap with the question but do not contain the answer (Jia and Liang, 2017). While this problem does not exist in TriviaQA it is admittedly noisy because of the use of distant supervision. Similarly, for cloze-style datasets, due to the automatic question generation process, it is very easy for current models to reach near human performance (Cui, 2017). This therefore limits the complexity in language understanding that a machine is required to demonstrate to d"
Q18-1022,P15-1166,0,0.151875,"dels that are better at transliterating multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models w"
Q18-1022,2010.iwslt-papers.7,0,0.0294356,"zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration"
Q18-1022,W15-3909,0,0.118921,"discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate trans"
Q18-1022,W16-2711,0,0.0547497,"rections. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify"
Q18-1022,N16-1101,0,0.0730592,"r at transliterating multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal paramete"
Q18-1022,N16-1155,0,0.0723241,"Missing"
Q18-1022,S17-2033,0,0.030994,"er architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts o"
Q18-1022,D12-1002,0,0.0350666,"Missing"
Q18-1022,P08-1103,0,0.0451685,"tup, results and analysis. Section 6 discusses various zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously"
Q18-1022,W09-3504,0,0.234378,"Section 6 discusses various zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilingua"
Q18-1022,Q17-1024,0,0.446771,"g may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are beneficial, without increasing the model size."
Q18-1022,N10-1065,1,0.79409,"se compatible scripts resulting in a shared vocabulary. We specialize just the output layer for target languages, but share the encoder, decoder and character embeddings across languages. In this respect, we differ from Johnson et al. (2017). They share all network components across languages, but add an artificial token at the beginning of the input sequence to indicate the target language. Zeroshot Transliteration We use the multilingual models to address zeroshot transliteration. Zeroshot transliteration using bridge/pivot language has been explored for statistical machine transliteration (Khapra et al., 2010) as well as neural machine transliteration (Saha et al., 2016). Unlike previous approaches which pivot over bilingual transliteration models, we propose zeroshot transliteration that pivots over multilingual transliteration models. We also propose a direct zeroshot transliteration method, a scenario which has been explored for machine translation by Johnson et al. (2017), but not investigated previously for transliteration. In our zeroshot model, sequences from multiple source languages are mapped to a common encoder representation without the need for a parallel corpus between the source lang"
Q18-1022,P06-1103,0,0.0550269,"nguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network for particular languages: specialized encoders (Zoph et al., 2016), decoders (Dong et al., 2"
Q18-1022,W15-3912,1,0.896917,"Missing"
Q18-1022,N15-3017,1,0.810922,"he parallel corpora are roughly of the same size. Better training schedules could be explored in future. Languages: We experimented with two sets of orthographically similar languages: Indian languages: (i) Hindi (hi), Bengali (bn) from the Indo-Aryan branch of Indo-European family (ii) Kannada (kn), Tamil (ta) from the Dravidian family. We studied Indic-Indic transliteration and transliteration involving a non-Indian language (English↔Indic). We mapped equivalent characters in different Indic scripts in order to build a common vocabulary based on the common offsets of the Unicode codepoints (Kunchukuttan et al., 2015). Slavic languages: Czech (cs), Polish (pl), Slovenian (sl) and Slovak (sk). We studied Arabic↔Slavic transliteration. Arabic is a non-Slavic language (Semitic branch of Afro-Asiatic) and uses an abjad script in which vowel diacritics are omitted in general usage. The languages chosen are representative of languages spoken by some major groups of peoples en-Indic en-hi 12K en-bn 13K en-kn 10K en-ta 10K Indic-en Indic-Indic hi-en 18K bn kn ta bn-en 12K hi 3620 5085 5290 kn-en 15K bn 2720 2901 ta-en 15K kn 4216 ar-Slavic ar-cs 15K ar-pl 15K ar-sl 10K ar-sk 10K Pair Table 1: Training set statisti"
Q18-1022,K16-1027,1,0.859366,"ding the phonetic properties of the character, one bit for each value of every property. The multiplication of the phonetic feature vector with the weight matrix in the first layer generates phonetic embeddings for each character. These are inputs to the encoder. Apart from this input change, the rest of the network architecture is the same as described in Section 3.2. Experiments: We experimented with Indian languages (Indic→English and Indic-Indic transliteration). Indic scripts generally have a one-one correspondence from characters to phonemes. Hence, we use phonetic features described by Kunchukuttan et al. (2016) to generate phonetic feature vectors for characters (available via the Indic NLP Library1 ). These Indic languages are spoken by nearly a billion people and hence the use of phonetic features is useful for many of the world’s most widely spoken languages. Results and Discussion: Table 8 shows the results. We observe that phonetic feature input improves transliteration accuracy for Indic-English transliteration. The improvements are primarily due to reduction in errors related to similar consonants like (T,D), (P,B), (C,K) and the use of H for aspiration. 1 https://github.com/anoopkunchukuttan"
Q18-1022,Q17-1026,0,0.0551909,"multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are bene"
Q18-1022,W12-4810,0,0.0317588,"r its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network f"
Q18-1022,E17-3017,0,0.0348879,"ayer (stride size = 1 and SAME padding), followed by ReLU units and max pooling. We use filters of different sizes and concatenate their output to produce the encoder output. Figure 1b shows a schematic of the encoder. We chose CNN over the conventional bidirectional LSTM layer since the temporal dependencies for transliteration are mostly local, which can be handled by the CNN encoder. We observed that training and decoding are significantly faster, with little impact on accuracy. The decoder contains a layer of LSTM cells and their start state is the average of the encoder’s output vectors (Sennrich et al., 2017). set for English-Hindi, this model was used for reporting test set results for English-Hindi. We observed that this criterion performed better than choosing the model with least validation set loss over all language pairs. Parameter Sharing: The vocabulary of the orthographically similar languages (at input and/or output) is comprised of the union of character sets of all these languages. Since the character set of these languages overlaps to a large extent, we share their character embeddings too. The encoder is shared across all source languages and the decoder is shared across all target l"
Q18-1022,I08-1009,0,0.0326398,"n with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that s"
Q18-1022,P07-1015,0,0.0213722,"n To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network for particular languages: specialized encoders (Zoph et al., 2016), decoders (Dong et al., 2015) or both (Firat"
Q18-1022,D16-1163,0,0.109551,"pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are beneficial, without inc"
Q19-1034,J90-1003,0,0.290109,"like Hinglish (Hindi–English) (Banerjee et al., 2018), then we need an alternate way of extracting a graph structure from the utterances. One simple solution that has worked well in practice was to create a word co-occurrence matrix from the entire corpus where the context window is an entire sentence. Once we have such a co-occurrence matrix, for a given sentence we can connect an edge between two words if their co-occurrence frequency is above a threshold value. The co-occurrence matrix can either contain co-occurrence frequency counts or positivepointwise mutual information (PPMI) values (Church and Hanks, 1990; Dagan et al., 1993; Niwa and Nitta, 1994). (15) Here v2 , W3 , W4 , and W5 are parameters. Finally, we obtain a query and history aware representation of the KB by computing an attention score over all the nodes in the final H layer of KB-GCN using hQ t and ht as shown below: ωjt = v3T tanh(W6 rfj + W7 dt−1 + W8 hQ t γ t = softmax(ω t ) m f hK t = j  =1 γj  t rj  (16) (17) (18) Here v3 , W6 , W7 , W8 and W9 are parameters. This sequential attention mechanism is illustrated in Figure 2. For simplicity, we depict the GCN and RNN+GCN encoders as blocks. The internal structure of these block"
Q19-1034,P93-1022,0,0.337773,"glish) (Banerjee et al., 2018), then we need an alternate way of extracting a graph structure from the utterances. One simple solution that has worked well in practice was to create a word co-occurrence matrix from the entire corpus where the context window is an entire sentence. Once we have such a co-occurrence matrix, for a given sentence we can connect an edge between two words if their co-occurrence frequency is above a threshold value. The co-occurrence matrix can either contain co-occurrence frequency counts or positivepointwise mutual information (PPMI) values (Church and Hanks, 1990; Dagan et al., 1993; Niwa and Nitta, 1994). (15) Here v2 , W3 , W4 , and W5 are parameters. Finally, we obtain a query and history aware representation of the KB by computing an attention score over all the nodes in the final H layer of KB-GCN using hQ t and ht as shown below: ωjt = v3T tanh(W6 rfj + W7 dt−1 + W8 hQ t γ t = softmax(ω t ) m f hK t = j  =1 γj  t rj  (16) (17) (18) Here v3 , W6 , W7 , W8 and W9 are parameters. This sequential attention mechanism is illustrated in Figure 2. For simplicity, we depict the GCN and RNN+GCN encoders as blocks. The internal structure of these blocks are shown in Figur"
Q19-1034,C18-1319,1,0.927298,"arse for the utterances. To overcome this, we construct a co-occurrence matrix from the entire corpus and use this matrix to impose a graph structure on the utterances. More specifically, we add an edge between two words in a sentence if they co-occur frequently in the corpus. Our experiments suggest that this simple strategy acts as a reasonable substitute for dependency parse trees. We perform experiments with the modified DSTC2 (Bordes et al., 2017) dataset, which contains goal-oriented conversations for making restaurant reservations. We also use its recently released code-mixed versions (Banerjee et al., 2018), which contain code-mixed conversations in four different languages: Hindi, Bengali, Gujarati, and Tamil. We compare with recent state-of-theart methods and show that on average, the proposed model gives an improvement of 2.8 BLEU points and 2 ROUGE points. We also perform experiments on two human–human dialogue datasets of different sizes: (i) Cam676 (Wen et al., 2017): a small scale dataset containing 676 dialogues from the restaurant domain; and (ii) MultiWOZ (Budzianowski et al., 2018): a largescale dataset containing around 10k dialogues and spanning multiple domains for each dialogue. O"
Q19-1034,N19-1240,0,0.0337335,"Missing"
Q19-1034,D17-1209,0,0.0990796,"code them along with the entities in the knowledge base. The encodings of the utterances and memory elements are then suitably combined using an attention network and fed to the decoder to generate the response, one word at a time. However, these methods do not exploit the structure in the knowledge base as defined by entity–entity relations and the structure in the utterances as defined by a dependency parse. Such structural information can be exploited to improve the performance of the system, as demonstrated by recent works on syntax-aware neural machine translation (Eriguchi et al., 2016; Bastings et al., 2017; Chen et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), and document dating (Vashishth et al., 2018), which use Graph Convolutional Networks (GCNs) Introduction Goal-oriented dialogue systems that can assist humans in various day-to-day activities have 485 Transactions of the Association for Computational Linguistics, vol. 7, pp. 485–500, 2019. https://doi.org/10.1162/tacl a 00284 Action Editor: Asli Celikyilmaz. Submission batch: 1/2019; Revision batch: 5/2019; Published 9/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  (De"
Q19-1034,D18-1547,0,0.0334895,"Missing"
Q19-1034,P17-1177,0,0.107704,"e entities in the knowledge base. The encodings of the utterances and memory elements are then suitably combined using an attention network and fed to the decoder to generate the response, one word at a time. However, these methods do not exploit the structure in the knowledge base as defined by entity–entity relations and the structure in the utterances as defined by a dependency parse. Such structural information can be exploited to improve the performance of the system, as demonstrated by recent works on syntax-aware neural machine translation (Eriguchi et al., 2016; Bastings et al., 2017; Chen et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), and document dating (Vashishth et al., 2018), which use Graph Convolutional Networks (GCNs) Introduction Goal-oriented dialogue systems that can assist humans in various day-to-day activities have 485 Transactions of the Association for Computational Linguistics, vol. 7, pp. 485–500, 2019. https://doi.org/10.1162/tacl a 00284 Action Editor: Asli Celikyilmaz. Submission batch: 1/2019; Revision batch: 5/2019; Published 9/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  (Defferrard et al., 201"
Q19-1034,W17-5506,0,0.0187544,"., 2016) to the memory network in order to regulate the access to the memory blocks. Seo et al. (2017) developed a variant of RNN cell that computes a refined representation of the query over multiple iterations before querying the memory. However, all these approaches retrieve the response from a set of 486 candidate responses and such a candidate set is not easy to obtain for any new domain of interest. To account for this, Eric and Manning (2017) and Zhao et al. (2017) adapted RNN-based encoderdecoder models to generate appropriate responses instead of retrieving them from a candidate set. Eric et al. (2017) introduced a key-value memory network based generative model that integrates the underlying KB with RNN-based encodeattend-decode models. Madotto et al. (2018) used memory networks on top of the RNN decoder to tightly integrate KB entities with the decoder in order to generate more informative responses. However, as opposed to our work, all these works ignore the underlying structure of the entity– entity graph of the KB and the syntactic structure of the utterances. To the best of our knowledge, ours is the first work that uses GCNs to incorporate dependency structural information and the en"
Q19-1034,D14-1179,0,0.0201481,"Missing"
Q19-1034,E17-2075,0,0.349874,"ied DSTC2 dataset, (ii) recently released code-mixed versions of DSTC2 dataset in four languages, (iii) Wizard-of-Oz style CAM676 dataset, and (iv) Wizard-of-Oz style MultiWOZ dataset. On all four datasets our method outperforms existing methods, on a wide range of evaluation metrics. 1 More formally, the task here is to generate the next response given (i) the previous utterances in the conversation history, (ii) the current user utterance (known as the query), and (iii) the entities and their relationships in the associated knowledge base. Current state-of-the-art methods (Seo et al., 2017; Eric and Manning, 2017; Madotto et al., 2018) typically use variants of Recurrent Neural Networks (RNNs) (Elman, 1990) to encode the history and current utterance or an external memory network (Sukhbaatar et al., 2015) to encode them along with the entities in the knowledge base. The encodings of the utterances and memory elements are then suitably combined using an attention network and fed to the decoder to generate the response, one word at a time. However, these methods do not exploit the structure in the knowledge base as defined by entity–entity relations and the structure in the utterances as defined by a de"
Q19-1034,E17-1001,0,0.0254728,"ogue state and required per-module based labeling. To mitigate this problem, Bordes et al. (2017) released a version of goal-oriented dialogue dataset that focuses on the development of end-to-end neural models. Such models need to reason over the associated KB triples and generate responses directly from the utterances without any additional annotations. For example, Bordes et al. (2017) proposed a Memory Network (Sukhbaatar et al., 2015) based model to match the response candidates with the multi-hop attention weighted representation of the conversation history and the KB triples in memory. Liu and Perez (2017) further added highway (Srivastava et al., 2015) and residual connections (He et al., 2016) to the memory network in order to regulate the access to the memory blocks. Seo et al. (2017) developed a variant of RNN cell that computes a refined representation of the query over multiple iterations before querying the memory. However, all these approaches retrieve the response from a set of 486 candidate responses and such a candidate set is not easy to obtain for any new domain of interest. To account for this, Eric and Manning (2017) and Zhao et al. (2017) adapted RNN-based encoderdecoder models"
Q19-1034,P16-1078,0,0.133676,"tar et al., 2015) to encode them along with the entities in the knowledge base. The encodings of the utterances and memory elements are then suitably combined using an attention network and fed to the decoder to generate the response, one word at a time. However, these methods do not exploit the structure in the knowledge base as defined by entity–entity relations and the structure in the utterances as defined by a dependency parse. Such structural information can be exploited to improve the performance of the system, as demonstrated by recent works on syntax-aware neural machine translation (Eriguchi et al., 2016; Bastings et al., 2017; Chen et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), and document dating (Vashishth et al., 2018), which use Graph Convolutional Networks (GCNs) Introduction Goal-oriented dialogue systems that can assist humans in various day-to-day activities have 485 Transactions of the Association for Computational Linguistics, vol. 7, pp. 485–500, 2019. https://doi.org/10.1162/tacl a 00284 Action Editor: Asli Celikyilmaz. Submission batch: 1/2019; Revision batch: 5/2019; Published 9/2019. c 2019 Association for Computational Linguistics. Distributed under a C"
Q19-1034,P18-1136,0,0.193513,"recently released code-mixed versions of DSTC2 dataset in four languages, (iii) Wizard-of-Oz style CAM676 dataset, and (iv) Wizard-of-Oz style MultiWOZ dataset. On all four datasets our method outperforms existing methods, on a wide range of evaluation metrics. 1 More formally, the task here is to generate the next response given (i) the previous utterances in the conversation history, (ii) the current user utterance (known as the query), and (iii) the entities and their relationships in the associated knowledge base. Current state-of-the-art methods (Seo et al., 2017; Eric and Manning, 2017; Madotto et al., 2018) typically use variants of Recurrent Neural Networks (RNNs) (Elman, 1990) to encode the history and current utterance or an external memory network (Sukhbaatar et al., 2015) to encode them along with the entities in the knowledge base. The encodings of the utterances and memory elements are then suitably combined using an attention network and fed to the decoder to generate the response, one word at a time. However, these methods do not exploit the structure in the knowledge base as defined by entity–entity relations and the structure in the utterances as defined by a dependency parse. Such st"
Q19-1034,N18-2078,0,0.0167032,"and show that their formulation is applicable to any graph structure and Tree-LSTMs can be thought of as a special case of it. In parallel, Graph Convolutional Networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kipf and Welling, 2017) and their variants (Li et al., 2016) have emerged as state-of-the-art methods for computing representations of entities in a knowledge graph. They provide a more flexible way of encoding such graph structures by capturing multi-hop relationships between nodes. This has led to their adoption for various NLP tasks such as neural machine translation (Marcheggiani et al., 2018; Bastings et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), document dating (Vashishth et al., 2018), and question answering (Johnson, 2017; De Cao et al., 2019). u∈N (v ) (1) Here W ∈ Rd×m is the model parameter matrix, b ∈ Rd is the bias vector, and ReLU is the rectified linear unit activation function. N (v ) is the set of neighbors of node v and is assumed to also include the node v so that the previous representation of the node v is also considered while computing its new hidden representation. To capture interactions with nodes that are multiple hops away, multiple"
Q19-1034,W14-4337,0,0.226344,"We leverage co-occurrence frequencies and PPMI (positive-pointwise mutual information) values to construct contextual graphs for code-mixed utterances; and (iv) We show that the proposed model obtains state-of-the-art results on four different datasets spanning five different languages. 2 Related Work In this section, we review the previous work in goal-oriented dialogue systems and describe the introduction of GCNs in NLP. Goal-Oriented Dialogue Systems: Initial goaloriented dialogue systems (Young, 2000; Williams and Young, 2007) were based on dialogue state tracking (Williams et al., 2013; Henderson et al., 2014a,b) and included pipelined modules for natural language understanding, dialogue state tracking, policy management, and natural language generation. Wen et al. (2017) used neural networks for these intermediate modules but still lacked absolute end-to-end trainability. Such pipelined modules were restricted by the fixed slot-structure assumptions on the dialogue state and required per-module based labeling. To mitigate this problem, Bordes et al. (2017) released a version of goal-oriented dialogue dataset that focuses on the development of end-to-end neural models. Such models need to reason o"
Q19-1034,D17-1159,0,0.0648667,"Missing"
Q19-1034,C94-1049,0,0.168673,"al., 2018), then we need an alternate way of extracting a graph structure from the utterances. One simple solution that has worked well in practice was to create a word co-occurrence matrix from the entire corpus where the context window is an entire sentence. Once we have such a co-occurrence matrix, for a given sentence we can connect an edge between two words if their co-occurrence frequency is above a threshold value. The co-occurrence matrix can either contain co-occurrence frequency counts or positivepointwise mutual information (PPMI) values (Church and Hanks, 1990; Dagan et al., 1993; Niwa and Nitta, 1994). (15) Here v2 , W3 , W4 , and W5 are parameters. Finally, we obtain a query and history aware representation of the KB by computing an attention score over all the nodes in the final H layer of KB-GCN using hQ t and ht as shown below: ωjt = v3T tanh(W6 rfj + W7 dt−1 + W8 hQ t γ t = softmax(ω t ) m f hK t = j  =1 γj  t rj  (16) (17) (18) Here v3 , W6 , W7 , W8 and W9 are parameters. This sequential attention mechanism is illustrated in Figure 2. For simplicity, we depict the GCN and RNN+GCN encoders as blocks. The internal structure of these blocks are shown in Figure 1. 5 Experimental Set"
Q19-1034,P02-1040,0,0.107425,"ngma and Ba, 2015) and tuned the initial learning rates in the range of 0.0006 to 0.001. For regularization we used an L2 penalty of 0.001 in addition to a dropout (Srivastava et al., 2014) of 0.1. We used randomly initialized word embeddings of size 300. The RNN and GCN hidden dimensions were also chosen to be 300. We used GRU (Cho et al., 2014) cells for the RNNs. All parameters were initialized from a truncated normal distribution with a standard deviation of 0.1. 6 Results and Discussions In this section, we discuss the results of our experiments as summarized in Tables 1– 5. We use BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) metrics to evaluate the generation quality of responses. We also report the per-response accuracy, which computes the percentage of responses in which the generated response exactly matches the ground truth response. To evaluate the model’s capability of correctly injecting entities in the generated response, we report the entity F1 measure as defined in Eric and Manning (2017). Results on En-DSTC2: We compare our model with the previous works on the English version of modified DSTC2 in Table 1. For most of the retrieval-based models, the BLEU or ROUGE scores are not ava"
Q19-1034,Q17-1008,0,0.0230532,"re has been an active interest in enriching existing encodeattend-decode models (Bahdanau et al., 2015) with structural information for various NLP tasks. Such structure is typically obtained from the constituency and/or dependency parse of sentences. The idea is to treat the output of a parser as a graph and use an appropriate network to capture the interactions between the nodes of this graph. For example, Eriguchi et al. (2016) and Chen et al. (2017) showed that incorporating such syntactical structures as Tree-LSTMs in the encoder can improve the performance of neural machine translation. Peng et al. (2017) use GraphLSTMs to perform cross sentence n-ary relation extraction and show that their formulation is applicable to any graph structure and Tree-LSTMs can be thought of as a special case of it. In parallel, Graph Convolutional Networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kipf and Welling, 2017) and their variants (Li et al., 2016) have emerged as state-of-the-art methods for computing representations of entities in a knowledge graph. They provide a more flexible way of encoding such graph structures by capturing multi-hop relationships between nodes. This has led to their"
Q19-1034,W04-1013,0,0.0293461,"nitial learning rates in the range of 0.0006 to 0.001. For regularization we used an L2 penalty of 0.001 in addition to a dropout (Srivastava et al., 2014) of 0.1. We used randomly initialized word embeddings of size 300. The RNN and GCN hidden dimensions were also chosen to be 300. We used GRU (Cho et al., 2014) cells for the RNNs. All parameters were initialized from a truncated normal distribution with a standard deviation of 0.1. 6 Results and Discussions In this section, we discuss the results of our experiments as summarized in Tables 1– 5. We use BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) metrics to evaluate the generation quality of responses. We also report the per-response accuracy, which computes the percentage of responses in which the generated response exactly matches the ground truth response. To evaluate the model’s capability of correctly injecting entities in the generated response, we report the entity F1 measure as defined in Eric and Manning (2017). Results on En-DSTC2: We compare our model with the previous works on the English version of modified DSTC2 in Table 1. For most of the retrieval-based models, the BLEU or ROUGE scores are not available as they select"
Q19-1034,E17-1042,0,0.0688057,"Missing"
Q19-1034,W13-4065,0,0.0256738,"representations; (iii) We leverage co-occurrence frequencies and PPMI (positive-pointwise mutual information) values to construct contextual graphs for code-mixed utterances; and (iv) We show that the proposed model obtains state-of-the-art results on four different datasets spanning five different languages. 2 Related Work In this section, we review the previous work in goal-oriented dialogue systems and describe the introduction of GCNs in NLP. Goal-Oriented Dialogue Systems: Initial goaloriented dialogue systems (Young, 2000; Williams and Young, 2007) were based on dialogue state tracking (Williams et al., 2013; Henderson et al., 2014a,b) and included pipelined modules for natural language understanding, dialogue state tracking, policy management, and natural language generation. Wen et al. (2017) used neural networks for these intermediate modules but still lacked absolute end-to-end trainability. Such pipelined modules were restricted by the fixed slot-structure assumptions on the dialogue state and required per-module based labeling. To mitigate this problem, Bordes et al. (2017) released a version of goal-oriented dialogue dataset that focuses on the development of end-to-end neural models. Such"
Q19-1034,W17-5505,0,0.0196208,"history and the KB triples in memory. Liu and Perez (2017) further added highway (Srivastava et al., 2015) and residual connections (He et al., 2016) to the memory network in order to regulate the access to the memory blocks. Seo et al. (2017) developed a variant of RNN cell that computes a refined representation of the query over multiple iterations before querying the memory. However, all these approaches retrieve the response from a set of 486 candidate responses and such a candidate set is not easy to obtain for any new domain of interest. To account for this, Eric and Manning (2017) and Zhao et al. (2017) adapted RNN-based encoderdecoder models to generate appropriate responses instead of retrieving them from a candidate set. Eric et al. (2017) introduced a key-value memory network based generative model that integrates the underlying KB with RNN-based encodeattend-decode models. Madotto et al. (2018) used memory networks on top of the RNN decoder to tightly integrate KB entities with the decoder in order to generate more informative responses. However, as opposed to our work, all these works ignore the underlying structure of the entity– entity graph of the KB and the syntactic structure of t"
Q19-1034,P18-1149,0,\N,Missing
S10-1028,J93-2003,0,0.0208032,"Missing"
S10-1028,2005.mtsummit-papers.11,0,0.04577,"Missing"
S10-1028,S10-1003,0,0.35227,"Missing"
S10-1028,H05-1052,0,0.0302836,"d in both the cases the word bus appears in the context. Hence, the surface similarity (i.e., word-overlap count) of S1 and S2 would be higher than that of S1 and S3 and S2 and S3 . This highlights the strength of overlap based approaches – frequently co-occurring words can provide strong clues for identifying similar usage patterns of a word. Next, consider the following two occurrences of the word coach: Knowledge based approaches to WSD such as Lesk’s algorithm (Lesk, 1986), Walker’s algorithm (Walker and Amsler, 1986), Conceptual Density (Agirre and Rigau, 1996) and Random Walk Algorithm (Mihalcea, 2005) are fundamentally overlap based algorithms which suffer from data sparsity. While these approaches do well in cases where there is a surface match (i.e., exact word match) between two occurrences of the target word (say, training and test sentence) they fail in cases where their is a semantic match between two occurrences of the target word even though there is no surface match between them. The main reason for this failure is that these approaches do not take into account semantic generalizations (e.g., train isa vehicle). On the other hand, WSD approaches which use Wordnet based semantic si"
S10-1028,C96-1005,0,\N,Missing
S10-1028,W09-2413,0,\N,Missing
S10-1028,J03-1002,0,\N,Missing
S10-1028,P96-1006,0,\N,Missing
S10-1094,H05-1053,0,0.0463367,"thm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. 1 Introduction Domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. However, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been a matter of concern. Therefore, attempts have been made to develop unsupervised (McCarthy et al., 2007; Koeling et al., 2005) and knowledge based 421 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421–426, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics sentence. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. The remainder of this paper is organized as follows. In section 2 we describe related work on domain-specific WSD. In section 3 we discuss an Iterative Word Sense Disambiguation algorithm which lies at the"
S10-1094,J07-4005,0,0.373877,"e disambiguation algorithm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. 1 Introduction Domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. However, the effort of creating the training corpus - annotated sense marked corpora - for every domain of interest has always been a matter of concern. Therefore, attempts have been made to develop unsupervised (McCarthy et al., 2007; Koeling et al., 2005) and knowledge based 421 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 421–426, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics sentence. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain. The remainder of this paper is organized as follows. In section 2 we describe related work on domain-specific WSD. In section 3 we discuss an Iterative Word Sense Disambiguation algo"
S10-1094,H93-1061,0,0.793627,"ted using an iterative disambiguation process by considering only those candidate synsets which appear in the top-k largest connected components. Our knowledge based approach performed better than current state of the art knowledge based approach (Agirre et al., 2009). Also, the precision was better than the Wordnet first sense baseline even though the F-score was slightly lower than the baseline. The second approach is a weakly supervised approach which uses a few hand labeled examples for the most frequent words in the target domain in addition to the publicly available mixed-domain SemCor (Miller et al., 1993) corpus. The underlying assumption is that words exhibit “One Sense Per Domain” phenomenon and hence even as few as 5 training examples per word would be sufficient to identify the predominant sense of the most frequent words in the target domain. Further, once the most frequent words have been disambiguated using the predominant sense, they can provide strong clues for disambiguating other words in the We describe two approaches for All-words Word Sense Disambiguation on a Specific Domain. The first approach is a knowledge based approach which extracts domain-specific largest connected compon"
S10-1094,P96-1006,0,0.201004,"Missing"
S10-1094,S10-1013,0,\N,Missing
S10-1094,P07-1007,0,\N,Missing
W09-3518,kang-choi-2000-automatic,0,0.0445592,"Missing"
W09-3518,W98-1005,0,0.118541,"Missing"
W09-3518,P00-1056,0,0.0933892,"Missing"
W09-3518,J93-2003,0,0.00885606,"ied 3000 words from the training set into words of Indic origin and Western origin. These words were used as seed input for the bootstrapping algorithm described below: P (Y |X; λ) = PT PK 1 · e t=1 k=1 λk fk (Yt−1 ,Yt ,X,t) Z(X) (1) where, X = source word (English) Y = target word (Hindi, Kannada) T = length of source word (English) K = number of f eatures λk = f eature weight Z(X) = normalization constant CRF++2 which is an open source implementation of CRF was used for training and decoding. GIZA++ (Och and Ney, 2000), which is a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for English-Hindi word pairs in the training data. Under this alignment, each character in the English word is aligned to zero or more characters in the corresponding Hindi word. The following features are then generated using this character-aligned data (here ei and hi are the characters at position i of the source word and target word respectively): 1. Build two n-gram language models: one for the already classified names of Indic origin and another for the names of Western origin. Here, by n-gram we mean n-character obtained by splitting the words"
W09-3518,2003.mtsummit-papers.17,0,0.068071,"Missing"
W09-3518,C00-1056,0,0.0674311,"Missing"
W09-3518,I08-6006,0,0.0296835,"Missing"
W09-3518,C02-1099,0,0.0528584,"Missing"
W09-3518,P02-1051,0,\N,Missing
W09-3518,W09-3502,0,\N,Missing
W09-3518,P97-1017,0,\N,Missing
W10-2403,P07-1083,0,0.0172327,"sed method where a pair of strings is classified as a transliteration pair if the Normalized Edit Distance (NED) between them is above a certain threshold. To calculate the NED, the target language string is first Romanized by replacing each target grapheme by the source grapheme having the highest conditional probability. These conditional probabilities are obtained by aligning the seed set of transliteration pairs using an M2Maligner approach (Jiampojamarn et. al., 2007). The second system uses a SVM based discriminative classifier trained using an improved feature representation (BK 2007) (Bergsma and Kondrak, 2007). These features include all substring pairs up to a maximum length of three as extracted from the aligned word pairs. The transliteration pairs in the seed data provided for the shared task were used as positive examples. The negative examples were obtained by generating all possible source-target pairs in the seed data and taking those pairs which are not transliterations but have a longest common subsequence ratio above a certain threshold. One drawback of this system is that longer substrings cannot be used due to the combinatorial explosion in the number of unique features as the substrin"
W10-2403,W10-2407,0,0.126423,"Missing"
W10-2403,W10-2412,0,0.178331,"Missing"
W10-2403,W10-2408,0,0.0557878,"Missing"
W10-2403,E09-1091,1,0.746389,"Missing"
W10-2403,W10-2405,0,\N,Missing
W10-2403,P06-1103,0,\N,Missing
W10-2403,J98-4003,0,\N,Missing
W10-2403,W10-2404,1,\N,Missing
W10-4011,P10-1137,1,0.853465,"ack documents and (b) Lack of Robustness: due to the inherent assumption in PRF, i.e., relevance of top k documents, performance is sensitive to that of the initial retrieval algorithm and as a result is not robust. Typically, larger coverage ensures higher proportion of relevant documents in the top k retrieval (Hawking et al., 1999). However, some resource-constrained languages do not have adequate information coverage in their own language. For example, languages like Hungarian and Finnish have meager online content in their own languages. Multilingual Pseudo-Relevance Feedback (MultiPRF) (Chinnakotla et al., 2010a) is a novel framework for PRF to overcome the above limitations of PRF. It does so by taking the help of a different language called the assisting language. Thus, the performance of a resource-constrained language could be improved by harnessing the good coverage of another language. MulitiPRF showed significant improvements on standard CLEF collections (Braschler and Peters, 2004) over state-of-art PRF system. On the web, each language has its own exclusive topical coverage besides sharing a large number of common topics with other languages. For example, information about Saudi Arabia gove"
W10-4011,J03-1002,0,0.00249935,"n the languages. In case of French, since some function words like l’, d’ etc., occur as prefixes to a word, we strip them off during indexing and query processing, since it significantly improves the baseline performance. We use standard evaluation measures like MAP, P@5 and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the parameters of MBF, specifically λ and α, and choose the values which give the optimal performance on a given collection. We observe that the optimal parameters γ and β are uniform across collections and vary in the range 0"
W10-4011,2005.mtsummit-papers.11,0,0.00417888,"m off during indexing and query processing, since it significantly improves the baseline performance. We use standard evaluation measures like MAP, P@5 and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the parameters of MBF, specifically λ and α, and choose the values which give the optimal performance on a given collection. We observe that the optimal parameters γ and β are uniform across collections and vary in the range 0.4-0.48. We Source Assist. Langs Langs DE-NL DE-FI NL-ES EN ES-FR ES-FI FR-FI EN-FR NL-DE ES-DE FI FR-DE NL-ES NL-FR"
W12-5901,W10-1749,0,0.0203997,"g, we use the BLEU metric by comparing candidate reorderings with the reference reorderings that we create from the hand-alignments. BLEU is calculated as: N X r 1 log(BLEU ) = min(1 − , 0) + log(pn ) c N n=1 where, N = 4(unigrams, bigrams, trigrams, and 4-grams are matched) r = length of reference reordering c = length of candidate reordering and 4 P pn = P C∈Candidates C∈Candidates P Pn-gram∈C n-gram∈C Countclip (n-gram) Countclip (n-gram) where C runs over the entire set of candidate reorderings, and Countclip returns the number of n-grams that match in the reference reordering. • LRscore (Birch and Osborne, 2010): LRscore was introduced a couple of years ago as a metric to directly measure reordering performance. LRscore uses a distance score in conjunction with BLEU to help evaluate the word order of MT outputs better. Experiments show that this combined metric correlates better with human judgments than BLEU alone (Birch and Osborne, 2010). Since we do not need a lexical metric, we use only the distance metric from LRscore. We will evaluate reordering distance using the following two scores: – Hamming distance: This measures the number of disagreements between two permutations: dH (π, ρ) = 1 − Pn i="
W12-5901,P05-1066,0,0.135256,"Missing"
W12-5901,D11-1018,0,0.0419931,"Missing"
W12-5901,2001.mtsummit-papers.68,0,0.00970914,"ll files should be zipped into a single zip file and mailed to mikhapra@in.ibm.com. The “standard” and “non-standard” runs must be labeled clearly. 4.1 Short Papers on Task Each participating group is required to submit a short paper describing their approach. Participants should follow COLING 2012 paper submission policy including paper format, blind review policy and title and author format convention. The task paper should be a short paper containing 8 A5 sized pages with any number of reference pages. 5 Evaluation Metrics The output reorderings will be evaluated using two metrics: • BLEU (Papineni et al., 2001): In the past decade, BLEU has been the most widely used metric for MT evaluation. BLEU compares N-grams in the output translation and the reference translation(s), and uses a “brevity penalty” to prevent outputs that are accurate in terms of N-gram match, but too short. For reordering, we use the BLEU metric by comparing candidate reorderings with the reference reorderings that we create from the hand-alignments. BLEU is calculated as: N X r 1 log(BLEU ) = min(1 − , 0) + log(pn ) c N n=1 where, N = 4(unigrams, bigrams, trigrams, and 4-grams are matched) r = length of reference reordering c ="
W12-5901,D09-1105,0,0.0502627,"Missing"
W12-5901,C10-1126,1,0.896253,"Missing"
W12-5901,D11-1045,1,0.898816,"Missing"
W12-5901,P02-1040,0,\N,Missing
W12-5902,W10-1749,0,0.0191504,", we use the BLEU metric by comparing candidate reorderings with the reference reorderings that we create from the hand-alignments. BLEU is calculated as: N X r 1 log(BLEU ) = min(1 − , 0) + log(pn ) c N n=1 where, N = 4(unigrams, bigrams, trigrams, and 4-grams are matched) r = length of reference reordering c = length of candidate reordering 11 and P P C∈Candidates Pn-gram∈C pn = P C∈Candidates n-gram∈C Countclip (n-gram) Countclip (n-gram) where C runs over the entire set of candidate reorderings, and Countclip returns the number of n-grams that match in the reference reordering. • LRscore (Birch and Osborne, 2010): LRscore was introduced a couple of years ago as a metric to directly measure reordering performance. LRscore uses a distance score in conjunction with BLEU to help evaluate the word order of MT outputs better. Experiments show that this combined metric correlates better with human judgments than BLEU alone (Birch and Osborne, 2010). Since we do not need a lexical metric, we use only the distance metric from LRscore. We will evaluate reordering distance using the following two scores: – Hamming distance: This measures the number of disagreements between two permutations: dH (π, ρ) = 1 − Pn i="
W12-5902,W12-5904,0,0.0884635,"the task organizers. The third step is modeled as a Traveling Salesperson (TSP) problem. They consider word sequences, instead of words, to be the cities, and define the cost of traveling from one city to another. The costs are assigned so that the total cost will be minimum for the correct reordering of word sequences. The costs are computed as a function of features of the word sequences involved, and a regression based cost model is learned. The use of word sequences makes solving the TSP problem more tractable, and helps define relevant word-sequence level features for modeling the cost. Dlougach and Galinskaya (2012) built a syntax-based reordering system using on opensource SMT toolkit (Moses). Using source side parses and word alignment information they learn reordering rules from the small corpus provided by the task organizers. They then apply these rules to reorder the test sentences. They claim that this approach works especially well when source and target languages have different sentence-level order (like Subject-Verb-Object vs. Subject-Object-Verb). Its also accounts for word-level reordering (when nouns are swapped with their corresponding adjectives). While working on this shared task they hav"
W12-5902,W12-5906,0,0.0753619,"oses/ 12 the target corpus. They experimented with both, a phrase based model and a factor based model. The phrase based model was trained without any preprocessing or reordering of data. The factored based model used ‘surface word form’ and the ‘POS tag’ factors as translation-factors for training. The map value &lt;0-0,1> was provided in the training script which indicated a source side (surface) to a target side (surface, POS) mapping. They experimented with different values of distortion-limit and used default settings for all other parameters (for both translation model and language model). Kunchukuttan and Bhattacharyya (2012) model the problem of reordering source sentences as a problem of reordering word sequences (as opposed to reordering words). They consider source side reordering to be a composition of the following operations on a sentence: (1) Breaking the sentence into word sequences (2) Reversing the words within some word sequences and (3) Reordering the word sequences. They model the first two steps as a sequence labeling problem. The labeling scheme captures word sequence boundaries and reversals, and the training data labels are extracted using the word alignment information provided by the task organ"
W12-5902,P02-1040,0,0.0866325,"info to find the words which were left unaligned. 2.1 Language pairs Table 1 lists the language pairs that were included in the Shared Task and the amount of hand aligned data that was released for each language pair (En: English, Fa: Farsi, Ur: Urdu, It: Italian). This data which was released as a part of the shared task can be obtained by sending a mail to mikhapra@in.ibm.com. Language Pair En-Fa En-Ur En-It Train 5K 5K 4K Dev 500 500 500 Test 500 500 500 Table 1: Language Pairs included in the Shared Task 3 Evaluation Metrics The output reorderings were evaluated using two metrics: • BLEU (Papineni et al., 2002): In the past decade, BLEU has been the most widely used metric for MT evaluation. BLEU compares N-grams in the output translation and the reference translation(s), and uses a “brevity penalty” to prevent outputs that are accurate in terms of N-gram match, but too short. For reordering, we use the BLEU metric by comparing candidate reorderings with the reference reorderings that we create from the hand-alignments. BLEU is calculated as: N X r 1 log(BLEU ) = min(1 − , 0) + log(pn ) c N n=1 where, N = 4(unigrams, bigrams, trigrams, and 4-grams are matched) r = length of reference reordering c ="
W12-5902,D11-1045,1,0.917338,"adjectives). While working on this shared task they have also made changes to the source code of Moses, especially its chart decoder. These changes are available in the public repository of Moses (Dlougach and Galinskaya, 2012). 5 Results As mentioned earlier, the different systems that participated in the Shared Task were evaluated using mBLEU (Table 2), LRHamming (Table 3) and LRKendall (Table 4) . To put the results in perspective we compare these systems with a baseline system (which uses no reordering) and a state of the art system which models reordering as a Traveling Salesman Problem (Visweswariah et al., 2011). Note that Visweswariah et al. (2011) did not participate in the Shared Task. Their results are included only for the sake of comparison. 6 Summary and future possibilities We conducted a Shared Task on Learning Reordering from Word Alignments. The participants were supposed to train reordering models using high quality alignment data as well as pos tagged and parsed source sentences. We provided data for three language pairs (viz., En-Farsi, En-Urdu and En-Italian). A total of seven groups requested for this data but eventually only three groups made a clean submission. These three systems w"
