2008.eamt-1.6,A94-1016,0,0.0304311,"ng such combinations of rule-based and statistical knowledge sources, one of the approaches being an integration of existing rule-based MT systems into a multi-engine architecture. This paper describes several incarnations of such multi-engine architectures within the project. A careful analysis of the results will guide us in the choice of further steps towards the construction of hybrid MT systems for practical applications. 2 2.1 Merging multiple MT results via a SMT decoder Architecture Combinations of MT systems into multi-engine architectures have a long tradition, starting perhaps with [4]. Multi-engine systems can be roughly divided into simple architectures that try to select the best output from a number of systems but leave the individual hypotheses as is on the one hand [5–10], and more sophisticated setups on the other hand that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in [11–16]. Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not"
2008.eamt-1.6,C00-2122,0,0.0602105,"Missing"
2008.eamt-1.6,2001.mtsummit-papers.3,0,0.0464118,"Missing"
2008.eamt-1.6,2001.mtsummit-papers.12,0,0.0319946,"Missing"
2008.eamt-1.6,C02-1076,0,0.0555903,"Missing"
2008.eamt-1.6,P04-1063,0,0.0385466,"Missing"
2008.eamt-1.6,W05-0828,1,0.796901,"Missing"
2008.eamt-1.6,hogan-frederking-1998-evaluation,0,0.0443059,"Missing"
2008.eamt-1.6,P05-3026,0,0.029568,"Missing"
2008.eamt-1.6,E06-1005,0,0.0371106,"Missing"
2008.eamt-1.6,N07-1029,0,0.0640876,"Missing"
2008.eamt-1.6,W08-0328,1,0.816746,"combinations of highly probable partial translations. Instead of implementing a special-purpose search procedure from scratch, we transform the information contained in the MT output into a form that is suitable as input for an existing SMT decoder. This has the additional advantage that it is simple to combine resources used in standard phrase-based SMT with the material extracted from the rule-based MT results; the optimal combination can essentially be reduced to the task of finding good relative weights for the various phrase table entries. This architecture is described in more detail in [17], where also examples of results are given. It should be noted that this is certainly not the only way to combine systems. In particular, as this proposed 28 12th EAMT conference, 22-23 September 2008, Hamburg, Germany setup gives the last word to the SMT decoder, there is the risk that linguistically well-formed constructs from one of the rule-based engines will deteriorate in the final decoding step. Alternative architectures are under exploration and one such approach will be described below. For experiments in the framework of the shared task of the 2008 ACL workshop on SMT [17] we used a"
2008.eamt-1.6,P07-2045,0,0.00439401,"tructs from one of the rule-based engines will deteriorate in the final decoding step. Alternative architectures are under exploration and one such approach will be described below. For experiments in the framework of the shared task of the 2008 ACL workshop on SMT [17] we used a set of six rule-based MT engines that are partly available via web interfaces and partly installed locally. In addition to these engines, we generated phrase tables from the training data following the baseline methodology given in the description of the shared task and using the scripts included in the Moses toolkit [18]. In order to improve alignment quality, the source text and the output text of the MT systems were aligned with the help of a modified version of GIZA++ that it is able to load given models and which is embedded into a client-server setup, as described in [19]. The original Moses phrase table and separate phrase tables for each of the RBMT systems were then combined into a unified phrase table. By combining domain-specific lexical knowledge learned from the training data with more general knowledge contained in the linguistic rules, the hybrid system can both handle a wider range of syntactic"
2008.eamt-1.6,W08-0309,0,0.0966573,"sess about the particular vocabulary of the source text. 2.2 Results We submitted the results of the hybrid system as well as the results from each of the rule-based systems (suitably anonymized) to the shared task of the WMT 2008 workshop. This gives us the opportunity to compare the results with many other systems under fair conditions, both using automatic evaluation metric and comparisons involving human inspection. Detailed results of this evaluation are Fig. 1. Relative performance of system types for in-domain (EuroParl, upper row) and out-of-domain (News, lower row) data documented in [20]. By condensing several of the tables into a joint plot, it 29 12th EAMT conference, 22-23 September 2008, Hamburg, Germany becomes easier to see some of the salient patterns contained in these datasets. Fig. 1 project the results of two different types of human evaluation into twodimensional plots, and it is interesting to study the different behavior of the systems that depend strongly on whether the tests are done on data from the same or from a different domain as the training data. The plot displays the relative performance of the systems for the directions German ↔ English according to s"
2008.eamt-1.6,2001.mtsummit-papers.39,0,0.0119217,"lemma information. The two representations are then combined and filters based on PoS sequences on both sides are used to obtain a set of candidates for the lexicon. A list of acceptable pairs of PoS sequences is generated by inspecting several hundred of the most frequently occurring PoS sequences and excluding those that either do not form a pair of linguistic phrases or where the interpretation on both sides is incompatible. Morphological classification is applied to these lexical entries to augment them with inflection classes, following the open lexicon interchange format (OLIF) standard [21]. Even if statistical alignment and linguistic preprocessing can lead us a long way towards the automatic creation of lexical entries, it is crucial to manually inspect and correct the results because rule-based MT systems have no other mechanism for preventing errors from incorrect lexical entries. In cases of technical terminology, the validation of the terminology requires both linguistic and technical competence and it may be necessary to distribute some steps over different groups of people. In order to facilitate this process, we have built up a web-based front end for lexical database m"
2008.eamt-1.6,W07-0732,0,0.0135969,"ss natural and fluent in comparison with typical SMT results because standard RBMT approaches do not have access to statistical language models which are the main source of fluency (at least on a local, n-gram level) in the typical SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if"
2008.eamt-1.6,W07-0728,0,0.0198984,"ss natural and fluent in comparison with typical SMT results because standard RBMT approaches do not have access to statistical language models which are the main source of fluency (at least on a local, n-gram level) in the typical SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if"
2008.eamt-1.6,vilar-etal-2006-error,0,0.026094,"SMT setup. A fluency model can be integrated into a RBMT-based architecture via post-editing. This allows the replacement of output expressions by alternatives that fit the context better in the target language. A series of papers has explored this approach both within and beyond 32 12th EAMT conference, 22-23 September 2008, Hamburg, Germany the EuroMatrix project [23, 24], and results of such systems have been submitted to the shared task of the WMT08 workshop [20]. [19] investigates the effect of post-editing on the frequency of typical error types along an error classification inspired by [25] and compares BLEU scores with the results of the architecture proposed in Section 2. Similar types of evaluations are currently going on for more language pairs. Automatic post-editing of MT results can be applied to both architectures presented above and could be used to reduce the impact of disfluencies of the raw MT results. However, it should be clear that even if one or both of these approaches can be made to deliver significant improvements under fairly general conditions, the improvements will essentially only alleviate the problem of lexical coverage but will not touch some other well"
2011.eamt-1.31,2003.mtsummit-systems.1,0,0.763092,"istical techniques. Detailed evaluation reveals that the translation quality can be improved substantially in this way. 1 Introduction A promising integration point for statistical techniques into Rule-Based Machine Translation (RBMT) systems is the transfer phase. A key problem here is to deal with non-deterministic rules and preferences in order to disambiguate and select the most natural expressions in the target language (cf. (Eisele et al., 2008b; Thurmair, 2009)). We performed studies on a phrase-table-driven transfer approach based on an RBMT system (for details on the RBMT system see (Alonso and Thurmair, 2003)): A prototype was built accessing statistically generated bilingual phrase tables at runtime in addition to system lexicons and grammars. This hybrid prototype showed indeed better lexical selection, improved coverage and even sometimes enhanced syntax, but well-known issues in morpho-syntax and a very low hit rate of the phrase table module remained as limiting factors. Thus although the data which were accessed and preferred are obviously the desired ones, the first challenge was the lack of deeper linguistic knowledge within the data while the second challenge was the small F-measure of th"
2011.eamt-1.31,E06-1032,0,0.14749,"Missing"
2011.eamt-1.31,W09-0405,1,0.909237,"Missing"
2011.eamt-1.31,W09-0419,0,0.102734,"istic lexical coverage and choice (Eisele et al., 2008b), this study concentrates on automatic enlargements of RBMT lexicons and enhanced transfer-generation operations, while taking into account the peculiarities of a specific RBMT system and statistical techniques. Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 225232 Leuven, Belgium, May 2011 The considerable potential of statistical term extraction combined with RBMT has been evaluated by other researchers, such as (Thurmair, 2003; Dugast et al., 2009). Here we go one step further by already integrating some RBMT techniques into the very early stages of the statistical term extraction process. This helps to avoid the wellknown problems found in term guessing and identification (Heid, 1999). Additional linguistic layers assure that the extracted terms and phrases are tailored and augmented for the RBMT system in question. 3 Intelligent Terminology Extraction The underlying term extraction tool extracts term pairs by means of statistical algorithms from existing translation memories or bilingual corpora (Eisele et al., 2008a). As a bilingual"
2011.eamt-1.31,2008.tc-1.2,0,0.215749,"for translation equivalent selection within a transfer-based machine translation system using an intertwined net of traditional linguistic methods together with statistical techniques. Detailed evaluation reveals that the translation quality can be improved substantially in this way. 1 Introduction A promising integration point for statistical techniques into Rule-Based Machine Translation (RBMT) systems is the transfer phase. A key problem here is to deal with non-deterministic rules and preferences in order to disambiguate and select the most natural expressions in the target language (cf. (Eisele et al., 2008b; Thurmair, 2009)). We performed studies on a phrase-table-driven transfer approach based on an RBMT system (for details on the RBMT system see (Alonso and Thurmair, 2003)): A prototype was built accessing statistically generated bilingual phrase tables at runtime in addition to system lexicons and grammars. This hybrid prototype showed indeed better lexical selection, improved coverage and even sometimes enhanced syntax, but well-known issues in morpho-syntax and a very low hit rate of the phrase table module remained as limiting factors. Thus although the data which were accessed and prefer"
2011.eamt-1.31,2008.eamt-1.6,1,0.870036,"for translation equivalent selection within a transfer-based machine translation system using an intertwined net of traditional linguistic methods together with statistical techniques. Detailed evaluation reveals that the translation quality can be improved substantially in this way. 1 Introduction A promising integration point for statistical techniques into Rule-Based Machine Translation (RBMT) systems is the transfer phase. A key problem here is to deal with non-deterministic rules and preferences in order to disambiguate and select the most natural expressions in the target language (cf. (Eisele et al., 2008b; Thurmair, 2009)). We performed studies on a phrase-table-driven transfer approach based on an RBMT system (for details on the RBMT system see (Alonso and Thurmair, 2003)): A prototype was built accessing statistically generated bilingual phrase tables at runtime in addition to system lexicons and grammars. This hybrid prototype showed indeed better lexical selection, improved coverage and even sometimes enhanced syntax, but well-known issues in morpho-syntax and a very low hit rate of the phrase table module remained as limiting factors. Thus although the data which were accessed and prefer"
2011.eamt-1.31,federmann-2010-appraise,1,0.850798,"Missing"
2011.eamt-1.31,2005.mtsummit-papers.11,0,0.123412,"by already integrating some RBMT techniques into the very early stages of the statistical term extraction process. This helps to avoid the wellknown problems found in term guessing and identification (Heid, 1999). Additional linguistic layers assure that the extracted terms and phrases are tailored and augmented for the RBMT system in question. 3 Intelligent Terminology Extraction The underlying term extraction tool extracts term pairs by means of statistical algorithms from existing translation memories or bilingual corpora (Eisele et al., 2008a). As a bilingual corpus, the Europarl corpus (Koehn, 2005) was used for development. As a test set, we choose the ACL WMT 2008 test set which contains the Q4/2000 portion of the EuroParl data (2000-10 to 2000-12). The initial design study on peculiarities of a specific statistically based term extraction that satisfies the requirements of an RBMT system dealt with issues like term definition in a strict linguistic and system-related sense, term identification as single words vs. multiword expressions, base form reduction and application of linguistic category patterns for identification or elimination of noise at the end of the first multi-layered ex"
2011.eamt-1.31,2003.eamt-1.20,0,0.722282,"hniques. Detailed evaluation reveals that the translation quality can be improved substantially in this way. 1 Introduction A promising integration point for statistical techniques into Rule-Based Machine Translation (RBMT) systems is the transfer phase. A key problem here is to deal with non-deterministic rules and preferences in order to disambiguate and select the most natural expressions in the target language (cf. (Eisele et al., 2008b; Thurmair, 2009)). We performed studies on a phrase-table-driven transfer approach based on an RBMT system (for details on the RBMT system see (Alonso and Thurmair, 2003)): A prototype was built accessing statistically generated bilingual phrase tables at runtime in addition to system lexicons and grammars. This hybrid prototype showed indeed better lexical selection, improved coverage and even sometimes enhanced syntax, but well-known issues in morpho-syntax and a very low hit rate of the phrase table module remained as limiting factors. Thus although the data which were accessed and preferred are obviously the desired ones, the first challenge was the lack of deeper linguistic knowledge within the data while the second challenge was the small F-measure of th"
2011.eamt-1.31,2009.mtsummit-posters.21,0,0.236459,"lent selection within a transfer-based machine translation system using an intertwined net of traditional linguistic methods together with statistical techniques. Detailed evaluation reveals that the translation quality can be improved substantially in this way. 1 Introduction A promising integration point for statistical techniques into Rule-Based Machine Translation (RBMT) systems is the transfer phase. A key problem here is to deal with non-deterministic rules and preferences in order to disambiguate and select the most natural expressions in the target language (cf. (Eisele et al., 2008b; Thurmair, 2009)). We performed studies on a phrase-table-driven transfer approach based on an RBMT system (for details on the RBMT system see (Alonso and Thurmair, 2003)): A prototype was built accessing statistically generated bilingual phrase tables at runtime in addition to system lexicons and grammars. This hybrid prototype showed indeed better lexical selection, improved coverage and even sometimes enhanced syntax, but well-known issues in morpho-syntax and a very low hit rate of the phrase table module remained as limiting factors. Thus although the data which were accessed and preferred are obviously"
2012.amta-papers.23,W11-2103,0,0.0243141,"al decision on the best translation for the current sentence. We investigate the performance of our methodology in the following section. 4 4.1 Experiments Oracle Experiments Before working on the fine-tuning of the Machine Learning training method, we want to investigate more closely what improvements can be achieved using our method. Hence, we perform a series of oracle experiments in which we simulate a perfect classifier resulting in an optimal selection of hybrid sentences and, thus, an optimal hybrid translation. 4.2 Experimental Setup We make use of the official results from the WMT11 (Callison-Burch et al., 2011), including translation output, automatic metrics’ scores, and also results from human judgement. We focus on 4.3 4.4 Prototypical Experiments Training Data As training data, we receive a development set with references as well as a test set without reference. Applying our total order on the given translations, we determine a system ranking on the sentence level. We compute pairwise system comparisons for usage as SVM class labels and annotate each individual translation with parser output and language model scores. We use the Stanford Parser (Green and Manning, 2010; Klein and Manning, 2003;"
2012.amta-papers.23,W07-0726,1,0.842355,"n 5. We conclude by giving a summary of our findings and an outlook to future research questions in Section 6. 2 Related Work Hybrid translation methods and system combination approaches have received a lot of research attention over the last decades. There is general consensus in the literature that it is possible to generate hybrid translations from different systems resulting in an improvement over the individual baseline systems. See seminal work from Frederking and Nirenburg (1994), or, e.g., Macherey and Och (2007) and Rosti et al. (2007). Confusion Networks can be used for combination (Chen et al., 2007; Eisele et al., 2008; Matusov et al., 2006). One of the MT systems is selected as the backbone or skeleton of the hybrid translation, while other translations are connected via word alignment techniques such as GIZA++ (Och and Ney, 2003). Together, the systems then form a connected graph in which different paths through the network model different translations. An open-source toolkit for system combination implementing this technique is described in Barrault (2010). As the combination of translation output using phrase-based methods may not preserve the underlying syntactic structure of the t"
2012.amta-papers.23,W11-2107,0,0.0504112,"method: - We must define a sufficiently good order; and - We need to convert this order, using Machine Learning tools, into an equivalent classification model. We address both issues in the following sections. 3.2 An Extensible, Total Order on Translations In order to rank the given source translations, we first need to define an ordering relation over the set of translation outputs. For this, we apply three renowned MT evaluation metrics which are the defacto standards for automated assessment of MT quality. We consider: 1. The Meteor score, both on the sentence and on the corpus level, see Denkowski and Lavie (2011); 2. The NIST n-gram co-occurence score on the corpus level, see Doddington (2002); and 3. The BLEU score which is the most widely used evaluation metric, see Papineni et al. (2002). While both BLEU and NIST scores are designed to have a high correlation with results from manual evaluation on the corpus level (denoted by suffix C ), the Meteor metric can also be used to meaningfully compare translations down to the level of individual sentences (denoted by suffix S ). We make use of this property when defining our order ord(A, B) on translations, as shown in equations 3–7 on Page 9. Note that"
2012.amta-papers.23,W08-0328,1,0.821062,"giving a summary of our findings and an outlook to future research questions in Section 6. 2 Related Work Hybrid translation methods and system combination approaches have received a lot of research attention over the last decades. There is general consensus in the literature that it is possible to generate hybrid translations from different systems resulting in an improvement over the individual baseline systems. See seminal work from Frederking and Nirenburg (1994), or, e.g., Macherey and Och (2007) and Rosti et al. (2007). Confusion Networks can be used for combination (Chen et al., 2007; Eisele et al., 2008; Matusov et al., 2006). One of the MT systems is selected as the backbone or skeleton of the hybrid translation, while other translations are connected via word alignment techniques such as GIZA++ (Och and Ney, 2003). Together, the systems then form a connected graph in which different paths through the network model different translations. An open-source toolkit for system combination implementing this technique is described in Barrault (2010). As the combination of translation output using phrase-based methods may not preserve the underlying syntactic structure of the translation backbone,"
2012.amta-papers.23,A94-1016,0,0.617381,"r ML is introduced in Section 3.3. In Section 4 we present the suite of experiments we have conducted; their results are discussed in Section 5. We conclude by giving a summary of our findings and an outlook to future research questions in Section 6. 2 Related Work Hybrid translation methods and system combination approaches have received a lot of research attention over the last decades. There is general consensus in the literature that it is possible to generate hybrid translations from different systems resulting in an improvement over the individual baseline systems. See seminal work from Frederking and Nirenburg (1994), or, e.g., Macherey and Och (2007) and Rosti et al. (2007). Confusion Networks can be used for combination (Chen et al., 2007; Eisele et al., 2008; Matusov et al., 2006). One of the MT systems is selected as the backbone or skeleton of the hybrid translation, while other translations are connected via word alignment techniques such as GIZA++ (Och and Ney, 2003). Together, the systems then form a connected graph in which different paths through the network model different translations. An open-source toolkit for system combination implementing this technique is described in Barrault (2010). As"
2012.amta-papers.23,2005.eamt-1.15,0,0.650301,"ifferent paths through the network model different translations. An open-source toolkit for system combination implementing this technique is described in Barrault (2010). As the combination of translation output using phrase-based methods may not preserve the underlying syntactic structure of the translation backbone, there also are methods which perform Sentencebased Combination, trying to select the best option out of a set of several black-box translations for a given source text. The concept is similar to Reranking Approaches in statistical MT. See research described in Avramidis (2011), Gamon et al. (2005), or Rosti et al. (2007). Finally, there are methods for Machine-Learningbased Combination. These usually train classifiers using, e.g., Support Vector Machines (Vapnik, 1995) to determine if a given translation is good or bad. Recent work such as He et al. (2010a) and He et al. (2010b) applies Machine Learning tools to estimate individual translation quality and re-rank a given set of candidate translations on the sentence level. Of course, there also exist various combinations of the aforementioned methods, e.g., Avramidis (2011) or Okita and van Genabith (2011). 3 Methodology 3.1 Classifica"
2012.amta-papers.23,C10-1045,0,0.0123922,"sults from the WMT11 (Callison-Burch et al., 2011), including translation output, automatic metrics’ scores, and also results from human judgement. We focus on 4.3 4.4 Prototypical Experiments Training Data As training data, we receive a development set with references as well as a test set without reference. Applying our total order on the given translations, we determine a system ranking on the sentence level. We compute pairwise system comparisons for usage as SVM class labels and annotate each individual translation with parser output and language model scores. We use the Stanford Parser (Green and Manning, 2010; Klein and Manning, 2003; Levy and Manning, 2003) to process the source text and the corresponding translations. For language model scoring, we use the SRILM toolkit (Stolcke, 2002) training a 5-gram language model for English. In this work, we do not consider any source language language models. 4.5 Classifier Training Figure 3 shows the optimisation grids we obtained during SVM tuning. They show which settings for C and γ result in the best prediction rate. Note how the graphs are similar regarding the optimal area. We train our final SVM classifiers with parameters from this area, giving p"
2012.amta-papers.23,P10-1064,0,0.122195,"Missing"
2012.amta-papers.23,C10-2043,0,0.0610972,"Missing"
2012.amta-papers.23,P03-1054,0,0.00534805,"lison-Burch et al., 2011), including translation output, automatic metrics’ scores, and also results from human judgement. We focus on 4.3 4.4 Prototypical Experiments Training Data As training data, we receive a development set with references as well as a test set without reference. Applying our total order on the given translations, we determine a system ranking on the sentence level. We compute pairwise system comparisons for usage as SVM class labels and annotate each individual translation with parser output and language model scores. We use the Stanford Parser (Green and Manning, 2010; Klein and Manning, 2003; Levy and Manning, 2003) to process the source text and the corresponding translations. For language model scoring, we use the SRILM toolkit (Stolcke, 2002) training a 5-gram language model for English. In this work, we do not consider any source language language models. 4.5 Classifier Training Figure 3 shows the optimisation grids we obtained during SVM tuning. They show which settings for C and γ result in the best prediction rate. Note how the graphs are similar regarding the optimal area. We train our final SVM classifiers with parameters from this area, giving preference to smaller valu"
2012.amta-papers.23,P03-1056,0,0.0128471,", including translation output, automatic metrics’ scores, and also results from human judgement. We focus on 4.3 4.4 Prototypical Experiments Training Data As training data, we receive a development set with references as well as a test set without reference. Applying our total order on the given translations, we determine a system ranking on the sentence level. We compute pairwise system comparisons for usage as SVM class labels and annotate each individual translation with parser output and language model scores. We use the Stanford Parser (Green and Manning, 2010; Klein and Manning, 2003; Levy and Manning, 2003) to process the source text and the corresponding translations. For language model scoring, we use the SRILM toolkit (Stolcke, 2002) training a 5-gram language model for English. In this work, we do not consider any source language language models. 4.5 Classifier Training Figure 3 shows the optimisation grids we obtained during SVM tuning. They show which settings for C and γ result in the best prediction rate. Note how the graphs are similar regarding the optimal area. We train our final SVM classifiers with parameters from this area, giving preference to smaller values for both C and γ to re"
2012.amta-papers.23,D07-1105,0,0.0165004,"on 4 we present the suite of experiments we have conducted; their results are discussed in Section 5. We conclude by giving a summary of our findings and an outlook to future research questions in Section 6. 2 Related Work Hybrid translation methods and system combination approaches have received a lot of research attention over the last decades. There is general consensus in the literature that it is possible to generate hybrid translations from different systems resulting in an improvement over the individual baseline systems. See seminal work from Frederking and Nirenburg (1994), or, e.g., Macherey and Och (2007) and Rosti et al. (2007). Confusion Networks can be used for combination (Chen et al., 2007; Eisele et al., 2008; Matusov et al., 2006). One of the MT systems is selected as the backbone or skeleton of the hybrid translation, while other translations are connected via word alignment techniques such as GIZA++ (Och and Ney, 2003). Together, the systems then form a connected graph in which different paths through the network model different translations. An open-source toolkit for system combination implementing this technique is described in Barrault (2010). As the combination of translation out"
2012.amta-papers.23,E06-1005,0,0.023857,"our findings and an outlook to future research questions in Section 6. 2 Related Work Hybrid translation methods and system combination approaches have received a lot of research attention over the last decades. There is general consensus in the literature that it is possible to generate hybrid translations from different systems resulting in an improvement over the individual baseline systems. See seminal work from Frederking and Nirenburg (1994), or, e.g., Macherey and Och (2007) and Rosti et al. (2007). Confusion Networks can be used for combination (Chen et al., 2007; Eisele et al., 2008; Matusov et al., 2006). One of the MT systems is selected as the backbone or skeleton of the hybrid translation, while other translations are connected via word alignment techniques such as GIZA++ (Och and Ney, 2003). Together, the systems then form a connected graph in which different paths through the network model different translations. An open-source toolkit for system combination implementing this technique is described in Barrault (2010). As the combination of translation output using phrase-based methods may not preserve the underlying syntactic structure of the translation backbone, there also are methods"
2012.amta-papers.23,J03-1002,0,0.00297693,"last decades. There is general consensus in the literature that it is possible to generate hybrid translations from different systems resulting in an improvement over the individual baseline systems. See seminal work from Frederking and Nirenburg (1994), or, e.g., Macherey and Och (2007) and Rosti et al. (2007). Confusion Networks can be used for combination (Chen et al., 2007; Eisele et al., 2008; Matusov et al., 2006). One of the MT systems is selected as the backbone or skeleton of the hybrid translation, while other translations are connected via word alignment techniques such as GIZA++ (Och and Ney, 2003). Together, the systems then form a connected graph in which different paths through the network model different translations. An open-source toolkit for system combination implementing this technique is described in Barrault (2010). As the combination of translation output using phrase-based methods may not preserve the underlying syntactic structure of the translation backbone, there also are methods which perform Sentencebased Combination, trying to select the best option out of a set of several black-box translations for a given source text. The concept is similar to Reranking Approaches i"
2012.amta-papers.23,P02-1040,0,0.0889629,"n the following sections. 3.2 An Extensible, Total Order on Translations In order to rank the given source translations, we first need to define an ordering relation over the set of translation outputs. For this, we apply three renowned MT evaluation metrics which are the defacto standards for automated assessment of MT quality. We consider: 1. The Meteor score, both on the sentence and on the corpus level, see Denkowski and Lavie (2011); 2. The NIST n-gram co-occurence score on the corpus level, see Doddington (2002); and 3. The BLEU score which is the most widely used evaluation metric, see Papineni et al. (2002). While both BLEU and NIST scores are designed to have a high correlation with results from manual evaluation on the corpus level (denoted by suffix C ), the Meteor metric can also be used to meaningfully compare translations down to the level of individual sentences (denoted by suffix S ). We make use of this property when defining our order ord(A, B) on translations, as shown in equations 3–7 on Page 9. Note that the our order ord(A, B) is an extensible, total order. It can easily be extended to include, e.g., results from manual evaluation of translation output. In fact, this would be a hel"
2012.amta-papers.23,N07-1029,0,0.138523,"experiments we have conducted; their results are discussed in Section 5. We conclude by giving a summary of our findings and an outlook to future research questions in Section 6. 2 Related Work Hybrid translation methods and system combination approaches have received a lot of research attention over the last decades. There is general consensus in the literature that it is possible to generate hybrid translations from different systems resulting in an improvement over the individual baseline systems. See seminal work from Frederking and Nirenburg (1994), or, e.g., Macherey and Och (2007) and Rosti et al. (2007). Confusion Networks can be used for combination (Chen et al., 2007; Eisele et al., 2008; Matusov et al., 2006). One of the MT systems is selected as the backbone or skeleton of the hybrid translation, while other translations are connected via word alignment techniques such as GIZA++ (Och and Ney, 2003). Together, the systems then form a connected graph in which different paths through the network model different translations. An open-source toolkit for system combination implementing this technique is described in Barrault (2010). As the combination of translation output using phrase-based m"
2020.findings-emnlp.375,N15-1124,1,0.853486,"verage ratings attributed to translations sampled from large test sets, and although such methodology does allow application of statistical significance testing to identify potentially meaningful differences in system performance, they do not provide any insight into the reasons behind a significantly higher score or the degree to which systems perform better when translating individual segments. Furthermore, DA score distributions produced in the human evaluation of the news task are based on individual DA scores that alone cannot be relied upon to reflect the quality of individual segments (Graham et al., 2015). Past work, has however provided a means of running a DA human evaluation in such a way that DA scores accurately reflect the performance of a system on a given individual segment (Graham 1 DA is also used in other task evaluations such as Video Captioning and Multilingual Surface Realisation (Awad et al., 2019; Graham et al., 2018; Mille et al., 2019). 4199 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4199–4207 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2015). This method comes with the trade-off of requiring substantially"
2020.findings-emnlp.375,W19-5301,1,0.889077,"Missing"
2020.findings-emnlp.375,D18-1512,0,0.0479597,"Missing"
2020.findings-emnlp.375,W19-5302,1,0.82035,"Missing"
2020.findings-emnlp.375,D19-6301,1,0.787469,"etter when translating individual segments. Furthermore, DA score distributions produced in the human evaluation of the news task are based on individual DA scores that alone cannot be relied upon to reflect the quality of individual segments (Graham et al., 2015). Past work, has however provided a means of running a DA human evaluation in such a way that DA scores accurately reflect the performance of a system on a given individual segment (Graham 1 DA is also used in other task evaluations such as Video Captioning and Multilingual Surface Realisation (Awad et al., 2019; Graham et al., 2018; Mille et al., 2019). 4199 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4199–4207 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2015). This method comes with the trade-off of requiring substantially more repeat assessments per segment than the test set level evaluation generally run, for example, to evaluate all primary submissions in the WMT news task. In this work we demonstrate how this method has the potential to be employed as a secondary method of evaluation in WMT tasks for a smaller subset of systems to provide segment-level insight into w"
2020.findings-emnlp.375,W19-5333,0,0.0132682,"++ + ++ + + ++ + + ++ + + + + ++ + + ++ + + + 0.000 0 25 50 adequacy 75 100 20 40 60 80 100 Facebook-FAIR (src) Figure 3: Density plot of sample of 540 accurate segment-level DA scores for German to English translation new translation for the top-performing system, FACEBOOK -FAIR, in WMT-19 versus the human translator where in the official results the system beat human performance; Human denotes evaluation of segments translated by the creator of the standard WMT reference translations Figures 1, 2 and 3 include density plots for human translation and the top-performing FACEBOOK -FAIR system (Ng et al., 2019) for the same 540 translated segments from WMT-19 for the three language pairs we investigate. For German to English and English to German translation in Figures 2 and 3 a similar pattern emerges in terms of comparison of human and machine-translated segments, as for both a slightly larger proportion of FACEBOOK -FAIR translations are scored high compared to the human translator – as can be seen from the higher red peak close to the extreme right of both plots indicating that the machine produces a marginally higher number of translations with higher levels of adequacy. For English to Russian"
2020.findings-emnlp.375,W18-6312,0,0.016085,"ments. 2 Related Work Over the past number of years, machine translation has been biting at the heels of human translation for a small number of language pairs. Beginning with the first claims that machines have surpassed human quality of translation for Chinese to English news text, conclusions received with some skepticism and even controversy (Hassan et al., 2018), as claims of human performance resulted in re-evaluations that scrutinized the methodology applied, highlighting the influence of reverse-created test data and lack of wider document context in evaluations (L¨aubli et al., 2018; Toral et al., 2018). Despite re-evaluations taking somewhat more care to eliminate such sources of inaccuracies, they additionally included some potential issues of their own, such as employing somewhat outdated human evaluation methodologies, non-standard methods of statistical significance testing and lack of planning evaluations in terms of statistical power. Graham et al. (2019, 2020), on the other hand re-run the evaluation, identify and fix remaining causes of error, and subsequently confirm that, on the overall level of the test set, with increased scrutiny on evaluation procedures, conclusions of human p"
2020.findings-emnlp.375,2020.emnlp-main.6,1,0.816952,"Missing"
2020.iwslt-1.1,P18-4020,0,0.0279555,"Missing"
2020.iwslt-1.1,2005.iwslt-1.19,0,0.174539,"Missing"
2020.iwslt-1.1,2020.iwslt-1.11,0,0.0306756,"Missing"
2020.iwslt-1.1,W18-6319,0,0.0215087,"Missing"
2020.iwslt-1.1,2013.iwslt-papers.14,0,0.069836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.9,0,0.053316,"Missing"
2020.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0549836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.22,0,0.0613503,"Missing"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2021.humeval-1.11,W19-5301,1,0.887319,"Missing"
2021.humeval-1.11,2020.wmt-1.137,0,0.128088,". Castilho et al. (2020) have examined the necessary context span needed for evaluation across different domains, and for relatively short documents like news articles, the authors recommend presenting the whole document during the assessment of individual segments. Using document context has also been recommended by Toral (2020) who reported that this information was needed for evaluators to rank systems in a contrastive evaluation setting. Having the text available during the assessment of fluency or adequacy might be essential for some evaluators who spend more time reading than assessing (Castilho, 2020). Although the literature is consistent about the need of document context in human evaluation of MT, little research has been done on the impact of experimental design and user interfaces on annotator productivity and the reliability of assessments in this context. The existing research on experimental designs for machine translation evaluation focuses on contrasting direct assessments with pairwise rankings (Novikova et al., 2018; Sakaguchi and Van Durme, 2018) and not on the optimal presentation of the document-level information. However, Document context in human evaluation of MT outputs R"
2021.humeval-1.11,2020.lrec-1.461,0,0.0578943,"Missing"
2021.humeval-1.11,2004.iwslt-evaluation.1,0,0.137608,"Missing"
2021.humeval-1.11,C18-2019,1,0.838243,"translation more favourably if they cannot identify errors related to 98 even the simple UI design decision of aligning document translations on the sentence level impacts efficiency of some evaluators (Popovi´c, 2020). With this work, we want to promote that direction of research. 3 Statistic All L4 Document-level human evaluation campaigns at WMT During the WMT evaluation campaigns of 2019 and 2020, segment and document-level assessments of document translations were collected, but using different methods and thus user interfaces. Both were implemented in the Appraise evaluation framework (Federmann, 2018) as a source-based direct assessment task (Graham et al., 2013; Cettolo et al., 2017), i.e. all segments and entire documents were judged on a continuous scale between 0 and 100 by bilingual annotators. 3.1 WMT20 Annotators Seg. judgements Doc. judgements cs, de, fi, gu kk, lt, ru, zh 1,271 207,916 12,907 cs, de, iu, jp pl, ru, ta, zh 1,213 186,813 13,790 Languages Annotators Seg. judgements Doc. judgements cs, de, ru, zh 779 127,178 7,894 cs, de, ru, zh 746 115,571 10,019 Languages Table 1: Statistics of data from the WMT19 and WMT20 campaigns, including languages, the total number of annotat"
2021.humeval-1.11,W13-2305,0,0.0295728,"related to 98 even the simple UI design decision of aligning document translations on the sentence level impacts efficiency of some evaluators (Popovi´c, 2020). With this work, we want to promote that direction of research. 3 Statistic All L4 Document-level human evaluation campaigns at WMT During the WMT evaluation campaigns of 2019 and 2020, segment and document-level assessments of document translations were collected, but using different methods and thus user interfaces. Both were implemented in the Appraise evaluation framework (Federmann, 2018) as a source-based direct assessment task (Graham et al., 2013; Cettolo et al., 2017), i.e. all segments and entire documents were judged on a continuous scale between 0 and 100 by bilingual annotators. 3.1 WMT20 Annotators Seg. judgements Doc. judgements cs, de, fi, gu kk, lt, ru, zh 1,271 207,916 12,907 cs, de, iu, jp pl, ru, ta, zh 1,213 186,813 13,790 Languages Annotators Seg. judgements Doc. judgements cs, de, ru, zh 779 127,178 7,894 cs, de, ru, zh 746 115,571 10,019 Languages Table 1: Statistics of data from the WMT19 and WMT20 campaigns, including languages, the total number of annotators and collected segment-level and document-level scores, aft"
2021.humeval-1.11,2020.emnlp-main.6,0,0.125284,"ity of assessments in this context. The existing research on experimental designs for machine translation evaluation focuses on contrasting direct assessments with pairwise rankings (Novikova et al., 2018; Sakaguchi and Van Durme, 2018) and not on the optimal presentation of the document-level information. However, Document context in human evaluation of MT outputs Recent research emphasized the importance of document context in human evaluation of machine translation, especially in terms of accessing potential human parity or super-human performance (L¨aubli et al., 2018; Toral et al., 2018; Graham et al., 2020; Toral, 2020). Several works have compiled sets of recommendations for document-level evaluation. For example, Laubli et al. (2020) recommend evaluation of documents instead of independent sentences as translators tend to judge machine translation more favourably if they cannot identify errors related to 98 even the simple UI design decision of aligning document translations on the sentence level impacts efficiency of some evaluators (Popovi´c, 2020). With this work, we want to promote that direction of research. 3 Statistic All L4 Document-level human evaluation campaigns at WMT During the W"
2021.humeval-1.11,D18-1512,0,0.0376965,"Missing"
2021.humeval-1.11,N18-2012,0,0.0299627,"Missing"
2021.humeval-1.11,2020.coling-main.444,0,0.0372345,"Missing"
2021.humeval-1.11,P18-1020,0,0.0249078,"Missing"
2021.humeval-1.11,2020.eamt-1.20,0,0.189489,"ingle document translated by the same MT system were provided sequentially to evaluators in the order as they appear in the document, only one segment shown at a time (Fig. 1a), followed by the entire document comprised of already scored segments (Fig. 1b). WMT 2020 (Barrault et al., 2020) implemented a more document-centric approach, displaying the full translated document on a single screen (Fig. 1c) for most of the out-of-English language pairs. While the change was primarily about the user interface (UI), we believe it can impact the quality of document-level evaluation to a large extent. Toral (2020) has noticed potential issues arising from the limited inter-sentential context in the WMT19 method, in which the evaluator does not have continuous access to all segments from the document. Unable to revisit previous sentences and never seeing subsequent sentences, the evaluator might forget or lack access to important details necessary to rate the current segment. On the other hand, displaying a long document on a screen can notably increase cognitive load, potentially lowering reliability of assessments over time (Gonzalez et al., 2011), and increase annotation time and costs, especially at"
avramidis-etal-2012-involving,eisele-chen-2010-multiun,0,\N,Missing
avramidis-etal-2012-involving,federmann-2010-appraise,1,\N,Missing
avramidis-etal-2012-involving,J11-4002,1,\N,Missing
avramidis-etal-2012-involving,P02-1040,0,\N,Missing
avramidis-etal-2012-involving,W10-1738,1,\N,Missing
avramidis-etal-2012-involving,P07-2045,0,\N,Missing
avramidis-etal-2012-involving,W11-2104,1,\N,Missing
avramidis-etal-2012-involving,W10-1703,0,\N,Missing
avramidis-etal-2012-involving,vilar-etal-2006-error,1,\N,Missing
avramidis-etal-2012-involving,W11-2100,0,\N,Missing
avramidis-etal-2012-involving,2011.eamt-1.36,1,\N,Missing
avramidis-etal-2012-involving,2010.amta-papers.27,0,\N,Missing
avramidis-etal-2012-richly,steinberger-etal-2006-jrc,0,\N,Missing
avramidis-etal-2012-richly,vandeghinste-etal-2008-evaluation,1,\N,Missing
avramidis-etal-2012-richly,W09-0424,0,\N,Missing
avramidis-etal-2012-richly,P07-2045,0,\N,Missing
avramidis-etal-2012-richly,C04-1072,0,\N,Missing
avramidis-etal-2012-richly,W08-0309,0,\N,Missing
avramidis-etal-2012-richly,2005.mtsummit-papers.11,0,\N,Missing
avramidis-etal-2012-richly,W10-1720,1,\N,Missing
avramidis-etal-2012-richly,P03-1021,0,\N,Missing
C12-1005,W11-2107,0,0.0157244,"ted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators"
C12-1005,W11-1204,0,0.0405268,"Missing"
C12-1005,P07-2045,0,0.0139662,"translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators (Section 4.5). 4.1 Translation System: Moses Toolkit For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language 12 Meteor configuration: exact, stem, paraphrase 73 model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). 4.2 Translating aligned UK – German GAAP labels The UK GAAP is a monolingual ontology which holds 142 financial labels. With the help of the German equivalent, i.e. German GAAP, we aligned 16 German labels with the English o"
C12-1005,J03-1002,0,0.00499596,"and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cross-lingual evaluation with six annotators (Section 4.5). 4.1 Translation System: Moses Toolkit For generating the translations from English into German, we used the statistical translation toolkit Moses (Koehn et al., 2007). Furthermore, we aimed to improve the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with the GIZA++ toolkit (Och and Ney, 2003), where the 5-gram language 12 Meteor configuration: exact, stem, paraphrase 73 model was built by SRILM with Kneser-Ney smoothing (Stolcke, 2002). 4.2 Translating aligned UK – German GAAP labels The UK GAAP is a monolingual ontology which holds 142 financial labels. With the help of the German equivalent, i.e. German GAAP, we aligned 16 German labels with the English ones, stored in the UK GAAP. This allowed us to do a small automatic evaluation, regardless of the low number of labels to be translated. Scoring Metric Source # correct BLEU-2 BLEU-4 NIST TER Meteor JRC-Acquis ECB Linguee+Wikipe"
C12-1005,P02-1040,0,0.0881958,"rman one. These translation pairs were used to suggest the SMT system to choose the extracted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to"
C12-1005,2006.amta-papers.25,0,0.0395008,"the SMT system to choose the extracted translations by annotating the decoder input using the XML input markup scheme. 4 Experiments and Evaluation Since the UK GAAP is a monolingual ontology, it holds no reference translation needed for automatic evaluation. Therefore we performed several experiments to find the best approach to translate this financial ontology. For decoding, we used the Moses Toolkit, with its standard settings (Section 4.1). If reference translations were available, we undertook an automatic evaluation using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), and Meteor12 (Denkowski and Lavie, 2011) algorithms. With the first evaluation experiment we translated 16 aligned English-German labels with different translation models (Section 4.2). Furthermore, we translated the bilingual German GAAP to see which translation model performs best regarding the 2794 financial labels that are stored in this ontology (Section 4.3). We also compared the perplexity between several language models and the vocabulary stored in the UK GAAP ontology (Section 4.4). Finally we applied the best translation model to the monolingual ontology and undertook a manual, cro"
C12-1005,steinberger-etal-2006-jrc,0,0.328224,"Missing"
C12-1005,C08-1125,0,0.0916576,"ties, Charges, Balance, Capital, Reserves . . . 2 1 Table 1: Examples for financial labels in the UK GAAP # of labels 30 20 10 0 1 2 3 4 5 7 6 Length of a label 8 9 10 11 Figure 1: Label length of the UK GAAP ontology 3.2 JRC-Acquis The general parallel corpus JRC-Acquis3 was used as baseline training data. This corpus is available in almost every EU official language (except Irish), and is a collection of legislative texts written between 1950 and now. Although previous research showed, that a training model built by using a general resource cannot be used to translate domain-specific terms (Wu et al., 2008), we decided to evaluate the translations on these resources to illustrate any improvement steps from a general resource to specialised domain resources. 3.3 European Central Bank Corpus For comparison with JRC-Acquis, we also did experiments using the European Central Bank Corpus4 , which contains a financial vocabulary. The multilingual corpus is generated by extracting the website and documentation from the European Central Bank and is aligned among 19 European languages. For our research we used the English-German language pair, which consists of 113,171 sentence pairs or 2.8 million Engli"
C12-1005,zesch-etal-2008-extracting,0,0.0118582,"a, they used thresholds to avoid storing undesirable categories. Müller and Gurevych (2008) used 68 Wikipedia and Wiktionary as knowledge bases to integrate semantic knowledge into Information Retrieval. Their models, text semantic relatedness (for Wikipedia) and word semantic relatedness (for Wiktionary), are compared to a statistical model implemented in Lucene. In their approach to bilingual retrieval, they use the cross-language links in Wikipedia, which improved the retrieval performance in their experiment, especially when the machine translation system generated incorrect translations. Zesch et al. (2008) address the issues in accessing the largest collaborative resources: Wikipedia and Wiktionary. They describe several modules and APIs for converting a Wikipedia XML Dump into a more suitable format. Instead of parsing the large Wikipedia XML Dump, they suggest to store the Dump into a database, which significantly increases the performance in retrieval time of queries. 3 Experimental Data We are investigating the problem of translating a domain-specific vocabulary, therefore our experiments started with an analysis of the financial terms stored in the investigated ontology. With these extract"
C18-2019,P02-1040,0,0.138179,"ranslation. It has also been used at IWSLT 2017 and, recently, to measure human parity for machine translation for Chinese to English news text. The demo will present the full end-to-end life cycle of an Appraise evaluation campaign, from task creation to annotation and interpretation of results. 1 Motivation Human evaluation of machine translation is the ultimate measure of translation quality. However, due to data collection effort and annotation cost, many experiments and publications do not report results from human evaluation and rely on scores computed by automated metrics such as BLEU (Papineni et al., 2002) instead. We believe that machine translation researchers should be able to conduct manual annotation campaigns at scale, without having to re-implement the necessary infrastructure from scratch. Since 2007, development of the Appraise evaluation framework for machine translation has supported the research community, trying to bring more human evaluation into MT research. 2 Introduction The Appraise framework has become a standard tool for machine translation evaluation. It is used for shared tasks at the yearly Conference on Machine Translation (WMT) (Bojar et al., 2017) and has been adopted"
C18-2019,W05-0908,0,0.056854,"em (C OMBO -6) and the original WMT17 reference translation (WMT). Our system demonstration provides an end-to-end overview on all aspects of a machine translation evaluation campaign within Appraise. We first describe input data, task creation, and campaign setup, including best practices regarding user and team management. Then, we show the annotation interface and discuss how annotator reliability is measured and monitored, allowing to detect spammers assigning random scores to candidate translations. We describe how statistical significance testing (Wilcoxon, 1945; Mann and Whitney, 1947; Riezler and Maxwell, 2005) can help to solve this problem. Lastly, we explain how final campaign results can be computed, extracted and visualised effectively, so that results are easily interpretable. We also describe the annotation system’s Python-based architecture and highlight implementation details as well as lessons learnt during ten years of human evaluation campaigns based on Appraise. 3 License Appraise source code is available on GitHub1 and is shared under a permissive license2 . 4 Conclusion Our system demonstration explains the full end-to-end life cycle of an Appraise evaluation campaign. It gives an in-"
D19-5503,P05-1074,0,0.373742,"Missing"
D19-5503,C18-2019,1,0.852844,"4, 2019. 2019 Association for Computational Linguistics Our primary contribution is to evaluate various methods of constructing paraphrase corpora, including monolingual methods with experts and nonexperts as well as automated, semi-automated, and manual translation-based approaches. Each paraphrasing method is evaluated for fluency (“does the resulting paraphrase sound not only grammatical but natural?”) and adequacy (“does the paraphrase accurately convey the original meaning of the source?”) using human direct assessment, inspired by effective techniques in machine translation evaluation (Federmann, 2018). In addition, we measure the degree of change between the original and rewritten sentence using both edit distance and BLEU (Papineni et al., 2002). Somewhat surprisingly, fully automatic neural machine translation actually outperforms manual human translation in terms of adequacy. The semi-automatic method of post-editing neural machine translation output with human editors leads to fluency improvements while retaining diversity and adequacy. Although none of the translationbased approaches outperform monolingual rewrites in terms of adequacy or fluency, they do produce greater diversity. Hu"
D19-5503,P01-1008,0,0.141237,"ovides volumes of casual online conversations; the Enron email corpus represents communication in the professional world.2 Both are noisier than usual NMT training data; traditionally, such noise has been challenging for NMT systems (Michel and Neubig, 2018) and should provide a lowerbound on their performance. It would definitely be valuable, albeit expensive, to rerun our experiments on a cleaner data source. Related Work Translation as a means of generating paraphrases has been explored for decades. Paraphrase corpora can be extracted from multiple translations of the same source material (Barzilay and McKeown, 2001). Sub-sentential paraphrases (mostly 2 However, the Enron emails often contain conversations about casual and personal matters. 18 Tokens per segment Tokens per segment Segments Types Tokens median mean min max 500 2,370 9,835 19 19.67 4 Segments Types 46 14,500 Tokens median mean min max 7,196 285,833 19 19.72 1 68 Table 1: Key characteristics of the source sentences. Table 2: Key characteristics of collected paraphrases. As an initial filtering step, we ran automatic grammar and spell-checking, in order to select sentences that exhibit some disfluency or clear error. Additionally, we asked c"
D19-5503,N13-1092,0,0.200978,"Missing"
D19-5503,W13-2305,0,0.052917,"en both candidates. Note that this may mean two things: Paraphrase diversity Additionally, we measure diversity of all paraphrases (both monolingual and based on translation) by computing the average number of token edits between source and candidate texts. To focus our attention on meaningful changes as opposed to minor function word rewrites, we normalize both source and candidate by lower-casing and excluding any punctuation and stop words using NLTK (Bird et al., 2009). We adopt source-based direct assessment (src-DA) for human evaluation of adequacy and fluency. The original DA approach (Graham et al., 2013, 2014) is reference-based and, thus, needs to be adapted for use in our paraphrase assessment and translation scoring scenarios. In both cases, we can use the source sentence to guide annotators in their assessment. Of course, this makes translation evaluation more difficult, as we require bilingual annotators. Src-DA has previously been used, e.g., in (Cettolo et al., 2017; Bojar et al., 2018). Direct assessment initializes mental context for annotators by asking a priming question. The user interface shows two sentences: 1. candidates are semantically equivalent but similarly fluent or non-"
D19-5503,E14-1047,0,0.0419479,"Missing"
D19-5503,P17-2017,0,0.0327967,"Missing"
D19-5503,2004.iwslt-evaluation.1,0,0.160704,"Missing"
D19-5503,D17-1126,0,0.0189255,"non-linguistic input, then ask humans to describe this input in language. For instance, crowd-sourced descriptions of videos provide a rich source of paraphrase data that is grounded in visual phenomena (Chen and Dolan, 2011). Such visual grounding helps users focus on a clear and specific activity without imparting a bias toward particular lexical realizations. Unfortunately, these paraphrases are limited to phenomena that can be realized visually. Another path is to find multiple news stories describing the same event (Dolan et al., 2004), or multiple commentaries about the same news story (Lan et al., 2017). Although this provides a rich and growing set of paraphrases, the language is again biased, this time toward events commonly reported in the news. An alternative is to provide input in a foreign language. Nearly anything expressible in one human language can be written in another language. When users translate content, some variation in lexical realization occurs. To gather monolingual paraphrases, we can first translate a source sentence into a variety of target languages, then translate back into the source language, using either humans or machines. This provides naturalistic variation in"
D19-5503,P11-1020,0,0.0418854,"ses. Surprisingly, human translators do not reliably outperform neural systems. The resulting data release will not only be a useful test set, but will also allow additional explorations in translation and paraphrase quality assessments and relationships. 1 Figure 1: Generating broad-coverage paraphrases through pivot translation. One path around the obstacle of reference bias is to provide a non-linguistic input, then ask humans to describe this input in language. For instance, crowd-sourced descriptions of videos provide a rich source of paraphrase data that is grounded in visual phenomena (Chen and Dolan, 2011). Such visual grounding helps users focus on a clear and specific activity without imparting a bias toward particular lexical realizations. Unfortunately, these paraphrases are limited to phenomena that can be realized visually. Another path is to find multiple news stories describing the same event (Dolan et al., 2004), or multiple commentaries about the same news story (Lan et al., 2017). Although this provides a rich and growing set of paraphrases, the language is again biased, this time toward events commonly reported in the news. An alternative is to provide input in a foreign language. N"
D19-5503,D18-1512,0,0.0387336,"Missing"
D19-5503,E17-1083,0,0.0312977,"emantic equivalence) and fluency of paraphrases, as well as translation adequacy assessments. Data is publicly available at https://aka.ms/MultilingualWhispers. 2 phrasal replacements) can be gathered from these multiple translations. Alternatively, one can create a large body of phrasal replacements from by pivoting on the phrase-tables used by phrase-based statistical machine translation (Bannard and CallisonBurch, 2005; Ganitkevitch et al., 2013; Pavlick et al., 2015). Recent work has also explored using neural machine translation to generate paraphrases via pivoting (Prakash et al., 2016; Mallinson et al., 2017). One can also use neural MT systems to generate large monolingual paraphrase corpora. Another line of work has translated the Czech side of a Czech-English parallel corpus into English, thus producing 50 million words of English paraphrase data (Wieting and Gimpel, 2018). Not only can the system generate interesting paraphrases, but embeddings trained on the resulting data set prove useful in sentence similarity tasks. When added to a paraphrase system, constraints obtained from a semantic parser can reduce the semantic drift encountered during rewrites (Wang et al., 2018). Adding lexical con"
D19-5503,D18-1050,0,0.135962,"oaches and machine translation systems on gathering paraphrase quality. 3 Methodology To run a comprehensive evaluation of paraphrase techniques, we create many paraphrases of a common data set using multiple methods, then evaluate using human direct assessment as well as automatic diversity measurements. 3.1 Data Input data was sampled from two sources: Reddit provides volumes of casual online conversations; the Enron email corpus represents communication in the professional world.2 Both are noisier than usual NMT training data; traditionally, such noise has been challenging for NMT systems (Michel and Neubig, 2018) and should provide a lowerbound on their performance. It would definitely be valuable, albeit expensive, to rerun our experiments on a cleaner data source. Related Work Translation as a means of generating paraphrases has been explored for decades. Paraphrase corpora can be extracted from multiple translations of the same source material (Barzilay and McKeown, 2001). Sub-sentential paraphrases (mostly 2 However, the Enron emails often contain conversations about casual and personal matters. 18 Tokens per segment Tokens per segment Segments Types Tokens median mean min max 500 2,370 9,835 19 1"
D19-5503,P02-1040,0,0.110105,"rpora, including monolingual methods with experts and nonexperts as well as automated, semi-automated, and manual translation-based approaches. Each paraphrasing method is evaluated for fluency (“does the resulting paraphrase sound not only grammatical but natural?”) and adequacy (“does the paraphrase accurately convey the original meaning of the source?”) using human direct assessment, inspired by effective techniques in machine translation evaluation (Federmann, 2018). In addition, we measure the degree of change between the original and rewritten sentence using both edit distance and BLEU (Papineni et al., 2002). Somewhat surprisingly, fully automatic neural machine translation actually outperforms manual human translation in terms of adequacy. The semi-automatic method of post-editing neural machine translation output with human editors leads to fluency improvements while retaining diversity and adequacy. Although none of the translationbased approaches outperform monolingual rewrites in terms of adequacy or fluency, they do produce greater diversity. Human editors, particularly nonexperts, tend toward small edits rather than substantial rewrites. We conclude that round-tripping with neural machine"
D19-5503,P15-2070,0,0.0605049,"Missing"
D19-5503,C16-1275,0,0.0540999,"sments for adequacy (semantic equivalence) and fluency of paraphrases, as well as translation adequacy assessments. Data is publicly available at https://aka.ms/MultilingualWhispers. 2 phrasal replacements) can be gathered from these multiple translations. Alternatively, one can create a large body of phrasal replacements from by pivoting on the phrase-tables used by phrase-based statistical machine translation (Bannard and CallisonBurch, 2005; Ganitkevitch et al., 2013; Pavlick et al., 2015). Recent work has also explored using neural machine translation to generate paraphrases via pivoting (Prakash et al., 2016; Mallinson et al., 2017). One can also use neural MT systems to generate large monolingual paraphrase corpora. Another line of work has translated the Czech side of a Czech-English parallel corpus into English, thus producing 50 million words of English paraphrase data (Wieting and Gimpel, 2018). Not only can the system generate interesting paraphrases, but embeddings trained on the resulting data set prove useful in sentence similarity tasks. When added to a paraphrase system, constraints obtained from a semantic parser can reduce the semantic drift encountered during rewrites (Wang et al.,"
D19-5503,P18-1042,0,0.247489,"lternative is to provide input in a foreign language. Nearly anything expressible in one human language can be written in another language. When users translate content, some variation in lexical realization occurs. To gather monolingual paraphrases, we can first translate a source sentence into a variety of target languages, then translate back into the source language, using either humans or machines. This provides naturalistic variation in language, centered around a common yet relatively unconstrained starting point. Although several research threads have explored this possibility (e.g., (Wieting and Gimpel, 2018)), we have seen few if any comparative evaluations of the quality of this approach. Introduction Humans naturally paraphrase. These paraphrases are often a byproduct: when we can’t recall the exact words, we can often generate approximately the same meaning with a different surface realization. Recognizing and generating paraphrases are key challenges in many tasks, including translation, information retrieval, question answering, and semantic parsing. Large collections of sentential paraphrase corpora could benefit such systems.1 Yet when we ask humans to generate paraphrases of a given task,"
federmann-2010-appraise,P02-1040,0,\N,Missing
federmann-2010-appraise,W09-0401,0,\N,Missing
federmann-2010-appraise,W07-0734,0,\N,Missing
federmann-2010-appraise,W07-0718,0,\N,Missing
federmann-2010-appraise,J96-2004,0,\N,Missing
federmann-2010-appraise,J03-1002,0,\N,Missing
federmann-2010-appraise,W02-0406,0,\N,Missing
federmann-2010-appraise,W08-0309,0,\N,Missing
federmann-2010-appraise,2003.mtsummit-papers.9,0,\N,Missing
federmann-2010-appraise,P03-1021,0,\N,Missing
federmann-2010-appraise,D09-1006,0,\N,Missing
federmann-etal-2012-meta,piperidis-2012-meta,0,\N,Missing
federmann-etal-2012-meta,gavrilidou-etal-2012-meta,0,\N,Missing
federmann-etal-2012-ml4hmt,vandeghinste-etal-2008-evaluation,1,\N,Missing
federmann-etal-2012-ml4hmt,federmann-2010-appraise,1,\N,Missing
federmann-etal-2012-ml4hmt,E06-1005,0,\N,Missing
federmann-etal-2012-ml4hmt,W09-0424,0,\N,Missing
federmann-etal-2012-ml4hmt,P02-1040,0,\N,Missing
federmann-etal-2012-ml4hmt,W02-1019,0,\N,Missing
federmann-etal-2012-ml4hmt,W05-0909,0,\N,Missing
federmann-etal-2012-ml4hmt,W08-0309,0,\N,Missing
federmann-etal-2012-ml4hmt,W10-1720,1,\N,Missing
federmann-etal-2012-ml4hmt,W11-2101,0,\N,Missing
schafer-etal-2008-extracting,copestake-flickinger-2000-open,0,\N,Missing
schafer-etal-2008-extracting,schafer-2006-ontonerdie,1,\N,Missing
W07-0726,W05-0828,1,0.642017,"s has the additional advantage that resources used in standard phrase-based SMT can be flexibly combined with the material extracted from the rule-based MT results; the optimal combination can essentially be reduced to the task of finding good relative weights for the various phrase table entries. Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make i"
W07-0726,P05-3026,0,0.0146287,"rase table entries. Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized module for word alignment is a central component of our setup. Additionally, a r"
W07-0726,2005.mtsummit-papers.11,0,0.0155105,"ck of time for fine-tuning we could not yet show that it indeed helps in increasing the overall quality of the output. 3 see http://www.statmt.org/moses/ 4 4.1 Implementation Details Alignment of MT output The input text and the output text of the MT systems was aligned by means of GIZA++ (Och and Ney, 2003), a tool with which statistical models for alignment of parallel texts can be trained. Since training new models on merely short texts does not yield very accurate results, we applied a method where text can be aligned based on existing models that have been trained on the Europarl Corpus (Koehn, 2005) beforehand. This was achieved by using a modified version of GIZA++ that is able to load given models. The modified version of GIZA++ is embedded into a client-server setup. The user can send two corresponding files to the server, and specify two models for both translation directions from which alignments should be generated. After generating alignments in both directions (by running GIZA++ twice), the system also delivers a combination of these alignments which then serves as input to the following steps described below. 4.2 Phrase tables from MT output We then concatenated the phrase table"
W07-0726,E06-1005,0,0.0772648,"1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized module for word alignment is a central component of our setup. Additionally, a recombination system ne"
W07-0726,P04-1063,0,0.0181544,"T decoder. This has the additional advantage that resources used in standard phrase-based SMT can be flexibly combined with the material extracted from the rule-based MT results; the optimal combination can essentially be reduced to the task of finding good relative weights for the various phrase table entries. Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the ou"
W07-0726,J03-1002,0,0.00454888,"it (Koehn et al., 2006)3 to generate phrase tables from the training data. We enhanced the phrase tables with information on whether a given pair of phrases can also be derived via a third, intermediate language. We assume that this can be useful to distinguish different degrees of reliability, but due to lack of time for fine-tuning we could not yet show that it indeed helps in increasing the overall quality of the output. 3 see http://www.statmt.org/moses/ 4 4.1 Implementation Details Alignment of MT output The input text and the output text of the MT systems was aligned by means of GIZA++ (Och and Ney, 2003), a tool with which statistical models for alignment of parallel texts can be trained. Since training new models on merely short texts does not yield very accurate results, we applied a method where text can be aligned based on existing models that have been trained on the Europarl Corpus (Koehn, 2005) beforehand. This was achieved by using a modified version of GIZA++ that is able to load given models. The modified version of GIZA++ is embedded into a client-server setup. The user can send two corresponding files to the server, and specify two models for both translation directions from which"
W07-0726,2001.mtsummit-papers.3,0,0.0206644,"d in the MT output into a form that is suitable as input for an existing SMT decoder. This has the additional advantage that resources used in standard phrase-based SMT can be flexibly combined with the material extracted from the rule-based MT results; the optimal combination can essentially be reduced to the task of finding good relative weights for the various phrase table entries. Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is genera"
W07-0726,C02-1076,0,0.0250094,"t for an existing SMT decoder. This has the additional advantage that resources used in standard phrase-based SMT can be flexibly combined with the material extracted from the rule-based MT results; the optimal combination can essentially be reduced to the task of finding good relative weights for the various phrase table entries. Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and er"
W07-0726,2001.mtsummit-papers.12,0,0.0249354,"nto a form that is suitable as input for an existing SMT decoder. This has the additional advantage that resources used in standard phrase-based SMT can be flexibly combined with the material extracted from the rule-based MT results; the optimal combination can essentially be reduced to the task of finding good relative weights for the various phrase table entries. Figure 1: Architecture for multi-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as differe"
W07-0726,P03-1021,0,0.00635791,"gnments which then serves as input to the following steps described below. 4.2 Phrase tables from MT output We then concatenated the phrase tables from the SMT baseline system and the phrase tables obtained from the rule-based MT systems and augmented them by additional columns, one for each system used. With this additional information it is clear which of the MT systems a phrase pair stems from, enabling us to assign relative weights to the contributions of the different systems. The optimal weights for the different columns can then be assigned with the help of minimum error rate training (Och, 2003). 5 entries are trained on documents from different domains. However, due to the distinct mechanisms used to generate these entries, rule-based systems and statistical systems usually differ in coverage. Our system managed to utilize lexical entries from various sources by integrating the phrase tables derived from rule-based systems into the phrase table trained on a large parallel corpus. Table 1 shows Systems Ref. R-I R-II SMT Hybrid Table 1: Untranslated tokens (excl. numbers and punctuations) in output for news commentary task (de-en) from different systems a rough estimation of the numbe"
W07-0726,N07-1029,0,0.0816756,"lti-engine MT driven by a SMT decoder architectures that try to select the best output from a number of systems, but leave the individual hypotheses as is (Tidhar and K¨ussner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004; Eisele, 2005) and more sophisticated setups that try to recombine the best parts from multiple hypotheses into a new utterance that can be better than the best of the given candidates, as described in (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Rosti et al., 2007). Recombining multiple MT results requires finding the correspondences between alternative renderings of a source-language expression proposed by different MT systems. This is generally not straightforward, as different word order and errors in the output can make it hard to identify the alignment. Still, we assume that a good way to combine the various MT outcomes will need to involve word alignment between the MT output and the given source text, and hence a specialized module for word alignment is a central component of our setup. Additionally, a recombination system needs a way to pick the"
W07-0726,C00-2122,0,0.0385552,"Missing"
W07-0726,hogan-frederking-1998-evaluation,0,\N,Missing
W07-0726,A94-1016,0,\N,Missing
W07-0726,D08-1076,0,\N,Missing
W07-0726,2005.eamt-1.20,0,\N,Missing
W08-0328,W07-0726,1,0.871462,"This motivated us to re-do these experiments in a somewhat more systematic way for this year’s shared translation task, paying the required attention to all the technical details and also to try it out on more language pairs. Based on an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup, we present new results that show that this type of system combination can actually increase the lexical coverage of the resulting hybrid system, at least as far as this can be measured via BLEU score. 1 2 Introduction (Chen et al., 2007) describes an architecture that allows to combine statistical machine translation (SMT) with one or multiple rule-based machine translation (RBMT) systems in a multi-engine setup. It uses a variant of standard SMT technology to align translations from one or more RBMT systems with the source text and incorporated phrases extracted from these alignments into the phrase table of the SMT system. Using this approach it is possible to employ a vanilla installation of the open-source decoder Moses1 (Koehn et al., 2007) to find good combinations of phrases from SMT training data with the phrases deri"
W08-0328,W07-0732,0,0.371255,"Missing"
W08-0328,P07-2045,0,0.0225822,"system, at least as far as this can be measured via BLEU score. 1 2 Introduction (Chen et al., 2007) describes an architecture that allows to combine statistical machine translation (SMT) with one or multiple rule-based machine translation (RBMT) systems in a multi-engine setup. It uses a variant of standard SMT technology to align translations from one or more RBMT systems with the source text and incorporated phrases extracted from these alignments into the phrase table of the SMT system. Using this approach it is possible to employ a vanilla installation of the open-source decoder Moses1 (Koehn et al., 2007) to find good combinations of phrases from SMT training data with the phrases derived from RBMT. A similar method was presented in (Rosti et al., 2007). This setup provides an elegant solution to the fairly complex task of integrating multiple MT results that may differ in word order using only standard software modules, in particular GIZA++ (Och and Ney, 2003) for the identification of building blocks and Moses for the recombination, but the authors were not able to observe improvements in 1 see http://www.statmt.org/moses/ System Architecture For conducting the translations, we use a multien"
W08-0328,J03-1002,0,0.00312402,"RBMT systems with the source text and incorporated phrases extracted from these alignments into the phrase table of the SMT system. Using this approach it is possible to employ a vanilla installation of the open-source decoder Moses1 (Koehn et al., 2007) to find good combinations of phrases from SMT training data with the phrases derived from RBMT. A similar method was presented in (Rosti et al., 2007). This setup provides an elegant solution to the fairly complex task of integrating multiple MT results that may differ in word order using only standard software modules, in particular GIZA++ (Och and Ney, 2003) for the identification of building blocks and Moses for the recombination, but the authors were not able to observe improvements in 1 see http://www.statmt.org/moses/ System Architecture For conducting the translations, we use a multiengine MT approach based on a ”vanilla” Moses SMT system with a modified phrase table as a central element. This modification is performed by augmenting the standard phrase table with entries obtained from translating the data with several rulebased MT systems. The resulting phrase table thus combines statistically gathered phrase pairs with phrase pairs generate"
W08-0328,P03-1021,0,0.0250543,"to be translated, and corresponding reference translations. In our hybrid setup, it is equally essential to conduct tuning since the combined phrase table we use contains 7 more columns than the original Moses phrase table. All these columns are given the same default weight initially and thus still need be to be tuned to more meaningful values. From this year’s Europarl development data the first 200 sentences of each of the data sets dev2006, test2006, test2007 and devtest2006 were concatenated to build our development set. This set of 800 sentences was used for Minimum Error Rate Training (Och, 2003) to tune the weights of our system with respect to BLEU score. 4 Results In order to be able to evaluate our hybrid approaches in contrast to stand-alone rule-based approaches, we also calculated BLEU scores for the translations conducted by the RBMT systems used in the hybrid setup. Our hybrid system is compared to a SMT baseline and all the 6 RBMT systems that we used. Table 1 shows the evaluation of all the systems in terms of BLEU score (Papineni et al., 2002) with the best score highlighted. The empty cells in the table indicate the language pairs which are not available in the correspond"
W08-0328,P02-1040,0,0.104572,"6, test2007 and devtest2006 were concatenated to build our development set. This set of 800 sentences was used for Minimum Error Rate Training (Och, 2003) to tune the weights of our system with respect to BLEU score. 4 Results In order to be able to evaluate our hybrid approaches in contrast to stand-alone rule-based approaches, we also calculated BLEU scores for the translations conducted by the RBMT systems used in the hybrid setup. Our hybrid system is compared to a SMT baseline and all the 6 RBMT systems that we used. Table 1 shows the evaluation of all the systems in terms of BLEU score (Papineni et al., 2002) with the best score highlighted. The empty cells in the table indicate the language pairs which are not available in the corresponding systems2 . The SMT system is the one upon which we build the hybrid system. According to the scores, the hybrid system produces better results than the baseline SMT system in all 2 The identities of respective RBMT systems are not revealed in this paper. RBMT1 is evaluated on the partial results produced due to some technical problems. 181 cases. The difference between our system and the baseline is more significant for out-of-domain tests, where gaps in the l"
W08-0328,N07-1029,0,0.0526028,"statistical machine translation (SMT) with one or multiple rule-based machine translation (RBMT) systems in a multi-engine setup. It uses a variant of standard SMT technology to align translations from one or more RBMT systems with the source text and incorporated phrases extracted from these alignments into the phrase table of the SMT system. Using this approach it is possible to employ a vanilla installation of the open-source decoder Moses1 (Koehn et al., 2007) to find good combinations of phrases from SMT training data with the phrases derived from RBMT. A similar method was presented in (Rosti et al., 2007). This setup provides an elegant solution to the fairly complex task of integrating multiple MT results that may differ in word order using only standard software modules, in particular GIZA++ (Och and Ney, 2003) for the identification of building blocks and Moses for the recombination, but the authors were not able to observe improvements in 1 see http://www.statmt.org/moses/ System Architecture For conducting the translations, we use a multiengine MT approach based on a ”vanilla” Moses SMT system with a modified phrase table as a central element. This modification is performed by augmenting"
W08-0328,W07-0728,0,0.0854679,"Missing"
W08-0328,D08-1076,0,\N,Missing
W09-0405,W07-0726,1,0.843237,"ata is included in this year’s With the wealth of machine translation systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton"
W09-0405,W08-0328,1,0.915116,"ction 4 its application in a number of experiments. Finally, Section 5 concludes this paper with a summary and some thoughts on future work. We present a simple method for generating translations with the Moses toolkit (Koehn et al., 2007) from existing hypotheses produced by other translation engines. As the structures underlying these translation engines are not known, an evaluationbased strategy is applied to select systems for combination. The experiments show promising improvements in terms of BLEU. 1 2 Introduction Integrating Multiple Systems of Unknown Type and Quality When comparing (Eisele et al., 2008) to the present work, our proposal is more general in a way that the requirement for knowledge about the systems is minimum. The types and the identities of the participated systems are assumed unknown. Accordingly, we are not able to restrict ourselves to a certain class of systems as (Eisele et al., 2008) did. We rely on a standard phrase-based SMT framework to extract the valuable pieces from the system outputs. These extracted segments are also used to improve an existing SMT system that we have access to. While (Eisele et al., 2008) included translations from all of a fixed number of RBMT"
W09-0405,P07-1040,0,0.046861,"ion systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the additional translation"
W09-0405,W08-0332,0,0.037797,"h directions. The phrases are extracted from the intersection of the alignments with the “grow” heuristics. In addition, we also generate a reordering model with the default configuration as included in the Moses toolkit. This “hypothesis” translation model can already be used by the 3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU (Papineni et al., 2001) and METEOR (Banerjee and Lavie, 2005) and a combined scoring scheme provided by the ULC toolkit (Gimenez and Marquez, 2008). In our experiments, we selected a subset of 5 systems for the combination, in most cases, based on BLEU. On the other hand, some systems may be designed in a way that they deliver interesting unique translation segments. Therefore, we also measure the similarity among system outputs as shown in Table 2 in a given collection by calculating average similarity scores across every pair of outputs. Num. Median Range Top 5 Median Range de-en 20 19.87 16.37 de-en 22.26 4.31 fr-en 23 26.55 17.06 fr-en 27.93 4.76 es-en 28 22.50 9.74 es-en 26.43 5.71 en-de 15 13.78 4.75 en-de 15.21 1.71 en-fr 16 24.76"
W09-0405,D08-1011,0,0.0384327,"e nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the additional translations should be produc"
W09-0405,W07-0733,0,0.0224942,"ur hope is that the additional translation hypotheses could bring in new phrases or, more generally, new information that was not contained in the Europarl model. In order to facilitate comparisons, we use in-domain LMs for all setups. We investigate two alternative ways of integrating the additional phrases into the existing SMT system: One is to take the hypothesis translation model described in Section 3.1, the other is to construct system-specific models constructed with only translations from one system at a time. Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. The method requires tuning on at least six more features, which expands the search space for the translation task unnecessarily. We instead integrate the translation models from multiple sources by extending the phrase table. In contrast to the prior approach presented in (Chen et al., 2007) and (Eisele et al., 2008) which concatenates the phrase tables and adds new features as system markers, our extension method avoids duplicate entries in the final combined table. Given a set of hypothesis translation models"
W09-0405,2005.mtsummit-papers.11,0,0.132271,"ing sentence set. task. In this paper, we use the Moses decoder to construct translations from the given system outputs. We mainly propose two slightly different ways: One is to construct translation models solely from the given translations and the other is to extend an existing translation model with these additional translations. 3 3.2 Sometimes, the goal of system combination is not only to produce a translation but also to improve one of the systems. In this paper, we aim at incorporating the additional system outputs to improve an out-of-domain SMT system trained on the Europarl corpus (Koehn, 2005). Our hope is that the additional translation hypotheses could bring in new phrases or, more generally, new information that was not contained in the Europarl model. In order to facilitate comparisons, we use in-domain LMs for all setups. We investigate two alternative ways of integrating the additional phrases into the existing SMT system: One is to take the hypothesis translation model described in Section 3.1, the other is to construct system-specific models constructed with only translations from one system at a time. Although the Moses decoder is able to work with two phrase tables at onc"
W09-0405,E06-1005,0,0.0367832,"th of machine translation systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the ad"
W09-0405,J03-1002,0,0.0039123,"parseness problem). However, in the system combination task, this is no longer an issue as the system only needs to translate sentences within the data set. When more translation engines are available, the size of this set becomes larger. Hence, we collect translations from all available systems and pair them with the corresponding input text, thus forming a medium-sized “hypothesis” corpus. Our system starts processing this corpus with a standard phrase-based SMT setup, using the Moses toolkit (Koehn et al., 2007). The hypothesis corpus is first tokenized and lowercased. Then, we run GIZA++ (Och and Ney, 2003) on the corpus to obtain word alignments in both directions. The phrases are extracted from the intersection of the alignments with the “grow” heuristics. In addition, we also generate a reordering model with the default configuration as included in the Moses toolkit. This “hypothesis” translation model can already be used by the 3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU (Papineni et al., 2001) and METEOR (Banerjee and Lavie, 2005) and a combin"
W09-0405,P03-1021,0,0.0175625,"15.21 1.71 en-fr 16 24.76 11.05 en-fr 26.62 0.68 input texts. Moreover, we also generate “hypothesis” LMs solely based on the given system outputs, that is, LMs that model how the candidate systems convey information in the target language. These LMs do not require any additional training data. Therefore, we do not require any training data other than the given system outputs by using the “hypothesis” language model and the “hypothesis” translation model. 3.5 After building the models, it is essential to tune the SMT system to optimize the feature weights. We use Minimal Error Rate Training (Och, 2003) to maximize BLEU on the complete development data. Unlike the standard tuning procedure, we do not tune the final system directly. Instead, we obtain the weights using models built from the tuning portion of the system outputs. For each combination variant, we first train models on the provided outputs corresponding to the tuning set. This system, called the tuning system, is also tuned on the tuning set. The initial weights of any additional features not included in the standard setting are set to 0. We then adapt the weights to the system built with translations corresponding to the test se"
W09-0405,P02-1040,0,\N,Missing
W09-0405,W05-0909,0,\N,Missing
W09-0405,P07-2045,0,\N,Missing
W09-0411,W07-0732,0,0.028283,"d MT technology as summarized by (Lopez, 2008), including the best statistical, rulebased and example-based systems, produces output rife with errors. Those systems may employ different algorithms or vary in the linguistic resources they use which in turn leads to different characteristic errors. Besides continued research on improving MT techniques, one line of research is dedicated to better exploitation of existing methods for the combination of their respective advantages (Macherey and Och, 2007; Rosti et al., 2007a). Current approaches for system combination involve post-editing methods (Dugast et al., 2007; Theison, 2007), re-ranking strategies, or shallow phrase substitution. The combination procedure applied for this pape tries to optimize word-level translations within a ”trusted” sentence Compute POS tags for translations. We apply part-of-speech (POS) tagging to prepare the selection of possible substitution candidates. For the determination of POS tags we use the Stuttgart TreeTagger (Schmid, 1994). Create word alignment. The alignment between source text and translations is needed to identify translation options within the different systems’ translations. Word alignment is computed using"
W09-0411,gimenez-amigo-2006-iqmt,0,0.0426571,"Missing"
W09-0411,D07-1105,0,0.0237618,"tion (MT) in the last decade, automatic translation is still far away from satisfactory quality. Even the most advanced MT technology as summarized by (Lopez, 2008), including the best statistical, rulebased and example-based systems, produces output rife with errors. Those systems may employ different algorithms or vary in the linguistic resources they use which in turn leads to different characteristic errors. Besides continued research on improving MT techniques, one line of research is dedicated to better exploitation of existing methods for the combination of their respective advantages (Macherey and Och, 2007; Rosti et al., 2007a). Current approaches for system combination involve post-editing methods (Dugast et al., 2007; Theison, 2007), re-ranking strategies, or shallow phrase substitution. The combination procedure applied for this pape tries to optimize word-level translations within a ”trusted” sentence Compute POS tags for translations. We apply part-of-speech (POS) tagging to prepare the selection of possible substitution candidates. For the determination of POS tags we use the Stuttgart TreeTagger (Schmid, 1994). Create word alignment. The alignment between source text and translations is"
W09-0411,J03-1002,0,0.00263951,"ranking strategies, or shallow phrase substitution. The combination procedure applied for this pape tries to optimize word-level translations within a ”trusted” sentence Compute POS tags for translations. We apply part-of-speech (POS) tagging to prepare the selection of possible substitution candidates. For the determination of POS tags we use the Stuttgart TreeTagger (Schmid, 1994). Create word alignment. The alignment between source text and translations is needed to identify translation options within the different systems’ translations. Word alignment is computed using the GIZA++ toolkit (Och and Ney, 2003), only one-to-one word alignments are employed. Select substitution candidates. For the shared task, we decide to substitute nouns, verbs and adjectives based on the available POS tags. Initially, any such source word is considered as a possible substitution candidate. As we do not want to require substitution canProceedings of the Fourth Workshop on Statistical Machine Translation , pages 70–74, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 70 • University of Karlsruhe (uka) didates to have exactly the same POS tag as the source, we use groups of “"
W09-0411,N07-1029,0,0.212533,"2 , Andreas Eisele1,2 , Hans Uszkoreit1,2 , Yu Chen2 , Michael Jellinghaus2 , Sabine Hunsicker2 1: Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz GmbH, Saarbr¨ucken, Germany 2: Universit¨at des Saarlandes, Saarbr¨ucken, Germany {cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de Abstract frame selected due to the high quality of its syntactic structure. The underlying idea of the approach is the improvement of a given (original) translation through the exploitation of additional translations of the same text. This can be seen as a simplified version of (Rosti et al., 2007b). Considering our submission from the shared translation task as the ”trusted” frame, we add translations from four additional MT systems that have been chosen based on their performance in terms of automatic evaluation metrics. In total, the combination system performs 1,691 substitutions, i.e., an average of 0.67 substitutions per sentence. We present a word substitution approach to combine the output of different machine translation systems. Using part of speech information, candidate words are determined among possible translation options, which in turn are estimated through a precompute"
W09-0411,P07-1040,0,0.0901855,"2 , Andreas Eisele1,2 , Hans Uszkoreit1,2 , Yu Chen2 , Michael Jellinghaus2 , Sabine Hunsicker2 1: Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz GmbH, Saarbr¨ucken, Germany 2: Universit¨at des Saarlandes, Saarbr¨ucken, Germany {cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de Abstract frame selected due to the high quality of its syntactic structure. The underlying idea of the approach is the improvement of a given (original) translation through the exploitation of additional translations of the same text. This can be seen as a simplified version of (Rosti et al., 2007b). Considering our submission from the shared translation task as the ”trusted” frame, we add translations from four additional MT systems that have been chosen based on their performance in terms of automatic evaluation metrics. In total, the combination system performs 1,691 substitutions, i.e., an average of 0.67 substitutions per sentence. We present a word substitution approach to combine the output of different machine translation systems. Using part of speech information, candidate words are determined among possible translation options, which in turn are estimated through a precompute"
W10-1708,2001.mtsummit-papers.68,0,0.0402506,"n-matching insertions. For this, we have (manually) derived a set of factors that are checked for each of the phrase pairs that are processed. The factors are described briefly below: prepositions finally, we give prepositions a special treatment. Experience from last year’s shared task had shown that things like double prepositions contributed to a large extent to the amount of avoidable errors. We tried to circumvent this class of error by identifying the correct prepositions; erroneous prepositions are removed. 3 Hybrid Translation Analysis We evaluated the intermediate outputs using BLEU (Papineni et al., 2001) against human references as in table 3. The BLEU score is calculated in lower case after the text tokenization. The translation systems compared are Moses, Lucy, Google and our hybrid system with different configurations: identical? simply checks whether two candidate phrases are identical. too complex? a Lucy phrase is “too complex” to substitute if it contains more than 2 embedded noun phrases. many-to-one? this factor checks if a Lucy phrase containing more than one word is mapped to a Moses phrase with only one token. contains pronoun? checks if the Lucy phrase contains a pronoun. contain"
W10-1708,W09-0411,1,0.72292,"shows that the substitution method works for different language configurations. Introduction In recent years the quality of machine translation (MT) output has improved greatly, although each paradigm suffers from its own particular kind of errors: statistical machine translation (SMT) often shows poor syntax, while rule-based engines (RBMT) experience a lack in vocabulary. Hybrid systems try to avoid these typical errors by combining techniques from both paradigms in a most useful manner. In this paper we present the improved version of the hybrid system we developed last year’s shared task (Federmann et al., 2009). We take the output from an RBMT engine as basis for our hybrid translations and substitute noun phrases by translations from an SMT engine. Even though a general increase in quality could be observed, our system introduced errors of its own during the substi2 Architecture Our hybrid translation system takes translation output from a) the Lucy RBMT system (Alonso and Thurmair, 2003) and b) a Moses-based SMT system (Koehn et al., 2007). We then identify noun phrases inside the rule-based translation and compute the most likely correspondences in the statistical translation output. For these, w"
W10-1708,P07-2045,0,0.00999536,"niques from both paradigms in a most useful manner. In this paper we present the improved version of the hybrid system we developed last year’s shared task (Federmann et al., 2009). We take the output from an RBMT engine as basis for our hybrid translations and substitute noun phrases by translations from an SMT engine. Even though a general increase in quality could be observed, our system introduced errors of its own during the substi2 Architecture Our hybrid translation system takes translation output from a) the Lucy RBMT system (Alonso and Thurmair, 2003) and b) a Moses-based SMT system (Koehn et al., 2007). We then identify noun phrases inside the rule-based translation and compute the most likely correspondences in the statistical translation output. For these, we apply a factored substitution method that decides whether the original RBMT phrase should be kept or rather be replaced by the Moses phrase. As this shallow substitution process may introduce problems at 77 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 77–81, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Moses SMT System We used a state-of-the-art"
W10-1708,P02-1040,0,\N,Missing
W11-2141,2003.mtsummit-systems.1,0,0.803897,"trees that have been computed for the respective input sentences. Henceforth, any improvements that can be achieved for the analysis phase of a given RBMT system directly lead to improved translation output which makes this an interesting topic in the context of hybrid MT. In this paper we present a study how the rulebased analysis phase of a commercial RBMT system can be supplemented by a stochastic parser. The system under investigation is the rule-based engine Lucy LT. This software uses a sophisticated RBMT transfer approach with a long research history; it is explained in more detail in (Alonso and Thurmair, 2003). The output of its analysis phase is a parse forest containing a small number of tree structures. For our hybrid system we investigated if the existing rule base of the Lucy LT system chooses the best tree from the analysis forest and how the selection of this best tree out of the set of candidates can be improved by adding stochastic knowledge to the rule-based system. The remainder of this paper is structured in the following way: in Section 2 we first describe the transfer-based architecture of the rule-based Lucy LT engine, giving special focus to its analysis phase which we are trying to"
W11-2141,2011.eamt-1.31,1,0.887895,"Missing"
W11-2141,P03-1054,0,0.00496386,"lysis tree resulting from stochastic parse selection good indication of which trees lead to good translations, as is depicted in Table 1. Still, in many cases an alternative tree would have lead to a better translation. As additional feature, we chose to use the tree edit distance of each analysis candidate to a stochastic parse tree. An advantage of stochastic parsing lies in the fact that parsers from this class can deal very well even with ungrammatical or unknown output, which we have seen is problematic for a rulebase parser. We decided to make use of the Stanford Parser as described in (Klein and Manning, 2003), which uses an unlexicalised probabilistic contextfree grammar that was trained on the Penn Treebank2 . We parse the original source sentence with this PCFG grammar to get a stochastic parse tree that can be compared to the trees from the Lucy analysis forest. In our experiments, we compare the stochastic parse tree with the alternatives given by Lucy LT. Tree comparison is implemented based on the Tree Edit Distance, as originally defined in (Zhang and Shasha, 1989). In analogy to the Word Edit or Levenshtein Distance, the distance between two trees is the number of editing actions that are"
W11-2141,2001.mtsummit-papers.68,0,0.0223749,"The modified text is translated using the hybrid set-up described above. After the translation is finished, the placeholders are replaced by their respective translations. 3 Evaluation 3.1 Shared Task Setup For the WMT11 shared translation task, we submitted three different runs of our hybrid MT system: 1. Hybrid Transfer (without the Tree Selector, but with the extended lexicon) 2. Full Hybrid (with both the Tree Selector and the extended lexicon) 3. Full Hybrid+Named Entities (full hybrid and named entity handling) Our primary submission was run #3. All three runs were evaluated using BLEU (Papineni et al. (2001)) and TER (Snover et al. (2006)). The results from these automated metrics are reported in Table 2. Table 2: Automatic metric scores for WMT11 System Hybrid Transfer Full Hybrid Full Hybrid+Named Entities 3 BLEU 13.4 13.1 12.8 TER 0.792 0.796 0.800 http://incubator.apache.org/opennlp/ 354 Table 3 shows that we were able to outperform the original Lucy version. Furthermore, it turned out that our hybrid system was the best-scoring system from all shared task participants. Table 3: Manual evaluation scores for WMT11 System Full Hybrid+Named Entities Original Lucy 3.2 Normalized Score 0.6805 0.65"
W11-2141,2006.amta-papers.25,0,0.029128,"sing the hybrid set-up described above. After the translation is finished, the placeholders are replaced by their respective translations. 3 Evaluation 3.1 Shared Task Setup For the WMT11 shared translation task, we submitted three different runs of our hybrid MT system: 1. Hybrid Transfer (without the Tree Selector, but with the extended lexicon) 2. Full Hybrid (with both the Tree Selector and the extended lexicon) 3. Full Hybrid+Named Entities (full hybrid and named entity handling) Our primary submission was run #3. All three runs were evaluated using BLEU (Papineni et al. (2001)) and TER (Snover et al. (2006)). The results from these automated metrics are reported in Table 2. Table 2: Automatic metric scores for WMT11 System Hybrid Transfer Full Hybrid Full Hybrid+Named Entities 3 BLEU 13.4 13.1 12.8 TER 0.792 0.796 0.800 http://incubator.apache.org/opennlp/ 354 Table 3 shows that we were able to outperform the original Lucy version. Furthermore, it turned out that our hybrid system was the best-scoring system from all shared task participants. Table 3: Manual evaluation scores for WMT11 System Full Hybrid+Named Entities Original Lucy 3.2 Normalized Score 0.6805 0.6599 Error Analysis The selection"
W11-2141,wentland-etal-2008-building,0,0.0847907,"Missing"
W11-2141,P02-1040,0,\N,Missing
W12-0115,W11-2134,0,0.0287674,"est2012” test set data for language pair English→German. 4.2 Experimental Results 5 Using the annotated data set, we then trained a decision tree and integrated it into our hybrid system. To evaluate translation quality, we created translations of the WMT12 “newstest2012” test set, for the language pair English→German, with a) a baseline hybrid system using hand-crafted decision rules and b) an extended version of our hybrid system using the decision tree. Both hybrid systems relied on a Lucy translation template and were given additional translation candidates from another rule-based system (Aleksic and Thurmair, 2011), a statistical system based on the Moses decoder, and a statistical system based on Joshua. If more than one “good” translation was found, we used the handcrafted rules to determine the single, winning translation candidate (implementing SELECTBEST in the simplest, possible way). Table 2 shows results for our two hybrid system variants as well as for the individual baseline systems. We report results from automatic BLEU (Papineni et al., 2001) scoring and also from its case-sensitive variant, BLEU-cased. In this paper, we reported on experiments aiming to improve the phrase selection componen"
W12-0115,2003.mtsummit-systems.1,0,0.48873,"Chapter of the Association for Computational Linguistics, pages 113–118, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics have defined and how these could be used in feature vectors for machine learning methods in Section 3. Our experiments with the classifier-based, hybrid MT system are reported in Section 4. We conclude by giving a summary of our work and then provide an outlook to related future work in Section 5. 2 Architecture Our hybrid machine translation system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003), and b) one or several other MT systems, e.g. Moses (Koehn et al., 2007), or Joshua (Li et al., 2009). The rule-based component of our hybrid system is described in more detail in section 2.2 while we provide more detailed information on the “other” systems in section 2.3. 2.1 Basic Approach We first identify noun phrases inside the rulebased translation and compute the most probable correspondences in the translation output from the other systems. For the resulting phrases, we apply a factored substitution method that decides whether the original RBMT phrase should be kept or rather be repla"
W12-0115,W11-2141,1,0.697484,"- phrase substitution failed. Introduction In recent years, the overall quality of machine translation output has improved greatly. Still, each technological paradigm seems to suffer from its own particular kinds of errors: statistical MT (SMT) engines often show poor syntax, while rule-based MT systems suffer from missing data in their vocabularies. Hybrid approaches try to overcome these typical errors by combining techniques from both (or even more) paradigms in an optimal manner. In this paper we report on experiments with an extended version of the hybrid system we develop in our group (Federmann and Hunsicker, 2011; Federmann et al., 2010). We take the output from an RBMT engine as “translation template” for our Errors of the first class cannot be corrected, as we do not have an easy way of knowing when the translation obtained from an external MT engine is incorrect. The other classes could, however, be eliminated by introducing additional steps for preand post-processing as well as by improving the hybrid substitution algorithm itself. So far, our algorithm relied on many, hand-crafted decision factors; in order to improve translation quality and processing speed, we decided to apply machine learning"
W12-0115,W10-1708,1,0.874107,"Introduction In recent years, the overall quality of machine translation output has improved greatly. Still, each technological paradigm seems to suffer from its own particular kinds of errors: statistical MT (SMT) engines often show poor syntax, while rule-based MT systems suffer from missing data in their vocabularies. Hybrid approaches try to overcome these typical errors by combining techniques from both (or even more) paradigms in an optimal manner. In this paper we report on experiments with an extended version of the hybrid system we develop in our group (Federmann and Hunsicker, 2011; Federmann et al., 2010). We take the output from an RBMT engine as “translation template” for our Errors of the first class cannot be corrected, as we do not have an easy way of knowing when the translation obtained from an external MT engine is incorrect. The other classes could, however, be eliminated by introducing additional steps for preand post-processing as well as by improving the hybrid substitution algorithm itself. So far, our algorithm relied on many, hand-crafted decision factors; in order to improve translation quality and processing speed, we decided to apply machine learning methods to our training d"
W12-0115,federmann-2010-appraise,1,0.849784,"ron (Rosenblatt, 1958) algorithm, we have to prepare our training set with a sufficiently large number of annotated training instances. We give further details on the creation of an annotated training set in section 4.1. Generating Training Data For this, we first created an annotated data set. In a nutshell, we computed feature vectors and potential substitution candidates for all noun phrases in our training data4 and then collected data from human annotators which of the substitution candidates were “good” translations and which should rather be considered “bad” examples. We used Appraise (Federmann, 2010) for the annotation, and collected 24,996 labeled training instances with the help of six human annotators. Table 1 gives an overview of the data sets characteristics. Translation Candidates 3.2 Creating Hybrid Translations Using suitable training data, we can train a binary classifier (using either a decision tree, an SVM, or the Perceptron algorithm) that can be used in our hybrid combination algorithm. The pseudo-code in Algorithm 1 illustrates how such a classifier can be used in our hybrid MT decoder. Count Total “good” “bad” 24,996 10,666 14,330 Table 1: Training data set characteristics"
W12-0115,P07-2045,0,0.00361719,"non, France, April 23 - 27 2012. 2012 Association for Computational Linguistics have defined and how these could be used in feature vectors for machine learning methods in Section 3. Our experiments with the classifier-based, hybrid MT system are reported in Section 4. We conclude by giving a summary of our work and then provide an outlook to related future work in Section 5. 2 Architecture Our hybrid machine translation system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003), and b) one or several other MT systems, e.g. Moses (Koehn et al., 2007), or Joshua (Li et al., 2009). The rule-based component of our hybrid system is described in more detail in section 2.2 while we provide more detailed information on the “other” systems in section 2.3. 2.1 Basic Approach We first identify noun phrases inside the rulebased translation and compute the most probable correspondences in the translation output from the other systems. For the resulting phrases, we apply a factored substitution method that decides whether the original RBMT phrase should be kept or rather be replaced by one of the candidate phrases. As this shallow substitution process"
W12-0115,2005.mtsummit-papers.11,0,0.0129497,"ility of these 1-best trees already allowed us to improve the translation quality of the RBMT system as we had shown in previous work. 2.3 Substitution Candidate Translations We use state-of-the-art SMT systems to create statistical, phrase-based translations of our input text, together with the bidirectional word alignments between the source texts and the translations. Again, we make use of markup which helps to identify unknown words as this will later be useful in the factored substitution method. Translation models for our SMT systems were trained with lower-cased and tokenised Europarl (Koehn, 2005) training data. We used the LDC Gigaword corpus to train large scale language models and tokenised the source texts using the tokenisers available from the WMT shared task website3 . All translations are re-cased before they are sent to the hybrid system together with the word alignment information. 114 3 Available at http://www.statmt.org/wmt12/ The hybrid MT system can easily be adapted to support other translation engines. If there is no alignment information available directly, a word alignment tool is needed as the alignment is a key requirement for the hybrid system. For partof-speech ta"
W12-0115,W09-0424,0,0.0195202,"2012 Association for Computational Linguistics have defined and how these could be used in feature vectors for machine learning methods in Section 3. Our experiments with the classifier-based, hybrid MT system are reported in Section 4. We conclude by giving a summary of our work and then provide an outlook to related future work in Section 5. 2 Architecture Our hybrid machine translation system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003), and b) one or several other MT systems, e.g. Moses (Koehn et al., 2007), or Joshua (Li et al., 2009). The rule-based component of our hybrid system is described in more detail in section 2.2 while we provide more detailed information on the “other” systems in section 2.3. 2.1 Basic Approach We first identify noun phrases inside the rulebased translation and compute the most probable correspondences in the translation output from the other systems. For the resulting phrases, we apply a factored substitution method that decides whether the original RBMT phrase should be kept or rather be replaced by one of the candidate phrases. As this shallow substitution process may introduce errors at phra"
W12-0115,2001.mtsummit-papers.68,0,0.014936,"n tree. Both hybrid systems relied on a Lucy translation template and were given additional translation candidates from another rule-based system (Aleksic and Thurmair, 2011), a statistical system based on the Moses decoder, and a statistical system based on Joshua. If more than one “good” translation was found, we used the handcrafted rules to determine the single, winning translation candidate (implementing SELECTBEST in the simplest, possible way). Table 2 shows results for our two hybrid system variants as well as for the individual baseline systems. We report results from automatic BLEU (Papineni et al., 2001) scoring and also from its case-sensitive variant, BLEU-cased. In this paper, we reported on experiments aiming to improve the phrase selection component of a hybrid MT system using machine learning. We described the architecture of our hybrid machine translation system and its main components. We explained how to train a decision tree based on feature vectors that emulate previously used, hand-crafted decision factors. To obtain training data for the classifier, we manually annotated a set of 24,996 feature vectors and compared the decision-tree-based, hybrid system to a baseline version. We"
W12-0115,P02-1040,0,\N,Missing
W12-3138,2003.mtsummit-systems.1,0,0.372079,"resting” translation candidates from one or more translation engines which could be substituted into our translation templates. The substitution process is either controlled by the output from a binary classifier trained on feature vectors from the different MT engines, or it is depending on weights for the decision factors, which have been tuned using MERT. We are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system. 1 2 System Architecture Our hybrid MT system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003); b) the Linguatec RBMT system (Aleksic and Thurmair, 2011); c) Moses (Koehn et al., 2007); d) Joshua (Li et al., 2009). Introduction In recent years, machine translation (MT) systems have achieved increasingly better translation quality. Still each paradigm has its own challenges: while statistical MT (SMT) systems suffer from a lack of grammatical structure, resulting in ungrammatical sentences, RBMT systems have to deal with a lack of lexical coverage. Hybrid architectures intend to combine the advantages of the individual paradigms to achieve an overall better translation. Federmann et al."
W12-3138,W11-2141,1,0.825388,"guatec RBMT system (Aleksic and Thurmair, 2011); c) Moses (Koehn et al., 2007); d) Joshua (Li et al., 2009). Introduction In recent years, machine translation (MT) systems have achieved increasingly better translation quality. Still each paradigm has its own challenges: while statistical MT (SMT) systems suffer from a lack of grammatical structure, resulting in ungrammatical sentences, RBMT systems have to deal with a lack of lexical coverage. Hybrid architectures intend to combine the advantages of the individual paradigms to achieve an overall better translation. Federmann et al. (2010) and Federmann and Hunsicker (2011) have shown that using a substitutionbased approach can improve the translation quality of a baseline RBMT system. Our submission to Lucy provides us with the translation skeleton, which is described in more detail in Section 2.2 while systems b)–d) are aligned to this translation template and mined for substitution candidates. We give more detailed information on these systems in Section 2.3. 2.1 Basic Approach We first identify “interesting” phrases inside the rule-based translation and then compute the most probable correspondences in the translation output from the other systems. For the r"
W12-3138,W10-1708,1,0.888582,"Thurmair, 2003); b) the Linguatec RBMT system (Aleksic and Thurmair, 2011); c) Moses (Koehn et al., 2007); d) Joshua (Li et al., 2009). Introduction In recent years, machine translation (MT) systems have achieved increasingly better translation quality. Still each paradigm has its own challenges: while statistical MT (SMT) systems suffer from a lack of grammatical structure, resulting in ungrammatical sentences, RBMT systems have to deal with a lack of lexical coverage. Hybrid architectures intend to combine the advantages of the individual paradigms to achieve an overall better translation. Federmann et al. (2010) and Federmann and Hunsicker (2011) have shown that using a substitutionbased approach can improve the translation quality of a baseline RBMT system. Our submission to Lucy provides us with the translation skeleton, which is described in more detail in Section 2.2 while systems b)–d) are aligned to this translation template and mined for substitution candidates. We give more detailed information on these systems in Section 2.3. 2.1 Basic Approach We first identify “interesting” phrases inside the rule-based translation and then compute the most probable correspondences in the translation outpu"
W12-3138,federmann-2010-appraise,1,0.854725,"“bad” translations. In order to make use of machine (ML) learning methods such as decision trees (Breiman et al., 1984), Support Vector Machines (Vapnik, 1995), or the Perceptron (Rosenblatt, 1958) algorithm, we have to prepare our training set with a sufficiently large amount of annotated training instances. To create the training data set, we computed the feature vectors and all possible substitution candidates for the WMT12 “newstest2011” development set. Human annotators were then given the task to assign to each candidate whether it was a “good” or a “bad” substitution. We used Appraise (Federmann, 2010) for the annotation, and collected a set of 24,996 labeled training instances with the help of six human annotators. Table 1 gives an overview of the data sets characteristics. The decision tree learned from this data replaces the hand-crafted rules. Machine-Learned Decision Tree Previous work used hand-crafted rules. These are now replaced by a classifier which was trained on annotated data. Our training set D can formally be 1 D = {(xi , yi )|xi ∈ Rp , yi ∈ {−1, 1}}ni=1 Hybrid Systems Baseline Systems Baseline +Decision Tree +MERT Lucy Linguatec Joshua Moses BLEU 13.9 14.2 14.3 14.0 14.7 14."
W12-3138,P07-2045,0,0.00591486,"nto our translation templates. The substitution process is either controlled by the output from a binary classifier trained on feature vectors from the different MT engines, or it is depending on weights for the decision factors, which have been tuned using MERT. We are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system. 1 2 System Architecture Our hybrid MT system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003); b) the Linguatec RBMT system (Aleksic and Thurmair, 2011); c) Moses (Koehn et al., 2007); d) Joshua (Li et al., 2009). Introduction In recent years, machine translation (MT) systems have achieved increasingly better translation quality. Still each paradigm has its own challenges: while statistical MT (SMT) systems suffer from a lack of grammatical structure, resulting in ungrammatical sentences, RBMT systems have to deal with a lack of lexical coverage. Hybrid architectures intend to combine the advantages of the individual paradigms to achieve an overall better translation. Federmann et al. (2010) and Federmann and Hunsicker (2011) have shown that using a substitutionbased appro"
W12-3138,2005.mtsummit-papers.11,0,0.0095066,"nd tuning, this new approach has resulted in a reduced number of erroneous alignment links. Additionally, we also changed our set of decision factors, increasing their total number. Whereas an older version of this system only used four factors, we now consider the following twelve factors: 1. frequency: frequency of a given candidate phrase compared to total number of candidates for the current phrase; 2. LM(phrase): language model (LM) score of the phrase; 3. LM(phrase+1): phrase with right-context; 313 The language model was trained using the SRILM toolkit (Stolcke, 2002), on the EuroParl (Koehn, 2005) corpus, and lemmatised or part-of-speech tagged versions, respectively. We used the TreeTagger (Schmid, 1994) for lemmatisation as well as part-of-speech tagging. The substitution algorithm itself was also adapted. We investigated two machine learning approaches. In the previous version, the system used a handwritten decision tree to perform the substitution: 1. the first of the two new approaches consisted of machine learning this decision tree from annotated data; 2. the second approach was to assign a weight to each factor and using MERT tuning of these weights on a development set. Both a"
W12-3138,W09-0424,0,0.0153735,"he substitution process is either controlled by the output from a binary classifier trained on feature vectors from the different MT engines, or it is depending on weights for the decision factors, which have been tuned using MERT. We are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system. 1 2 System Architecture Our hybrid MT system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003); b) the Linguatec RBMT system (Aleksic and Thurmair, 2011); c) Moses (Koehn et al., 2007); d) Joshua (Li et al., 2009). Introduction In recent years, machine translation (MT) systems have achieved increasingly better translation quality. Still each paradigm has its own challenges: while statistical MT (SMT) systems suffer from a lack of grammatical structure, resulting in ungrammatical sentences, RBMT systems have to deal with a lack of lexical coverage. Hybrid architectures intend to combine the advantages of the individual paradigms to achieve an overall better translation. Federmann et al. (2010) and Federmann and Hunsicker (2011) have shown that using a substitutionbased approach can improve the translati"
W12-3138,E06-1005,0,0.063574,"Missing"
W12-3138,J03-1002,0,0.0028412,"ult, 2010). This module uses a modified version of TERp (Snover et al., 2009) and a set of different costs to create the best alignment between any two given sentences. In our case, each single candidate translation is aligned to the translation template that has been produced by the Lucy RBMT system. As we do not use the source in this alignment technique, we can use any translation system, regardless of whether this system provides us with a source-totarget alignment. In earlier versions of this system, we compiled the source-to-target alignments for the candidate translations using GIZA++ (Och and Ney, 2003), but these alignments contained many errors. By using target-to-target alignments, we are able to reduce the amount of those errors which is, of course, preferred. 2.4 Substitution Approaches Using the parse tree structures provided by Lucy, we extract “interesting” phrases for substitution. This includes noun phrases of various complexity, then simple verb phrases consisting of only the main verb, and finally adjective phrases. Through the target-to-target alignments we identify and collect the set of potential substitution candidates. Phrase substitution can be performed using two methods."
W12-3138,2001.mtsummit-papers.68,0,0.0149883,"e feature vectors making up each hypothesis. We used Z-MERT (Zaidan, 2009) to optimise the set of feature weights on the “newstest2011” development set. 3 Evaluation Using the “newstest2012” test set, we created baseline translations for the four MT systems used in our hybrid system. Then we performed three runs of our hybrid system: a) a baseline run, using the factors and uniformly distributed weights; b) a run using the weights trained on the development set; c) a run using the decision tree learned from annotated data. Table 2 shows the results for automatic metrics’ scores. Besides BLEU (Papineni et al., 2001), we also report its case-sensitive variant, BLEU-cased, and TER (Snover et al., 2006) scores. Comparing the scores, we see that both advanced hybrid methods perform better than the original, baseline hybrid as well as the Lucy baseline system. The MERT approach performs slightly better than the decision tree. This proves that using machinelearning to adapt the substitution approach results in better translation quality. Other baseline systems, however, still outperform the hybrid systems. In part this is due to the fact that we are preserving the basic structure of the RBMT translation and do"
W12-3138,2006.amta-papers.25,0,0.0439265,"he set of feature weights on the “newstest2011” development set. 3 Evaluation Using the “newstest2012” test set, we created baseline translations for the four MT systems used in our hybrid system. Then we performed three runs of our hybrid system: a) a baseline run, using the factors and uniformly distributed weights; b) a run using the weights trained on the development set; c) a run using the decision tree learned from annotated data. Table 2 shows the results for automatic metrics’ scores. Besides BLEU (Papineni et al., 2001), we also report its case-sensitive variant, BLEU-cased, and TER (Snover et al., 2006) scores. Comparing the scores, we see that both advanced hybrid methods perform better than the original, baseline hybrid as well as the Lucy baseline system. The MERT approach performs slightly better than the decision tree. This proves that using machinelearning to adapt the substitution approach results in better translation quality. Other baseline systems, however, still outperform the hybrid systems. In part this is due to the fact that we are preserving the basic structure of the RBMT translation and do not reorder the new hybrid translation. To improve the hybrid approach further, there"
W12-3138,W09-0441,0,0.0224841,"so included an additional RBMT system into the architecture. Knowing that statistical systems make similar errors, we hope to balance out this fact by exploiting also a system of a different paradigm, namely RBMT. To create the statistical translations, we used stateof-the-art SMT systems. Both our Moses and Joshua systems were trained on the EuroParl corpus and News Commentary1 training data. We performed tuning on the “newstest2011” data set using MERT. We compile alignments between translations with the alignment module of MANY (Barrault, 2010). This module uses a modified version of TERp (Snover et al., 2009) and a set of different costs to create the best alignment between any two given sentences. In our case, each single candidate translation is aligned to the translation template that has been produced by the Lucy RBMT system. As we do not use the source in this alignment technique, we can use any translation system, regardless of whether this system provides us with a source-totarget alignment. In earlier versions of this system, we compiled the source-to-target alignments for the candidate translations using GIZA++ (Och and Ney, 2003), but these alignments contained many errors. By using targ"
W12-3138,P02-1040,0,\N,Missing
W12-3138,W11-2134,0,\N,Missing
W12-4210,W11-1204,0,0.043027,"Missing"
W12-4210,P07-2045,0,0.00794763,"ailable knowledge5 . With the heavily interlinked information base, Wikipedia forms a rich lexical and semantic resource. Besides a large amount of articles, it also holds a hierarchy of Categories that Wikipedia Articles are tagged with. It includes knowledge about named entities, domain-specific terms and word senses. Furthermore, the redirect system of Wikipedia articles can be used as a dictionary for synonyms, spelling variations and abbreviations. 3.5 Translation System: Moses For generating translations from English into German and vice versa, the statistical translation toolkit Moses (Koehn et al., 2007) was used to build the training model and for decoding. For this approach, a phrase-based approach was taken instead of a tree based model. Further, we aimed at improving the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with 5 http://en.wikipedia.org/wiki/Wikipedia:Size_comparison the GIZA++ toolkit (Och and Ney, 2003), whereby the 5-gram language model was built by SRILM (Stolcke, 2002). 4 Domain-specific Resource Generation In this section, two different types of data and the approach of buil"
W12-4210,2005.mtsummit-papers.11,0,0.00794548,"esults of Google Translate. This finding leads us to the conclusion that a hybrid translation system, a combination of bilingual terminological resources and statistical machine translation can help to improve translation of domain-specific terms. 1 Introduction Our research on translation of ontology vocabularies is motivated by the challenge of translating domainspecific terms with restricted or no additional textual context that in other cases can be used for translation improvement. For our experiment we started by translating financial terms with baseline systems trained on the EuroParl (Koehn, 2005) corpus and the JRC-Acquis (Steinberger et al., 2006) corpus. Although both resources contain a large amount of parallel data, the translations were not satisfying. To improve the translations of the financial ontology vocabulary we built a new parallel resource, which was generated using Linguee1 , an online translation query service. With this data, we could train a small system, which produced better translations than the baseline model using only general resources. Since the manual development of terminological resources is a time intensive and expensive task, we used Wikipedia as a backgr"
W12-4210,W05-0909,0,0.0424273,". As an example, we examined the Category Accounting terminology and stored the English Wikipedia Title Balance sheet with the German equivalent Wikipedia Title Bilanz. At the end of the lexicon generation we examined 5228 Wikipedia Articles, which were tagged with one or more financial Categories. From this set of Articles we were able to generate a terminological lexicon with 3228 English-German entities. 5 Evaluation Tables 4 to 5 illustrate the final results for our experiments on translating xEBR ontology terms, using the NIST (Doddington, 2002), BLEU (Papineni et al., 2002), and Meteor (Lavie and Agarwal, 2005) algorithms. To further study any translation improvements of our experiment, we also used Google Translate8 in translating 63 financial xEBR terms (cf. Section 3.1) from English into German and from German into English. 5.1 Interpretation of Evaluation Metrics In our experiments translation models built from a general resource performed worst. These re8 Translations were generated on February 2012. 90 Scoring Metric Source # correct BLEU NIST Google Translate JRC-Acquis EuroParl Linguee Lexical substitution Linguee+Wiki 21 9 5 15 4 22 0.452 0.127 0.021 0.364 0.006 0.348 4.830 2.458 1.307 3.93"
W12-4210,J03-1002,0,0.00271255,"lling variations and abbreviations. 3.5 Translation System: Moses For generating translations from English into German and vice versa, the statistical translation toolkit Moses (Koehn et al., 2007) was used to build the training model and for decoding. For this approach, a phrase-based approach was taken instead of a tree based model. Further, we aimed at improving the translations only on the surface level, and therefore no part-of-speech information was taken into account. Word and phrase alignments were built with 5 http://en.wikipedia.org/wiki/Wikipedia:Size_comparison the GIZA++ toolkit (Och and Ney, 2003), whereby the 5-gram language model was built by SRILM (Stolcke, 2002). 4 Domain-specific Resource Generation In this section, two different types of data and the approach of building them will be presented. Section 4.1 gives an overview of generating a parallel resource from Linguee, which was used in generating a new domain-specific training model. In Section 4.2 a detailed description is given how we extracted terms from Wikipedia for generating a domain-specific lexicon. 4.1 Domain-specific parallel corpus generation To build a new training model that is specialised on our xEBR ontology, w"
W12-4210,P02-1040,0,0.0827818,"ikipedia knowledge base also existed. As an example, we examined the Category Accounting terminology and stored the English Wikipedia Title Balance sheet with the German equivalent Wikipedia Title Bilanz. At the end of the lexicon generation we examined 5228 Wikipedia Articles, which were tagged with one or more financial Categories. From this set of Articles we were able to generate a terminological lexicon with 3228 English-German entities. 5 Evaluation Tables 4 to 5 illustrate the final results for our experiments on translating xEBR ontology terms, using the NIST (Doddington, 2002), BLEU (Papineni et al., 2002), and Meteor (Lavie and Agarwal, 2005) algorithms. To further study any translation improvements of our experiment, we also used Google Translate8 in translating 63 financial xEBR terms (cf. Section 3.1) from English into German and from German into English. 5.1 Interpretation of Evaluation Metrics In our experiments translation models built from a general resource performed worst. These re8 Translations were generated on February 2012. 90 Scoring Metric Source # correct BLEU NIST Google Translate JRC-Acquis EuroParl Linguee Lexical substitution Linguee+Wiki 21 9 5 15 4 22 0.452 0.127 0.021 0."
W12-4210,C08-1125,0,0.0513713,"Missing"
W12-4210,zesch-etal-2008-extracting,0,0.0196027,"Missing"
W12-4210,steinberger-etal-2006-jrc,0,\N,Missing
W12-4210,W07-0734,0,\N,Missing
W12-5708,W11-2107,0,0.0433795,"Missing"
W12-5708,W12-0115,1,0.871975,"Missing"
W12-5708,2012.amta-papers.23,1,0.832963,"Missing"
W12-5708,W12-3138,1,0.878293,"Missing"
W12-5708,W11-2141,1,0.868128,"Missing"
W12-5708,W09-0411,1,0.902275,"Missing"
W12-5708,A94-1016,0,0.318854,"Missing"
W12-5708,2005.eamt-1.15,0,0.0611863,"Missing"
W12-5708,P10-1064,0,0.0710613,"Missing"
W12-5708,P03-1054,0,0.00817899,"Missing"
W12-5708,P02-1040,0,0.0940555,"Missing"
W12-5708,N07-1029,0,0.0598436,"Missing"
W12-5709,2003.mtsummit-systems.1,0,0.0818841,"Missing"
W12-5709,W11-2107,0,0.0383819,"Missing"
W12-5709,W12-3138,1,0.877314,"Missing"
W12-5709,P07-2045,0,0.00673657,"Missing"
W12-5709,W12-5706,1,0.882913,"Missing"
W12-5709,W12-5705,1,0.848415,"Missing"
W12-5709,P02-1040,0,0.0881819,"Missing"
W12-5709,2006.tc-1.12,0,0.132657,"Missing"
W12-5709,W12-5704,1,0.829445,"Missing"
W12-5709,2009.iwslt-evaluation.8,0,\N,Missing
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W18-6401,W18-6432,1,0.779979,"Missing"
W18-6401,W07-0718,1,0.492999,"Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation. 1 Introduction The Third Conference on Machine Translation (WMT) held at EMNLP 20181 host a number of shared tasks on various aspects of machine translation. This conference builds on twelve previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants we"
W18-6401,W11-2101,1,0.731598,"Missing"
W18-6401,W08-0309,1,0.64499,"Missing"
W18-6401,W10-1703,1,0.498736,"Missing"
W18-6401,W12-3102,1,0.647494,"Missing"
W18-6401,E14-2008,0,0.0242419,"n the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated data. At inference time, translations which are copies of the source are filtered out, replacing them with the output of a very small news-commentary only"
W18-6401,E17-2058,0,0.0576994,"Missing"
W18-6401,W18-6406,0,0.107817,"t (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous y"
W18-6401,W13-2305,1,0.876146,"017 (English) and news 2011 (Chinese). Subwords (BPE) are used for both English and Chinese sentences. 3 Human Evaluation A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition. This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and two years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR) and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established in 2016 (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for"
W18-6401,E14-1047,1,0.89975,"Missing"
W18-6401,W18-6407,1,0.888041,"ns Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered pa"
W18-6401,W18-6410,0,0.0609282,"ions, organized into 35 teams are listed in Table 2 and detailed in the rest of this section. Each system did not necessarily appear in all translation tasks. We also included 39 online MT systems (originating from 5 services), which we anonymized as ONLINE -A,B,F,G. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations. 2.3.1 A ALTO (Grönroos et al., 2018) Aalto participated in the constrained condition of the multi-lingual subtrack, with a single system trained to translate from English to both Finnish 3 http://www.yandex.com/ Estonian Research Council institutional research grant IUT20-56: “Computational models of the Estonian Language” 4 5 As of Fall 2011, the proceedings of the European Parliament are no longer translated into all official languages. 273 Europarl Parallel Corpus German ↔ English Czech ↔ English Finnish ↔ English Estonian ↔ English Sentences 1,920,209 646,605 1,926,114 652,944 Words 50,486,398 53,008,851 14,946,399 17,376,43"
W18-6401,D18-1045,0,0.0609466,"Missing"
W18-6401,W11-2123,0,0.0087119,"t sets. The second is a Marian (Junczys-Dowmunt et al., 2018) system ensembling 5 Univ. Edinburgh “bi-deep” and 6 transformer models all trained on the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated da"
W18-6401,E17-3017,1,0.773822,"eriment, right-toleft reranking does not help. Another focus is 277 (SMT) submission for the Finnish morphology test suite (Burlot et al., 2018). given to data filtering through rules, translation model and language model including parallel data and monolingual data. The language model is based the Transformer architecture as well. The final system is trained with four different seeds and mixed data. 2.3.8 2.3.9 JHU (Koehn et al., 2018a) The JHU systems are the result of two relatively independent efforts on German–English language directions and Russian–English, using the Marian and Sockeye (Hieber et al., 2017) neural machine translation toolkits, respectively. The novel contributions are iterative back-translation (for German) and fine-tuning on test sets from prior years (for both languages). HY (Raganato et al., 2018; Hurskainen and Tiedemann, 2017) The University of Helsinki (HY) submitted four systems: HY-AH, HY-NMT, HY-NMT-2 STEP and HY-SMT. 2.3.10 JUCBNMT (Mahata et al., 2018) JUCBNMT is an encoder-decoder sequence-tosequence NMT model with character level encoding. The submission uses preprocessing like tokenization, truecasing and corpus cleaning. Both encoder and decoder use a single LSTM"
W18-6401,P17-4012,0,0.0266435,"ted paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered parallel and filtered back-translated monolingual data. The main contribution is a novel cross-lingual Morfessor (Virpioja et al., 2013) segmentation using cognates extracted from the parallel data. The aim is to improve the consistency of the morphological segmentation. Aalto decode using an ensemble of 3 (et) or 8 (fi) models. 2.3.4 2.3.2 2.3.5 AFRL The CUNI-KOCMI submission focuses on the low-resource language neural machine translation (NMT). The final submission uses a method of transfer learning: the model is pretrained on a related high-resource language (her"
W18-6401,W18-6413,0,0.01703,"he constrained systems, however, the data, taking into account its relatively large size, was not factored. T ENCENT (Wang et al., 2018a) T ENCENT- ENSEMBLE (called TenTrans) is an improved NMT system on Transformer based on self-attention mechanism. In addition to the basic settings of Transformer training, T ENCENTENSEMBLE uses multi-model fusion techniques, multiple features reranking, different segmentation models and joint learning. Additionally, data selection strategies were adopted to fine-tune the trained system, achieving a stable performance improvement. An additional system paper (Hu et al., 2018) describes a non-primary submission. 2.3.29 TILDE 2.3.30 U BIQUS The U BIQUS -NMT system is probably developed by the Ubiqus company (www.ubiqus.com). No further information is available. 2.3.31 UCAM (Stahlberg et al., 2018) UCAM is a generalization of previous work (de Gispert et al., 2017) to multiple architectures. It is a system combination of two Transformer-like models, a recurrent model, a convolutional model, and a phrase-based SMT system. The output is probably dominated by the Transformer, and to some extend by the SMT system. (Pinnis et al., 2018) submitted four systems: TILDE - C -"
W18-6401,W18-6416,1,0.791378,"Missing"
W18-6401,W17-4730,0,0.0118952,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6417,1,0.821949,"as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We held 1 2 http://www.statmt.org/wmt18/ 272 http://statmt.org/wmt18/results.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 272–303 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64028 tions are also available for interactive visualization and comparison of diff"
W18-6401,W17-4706,0,0.0132191,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6430,0,0.299738,"of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Part"
W18-6401,W18-6427,0,0.0533796,"Missing"
W18-6401,W18-6428,0,0.0966755,"D U NISOUND U NSUP TARTU Institution Aalto University (Grönroos et al., 2018) Air Force Research Laboratory (Gwinnup et al., 2018) Alibaba Group (Deng et al., 2018) Charles University (Kocmi et al., 2018) Charles University (Popel, 2018) Facebook AI Research (Edunov et al., 2018) Global Tone Communication Technology (Bei et al., 2018) University of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Ca"
W18-6401,W18-6431,0,0.0938702,"nications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implement"
W18-6401,W16-2326,0,0.0278975,"Missing"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5401,D18-1274,0,0.107749,"ntence-level task. Their setup is similar to ETRI’s, but they pretrain a BiLSTM encoder to predict words in the target conditioned on the source. Then, a regressor is fed the concatenation of each encoded word vector in the target with the embeddings of its neighbours and a mismatch feature indicating the difference between the prediction score of the target word and the highest one in the vocabulary. 5.4 Unbabel Unbabel participated in Tasks 1 and 2 for all language pairs. Their submissions were built upon the OpenKiwi framework: they combined linear, neural, and predictor-estimator systems (Chollampatt and Ng, 2018) with new transfer learning approaches using BERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) pre-trained models. They proposed new ensemble techniques for word and sentence-level predictions. For Task 2, they combined a predictor-estimator for wordlevel predictions with a simple technique for converting word labels into document-level predictions. 5.5 UTartu UTartu participated in the sentence-level track of task 1 and in task 3. They combined BERT (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2018) embeddings to train a regression neural network model. The output objecti"
W19-5401,N19-1423,0,0.330155,"y describe their strategies and which sub-tasks they participated in. 5.1 MIPT MIPT only participated in the word-level ENDE task. They used a BiLSTM, BERT and a baseline hand designed-feature extractor to generate word representations, followed by Conditional Random Fields (CRF) to output token labels. Their BiLISTM did not have any pretraining, unlike BERT, and combined the source and target vectors using a global attention mechanism. Their submitted runs combining the baseline features with the BiLSTM and with BERT. 5.2 ETRI ETRI participated in Task 1 only. They pretrained bilingual BERT (Devlin et al., 2019) models (one for EN-RU and another for EN-DE), and then finetuned them to predict all the outputs for each language pair, using different output weight matrices for each subtask (predicting source tags, target word tags, target gap tags, and the HTER score). Training the same model for both subtasks effectively enhanced the amount of training data. 5.3 CMU CMU participated only in the sentence-level task. Their setup is similar to ETRI’s, but they pretrain a BiLSTM encoder to predict words in the target conditioned on the source. Then, a regressor is fed the concatenation of each encoded word"
W19-5401,N13-1073,0,0.0598994,"ing and another in the end of the sentence. Words correctly aligned with the source are tagged as OK, and BAD otherwise. If one or more words are missing in the translation, the gap where they should have been is tagged as BAD, and OK otherwise. As in previous years, in order to obtain word level labels, first both the machine translated sentence and the source sentence are aligned with the post-edited version. Machine translation and post-edited pairs are aligned using the TERCOM tool (https://github. com/jhclark/tercom);2 source and postedited use the IBM Model 2 alignments from fast align (Dyer et al., 2013). Target word and gap labels Target tokens originating from insertion or substitution errors were labeled as BAD (i.e., tokens absent in the postedit sentence), and all other tokens were labeled as OK. Similarly to last year, we interleave these target word labels with gap labels: gaps were labeled as BAD in the presence of one or more deletion errors (i.e., a word from the source missing in the translation) and OK otherwise. Source word labels For each token in the postedited sentence deleted or substituted in the machine translated text, the corresponding aligned 1 This is true for tasks 1 a"
W19-5401,C18-1266,0,0.0919623,"ion task data, and to train the estimator, the WMT 2019 sentence level QE task data. 5.9 DCU DCU submitted two unsupervised metrics to task 3, both based on the IBM1 word alignment model. The main idea is to align the source and hypothesis using a model trained on a parallel corpus, and then use the average alignment strength (average word pair probabilities) as the metric. The varieties and other details are described in (Popovi´c et al., 2011). 5.10 USFD The two Sheffield submissions to the task 3 are based on the BiRNN sentence-level QE model from the deepQuest toolkit for neural-based QE (Ive et al., 2018). The BiRNN model uses two bi-directional recurrent neural networks (RNNs) as encoders to learn the representation of a ¡source,translation¿ sentence pair. The two encoders are trained independently from each other, before being combined as the weighted sum of the two sentence representations, using an attention mechanism. The first variant of our submission, ’USFD’, is a BiRNN model trained on Direct Assessment data from WMT’18. In this setting, the DA score is used as a sentence-level quality label. The second variant, ’USFD-TL’, is a BiRNN model previously trained on submissions to the WMT"
W19-5401,P19-3020,1,0.848097,"Missing"
W19-5401,W19-5358,0,0.0834842,"t variant of our submission, ’USFD’, is a BiRNN model trained on Direct Assessment data from WMT’18. In this setting, the DA score is used as a sentence-level quality label. The second variant, ’USFD-TL’, is a BiRNN model previously trained on submissions to the WMT News task from 2011 to 2017, with sent-BLEU as a quality label. We only considered the best performing submission, as well as one of the worst performing one. The model is then adapted to the downstream task of predicting DA score, using a transfer learning and fine-tuning approach. 5.11 NRC-CNRC The submissions from NRC-CNRC (kiu Lo, 2019) included two metrics submitted to task 3. They constitute a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. They use BERT (Devlin et al., 2019) and semantic role-labelling as additional sources of information. 6 Results The results for Task 1 are shown in Tables 4, 5, 6 and 7. Systems are ranked according to their F1 on the target side. The evaluation scripts are available at https://github. com/deep-spin/qe-evaluation. We computed the statistical significance of the results, and considered as"
W19-5401,P15-4020,0,0.123226,"ps inside an error annotation are given BAD tags, and all others are given OK. Then, we train the same wordlevel estimator as in the baseline for Task 1. At test time, for the fine-grained subtask, we group consecutive BAD tags produced by the word-level baseline in a single error annotation and always give it severity major (the most common in the training data). As such, the baseline only produces error annotations with a single error span. For the MQM score, we consider the ratio of bad tags to the document size: nbad (3) n This simple baseline contrasts with last year, which used QuEst++ (Specia et al., 2015), a QE tool based on training an SVR on features extracted from the data. We found that the new baseline performed better than QuEst++ on the development data, and thus adopted it as the official baseline. MQM = 1 − have an HTER of 0, this is not always the case. When preprocessing the shared task data, word-level tags were determined in a case-sensitive fashion, while sentence-level scores were not. The same issue also happened last year, but unfortunately we only noticed it after releasing the training data for this edition. 4.4 QE as a Metric The QE as a metric task included two baselines,"
W19-5401,W19-5410,1,0.843237,"Missing"
W19-5401,C00-2137,0,0.130863,"or languages with different levels of available resources. They use BERT (Devlin et al., 2019) and semantic role-labelling as additional sources of information. 6 Results The results for Task 1 are shown in Tables 4, 5, 6 and 7. Systems are ranked according to their F1 on the target side. The evaluation scripts are available at https://github. com/deep-spin/qe-evaluation. We computed the statistical significance of the results, and considered as winning systems the ones which had significantly better scores than all the rest with p &lt; 0.05. For the word-level task, we used randomization tests (Yeh, 2000) with Bonferroni correction6 (Abdi, 2007); for Pearson correlation scores used in the sentence-level and MQM scoring tasks, we used William’s test7 . In the word-level task, there is a big gap between Unbabel’s winning submission and ETRI’s, which in turn also had significantly better results than MIPT and BOUN. Unfortunately, we cannot do a direct comparison with last year’s results, since i) we now evaluate a single score for target words and gaps, which were evaluated separately before, and ii) only two systems submitted results for source words last year. The newly proposed metric, MCC, is"
W19-5402,W16-2301,1,0.88964,"Missing"
W19-5402,2004.iwslt-evaluation.1,0,0.144319,"Missing"
W19-5402,P15-2026,1,0.860832,"correction of phrase-based MT output, this year only neural MT (NMT) output has been considered. However, this year’s campaign allows both for a fair assessment of the progress in APE technology and for tests in more challenging conditions. On one side, reusing the same test English-German set used last year, the evaluation framework allows us for a direct comparison with the last year’s outcomes at least on one language. On the other side, dealing with a Introduction MT Automatic Post-Editing (APE) is the task of automatically correcting errors in a machinetranslated text. As pointed out by (Chatterjee et al., 2015), from the application point of view the 11 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 11–28 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics to beat the baseline (16.16 TER, 76.20 BLEU). These results confirm one of the main findings of previous rounds (Bojar et al., 2017; Chatterjee et al., 2018a): improving high-quality MT output remains the biggest challenge for APE. This motivates further research on precise and conservative solutions able to mimic human behaviour by performing only the m"
W19-5402,W18-1804,1,0.783788,"t least on one language. On the other side, dealing with a Introduction MT Automatic Post-Editing (APE) is the task of automatically correcting errors in a machinetranslated text. As pointed out by (Chatterjee et al., 2015), from the application point of view the 11 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 11–28 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics to beat the baseline (16.16 TER, 76.20 BLEU). These results confirm one of the main findings of previous rounds (Bojar et al., 2017; Chatterjee et al., 2018a): improving high-quality MT output remains the biggest challenge for APE. This motivates further research on precise and conservative solutions able to mimic human behaviour by performing only the minimum amount of edit operations needed. difficult language like Russian and only with highquality NMT output, also this round presented participants with an increased level of difficulty with respect to the past. Seven teams participated in the EnglishGerman task, submitting 18 runs in total. Two teams participated in the English-Russian task, submitting 2 runs each. Similar to last year, all the"
W19-5402,P07-2045,0,0.00969238,"Missing"
W19-5402,P19-1292,0,0.0959054,"llowed by ii) a Transformer decoder block, but without masking, for self-attention on the MT segment, which effectively acts as second encoder combining source and MT output, and iii) feeds this representation into a final decoder block generating the post-edit. The intuition behind the proposed architecture is to generate better representations via both self- and cross- attention and to further facilitate the learning capacity of the feedforward layer in the decoder block. Also in this case, model training takes advantage of the eSCAPE synthetic data (Negri et al., 2018). Unbabel. Following (Correia and Martins, 2019), Unbabel’s submission (English-German subtask) adapts BERT (Devlin et al., 2018) to the APE task with an encoder-decoder framework. The system consists in a BERT encoder initialised with the pretrained model’s weights and a BERT decoder initialised analogously, where the multi-head context attention is initialised with University of Sheffield & Imperial College London. IC USFD’s submission (English-German subtask) is based on the dual-source Transformer model (Junczys-Dowmunt and Grundkiewicz, 2018), which was re-implemented in the 17 Tensor2Tensor (Vaswani et al., 2017) toolkit. The model wa"
W19-5402,W04-3250,0,0.015291,"s are discussed in Section 6. 2.2 2.3 Baseline In continuity with the previous rounds, the official baseline results were the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “do-nothing” system that leaves all the test targets unmodified. Baseline results, the same shown in Table 2, are also reported in Tables 4 and 5 for comparison with participants’ submissions.8 For each submitted run, the statistical significance of performance differences with respect to the baseline was calculated with the bootstrap test (Koehn, 2004). Evaluation metrics System performance was evaluated both by means of automatic metrics and manually. Automatic metrics were used to compute the distance between automatic and human post-edits of the machine-translated sentences present in the test sets. To this aim, TER and BLEU (case-sensitive) were respectively used as primary and secondary evaluation metrics. Systems were ranked based on the average TER calculated on the test set by using the TERcom6 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package7 available 3 Participants"
W19-5402,W17-4713,1,0.837945,"“light post-edit” (minimal edits are required), and “heavy post-edit” (a large number of edits are required. At training time, the instances are labelled based on the TER computed between the MT output and its post-edited version, with the boundary between light and heavy post-edit set to TER=0.4 based on the findings reported in (Turchi et al., 2013; Turchi et al., 2014). At test time, tokens are predicted with two approaches. One is based on a classifier obtained by fine-tuning BERT (Devlin et al., 2018) on the in-domain data. The other approach exploits a retrieval-based method similar to (Farajian et al., 2017): given a query containing the source and the MT output to be post-edited, it: i) retrieves similar triplets from the training data, ii) ranks them based on the sentence level BLEU score between the MT output and the post-edit, and iii) creates the token based on the TER computed between the MT output and the post-edit of the most similar triplet. The backbone architecture is the multi-source extension of Transformer (Vaswani et al., 2017) described in (Tebbifakhr et al., 2018), which is trained both on the task data and on the available artificial corpora. ADAPT Centre & Dublin City Universit"
W19-5402,W19-5412,0,0.115706,"Missing"
W19-5402,W13-2305,0,0.108674,"ny. Each evaluator had experience with the evaluation task through previous work using the same evaluation platform in order to be familiar with the user interface and its functionalities. A screenshot of the evaluation interface is presented in Figure 4. We measure post-editing quality using sourcebased direct assessment (src-DA), as implemented in Appraise (Federmann, 2012). Scores are collected as x ∈ [0, 100], focusing on adequacy (and not fluency, which previous WMT evaluation campaigns have found to be highly correlated with adequacy direct assessment results). The original DA approach (Graham et al., 2013; Graham et al., 2014) is reference-based and, thus, needs to be adapted for use in our paraphrase assessment and translation scoring scenarios. Of course, this makes translation evaluation more difficult, as we require bilingual annotators. Src-DA has previously been used, e.g., in (Cettolo et al., 2017; Bojar et al., 2018). Direct assessment initializes mental context for annotators by asking a priming question. The user interface shows two sentences: For both the subtasks, the differences in system’s behaviour are indeed barely visible, mainly due to the fact that, in most of the cases, the"
W19-5402,W19-5413,0,0.167547,"llowed by ii) a Transformer decoder block, but without masking, for self-attention on the MT segment, which effectively acts as second encoder combining source and MT output, and iii) feeds this representation into a final decoder block generating the post-edit. The intuition behind the proposed architecture is to generate better representations via both self- and cross- attention and to further facilitate the learning capacity of the feedforward layer in the decoder block. Also in this case, model training takes advantage of the eSCAPE synthetic data (Negri et al., 2018). Unbabel. Following (Correia and Martins, 2019), Unbabel’s submission (English-German subtask) adapts BERT (Devlin et al., 2018) to the APE task with an encoder-decoder framework. The system consists in a BERT encoder initialised with the pretrained model’s weights and a BERT decoder initialised analogously, where the multi-head context attention is initialised with University of Sheffield & Imperial College London. IC USFD’s submission (English-German subtask) is based on the dual-source Transformer model (Junczys-Dowmunt and Grundkiewicz, 2018), which was re-implemented in the 17 Tensor2Tensor (Vaswani et al., 2017) toolkit. The model wa"
W19-5402,E14-1047,0,0.0536777,"Missing"
W19-5402,L18-1004,1,0.577374,"a phrase-based APE system. In most of the cases, participants experimented with the Transformer architecture (Vaswani et al., 2017), either directly or by adapting it to the task (see Section 3). Another common trait of the submitted systems is the reliance on the consolidated multi-source approach (Zoph and Knight, 2016; Libovick´y et al., 2016), which is able to exploit information from both the MT output to be corrected and the corresponding source sentence. The third aspect common to all submissions is the exploitation of synthetic data, either those provided together with the task data (Negri et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016) or similar, domain-specific resources created ad-hoc by participants. 2 Task description In continuity with all the previous rounds of the APE task, participants were provided with training and development data consisting of (source, target, human post-edit) triplets, and were asked to return automatic post-edits for a test set of unseen (source, target) pairs. 2.1 Data This year, the evaluation was performed on two language pairs, English-German and EnglishRussian. For both the subtasks, data were selected from the Information Technology (IT) domain."
W19-5402,W19-5414,0,0.144563,"Missing"
W19-5402,W16-2378,0,0.494675,"system. In most of the cases, participants experimented with the Transformer architecture (Vaswani et al., 2017), either directly or by adapting it to the task (see Section 3). Another common trait of the submitted systems is the reliance on the consolidated multi-source approach (Zoph and Knight, 2016; Libovick´y et al., 2016), which is able to exploit information from both the MT output to be corrected and the corresponding source sentence. The third aspect common to all submissions is the exploitation of synthetic data, either those provided together with the task data (Negri et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016) or similar, domain-specific resources created ad-hoc by participants. 2 Task description In continuity with all the previous rounds of the APE task, participants were provided with training and development data consisting of (source, target, human post-edit) triplets, and were asked to return automatic post-edits for a test set of unseen (source, target) pairs. 2.1 Data This year, the evaluation was performed on two language pairs, English-German and EnglishRussian. For both the subtasks, data were selected from the Information Technology (IT) domain. As emerged from the previous evaluations,"
W19-5402,P02-1040,0,0.109642,"hort segments (menu commands, short messages, etc.), is shared with the English-Russian Quality Estimation shared task.3 The training and development set respectively contain 15,089 and 1,000 triplets, while the test set comprises 1,023 instances. For this language pair, the eSCAPE corpus has been extended to provide participants with additional Table 2 provides a view of the data from a task difficulty standpoint. For each dataset released in the five rounds of the APE task, it shows the repetition rate of SRC, TGT and PE elements, as well as the TER (Snover et al., 2006) and the BLEU score (Papineni et al., 2002) of the TGT elements (i.e. the original target translations). The repetition rate measures the repetitiveness inside a text by looking at the rate of non-singleton n-gram types (n=1...4) and combining them using the geometric mean. Larger values indicate a higher text repetitiveness and, as discussed in (Bojar et al., 2016; Bojar et al., 2017; Chatterjee et al., 2018a), suggest a higher chance of learning from the training set correction patterns that are applicable also to the test set. In the previous rounds of the task, we considered the large differences in repetitiveness across the datase"
W19-5402,E17-2025,0,0.0173592,"der that computes the attenFondazione Bruno Kessler. Also FBK participated in both the subtasks. Their submissions focus on mitigating the “over-correction” problem in APE, that is the systems’ tendency to rephrase and correct MT output that is already acceptable, thus producing translations that will be penalized by evaluation against human post-edits. Following (Chatterjee et al., 2018b), the underlying idea is that over-correction can be prevented by inform9 https://marian-nmt.github.io/ 16 the self-attention weights. Additionally, source embeddings, target embeddings and projection layer (Press and Wolf, 2017) are shared, as well as the self-attention weights of the encoder and decoder. The system exploits BERT training schedule with streams A and B: the encoder receives as input both the source and the MT output separated by the special symbol “[SEP]”, assigning to the first “A” segment embeddings and to the latter “B” segment embeddings. Regarding the BERT decoder, they use just the post-edit with “B” segment embeddings. In addition, as the NMT system has a strong in-domain performance, a conservativeness factor to avoid over-correction is explored. Similarly to (Junczys-Dowmunt and Grundkiewicz,"
W19-5402,P17-4012,0,0.032833,"ens and the topic induced via LDA clustering (Blei et al., 2003). The statistical APE models, which are based on Moses (Koehn et al., 2007), were trained to explore the idea of interleaving different MT technologies to improve NMT output quality. All the models are built by taking advantage of both the released training material and the provided artificial data (Negri et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). Pohang University of Science and Technology. POSTECH’s system (English-German subtask) is a multi-source model that extends the Transformer implementation of the OpenNMT-py (Klein et al., 2017) library. It includes: i) a joint encoder that is able to generate joint representations reflecting the relationship between two input sources (SRC, TGT) with optional future masking to mimic the general decoding process of machine translation systems, and ii) two types of multi-source attention layers in the decoder that computes the attenFondazione Bruno Kessler. Also FBK participated in both the subtasks. Their submissions focus on mitigating the “over-correction” problem in APE, that is the systems’ tendency to rephrase and correct MT output that is already acceptable, thus producing trans"
W19-5402,P16-1162,0,0.0407548,"e; • The target (TGT) is a tokenized German/Russian translation of the source, which was produced by a black-box system unknown to participants. For both the languages, translations were obtained from neural MT systems:1 this implies that their overall quality is generally high, making the task harder compared to previous rounds, which 1 For English-German, the NMT system was trained with generic and in-domain parallel training data using the attentional encoder-decoder architecture (Bahdanau et al., 2014) implemented in the Nematus toolkit (Sennrich et al., 2017). We used byte-pair encoding (Sennrich et al., 2016) for vocabulary reduction, mini-batches of 100, word embeddings of 500 dimensions, and gated recurrent unit layers of 1,024 units. Optimization was done using Adam and by re-shuffling the training set at each epoch. For English-Russian, the NMT system used was the Microsoft Translator production system, which was trained with both generic and in-domain parallel training data. The newly proposed English-Russian task represents a more challenging evaluation scenario, mainly due to the higher quality of the NMT output to be corrected. In this case, even the best submission (16.59 TER, 75.27 TER)"
W19-5402,E17-3017,0,0.0655502,"Missing"
W19-5402,W19-5415,0,0.0287686,"Missing"
W19-5402,W19-5417,0,0.230128,"Missing"
W19-5402,D18-1049,0,0.0590858,"Missing"
W19-5402,N07-1064,0,0.0460932,"metrics. Systems were ranked based on the average TER calculated on the test set by using the TERcom6 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package7 available 3 Participants Seven teams submitted a total of 18 runs for the English-German subtask. Two of them participated also in the English-Russian subtask by subgeneric/multi-bleu.perl 8 In addition to the do-nothing baseline, in the first three rounds of the task we also compared systems’ performance with a re-implementation of the phrase-based approach firstly proposed by Simard et al. (2007), which represented the common backbone of APE systems before the spread of neural solutions. As shown in (Bojar et al., 2016; Bojar et al., 2017), the steady progress of neural APE technology has made the phrase-based solution not competitive with current methods reducing the importance of having it as an additional term of comparison. In 2018, we hence opted for considering only one baseline. 6 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ 7 15 ID ADAPT DCU FBK POSTECH UDS UNBABEL USAAR DFKI IC USFD Participating team ADAPT Centre & Dub"
W19-5402,2006.amta-papers.25,0,0.23352,"This material, which mainly consists of short segments (menu commands, short messages, etc.), is shared with the English-Russian Quality Estimation shared task.3 The training and development set respectively contain 15,089 and 1,000 triplets, while the test set comprises 1,023 instances. For this language pair, the eSCAPE corpus has been extended to provide participants with additional Table 2 provides a view of the data from a task difficulty standpoint. For each dataset released in the five rounds of the APE task, it shows the repetition rate of SRC, TGT and PE elements, as well as the TER (Snover et al., 2006) and the BLEU score (Papineni et al., 2002) of the TGT elements (i.e. the original target translations). The repetition rate measures the repetitiveness inside a text by looking at the rate of non-singleton n-gram types (n=1...4) and combining them using the geometric mean. Larger values indicate a higher text repetitiveness and, as discussed in (Bojar et al., 2016; Bojar et al., 2017; Chatterjee et al., 2018a), suggest a higher chance of learning from the training set correction patterns that are applicable also to the test set. In the previous rounds of the task, we considered the large diff"
W19-5402,W18-6471,1,0.90356,"ning BERT (Devlin et al., 2018) on the in-domain data. The other approach exploits a retrieval-based method similar to (Farajian et al., 2017): given a query containing the source and the MT output to be post-edited, it: i) retrieves similar triplets from the training data, ii) ranks them based on the sentence level BLEU score between the MT output and the post-edit, and iii) creates the token based on the TER computed between the MT output and the post-edit of the most similar triplet. The backbone architecture is the multi-source extension of Transformer (Vaswani et al., 2017) described in (Tebbifakhr et al., 2018), which is trained both on the task data and on the available artificial corpora. ADAPT Centre & Dublin City University. The ADAPT DCU team participated in both the subtasks proposed this year. Their submissions pursue two main objectives, namely: i) investigating the effect of adding extra information in the form of prefix tokens in a neural APE system; and ii) assessing whether an SMT-based approach can be effective for post-editing NMT output. The neural APE system exploits a multisource approach based on Marian-NMT.9 Training data were augmented with two types of extra context tokens that"
W19-5402,W19-5416,1,0.827672,"Missing"
W19-5402,W13-2231,1,0.834987,"ected amount of corrections needed. The proposed solution is based on prepending a special token to the source text and the MT output, so to indicate the required amount of post-editing. Three different tokens are used, namely “no post-edit” (no edits are required), “light post-edit” (minimal edits are required), and “heavy post-edit” (a large number of edits are required. At training time, the instances are labelled based on the TER computed between the MT output and its post-edited version, with the boundary between light and heavy post-edit set to TER=0.4 based on the findings reported in (Turchi et al., 2013; Turchi et al., 2014). At test time, tokens are predicted with two approaches. One is based on a classifier obtained by fine-tuning BERT (Devlin et al., 2018) on the in-domain data. The other approach exploits a retrieval-based method similar to (Farajian et al., 2017): given a query containing the source and the MT output to be post-edited, it: i) retrieves similar triplets from the training data, ii) ranks them based on the sentence level BLEU score between the MT output and the post-edit, and iii) creates the token based on the TER computed between the MT output and the post-edit of the mo"
