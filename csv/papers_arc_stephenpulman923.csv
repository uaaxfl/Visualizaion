2021.nlp4convai-1.22,Using Pause Information for More Accurate Entity Recognition,2021,-1,-1,6,0,2984,sahas dendukuri,Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI,0,"Entity tags in human-machine dialog are integral to natural language understanding (NLU) tasks in conversational assistants. However, current systems struggle to accurately parse spoken queries with the typical use of text input alone, and often fail to understand the user intent. Previous work in linguistics has identified a cross-language tendency for longer speech pauses surrounding nouns as compared to verbs. We demonstrate that the linguistic observation on pauses can be used to improve accuracy in machine-learnt language understanding tasks. Analysis of pauses in French and English utterances from a commercial voice assistant shows the statistically significant difference in pause duration around multi-token entity span boundaries compared to within entity spans. Additionally, in contrast to text-based NLU, we apply pause duration to enrich contextual embeddings to improve shallow parsing of entities. Results show that our proposed novel embeddings improve the relative error rate by up to 8{\%} consistently across three domains for French, without any added annotation or alignment costs to the parser."
2021.naacl-main.44,Open-Domain Question Answering Goes Conversational via Question Rewriting,2021,-1,-1,5,0,3364,raviteja anantha,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement."
2021.naacl-industry.25,Noise Robust Named Entity Understanding for Voice Assistants,2021,-1,-1,6,0,4748,deepak muralidharan,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers,0,"Named Entity Recognition (NER) and Entity Linking (EL) play an essential role in voice assistant interaction, but are challenging due to the special difficulties associated with spoken user queries. In this paper, we propose a novel architecture that jointly solves the NER and EL tasks by combining them in a joint reranking module. We show that our proposed framework improves NER accuracy by up to 3.13{\%} and EL accuracy by up to 3.6{\%} in F1 score. The features used also lead to better accuracies in other natural language understanding tasks, such as domain classification and semantic parsing."
W14-1806,The pragmatics of margin comments: An empirical study,2014,28,0,2,0.9965,38733,debora field,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper describes the design and rationale behind a classification scheme for English margin comments. The schemexe2x80x99s design was informed by pragmatics and pedagogy theory, and by observations made from a corpus of 24,387 margin comments from assessed university assignments. The purpose of the scheme is to computationally explore content and form relationships between margin comments and the passages to which they point. The process of designing the scheme resulted in the conclusion that margin comments require more work to understand than utterances do, and that they are more prone to being misunderstood."
W13-3513,Separating Disambiguation from Composition in Distributional Semantics,2013,24,27,3,1,20638,dimitri kartsaklis,Proceedings of the Seventeenth Conference on Computational Natural Language Learning,0,"Most compositional-distributional models of meaning are based on ambiguous vector representations, where all the senses of a word are fused into the same vector. This paper provides evidence that the addition of a vector disambiguation step prior to the actual composition would be beneficial to the whole process, producing better composite representations. Furthermore, we relate this issue with the current evaluation practice, showing that disambiguation-based tasks cannot reliably assess the quality of composition. Using a word sense disambiguation scheme based on the generic procedure of Schutze (1998), we first provide a proof of concept for the necessity of separating disambiguation from composition. Then we demonstrate the benefits of an xe2x80x9cunambiguousxe2x80x9d system on a composition-only task."
R13-1036,Did {I} really mean that? Applying automatic summarisation techniques to formative feedback,2013,23,4,2,0.9965,38733,debora field,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"(2)This paper reports on an application that delivers automated formative feedback designed to help university students improve their assignments. (3)The aim of the system is to improve the confidence and skills of the user by promoting selfdirected learning through metacognition. (4)The system focuses on the content of an essay by using automatic summarisation techniques, automatic structure recognition, diagrams, animations, and interactive exercises that promote reflection. (15)The system is currently undergoing initial exploratory rounds of testing by ex-student volunteers and will be the subject of two full-scale empirical evaluations starting in September 2013. (1)The main claims of this paper are the application and adaptation of graph-based key word and key sentence ranking methods for a novel purpose, and ensuing observations concerning the suitability of two different centrality algorithms for the purposes of key word extraction."
S12-1011,Learning Semantics and Selectional Preference of Adjective-Noun Pairs,2012,15,1,4,0,28945,karl hermann,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We investigate the semantic relationship between a noun and its adjectival modifiers. We introduce a class of probabilistic models that enable us to to simultaneously capture both the semantic similarity of nouns and modifiers, and adjective-noun selectional preference. Through a combination of novel and existing evaluations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning."
S12-1021,An Unsupervised Ranking Model for Noun-Noun Compositionality,2012,24,13,3,0,28945,karl hermann,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We propose an unsupervised system that learns continuous degrees of lexicality for noun-noun compounds, beating a strong baseline on several tasks. We demonstrate that the distributional representations of compounds and their parts can be used to learn a fine-grained representation of semantic contribution. Finally, we argue such a representation captures compositionality better than the current status-quo which treats compositionality as a binary classification problem."
C12-2054,A Unified Sentence Space for Categorical Distributional-Compositional Semantics: Theory and Experiments,2012,18,50,3,1,20638,dimitri kartsaklis,Proceedings of {COLING} 2012: Posters,0,"This short paper summarizes a faithful implementation of the categorical framework of Coecke et al. (2010), the aim of which is to provide compositionality in distributional models of lexical semantics. Based on Frobenius Algebras, our method enable us to (1) have a unifying meaning space for phrases and sentences of different structure and word vectors, (2) stay faithful to the linguistic types suggested by the underlying type-logic, and (3) perform the concrete computations in lower dimensions by reducing the space complexity. We experiment with two different parameters of the model and apply the setting to a verb disambiguation and a term/definition classification task with promising results."
W11-0114,Concrete Sentence Spaces for Compositional Distributional Models of Meaning,2011,18,56,5,0,28946,edward grefenstette,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"Coecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributional semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the morphism corresponding to the grammatical structure of the sentence in the category of finite dimensional vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map, by constructing a corpus-based vector space for the type of sentence. Our construction method is based on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This enables us to compare meanings of sentences by simply taking the inner product of their vectors."
W11-0149,Semantic Relatedness from Automatically Generated Semantic Networks,2011,11,10,2,0,44475,piaramona wojtinnek,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"We introduce a novel approach to measuring semantic relatedness of terms based on an automatically generated, large-scale semantic network. We present promising first results that indicate potential competitiveness with approaches based on manually created resources."
J11-3006,"Book Reviews: Automated Grammatical Error Detection for Language Learners by Claudia Leacock, {M}artin Chodorow, {M}ichael Gamon, and Joel Tetreault",2011,-1,-1,1,1,2989,stephen pulman,Computational Linguistics,0,None
W10-2707,How Was Your Day?,2010,8,19,1,1,2989,stephen pulman,Proceedings of the 2010 Workshop on Companionable Dialogue Systems,0,"We describe a 'How was your day?' (HWYD) Companion whose purpose is to establish a comforting and supportive relationship with a user via a conversation on a variety of work-related topics. The system has several fairly novel features aimed at increasing the naturalness of the interaction: a rapid 'short loop' response primed by the results of acoustic emotion analysis, and an 'interruption manager', enabling the user to interrupt lengthy or apparently inappropriate system responses, prompting a replanning of behaviour on the part of the system. The 'long loop' also takes into account the emotional state of the user, but using more conventional dialogue management and planning techniques. We describe the architecture and components of the implemented prototype HWYD system."
W09-3949,Simultaneous Dialogue Act Segmentation and Labelling using Lexical and Syntactic Features,2009,7,1,2,0,27875,ramon granell,Proceedings of the {SIGDIAL} 2009 Conference,0,"Segmentation of utterances and annotation as dialogue acts can be helpful for several modules of dialogue systems. In this work, we study a statistical machine learning model to perform these tasks simultaneously using lexical features and incorporating deterministic syntactic restrictions. There is a slight improvement in both segmentation and labelling due to these restrictions."
W09-3951,Unsupervised Classification of Dialogue Acts using a {D}irichlet Process Mixture Model,2009,18,29,3,0,45112,nigel crook,Proceedings of the {SIGDIAL} 2009 Conference,0,"In recent years Dialogue Acts have become a popular means of modelling the communicative intentions of human and machine utterances in many modern dialogue systems. Many of these systems rely heavily on the availability of dialogue corpora that have been annotated with Dialogue Act labels. The manual annotation of dialogue corpora is both tedious and expensive. Consequently, there is a growing interest in unsupervised systems that are capable of automating the annotation process. This paper investigates the use of a Dirichlet Process Mixture Model as a means of clustering dialogue utterances in an unsupervised manner. These clusters can then be analysed in terms of the possible Dialogue Acts that they might represent. The results presented here are from the application of the Dirichlet Process Mixture Model to the Dihana corpus."
R09-1048,Multi-entity Sentiment Scoring,2009,18,30,2,1,21661,karo moilanen,Proceedings of the International Conference {RANLP}-2009,0,"We present a compositional framework for modelling entity-level sentiment (sub)contexts, and demonstrate how holistic multi-entity polarity scoring emerges as a by-product of compositional sentiment parsing. A data set of five annotatorsxe2x80x99 multi-entity judgements is presented, and a human ceiling is established for the challenging new task. The accuracy of an initial implementation, which includes both supervised learning and heuristic distance-based scoring methods, is 5.6xe2x88xbc6.8 points below the human ceiling amongst sentences and 8.1xe2x88xbc8.7 points amongst phrases."
W08-2212,Automatic Fine-Grained Semantic Classification for Domain Adaptation,2008,25,6,2,1,3114,maria liakata,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"Assigning arguments of verbs to different semantic classes ('semantic typing'), or alternatively, checking the 'selectional restrictions' of predicates, is a fundamental component of many natural language processing tasks. However, a common experience has been that general purpose semantic classes, such as those encoded in resources like WordNet, or handcrafted subject-specific ontologies, are seldom quite right when it comes to analysing texts from a particular domain. In this paper we describe a method of automatically deriving fine-grained, domain-specific semantic classes of arguments while simultaneously clustering verbs into semantically meaningful groups: the first step in verb sense induction. We show that in a small pilot study on new examples from the same domain we are able to achieve almost perfect recall and reasonably high precision in the semantic typing of verb arguments in these texts."
P08-2028,"The Good, the Bad, and the Unknown: Morphosyllabic Sentiment Tagging of Unseen Words",2008,11,22,2,1,21661,karo moilanen,"Proceedings of ACL-08: HLT, Short Papers",0,"The omnipresence of unknown words is a problem that any NLP component needs to address in some form. While there exist many established techniques for dealing with unknown words in the realm of POS-tagging, for example, guessing unknown words' semantic properties is a less-explored area with greater challenges. In this paper, we study the semantic field of sentiment and propose five methods for assigning prior sentiment polarities to unknown words based on known sentiment carriers. Tested on 2000 cases, the methods mirror human judgements closely in three- and two-way polarity classification tasks, and reach accuracies above 63% and 81%, respectively."
C08-2012,{ILP}-based Conceptual Analysis for {C}hinese {NP}s,2008,11,0,2,0,48715,paul ji,Coling 2008: Companion volume: Posters,0,"In this paper, we explore a conceptual resource for Chinese nominal phrases, which allows multi-dependency and distinction between dependency and the corresponding exact relation. We also provide an ILP-based method to learn mapping rules from training data, and use the rules to analyze new nominal phrases."
C08-1022,A Classifier-Based Approach to Preposition and Determiner Error Correction in {L}2 {E}nglish,2008,8,105,2,0,48725,rachele felice,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper, we present an approach to the automatic identification and correction of preposition and determiner errors in non-native (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing."
W07-1607,Automatically Acquiring Models of Preposition Use,2007,6,40,2,0,48725,rachele felice,Proceedings of the Fourth {ACL}-{SIGSEM} Workshop on Prepositions,0,"This paper proposes a machine-learning based approach to predict accurately, given a syntactic and semantic context, which preposition is most likely to occur in that context. Each occurrence of a preposition in an English corpus has its context represented by a vector containing 307 features. The vectors are processed by a voted perceptron algorithm to learn associations between contexts and prepositions. In preliminary tests, we can associate contexts and prepositions with a success rate of up to 84.5%."
J07-1008,"Book Reviews: Flexible Semantics for Reinterpretation Phenomena, by Markus Egg",2007,0,0,1,1,2989,stephen pulman,Computational Linguistics,0,None
W06-1662,Sentence ordering with manifold-based classification in multi-document summarization,2006,13,18,2,0,48715,paul ji,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy. The classification is based on the manifold structure underlying sentences, addressing the problem of limited labeled data. The historical ordering helps to ensure topic continuity and avoid topic bias. Experiments demonstrate that the method is effective."
W05-0202,Automatic Short Answer Marking,2005,13,84,1,1,2989,stephen pulman,Proceedings of the Second Workshop on Building Educational Applications Using {NLP},0,"Our aim is to investigate computational linguistics (CL) techniques in marking short free text responses automatically. Successful automatic marking of free text answers would seem to presuppose an advanced level of performance in automated natural language understanding. However, recent advances in CL techniques have opened up the possibility of being able to automate the marking of free text responses typed into a computer without having to create systems that fully understand the answers. This paper describes some of the techniques we have tried so far vis-a-vis this problem with results, discussion and description of the main issues encountered.1"
C04-1027,Learning theories from text,2004,11,7,2,1,3114,maria liakata,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper we describe a method of automatically learning domain theories from parsed corpora of sentences from the relevant domain and use FSA techniques for the graphical representation of such a theory. By a 'domain theory' we mean a collection of facts and generalisations or rules which capture what commonly happens (or does not happen) in some domain of interest. As language users, we implicitly draw on such theories in various disambiguation tasks, such as anaphora resolution and prepositional phrase attachment, and formal encodings of domain theories can be used for this purpose in natural language processing. They may also be objects of interest in their own right, that is, as the output of a knowledge discovery process. The approach is generizable to different domains provided it is possible to get logical forms for the text in the domain."
C02-1105,From Trees to Predicate-argument Structures,2002,10,19,2,1,3114,maria liakata,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"The Penn Treebank encodes valuable information such as grammatical function, semantic roles, and identification of traces. The addition of such information was intended to facilitate the process of predicate-argument extraction. However, even with the enriched annotation this task is far from trivial and, to our knowledge, no complete set of predicate argument structures derived from the Treebank exists. Our paper describes a method for retrieving predicate-argument structures that circumvents the complexity of the tree structures in the corpus, while employing few template rules. Our system operates on a flattened, morphologically enriched version of the corpus. This flattened representation allows access to all levels of the tree simultaneously and thus enables the detection of the main sentence constituents by means of simple template rules. A small number of rules apply to identify the head words of each constituent and the latter fill in the constituent templates, to build the logical forms representative of the predicate argument structure. The system is robust in the face of incomplete syntactic coverage."
W00-0740,Incorporating Linguistics Constraints into Inductive Logic Programming,2000,15,15,2,0,54274,james cussens,Fourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop,0,"We report work on effectively incorporating linguistic knowledge into grammar induction. We use a highly interactive bottom-up inductive logic programming (ILP) algorithm to learn 'missing' grammar rules from an incomplete grammar. Using linguistic constraints on, for example, head features and gap threading, reduces the search space to such an extent that, in the small-scale experiments reported here, we can generate and store all candidate grammar rules together with information about their coverage and linguistic properties. This allows an appealingly simple and controlled method for generating linguistically plausible grammar rules. Starting from a base of highly specific rules, we apply least general generalisation and inverse resolution to generate more general rules. Induced rules are ordered, for example by coverage, for easy inspection by the user and at any point, the user can commit to a hypothesised rule and add it to the grammar. Related work in ILP and computational linguistics is discussed."
J00-4002,Bidirectional Contextual Resolution,2000,47,7,1,1,2989,stephen pulman,Computational Linguistics,0,"This paper describes a formalism and implementation for the interpretation and generation of sentences containing context-dependent constructs like determiners, pronouns, focus, and ellipsis. A variant of quasi-logical form is used as an underspecified meaning representation, related to resolved logical forms via conditional equivalences. These equivalences define the interpretation of contextually dependent constructs with respect to a given context. Higher-order unification and abduction are used in relating expressions to contexts. The conditional equivalences can be used unchanged in both the interpretation and the generation direction."
J99-3007,Book Reviews: Type-Logical Semantics,1999,0,0,1,1,2989,stephen pulman,Computational Linguistics,0,None
J96-3001,Unification Encodings of Grammatical Notations,1996,27,16,1,1,2989,stephen pulman,Computational Linguistics,0,"This paper describes various techniques for enriching unification-based grammatical formalisms with notational devices that are compiled into categories and rules of a standard unification grammar. This enables grammarians to avail themselves of apparently richer notations that allow for the succinct and relatively elegant expression of grammatical facts, while still allowing for efficient processing for the analysis or synthesis of sentences using such grammars."
C96-1077,Compiling a Partition-Based Two-Level Formalism,1996,11,28,3,0,56046,edmund grimleyevans,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"This paper describes an algorithm for the compilation of a two (or more) level orthographic or phonological rule notation into finite state transducers. The notation is an alternative to the standard one deriving from Koskenniemi's work: it is believed to have some practical descriptive advantages, and is quite widely used, but has a different interpretation. Efficient interpreters exist for the notation, but until now it has not been clear how to compile to equivalent automata in a transparent way. The present paper shows how to do this, using some of the conceptual tools provided by Kaplan and Kay's regular relations calculus."
J87-3008,A Computational Framework for Lexical Description,1987,9,23,2,0,47097,graeme ritchie,Computational Linguistics,0,"To achieve generality, natural language parsers require dictionaries which handle lexical information in a linguistically motivated but computationally viable manner. Various rule formalisms are presented which process orthographic effects, word structure, and lexicai redundancy in a manner which allows the statement of linguistic generalisations with a clear computational interpretation. A compact description of a medium-sized subset of the English lexicon can be stated using these formalisms. The proposed mechanisms have been implemented and tested, but require to be refined further if they are to be regarded as an interesting linguistic theory."
