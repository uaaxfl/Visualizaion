2014.eamt-1.21,W08-0330,0,0.0168787,"(Graesser et al., 2004). Different from lexical cohesion features, LSA features are able to find correlations among different words, which are not repetitions and may not be synonyms, but are instead related (as given by co-occurrence patterns). 3.3 Pseudo-references Pseudo-references are translations produced by other MT systems than the system we want to predict the quality for. They are used as references to evaluate the output of the MT system of interest. They have also been used for other purposes, e.g., to fulfil the lack of human references available in reference-based MT evaluation (Albrecht and Hwa, 2008) and automatic summary evaluation (Louis and Nenkova, 2013). The application we are interested in, originally proposed in (Soricut and Echihabi, 2010), is to generate features for 104 QE. In this scenario, reference-based evaluation metrics (such as BLEU) are computed between the MT system output and the pseudo-references and used to train quality prediction models. Soricut and Echihabi (2010) discussed the importance of the pseudo-references being generated by MT system(s) which are as different as possible from the MT system of interest, and preferably of much better quality. This should ens"
2014.eamt-1.21,W11-2104,0,0.0441326,"rful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) conduct a feature analysis, at sentence-level, on a number of datasets and show that the BLEU-based pseudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features, but feature selection analysis showed that linguistic features were among the best performing ones. 3 Features for document-level QE QE is traditionally done at sen"
2014.eamt-1.21,P11-1022,0,0.0238051,"l features for documentlevel prediction. The authors claim that a pseudoreferences-based feature (based in BLEU) is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) conduct a feature analysis, at sentence-level, on a number of datasets and show that the BLEU-based pseudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features, but feature selection a"
2014.eamt-1.21,P02-1040,0,0.0906257,"nts with features extracted from pseudo-references led to the best results, but the discursive features also proved promising. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). This task is usually addressed with machine learning models trained on datasets composed of source texts, their machine translations, and a quality label assigned by humans or by an automatic metric (e.g.: BLEU (Papineni et al., 2002)). A common use of quality predictions is the estimation of postc 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 101 editing effort in order to decide whether to translate a text from scratch or post-edit its machine translation. Another use is the ranking of translations in order to select the best text from multiple MT systems. Feature engineering is an important component in QE. Although several feature sets have already been explored, most approaches focus on sentence-level quality prediction, with sentencelevel f"
2014.eamt-1.21,W05-0909,0,0.0456588,"ted work Work related to this research includes documentlevel MT evaluation metrics, QE features, and QE prediction, as well as work focusing on other linguistic features, and work using pseudoreferences. Wong and Kit (2012) use lexical cohesion metrics for MT evaluation at document-level. Lexical cohesion relates to word choices, captured in their work by reiteration and collocation. Words and stems were used for reiteration, and synonyms, near-synonyms and superordinates, for collocations. These metrics are integrated with traditional metrics like BLEU, TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The highest correlation against human assessments was found for the combination of METEOR and the discursive features. Rubino et al. (2013) explore topic model features for QE at sentence-level. Latent Dirichlet Allocation is used to model the topics in two ways: a bilingual view, where the bilingual corpus is concatenated at sentence-level to build a single model with two languages; and a polylingual view, where one topic model is built for each language. While the topics models are generated with information 102 from the entire corpus, the features are extracted at sentence-level. These ar"
2014.eamt-1.21,W11-1001,0,0.168984,"Missing"
2014.eamt-1.21,C04-1046,0,0.569591,"tend to explore very short contexts within sentence boundaries. In addition, most work has targeted sentence-level quality prediction. In this paper, we focus on documentlevel QE using novel discursive features, as well as exploiting pseudo-reference translations. Experiments with features extracted from pseudo-references led to the best results, but the discursive features also proved promising. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). This task is usually addressed with machine learning models trained on datasets composed of source texts, their machine translations, and a quality label assigned by humans or by an automatic metric (e.g.: BLEU (Papineni et al., 2002)). A common use of quality predictions is the estimation of postc 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 101 editing effort in order to decide whether to translate a text from scratch or post-edit its machine translation. Another use is"
2014.eamt-1.21,W12-3156,0,0.088453,"y Rubino et al. (2013) considered discourse-related information by studying topic model features for sentencelevel prediction. Soricut and Echihabi (2010) explored document-level quality prediction, but they did not use explicit discourse information, e.g. information to capture text cohesion or coherence. In this paper we focus on document-level features and document-level prediction. We believe that judgements on translation quality depend on units longer than just a given sentence, taking into account discourse phenomena for lexical choice, consistency, style and connectives, among others (Carpuat and Simard, 2012). This is particularly important in MT evaluation contexts, since most MT systems, and in particular statistical MT (SMT) systems, process sentences one by one, in isolation. Our hypothesis is that features that capture discourse phenomena can improve document-level prediction. We consider two families of features that have been successfully applied in reference-based MT evaluation (Wong and Kit, 2012) and readability assessment (Graesser et al., 2004). In terms of applications, document-level QE is very important in scenarios where the entire text needs to be used/published without posteditio"
2014.eamt-1.21,W12-3110,1,0.831406,"seudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features, but feature selection analysis showed that linguistic features were among the best performing ones. 3 Features for document-level QE QE is traditionally done at sentence-level. This happens mainly because the majority of MT systems translate texts at this level. Evaluating sentences instead of documents can be useful for many scenarios, e.g., post-editing effort prediction. 1 http://www.statmt.org/wmt12/ However, some linguistic phenomena can only be captured by considering"
2014.eamt-1.21,2011.eamt-1.32,0,0.141422,"r QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) conduct a feature analysis, at sentence-level, on a number of datasets and show that the BLEU-based pseudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features, but feature selection analysis showed that linguistic features were among the best performing ones. 3 Features for document-level QE QE is traditionally done at sentence-level. This"
2014.eamt-1.21,P07-2045,0,0.00722867,"Missing"
2014.eamt-1.21,J13-2002,0,0.0200886,"features, LSA features are able to find correlations among different words, which are not repetitions and may not be synonyms, but are instead related (as given by co-occurrence patterns). 3.3 Pseudo-references Pseudo-references are translations produced by other MT systems than the system we want to predict the quality for. They are used as references to evaluate the output of the MT system of interest. They have also been used for other purposes, e.g., to fulfil the lack of human references available in reference-based MT evaluation (Albrecht and Hwa, 2008) and automatic summary evaluation (Louis and Nenkova, 2013). The application we are interested in, originally proposed in (Soricut and Echihabi, 2010), is to generate features for 104 QE. In this scenario, reference-based evaluation metrics (such as BLEU) are computed between the MT system output and the pseudo-references and used to train quality prediction models. Soricut and Echihabi (2010) discussed the importance of the pseudo-references being generated by MT system(s) which are as different as possible from the MT system of interest, and preferably of much better quality. This should ensure that string similarity features (like BLEU) indicate mo"
2014.eamt-1.21,2013.mtsummit-posters.13,1,0.915273,"Missing"
2014.eamt-1.21,2013.mtsummit-papers.21,1,0.925431,"applications, document-level QE is very important in scenarios where the entire text needs to be used/published without postedition. Soricut and Echihabi (2010) and Soricut and Narsale (2012) explored a feature based on pseudo-references for document-level QE. Pseudo-references are translations produced by one or more external MT systems, which are different from the one producing the translations we want to predict the quality for. These are used as references against which the output of the MT system of interest can be compared using standard metrics such as BLEU. Soricut et al. (2012) and Shah et al. (2013) explored pseudo-references for sentence-level QE. In both cases, features based on pseudo-references led to significant improvements in prediction accuracy. Here we also use pseudoreferences for document-level QE, with a number of string similarity metrics to produce documentlevel scores as features, which are arguably more reliable than sentence-level scores, particularly for metrics like BLEU. In the remainder of this paper, Section 2 presents related work. Section 3 introduces the documentlevel QE features we propose. Section 4 describes the experimental setup of this work. Section 5 prese"
2014.eamt-1.21,2006.amta-papers.25,0,0.796928,"on 5 presents the results. 2 Related work Work related to this research includes documentlevel MT evaluation metrics, QE features, and QE prediction, as well as work focusing on other linguistic features, and work using pseudoreferences. Wong and Kit (2012) use lexical cohesion metrics for MT evaluation at document-level. Lexical cohesion relates to word choices, captured in their work by reiteration and collocation. Words and stems were used for reiteration, and synonyms, near-synonyms and superordinates, for collocations. These metrics are integrated with traditional metrics like BLEU, TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The highest correlation against human assessments was found for the combination of METEOR and the discursive features. Rubino et al. (2013) explore topic model features for QE at sentence-level. Latent Dirichlet Allocation is used to model the topics in two ways: a bilingual view, where the bilingual corpus is concatenated at sentence-level to build a single model with two languages; and a polylingual view, where one topic model is built for each language. While the topics models are generated with information 102 from the entire corpus, the features are"
2014.eamt-1.21,P10-1063,0,0.624,"Missing"
2014.eamt-1.21,W12-3121,0,0.409559,"MT evaluation contexts, since most MT systems, and in particular statistical MT (SMT) systems, process sentences one by one, in isolation. Our hypothesis is that features that capture discourse phenomena can improve document-level prediction. We consider two families of features that have been successfully applied in reference-based MT evaluation (Wong and Kit, 2012) and readability assessment (Graesser et al., 2004). In terms of applications, document-level QE is very important in scenarios where the entire text needs to be used/published without postedition. Soricut and Echihabi (2010) and Soricut and Narsale (2012) explored a feature based on pseudo-references for document-level QE. Pseudo-references are translations produced by one or more external MT systems, which are different from the one producing the translations we want to predict the quality for. These are used as references against which the output of the MT system of interest can be compared using standard metrics such as BLEU. Soricut et al. (2012) and Shah et al. (2013) explored pseudo-references for sentence-level QE. In both cases, features based on pseudo-references led to significant improvements in prediction accuracy. Here we also use"
2014.eamt-1.21,W12-3118,0,0.0581428,"Missing"
2014.eamt-1.21,2009.eamt-1.5,1,0.876274,"y short contexts within sentence boundaries. In addition, most work has targeted sentence-level quality prediction. In this paper, we focus on documentlevel QE using novel discursive features, as well as exploiting pseudo-reference translations. Experiments with features extracted from pseudo-references led to the best results, but the discursive features also proved promising. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). This task is usually addressed with machine learning models trained on datasets composed of source texts, their machine translations, and a quality label assigned by humans or by an automatic metric (e.g.: BLEU (Papineni et al., 2002)). A common use of quality predictions is the estimation of postc 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 101 editing effort in order to decide whether to translate a text from scratch or post-edit its machine translation. Another use is the ranking of transl"
2014.eamt-1.21,P13-4014,1,0.901413,"Missing"
2014.eamt-1.21,D12-1097,0,0.469149,"ents on translation quality depend on units longer than just a given sentence, taking into account discourse phenomena for lexical choice, consistency, style and connectives, among others (Carpuat and Simard, 2012). This is particularly important in MT evaluation contexts, since most MT systems, and in particular statistical MT (SMT) systems, process sentences one by one, in isolation. Our hypothesis is that features that capture discourse phenomena can improve document-level prediction. We consider two families of features that have been successfully applied in reference-based MT evaluation (Wong and Kit, 2012) and readability assessment (Graesser et al., 2004). In terms of applications, document-level QE is very important in scenarios where the entire text needs to be used/published without postedition. Soricut and Echihabi (2010) and Soricut and Narsale (2012) explored a feature based on pseudo-references for document-level QE. Pseudo-references are translations produced by one or more external MT systems, which are different from the one producing the translations we want to predict the quality for. These are used as references against which the output of the MT system of interest can be compared"
2014.eamt-1.21,P10-1062,0,0.0194684,"egation of sentence-level features for documentlevel prediction. The authors claim that a pseudoreferences-based feature (based in BLEU) is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) conduct a feature analysis, at sentence-level, on a number of datasets and show that the BLEU-based pseudoreference feature contributes the most to prediction performance. In terms of other types of linguistic features for QE, Xiong et al. (2010) and Bach et al. (2011) propose features for word-level QE and show that these improve over the state-of-the-art results. At sentence-level, Avramidis et al. (2011), Hardmeier (2011) and Almaghout and Specia (2013) consider syntactic features, achieving better results compared to competitive feature sets. Pighin and M`arquez (2011) obtain improvements over strong baselines from exploiting semantic role labelling to score MT outputs at sentence-level. Felice and Specia (2012) introduce several linguistic features for QE at sentence-level. These did not show improvement over shallower features,"
2015.eamt-1.17,2012.eamt-1.33,1,0.857982,"Missing"
2015.eamt-1.17,C04-1046,0,0.11191,"h levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between"
2015.eamt-1.17,W12-3156,0,0.102075,"sing automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an"
2015.eamt-1.17,P14-1065,0,0.0530227,"Missing"
2015.eamt-1.17,P10-1064,1,0.914336,"Missing"
2015.eamt-1.17,P14-2047,0,0.0164777,"the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use automatic evaluation m"
2015.eamt-1.17,W13-3303,0,0.0260842,"ssues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use a"
2015.eamt-1.17,P02-1040,0,0.0918502,"and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall"
2015.eamt-1.17,P09-2004,0,0.0370577,"omly selected from the full corpus. For PE1 and PE2, only source (English) paragraphs with 3-8 sentences were selected (filter SNUMBER) to ensure that there is enough information beyond sentence-level to be evaluated and make the task feasible for the annotators. These paragraphs were further filtered to select those with cohesive devices. Cohesive devices are linguistic units that play a role in establishing cohesion between clauses, sentences or paragraphs (Halliday and Hasan, 1976). Pronouns and discourse connectives are examples of such devices. A list of pronouns and the connectives from Pitler and Nenkova (2009) was considered for that. Finally, paragraphs were ranked according to the number of cohesive devices they contain and the top 200 paragraphs were selected (filter C-DEV). Table 3 shows the statistics of the initial corpus and the resulting selection after each filter. FULL CORPUS S-NUMBER C-DEV Number of Paragraphs 1, 215 394 200 Number of Cohesive devices 6, 488 3, 329 2, 338 Table 3: WMT13 English source corpus. For the PE1 experiment, the paragraphs in CDEV were randomised. Then, sets containing seven paragraphs each were created. For each set, the sentences of its paragraphs were also ran"
2015.eamt-1.17,potet-etal-2012-collection,0,0.122026,"Missing"
2015.eamt-1.17,2014.eamt-1.21,1,0.877315,"onsider more information than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT"
2015.eamt-1.17,2006.amta-papers.25,0,0.41877,"s on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the final version – HTER (Snover et al., 2006). c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required. This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it. The quality of a document is often seen as some form of aggregation of the quality"
2015.eamt-1.17,P10-1063,0,0.608392,"han sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a document that is fit for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes)"
2015.eamt-1.17,2009.eamt-1.5,1,0.876811,"ht crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the"
2015.eamt-1.17,D14-1025,0,0.053694,"Missing"
2015.eamt-1.17,C14-2028,0,\N,Missing
2015.eamt-1.17,W13-2201,1,\N,Missing
2020.aacl-main.91,J08-4004,0,0.016822,"analysis over the dataset to better understand its properties. Inter7 https://www.ibge.gov.br/en/home-eng. html 917 Table 1: Annotators demographic information. 8 6 count 3.2 Categories Male Female 4 2 0 18 19 20 21 22 23 24 25 26 27 29 30 35 37 age Figure 1: Annotators age distribution. α LGBTQ+phobia Insult Xenophobia Misogyny Obscene Racism 0.68 0.56 0.57 0.52 0.49 0.48 Mean 0.55 Table 2: Krippendorff ’s α for each label. annotator agreement is calculated in terms of Krippendorf ’s α (Table 2), since α is robust to multiple annotators, different degrees of disagreement and, missing values (Artstein and Poesio, 2008). The LGBTQ+phobia class shows the highest agreement, which may indicate that comments in this class have a more distinctive lexicon than other classes. The lowest agreement is seem in obscene and racism classes. Besides, we observed in the annotations many cases in which some examples were labelled as separate classes, although they intend o fdp do filho dela nao parava de tocar auto pra c*****o [...] Ann 1 Ann 2 Ann 3 Insult None Obscene her sob son did not stop to play loud as f**k [...] ˜ VC NAO ˜ E´ FELIZ PQ NAO QUER [...] VAI SE F***R IRMAO Obscene Insult Insult [...] f**k you brother yo"
2020.aacl-main.91,W19-3510,0,0.0195581,"large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data. Although there is some work on this topic for other languages – e.g. Arabic (Mubarak et al., 2017) and German (Wiegand et al., 2018) –, most of the resources and studies available are for English (Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019b).3 For Portuguese, only two previous works are available (Fortuna et al., 2019; de Pelle and Moreira, 2017) and their datasets are considerably small, mainly when compared to resources available for English. We present ToLD-Br (Toxic Language Dataset for Brazilian Portuguese), a new dataset with Twitter posts in the Brazilian Portuguese language.4 1 https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge/ overview 2 This is also similar to the usage of offensive comments in OffensEval (Zampieri et al., 2019b, 2020). 3 A large list of resources is available at http:// hatespeechdata.com. 4 It is important to distinguish the language variant, since 914 Pro"
2020.aacl-main.91,S19-2007,0,0.0330831,"lassification, where each different type of toxicity is automatically classified. This is a considerably harder problem than binary classification, where BERTbased models do not outperform the baseline. Section 2 presents an overview of relevant previous work. Section 3 shows details about the ToLDBr dataset. Material and methods are presented in Section 4, whilst results are discussed in Section 5. Finally, Section 6 shows a final discussion and future work. 2 Related Work Although multiple researchers have addressed the topic of hate speech (e.g. Waseem and Hovy (2016), Chung et al. (2019), Basile et al. (2019)), we focus the literature review on previous work related to toxic comments detection, the topic of our paper. Due to space constraints, we only describe papers that create and use Twitter-based datasets and/or focus on the Brazilian Portuguese language. English Davidson et al. (2017) present a dataset with around 25K tweets annotated by crowdworkers as containing hate, offensive language, or neither. They build a feature-based classifier with TF-IDF transformation over n-grams, part-ofspeech information, sentiment analysis, network information (e.g., number of replies), among other features."
2020.aacl-main.91,P19-1271,0,0.0199259,"nt with multi-label classification, where each different type of toxicity is automatically classified. This is a considerably harder problem than binary classification, where BERTbased models do not outperform the baseline. Section 2 presents an overview of relevant previous work. Section 3 shows details about the ToLDBr dataset. Material and methods are presented in Section 4, whilst results are discussed in Section 5. Finally, Section 6 shows a final discussion and future work. 2 Related Work Although multiple researchers have addressed the topic of hate speech (e.g. Waseem and Hovy (2016), Chung et al. (2019), Basile et al. (2019)), we focus the literature review on previous work related to toxic comments detection, the topic of our paper. Due to space constraints, we only describe papers that create and use Twitter-based datasets and/or focus on the Brazilian Portuguese language. English Davidson et al. (2017) present a dataset with around 25K tweets annotated by crowdworkers as containing hate, offensive language, or neither. They build a feature-based classifier with TF-IDF transformation over n-grams, part-ofspeech information, sentiment analysis, network information (e.g., number of replies),"
2020.aacl-main.91,N19-1423,0,0.0943486,"Missing"
2020.aacl-main.91,W18-3504,0,0.0673115,"Missing"
2020.aacl-main.91,W17-3008,0,0.0284079,"eful for platform’s moderators and to select content for specific users (e.g. children). Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms; (ii) there is a large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data. Although there is some work on this topic for other languages – e.g. Arabic (Mubarak et al., 2017) and German (Wiegand et al., 2018) –, most of the resources and studies available are for English (Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019b).3 For Portuguese, only two previous works are available (Fortuna et al., 2019; de Pelle and Moreira, 2017) and their datasets are considerably small, mainly when compared to resources available for English. We present ToLD-Br (Toxic Language Dataset for Brazilian Portuguese), a new dataset with Twitter posts in the Brazilian Portuguese language.4 1 https://www.kaggle.com/c/ jigsaw-toxic-c"
2020.aacl-main.91,W19-3512,0,0.0327688,"Missing"
2020.aacl-main.91,N16-2013,0,0.205525,"oxicity. 1 Introduction Social media can be a powerful tool that enables virtual human interactions, connecting people and enhancing businesses’ presence. On the other hand, since users feel somehow protected under their virtual identities, social media has also become a platform for hate speech and use of toxic language. Although hate speech is a crime in most countries, identifying cases in social media is not an easy task, given the massive amounts of data posted every day. Therefore, automatic approaches for detecting online hate speech have received significant attention in recent years (Waseem and Hovy, 2016; Davidson et al., 2017; Zampieri et al., 2019b). In this paper, we focus on the analysis and automatic detection of toxic comments. Our definition of toxic is similar to the one used by the Jigsaw competition,1 where comments containing insults and obscene language are also considered, besides hate speech.2 Systems capable of automatically identifying toxic comments are useful for platform’s moderators and to select content for specific users (e.g. children). Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explic"
2020.aacl-main.91,N19-1144,0,0.218122,"powerful tool that enables virtual human interactions, connecting people and enhancing businesses’ presence. On the other hand, since users feel somehow protected under their virtual identities, social media has also become a platform for hate speech and use of toxic language. Although hate speech is a crime in most countries, identifying cases in social media is not an easy task, given the massive amounts of data posted every day. Therefore, automatic approaches for detecting online hate speech have received significant attention in recent years (Waseem and Hovy, 2016; Davidson et al., 2017; Zampieri et al., 2019b). In this paper, we focus on the analysis and automatic detection of toxic comments. Our definition of toxic is similar to the one used by the Jigsaw competition,1 where comments containing insults and obscene language are also considered, besides hate speech.2 Systems capable of automatically identifying toxic comments are useful for platform’s moderators and to select content for specific users (e.g. children). Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms;"
2020.aacl-main.91,S19-2010,0,0.385439,"powerful tool that enables virtual human interactions, connecting people and enhancing businesses’ presence. On the other hand, since users feel somehow protected under their virtual identities, social media has also become a platform for hate speech and use of toxic language. Although hate speech is a crime in most countries, identifying cases in social media is not an easy task, given the massive amounts of data posted every day. Therefore, automatic approaches for detecting online hate speech have received significant attention in recent years (Waseem and Hovy, 2016; Davidson et al., 2017; Zampieri et al., 2019b). In this paper, we focus on the analysis and automatic detection of toxic comments. Our definition of toxic is similar to the one used by the Jigsaw competition,1 where comments containing insults and obscene language are also considered, besides hate speech.2 Systems capable of automatically identifying toxic comments are useful for platform’s moderators and to select content for specific users (e.g. children). Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms;"
2020.aacl-main.91,2020.semeval-1.188,0,0.0939088,"Missing"
2020.aacl-main.92,S17-2080,0,0.0455375,"Missing"
2020.aacl-main.92,S19-2193,0,0.0343048,"Missing"
2020.aacl-main.92,S17-2081,0,0.0640373,"Missing"
2020.aacl-main.92,C18-1284,1,0.836142,"ammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but also two minority classes (deny and support) are the most important to classify well. Based on prior research on imbalanced datasets in areas other that NLP (e.g. Yijing et al. (201"
2020.aacl-main.92,S17-2082,0,0.0669704,"Missing"
2020.aacl-main.92,S19-2192,0,0.0304242,"Missing"
2020.aacl-main.92,S19-2196,0,0.0209881,"Missing"
2020.aacl-main.92,S17-2084,0,0.056951,"Missing"
2020.aacl-main.92,S19-2197,0,0.026033,"Missing"
2020.aacl-main.92,S19-2147,1,0.891312,"Missing"
2020.aacl-main.92,S19-2195,0,0.0290328,"Missing"
2020.aacl-main.92,C18-1288,0,0.017542,"emEval 2016 task 6 (Mohammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but also two minority classes (deny and support) are the most important to classify well. Based on prior research on imbalanced datasets in areas other that NLP (e."
2020.aacl-main.92,P19-1498,0,0.0145585,"not only an imbalanced, multi-class problem, but it also has classes with different importance. This is different from standard stance classification tasks (e.g. SemEval 2016 task 6 (Mohammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but"
2020.aacl-main.92,S19-2148,0,0.0840143,"tion tasks (e.g. SemEval 2016 task 6 (Mohammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but also two minority classes (deny and support) are the most important to classify well. Based on prior research on imbalanced datasets in"
2020.aacl-main.92,P19-1113,0,0.0451716,"tion tasks (e.g. SemEval 2016 task 6 (Mohammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but also two minority classes (deny and support) are the most important to classify well. Based on prior research on imbalanced datasets in"
2020.aacl-main.92,S19-2194,0,0.0262947,"Missing"
2020.aacl-main.92,S16-1003,0,0.0800136,"Missing"
2020.aacl-main.92,S17-2087,0,0.0655003,"Missing"
2020.aacl-main.92,S17-2085,0,0.0557501,"Missing"
2020.aacl-main.92,S17-2086,0,0.0477101,"Missing"
2020.aacl-main.92,S19-2191,0,0.0254569,"Missing"
2020.acl-main.424,I17-1030,1,0.922576,"Missing"
2020.acl-main.424,D19-3009,1,0.86211,"ms added, deleted and kept by the simplification system. It does so by comparing the output of the simplification model to multiple references and the original sentence, using both precision and recall. BLEU has shown positive correlation with human judgements of grammaticality and meaning preserˇ vation (Stajner et al., 2014; Wubben et al., 2012; Xu et al., 2016), while SARI has high correlation with judgements of simplicity gain (Xu et al., 2016). In our experiments, we used the implementations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syn"
2020.acl-main.424,2020.cl-1.4,1,0.846425,"experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e."
2020.acl-main.424,W19-5301,0,0.0593597,"Missing"
2020.acl-main.424,D18-1289,0,0.0383172,"Missing"
2020.acl-main.424,C96-2183,0,0.822657,"T, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this va"
2020.acl-main.424,P11-2117,0,0.0921157,"ese studies, it can be argued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or"
2020.acl-main.424,W14-1215,0,0.239238,"ivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automat"
2020.acl-main.424,W13-2305,0,0.114853,"to submit their level of agreement (0: Strongly disagree, 100: Strongly agree) with the following statements: 1. The Simplified sentence adequately expresses the meaning of the Original, perhaps omitting the least important information. 2. The Simplified sentence is fluent, there are no grammatical errors. 3. The Simplified sentence is easier to understand than the Original sentence. Using continuous scales when crowdsourcing human evaluations is common practice in Machine Translation (Bojar et al., 2018; Barrault et al., 2019), since it results in higher levels of interannotator consistency (Graham et al., 2013). The six sentence pairs for the Rating QT consisted of: • Three submissions to the Annotation QT, manually selected so that one contains splitting, one has a medium level of compression, and one contains grammatical and spelling mistakes. These allowed to check that the particular characteristics of each sentence pair affect the corresponding evaluation criteria. • One sentence pair from WikiLarge where the Original and the Simplification had no relation to each other. This served to check the attention level of the worker. All submitted ratings were manually reviewed to validate the quality"
2020.acl-main.424,N15-1022,0,0.0552503,"ued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or dropped and the rest"
2020.acl-main.424,W02-0109,0,0.0656209,"table) have been shown to be best indicators of word complexity (Paetzold and Specia, 2016). The ratio is then the value of this score on the simplification divided by that of the original sentence. In order to quantify the rewriting transformations, we computed several low-level features for all simplification instances using the tseval package (Martin et al., 2018): • Number of sentence splits: Corresponds to the difference between the number of sentences in the simplification and the number of sentences in the original sentence. In tseval, the number of sentences is calculated using NLTK (Loper and Bird, 2002). • Compression level: Number of characters in the simplification divided by the number of characters in the original sentence. • Dependency tree depth ratio: We compute the ratio of the depth of the dependency parse tree of the simplification relative to that of the original sentence. When a simplification is composed by more than one sentence, we choose the maximum depth of all dependency trees. Parsing is performed using spaCy.4 This feature serves as a proxy to measure improvements in structural simplicity. • Replace-only Levenshtein distance: Computed as the normalised character-level Lev"
2020.acl-main.424,W18-7005,1,0.877343,"Missing"
2020.acl-main.424,P14-1041,0,0.0247229,"mentations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syntax-based MT; NTSSARI (Nisioi et al., 2017), a neural sequence-tosequence model with a standard encoder-decoder architecture; and ACCESS (Martin et al., 2020), an encoder-decoder architecture conditioned on explicit attributes of sentence simplification. Collection of Human Ratings. We randomly chose 100 original sentences from ASSET and, for each of them, we sampled one system simplification. The automatic simplifications were selected so that the distribution of simplification transf"
2020.acl-main.424,P17-2014,0,0.10479,"scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syntax-based MT; NTSSARI (Nisioi et al., 2017), a neural sequence-tosequence model with a standard encoder-decoder architecture; and ACCESS (Martin et al., 2020), an encoder-decoder architecture conditioned on explicit attributes of sentence simplification. Collection of Human Ratings. We randomly chose 100 original sentences from ASSET and, for each of them, we sampled one system simplification. The automatic simplifications were selected so that the distribution of simplification transformations (e.g. sentence splitting, compression, paraphrases) would match that from human simplifications in ASSET. That was done so that we could obtain"
2020.acl-main.424,C16-1069,1,0.880195,"racteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the se"
2020.acl-main.424,P02-1040,0,0.116616,"of the Association for Computational Linguistics, pages 4668–4679 c July 5 - 10, 2020. 2020 Association for Computational Linguistics original sentence). Simplifications in ASSET were collected via crowdsourcing (§ 3), and encompass a variety of rewriting transformations (§ 4), which make them simpler than those in TurkCorpus and HSplit (§ 5), thus providing an additional suitable benchmark for comparing and evaluating automatic SS models. In addition, we study the applicability of standard metrics for evaluating SS using simplifications in ASSET as references (§ 6). We analyse whether BLEU (Papineni et al., 2002) or SARI (Xu et al., 2016) scores correlate with human judgements of fluency, adequacy and simplicity, and find that neither of the metrics shows a strong correlation with simplicity ratings. This motivates the need for developing better metrics for assessing SS when multiple rewriting transformations are performed. We make the following contributions: • A high quality large dataset for tuning and evaluation of SS models containing simplifications produced by applying multiple rewriting transformations.1 • An analysis of the characteristics of the dataset that turn it into a new suitable bench"
2020.acl-main.424,Q16-1005,0,0.027315,"mplification), we crowdsourced 15 human ratings on fluency (i.e. grammaticality), adequacy (i.e. meaning preservation) and simplicity, using the same worker selection criteria and HIT design of the Qualification Test as in § 5.1. 6.2 Inter-Annotator Agreement We followed the process suggested in (Graham et al., 2013). First, we normalised the scores of each rater by their individual mean and standard deviation, which helps eliminate individual judge preferences. Then, the normalised continuous scores were converted to five interval categories using equally spaced bins. After that, we followed Pavlick and Tetreault (2016) and computed quadratic weighted Cohen’s κ (Cohen, 1968) simulating two raters: for each sentence, we chose one worker’s rating as the category for annotator A, and selected the rounded average scores for the remaining workers as the category for annotator B. We then computed κ for this pair over the whole dataset. We repeated the process 1,000 times to compute the mean and variance of κ. The resulting values are: 0.687 ± 0.028 for Fluency, 0.686 ± 0.030 for Meaning and 0.628 ± 0.032 for Simplicity. All values point to a moderate level 4675 Metric References BLEU ASSET TurkCorpus ASSET TurkCor"
2020.acl-main.424,W14-1210,0,0.0497303,"litting the sentences. This prevents evaluating a model’s ability to perform a more diverse set of rewriting transformations when simplifying sentences. HSplit (Sulem et al., 2018a), on the other hand, provides simplifications involving only splitting for sentences in the test set of TurkCorpus. We build on TurkCorpus and HSplit by collecting a dataset that provides several manuallyproduced simplifications involving multiple types of rewriting transformations. 2.3 Crowdsourcing Manual Simplifications A few projects have been carried out to collect manual simplifications through crowdsourcing. Pellow and Eskenazi (2014a) built a corpus of everyday documents (e.g. driving test preparation materials), and analysed the feasibly of crowdsourcing their sentence-level simplifications. Of all the quality control measures taken, the most successful was providing a training session to workers, since it allowed to block spammers and those without the skills to perform the task. Additionally, they proposed to use workers’ self-reported confidence scores to flag submissions that could be discarded or reviewed. Later on, Pellow and Eskenazi (2014b) presented a preliminary study on producing simplifications through a col"
2020.acl-main.424,L18-1685,1,0.858597,"uld be discarded or reviewed. Later on, Pellow and Eskenazi (2014b) presented a preliminary study on producing simplifications through a collaborative process. Groups of four workers were assigned one sentence to simplify, and they had to discuss and agree on the process to perform it. Unfortunately, the data collected in these studies is no longer publicly available. Simplifications in TurkCorpus were also collected through crowdsourcing. Regarding the methodology followed, Xu et al. (2016) only report removing bad workers after manual check of their first several submissions. More recently, Scarton et al. (2018) used volunteers to collect simplifications for SimPA, a dataset with sentences from the Public Administration domain. One particular characteristic of the methodology followed is that lexical and syntactic simplifications were performed independently. 3 Creating ASSET We extended TurkCorpus (Xu et al., 2016) by using the same original sentences, but crowdsourced manual simplifications that encompass a richer set of rewriting transformations. Since TurkCorpus was adopted as the standard dataset for evaluating SS models, several system outputs on this data are already publicly available (Zhang"
2020.acl-main.424,D18-1081,0,0.370088,"Missing"
2020.acl-main.424,N18-1063,0,0.0727483,"Missing"
2020.acl-main.424,L18-1615,0,0.026282,"SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or dropped and the rest of the sentences left unchanged. More diverse simplifications are available in the Newsela corpus (Xu et al., 2015), a dataset of 1,130 news articles that were each manually simplified 4669 to up to 5 levels of simplicity. The parallel articles can be automatically aligned at the sentence level to train and test simplification models (Alvaˇ Manchego et al., 2017; Stajner et al., 2018). However, the Newsela corpus can only be accessed after signing a restrictive license that prevents publicly sharing train/test splits of the dataset, which impedes reproducibility. Evaluating models on automatically-aligned sentences is problematic. Even more so if only one (potentially noisy) reference simplification for each original sentence is available. With this concern in mind, Xu et al. (2016) collected the TurkCorpus, a dataset with 2,359 original sentences from EW, each with 8 manual reference simplifications. The dataset is divided into two subsets: 2,000 sentences for validation"
2020.acl-main.424,C10-1152,0,0.326813,"nces). From all these studies, it can be argued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with onl"
2020.acl-main.424,W14-1201,0,0.016775,"utputs: BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). BLEU is a precision-oriented metric that relies on the number of n-grams in the output that match n-grams in the references, independently of position. SARI measures improvement in the simplicity of a sentence based on the n-grams added, deleted and kept by the simplification system. It does so by comparing the output of the simplification model to multiple references and the original sentence, using both precision and recall. BLEU has shown positive correlation with human judgements of grammaticality and meaning preserˇ vation (Stajner et al., 2014; Wubben et al., 2012; Xu et al., 2016), while SARI has high correlation with judgements of simplicity gain (Xu et al., 2016). In our experiments, we used the implementations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse Sy"
2020.acl-main.424,P12-1107,0,0.313438,"Missing"
2020.acl-main.424,Q15-1021,0,0.263892,"syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this variety of transformations. For instance, TurkCorpus (Xu et al., 2016), a standard dataset for assessment in SS, contains simplifications produced mostly by lexical paraphrasing, while reference simplifications in HSplit (Sulem et al., 2018a) focus on splitting sentences. The Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying models’ capabilities to perform a broad range of simplification transformations. Even though most SS models are trained on simplification instances displaying several text transformations (e.g. WikiLarge (Zhang and Lapata, 2017)), we curr"
2020.acl-main.424,Q16-1029,0,0.180298,"ranco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this variety of transformations. For instance, TurkCorpus (Xu et al., 2016), a standard dataset for assessment in SS, contains simplifications produced mostly by lexical paraphrasing, while reference simplifications in HSplit (Sulem et al., 2018a) focus on splitting sentences. The Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying"
2020.acl-main.424,D17-1062,0,0.658579,"Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying models’ capabilities to perform a broad range of simplification transformations. Even though most SS models are trained on simplification instances displaying several text transformations (e.g. WikiLarge (Zhang and Lapata, 2017)), we currently do not measure their performance in more abstractive scenarios, i.e. cases with substantial modifications to the original sentences. In this paper we introduce ASSET (Abstractive Sentence Simplification Evaluation and Tuning), a new dataset for tuning and evaluation of automatic SS models. ASSET consists of 23,590 human simplifications associated with the 2,359 original sentences from TurkCorpus (10 simplifications per 4668 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4668–4679 c July 5 - 10, 2020. 2020 Association for Computati"
2020.acl-main.424,D18-1355,0,0.229799,"o collect simplifications for SimPA, a dataset with sentences from the Public Administration domain. One particular characteristic of the methodology followed is that lexical and syntactic simplifications were performed independently. 3 Creating ASSET We extended TurkCorpus (Xu et al., 2016) by using the same original sentences, but crowdsourced manual simplifications that encompass a richer set of rewriting transformations. Since TurkCorpus was adopted as the standard dataset for evaluating SS models, several system outputs on this data are already publicly available (Zhang and Lapata, 2017; Zhao et al., 2018; Martin et al., 2020). Therefore, we can now assess the capabilities of these and other systems in scenarios with varying simplification expectations: lexical paraphrasing with TurkCorpus, sentence splitting with HSplit, and multiple transformations with ASSET. 3.1 Data Collection Protocol Manual simplifications were collected using Amazon Mechanical Turk (AMT). AMT allows us to publish HITs (Human Intelligence Tasks), which workers can choose to work on, submit an answer, and collect a reward if the work is approved. This was also the platform used for TurkCorpus. Worker Requirements. Partic"
2020.acl-main.424,P06-4018,0,\N,Missing
2020.acl-main.424,S16-1085,1,\N,Missing
2020.acl-main.424,W18-6401,0,\N,Missing
2020.cl-1.4,D18-1399,0,0.0202962,"functions, Lrec and Ldenoi , are used for reconstructing sentences and denoising, respectively. The full architecture can be seen in Figure 4. The proposed model (UNTS) was trained using an English Wikipedia dump that was partitioned into Complex and Simple sets using a threshold based on Flesch Reading Ease scores. They also used 10,000 sentence pairs from EW-SEW (Hwang et al. 2015) and WebSplit (Narayan et al. 2017) data sets to train a model (UNTS+10K) with minimal supervision. Their models were compared against unsupervised systems from the MT literature (Artetxe, Labaka, and Agirre 2018; Artetxe et al. 2018), as well as SS models like NTS (Nisioi et al. 2017) and SBSMT (Xu et al. 2016), and using TurkCorpus as test data. When evaluated using automatic metrics, SBSMT scored the highest on SARI, but both UNTS and UNTS+10K were not far from the supervised models. This same behavior was observed with human evaluations. Even though the unsupervised model was trained using instances of sentence splitting from WebSplit, the authors do not report testing it on data for that specific text transformation. Figure 4 Model architecture for UNTS. Extracted from Surya et al. (2019). 169 Computational Linguistic"
2020.cl-1.4,bott-etal-2012-text,0,0.0652514,"Missing"
2020.cl-1.4,D15-1075,0,0.035628,"ored modeling, different from standard MT-based sequence-to-sequence approaches. 4.3.1 Split-and-Rephrase. Narayan et al. (2017) introduce a new task called split-andrephrase, focused on splitting a sentence into several others, and making the necessary changes to ensure grammaticality. No deletions should be performed so as to preserve meaning. The authors use the W EB S PLIT data set (described in Section 2.3) to train and test five models for the split-and-rephrase task: (1) Hybrid (Narayan and Gardent 2014); (2) Seq2Seq, which is an encoder-decoder with local-p attention (Luong, Pham, and Manning 2015); (3) MultiSeq2Seq, which is a multi-source sequence-to-sequence model (Zoph and Knight 2016) that takes as input the original sentence and its MR triples; and (4) one that models the problem in two steps: first learn to split, and then learn to rephrase. In this last model, the splitting step uses the original sentence and its MR to split the latter into several MR sets. However, two variations are explored for the rephrasing step: (1) Split-MultiSeq2Seq learns to rephrase from the split MRs and the original sentence in a multi-source fashion, while (2) Split-Seq2Seq only uses the split MRs a"
2020.cl-1.4,W15-1604,0,0.060762,"Missing"
2020.cl-1.4,W11-1601,0,0.403512,"pedia for research in SS comes from publicly available automatically collected alignments between sentences of equivalent articles in EW and SEW. Several techniques have been explored to produce such alignments with reasonable quality. A first approach consists of aligning texts according to their term frequency–inverse document frequency (tf-idf) cosine similarity. For the PWKP corpus, Zhu, Bernhard, and Gurevych (2010) measured this directly at sentence-level between all sentences of each article pair, and sentences whose similarity was above a certain threshold were aligned. For the C&K-1 (Coster and Kauchak 2011b) and C&K-2 (Kauchak 2013) corpora, the authors first aligned paragraphs with tf-idf cosine similarity, and then found the best overall sentence alignment with the dynamic programming algorithm proposed by Barzilay and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1"
2020.cl-1.4,P11-2117,0,0.587682,"pedia for research in SS comes from publicly available automatically collected alignments between sentences of equivalent articles in EW and SEW. Several techniques have been explored to produce such alignments with reasonable quality. A first approach consists of aligning texts according to their term frequency–inverse document frequency (tf-idf) cosine similarity. For the PWKP corpus, Zhu, Bernhard, and Gurevych (2010) measured this directly at sentence-level between all sentences of each article pair, and sentences whose similarity was above a certain threshold were aligned. For the C&K-1 (Coster and Kauchak 2011b) and C&K-2 (Kauchak 2013) corpora, the authors first aligned paragraphs with tf-idf cosine similarity, and then found the best overall sentence alignment with the dynamic programming algorithm proposed by Barzilay and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1"
2020.cl-1.4,P07-2009,0,0.0986503,"Missing"
2020.cl-1.4,W14-3348,0,0.0678559,"Missing"
2020.cl-1.4,P03-2041,0,0.0387327,"ata-Driven Sentence Simplification hand-sides (source and target) that are related. For example, we show a SCFG for a sentence in English and its translation to Japanese (Chiang 2006): S → hNP1 VP2 |NP1 VP2 i VP → hV1 NP2 |NP2 V1 i NP → hi|watashi wai NP → hthe box|hako woi V → hopen|akemasui The numbers in the non-terminals serve as links between nodes in the source and target. These links are 1-to-1 and every non-terminal is always linked to another. SCFGs have the limitation of only being able to relabel and reorder sibling nodes. In contrast, Synchronous Tree Substitution Grammars (STSGs, Eisner 2003) are able to perform more long-distance swapping. In a STSG, productions are pairs of elementary trees, which are tree fragments whose leaves can be non-terminal or terminal symbols: S S VP NP1 V misses VP NP2 NP2 NP NP John Jean NP NP Mary Marie NP V manque P NP1 a` SCFGs impose an isomorphism constraint between the aligned trees. This requirement is relaxed by the STSGs. However, to account for all the different movement patterns that could exist in a language would require powerful and, perhaps, slow grammars (Smith and Eisner 2006). Quasi-synchronous Grammars (QG, Smith and Eisner 2006) re"
2020.cl-1.4,W12-2038,0,0.0209691,"Missing"
2020.cl-1.4,W14-1215,0,0.178549,"Missing"
2020.cl-1.4,W13-2901,0,0.0547674,"Missing"
2020.cl-1.4,W08-1105,0,0.10375,"Missing"
2020.cl-1.4,N13-1092,0,0.163213,"Missing"
2020.cl-1.4,P17-1017,0,0.0573133,"Missing"
2020.cl-1.4,R13-2011,0,0.0675807,"Missing"
2020.cl-1.4,P15-2011,0,0.0605659,"Missing"
2020.cl-1.4,2015.mtsummit-papers.2,0,0.0930123,"Missing"
2020.cl-1.4,P16-1154,0,0.0631277,"Missing"
2020.cl-1.4,C18-1039,0,0.0248237,"Missing"
2020.cl-1.4,P17-1104,0,0.0478523,"Missing"
2020.cl-1.4,E17-1090,0,0.0217875,"Data-Driven Sentence Simplification classification approaches to determine if a sentence is simple or not may not be the appropriate way to model the task. Consequently, Vajjala and Meurers (2015) proposed using pair wise ranking to assess the readability of simplified sentences. They used the same features of the document-level model of Vajjala and Meurers (2014a), but now they attempt to learn to predict which of two given sentences is simpler than the other. Ambati, Reddy, and Steedman (2016) tested the usefulness of syntactic features extracted from an incremental parser for the task, and Howcroft and Demberg (2017) explored using more psycholinguistic features, such as idea density, surprisal, integration cost, and embedding depth. Although not detailed in this section, some research has used METEOR (Denkowski and Lavie 2014) from the MT literature, and ROUGE (Lin 2004), borrowed from summarization research. 3.3 Discussion In this section we have described how the outputs of SS models are evaluated using both human judgments and automatic metrics. We have attempted to not only explain these methods, but also to point out their advantages and disadvantages. In the case of human evaluation, one important"
2020.cl-1.4,N15-1022,0,0.673103,"wo sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1) or more (1-to-N) unique simplified sentences. A (*) indicates that some aligned simplified sentences may not be unique. Corpora PWKP (Zhu, Bernhard, and Gurevych 2010) C&K-1 (Coster and Kauchak 2011b) RevisionWL (Woodsend and Lapata 2011a) AlignedWL (Woodsend and Lapata 2011a) C&K-2 (Kauchak 2013) EW-SEW (Hwang et al. 2015) sscorpus (Kajiwara and Komachi 2016) WikiLarge (Zhang and Lapata 2017) Instances Alignment Types 108K 137K 15K 142K 167K 392K 493K 286K 1-to-1, 1-to-N 1-to-1, 1-to-N 1-to-1*, 1-to-N*, N-to-1* 1-to-1, 1-to-N 1-to-1, 1-to-N 1-to-1 1-to-1 1-to-1*, 1-to-N*, N-to-1* high similarity. Finally, Woodsend and Lapata (2011a) also adopt the two-step process of Coster and Kauchak (2011b), using tf-idf when compiling the AlignedWL corpus. Another approach is to take advantage of the revision histories in Wikipedia articles. When editors change the content of an article, they need to comment on what the cha"
2020.cl-1.4,A00-1043,0,0.386294,"cation, some deletion of content can also be performed. However, we could additionally replace words by more explanatory phrases, make co-references explicit, add connectors to improve fluency, and so forth. As a consequence, a simplified text could end up being longer than its original version while still improving the readability of the text. Therefore, although summarization and simplification are related, they have different objectives. Another related task is sentence compression, which consists of reducing the length of a sentence without losing its main idea and keeping it grammatical (Jing 2000). Most approaches focus on deleting unnecessary words. As such, this could be considered as a subtask of the simplification process, which also encompasses more complex transformations. Abstractive sentence compression (Cohn and Lapata 2013), on the other hand, does include transformations like substitution, reordering, and insertion. However, the goal is still to reduce content without necessarily improving readability. Split-and-rephrase (Narayan et al. 2017) focuses on splitting a sentence into several shorter ones, and making the necessary rephrasings to preserve meaning and grammaticality"
2020.cl-1.4,Q17-1024,0,0.0452132,"Missing"
2020.cl-1.4,C16-1109,0,0.0602558,"sentence similarity have also been explored. For their EW-SEW corpus, Hwang et al. (2015) implemented an alignment method using word-level semantic similarity based on Wiktionary.3 They first created a graph using synonym information and word-definition co-occurrence in Wiktionary. Then, similarity is measured based on the number of shared neighbors between words. This word-level similarity metric is then combined with a similarity score between dependency structures. This final similarity rate is used by a greedy algorithm that forces 1-to-1 matches between original and simplified sentences. Kajiwara and Komachi (2016) propose several similarity measures based on word embeddings alignments. Given two sentences, their best metric (1) finds, for each word in one sentence, the word that is most similar to it in the other sentence, and (2) averages the similarities for all words in the sentence. For symmetry, this measure is calculated twice (simplified → original, original → simplified) and their average is the final similarity measure between the two sentences. This metric was used to align original and simplified sentences from articles in a 2016 Wikipedia dump and produce the sscorpus. It contains 1-to-1 al"
2020.cl-1.4,P13-1151,0,0.0525306,"ublicly available automatically collected alignments between sentences of equivalent articles in EW and SEW. Several techniques have been explored to produce such alignments with reasonable quality. A first approach consists of aligning texts according to their term frequency–inverse document frequency (tf-idf) cosine similarity. For the PWKP corpus, Zhu, Bernhard, and Gurevych (2010) measured this directly at sentence-level between all sentences of each article pair, and sentences whose similarity was above a certain threshold were aligned. For the C&K-1 (Coster and Kauchak 2011b) and C&K-2 (Kauchak 2013) corpora, the authors first aligned paragraphs with tf-idf cosine similarity, and then found the best overall sentence alignment with the dynamic programming algorithm proposed by Barzilay and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1) or more (1-to-N) unique s"
2020.cl-1.4,W13-2902,0,0.0606494,"Missing"
2020.cl-1.4,P17-4012,0,0.0225399,"d×d , and Wh ∈ Rd×d ; |V |is the output vocabulary size and d is the hidden unit size. hTt is the hidden state of the decoder LSTM that summarizes y1:t (what has been generated so far): hTt = LSTM(yt , hTt−1 ) (19) The dynamic context vector ct is a weighted sum of the hidden states of the source sentence, whose weights αti are determined by an attention mechanism: ct = |X| X i=1 α ti hSi exp(hTt · hSi ) α ti = P S T i exp(ht · hi ) (20) NTS: Nisioi et al. (2017) introduced the first Neural Text Simplification approach using the encoder-decoder with attention architecture provided by OpenNMT (Klein et al. 2017). They experimented with using the default system, and also with combining pre-trained word2vec word embeddings (Mikolov et al. 2013) with locally trained ones. They also generated two candidate hypotheses for each beam size, and used BLEU and SARI to determine which hypothesis to choose from the n-best list of candidates. EW-SEW was used for training, and TurkCorpus for validation and testing. When compared against PBSMT-R and SBSMT (PPDB+SARI), NTS with its default features achieved the highest grammaticality and meaning preservation scores in human evaluation. SBSMT (PPDB+SARI) was still th"
2020.cl-1.4,klerke-sogaard-2012-dsim,0,0.0378873,"Missing"
2020.cl-1.4,P07-2045,0,0.012619,"g a best-first search algorithm, like A*, but exploring the entire search space of possible translations is expensive. Therefore, 154 Alva-Manchego, Scarton, and Specia Data-Driven Sentence Simplification Table 2 Performance of PBSMT-based sentence simplification models as reported by their authors. Model Moses (Brazilian Portuguese) Moses (English) Moses-Del PBSMT-R Train Corpus Test Corpus BLEU ↑ FKGL ↓ PorSimples C&K C&K PWKP PorSimples C&K C&K PWKP 60.75 59.87 60.46 43.00 13.38 decoders use beam-search to only retain, at every step, the most promising states to continue the search. Moses (Koehn et al. 2007) is a popular PBSMT system, freely available.9 It provides tools for easy training, tuning, and testing of translation models based on this SMT approach. Specia (2010) was the first to use this toolkit, with no adaptations, for the simplification task. Experiments were carried out on a parallel corpus of original and manually simplified newspaper articles in Brazilian Portuguese (Caseli et al. 2009). The trained model mostly executes lexical simplifications and simple rewritings. However, as expected, it is overcautious and cannot perform long distance operations like subjectverb-object reorde"
2020.cl-1.4,W04-1013,0,0.0659056,"s. They used the same features of the document-level model of Vajjala and Meurers (2014a), but now they attempt to learn to predict which of two given sentences is simpler than the other. Ambati, Reddy, and Steedman (2016) tested the usefulness of syntactic features extracted from an incremental parser for the task, and Howcroft and Demberg (2017) explored using more psycholinguistic features, such as idea density, surprisal, integration cost, and embedding depth. Although not detailed in this section, some research has used METEOR (Denkowski and Lavie 2014) from the MT literature, and ROUGE (Lin 2004), borrowed from summarization research. 3.3 Discussion In this section we have described how the outputs of SS models are evaluated using both human judgments and automatic metrics. We have attempted to not only explain these methods, but also to point out their advantages and disadvantages. In the case of human evaluation, one important but often overlooked aspect is that it should be carried out by individuals from the same target audience of the data on which the SS model was trained. This is especially relevant when collecting simplicity judgments because of its subjective nature: What a n"
2020.cl-1.4,D15-1166,0,0.0539924,"Missing"
2020.cl-1.4,E17-1083,0,0.0602083,"Missing"
2020.cl-1.4,C14-1188,0,0.0547247,"Missing"
2020.cl-1.4,2013.mtsummit-posters.8,0,0.118773,"Missing"
2020.cl-1.4,W14-5603,0,0.0692028,"Missing"
2020.cl-1.4,E17-1038,0,0.0297781,"nput sentences indicating (1) the grade level of the simplification instance, and/or (2) one of four possible text transformations: identical, elaboration, splitting, or joining. At test time, the text transformation token is either predicted (using a simple features-based naive Bayes classifier) or an oracle label is used. They experimented using the standard neural architecture available in OpenNMT and data from the Newsela corpus. Results showed improvements in BLEU, SARI, and Flesch scores when using this extra information. N SE L STM: Vu et al. (2018) used Neural Semantic Encoders (NSEs, Munkhdalai and Yu 2017) instead of LSTMs for the encoder. At any encoding time step, a NSE has access to all the tokens in the input sequence, and is thus able to capture more context information while encoding the current token, instead of only relying on the previous hidden state. Their approach is tested on PWKP, TurkCorpus, and Newsela. Two models 165 Computational Linguistics Volume 46, Number 1 are presented, one tuned using BLEU (N SE L STM -B) and one using SARI (N SE L STM -S). When compared against other models, N SE L STM -B achieved the best BLEU scores in the Newsela and TurkCorpus data sets, while N SE"
2020.cl-1.4,W10-0406,0,0.0959421,"Missing"
2020.cl-1.4,P14-1041,0,0.222183,"approach does not explicitly model sentence splitting. Table 4 summarizes the performance of the models trained with the SS approaches described. Unfortunately, results are not directly comparable. Overall, grammarinduction-based approaches, because of their pipeline architecture, offer more flexibility on how the rules are learned and how they are applied, as compared with end-toend approaches. Even though Woodsend and Lapata (2011a) were the only ones who attempted model splitting, the other approaches could be modified in a similar way, since the formalisms allow it. 4.3 Semantics-Assisted Narayan and Gardent (2014) argue that the simplification transformation of splitting is semantics-driven. In many cases, splitting occurs when an entity takes part in two (or more) distinct events described in a single sentence. For example, in Sentence (1), bricks is involved in two events: “being resistant to cold” and “enabling the construction of permanent buildings.” (1) Original: Being more resistant to cold, bricks enabled the construction of permanent buildings. 161 Computational Linguistics (2) Volume 46, Number 1 Simplified: Bricks were more resistant to cold. Bricks enabled the construction of permanent buil"
2020.cl-1.4,W16-6620,0,0.0911308,"e tree. The SS model is trained and tested using the PWKP corpus. Sentences for which Boxer failed to extract a semantic representation were excluded during training, and directly passed to the PBSMT system in testing. For evaluation, the model is compared against QG+ILP, PBSMT-R, and TSM. Hybrid performs splits closer in proportion to those of the references. It also achieves the highest BLEU score and smaller edit distance to references. With human evaluation, Hybrid obtains the highest score in simplicity, and is a close second to PBSMT-R for grammaticality and meaning preservation. UNSUP: Narayan and Gardent (2016) propose a method that does not require aligned original-simplified sentences to train a TS model. Their approach first uses a context-aware lexical simplifier (Biran, Brody, and Elhadad 2011) that learns simplification rules from articles of EW and SEW. Given an original sentence, these rules are applied and the best combination of simplifications is found using dynamic programming. Then, they use Boxer to extract the semantic representation of the sentence and identify the events/predicates. After that, they estimate the maximum likelihood of the sequences of semantic role sets that would re"
2020.cl-1.4,C16-2036,0,0.0261361,"old 2016), children (De Belder and Moens 2010), and people suffering from aphasia (Devlin and Tait 1998; Carroll et al. 1998), dyslexia (Rello et al. 2013b), or autism (Evans, Orasan, and Dornescu 2014). Furthermore, simplifying sentences automatically could improve performance on other Natural Language Processing tasks, which has become evident in parsing (Chandrasekar, Doran, and Srinivas 1996), summarization (Siddharthan, Nenkova, and McKeown 2004; Vanderwende et al. 2007; Silveira and Branco 2012), information extraction (Klebanov, Knight, and Marcu 2004; Evans 2011), relation extraction (Niklaus et al. 2016), semantic role labeling Vickrey and Koller 2008, and MT (Mirkin, Venkatapathy, and Dymetman 2013; Mishra et al. ˇ 2014; Stajner and Popovi´c 2016; Hasler et al. 2017). We refer the interested reader to Siddharthan (2014) for a more in-depth review of studies on the benefits of simplification for different target audiences and Natural Language Processing applications. 1.2 Text Transformations for Simplification A few corpus studies have been carried out to determine how humans simplify sentences. These studies shed some light on the simplification transformations that an automatic SS model sho"
2020.cl-1.4,P17-2014,0,0.543791,"here g(· ) is a neural network with one hidden layer and parametrized as follows: g(hTt , ct ) = Wo tanh(Uh hTt + Wh ct ) (18) where Wo ∈ R|V|×d , Uh ∈ Rd×d , and Wh ∈ Rd×d ; |V |is the output vocabulary size and d is the hidden unit size. hTt is the hidden state of the decoder LSTM that summarizes y1:t (what has been generated so far): hTt = LSTM(yt , hTt−1 ) (19) The dynamic context vector ct is a weighted sum of the hidden states of the source sentence, whose weights αti are determined by an attention mechanism: ct = |X| X i=1 α ti hSi exp(hTt · hSi ) α ti = P S T i exp(ht · hi ) (20) NTS: Nisioi et al. (2017) introduced the first Neural Text Simplification approach using the encoder-decoder with attention architecture provided by OpenNMT (Klein et al. 2017). They experimented with using the default system, and also with combining pre-trained word2vec word embeddings (Mikolov et al. 2013) with locally trained ones. They also generated two candidate hypotheses for each beam size, and used BLEU and SARI to determine which hypothesis to choose from the n-best list of candidates. EW-SEW was used for training, and TurkCorpus for validation and testing. When compared against PBSMT-R and SBSMT (PPDB+SARI)"
2020.cl-1.4,J04-4002,0,0.0300174,"Missing"
2020.cl-1.4,W13-4813,1,0.919517,"s chosen, the node it depends on is also chosen), coherence (if one partition of a split sentence is chosen, the other partition is also chosen), and always one (and only one) simplification per sentence. The authors trained two models: one extracting rules from the AlignedLP corpus (AlignILP) and the other using the RevisionWL corpus (RevILP). For evaluation, they used the test split from the PWKP instances. RevILP was their best model, achieving the closest scores to the references using both Flesh-Kincaid and human judgments on simplicity, grammaticality, and meaning preservation. T3+Rank: Paetzold and Specia (2013) extract candidate tree rewriting rules using T3 (Cohn and Lapata 2009), an abstractive sentence compression model that uses STSGs for deletion, reordering, and substitution. Using word-aligned parallel sentences, the model maps the word alignment into a constituent-level alignment between the source and target trees by adapting the alignment template method of Och and Ney (2004). These constituent alignments are then generalized (i.e., aligned nodes are replaced with links) to extract rules. This generalization is performed by a recursive algorithm that attempts to find the minimal most gener"
2020.cl-1.4,E17-2006,1,0.86031,"lume 46, Number 1 4.4.5 Simplification as Sequence Labeling. Alva-Manchego et al. (2017) model SS as a Sequence Labeling problem, identifying simplification transformations at word or phrase level. They use the token-level annotation algorithms of MASSAlign (Paetzold, Alva-Manchego, and Specia 2017) to automatically generate annotated data from which an LSTM learns to predict simplification transformations; more specifically, deletions and replacements. During decoding, words labeled to be deleted are just not included in the output. To produce replacements, they use the lexical simplifier of Paetzold and Specia (2017a). The proposed approach is compared against MT-based models: Moses (Koehn et al. 2007), Nematus (Sennrich et al. 2017), and NTS+word2vec (with default settings) using data from the Newsela corpus. Alva-Manchego et al. (2017) achieve the highest SARI score in the test set, and best simplicity score with human judgments. This approach is inspired in the abstractive sentence compression model of Bingel and Søgaard (2016), who propose a tree labeling approach to remove or paraphrase syntactic units in the dependency tree of a given sentence, using a Conditional Random Fields predictor. Most sequ"
2020.cl-1.4,I17-3001,1,0.924433,"Missing"
2020.cl-1.4,C16-1069,1,0.939527,"k of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments. 1. Introduction Text Simplification (TS) is the task of modifying the content and structure of a text in order to make it easier to read and understand, while retaining its main idea and approximating its original meaning. A simplified version of a text could benefit users with several reading difficulties, such as non-native speakers (Paetzold 2016), people with aphasia (Carroll et al. 1998), dyslexia (Rello et al. 2013b), or autism (Evans, Orasan, and Dornescu 2014). Simplifying a text automatically could also help improve performance Submission received: 8 June 2018; revise d version received: 9 August 2019; accepted for publication: 15 September 2019. https://doi.org/10.1162/COLI a 00370 © 2020 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 46, Number 1 on other language processing tasks, s"
2020.cl-1.4,P02-1040,0,0.109185,"cheaper results. Some of these metrics are based on comparing the automatic simplifications to manually produced references; others compute the readability of the text based on psycholinguistic metrics; whereas others are trained on specially annotated data so as to learn to predict the quality or usefulness of the simplification being evaluated. 3.2.1 String Similarity Metrics. These metrics are mostly borrowed from the MT literature, since SS can be seen as translating a text from complex to simple. The most commonly used are BLEU and TER. BLEU (BiLingual Evaluation Understudy), proposed by Papineni et al. (2002), is a precision-oriented metric, which means that it depends on the number of n-grams in the candidate translation that match with n-grams of the reference, independent of position. BLEU values range from 0 to 1 (or to 100); the higher the better. BLEU calculates a modified n-gram precision: (i) count the maximum number of times that an n-gram occurs in any of the references, (ii) clip the total count of each candidate n-gram by its maximum reference count (i.e, Countclip = min(Count, MaxRefCount)), and (iii) add these clipped counts up, and divide by the total (unclipped) number of candidate"
2020.cl-1.4,P16-2024,0,0.11142,"xtract the paraphrases, they used bilingual corpora with the following intuition: “two strings that translate to the same foreign string can be assumed to have the same meaning.” The authors utilized the synchronous contextfree grammar formalism to collect paraphrases. Using MT technology, they extracted grammar rules from foreign-to-English corpora. Then, the paraphrase is created from rule pairs where the left-hand side and foreign string match. Each paraphrase in PPDB has a similarity score, which was calculated using monolingual distributional similarity. 2.3.8 Simple Paraphrase Database. Pavlick and Callison-Burch (2016) created the Simple PPDB, a subset of the PPDB tailored for SS. They used machine learning models to select paraphrases that generate a simplification and preserve its meaning. First, they selected 1,000 words from PPDB which also appear in the Newsela corpus. They then selected up to 10 paraphrases for each word. After that, they crowd-sourced the manual evaluation of these paraphrases in two stages: (1) rate their meaning preservation in a scale of 1 to 5, and (2) label the ones with rates higher than 2 as simpler or not. Next, these data were used to train a multi-class logistic regression"
2020.cl-1.4,W18-6319,0,0.0154542,"ntly use metrics that rely on multiple references, like SARI. We do not use the Newsela corpus in our benchmark because researchers are prohibited from publicly releasing models’ outputs on these data. 5.1.2 Overall Performance Comparison. We first compare the models’ outputs using automatic metrics so as to obtain an overall measure of simplification quality. We calculate BLEU, SARI, SAMSA, and FKGL. We compute the scores for these metrics using EASSE (Alva-Manchego et al. 2019),10 a Python package for single access to (re)implementations of these metrics. Specifically, it uses S ACRE B LEU (Post 2018)11 to calculate BLEU, a re-implementation of SARI’s corpus-level version in Python (it was originally available in Java), a slightly modified version of the original SAMSA implementation12 for improved execution speed, and a re-implementation of FKGL based on publicly available scripts13 that fixes some edge case inconsistencies. 5.1.3 Transformation-Based Performance Comparison. We are also interested in an in-depth study of the simplification capabilities of each model. In particular, we want to determine which simplification transformations each model performs more effectively. In order to"
2020.cl-1.4,W13-2226,0,0.0312662,"TurkCorpus TurkCorpus PWKP Own TurkCorpus TurkCorpus TurkCorpus 38.00 99.05 74.48 72.36 FKGL ↓ SARI ↑ 7.9 12.88 10.75 10.90 26.05 34.18 37.91 The proposed simplification model also relies on paraphrasing rules available in the PPDB, which are expressed as a Synchronous Context-Free Grammar (SCFG). The authors also added nine new features to each rule in the PPDB (each rule already contains 33). These new features are simplification-specific, for example: length in characters, length in words, number of syllables, among others. These modifications were implemented in the SBSMT toolkit Joshua (Post et al. 2013) and performed experiments using TurkCorpus (described in Section 2.3) on three versions of the SBSMT system, changing the tuning metric (BLEU, FKBLEU, and SARI). Evaluations using human judgments show that all three models achieved better grammatically, meaning preservation, and simplicity gain than PBSMT-R (Wubben, van den Bosch, and Krahmer 2012). Table 3 summarizes the performance of the syntax-based models trained using the SS approaches described. These values are not directly comparable, because each approach used a different corpus for testing. In the case of the models based on Joshua"
2020.cl-1.4,J18-3002,0,0.0226795,"ns. Then, the accuracy of their responses is used to qualify the helpfulness of the simplified texts in the particular comprehension task. This type of human evaluation could be more goal-oriented, but they are costly to create and execute. Automatic metrics are useful for quickly assessing models and comparing different architectures. They could even be considered more objective than humans since personal biases do not play a role. However, the metrics used in SS research are flawed. BLEU has been found to only be reliable for assessment in MT but not other Natural Language Generation tasks (Reiter 2018), and it is not adequate for most rewriting transformations in SS (Sulem, Abend, and Rappoport 2018a). SARI is only useful as a proxy for simplicity gain assessment, limited to lexical simplifications and shortdistance reordering despite more text transformations being possible. Commonly-used Flesch metrics were developed to assess complete documents and not sentences, which is the focus of most simplification research nowadays. Therefore, when evaluating models using these automatic scores, it is essential to keep all their particular limitations 153 Computational Linguistics Volume 46, Numbe"
2020.cl-1.4,L18-1685,1,0.926262,"Missing"
2020.cl-1.4,L18-1553,1,0.929258,"Missing"
2020.cl-1.4,P18-2113,1,0.842686,"this standard neural architecture for SS, but used the implementation provided by Nematus (Sennrich et al. 2017). They experimented with different types of original-simplified sentence alignments extracted from the Newsela corpus. When experimenting with all possible sentence alignments, the model tended to be too aggresive, mostly performing deletions. When using only 1-to-1 alignments, the model became more conservative, and the simplifications performed were restricted to deletions and one-word replacements. targeTS: Inspired by the work of Johnson et al. (2017) on multilingual neural MT, Scarton and Specia (2018) enriched the encoder’s input with information about the target audience and the (predicted) simplification transformations to be performed. Concretely, an artificial token was added to the beginning of the input sentences indicating (1) the grade level of the simplification instance, and/or (2) one of four possible text transformations: identical, elaboration, splitting, or joining. At test time, the text transformation token is either predicted (using a simple features-based naive Bayes classifier) or an oracle label is used. They experimented using the standard neural architecture available"
2020.cl-1.4,P17-1099,0,0.0991683,"Missing"
2020.cl-1.4,E17-3017,0,0.0481566,"Missing"
2020.cl-1.4,W03-2314,0,0.373702,"n Sentence Simplification simplifications to comply with the readability requirements of the grade level of the current version. Xu, Callison-Burch, and Napoles (2015) also presented an analysis of the most frequent syntax patterns in original and simplified texts for PWKP and Newsela. These patterns correspond to parent node (head node) → children node(s) structures. Overall, the Wikipedia corpus has a higher tendency to retain complex patterns in its simple counterpart than Newsela. Finally, the authors present a study on discourse connectives that are important for readability according to Siddharthan (2003). They report that simple cue words are more likely to appear in Newsela’s simplifications, and that complex connectives have a higher probability to be retained in Wikipedia’s. This could enable research on how discourse features influence simplification. 2.2.1 Simplification Instances. Newsela is a corpus that can be obtained for free for research purposes,4 but it cannot be redistributed. As such, it is not possible to produce and release sentence alignments for the research community in SS. This is certainly a disadvantage, because it is difficult to compare SS models developed using this"
2020.cl-1.4,W11-2802,0,0.0345876,"ng text transformations from parallel corpora of aligned original-simplified sentences in English. Compared with approaches based on hand-crafted rules, data-driven approaches can perform multiple simplification transformations simultaneously, as well as learn very specific and complex rewriting patterns. As a result, they make it possible to model interdependencies among different transformations more naturally. Therefore, we do not include approaches to sentence simplification based on sets of hand-crafted rules, such as rules for splitting and reordering sentences (Candido Jr. et al. 2009; Siddharthan 2011; Bott, Saggion, and Mille 2012), nor approaches that only learn lexical simplifications, that is, which target one-word replacements (see Paetzold and Specia [2017b] for a survey). We classify data-driven approaches for SS as relying on statistical MT techniques (Section 4.1), induction of synchronous grammars (Section 4.2), semantics-assisted (Section 4.3), and neural sequence-to-sequence models (Section 4.4). 4.1 Monolingual Statistical Machine Translation Several approaches treat SS as a monolingual MT task, with original and simplified as source and target languages, respectively. Whereas"
2020.cl-1.4,C04-1129,0,0.199974,"Missing"
2020.cl-1.4,W06-3104,0,0.125517,"Missing"
2020.cl-1.4,2006.amta-papers.25,0,0.133887,"elation with human assessments of grammaticality and meaning preservation, but not simplicity. Also, Sulem, Abend, and Rappoport (2018a) show that this correlation is low or non-existent when sentence splitting has been performed. As such, BLEU 148 Alva-Manchego, Scarton, and Specia Data-Driven Sentence Simplification should not be used as the only metric for evaluation and comparison of SS models. In addition, because of its definition, this metric is more useful with simplification corpora that provides multiple references for each original sentence. TER (Translation Edit Rate), designed by Snover et al. (2006), measures the minimum number of edits necessary to change a candidate translation so that it matches perfectly to one of the references, normalized by the average length of the references. Only the reference that is closest (according to TER) is considered for the final score. The edits to be considered are insertions, deletions, substitutions of single words, and shifts (positional changes) of word sequences. TER is an edit-distance metric Equation (3), with values ranging from 0 to 100; lower values are better. TER = # of edits average # of reference words (3) In order to calculate the numb"
2020.cl-1.4,D18-1081,0,0.141163,"Missing"
2020.cl-1.4,N18-1063,0,0.0638026,"Missing"
2020.cl-1.4,P12-2008,0,0.0350483,"it distance using minimumedit-distance and dynamic programming. For simplification research, TER’s intermediate calculations (i.e., the edits counts) have been used to show the simplification operations that an SS model is able to perform (Zhang and Lapata 2017). However, this is not a general practice and no studies have been conducted to verify that the edits correlate with simplification transformations. Scarton, Paetzold, and Specia (2018b) use TER to study the differences between different simplification versions in articles of the Newsela corpus. iBLEU is a variant of BLEU introduced by Sun and Zhou (2012) as a way to measure the quality of a candidate paraphrase. The metric balances the semantic similarity between the candidate and the reference, with the dissimilarity between the candidate and the source. Given a candidate paraphrase c, human references rs , and input text s, iBLEU is computed as in Equation (4), with values ranging from 0 to 1 (or to 100); higher values are better. iBLEU(s, rs , c) = α × BLEU(c, rs ) − (1 − α ) × BLEU(c, s) (4) After empirical evaluations, the authors recommend using a value of α between 0.7 and 0.9. For example, Mallinson, Sennrich, and Lapata (2017) experi"
2020.cl-1.4,P19-1198,0,0.139372,"of the encoder, and corresponding generated outputs. The model was trained only using WikiLarge, and tested on TurkCorpus and Newsela. The authors evaluated using both mechanisms, DCSS and DMASS, independently, as well as in conjunction. Then compared to other models, DMASS+DCSS achieved the highest SARI score in both test sets. They also estimated the correctness of rule utilization based on ground-truth from SPPDB, and showed that their models also improved compared to previous work. 168 Alva-Manchego, Scarton, and Specia Data-Driven Sentence Simplification 4.4.4 Unsupervised Architectures. Surya et al. 2019 proposed an unsupervised approach for developing a simplification system. Their motivation was to design an architecture that could be exploited to train SS models for languages or domains that do not have large resources of parallel original-simplified instances. Their proposal is based on a modified auto encoder that uses a shared encoder E and two dedicated decoders: one for generating complex sentences (Gd ) and one for simple sentences (Gs ). In addition, their model relies on Discriminator and Classifier modules. The Discriminator determines if a given context vector sequence (from eith"
2020.cl-1.4,P17-2016,0,0.0913733,"s,4 but it cannot be redistributed. As such, it is not possible to produce and release sentence alignments for the research community in SS. This is certainly a disadvantage, because it is difficult to compare SS models developed using this corpus without a common split of the data and the same document, paragraph, and sentence alignments. Xu, Callison-Burch, and Napoles (2015) align sentences between consecutive versions of articles in the corpus using Jaccard similarity (Jaccard 1912) based on overlapping word lemmas. Alignments with the highest similarity become simplification instances. ˇ Stajner et al. (2017) explore three similarity metrics and two alignment methods to produce paragraph and sentence alignments in Newsela. The first similarity metric uses a character 3-gram model (Mcnamee and Mayfield 2004) with cosine similarity. The second metric averages the word embeddings (trained in EW) of the text snippet and then uses cosine similarity. The third metric computes the cosine similarity between all word embeddings in the text snippet (instead of the average). Regarding the alignment methods, the first one uses any of the previous metrics to compute the similarity between all possible sentence"
2020.cl-1.4,L18-1615,0,0.218429,"Missing"
2020.cl-1.4,W14-1201,0,0.184351,"Missing"
2020.cl-1.4,W16-3411,0,0.0516602,"Missing"
2020.cl-1.4,W18-0535,0,0.0790745,"Missing"
2020.cl-1.4,E14-1031,0,0.0251225,"aggion (2014)’s work ˇ with features from Stajner, Popovi´c, and B´echera (2016) to analyze how different feature groups correlate with human judgments on grammaticality, meaning preservation, and simplicity using data from QATS. Using Quality Estimation research for reference-less evaluation in simplification is still an area not sufficiently explored, mainly because it requires human annotations on example instances that can be used as training data, which can be expensive to collect. Another group of approaches is interested in ranking sentences according to their predicted reading levels. Vajjala and Meurers, (2014a,b) showed that, in the PWKP (Zhu, Bernhard, and Gurevych 2010) data set and and earlier version of the OneStopEnglish (Vajjala and Luˇci´c 2018) corpus, even if all simplified sentences were simpler than their aligned original counterpart, some sentences in the “simple” section had a higher reading level than some in the “original” section. As such, attempting to use binary 8 In Quality Estimation, the goal is to evaluate an output translation without comparing it to a reference. For a comprehensive review of this area of research, please refer to Specia, Scarton, and Paetzold (2018). 152 Al"
2020.cl-1.4,P08-1040,0,0.26931,"mance Submission received: 8 June 2018; revise d version received: 9 August 2019; accepted for publication: 15 September 2019. https://doi.org/10.1162/COLI a 00370 © 2020 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 46, Number 1 on other language processing tasks, such as parsing (Chandrasekar, Doran, and Srinivas 1996), summarization (Vanderwende et al. 2007; Silveira and Branco 2012), information extraction (Evans 2011), semantic role labeling (Vickrey and Koller 2008), and Machine Translation (MT) (Hasler et al. 2017). Most research on TS has focused on studying simplification of individual sentences. Reducing the scope of the problem has allowed the easier collection and curation of corpora, as well as adapting methods from other text generation tasks, mainly MT. It can be argued that “true” TS (i.e., document-level) cannot be achieved by simplifying sentences one at a time, and we make a call in Section 6 for the field to move in that direction. However, because the goal of this article is to review what has been done in TS so far, our survey is limited"
2020.cl-1.4,N18-2013,0,0.077644,"n artificial token was added to the beginning of the input sentences indicating (1) the grade level of the simplification instance, and/or (2) one of four possible text transformations: identical, elaboration, splitting, or joining. At test time, the text transformation token is either predicted (using a simple features-based naive Bayes classifier) or an oracle label is used. They experimented using the standard neural architecture available in OpenNMT and data from the Newsela corpus. Results showed improvements in BLEU, SARI, and Flesch scores when using this extra information. N SE L STM: Vu et al. (2018) used Neural Semantic Encoders (NSEs, Munkhdalai and Yu 2017) instead of LSTMs for the encoder. At any encoding time step, a NSE has access to all the tokens in the input sequence, and is thus able to capture more context information while encoding the current token, instead of only relying on the previous hidden state. Their approach is tested on PWKP, TurkCorpus, and Newsela. Two models 165 Computational Linguistics Volume 46, Number 1 are presented, one tuned using BLEU (N SE L STM -B) and one using SARI (N SE L STM -S). When compared against other models, N SE L STM -B achieved the best BL"
2020.cl-1.4,P18-1042,0,0.0200272,"u, and Bansal (2018) proposed learning this mixing ratio dynamically using a multi-armed bandits based controller. Basically, at each round, the controller selects a task based on some noise value estimates, observes “rewards” for the selected task (in their case, the reward was the negative validation loss of the main task), and switches accordingly. The proposed model was trained and tested using PWKP, WikiLarge (with TurkCorpus as test set), and Newsela for SS; the SNLI (Bowman et al. 2015) and the MultiNLI (Williams, Nangia, and Bowman 2018) corpora for entailment generation; and ParaNMT (Wieting and Gimpel 2018) for paraphrase generation. Using automatic metrics, PointerCopy+MTL achieved the highest SARI score only in the Newsela corpus. With human judgments, their model scored as the best in simplicity. 167 Computational Linguistics Volume 46, Number 1 Figure 3 Model architecture for PointerCopy+MTL. Extracted from Guo, Pasunuru, and Bansal (2018). 4.4.3 Adding External Knowledge. The previously described models attempted to learn how to simplify only using information from the training data sets. Zhao et al. (2018) argued that the relatively small size of these data sets prevents models from genera"
2020.cl-1.4,N18-1101,0,0.0747723,"Missing"
2020.cl-1.4,D11-1038,0,0.111902,"and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wikipedia.org. 2 https://wikipedia.org. 139 Computational Linguistics Volume 46, Number 1 Table 1 Summary of parallel corpora extracted from EW and SEW. An original sentence can be aligned to one (1-to-1) or more (1-to-N) unique simplified sentences. A (*) indicates that some aligned simplified sentences may not be unique. Corpora PWKP (Zhu, Bernhard, and Gurevych 2010) C&K-1 (Coster and Kauchak 2011b) RevisionWL (Woodsend and Lapata 2011a) AlignedWL (Woodsend and Lapata 2011a) C&K-2 (Kauchak 2013) EW-SEW (Hwang et al. 2015) sscorpus (Kajiwara and Komachi 2016) WikiLarge (Zhang and Lapata 2017) Instances Alignment Types 108K 137K 15K 142K 167K 392K 493K 286K 1-to-1, 1-to-N 1-to-1, 1-to-N 1-to-1*, 1-to-N*, N-to-1* 1-to-1, 1-to-N 1-to-1, 1-to-N 1-to-1 1-to-1 1-to-1*, 1-to-N*, N-to-1* high similarity. Finally, Woodsend and Lapata (2011a) also adopt the two-step process of Coster and Kauchak (2011b), using tf-idf when compiling the AlignedWL corpus. Another approach is to take advantage of the revision histories in Wikipedia artic"
2020.cl-1.4,P12-1107,0,0.374645,"Missing"
2020.cl-1.4,Q16-1029,0,0.0550075,"odels. 2.3 Other Resources for English In this section, we describe some additional resources that are used for SS in English with very specific reasons: tuning and testing of models in general purpose (TurkCorpus) and domain-specific (SimPA) data, evaluation of sentence splitting (HSplit), readability assessment (OneStopEnglish), training and testing of split-and-rephrase (W EB S PLIT and WikiSplit), and learning paraphrases (PPDB and SPPDB). 2.3.1 TurkCorpus. Just like with other text rewriting tasks, there is no single correct simplification possible for a given original sentence. As such, Xu et al. (2016) asked workers on Amazon Mechanical Turk to simplify 2,350 sentences extracted from the PWKP corpus to collect eight references for each one. This corpus was then randomly split into two sets: one with 2,000 instances intended to be used for system tuning, and one with 350 instances for measuring the performance of SS models using metrics that rely on multiple references (see SARI in Sec. 3.2.3). However, the instances chosen from PWKP are those that focus on paraphrasing (1-to-1 alignments with almost similar lengths), thus limiting the range of simplification operations that SS models can be"
2020.cl-1.4,P01-1067,0,0.339894,"Missing"
2020.cl-1.4,N10-1056,0,0.0936837,"e to the subject-verb-object order for their sentences, and avoiding compound sentences (Simple Wikipedia 2017a). 2.1.1 Simplification Instances. Much of the popularity of using Wikipedia for research in SS comes from publicly available automatically collected alignments between sentences of equivalent articles in EW and SEW. Several techniques have been explored to produce such alignments with reasonable quality. A first approach consists of aligning texts according to their term frequency–inverse document frequency (tf-idf) cosine similarity. For the PWKP corpus, Zhu, Bernhard, and Gurevych (2010) measured this directly at sentence-level between all sentences of each article pair, and sentences whose similarity was above a certain threshold were aligned. For the C&K-1 (Coster and Kauchak 2011b) and C&K-2 (Kauchak 2013) corpora, the authors first aligned paragraphs with tf-idf cosine similarity, and then found the best overall sentence alignment with the dynamic programming algorithm proposed by Barzilay and Elhadad (2003). This algorithm takes context into consideration: The similarity between two sentences is affected by their proximity to pairs of sentences with 1 https://simple.wiki"
2020.cl-1.4,D17-1062,0,0.269917,"Missing"
2020.cl-1.4,D18-1355,0,0.173206,"iams, Nangia, and Bowman 2018) corpora for entailment generation; and ParaNMT (Wieting and Gimpel 2018) for paraphrase generation. Using automatic metrics, PointerCopy+MTL achieved the highest SARI score only in the Newsela corpus. With human judgments, their model scored as the best in simplicity. 167 Computational Linguistics Volume 46, Number 1 Figure 3 Model architecture for PointerCopy+MTL. Extracted from Guo, Pasunuru, and Bansal (2018). 4.4.3 Adding External Knowledge. The previously described models attempted to learn how to simplify only using information from the training data sets. Zhao et al. (2018) argued that the relatively small size of these data sets prevents models from generalizing well, considering the vast amount of possible simplification transformations that exist. Therefore, they proposed to include human-curated paraphrasing rules from Simple Paraphrase Database (SPPDB; Pavlick and Callison-Burch 2016) into a neural encoderdecoder architecture. This intuition is similar to Xu et al. (2016), who incorporated those rewriting rules into a SBSMT-based model. In addition, the authors moved from the RNN-based architecture to one based on the Transformer (Vaswani et al. 2017). The"
2020.cl-1.4,C10-1152,0,0.727312,"Missing"
2020.cl-1.4,N16-1004,0,0.0479442,"Missing"
2020.cl-1.4,Q15-1021,0,\N,Missing
2020.cl-1.4,C96-2183,0,\N,Missing
2020.cl-1.4,W03-1004,0,\N,Missing
2020.cl-1.4,N16-1120,0,\N,Missing
2020.cl-1.4,P16-2055,0,\N,Missing
2020.cl-1.4,P17-1080,0,\N,Missing
2020.cl-1.4,W18-0503,0,\N,Missing
2020.cl-1.4,W18-7005,0,\N,Missing
2020.cl-1.4,W09-2105,1,\N,Missing
2020.cl-1.4,W14-1214,1,\N,Missing
2020.lrec-1.176,W10-1001,1,0.747351,"Missing"
2020.lrec-1.176,W13-4829,1,0.814744,"Missing"
2020.lrec-1.176,W11-2308,0,0.076365,"Missing"
2020.lrec-1.176,E17-1090,0,0.0641901,"Missing"
2020.lrec-1.176,W11-4504,1,0.434882,"Missing"
2020.lrec-1.176,D15-1133,0,0.0708394,"Missing"
2020.lrec-1.176,P18-1022,0,0.0659801,"Missing"
2020.lrec-1.176,N10-2011,1,0.782921,"Missing"
2020.lrec-1.176,W16-4123,0,0.0222544,"Missing"
2020.lrec-1.176,P17-2102,0,0.0484565,"Missing"
2020.rdsm-1.4,aker-etal-2017-simple,0,0.0221006,"erefore, our process for threshold moving is: (1) compute the output probability Pk for class k and (2) assign the class with highest Pk /ak , ak = numk /numtotal , in which numk is the number of class k in the training set, and numtotal is the total number of the training set. 3 Experiments and Results 3.1 Experimental setup Techniques presented in Section 2 are explored in two types of classification models. We use implementations of resampling methods from the imbalanced-learn python toolkit (Lemaˆıtre et al., 2017). Feature-based classifiers Our feature-based approach is an adaptation of (Aker et al., 2017). We use Twitter-based features like number of re-tweets, presence of URLs and hashtags, number of followers for the user, among others. These features are then concatenated with a word vector representation for the tweets, using a pre-trained Twitter GloVe embedding model (Pennington et al., 2014). We train Random Forest (RF), Multi-Layer Perceptron (MLP) and Logistic Regression with stochastic gradient descent (LR-SGD) models using the scikit-learn python toolkit (Pedregosa et al., 2011). 2 Our implementation is available at https://github.com/YLi999/RumorStanceClassification 39 RF MLP LR-SG"
2020.rdsm-1.4,N19-1423,0,0.0337848,"Missing"
2020.rdsm-1.4,S17-2082,0,0.0598512,"Missing"
2020.rdsm-1.4,S19-2192,0,0.0127192,"the performance on development data (n = 15 in our case). We also present results for BERT without TM (BERT-NT(single)) on RumourEval 2017 test set. For RumourEval 2017, we compare our models with Turing, ECNU, and NileTMRG (Enayet and ElBeltagy, 2017) (Table 2). BERT-TM(single), BERT-TM(ensemble), and FBE-RUS outperform other systems, showing similar performance for supports and denies. After applying TM on the output of BERT-NT (BERT-TM(single)), the performance on the minority classes is significantly enhanced (Figure 1). For RumourEval 2019, our models are compared with BLCU NLP, BUT-FIT (Fajcik et al., 2019), and eventAI (Table 3), outperforming BLCU NLP and BUT-FIT on supports and denies (Figure 2). Although eventAI performs better than our models, some details about its architecture are not provided in the paper and the code is not publicly available. 4 Conclusion and future work We experiment with traditional imbalanced data techniques for the task of rumour stance classification and show that: (i) our models are capable of outperforming all systems in RumourEval 2017 and all but one system in RumourEval 2019 in terms of both macro-F 1 and GMR scores, and (ii) a more in-depth evaluation is nee"
2020.rdsm-1.4,S17-2084,0,0.0637505,"Missing"
2020.rdsm-1.4,S19-2197,0,0.0342867,"Missing"
2020.rdsm-1.4,S19-2147,0,0.0339006,"Missing"
2020.rdsm-1.4,S19-2195,0,0.0312349,"Missing"
2020.rdsm-1.4,S17-2083,0,0.0508777,"Missing"
2020.rdsm-1.4,D14-1162,0,0.0853261,"and Results 3.1 Experimental setup Techniques presented in Section 2 are explored in two types of classification models. We use implementations of resampling methods from the imbalanced-learn python toolkit (Lemaˆıtre et al., 2017). Feature-based classifiers Our feature-based approach is an adaptation of (Aker et al., 2017). We use Twitter-based features like number of re-tweets, presence of URLs and hashtags, number of followers for the user, among others. These features are then concatenated with a word vector representation for the tweets, using a pre-trained Twitter GloVe embedding model (Pennington et al., 2014). We train Random Forest (RF), Multi-Layer Perceptron (MLP) and Logistic Regression with stochastic gradient descent (LR-SGD) models using the scikit-learn python toolkit (Pedregosa et al., 2011). 2 Our implementation is available at https://github.com/YLi999/RumorStanceClassification 39 RF MLP LR-SGD BERT NT 0.000 ± 0.000 0.357 ± 0.139 0.000 ± 0.000 0.482 ± 0.057 RF MLP LR-SGD BERT NT 0.345 ± 0.012 0.466 ± 0.026 0.402 ± 0.009 0.584 ± 0.029 Average GMR and standard deviations RUS ROS SMOTE 0.513 ± 0.025 0.453 ± 0.039 0.229 ± 0.242 0.541 ± 0.046 0.428 ± 0.154 0.442 ± 0.078 0.519 ± 0.076 0.149 ±"
2020.rdsm-1.4,S17-2087,0,0.0658048,"Missing"
2020.rdsm-1.4,S17-2086,0,0.0325344,"Missing"
2020.rdsm-1.4,S19-2191,0,0.0207311,"Missing"
2021.acl-long.212,N19-4010,0,0.0550351,"Missing"
2021.acl-long.212,2020.cl-1.4,1,0.834217,"en idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the NC Compositionality dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in English (90 of which came from Red"
2021.acl-long.212,N19-1050,0,0.0201961,"most 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by compositional operations. In this paper we take advantage of the NCTTI dataset to observe whether vector representations obtained with different strategies correlate with human annotations at both type and token levels. 3 The Noun Compound Type and Token Idiomaticity dataset This section describes the procedure to create the NCTTI dataset and its main characteristics.2 3.1 2731 Source data compositional) after reading various sentences with this NC. To obtain more"
2021.acl-long.212,J90-1003,0,0.334862,"of context and of the collocated component. It is implemented as the sum of the individual vectors of the NC components, where each NC component is fed individually to the model as a sentence, referred to as NC outComp . On each case, we calculate two Spearman correlations with human judgments: at token level, using all the sentences for each language; and at type level, comparing the average cosine similarities of each NC with their compositionality scores at type level. We also compute correlations between the similarities and frequency-based data, namely the NC raw frequency, and the PPMI (Church and Hanks, 1990) between its component words, to verify whether they have any impact in these measures of idiomaticity. The frequency data were obtained from ukWaC, with 2.25B tokens in English (Baroni et al., 2009), and brWaC, containing 2.7B tokens in Portuguese (Wagner Filho et al., 2018). The results by Cordeiro et al. (2019) suggested that if the two components of an NC are processed as a single token unit (for instance, by explic9 This representation equivalent to the Avg Phrase used by Yu and Ettinger (2020). itly linking them with an underscore) the resulting static representation captures the NC idio"
2021.acl-long.212,J19-1001,1,0.069189,"evel, to determine the potential of an MWE to be idiomatic in general. Some of these approaches are based on the assumption that the * Equal contribution. distance between the representation of an MWE as a unit and the representation of the compositional combination of its components is an indication of the degree of idiomaticity: they are closer if the MWE is more compositional. Good performances are obtained even with non-contextualised word embeddings like word2vec (Mikolov et al., 2013), and vector operations like addition and multiplication (Mitchell and Lapata, 2010; Reddy et al., 2011; Cordeiro et al., 2019). Additionally, for some MWEs, there is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occ"
2021.acl-long.212,N19-1423,0,0.118084,"tially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models could improve results (e.g. for metaphor identification (Mao et al."
2021.acl-long.212,W15-0904,0,0.0253873,"dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully 1 Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI. 2 The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better"
2021.acl-long.212,N13-1092,0,0.0790478,"Missing"
2021.acl-long.212,D18-1060,0,0.0195993,"whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models could improve results (e.g. for metaphor identification (Mao et al., 2019)). This complementarity between non-contextualised and contex2730 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2730–2741 August 1–6, 2021. ©20"
2021.acl-long.212,W17-6615,0,0.0552581,"Missing"
2021.acl-long.212,D19-1630,0,0.0466687,"Missing"
2021.acl-long.212,P18-2055,0,0.0160492,"re is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with c"
2021.acl-long.212,S14-1021,0,0.0313475,"cores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully 1 Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI. 2 The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and"
2021.acl-long.212,P19-1378,0,0.0129047,"al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models could improve results (e.g. for metaphor identification (Mao et al., 2019)). This complementarity between non-contextualised and contex2730 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2730–2741 August 1–6, 2021. ©2021 Association for Computational Linguistics tualised models may be an indication that enough core idiomatic information may already be available at type level. Moreover, type-based compositionality prediction measures that perform well with static embeddings may also perform well for token-based prediction with contextualised mode"
2021.acl-long.212,S07-1009,0,0.0963826,"from the performance obtained per token. Our contributions can be summarised as: (1) building the NCTTI dataset with information about type and token idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the N"
2021.acl-long.212,S10-1002,0,0.0483478,"d per token. Our contributions can be summarised as: (1) building the NCTTI dataset with information about type and token idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the NC Compositionality datas"
2021.acl-long.212,W19-2004,0,0.198183,"ithub.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by comp"
2021.acl-long.212,C16-1069,0,0.0133153,"out type and token idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the NC Compositionality dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in Engli"
2021.acl-long.212,D14-1162,0,0.0850405,"he ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised baseline we used GloVe (Pennington et al., 2014) (the English official models with 300 dimensions and trained on 840 billion tokens, and the equivalent Portuguese model released by Hartmann et al. (2017)). The vector representations were obtained with the flairNLP framework (Akbik et al., 2019) using the models provided by the transformers library (Wolf et al., 2020). The representations of NCs (and their sentences) were obtained by averaging the word (or subword, if adopted by the model) embeddings. We used the concatenation of the three layers for ELMo and of 8 https://www.nyu.edu/projects/bowman/ multinli/ https://nlp.stanford.edu/projec"
2021.acl-long.212,P16-1019,0,0.0242545,"iguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models c"
2021.acl-long.212,N18-1202,0,0.0511458,"exts to observe how the interpretation of the NCs varies across sentences, and whether this correlates with the contextualised representations produced by various models. More specifically, we assume that, if models adequately incorporate contextual information, the standard deviations of the similarities between the NCs in different contexts should be correlated with those of the human annotators. 4.1 Models We evaluate four contextualised models: three BERT variants, based on the Transformers architecture (Vaswani et al., 2017), and ELMo, which learns word vectors using bidirectional LSTMs (Peters et al., 2018). For English we used the ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised b"
2021.acl-long.212,P16-2026,1,0.842874,"ain compositionality scores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully 1 Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI. 2 The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individ"
2021.acl-long.212,I11-1024,0,0.0984062,"vestigated at type level, to determine the potential of an MWE to be idiomatic in general. Some of these approaches are based on the assumption that the * Equal contribution. distance between the representation of an MWE as a unit and the representation of the compositional combination of its components is an indication of the degree of idiomaticity: they are closer if the MWE is more compositional. Good performances are obtained even with non-contextualised word embeddings like word2vec (Mikolov et al., 2013), and vector operations like addition and multiplication (Mitchell and Lapata, 2010; Reddy et al., 2011; Cordeiro et al., 2019). Additionally, for some MWEs, there is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish id"
2021.acl-long.212,D19-1410,0,0.0204799,"ontextual information, the standard deviations of the similarities between the NCs in different contexts should be correlated with those of the human annotators. 4.1 Models We evaluate four contextualised models: three BERT variants, based on the Transformers architecture (Vaswani et al., 2017), and ELMo, which learns word vectors using bidirectional LSTMs (Peters et al., 2018). For English we used the ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised baseline we used GloVe (Pennington et al., 2014) (the English official models with 300 dimensions and trained on 840 billion tokens, and the equivalent Portuguese model released by Hartmann et al. (2017)). The vector representations were obtained w"
2021.acl-long.212,2020.emnlp-main.365,0,0.0255985,"word vectors using bidirectional LSTMs (Peters et al., 2018). For English we used the ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised baseline we used GloVe (Pennington et al., 2014) (the English official models with 300 dimensions and trained on 840 billion tokens, and the equivalent Portuguese model released by Hartmann et al. (2017)). The vector representations were obtained with the flairNLP framework (Akbik et al., 2019) using the models provided by the transformers library (Wolf et al., 2020). The representations of NCs (and their sentences) were obtained by averaging the word (or subword, if adopted by the model) embeddings. We used the concatenation of the three layers for ELMo and of"
2021.acl-long.212,N19-1048,0,0.0612801,"Missing"
2021.acl-long.212,2020.acl-main.368,0,0.034856,"Missing"
2021.acl-long.212,L16-1362,0,0.0563567,"Missing"
2021.acl-long.212,Q19-1027,0,0.0191584,"roni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by compositional operations. In this paper we take advantage of the NCTTI dataset to observe whether vector representations obtained with different strategies correlate with human annotations at both typ"
2021.acl-long.212,W13-1005,0,0.0299096,"(90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully 1 Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI. 2 The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti. 2 Related Work Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various c"
2021.acl-long.212,E09-1086,0,0.0413724,"ally, for some MWEs, there is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016). Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and the"
2021.acl-long.212,L18-1686,1,0.834341,"uman judgments: at token level, using all the sentences for each language; and at type level, comparing the average cosine similarities of each NC with their compositionality scores at type level. We also compute correlations between the similarities and frequency-based data, namely the NC raw frequency, and the PPMI (Church and Hanks, 1990) between its component words, to verify whether they have any impact in these measures of idiomaticity. The frequency data were obtained from ukWaC, with 2.25B tokens in English (Baroni et al., 2009), and brWaC, containing 2.7B tokens in Portuguese (Wagner Filho et al., 2018). The results by Cordeiro et al. (2019) suggested that if the two components of an NC are processed as a single token unit (for instance, by explic9 This representation equivalent to the Avg Phrase used by Yu and Ettinger (2020). itly linking them with an underscore) the resulting static representation captures the NC idiomatic meaning. This is not surprising since by linking the two components we create a new word that would be treated by the model as completely independent of the preexisting component words. But such preprocessing may not be desirable or even feasible. In this sense the cont"
2021.acl-long.212,2020.emnlp-main.397,0,0.443606,"at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English. Regarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by compositional operations. In this paper we take advantage of the NCTTI dataset to observe whether vector representations obtained with different strategies correlate with human annotations at both type and token levels. 3 The Noun Compound Type and Token Idiomaticity dataset This section describes the procedure to create the NCTTI dataset and its main characteristics.2 3.1 2731 Source"
2021.acl-long.212,2020.lrec-1.471,0,0.018533,"levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020). Section 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses the evaluation of the performance of different word embeddings in detecting idiomaticity. We used as basis the English and Portuguese subsets of the NC Compositionality dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0"
2021.eacl-main.310,Q16-1037,0,0.0344375,"event-related potential (ERP) data showed that idiomatic expressions, especially those with a salient meaning (Giora, 1999), have processing advantages (Laurent et al., 2006; Rommers et al., 2013). In NLP, probing tasks have been useful in revealing to what extent contextualised models are capable of learning different linguistic properties (Conneau et al., 2018). They allow for more controlled settings, removing obvious biases and potentially confounding factors from evaluations, and allowing both the use of artificially constructed but controlled sentences and naturally occurring sentences (Linzen et al., 2016; Gulordava et al., 2018). In priming tasks, related stimuli are easier to process than unrelated ones. One assumption is that, for models, related stimuli would achieve greater similarity than unrelated stimuli. These tasks have been used, for instance, to evaluate how neural language models represent syntax (van Schijndel and Linzen, 2018; Prasad et al., 2019), and the preferences that they may display, such as the use of mainly lexical information in a lexical substitution task even if contextual information is available (Aina et al., 2019). Concerning pre-trained neural language models, wh"
2021.eacl-main.310,N19-1112,0,0.0193904,"imuli would achieve greater similarity than unrelated stimuli. These tasks have been used, for instance, to evaluate how neural language models represent syntax (van Schijndel and Linzen, 2018; Prasad et al., 2019), and the preferences that they may display, such as the use of mainly lexical information in a lexical substitution task even if contextual information is available (Aina et al., 2019). Concerning pre-trained neural language models, which produce contextualised word representations, analyses about their abilities have shown, for instance, that they can encode syntactic information (Liu et al., 2019) including long-distance subject– verb agreement (Goldberg, 2019). Regarding semantic knowledge, the results of various experiments suggest that BERT can somewhat represent semantic roles (Ettinger, 2020). However, its improvements appear mainly in core roles that may be predicted from syntactic representations (Tenney et al., 2019). Moreover, from the representations generated by BERT, ELMo and Flair (Akbik et al., 2018) for word sense disambiguation, only the clusters of BERT vectors seem to be related to word senses (Wiedemann et al., 2019), although in crosslingual alignment of ELMo embedd"
2021.eacl-main.310,W19-2004,0,0.0968498,"ave also been observed (Schuster et al., 2019). The use of contextualised models for representing MWEs has been reported with mixed results. Shwartz and Dagan (2019) evaluated different classifiers initialised with contextualised and non-contextualised embeddings in five tasks related to lexical composition (including the literality of NCs) and found that contextualised models, especially BERT, obtained better performance across all tasks. However, for capturing idiomaticity in MWEs, static models like word2vec (Mikolov et al., 2013) seem to have better performance than contextualised models (Nandakumar et al., 2019; King and Cook, 2018). These mixed results suggest that a controlled evaluation setup is needed to obtain comparable results across models and languages. Therefore, we have carefully designed probing tasks to assess the representation of NCs in vector space models. As the same word can have different representations even in related paraphrased contexts (Shi et al., 2019), we adopt paraphrases with minimal modifications to compare the idiomatic and literal representations of a given NC. 3 3.1 Materials and Methods Noun Compound Senses Dataset The Noun Compound Senses (NCS) dataset is based on"
2021.eacl-main.310,D14-1162,0,0.0917283,"ructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models. 1 Introduction Contextualised word representation models, like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), seem to represent words more accurately than static word embeddings like GloVe (Pennington et al., 2014), as they can encode different usages of a word. In fact, representations of a word in several contexts can be grouped in different clusters, which seem to be related to the various senses of the word (Schuster et al., 2019), and they can be used to match polysemous words in context to specific sense definitions (Chang and Chen, 2019). However, multiword expressions (MWEs) fall into a continuum of idiomaticity1 (Sag et al., 2002; Fazly 1 We understand idiomaticity as semantic opacity and its continuum as different degrees of opacity (Cruse, 1986). In this paper, we propose a set of probing mea"
2021.eacl-main.310,N18-1202,0,0.277947,"cal choice, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models. 1 Introduction Contextualised word representation models, like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), seem to represent words more accurately than static word embeddings like GloVe (Pennington et al., 2014), as they can encode different usages of a word. In fact, representations of a word in several contexts can be grouped in different clusters, which seem to be related to the various senses of the word (Schuster et al., 2019), and they can be used to match polysemous words in context to specific sense definitions (Chang and Chen, 2019). However, multiword expressions (MWEs) fall into a continuum of idiomaticity1 (Sag et al., 2002; Fazly 1 We understand idiomaticity as semantic opacity and i"
2021.eacl-main.310,K19-1007,0,0.0335974,"Missing"
2021.eacl-main.310,I11-1024,0,0.0274059,"olled evaluation setup is needed to obtain comparable results across models and languages. Therefore, we have carefully designed probing tasks to assess the representation of NCs in vector space models. As the same word can have different representations even in related paraphrased contexts (Shi et al., 2019), we adopt paraphrases with minimal modifications to compare the idiomatic and literal representations of a given NC. 3 3.1 Materials and Methods Noun Compound Senses Dataset The Noun Compound Senses (NCS) dataset is based on the NC Compositionality dataset, which contains NCs in English (Reddy et al., 2011), Portuguese and French (Cordeiro et al., 2019). Using the protocol by Reddy et al. (2011), human judgments were collected about the interpretation of each NC in 3 naturalistic corpus sentences. The task was to judge, for each NC, how literal the contributions of its component were for its meaning (e.g., “Is climate change truly/literally a change in climate?”). Each NC got a score, which was the average of the human judgments with a Likert scale from 0 (non-literal/idiomatic) to 5 (lit3552 eral/compositional).2 3.2 For the NCS dataset, a set of probing sentences for the 280 NCs in English and"
2021.eacl-main.310,D19-1410,0,0.0864267,"etermine how much for a given model an NC in context differs from the same NC out of context we measure sim(P4) in-out = cos(NC ⊂ S , NC ). We expect similarity scores to be higher in the NEU condition, given their semantically vague context, than for the NAT condition. 3.3 Calculating Embeddings We use as a baseline the static non-contextualised GloVe model (Pennington et al., 2014) and, for contextualised embeddings, four widely adopted models: ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and two BERT variants, DistilBERT (DistilB) (Sanh et al., 2019) and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019b). For all the contextualised models, we use their pre-trained weights publicly available through the Flair implementation5 . For GloVe, the English and Portuguese models described in Pennington et al. (2014) and Hartmann et al. (2017). For ELMo, we use the small model provided by Peters et al. (2018), and for Portuguese we adopt the weights provided by Quinta de Castro et al. (2018). For all BERT-based models, we used the multilingual models for both English and Portuguese.6 To have a single embedding for the whole sentence or its parts, e.g., the NC representation, we use the standard proce"
2021.eacl-main.310,N19-1048,0,0.0588836,"Missing"
2021.eacl-main.310,2020.acl-main.368,0,0.0421216,"Missing"
2021.eacl-main.310,D18-1499,0,0.0202023,"Missing"
2021.eacl-main.310,N19-1162,0,0.152769,"four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models. 1 Introduction Contextualised word representation models, like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), seem to represent words more accurately than static word embeddings like GloVe (Pennington et al., 2014), as they can encode different usages of a word. In fact, representations of a word in several contexts can be grouped in different clusters, which seem to be related to the various senses of the word (Schuster et al., 2019), and they can be used to match polysemous words in context to specific sense definitions (Chang and Chen, 2019). However, multiword expressions (MWEs) fall into a continuum of idiomaticity1 (Sag et al., 2002; Fazly 1 We understand idiomaticity as semantic opacity and its continuum as different degrees of opacity (Cruse, 1986). In this paper, we propose a set of probing measures to examine how accurately idiomaticity in MWEs, particularly in noun compounds (NCs), is captured in vector space models, focusing on some widely used representations. Inspired by the semantic priming paradigm (Neely e"
2021.eacl-main.310,D19-1113,0,0.0202599,"especially BERT, obtained better performance across all tasks. However, for capturing idiomaticity in MWEs, static models like word2vec (Mikolov et al., 2013) seem to have better performance than contextualised models (Nandakumar et al., 2019; King and Cook, 2018). These mixed results suggest that a controlled evaluation setup is needed to obtain comparable results across models and languages. Therefore, we have carefully designed probing tasks to assess the representation of NCs in vector space models. As the same word can have different representations even in related paraphrased contexts (Shi et al., 2019), we adopt paraphrases with minimal modifications to compare the idiomatic and literal representations of a given NC. 3 3.1 Materials and Methods Noun Compound Senses Dataset The Noun Compound Senses (NCS) dataset is based on the NC Compositionality dataset, which contains NCs in English (Reddy et al., 2011), Portuguese and French (Cordeiro et al., 2019). Using the protocol by Reddy et al. (2011), human judgments were collected about the interpretation of each NC in 3 naturalistic corpus sentences. The task was to judge, for each NC, how literal the contributions of its component were for its"
2021.eacl-main.310,Q19-1027,0,0.11139,"However, its improvements appear mainly in core roles that may be predicted from syntactic representations (Tenney et al., 2019). Moreover, from the representations generated by BERT, ELMo and Flair (Akbik et al., 2018) for word sense disambiguation, only the clusters of BERT vectors seem to be related to word senses (Wiedemann et al., 2019), although in crosslingual alignment of ELMo embeddings, clusters of polysemous words related to different senses have also been observed (Schuster et al., 2019). The use of contextualised models for representing MWEs has been reported with mixed results. Shwartz and Dagan (2019) evaluated different classifiers initialised with contextualised and non-contextualised embeddings in five tasks related to lexical composition (including the literality of NCs) and found that contextualised models, especially BERT, obtained better performance across all tasks. However, for capturing idiomaticity in MWEs, static models like word2vec (Mikolov et al., 2013) seem to have better performance than contextualised models (Nandakumar et al., 2019; King and Cook, 2018). These mixed results suggest that a controlled evaluation setup is needed to obtain comparable results across models an"
2021.eacl-main.310,L18-1686,1,0.827727,"averaging to obtain the NC embedding, as it is the standard procedure to represent not only MWEs but also out-of-vocabulary words, which are split into sub-tokens in contextualised models (Nandakumar et al., 2019; Wiedemann et al., 2019). However, we have also explored other methods to represent NCs in a single vector. First, we have incorporated type-level vectors of the NCs into a BERT model, inspired by compositionality prediction methods (Baldwin et al., 2003; Cordeiro et al., 2019). To do so, we annotated the target NCs in large English and Portuguese corpora (Baroni et al., 2009; Wagner Filho et al., 2018) and used attentive mimicking with onetoken-approximation (Schick and Sch¨utze, 2019, 2020b) to learn up to 500 contexts for each NC. These new vectors encode each NC in a single representation, therefore avoiding possible biases produced by the compositional operations. Then, we used BERTRAM (Schick and Sch¨utze, 2020a) to inject these type-level vectors in the BERT multilingual model. As expected, learning the vectors 3558 of the NCs as single tokens improved the representation of idiomatic expressions (see BERTRAM in Tables 2 and 4), decreasing the correlation with idiomaticity in P1 (e.g.,"
2021.findings-emnlp.294,J17-4005,0,0.0648311,"Missing"
2021.findings-emnlp.294,J19-1001,1,0.932177,"ls, program code to the meaning of the MWE (Venkatapathy and and associated processing scripts, including hyper- Joshi, 2005), or both (Reddy et al., 2011; Cordeiro parameters, publicly available in the interest of et al., 2019; Schulte im Walde et al., 2016). reproducibility and for subsequent reuse1 . While most of these target only English, some inThis paper is organised as follows: Section 2 clude scores for other languages such as German presents a discussion of related work. We then (Schulte im Walde et al., 2016), and French and present AStitchInLanguageModels consisting of Portuguese (Cordeiro et al., 2019). the novel MWE dataset and the two associated Existing datasets of compositionality that intasks in Section 3. We discuss our experiments clude context often add context automatically by and results for these two tasks in Section 4, before first selecting MWEs that are either only compresenting a discussion of the more interesting elepositional or only idiomatic. For instance, the ments of our findings in Section 5. We present our VNC-Tokens Dataset (Cook et al., 2008) consists conclusions and possible avenues of future work in of 53 English MWEs each with a maximum of Section 6. 100 sentence"
2021.findings-emnlp.294,N19-1423,0,0.149479,"y well in the one-shot and few-shot scenarios, but that there is significant scope for improvement in the zero-shot scenario. On the task of representing idiomaticity, we find that pre-training is not always effective, while finetuning could provide a sample efficient method of learning representations of sentences containing MWEs. 1 phrases are explicitly designed to be compositional both in non-contextual (Mitchell and Lapata, 2010; Mikolov et al., 2013b) and contextual embedding models. Pre-trained language models in particular exploit compositionality at both the word and sub-word levels (Devlin et al., 2019) to reduce the size of their vocabulary, which makes representing idiomatic phrases particularly challenging. The effective representation of idiomatic MWEs is critical for them to be correctly interpreted in downstream tasks. Such an improvement will benefit both classification-based problems (e.g. sentiment analysis) and sequence-to-sequence tasks (e.g. machine translation). To this end, we present a dataset consisting of naturally occurring sentences containing potentially idiomatic MWEs and two tasks aimed at evaluating language models’ ability to effectively detect and represent idiomatic"
2021.findings-emnlp.294,W15-0904,0,0.0603424,"Missing"
2021.findings-emnlp.294,2021.eacl-main.310,1,0.907158,"ability to effectively detect and represent idiomaticity. The primary contributions of this work are: 1. A novel dataset consisting of: Introduction and Motivation Pre-trained language models such as BERT (De- 2. vlin et al., 2019) and XLNet (Yang et al., 2019) have been widely used in a variety of Natural Language Processing tasks. Despite their success in multiple downstream applications, such as sentence classification (Zhang et al., 2019) and reading comprehension (Raffel et al., 2019), they are unable to effectively represent idiomatic multiword expressions (MWEs) (Yu and Ettinger, 2020; Garcia et al., 2021). Capturing idiomaticity is particularly challenging as the representations of words and 3464 (a) naturally occurring sentences (and two surrounding sentences) containing potentially idiomatic MWEs annotated with a finegrained set of meanings: compositional meaning, idiomatic meaning(s), proper noun and “meta usage”; (b) paraphrases for each meaning of each MWE; Two tasks aiming at evaluating i) a model’s ability to detect idiomatic usage, and ii) the effectiveness of sentence embeddings in representing idiomaticity. Table 1 provides details of these tasks and associated subtasks, each designe"
2021.findings-emnlp.294,2020.cogalex-1.9,1,0.689067,"ends be proportional to the number of instances of a token, representations of MWEs are often lacking. The second is that non-contextual type level representations are inherently limited as MWEs often have multiple meanings, as detailed in Section 2.1. While contextual embeddings can handle polysemy, they fail to fully capture the meaning of MWEs as discussed earlier. How contextual embeddings fair in comparison to their non-contextual predecessors is not entirely clear as Nandakumar et al. (2019) found that they do worse on some tasks while Shwartz and Dagan (2019) found that they do better. Hashempour and Villavicencio (2020) adopted the idiom principle (MWE as a single token) with contextual language models (specifically BERT), and found that this method does not benefit transformer-based pre-trained models. However, they did not introduce a new token to represent each MWE as is required during the training of non-contextual models built on the idiom principle, but instead replaced MWEs with a single token in the input and rely on BERT’s word-piece tokenizer. To the best of our knowledge this work is the first to introduce new tokens for MWEs into a contextual pre-trained language model (see Section 4.2). The tas"
2021.findings-emnlp.294,P16-1020,0,0.0203551,"mples with a fine-grained set of meangle units in learning embeddings (Mikolov et al., ings associated with each usage. We restrict our attention to noun compounds, a subset of idiomatic 2013b). This method was improved upon by use of an explicit disambiguation step prior to com- MWEs, sourced from the Noun Compound Senses (NCS) dataset (Cordeiro et al., 2019), which exposition (Kartsaklis et al., 2014), and by the joint learning of compositional and idiomatic embed- tends the dataset by Reddy et al. (2011). dings using a “compositionality scoring” func3.1 Data Collection and Annotation tion (Hashimoto and Tsuruoka, 2016). This “single token” method has the advantage of being rooted in A total of 12 judges were asked to collect examthe linguistic idiom principle (Sinclair et al., 1991), ples containing a list of MWEs occurring natuwhich postulates that humans process idioms by rally in context, in both English and Portuguese. treating them as a “single independent token”. For each MWE, judges were instructed to obtain 3466 7 to 10 examples of each meaning ( “Idiomatic”, “Non-Idiomatic”, “Proper Noun” and “Meta Usage”) where possible, with between 20 and 30 total examples for each MWE. We define “Meta Usage” to"
2021.findings-emnlp.294,S13-2025,0,0.0268761,"MWEs. Dagan (2019) showed, using six tasks, that conDespite the importance of the context surround1 ing an MWE, where available, context, in the form https://github.com/H-TayyarMadabushi /AStitchInLanguageModels of sentences containing MWEs, is available only 3465 Subtask B for those MWEs that are either idiomatic or compositional. This significant shortcoming makes it impossible to train models to learn to differentiate between the compositional and idiomatic usage of the same MWE. Finally, while existing datasets also provide paraphrases for the compositional and idiomatic meanings of MWEs (Hendrickx et al., 2013; Garcia et al., 2021), they are limited to having exactly one compositional and one idiomatic meaning, which is not always the case as is exemplified by the phrase “head hunter” which, while not having a literal usage, has multiple idiomatic meanings (i.e recruiter, baseball pitcher who aims for the head, and hunter). AStitchInLanguageModels is designed to alleviate these shortcomings, specifically: a) the lack of context sentences, b) the need for fine grained classification of MWEs, and a more complete set of paraphrases for all possible meanings of MWEs (Section 3). 2.2 Methods Despite bei"
2021.findings-emnlp.294,P14-2035,0,0.0303782,"ral distribu- usage in naturally occurring sentences along with the two surrounding sentences. We then annotated tional semantic models such as word2vec (Mikolov et al., 2013a) wherein MWEs were taken as sin- these examples with a fine-grained set of meangle units in learning embeddings (Mikolov et al., ings associated with each usage. We restrict our attention to noun compounds, a subset of idiomatic 2013b). This method was improved upon by use of an explicit disambiguation step prior to com- MWEs, sourced from the Noun Compound Senses (NCS) dataset (Cordeiro et al., 2019), which exposition (Kartsaklis et al., 2014), and by the joint learning of compositional and idiomatic embed- tends the dataset by Reddy et al. (2011). dings using a “compositionality scoring” func3.1 Data Collection and Annotation tion (Hashimoto and Tsuruoka, 2016). This “single token” method has the advantage of being rooted in A total of 12 judges were asked to collect examthe linguistic idiom principle (Sinclair et al., 1991), ples containing a list of MWEs occurring natuwhich postulates that humans process idioms by rally in context, in both English and Portuguese. treating them as a “single independent token”. For each MWE, judge"
2021.findings-emnlp.294,W06-1203,0,0.105845,"izer. To the best of our knowledge this work is the first to introduce new tokens for MWEs into a contextual pre-trained language model (see Section 4.2). The task of identifying idiomaticity in sentences was initially addressed by use of symbolic methods (Baldwin and Villavicencio, 2002; Sag et al., 2002), statistical properties of text such as mutual information (Lin, 1999), and latent semantic analysis (Baldwin et al., 2003). The subsequent adoption of distributional semantics led to the use of constituent word embeddings to determine the compositionality of phrases, such as in the work by Katz and Giesbrecht (2006) who 3 AStitchInLanguageModels: Dataset made use of the semantic similarity between the and Tasks distributional vectors associated with an MWE as a whole and those associated with its parts to de- To create a dataset and tasks aimed at improving language models’ ability to identify and capture termine compositionality. This is achieved by use idiomaticity, we first collected examples of MWE of a single token to represent an MWE. This trend continued with the introduction of neural distribu- usage in naturally occurring sentences along with the two surrounding sentences. We then annotated tion"
2021.findings-emnlp.294,P99-1041,0,0.439374,"not introduce a new token to represent each MWE as is required during the training of non-contextual models built on the idiom principle, but instead replaced MWEs with a single token in the input and rely on BERT’s word-piece tokenizer. To the best of our knowledge this work is the first to introduce new tokens for MWEs into a contextual pre-trained language model (see Section 4.2). The task of identifying idiomaticity in sentences was initially addressed by use of symbolic methods (Baldwin and Villavicencio, 2002; Sag et al., 2002), statistical properties of text such as mutual information (Lin, 1999), and latent semantic analysis (Baldwin et al., 2003). The subsequent adoption of distributional semantics led to the use of constituent word embeddings to determine the compositionality of phrases, such as in the work by Katz and Giesbrecht (2006) who 3 AStitchInLanguageModels: Dataset made use of the semantic similarity between the and Tasks distributional vectors associated with an MWE as a whole and those associated with its parts to de- To create a dataset and tasks aimed at improving language models’ ability to identify and capture termine compositionality. This is achieved by use idioma"
2021.findings-emnlp.294,schneider-etal-2014-comprehensive,0,0.0214862,"ntences using both pre-training and fine-tuning. textual pre-trained language models, capable of handling polysemy, continued to be unable to effectively handle idiomatic MWEs, although they tend to do better than their non-contextual predecessors. Further experiments with probing pre-trained language models across multiple languages have also confirmed this result (Yu and Ettinger, 2020; Garcia et al., 2021). 2.1 Existing Datasets Datasets of MWE annotated corpora include that associated with the PARSEME shared task (Savary et al., 2017) which focuses on verbal MWEs and the STREUSLE dataset (Schneider et al., 2014; Table 1: AStitchInLanguageModels Tasks: The two Schneider and Smith, 2015; Schneider et al., 2016) tasks and associated subtasks. which includes noun, verb, prepositional and posThis dataset and associated tasks have the po- sessive expressions including “semantic supertential to catalyse research into representing more senses”. However, most existing datasets associcomplex elements of language beginning with id- ated with compositionality of MWEs consist of iomaticity, thus ensuring a timely stitch in language isolated phrases, labelled with overall composimodels. We call this dataset and a"
2021.findings-emnlp.294,N15-1177,0,0.0255888,"guage models, capable of handling polysemy, continued to be unable to effectively handle idiomatic MWEs, although they tend to do better than their non-contextual predecessors. Further experiments with probing pre-trained language models across multiple languages have also confirmed this result (Yu and Ettinger, 2020; Garcia et al., 2021). 2.1 Existing Datasets Datasets of MWE annotated corpora include that associated with the PARSEME shared task (Savary et al., 2017) which focuses on verbal MWEs and the STREUSLE dataset (Schneider et al., 2014; Table 1: AStitchInLanguageModels Tasks: The two Schneider and Smith, 2015; Schneider et al., 2016) tasks and associated subtasks. which includes noun, verb, prepositional and posThis dataset and associated tasks have the po- sessive expressions including “semantic supertential to catalyse research into representing more senses”. However, most existing datasets associcomplex elements of language beginning with id- ated with compositionality of MWEs consist of iomaticity, thus ensuring a timely stitch in language isolated phrases, labelled with overall composimodels. We call this dataset and associated tasks tionality scores (Venkatapathy and Joshi, 2005; AStitchInLa"
2021.findings-emnlp.294,L16-1362,0,0.0206534,", labelled with overall composimodels. We call this dataset and associated tasks tionality scores (Venkatapathy and Joshi, 2005; AStitchInLanguageModels, and make the dataset, Biemann and Giesbrecht, 2011; Farahmand et al., the associated splits for each task, pre-training data, 2015), scores of how individual words contribute pre-trained and fine-tuned models, program code to the meaning of the MWE (Venkatapathy and and associated processing scripts, including hyper- Joshi, 2005), or both (Reddy et al., 2011; Cordeiro parameters, publicly available in the interest of et al., 2019; Schulte im Walde et al., 2016). reproducibility and for subsequent reuse1 . While most of these target only English, some inThis paper is organised as follows: Section 2 clude scores for other languages such as German presents a discussion of related work. We then (Schulte im Walde et al., 2016), and French and present AStitchInLanguageModels consisting of Portuguese (Cordeiro et al., 2019). the novel MWE dataset and the two associated Existing datasets of compositionality that intasks in Section 3. We discuss our experiments clude context often add context automatically by and results for these two tasks in Section 4, bef"
2021.findings-emnlp.294,Q19-1027,0,0.098411,"o tasks in Section 4, before first selecting MWEs that are either only compresenting a discussion of the more interesting elepositional or only idiomatic. For instance, the ments of our findings in Section 5. We present our VNC-Tokens Dataset (Cook et al., 2008) consists conclusions and possible avenues of future work in of 53 English MWEs each with a maximum of Section 6. 100 sentences extracted from the BNC, while Tu and Roth (2012) collected 1,348 sentences associ2 Related work ated with 23 verb phrases annotated as composiThe problems posed by MWEs to NLP models have tional and idiomatic. Shwartz and Dagan (2019) been known for some time (Sag et al., 2002; Con- focused on a subset of noun compounds that are stant et al., 2017; Shwartz and Dagan, 2019). For only compositional or idiomatic from the dataset instance, Sag et al. (2002) refer to the idiomatic- provided by Reddy et al. (2011) and automatically ity problem and place the need for effective pro- added sentences from Wikipedia. Finally, the NCS cessing of MWEs on par with that for word sense Dataset (Garcia et al., 2021) consists of 280 Endisambiguation to be able to effectively process glish and 180 Portuguese MWEs, annotated with text. While"
2021.findings-emnlp.294,S12-1010,0,0.0248869,"two associated Existing datasets of compositionality that intasks in Section 3. We discuss our experiments clude context often add context automatically by and results for these two tasks in Section 4, before first selecting MWEs that are either only compresenting a discussion of the more interesting elepositional or only idiomatic. For instance, the ments of our findings in Section 5. We present our VNC-Tokens Dataset (Cook et al., 2008) consists conclusions and possible avenues of future work in of 53 English MWEs each with a maximum of Section 6. 100 sentences extracted from the BNC, while Tu and Roth (2012) collected 1,348 sentences associ2 Related work ated with 23 verb phrases annotated as composiThe problems posed by MWEs to NLP models have tional and idiomatic. Shwartz and Dagan (2019) been known for some time (Sag et al., 2002; Con- focused on a subset of noun compounds that are stant et al., 2017; Shwartz and Dagan, 2019). For only compositional or idiomatic from the dataset instance, Sag et al. (2002) refer to the idiomatic- provided by Reddy et al. (2011) and automatically ity problem and place the need for effective pro- added sentences from Wikipedia. Finally, the NCS cessing of MWEs o"
C16-3004,P12-3024,0,0.0270183,"Missing"
C16-3004,P10-1064,0,0.0826527,"Missing"
C16-3004,2015.iwslt-papers.4,1,0.838642,"Missing"
C16-3004,L16-1582,1,0.769073,"n entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence, word and document-leve"
C16-3004,2014.eamt-1.21,1,0.770324,"word-level QE over the years. An application that can benefit from word-level QE is spotting errors (incorrect words) in a post-editing/revision scenario. A recent variant of this task is quality prediction at the level of phrases (Logacheva and L.Specia, 2015; Blain et al., 2016), where a phrase can be defined in different ways, e.g. using the segmentation from a statistical MT decoder in WMT16 (Bojar et al., 2016). Document-level QE has received much less attention than the other levels. This task consists in predicting a single quality label for an entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et a"
C16-3004,W15-4916,1,0.786531,"Missing"
C16-3004,2015.iwslt-papers.11,0,0.0579597,"e quality label for an entire document, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence"
C16-3004,2014.eamt-1.22,1,0.76135,"arly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction level has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012 (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016). Wh"
C16-3004,P10-1063,0,0.198553,"these instances and quality predictions are then produced by the model. Figure 2: QE model prediction. QE is a reasonably new field, but over the last decade has become particularly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction level has been covered in shared tasks organised by the Workshop o"
C16-3004,P13-4014,1,0.892777,"Missing"
C16-3004,P15-4020,1,0.730678,"an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). It is most useful for gisting purposes, where post-editing is not an option. Two shared tasks on document-level QE were organised at WMT15 and WMT16. An open research question when it comes to document-level QE is to define effective quality labels for entire documents (Scarton et al., 2015). A few QE frameworks have been proposed in the last couple of years, namely (Gonz`alez et al., 2012; Specia et al., 2013; Servan et al., 2015; Logacheva et al., 2016; Specia et al., 2015). Q U E ST++ (Specia et al., 2015)1 is the most widely used one. It is a significantly refactored and expanded version of Q U E ST (Specia et al., 2013). It has been used as the official baseline system during all editions of the WMT shared task on QE (WMT12-WMT16) and is often the starting point upon which other participants build their systems, particularly for feature extraction. It has two main modules: feature extraction and 1 https://github.com/ghpaetzold/questplusplus 15 machine learning. The feature extraction module can operate at sentence, word and document-level and includes the ext"
C16-3004,2011.eamt-1.12,1,0.848372,"re used to train the QE model are extracted from these instances and quality predictions are then produced by the model. Figure 2: QE model prediction. QE is a reasonably new field, but over the last decade has become particularly popular in the area of Machine Translation (MT). With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency by filtering out segments which would require more effort or time to correct than to translate from scratch (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory (He et al., 2010), selecting the best translation from multiple MT systems (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Sentence-level QE represents the vast majority of existing work. It has been addressed as a supervised machine learning task using a variety of algorithms to train models from examples of sentence translations annotated with quality labels (e.g. 1 to 5 likert scores). This prediction leve"
C16-3004,2015.eamt-1.17,1,\N,Missing
D19-3009,N18-1063,0,0.14836,"Missing"
D19-3009,P17-1104,0,0.0138124,"pope (n) × rope (n) pope (n) + rope (n) 1 X = fope (n) k fope (n) = Fope Word-level Analysis and QE Features n=[1,..,k] Although Xu et al. (2016) indicate that only precision should be considered for the deletion operation, we follow the Java implementation that uses F1 score for all operations in corpus-level SARI. SAMSA measures structural simplicity (i.e. sentence splitting). This is in contrast to SARI, which is designed to evaluate simplifications involving paraphrasing. EASSE re-factors the original SAMSA implementation2 with some modifications: (1) an internal call to the TUPA parser (Hershcovich et al., 2017), which generates the semantic annotations for each original sentence; (2) a modified version of the monolingual word aligner (Sultan et al., 2014) that is compatible with Python 3, and uses Stanford CoreNLP (Manning et al., 2014)3 through their official Python interface; and (3) a single function call to get a SAMSA score instead of running a series of scripts. Quality Estimation Features Traditional automatic metrics used for SS rely on the existence and quality of references, and are often not enough to analyse the complex process of simplification. QE 1 https://github.com/mjpost/sacreBLEU"
D19-3009,Q14-1018,0,0.0285492,"ate that only precision should be considered for the deletion operation, we follow the Java implementation that uses F1 score for all operations in corpus-level SARI. SAMSA measures structural simplicity (i.e. sentence splitting). This is in contrast to SARI, which is designed to evaluate simplifications involving paraphrasing. EASSE re-factors the original SAMSA implementation2 with some modifications: (1) an internal call to the TUPA parser (Hershcovich et al., 2017), which generates the semantic annotations for each original sentence; (2) a modified version of the monolingual word aligner (Sultan et al., 2014) that is compatible with Python 3, and uses Stanford CoreNLP (Manning et al., 2014)3 through their official Python interface; and (3) a single function call to get a SAMSA score instead of running a series of scripts. Quality Estimation Features Traditional automatic metrics used for SS rely on the existence and quality of references, and are often not enough to analyse the complex process of simplification. QE 1 https://github.com/mjpost/sacreBLEU https://github.com/eliorsulem/SAMSA 3 https://stanfordnlp.github.io/ stanfordnlp/corenlp_client.html 2 4 https://github.com/mmautner/ readability 5"
D19-3009,P14-5010,0,0.00388726,"the Java implementation that uses F1 score for all operations in corpus-level SARI. SAMSA measures structural simplicity (i.e. sentence splitting). This is in contrast to SARI, which is designed to evaluate simplifications involving paraphrasing. EASSE re-factors the original SAMSA implementation2 with some modifications: (1) an internal call to the TUPA parser (Hershcovich et al., 2017), which generates the semantic annotations for each original sentence; (2) a modified version of the monolingual word aligner (Sultan et al., 2014) that is compatible with Python 3, and uses Stanford CoreNLP (Manning et al., 2014)3 through their official Python interface; and (3) a single function call to get a SAMSA score instead of running a series of scripts. Quality Estimation Features Traditional automatic metrics used for SS rely on the existence and quality of references, and are often not enough to analyse the complex process of simplification. QE 1 https://github.com/mjpost/sacreBLEU https://github.com/eliorsulem/SAMSA 3 https://stanfordnlp.github.io/ stanfordnlp/corenlp_client.html 2 4 https://github.com/mmautner/ readability 50 Figure 1: Example of automatic transformation annotations based on word alignment"
D19-3009,P12-1107,0,0.343006,"Missing"
D19-3009,W18-7005,1,0.908611,"Missing"
D19-3009,Q16-1029,0,0.497176,"rious metrics and features above and on how a particular SS output fares against reference simplifications. Through experiments, we show that these functionalities allow for better comparison and understanding of the performance of SS systems. 1 2 Introduction Sentence Simplification (SS) consists of modifying the content and structure of a sentence to improve its readability while retaining its original meaning. For automatic evaluation of a simplification output, it is common practice to use machine translation (MT) metrics (e.g. BLEU (Papineni et al., 2002)), simplicity metrics (e.g. SARI (Xu et al., 2016)), and readability metrics (e.g. FKGL (Kincaid et al., 1975)). Most of these metrics are available in individual code repositories, with particular software requirements that sometimes differ even in programming language (e.g. corpus-level SARI is implemented in Java, whilst sentence-level SARI is available in both Java and Python). Other metrics (e.g. SAMSA (Sulem et al., 2018b)) suffer from insufficient documentation or require executing multiple scripts with hard-coded paths, which prevents researchers from using them. 2.1 Package Overview Automatic Corpus-level Metrics Although human judge"
D19-3009,P14-1041,0,0.115191,"n TurkCorpus according to SARI. However, it gets the lowest SAMSA score, and the third to last BLEU score. PBSMT-R is the best in terms of these two metrics. Finally, across all metrics, the Reference stills gets the highest values, with significant differences from the top performing systems. Sentence Simplification Systems EASSE provides access to various SS system outputs that follow different approaches for the task. For instance, we include those that rely on phrasebased statistical MT, either by itself (e.g. PBSMTR (Wubben et al., 2012)), or coupled with semantic analysis, (e.g. Hybrid (Narayan and Gardent, 2014)). We also include SBSMT-SARI (Xu et al., 2016), which relies on syntax-based statistical MT; D RESS -L S (Zhang and Lapata, 2017), a neural model using the standard encoder-decoder architecture with attention combined with reinforcement learning; and DMASS-DCSS (Zhao et al., 2018), the current state-of-the-art in the TurkCorpus, which is based on the Transformer architecture (Vaswani et al., 2017). Word-level Transformations In order to better understand the previous results, we use the wordlevel annotations of text transformations (Table 3). Since SARI was design to evaluate mainly paraphras"
D19-3009,D17-1062,0,0.351654,"rate, those that display lexical simplifications, among others. Each of these aspects is illustrated with 10 instances. An example of the report can be viewed at https: //github.com/feralvam/easse/blob/ master/demo/report.gif. 3.2 3 Table 2: Comparison of systems’ performance based on automatic metrics. Automatic Metrics For illustration purposes, we compare systems’ outputs using BLEU and SARI in TurkCorpus (with 8 manual simplification references), and SAMSA in HSplit. For calculating Reference values in Table 2, we sample one of the 8 human references for each instance as others have done (Zhang and Lapata, 2017). When reporting SAMSA scores, we only use the first 70 sentences of TurkCorpus that also appear in HSplit.7 This allows us to compute Reference scores for instances that contain structural simplifications (i.e. sentence splits). We calculate SAMSA scores for each of the four manual simplifications in HSplit, and choose the highest as an upper-bound Reference value. The results for all three metrics are shown in Table 2. TurkCorpus Experiments We collected publicly available outputs of several SS systems (Sec. 3.1) to evaluate their performance using the functionalities available in EASSE. In"
D19-3009,D18-1355,0,0.106641,"ems. Sentence Simplification Systems EASSE provides access to various SS system outputs that follow different approaches for the task. For instance, we include those that rely on phrasebased statistical MT, either by itself (e.g. PBSMTR (Wubben et al., 2012)), or coupled with semantic analysis, (e.g. Hybrid (Narayan and Gardent, 2014)). We also include SBSMT-SARI (Xu et al., 2016), which relies on syntax-based statistical MT; D RESS -L S (Zhang and Lapata, 2017), a neural model using the standard encoder-decoder architecture with attention combined with reinforcement learning; and DMASS-DCSS (Zhao et al., 2018), the current state-of-the-art in the TurkCorpus, which is based on the Transformer architecture (Vaswani et al., 2017). Word-level Transformations In order to better understand the previous results, we use the wordlevel annotations of text transformations (Table 3). Since SARI was design to evaluate mainly paraphrasing transformations, the fact that SBSMTSARI is the best at performing replacements and second place in copying explains its high SARI score. DMASS-DCSS is second best in replacements, while PBSMT-R (which achieved the highest BLEU score) is the best at copying. Hybrid is the best"
D19-3009,I17-3001,1,0.857394,"for each operation (ope ∈ {add, del, keep}) and n-gram order, precision pope (n), recall rope (n) and F1 fope (n) scores are calculated. These are then averaged over the n-gram order to get the overall operation F1 score Fope : 2.2 Word-level Transformation Analysis EASSE includes algorithms to determine which specific text transformations a SS system performs more effectively. This is done based on word-level alignment and analysis. Since there is no available simplification dataset with manual annotations of the transformations performed, we re-use the annotation algorithms from MASSAlign (Paetzold et al., 2017). Given a pair of sentences (e.g. original and system output), the algorithms use word alignments to identify deletions, movements, replacements and copies (see Fig. 1). This process is prone to some errors: when compared to manual labels produced by four annotators in 100 original-simplified pairs, the automatic algorithms achieved a micro-averaged F1 score of 0.61 (Alva-Manchego et al., 2017). We generate two sets of automatic word-level annotations: (1) between the original sentences and their reference simplifications, and (2) between the original sentences and their automatic simplificati"
D19-3009,P02-1040,0,0.111759,"lly, EASSE generates easy-to-visualise reports on the various metrics and features above and on how a particular SS output fares against reference simplifications. Through experiments, we show that these functionalities allow for better comparison and understanding of the performance of SS systems. 1 2 Introduction Sentence Simplification (SS) consists of modifying the content and structure of a sentence to improve its readability while retaining its original meaning. For automatic evaluation of a simplification output, it is common practice to use machine translation (MT) metrics (e.g. BLEU (Papineni et al., 2002)), simplicity metrics (e.g. SARI (Xu et al., 2016)), and readability metrics (e.g. FKGL (Kincaid et al., 1975)). Most of these metrics are available in individual code repositories, with particular software requirements that sometimes differ even in programming language (e.g. corpus-level SARI is implemented in Java, whilst sentence-level SARI is available in both Java and Python). Other metrics (e.g. SAMSA (Sulem et al., 2018b)) suffer from insufficient documentation or require executing multiple scripts with hard-coded paths, which prevents researchers from using them. 2.1 Package Overview A"
D19-3009,C10-1152,0,0.234814,"crowdworkers to simplify 2,359 original sentences extracted from PWKP to collect multiple simplification references for each one. This dataset was then randomly split into tuning (2,000 instances) and test (359 instances) sets. The test set only contains 1-to-1 alignments, mostly with instances of paraphrasing and deletion. Each original sentence in TurkCorpus has 8 simplified references. As such, it is better suited for computing SARI and multireference BLEU scores. Access to Test Datasets EASSE provides access to three publicly available datasets for automatic SS evaluation (Table 1): PWKP (Zhu et al., 2010), TurkCorpus (Xu et al., 2016), and HSplit (Sulem et al., 2018a). All of them consist of the data from the original datasets, which are sentences extracted from English Wikipedia (EW) articles. EASSE can also evaluate system’s outputs in other custom datasets provided by the user. HSplit Sulem et al. (2018a) recognised that existing EW-based datasets did not contain sufficient instances of sentence splitting. As such, they collected four reference simplifications of this transformation for all 359 original sentences in the TurkCorpus test set. Even though SAMSA’s computation does not require a"
D19-3009,W18-6319,0,0.0340117,"Missing"
D19-3009,D18-1081,0,0.445546,"Missing"
E17-2057,N15-1124,1,0.941061,"slation quality and the actual quality of translated documents. Including such weights in the construction of a gold standard potentially invalidates the human evaluation, and is unfortunately very likely to exaggerate the apparent performance of some systems while under-rewarding others. 357 0.2 0.1 3 0.0 −0.2 0 10 20 30 40 Alternate Human Gold Standard A recent development in human evaluation of MT is direct assessment (“DA”), a human assessment shown to yield highly replicable segment-level scores, by combination of a minimum of 15 repeat human assessments per translation into mean scores (Graham et al., 2015). Human adequacy assessments are collected via a 0–100 rating scale that facilitates reliable quality control of crowd-sourcing. Document-level DA scores are computed by repeat assessment of the individual segments within a given document, computation of the mean score for each segment (micro-average), and finally, combination of the mean segment scores into an overall mean document score (macro-average).2 DA assessments are carried out by comparison of a given MT output segment (rendered in black) with a human-generated reference translation (in gray), and human annotators rate the degree to"
E17-2057,C16-1294,1,0.865252,"hree participating sys5 Post-editing cost estimates are based on 0.06 and 0.12 Euro per source document word converted to USD$. Further details provided by the post-editor in relation to estimates can be found at https://github.com/ygraham/ eacl2017 4 Variance in numbers of repeat assessments per document is due to sentences of all documents being sampled without preference for documents made up of larger numbers of sentences. 359 RTM-FS+PLS-TREE GRAPH-DISC BASE-EMB-GP BASELINE RTM-FS-SVR DA WMT-16 0.38 0.32 0.31 0.26 0.23 0.36 0.26 0.39 0.29 0.29 it in document-level QE evaluation therefore. Graham et al. (2016a) provide an investigation into reference bias in monolingual evaluation of MT and despite the risk of reference bias that DA adequacy could potentially encounter, experiment results show no evidence of reference bias. Human assessors of MT appear to genuinely read and compare the meaning of the reference translation and the MT output, as requested with DA, applying their human intelligence to the task in a reliable way, and are not overly influenced by the generic reference. Although DA fluency could still have its own applications, for the purpose of evaluating MT or MT QE, this additional"
E17-2057,P15-1174,1,0.837171,"ments (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system as good as the baseline system when evaluated with MAE. The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure, MAE, however, as outlined in Graham (2015). When evaluated with the Pearson correlation, such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison"
E17-2057,E06-1032,0,0.076271,"m (2015). When evaluated with the Pearson correlation, such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison-Burch et al., 2006). Consequently, for WMT-16, the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate (HTER) (Snover et al., 2006) scores assigned to a given document. Scores were produced via two human post-editing steps: firstly, sentences within a given MT-output document were post-edited independent of other sentences in that document, producing post-edition 1 (P E1 ). Secondly, P E1 sentences were concatenated to form a documentlevel translation, and post-edited a second time by the same annotator, with the aim of isolating errors only identifiabl"
E17-2057,W07-0718,0,0.0579243,"proving Evaluation of Document-level Machine Translation Quality Estimation Yvette Graham Dublin City University Qingsong Ma Chinese Academy of Sciences Timothy Baldwin University of Melbourne yvette.graham@dcu.ie maqingsong@ict.ac.cn tb@ldwin.net Qun Liu Dublin City University Carla Parra Dublin City University Carolina Scarton University of Sheffield qun.liu@dcu.ie carla.parra@adaptcentre.ie c.scarton@sheffield.ac.uk Abstract subjective, making high IAA difficult to achieve. For example, in past large-scale human evaluations of MT, low IAA levels have been highlighted as a cause of concern (Callison-Burch et al., 2007; Bojar et al., 2016). Such problems cause challenges not only for evaluation of MT systems, but also for MT quality estimation (QE), where the ideal gold standard comprises human assessment. Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for machine translation (MT). We demonstrate the degree to which MT system rankings are dependent on weight"
E17-2057,2006.amta-papers.25,0,0.11857,"ely be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison-Burch et al., 2006). Consequently, for WMT-16, the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate (HTER) (Snover et al., 2006) scores assigned to a given document. Scores were produced via two human post-editing steps: firstly, sentences within a given MT-output document were post-edited independent of other sentences in that document, producing post-edition 1 (P E1 ). Secondly, P E1 sentences were concatenated to form a documentlevel translation, and post-edited a second time by the same annotator, with the aim of isolating errors only identifiable when more context is available, to produce post-edition 2 (P E2 ). Next, two translation edit rate (TER) scores were computed by: (1) comparing the document-level MT outp"
E17-2057,W11-2107,0,0.0349009,"machine translation (MT), human assessment is more 356 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 356–361, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Background label, G, as follows: Document-level QE (Soricut and Echihabi, 2010) is a relatively new area, with only two shared tasks taking place to date (Bojar et al., 2015; Bojar et al., 2016). In WMT-15, gold standard labels took the form of automatic metric scores for documents (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system as good as the baseline system when evaluated with MAE. The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure, MAE, however, as outlined in Graham (2015). When evaluated with the Pearson correlation, s"
E17-2057,P10-1063,0,0.0362197,"(IAA) enable the likelihood of replicability to be taken into account, were an evaluation to be repeated with a distinct set of human annotators. One approach to achieving high IAA is through the development of a strict set of annotation guidelines, while for machine translation (MT), human assessment is more 356 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 356–361, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Background label, G, as follows: Document-level QE (Soricut and Echihabi, 2010) is a relatively new area, with only two shared tasks taking place to date (Bojar et al., 2015; Bojar et al., 2016). In WMT-15, gold standard labels took the form of automatic metric scores for documents (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system"
E17-2057,D14-1020,1,0.877386,"ct assessment (DA) and original gold standard (WMT-16 QE English to Spanish) tems, while under-rewarding two other systems. Notably, system GRAPH-DISC, which includes discourse features learned from document-level features, achieves a higher correlation when evaluated with DA compared to the original gold standard. Differences in correlations are small, however, and can’t be interpreted as differences in performance without significance testing. Differences in dependent correlations showed no significant difference for all pairs of competing systems according to Williams test (Williams, 1959; Graham and Baldwin, 2014). 3.3 4 Conclusion Methodological concerns were raised with respect to optimization of weights employed in construction of document-level QE gold standards in WMT-16. We demonstrated the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard. Experiments showed with respect to the alternate gold standard we propose, direct assessment (DA), scores for documents are highly reliable, achieving a correlation of above 0.9 in a self-replication experiment. Finally, DA resulted in a substantial estimated cost reduction, with the original post-edi"
E17-2057,W13-2305,1,0.877135,"by qualitycontrolled crowd-sourcing in two separate data collection runs (Runs A and B) on Mechanical Turk, and compare scores for individual documents collected in each run. Quality control is carried out by inclusion of pairs of genuine MT outputs and automatically degraded versions of them (bad references) within 100-translation HITs, before a difference of means significance test is applied to the ratings belonging to a given worker. The resulting p-value is employed as an estimate of the reliability of a given human assessor to accurately distinguish between the quality of translations (Graham et al., 2013; Graham et al., 2014). Table 1 shows numbers of judgments collected in total for each data collection run on Mechanical Turk, including numbers of assessments before and after quality control filtering, where only data belonging to workers with a p-value below 0.05 were retained. Figure 2 shows the correlation between document-level DA scores collected in Run A with scores produced in Run B, where, for Run B, repeat assessments are down-sampled to show the increasing correspondence between scores as ever-increasing numbers of repeat assessments are collected for a given document. Correlation"
E17-2057,E14-1047,1,\N,Missing
E17-2057,W16-2301,1,\N,Missing
I17-1030,E17-2006,1,0.875786,"Missing"
I17-1030,P02-1040,0,0.100406,"thermore, we use a more reliable (professionally created) corpus and our approach is more flexible as we do not rely on syntactic parse trees at test time. recent work that relies on the professionally edited Newsela corpus (Xu et al., 2015). Simple English Wikipedia Zhu et al. (2010) propose a syntax-based translation model for TS that learns operations over the parse trees of the complex sentences. They outperform several baselines in terms of Flesch index. Coster and Kauchak (2011b) train a phrase-based machine translation (PBMT) system and obtain significant improvements in terms of BLEU (Papineni et al., 2002) over a baseline. Coster and Kauchak (2011a) extend a PBMT model to include phrase deletion and outperform Coster and Kauchak (2011b). Wubben et al. (2012) also train a PBMT system for TS with a dissimilarity-based re-ranking heuristic, outperforming Zhu et al. (2010) in terms of BLEU. Narayan and Gardent (2014) built TS systems by combining discourse representation structures with a PBMT model, which outperforms previous approaches. Xu et al. (2016) modify a syntax-based MT system in order to use a new metric – SARI – for optimization and to include special rules for paraphrasing. Although th"
I17-1030,W03-1004,0,0.14258,"el we implement as a baseline in this paper is equivalent to that in Zhang et al. (2017) without the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative form"
I17-1030,E17-3017,0,0.038702,"Missing"
I17-1030,P16-2055,1,0.841416,"s are learned from a more informed labeled FA and JB contributed equally to this paper. 295 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 295–305, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP dataset of natural simplifications, and can then be applied in a controlled way, e.g., in adaptive simplification scenarios that prioritize different ways of simplifying (e.g. compression or sentence splitting) depending on a particular user’s needs. The only previous work on TS via explicitly predicting simplification operations is that by Bingel and Søgaard (2016), who create training data from comparable text to label entire syntactic units and train a sequence labeling model to predict deletions and phrase substitutions in a complex sentence. Our approach is different in that it captures a larger variety of operations in a more global fashion, by using sentence-wide word alignments rather than surface heuristics. Furthermore, we use a more reliable (professionally created) corpus and our approach is more flexible as we do not rely on syntactic parse trees at test time. recent work that relies on the professionally edited Newsela corpus (Xu et al., 20"
I17-1030,W11-1603,0,0.0232335,"hout the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative formulations of the original text. With a few exceptions (by the neural model), they tend to"
I17-1030,W16-2323,0,0.0609456,"Missing"
I17-1030,W11-1601,0,0.796852,"Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS s"
I17-1030,N10-1063,0,0.0542761,"nt to that in Zhang et al. (2017) without the lexical constraints, while the statistical model is equivalent to the one in Coster and Kauchak (2011b). 3 corpus, which only aligns different versions of the same document, we first align sentences using the algorithms described in (Paetzold and Specia, 2016b). Their algorithms search for the best alignment path between the paragraphs and sentences of parallel documents based on TF-IDF cosine similarity and an incremental vicinity search range. They address limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011b; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for (semi-) supervised training, allowing long-distance alignment skips, and capturing N-to-N alignments. The alignments produced are categorized as: Simplification via End-to-End Models In addition to requiring large amounts of training data, MT-based approaches to TS are limited because of their black-box way of addressing the problem. As we are going to show in this section, standard end-to-end systems without special adaptation to TS do not succeed in learning alternative formulations of the original text. With a few excep"
I17-1030,P11-2117,0,0.827854,"Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS s"
I17-1030,2006.amta-papers.25,0,0.237758,"Missing"
I17-1030,W13-2901,0,0.0320694,"Missing"
I17-1030,Q14-1018,0,0.0483654,"Missing"
I17-1030,P07-2045,0,0.00635879,"t audience rather than research) resource to date. • Identical: The alignment is one-to-one and the sentences are exactly the same (96,909 pairs across all adjacent levels). • 1-to-1: The alignment is one-to-one and the original-simplified sentences are different (130,790 pairs across all adjacent levels). • Split: The alignment is 1-to-N (42,545 pairs across all adjacent levels). • Join: The alignment is N-to-1 (7,962 pairs across all adjacent levels). Translation Models. We built two types of models using state-of-the-art MT-based approaches: a phrase-based statistical MT model using Moses (Koehn et al., 2007),3 and a neural MT model using Nematus (Sennrich et al., 2017).4 The Neural Text Simplification tool (NTS) made available by Nisioi et al. (2017) was also used for comparison.5 For our translation-based experiments, we consider two combinations of sentence alignments, using (i) only one-to-one alignments (1-to-1) (130,970 sentence pairs), and (ii) all alignments (all), i.e., the entire sentence-aligned corpus with identical, 1-to-1, split and join alignments (278,206 sentence pairs). The first type of data (1to-1) is the focus of this paper (see §4). The latter variant is included in the exper"
I17-1030,D11-1038,0,0.271991,"Missing"
I17-1030,P12-1107,0,0.636948,"Missing"
I17-1030,P14-1041,0,0.764692,"resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotat"
I17-1030,Q15-1021,0,0.54195,"f.alva,g.h.paetzold,c.scarton,l.specia}@sheffield.ac.uk bingel@di.ku.dk (Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extens"
I17-1030,P17-2014,0,0.294853,"m given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degr"
I17-1030,Q16-1029,0,0.385239,"m favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These compl"
I17-1030,D17-1062,0,0.328819,"earning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural models for end-to-end TS systems that bear resemblance to MT models (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Zhang et al., 2017; Nisioi et al., 2017). Abstract Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sente"
I17-1030,C10-1152,0,0.835255,"bingel@di.ku.dk (Bingel and Søgaard, 2016) or the identification of difficult words (Paetzold and Specia, 2016a), and typically encodes relevant simplification operations as discrete labels on tokens. The other variant includes more general, higher-level types of simplifications that often entail the rephrasing or re-structuring of sentences, with content added or removed. These “natural” simplifications are often created for end-users rather than for research purposes. Examples of the latter simplification resources include the Newsela (Xu et al., 2015) and Simple English Wikipedia corpora (Zhu et al., 2010; Coster and Kauchak, 2011b). These resources generally encode interdependencies between different types of simplification better than single-purpose resources and may thus seem favorable for learning simplifications. However, the freedom given to editors and lack of explicit labels on the modifications performed makes generalization much more difficult, especially when existing resources are relatively small in comparison to corpora for other text-to-text problems like machine translation (MT). Nevertheless, these corpora have been extensively used to learn phrasebased statistical and neural"
I17-3007,D14-1082,0,0.0105669,"tep was to design general-purpose simplification rules which will later be specialised for the domain under consideration (PA). This solution led to the development of MUSST, which includes SS modules for three languages. MUSST is based on the framework proposed by Siddharthan (2004) and is available as an open source Python implementation. Our rules split conjoint clauses, relative clauses and appositive phrases, and change sentences from passive into active voice. These are arguably the most widely applicable simplification operations across languages. We use the Stanford dependency parser (Chen and Manning, 2014) for the three languages, which enabled us to build a consistent multilingual tool. MUSST is evaluated using corpora extracted from the SIMPATICO use cases data. Such corpora (one for each language) were checked and – where applicable – syntactically simplified by experts in the area. Inspired by the work of Gasperin et al. (2009), we also developed a complexity checker module in order to select sentences that should be simplified. In addition, we implemented a confidence model in order to predict whether or not a simplification produced by MUSST is good enough to be shown to the end-user. Dev"
I17-3007,W11-2123,0,0.0097727,"a sentence is “good enough” for a user, we trained a confidence model to classify a simplification as acceptable or not. Using the 292 sentences simplified by the English system and evaluated in Section 3, we built a confidence model for this language. The 70 sentences classified as incorrect (Section 3) were used as negative examples, whilst the remaining sentences received the positive label. As features, we used the same basic counts as for the complexity checker (Section 4.1) along with language model (LM) probabilities and perplexity and grammar checking on the simplifications. KenLM11 (Heafield, 2011) was used to extract LM features. A Python grammar checker was used for evaluating grammaticality12 . The model was trained using the Random Forest implementation from scikit-learn with 10fold cross-validation and achieved 0.80 of accuracy (F1/Precision/recall = 0.60/0.69/0.53), outperforming the MC classifier (accuracy = 0.61). For Italian and Spanish, we also experimented with the datasets presented in Section 3, but the performance is worse because of the significantly smaller training sets. Nevertheless, both models outperform the majority class baseline in terms of accuracy. For Italian,"
I17-3007,L16-1491,1,0.821273,"ing the lexical and/or syntactic complexity of a text (Siddharthan, 2004). It is common to divide this task in two subtasks: lexical simplification (LS) and syntactic simplification (SS). Whilst LS deals with the identification and replacement of difficult words or phrases, SS focuses on making complex syntactic constructions simpler. It is known, for instance, that passive voice constructions are more complex than active voice, and that long sentences with multiple clauses are more difficult to be understood than short sentences with a single clause. Several tools have been developed for LS (Paetzold and Specia, 2016). However, we are not aware of freely available tools for SS. 1 https://www.simpatico-project.eu/ 25 The Companion Volume of the IJCNLP 2017 Proceedings: System Demonstrations, pages 25–28, c Taipei, Taiwan, November 27 – December 1, 2017. 2017 AFNLP in Tint4 (Palmero Aprosio and Moretti, 2016) (an adapted version of CoreNLP for Italian). Figure 1 shows the parser output for the sentence “These organisations have been checked by us and should provide you with a quality service.”, as an example. The sentence is first sent to the Analysis module that will search for discourse markers. In this ca"
L16-1579,C04-1046,0,0.0689396,"are MT systems, to inform end-users or to assist in the translation process (such as in post-editing), appropriate evaluation methods and metrics need to be applied in order to provide reliable assessments. Automatic metrics that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications a"
L16-1579,W14-3302,1,0.837552,"s that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels"
L16-1579,W15-3001,1,0.926955,"em outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so"
L16-1579,W12-3102,1,0.817573,"to provide reliable assessments. Automatic metrics that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence a"
L16-1579,W11-2401,0,0.0269932,"es for the CREG corpus. between test takers could be calculated. Translation Methods Set 1 Document 1 4.1. Document 2 As previously mentioned, the reading comprehension questions are open questions, and thus any answer could be provided by the test takers. Another important detail is that these questions have different levels of complexity, meaning that some questions require more effort to be answered. Since our aim is to generate quality labels from the answers, information about the question complexity level is important. We therefore manually classified the questions using the classes in (Meurers et al., 2011), focusing on question forms and comprehension types (Day and Park, 2005). Document 3 Document 4 Document 5 Test takers Document 6 . . . Set m Document n-5 Document n-4 Document n-3 Question classification Question forms: these can be directly defined by the question structure and by the expected answer. The question forms available in the CREG corpus are: Document n-2 Document n-1 Document n • Yes/no questions: are simple questions that admit either yes or no as valid answers. Figure 1: Split of corpus in sets and translation approach. • Alternative questions: are a combination of yes/no ques"
L16-1579,P02-1040,0,0.101708,"t the reading comprehension test into document-level quality scores. Keywords: Machine Translation, Reading Comprehension, Quality Estimation 1. show similar quality scores. Introduction Evaluating Machine Translation (MT) systems outputs is a challenging task. Whether the evaluation goal is to compare MT systems, to inform end-users or to assist in the translation process (such as in post-editing), appropriate evaluation methods and metrics need to be applied in order to provide reliable assessments. Automatic metrics that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are prob"
L16-1579,2014.eamt-1.21,1,0.77146,"r parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so far (e.g. HTER (Snover et al., 2006), likert), there is a lack of studies in quality labels for document-level. Previous work use BLEU-style metrics as labels (Scarton and Specia, 2014; Soricut and Echihabi, 2010; Scarton, 2015) . The WMT15 QE shared task also followed this approach, using METEOR (Banerjee and Lavie, 2005) for a paragraph-level QE task (Bojar et al., 2015). However, as shown by Scarton et al. (2015), these metrics do not distinguish well among documents, i.e. most documents produced by the same or a similar MT system One issue of document-level quality labeling, noted by Scarton et al. (2015), is that the task of asking humans to assess documents is not trivial. While likert scores can be successfully applied for sentence and word levels, documentlevel qual"
L16-1579,W15-4916,1,0.866927,"Missing"
L16-1579,N15-2016,1,0.842109,"nstead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so far (e.g. HTER (Snover et al., 2006), likert), there is a lack of studies in quality labels for document-level. Previous work use BLEU-style metrics as labels (Scarton and Specia, 2014; Soricut and Echihabi, 2010; Scarton, 2015) . The WMT15 QE shared task also followed this approach, using METEOR (Banerjee and Lavie, 2005) for a paragraph-level QE task (Bojar et al., 2015). However, as shown by Scarton et al. (2015), these metrics do not distinguish well among documents, i.e. most documents produced by the same or a similar MT system One issue of document-level quality labeling, noted by Scarton et al. (2015), is that the task of asking humans to assess documents is not trivial. While likert scores can be successfully applied for sentence and word levels, documentlevel quality can not be evaluated in the same way. Is"
L16-1579,2006.amta-papers.25,0,0.482024,"uality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so far (e.g. HTER (Snover et al., 2006), likert), there is a lack of studies in quality labels for document-level. Previous work use BLEU-style metrics as labels (Scarton and Specia, 2014; Soricut and Echihabi, 2010; Scarton, 2015) . The WMT15 QE shared task also followed this approach, using METEOR (Banerjee and Lavie, 2005) for a paragraph-level QE task (Bojar et al., 2015). However, as shown by Scarton et al. (2015), these metrics do not distinguish well among documents, i.e. most documents produced by the same or a similar MT system One issue of document-level quality labeling, noted by Scarton et al. (2015), is that the task o"
L16-1579,P10-1063,0,0.0239598,"at are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of end-users (such as gisting ) and where fully automated MT is needed (e.g. because the amount of data is unfeasible for human post-editing). While for sentence and word-level QE several quality labels have been proposed so far (e.g. HTER (Snover et al., 2006), likert), there is a lack of studies in quality labels for document-level. Previous work use BLEU-style metrics as labels (Scarton and Specia, 2014; Soricut and Echihabi, 2010; Scarton, 2015) . The WMT15 QE shared task also followed this approach, using METEOR (Banerjee and Lavie, 2005) for a paragraph-level QE task (Bojar et al., 2015). However, as shown by Scarton et al. (2015), these metrics do not distinguish well among documents, i.e. most documents produced by the same or a similar MT system One issue of document-level quality labeling, noted by Scarton et al. (2015), is that the task of asking humans to assess documents is not trivial. While likert scores can be successfully applied for sentence and word levels, documentlevel quality can not be evaluated in"
L16-1579,2009.eamt-1.5,1,0.803854,"nform end-users or to assist in the translation process (such as in post-editing), appropriate evaluation methods and metrics need to be applied in order to provide reliable assessments. Automatic metrics that contrast system outputs against reference translations, such as BLEU (Papineni et al., 2002), are widely explored to compare MT systems and measure the progress of a given MT system over time. Quality Estimation (QE) is a different evaluation method that provides a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009). These metrics are useful to inform end-users and post-editors. Most work in QE focuses on sentence-level and word-level prediction (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). Estimation of quality at sentence and word levels are probably the most useful types of prediction for post-editing, since post-editors can benefit from smaller parts of the document that are already acceptable, instead of relying on a single quality score for the entire document. On the other hand, document-level QE can be desirable for applications aimed at other types of"
L16-1579,W05-0909,0,\N,Missing
L16-1579,2015.eamt-1.17,1,\N,Missing
L18-1553,W03-1004,0,0.239373,"a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all"
L18-1553,P11-2087,0,0.024947,"Missing"
L18-1553,W11-1603,0,0.01484,"s a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments (Stajner et al., 2017), while still offeri"
L18-1553,P11-2117,0,0.247664,"work. Keywords: text Simplification, simplification corpora, Newsela 1. Introduction Text Simplification (TS) consists in making texts more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS"
L18-1553,W11-2107,0,0.0303121,"old 0.352 0.341 0.238 0.231 0.378 0.400 Table 4: Accuracy in the full pipeline evaluation Here we assess the potential of our corpus in LS. LS is commonly addressed as a pipeline of steps: candidates for a target complex word are produced via a Substitution Generation (SG) method, filtered with respect to the context of the complex word via a Substitution Selection (SS) method, and finally ordered for simplicity by a Substitution Ranking (SR) method. We use our aligned corpus for SG following the state of the art approach in (Horn et al., 2014). First, we produce word alignments using Meteor (Denkowski and Lavie, 2011) and extract complex-to-simple word correspondences. Then we filter word pairs with different POS tags, where the complex word is a stop word, or either word is a proper noun. Finally, we generate all possible inflections for nouns and verbs (Burns, 2013). We compare this approach to six other generators from a recent benchmark (Paetzold and Specia, 2016a): the Horn generator (Horn et al., 2014), which employs the approach described above over Wikipedia-Simple Wikipedia data, the Devlin (Devlin and Tait, 1998), Biran (Biran et ˇ al., 2011), Glavas (Glavaˇs and Stajner, 2015) and Paetzold (Paet"
L18-1553,W13-2901,0,0.129862,"e. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 20"
L18-1553,P15-2011,0,0.0613146,"Missing"
L18-1553,P14-2075,0,0.0841375,"implification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplification operations, which are applied in ad hoc ways by volunteer editors. Xu et al. (2015) introduce a new resou"
L18-1553,P07-2045,0,0.00703543,"Missing"
L18-1553,W13-4813,1,0.897302,"s more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence"
L18-1553,P15-4015,1,0.92596,"verbs (Burns, 2013). We compare this approach to six other generators from a recent benchmark (Paetzold and Specia, 2016a): the Horn generator (Horn et al., 2014), which employs the approach described above over Wikipedia-Simple Wikipedia data, the Devlin (Devlin and Tait, 1998), Biran (Biran et ˇ al., 2011), Glavas (Glavaˇs and Stajner, 2015) and Paetzold (Paetzold and Specia, 2016c) generators, which exploit WordNet, comparable complex-to-simple documents, typical word embeddings and context-aware word embeddings, respectively. All generators were implemented with the LEXenstein framework (Paetzold and Specia, 2015). We use the BenchLS dataset as our gold-standard dataset (Paetzold and Specia, 2016a). It is the largest dataset of its kind, with 929 instances, each composed by a sentence, a target complex word, and a set of gold substitutions given by humans. To compare the generators, we use standard metrics: Potential – the proportion of instances in which at least one of the candidates generated is in the gold-standard, Precision – the proportion of generated substitutions that are in the gold-standard, Recall – the proportion of goldstandard substitutions that are among the generated substitutions, an"
L18-1553,L16-1491,1,0.858958,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,N16-1050,1,0.859101,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,W16-4912,0,0.0361996,"m 2 to 12, where 2 represents the lowest and 12 the highest level. Version identifiers capture the relationship between the reading levels of a pair of documents: e.g. version 1 has a lower reading level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptio"
L18-1553,N10-1063,0,0.0372608,"ng level than version 0, version 2 has a lower reading level than version 1. Articles are only aligned at document level and there is no guarantee that different versions of an article will have the same number of sentences, nor that they will be aligned in 1-to-1 fashion. The absence of paragraph and sentence alignments limits the use of the data. To produce such alignments, we use the algorithms in (Paetzold and Specia, 2016d), which employ a vicinitydriven search approach. These algorithms address the limitations of previous strategies (Barzilay and Elhadad, 2003; Coster and Kauchak, 2011; Smith et al., 2010; Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments"
L18-1553,P17-2016,0,0.0934999,"Xu et al., 2015; Bott and Saggion, 2011) by disregarding the need for supervised or semi-supervised training, allowing long-distance alignment skips, capturing 1-N and N-1 alignments, and exploiting the fact that the order in which information is presented is constant between pairs of aligned Newsela articles. Because the vicinity-driven approach of Paetzold and Specia (2016d) exploits a series of assumptions that can be made about the Newsela corpus, it is more efficient than more sophisticated approaches that perform exhaustive search over all possible 3504 ˇ paragraph/sentence alignments (Stajner et al., 2017), while still offering comparable alignment accuracy. The result of the alignment is a corpus with 19, 198 pairs of articles aligned at both paragraph (300, 475 pairs) and sentence (550, 644 pairs) levels. This is over three times larger than the Wikipedia–Simple Wikipedia corpus (Coster and Kauchak, 2011), making it the largest corpus of its kind. Columns 2 to 4 in Table 1 illustrate the number of paragraph and sentence alignments for all version pairs in the corpus. We categorise the sentence alignments according to four types of simplification: • None: Complex and simple sentences are ident"
L18-1553,D11-1038,0,0.0732401,"ch complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar mo"
L18-1553,Q15-1021,0,0.457792,"hu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplification operations, which are applied in ad hoc ways by volunteer editors. Xu et al. (2015) introduce a new resource that allegedly addresses these limitations: the Newsela corpus (Newsela, 2016). Unlike Simple Wikipedia, the Newsela corpus was created by professional editors and targets a specific audience (students), which should make it a more reliable resource for TS. However,"
L18-1553,Q16-1029,0,0.0482634,"0.330 0.238 0.272 0.330 0.240 FLESCH-S 66.93 71.12 67.29 75.04 87.49 70.34 75.10 87.52 70.41 FLESCH-O 66.21 69.85 65.34 74.89 87.33 70.19 74.89 87.33 70.19 FLESCH-R 74.32 69.85 79.98 80.62 87.33 78.91 80.62 87.33 78.91 Table 5: Results for SMT-based simplifiers not a reliable metric when original, reference and simplified sentences are the same. For all cases where TER = 0, the SARI value was 0.330, which can be seem as a low value if the systems are producing an output equal to the reference. Since this metric was designed for cases where sentences should also be simplified (as explained in Xu et al. (2016)), the use of SARI for cases where the original sentences are already simple is not reliable. 6. Conclusions Upon studying the sentence-aligned Newsela corpus we found that: (i) it follows an expected TER distribution, with the lowest TER being between adjacent levels; (ii) the simplified sentences score as more readable than their original counterparts according to traditional readability metrics, and (iii) the corpus proved a more reliable source of complex-simple correspondences for LS and MT-based simplification than the Wikipedia-Simple Wikipedia corpus. We achieve some the highest perfor"
L18-1553,N10-1056,0,0.0550178,"ation (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015), it is very small (167, 689 parallel sentence pairs (Coster and Kauchak, 2011)) in comparison to bilingual corpora used with similar modelling techniques in MT and, more critically, covers a limited range of simplificat"
L18-1553,D17-1062,0,0.0627507,"Missing"
L18-1553,C10-1152,0,0.235267,"ification (TS) consists in making texts more easily comprehensible. It can take many forms: Lexical Simplification (LS), in which complex words are replaced by simpler alternatives (Devlin, 1999), Syntactic Simplification (SS), which consists in changing the syntactic structure of a sentence (Siddharthan, 2006), and Semantic Simplification, in which portions of the text are paraphrased (Kandula et al., 2010). Current empirical approaches rely mostly on the Wikipedia-Simple Wikipedia parallel corpus (Coster and Kauchak, 2011). This resource has been used by Machine Translation (MT) approaches (Zhu et al., 2010), tree transductors (Paetzold and Specia, 2013; Feblowitz and Kauchak, 2013), integer programming techniques (Woodsend and Lapata, 2011), and discriminative linear models (Bach et al., 2011). In LS, Yatskar et al. (2010) extract candidate simplifications from Wikipedia and Simple Wikipedia edit histories, and Horn et al. (2014) extract word correspondences from word alignments between the complex-simple segments in the corpus. Even though the Simple Wikipedia corpus has been a valuable resource for modern TS, as discussed in Yasseri et al. (2012), Amancio and Specia (2014) and Xu et al. (2015)"
L18-1685,P11-2087,0,0.00887031,"ification (TS) is the task of reducing lexical and/or structural complexity of texts (Siddharthan, 2004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan an"
L18-1685,bott-etal-2012-text,0,0.147002,"focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplificat"
L18-1685,W14-1206,0,0.0470791,"Missing"
L18-1685,W09-2105,1,0.911308,"Missing"
L18-1685,E99-1042,0,0.257767,"ntroduction Text simplification (TS) is the task of reducing lexical and/or structural complexity of texts (Siddharthan, 2004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et"
L18-1685,W11-1601,0,0.136429,"l et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specifi"
L18-1685,P15-2011,0,0.0783328,"Missing"
L18-1685,P14-2075,0,0.0215017,"implification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations joint"
L18-1685,P14-1041,0,0.258899,"al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS e"
L18-1685,P17-2014,0,0.261149,"Missing"
L18-1685,W13-4813,1,0.897804,"in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain"
L18-1685,L16-1491,1,0.915491,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,N16-1050,1,0.921091,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,W16-4912,0,0.0353036,"004). It is common to divide this task in two: lexical simplification (LS) and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches te"
L18-1685,E17-2006,1,0.871376,"and syntactic simplification (SS). LS deals with the identification and replacement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main pa"
L18-1685,I17-3007,1,0.734194,"ng passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish a"
L18-1685,E14-1076,0,0.018032,"de simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not b"
L18-1685,W11-2802,0,0.14289,"ement of difficult words or phrases, while SS focuses on making complex syntactic structures simpler, e.g. by changing passive into active voice or splitting a sentence with coordination in two sentences. LS has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela"
L18-1685,D11-1038,0,0.0952986,"S has been widely explored in recent years. Work include simple word frequency-based approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover"
L18-1685,P12-1107,0,0.437046,"Missing"
L18-1685,Q16-1029,0,0.334396,"proaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS evaluation, only a"
L18-1685,D17-1062,0,0.0586889,"word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to model operations for specific domains. In terms of specific corpora for TS evaluation, only a few exist, mostly for E"
L18-1685,C10-1152,0,0.727031,"approaches (Carroll et al., 1998; Carroll et al., 1999; Biran et al., 2011), unsupervised approaches that use word embeddings (Glavaˇs and ˇ Stajner, 2015; Paetzold and Specia, 2016c), and supervised approaches (Horn et al., 2014; Paetzold and Specia, 2017). For SS, some approaches apply hand-crafted rules (Siddharthan, 2011; Candido Jr. et al., 2009; Bott et al., 2012; Brouwers et al., 2014; Barlacchi and Tonelli, 2013; Scarton et al., 2017), while others use parallel data to learn simplification operations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013; Siddharthan and Angrosh, 2014; Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016; Zhang and Lapata, 2017; Nisioi et al., 2017). Corpus-based approaches tend to learn both LS and SS transformations jointly. For English, two main parallel corpora exist: Simple Wikipedia (Zhu et al., 2010) and Newsela (Newsela, 2016), the latter with simplifications performed by professionals. However, such corpora do not distinguish among different types of simplification (i.e. lexical from syntactic transformations). Moreover, they cover general domain texts, and therefore may not be sufficient to mo"
N10-2011,W09-2105,1,0.768735,"Missing"
N15-2016,W11-2104,0,0.0272487,"QE model. In this training phase supervised ML techniques, such as regression, can be applied. A training set with quality labels is provided for an ML model. These quality labels are the scores that the QE model will learn to predict. Therefore, the QE model will be able to predict a quality score for a new, unseen data points. The quality labels can be likert scores, HTER, BLEU, just to cite some widely used examples. Also the ML algorithm can vary (SVM and Gaussian Process are the state-of-the-art algorithms for QE). Some work in the area include linguistic information as features for QE (Avramidis et al., 2011; Pighin and M`arquez, 2011; Hardmeier, 2011; Felice and Specia, 2012; Almaghout and Specia, 2013) at sentence level. Only Scarton and Specia (2014) (predicting quality at document level) and Rubino et al. (2013) (sentence level) focus on the use of discourse information for QE. It is important to notice that frameworks like QuEst2 (Specia et al., 2013) are available for QE at 2 http://www.quest.dcs.shef.ac.uk sentence level. QuEst has modules to extract several features for QE from source and target documents and to experiment with ML techniques for predicting QE. Features are divided in two"
N15-2016,W05-0909,0,0.174214,"in order to identify the most important sentences (or even paragraphs) and assign different weights according to the relevance of the sentence. Moreover, we studied several traditional evaluation metrics as quality labels for QE at document level and found out that, on average, all the documents seem to be similar. Part of this study is showed in Table 1 for 9 documents of WMT2013 QE shared task corpus (English-Spanish translations) and for 119 documents of LIG corpus (Potet et al., 2012) (French-English translations, with posteditions).3 The quality metrics considered were BLEU, TER, METEOR (Banerjee and Lavie, 2005) and an average of HTER scores at sentence level. All traditional MT evaluation metrics showed low standard deviation (STDEV) in both corpora. Also the HTER at sentence level averaged to obtain a document-score showed low variation. This means 3 Both corpora were translated by only one SMT system. 121 that all documents in the corpora seem similar in terms of quality. Our hypothesis is that this evaluation is wrong and other factors should be considered in order to achieve a suitable quality label for document-level prediction. Besides quality scores, another issue in document-level QE is the"
N15-2016,J08-1001,0,0.0172991,"e should be able to deal with evaluation for several language pairs, considering features for source and target. Another issue is that QE features should correlate with the quality score used. Therefore, the use of discourse for QE purposes deserves further investigation. We intend to model discourse for QE by applying linguistic and statistical knowledge. Two cases are being explored: 3.2.1 Linguistic-based models Certain discourse theories could be used to model discourse for QE purposes, such as such as the Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) and Entity-Grid models (Barzilay and Lapata, 2008; Elsner, 2011). We refer to these two theories mainly because they can be readily applied, for English language, given the existence of BLEU (↑) TER (↓) METEOR-ex (↑) METEOR-st (↑) Averaged HTER (↓) WMT Average STDEV 0.26 0.046 0.52 0.049 0.46 0.050 0.43 0.050 0.25 0.071 LIG Average STDEV 0.27 0.052 0.53 0.069 0.29 0.031 0.30 0.030 0.21 0.032 Table 1: Average values of evaluation metrics in the WMT and LIG corpora parsers (RST parser (Joty et al., 2013) and Entity Grid parser).4 Although these resources are only available for English, it is important in this stage to study the impact of this"
N15-2016,P13-2068,0,0.0148714,"ted fact. However, it is hard to integrate discourse information into traditional state-of-the-art sentencelevel MT systems. It is also challenging to build a document-level or discourse-based MT system from scratch. Therefore, the initiatives focus on the integration of discourse as features into the decoding phase or previously annotate discourse phenomena in the parallel corpora. Lexical Cohesion is related to word usage: word repetitions, synonyms repetitions and collocations. Besides initiatives to improve MT system and outputs with lexical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory)"
N15-2016,C04-1046,0,0.265293,"y different from the references, it does not really mean that it is a bad output. Another problem is that these metrics cannot be used in scenarios where the output of the system is to be used directly by end-users, for example a user reading the output of Google Translate1 for a given news text cannot count on a reference for that translated text. Quality Estimation (QE) approaches aim to predict the quality of MT systems without using references. Instead, features (that may be or may not be related to the MT system that produced this translations) are applied to source and target documents (Blatz et al., 2004; Bojar et al., 2013). The only requirement is data points with scores (e.g.: Humantargeted Translation Error Rate (HTER) (Snover et al., 2006) or even BLEU-style metrics). These data points can be used to train supervised machine learning models (regressors or classifiers) to predict the scores of unseen data. The advantage of these approaches is that we do not need to have all the words, sentences or documents of a task evaluated manually, we just need enough data points to train the machine learning model. QE systems predict scores that reflect how good a translation is for a given scenario"
N15-2016,W12-3156,0,0.0229364,"er the discourse models built for QE can be used for the evaluation of other tasks in NLP. AS evaluation could benefit from QE: to an extent, AS outputs could be viewed as “translations” from “source language” into “source summarised language”. Up to now, only (Louis and Nenkova, 2013) proposed an approach for evaluating summaries without references (by using pseudo-references). Moreover, discourse evaluation of AS outputs is expected to show more correlation with quality scores than MT because of the nature of the tasks. While MT outputs are dependent on the source language (and, as shown by Carpuat and Simard (2012), they tend to preserve discourse constituents of the source), AS outputs are built by choosing sentences from one or more documents trying to keep as much relevant information as possible. The combination of text from multiple documents can lead to loss of coherence of automatic summaries more than MT does to translated texts. Another task in NLP that could benefit from advances in QE is Readability Assessment (RA). This task consists in evaluating the complexity of documents for a given audience (therefore, the task is an evaluation per se). Several studies have already explored discourse in"
N15-2016,P12-2023,0,0.0230728,"epresented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT system. Guzm´an et al. (2014) use RST trees comparison for MT evaluation. Topic models capture word usage, although they are more robust than lexical cohesion structures because they can correlate words that are not repetitions or do not present any semantic relation. These methods can measure if a document follows a topic, is related to a genre or belongs to a specific domain. Work on improving MT that uses topic models include Zhengxian et al. (2010) and Eidelman et al. (2012). 3 Planned Work In this paper, we describe the three main research questions that we aim to answer in this PhD work: 1. How to address document-level QE? 2. Are discourse models appropriate to be used for QE at document level? Are these models applicable for different languages? 3. How can we use the discourse information for the evaluation of Automatic Summarisation and Readability Assessment? In this section, we summarise how we are addressing these research questions. 3.1 Document-level Quality Estimation As mentioned previously, one aim of this PhD is to identify a suitable quality label"
N15-2016,W12-3110,0,0.0197854,"regression, can be applied. A training set with quality labels is provided for an ML model. These quality labels are the scores that the QE model will learn to predict. Therefore, the QE model will be able to predict a quality score for a new, unseen data points. The quality labels can be likert scores, HTER, BLEU, just to cite some widely used examples. Also the ML algorithm can vary (SVM and Gaussian Process are the state-of-the-art algorithms for QE). Some work in the area include linguistic information as features for QE (Avramidis et al., 2011; Pighin and M`arquez, 2011; Hardmeier, 2011; Felice and Specia, 2012; Almaghout and Specia, 2013) at sentence level. Only Scarton and Specia (2014) (predicting quality at document level) and Rubino et al. (2013) (sentence level) focus on the use of discourse information for QE. It is important to notice that frameworks like QuEst2 (Specia et al., 2013) are available for QE at 2 http://www.quest.dcs.shef.ac.uk sentence level. QuEst has modules to extract several features for QE from source and target documents and to experiment with ML techniques for predicting QE. Features are divided in two types: glass-box (dependent on the MT system) and black-box (independ"
N15-2016,W10-1750,0,0.0757273,"Missing"
N15-2016,P14-1065,0,0.0366516,"Missing"
N15-2016,2010.iwslt-papers.10,0,0.0234103,"usage: word repetitions, synonyms repetitions and collocations. Besides initiatives to improve MT system and outputs with lexical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) is a linguistic theory that correlates macro and micro units of discourse in a coherent way. The correlation is made among EDUs (Elementary Discourse Units). EDUs are defined at sentence, phrase or paragraph-level. These correlations are represented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT system. Guzm´an et al. (2014)"
N15-2016,2011.eamt-1.32,0,0.021194,"hniques, such as regression, can be applied. A training set with quality labels is provided for an ML model. These quality labels are the scores that the QE model will learn to predict. Therefore, the QE model will be able to predict a quality score for a new, unseen data points. The quality labels can be likert scores, HTER, BLEU, just to cite some widely used examples. Also the ML algorithm can vary (SVM and Gaussian Process are the state-of-the-art algorithms for QE). Some work in the area include linguistic information as features for QE (Avramidis et al., 2011; Pighin and M`arquez, 2011; Hardmeier, 2011; Felice and Specia, 2012; Almaghout and Specia, 2013) at sentence level. Only Scarton and Specia (2014) (predicting quality at document level) and Rubino et al. (2013) (sentence level) focus on the use of discourse information for QE. It is important to notice that frameworks like QuEst2 (Specia et al., 2013) are available for QE at 2 http://www.quest.dcs.shef.ac.uk sentence level. QuEst has modules to extract several features for QE from source and target documents and to experiment with ML techniques for predicting QE. Features are divided in two types: glass-box (dependent on the MT system"
N15-2016,P14-6007,0,0.027874,"yms repetitions and collocations. Besides initiatives to improve MT system and outputs with lexical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) is a linguistic theory that correlates macro and micro units of discourse in a coherent way. The correlation is made among EDUs (Elementary Discourse Units). EDUs are defined at sentence, phrase or paragraph-level. These correlations are represented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT system. Guzm´an et al. (2014) use RST trees com"
N15-2016,P13-1048,0,0.057694,"del discourse for QE purposes, such as such as the Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) and Entity-Grid models (Barzilay and Lapata, 2008; Elsner, 2011). We refer to these two theories mainly because they can be readily applied, for English language, given the existence of BLEU (↑) TER (↓) METEOR-ex (↑) METEOR-st (↑) Averaged HTER (↓) WMT Average STDEV 0.26 0.046 0.52 0.049 0.46 0.050 0.43 0.050 0.25 0.071 LIG Average STDEV 0.27 0.052 0.53 0.069 0.29 0.031 0.30 0.030 0.21 0.032 Table 1: Average values of evaluation metrics in the WMT and LIG corpora parsers (RST parser (Joty et al., 2013) and Entity Grid parser).4 Although these resources are only available for English, it is important in this stage to study the impact of this information for documentlevel QE, considering English as source or target language. In this scenario, we intend to explore source and target features isolated (source features will be applied only when English is source language and target features only when English is target). Moreover, other linguistic information could be used to model discourse for QE. Anaphoric information, co-reference resolution and discourse connectives classification could be us"
N15-2016,W10-1737,0,0.0317401,"hesion is related to word usage: word repetitions, synonyms repetitions and collocations. Besides initiatives to improve MT system and outputs with lexical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) is a linguistic theory that correlates macro and micro units of discourse in a coherent way. The correlation is made among EDUs (Elementary Discourse Units). EDUs are defined at sentence, phrase or paragraph-level. These correlations are represented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT"
N15-2016,P14-2047,0,0.0271004,"e et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) is a linguistic theory that correlates macro and micro units of discourse in a coherent way. The correlation is made among EDUs (Elementary Discourse Units). EDUs are defined at sentence, phrase or paragraph-level. These correlations are represented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT system. Guzm´an et al. (2014) use RST trees comparison for MT evaluation. Topic models capture word usage, although they are more robust than lexical cohesion s"
N15-2016,P04-1077,0,0.0188792,"her NLP tasks, such as AS. 1 Introduction Evaluation metrics for Machine Translation (MT) and Automatic Summarisation (AS) tasks should be able to measure quality with respect to different aspects (e.g. fluency and adequacy) and they should be fast and scalable. Human evaluation seems to be the most reliable (although it might introduce biases of reviewers). However, it is expensive and cumbersome for large datasets; it is also not practical for certain scenarios, such as gisting in MT and summarisation of webpages. Automatic evaluation metrics (such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Och, 2004)), based on human references, are widely used to evaluate MT and AS outputs. One limitation of these metrics is that if the MT or AS system outputs a translation or summary considerably different from the references, it does not really mean that it is a bad output. Another problem is that these metrics cannot be used in scenarios where the output of the system is to be used directly by end-users, for example a user reading the output of Google Translate1 for a given news text cannot count on a reference for that translated text. Quality Estimation (QE) approaches aim to predict the quality of"
N15-2016,J13-2002,0,0.0320368,"rget QuEst 1 QuEst 2 QuEst3 QuEst 4 0.8 * 0.6 * * * Person’s r 0.4 0.2 0.0 0.2 * 0.6 * * * * 0.4 10 documents 20 documents 40 documents Bins 80 documents 119 documents Figure 1: Impact of discourse features on document-level QE - ‘*’ means p-value &lt; 0.05 3.3 Using discourse models for other NLP tasks One of our aims is to evaluate whether the discourse models built for QE can be used for the evaluation of other tasks in NLP. AS evaluation could benefit from QE: to an extent, AS outputs could be viewed as “translations” from “source language” into “source summarised language”. Up to now, only (Louis and Nenkova, 2013) proposed an approach for evaluating summaries without references (by using pseudo-references). Moreover, discourse evaluation of AS outputs is expected to show more correlation with quality scores than MT because of the nature of the tasks. While MT outputs are dependent on the source language (and, as shown by Carpuat and Simard (2012), they tend to preserve discourse constituents of the source), AS outputs are built by choosing sentences from one or more documents trying to keep as much relevant information as possible. The combination of text from multiple documents can lead to loss of coh"
N15-2016,A00-2002,0,0.0271409,"vement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) is a linguistic theory that correlates macro and micro units of discourse in a coherent way. The correlation is made among EDUs (Elementary Discourse Units). EDUs are defined at sentence, phrase or paragraph-level. These correlations are represented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT system. Guzm´an et al. (2014) use RST trees comparison for MT evaluation. Topic models capture word usage, although they are more robust than lexical cohesion structures because they can correlate words that are not repetitions or do not present any semantic relation. These methods can measure if a document follows a topic, is related to a genre or belongs to a specific domain. Work on improving MT that uses topic models include Zhengxian et al. (2010) and Eidelman et al. (2012). 3 Planned Work In this paper,"
N15-2016,W12-0117,0,0.0251496,"ve MT system and outputs with lexical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) is a linguistic theory that correlates macro and micro units of discourse in a coherent way. The correlation is made among EDUs (Elementary Discourse Units). EDUs are defined at sentence, phrase or paragraph-level. These correlations are represented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT system. Guzm´an et al. (2014) use RST trees comparison for MT evaluation. Topic models capture word usage, although they a"
N15-2016,2012.amta-papers.20,0,0.0157304,"exical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) is a linguistic theory that correlates macro and micro units of discourse in a coherent way. The correlation is made among EDUs (Elementary Discourse Units). EDUs are defined at sentence, phrase or paragraph-level. These correlations are represented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT system. Guzm´an et al. (2014) use RST trees comparison for MT evaluation. Topic models capture word usage, although they are more robust than"
N15-2016,P02-1040,0,0.093705,"gs of this research can improve other NLP tasks, such as AS. 1 Introduction Evaluation metrics for Machine Translation (MT) and Automatic Summarisation (AS) tasks should be able to measure quality with respect to different aspects (e.g. fluency and adequacy) and they should be fast and scalable. Human evaluation seems to be the most reliable (although it might introduce biases of reviewers). However, it is expensive and cumbersome for large datasets; it is also not practical for certain scenarios, such as gisting in MT and summarisation of webpages. Automatic evaluation metrics (such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Och, 2004)), based on human references, are widely used to evaluate MT and AS outputs. One limitation of these metrics is that if the MT or AS system outputs a translation or summary considerably different from the references, it does not really mean that it is a bad output. Another problem is that these metrics cannot be used in scenarios where the output of the system is to be used directly by end-users, for example a user reading the output of Google Translate1 for a given news text cannot count on a reference for that translated text. Quality Estimation (QE) approaches"
N15-2016,W11-1001,0,0.039633,"Missing"
N15-2016,D08-1020,0,0.0315061,"uents of the source), AS outputs are built by choosing sentences from one or more documents trying to keep as much relevant information as possible. The combination of text from multiple documents can lead to loss of coherence of automatic summaries more than MT does to translated texts. Another task in NLP that could benefit from advances in QE is Readability Assessment (RA). This task consists in evaluating the complexity of documents for a given audience (therefore, the task is an evaluation per se). Several studies have already explored discourse information for RA (Graesser et al., 2004; Pitler and Nenkova, 2008; Todirascu et al., 2013). QE techniques can benefit RA in scenarios where we need to compare texts produced by or for 123 native speakers or second language learners (SLL) or texts produced by or for mentally impaired patient compared to healthy subjects (in these scenarios, the documents produced by or for the “experts” could be considered as source documents and documents produced by or for “inexpert or mentally impaired” as target documents). 4 Conclusion In this paper we presented a proposal to address to document-level quality estimation. This includes the study of quality labels for doc"
N15-2016,popescu-belis-etal-2012-discourse,0,0.0162508,"Besides initiatives to improve MT system and outputs with lexical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical Structure Theory) (Mann and Thompson, 1987) is a linguistic theory that correlates macro and micro units of discourse in a coherent way. The correlation is made among EDUs (Elementary Discourse Units). EDUs are defined at sentence, phrase or paragraph-level. These correlations are represented in the form of a tree. Marcu et al. (2000) explore RST focusing on identifying the feasibility of building a discourse-based MT system. Guzm´an et al. (2014) use RST trees comparison for MT evaluation. Topic models capt"
N15-2016,potet-etal-2012-collection,0,0.0603172,"Missing"
N15-2016,2013.mtsummit-posters.13,0,0.0459757,"Missing"
N15-2016,2014.eamt-1.21,1,0.937471,"r an ML model. These quality labels are the scores that the QE model will learn to predict. Therefore, the QE model will be able to predict a quality score for a new, unseen data points. The quality labels can be likert scores, HTER, BLEU, just to cite some widely used examples. Also the ML algorithm can vary (SVM and Gaussian Process are the state-of-the-art algorithms for QE). Some work in the area include linguistic information as features for QE (Avramidis et al., 2011; Pighin and M`arquez, 2011; Hardmeier, 2011; Felice and Specia, 2012; Almaghout and Specia, 2013) at sentence level. Only Scarton and Specia (2014) (predicting quality at document level) and Rubino et al. (2013) (sentence level) focus on the use of discourse information for QE. It is important to notice that frameworks like QuEst2 (Specia et al., 2013) are available for QE at 2 http://www.quest.dcs.shef.ac.uk sentence level. QuEst has modules to extract several features for QE from source and target documents and to experiment with ML techniques for predicting QE. Features are divided in two types: glass-box (dependent on the MT system) and black-box (independent on the MT system). At document level, Soricut and Echihabi (2010) explore d"
N15-2016,2006.amta-papers.25,0,0.661371,"narios where the output of the system is to be used directly by end-users, for example a user reading the output of Google Translate1 for a given news text cannot count on a reference for that translated text. Quality Estimation (QE) approaches aim to predict the quality of MT systems without using references. Instead, features (that may be or may not be related to the MT system that produced this translations) are applied to source and target documents (Blatz et al., 2004; Bojar et al., 2013). The only requirement is data points with scores (e.g.: Humantargeted Translation Error Rate (HTER) (Snover et al., 2006) or even BLEU-style metrics). These data points can be used to train supervised machine learning models (regressors or classifiers) to predict the scores of unseen data. The advantage of these approaches is that we do not need to have all the words, sentences or documents of a task evaluated manually, we just need enough data points to train the machine learning model. QE systems predict scores that reflect how good a translation is for a given scenario. For example, a widely predicted score in QE is HTER, which measures the effort needed to post-edit a sentence. A 1 https://translate.google.c"
N15-2016,P10-1063,0,0.355479,"level. Only Scarton and Specia (2014) (predicting quality at document level) and Rubino et al. (2013) (sentence level) focus on the use of discourse information for QE. It is important to notice that frameworks like QuEst2 (Specia et al., 2013) are available for QE at 2 http://www.quest.dcs.shef.ac.uk sentence level. QuEst has modules to extract several features for QE from source and target documents and to experiment with ML techniques for predicting QE. Features are divided in two types: glass-box (dependent on the MT system) and black-box (independent on the MT system). At document level, Soricut and Echihabi (2010) explore document-level QE prediction to rank documents translated by a given MT system, predicting BLEU scores. Features include text-based, language model-based, pseudo-reference-based, examplebased and training-data-based. Pseudo-reference features are BLEU scores based on pseudoreferences from an off-the-shelf MT system, for both the target and the source languages. Scarton and Specia (2014) explore lexical cohesion and LSA (Latent Semantic Analysis) (Landauer et al., 1998) cohesion for document-level QE. The lexical cohesion features are repetitions (Wong and Kit, 2012) and the LSA cohesi"
N15-2016,P13-4014,0,0.0737064,"Missing"
N15-2016,N12-1046,0,0.0141127,"ion for improving MT is a widely accepted fact. However, it is hard to integrate discourse information into traditional state-of-the-art sentencelevel MT systems. It is also challenging to build a document-level or discourse-based MT system from scratch. Therefore, the initiatives focus on the integration of discourse as features into the decoding phase or previously annotate discourse phenomena in the parallel corpora. Lexical Cohesion is related to word usage: word repetitions, synonyms repetitions and collocations. Besides initiatives to improve MT system and outputs with lexical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 201"
N15-2016,D12-1097,0,0.1031,"ent level, Soricut and Echihabi (2010) explore document-level QE prediction to rank documents translated by a given MT system, predicting BLEU scores. Features include text-based, language model-based, pseudo-reference-based, examplebased and training-data-based. Pseudo-reference features are BLEU scores based on pseudoreferences from an off-the-shelf MT system, for both the target and the source languages. Scarton and Specia (2014) explore lexical cohesion and LSA (Latent Semantic Analysis) (Landauer et al., 1998) cohesion for document-level QE. The lexical cohesion features are repetitions (Wong and Kit, 2012) and the LSA cohesion is achieved following the work of Graesser et al. (2004). Pseudoreference features are also applied in this work, according to the work of Soricut and Echihabi (2010). BLEU and TER (Snover et al., 2006) are used as quality labels. The best results were achieved with pseudo-reference features. However, LSA cohesion features alone also showed improvements over the baseline. 2.2 Discourse phenomena in MT In the MT area, there have been attempts to use discourse information that can be used as inspiration source for QE features. The need of document-level information for impr"
N15-2016,2011.mtsummit-papers.13,0,0.0326777,"T is a widely accepted fact. However, it is hard to integrate discourse information into traditional state-of-the-art sentencelevel MT systems. It is also challenging to build a document-level or discourse-based MT system from scratch. Therefore, the initiatives focus on the integration of discourse as features into the decoding phase or previously annotate discourse phenomena in the parallel corpora. Lexical Cohesion is related to word usage: word repetitions, synonyms repetitions and collocations. Besides initiatives to improve MT system and outputs with lexical cohesion (Ture et al., 2012; Xiao et al., 2011; Ben et al., 2013), Wong and Kit (2012) apply lexical cohesion metrics for evaluation of MT 120 systems at document level. Coreference is related to coherence clues, such as pronominal anaphora and connectives. Machine translation can break coreference chains since it is done at sentence level. Initiatives for improvement of coreference in MT include anaphora resolution (Gim´enez et al., 2010; LeNagard and Kohen, 2010; Hardmeier and Federico, 2010; Hardmeier, 2014) and connectives (Popescu-Belis et al., 2012; Meyer and Popescu-Belis, 2012; Meyer et al., 2012; Li et al., 2014). RST (Rhetorical"
N15-2016,W13-2201,0,\N,Missing
P15-4020,W14-3340,0,0.0963817,"Missing"
P15-4020,2014.eamt-1.21,1,0.93638,"al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are available and thus scores produced by automatic metr"
P15-4020,W15-4916,1,0.841275,"Missing"
P15-4020,2014.eamt-1.22,1,0.821768,"hine learning algorithms to build and test quality estimation models. Results on recent datasets show that Q U E ST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source f"
P15-4020,P10-1063,0,0.153973,"on. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are available and thus scores produced by automatic metrics have to be used as approximation (Scarton et al., 2015). Some applications require fine-g"
P15-4020,P13-4014,1,0.872058,"Missing"
P15-4020,2011.eamt-1.12,1,0.122579,"a higher level (e.g. sentences). Q U E ST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that Q U E ST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard"
P15-4020,W14-3339,0,0.0335233,"esources and preprocessing steps so that extractors for new features can be easily added. The basic functioning of the feature extraction module requires raw text files with the source and translation texts, and a few resources (where available) such as the MT source training corpus and source and target language models (LMs). Configuration files are used to indicate paths for resources and the features that should be extracted. For its main resources (e.g. LMs), if a resource is missing, Q U E ST++ can generate it automatically. 3.1 Word level We explore a range of features from recent work (Bicici and Way, 2014; Camargo de Souza et al., 2014; Luong et al., 2014; Wisniewski et al., 2014), totalling 40 features of seven types: Figure 1 depicts the architecture of Q U E ST++ . Document and Paragraph classes are used for document-level feature extraction. A Document is a group of Paragraphs, which in turn is a group of Sentences. Sentence is used for both word- and sentence-level feature extraction. A Feature Processing Module was created for each level. Each processing level is independent and can deal with the peculiarities of its type of feature. Target context These are features that explore the con"
P15-4020,C04-1046,0,0.539275,"uality Prediction with Q U E ST++ Lucia Specia, Gustavo Henrique Paetzold and Carolina Scarton Department of Computer Science University of Sheffield, UK {l.specia,ghpaetzold1,c.scarton}@sheffield.ac.uk extraction toolkits are available for that: A SIYA1 and Q U E ST2 (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While ce"
P15-4020,D12-1097,0,0.0149439,"at always predicts “Unintelligible” for Multi-Class, “Fluency” for Level 1, and “Bad” for the Binary setup. Document level Results The F-1 scores for the WMT14 datasets are given in Tables 1–4, for Q U E ST++ and systems that oficially participated in the task. The results show that Q U E ST++ was able to outperform all participating systems in WMT14 except for the English-Spanish baseline in the Binary and Level 1 tasks. The results in Table 5 also highlight the importance of selecting an adequate learning algorithm in CRF models. Our document-level features follow from those in the work of (Wong and Kit, 2012) on MT evaluation and (Scarton and Specia, 2014) for documentlevel QE. Nine features are extracted, in addition to aggregated values of sentence-level features for the entire document: • content words/lemmas/nouns repetition in S/T , • ratio of content words/lemmas/nouns in S/T , 4 System Q U E ST++ Baseline LIG/BL LIG/FS FBK-1 FBK-2 LIMSI RTM-1 RTM-2 Experiments In what follows, we evaluate Q U E ST++’s performance for the three prediction levels and various datasets. 4.1 Word-level QE Binary 0.502 0.525 0.441 0.444 0.487 0.426 0.473 0.350 0.328 Level 1 0.392 0.404 0.317 0.317 0.372 0.385 − 0"
P15-4020,P11-1022,0,\N,Missing
P15-4020,W14-3344,0,\N,Missing
P15-4020,2015.eamt-1.17,1,\N,Missing
P18-2113,I17-1030,1,0.775628,"ingly, the Newsela corpus has a feature that has been ignored thus far: Each instance in the corpus was created for readers with a certain school grade level. Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level. For example, a level 12 article may have simplified counterparts for levels 8 and 4. In other words, the corpus contains instances where the same input leads to different outputs. Disregarding this factor may lead to suboptimal models. To avoid this problem, previous work (Alva-Manchego et al., 2017; Zhang and Lapata, 2017; Scarton et al., 2018b) has used subsets of the corpus with only certain combinations of complex-simplified article pairs, e.g. adjacent or non-adjacent pairs. This however reduces the amount of data available for training. 2 System architecture Our approach follows that of Johnson et al. (2017), a multilingual MT approach that adds an artificial token to encode the target language to the beginning of each source sentence in the parallel corpus. With this modified version of the corpus, a single encoder-decoder architecture is used to deal with different language pairs"
P18-2113,W13-4813,1,0.918544,"Missing"
P18-2113,C16-1069,1,0.864667,"and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning. 1 Introduction Text simplification (TS) is the task of modifying an original text into a simpler version of it. One of the main parameters for defining a suitable simplification is the target audience. Examples include elderly, children, cognitively impaired users, nonnative speakers and low-literacy readers. Traditionally, work on TS has been divided in lexical simplification (LS) and syntactic simplification (SS). LS (Paetzold, 2016) deals with the identification and replacement of complex words or phrases. SS (Siddharthan, 2011) performs 1 https://newsela.com/data, v.2016-01-29. 712 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 712–718 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics • Many-to-one: joining – 2+ original sentences are aligned to a single simplified sentence. at training time can still be generated during testing. We show that our zero-shot learning models perform virtually as well as our grade/operati"
P18-2113,W11-1601,0,0.250409,"Carolina Scarton and Lucia Specia Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions create"
P18-2113,P11-2117,0,0.162146,"Carolina Scarton and Lucia Specia Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions create"
P18-2113,Q17-1024,0,0.0978286,"Missing"
P18-2113,P02-1040,0,0.100965,"cted between 0-{1,2,3,4}, 1{2,3,4}, 2-{3,4} and 3-4, where available. Our corpus is also larger than the ones used in (AlvaManchego et al., 2017; Scarton et al., 2018b) and (Zhang and Lapata, 2017). While the former use only adjacent levels (e.g. 0-1, 1-2) and the latter only non-adjacent levels (e.g. 0-2, 1-4), we make use of the full dataset. As baseline we trained a model using OpenNMT and the same hyperparameters as described in §2 on the entire Newsela corpus but without artificial tokens (s2s model). The state-of-theFigure 1: Neural model architecture. We evaluate our models with BLEU3 (Papineni et al., 2002) (a proxy for grammaticality assessment), SARI (Xu et al., 2016)4 (a proxy for simplicity assessment) and Flesch Reading Ease5 (a 2 Torch version: http://opennmt.net/OpenNMT/ The multi-blue.perl script from https://github. com/moses-smt/mosesdecoder 4 https://github.com/cocoxu/ simplification 5 https://github.com/mmautner/ readability 3 714 art model is represented by NTS, which was also trained on the entire corpus using a similar OpenNMT architecture with the same hyperparameters but additional pre-trained word embeddings as described in Nisioi et al. (2017).6 As shown in Table 2 the NTS sys"
P18-2113,L18-1685,1,0.921882,"n ignored thus far: Each instance in the corpus was created for readers with a certain school grade level. Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level. For example, a level 12 article may have simplified counterparts for levels 8 and 4. In other words, the corpus contains instances where the same input leads to different outputs. Disregarding this factor may lead to suboptimal models. To avoid this problem, previous work (Alva-Manchego et al., 2017; Zhang and Lapata, 2017; Scarton et al., 2018b) has used subsets of the corpus with only certain combinations of complex-simplified article pairs, e.g. adjacent or non-adjacent pairs. This however reduces the amount of data available for training. 2 System architecture Our approach follows that of Johnson et al. (2017), a multilingual MT approach that adds an artificial token to encode the target language to the beginning of each source sentence in the parallel corpus. With this modified version of the corpus, a single encoder-decoder architecture is used to deal with different language pairs. Based on the tokens, the source sentences ar"
P18-2113,D15-1166,0,0.0530278,"the rust of the fence near Sasabe. dusty handprints could be seen on the fence near Sasabe. Table 1: Examples of artificial tokens used. fied to grade level 4 or grade level 2. Since the reference for grade level 4 is a copy of the original, the operation token for this case is <identical>. For level 2 the reference is a rewrite and, therefore, the operation token is <elaboration>. We use OpenNMT2 as our encoder-decoder architecture. Both encoder and decoder have two LSTM layers, hidden states of size 500 and dropout = 0.3. Global attention combined with input-feeding is used, as describe in (Luong et al., 2015). A model is trained for each dataset constructed with different artificial tokens for 13 epochs. The best model is selected according to perplexity on the development set. Figure 1 shows the architecture of the neural network, including attention and input-feeding. In this example, <token> represents the artificial token added to the pre-processed data. proxy for readability assessment). According to Xu et al. (2016), BLEU shows high correlation with human scores for grammaticality and meaning preservation, whilst SARI shows high correlation with human scores for simplicity. Although previous"
P18-2113,L18-1553,1,0.922898,"n ignored thus far: Each instance in the corpus was created for readers with a certain school grade level. Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level. For example, a level 12 article may have simplified counterparts for levels 8 and 4. In other words, the corpus contains instances where the same input leads to different outputs. Disregarding this factor may lead to suboptimal models. To avoid this problem, previous work (Alva-Manchego et al., 2017; Zhang and Lapata, 2017; Scarton et al., 2018b) has used subsets of the corpus with only certain combinations of complex-simplified article pairs, e.g. adjacent or non-adjacent pairs. This however reduces the amount of data available for training. 2 System architecture Our approach follows that of Johnson et al. (2017), a multilingual MT approach that adds an artificial token to encode the target language to the beginning of each source sentence in the parallel corpus. With this modified version of the corpus, a single encoder-decoder architecture is used to deal with different language pairs. Based on the tokens, the source sentences ar"
P18-2113,P14-1041,0,0.138993,"Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers. The latter consists of news arti"
P18-2113,I17-3007,1,0.831656,"ation: concatenation of the two above tokens. Different from the grade level, which can be available at test time simply by knowing the intended reader of the text, information about the operations to be performed, which we extracted from the parallel corpus, will not be available at test time. We use gold labels extracted from the parallel corpus for an oracle experiment but also use a classifier that predicts the operations for the test set based on those in the training data. We built a simple Naive Bayes classifier using the scikit-learn toolkit (Pedregosa et al., 2011) and nine features (Scarton et al., 2017): • number of tokens / punctuation / content words / clauses, • ratio of the number of verbs / nouns / adjectives / adverbs / connectives to the number of content words. Table 1 shows examples of the tokens used when an original instance is marked to be simpliWe propose a way of making use of this information to build more informed TS models that are aware of different types of target audiences, while still making use of the full dataset for learning. Inspired by the work of Johnson et al. (2017) for MT, we add to each original instance an artificial token that represents the target grade leve"
P18-2113,P17-2014,0,0.442451,"y of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers. The latter consists of news articles professionally s"
P18-2113,P17-2016,0,0.0447413,"erparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers. The latter consists of news articles professionally simplified for various specific audiences following the US school grade system. To build simplification models, the pairs of articles in these corpora have been aligned at the level of smaller units using standard algorithms (Coster and Kauchak, 2011b; Paetzold and ˇ Specia, 2016; Stajner et al., 2017). Based on the number of sentences involved in these alignments, one can categorise alignments into four types of coarse-grained simplification operations: • Identical: an original sentence is aligned to itself, i.e. no simplification is performed. • Elaboration: an original sentence is aligned to a single, rewritten simplified sentence. • One-to-many: splitting – an original sentence is aligned to 2+ simplified sentences. Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is"
P18-2113,D11-1038,0,0.174376,"Missing"
P18-2113,P12-1107,0,0.468692,"Missing"
P18-2113,Q15-1021,0,0.211546,"e. at training time can still be generated during testing. We show that our zero-shot learning models perform virtually as well as our grade/operationinformed models (§4). To the best of our knowledge, this is the first work to build TS models for specific target audiences and to explore zero-shot learning for this application. We hereafter refer to the unit of simplification, i.e. one or more original or simplified sentences, as instances. The Newsela corpus is seen as having higher quality than W-SW because its simplifications are created by professionals, following well defined guidelines (Xu et al., 2015). It is also larger which is preferable for training corpus-based models. More interestingly, the Newsela corpus has a feature that has been ignored thus far: Each instance in the corpus was created for readers with a certain school grade level. Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level. For example, a level 12 article may have simplified counterparts for levels 8 and 4. In other words, the corpus contains instances where the same input leads to different outputs. Disrega"
P18-2113,Q16-1029,0,0.324751,"architecture. Both encoder and decoder have two LSTM layers, hidden states of size 500 and dropout = 0.3. Global attention combined with input-feeding is used, as describe in (Luong et al., 2015). A model is trained for each dataset constructed with different artificial tokens for 13 epochs. The best model is selected according to perplexity on the development set. Figure 1 shows the architecture of the neural network, including attention and input-feeding. In this example, <token> represents the artificial token added to the pre-processed data. proxy for readability assessment). According to Xu et al. (2016), BLEU shows high correlation with human scores for grammaticality and meaning preservation, whilst SARI shows high correlation with human scores for simplicity. Although previous work have also relied on human judgements of grammaticality, meaning preservation and simplicity, in our case such a type of evaluation is infeasible: we would need to involve judges with specific grade levels or rely on professionals who are experts in grade level-specific simplification to make such assessments. 3 Reader-specific TS models Our version of the Newsela corpus has 550, 644 instance pairs (11M original"
P18-2113,D17-1062,0,0.597037,"Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers. The latter consists of news articles professionally simplified for various spe"
P18-2113,C10-1152,0,0.271894,"Target Audiences Carolina Scarton and Lucia Specia Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Street, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract structural transformations such as changing a sentence from passive to active voice. However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether. These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017). This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version. In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed. For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their"
S15-2015,S13-1004,0,0.0316499,"submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 201"
S15-2015,W05-0909,0,0.143784,"STS English Task: • ModelX: Deep Regression framework with the full feature set from n-gram overlaps, Shallow Parsing and METEOR. For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score over a sequence of 1-5 ngrams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...) or SP-LEMMA(a+dog,a+dog+jump, ...). 5 Experiments and Results METEOR METEOR aligns the translation to a reference translation first then it uses unigram mapping to match words at their surface forms, word stems, synonym matches and paraphrase matches (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). Different from the n-gram and shallow parsing features, METEOR makes a distinction between content words and function words and the precision and recall is measured by weighing them differently. 87 • ModelY: Bayesian Ridge Regressor with the full feature set • ModelZ: Deep Regression framework with only METEOR features For the hidden regressors layer of the deep regression models, we have used the multivariate linear, logistic, Bayesian ridge, elastic net, random sample consensus and support vector (radial basis function kernel) regressors.2 The final layer regres"
S15-2015,S13-1020,0,0.218511,"Missing"
S15-2015,S13-1034,1,0.905276,"Missing"
S15-2015,S14-2085,0,0.112676,"Missing"
S15-2015,N10-1031,0,0.0555248,"X: Deep Regression framework with the full feature set from n-gram overlaps, Shallow Parsing and METEOR. For SP-POS, SP-LEMMA and SP-IOB, we use the NIST-like measure where we not only consider the individual POS, LEMMA or IOB tags but an accumulated score over a sequence of 1-5 ngrams, e.g. SP-POS(DT+NN,DT+NN+VBZ, ...) or SP-LEMMA(a+dog,a+dog+jump, ...). 5 Experiments and Results METEOR METEOR aligns the translation to a reference translation first then it uses unigram mapping to match words at their surface forms, word stems, synonym matches and paraphrase matches (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). Different from the n-gram and shallow parsing features, METEOR makes a distinction between content words and function words and the precision and recall is measured by weighing them differently. 87 • ModelY: Bayesian Ridge Regressor with the full feature set • ModelZ: Deep Regression framework with only METEOR features For the hidden regressors layer of the deep regression models, we have used the multivariate linear, logistic, Bayesian ridge, elastic net, random sample consensus and support vector (radial basis function kernel) regressors.2 The final layer regressor is a Bayesian ridge regr"
S15-2015,W14-3351,0,0.173018,"Missing"
S15-2015,C12-2044,0,0.0694697,": 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous approaches have trained a different system for each subcorpus provided by the task organizers. We have chosen to combine the different subcorpora since MT evaluation metrics are expected to be robust against text types and domains (Han et al., 2012; Pad´o et al., 2009). Much of the previous work on using MT evaluation metrics is based on improving the regressors through algorithm choice, feature selection and parameters tuning. We introduce a novel architecture of hybrid supervised machine learning, Deep Regression, which attempts to combine different regressors and automating feature selection by means of dimensionality reduction. 1 Refers to the token cosine baseline (baseline-tokencos) from the task organizers. 85 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 85–89, c Denver, Colorado, Jun"
S15-2015,S14-2102,0,0.348269,"ce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous approaches have trained a different system for each subcorpus provided by the task organizers. We have chosen to combine the different subcorpora since MT evaluation metrics are expected to be robust against text types and domains (Han et al., 2012; Pad´o et al., 2"
S15-2015,P09-1034,0,0.064364,"Missing"
S15-2015,P02-1040,0,0.0962895,"ch metric comprises several features that compute the translation quality by comparing every translation against one or several reference translations. We consider three sets of features: n-gram overlaps, Shallow Parsing metrics and METEOR. These metrics correspond to the lexical, syntactic and semantic levels respectively. 4.1 N -gram Overlaps Gonz`alez et al. (2014) reintroduces the notion of language independent metrics relying on n-gram overlaps. This is similar to the BLEU metric that calculates the geometric mean of n-gram precision by comparing the translation against its reference(s) (Papineni et al., 2002) without the brevity penalty. Different from BLEU, the n-gram overlaps are computed as similarity coefficients instead of taking the crude proportion of overlap n-gram. n -gramoverlap = sim n -gramtrans ∩ n -gramref Figure 1: Deep Regression Architecture. Figure 1 presents the Deep Regression architecture where the inputs are fed into the different hidden regressors and unlike traditional neural network, each regressor produces a discrete output with a different cost function unlike the consistent activation function in neural nets. Different from ensemble learning, the voting/selection determ"
S15-2015,S12-1100,1,0.739167,"University of Sheffield to the STS English shared task at SemEval2015. We have submitted three models that use Machine Translation (MT) evaluation metrics as features to build supervised regressors that predict the similarity scores for the STS task. We introduce two variants of a novel deep regressor architecture and a classical baseline regression system that uses MT evaluation metrics as input features. Related Work Previously, research teams have applied MT evaluation metrics for the STS task with increasingly better results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Rios et al. (2012) trained a Support Vector Regressor scoring a Pearson correlation mean of 0.3825 (Baseline1 : 0.4356). Barr´onCede˜no et al. (2013) also used a Support Vector Regressor and did better than the baseline at 0.4037 mean score (Baseline: 0.3639). Huang and Chang (2014) used a linear regressor and scored 0.792 beating the baseline system (Baseline: 0.613). Another notable mention of MT technology in the STS task is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014). These previous"
S15-2015,W00-0726,0,0.036069,"Missing"
S15-2015,1992.tmi-1.7,0,0.226955,"input is reduced to the number of hidden regressors and the input for the last layer regressors is a latent layer in the higher dimensional space. Within a standard neural net, every node in the latent layer is influenced by all the perceptrons in the previous layer. In contrast, each latent dimen86  We use 16 features of n-gram overlap by considering both the cosine similarity and Jaccard Index in calculating the n-gram overlaps for character and token n-gram from the order of bigrams to 5-grams. In addition, we use the ratio of n-gram lengths and the Jaccard similarity of pseudo-cognates (Simard et al., 1992) as the 17th and 18th n-gram overlap features. 4.2 Shallow Parsing The Shallow Parsing (SP) metric measures the syntactic similarities by computing the overlaps between the translation and the reference translation at the Parts-Of-Speech (POS), word lemmas and base phrase chunks level. The purpose of the SP metric is to capture the proportion of lexical items correctly translated according to their shallow syntactic realization. The base phrase chunks are tagged using the BIOS toolkit (Surdeanu et al., 2005) and POS tagging and lemmatization are achieved using SVMTool (Gim´enez and M`arquez, 2"
S15-2015,S14-2010,0,\N,Missing
S15-2015,S12-1051,0,\N,Missing
S16-1095,S13-1004,0,0.0692141,"cutting open a box.”, an STS system predicts a real number similarity score on a scale of 0 (no relation) to 5 (semantic equivalence). This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Su"
S16-1095,1983.tc-1.13,0,0.706143,"Missing"
S16-1095,N10-1031,0,0.113645,"ls that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit sema"
S16-1095,W14-3351,0,0.058462,"Missing"
S16-1095,D15-1124,1,0.873884,"Missing"
S16-1095,S14-2102,0,0.554526,"Missing"
S16-1095,S14-2003,0,0.013839,"LEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scores for text of different granularity. Huang and Chang (2014) used a linear regressor solely with MT evalu1 Refers to the token cosine (baseline-tokencos) in STS-2012. 628 Proceedings of SemEval-2016, pages 628–633, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics baseline system ation metrics (BLEU, METEOR, ROUGE) to compute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 Eng"
S16-1095,P04-1077,0,0.0772826,"2) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al.,"
S16-1095,W05-0904,0,0.0464141,"ineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scores for text of different granularity. Huang and Chang (2014) used a linear regressor solely with MT ev"
S16-1095,W12-3129,0,0.0199825,"ompute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 English STS and Twitter similarity tasks, Bertero and Fung (2015) trained a neural network classifier using (i) lexical similarity features based on WordNet (Miller, 1995), (ii) neural auto-encoders (Socher et al., 2011), syntactic features based on parse tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention of MT technology in the STS tasks is the use of referential translation machines to predict and derive features instead of using MT ev"
S16-1095,P02-1040,0,0.114057,"task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 200"
S16-1095,S12-1100,1,0.813789,"ation) to 5 (semantic equivalence). This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 2 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improving results (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). At the pilot English STS-2012 task, Rios et al. (2012) trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Li"
S16-1095,S12-1060,0,0.060836,"Missing"
S16-1095,2006.amta-papers.25,0,0.0633173,"xical overlaps between the surface strings, named entities and semantic role labels and the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers’ baseline system1 which scored 0.4356. For the English STS-2013 task, Barr´on-Cede˜no et al. (2013) also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE (Lin and Och, 2004), NIST (Doddington, 2002), TER (Snover et al., 2006)) with measures that compute similarities of dependency and constituency parses (Liu and Gildea, 2005) and semantic roles, discourse representation and explicit semantic analysis (Gabrilovich and Markovitch, 2007) annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit (Gim´enez and M`arquez, 2010). They scored 0.4037 mean score and performed better than the Takelab baseˇ c et al., 2012) at 0.3639. line (Sari´ At the SemEval-2014 Cross-level Semantic Similarity task (Jurgens et al., 2014; Jurgens et al., 2015), participating teams submitted similarity scor"
S16-1095,W14-3354,0,0.0234987,"feature set, we use 52 shallow parsing features described in (Tan et al., 2015); they measure the similarity coefficients from the n-gram overlaps of the lexicalized shallow parsing (aka chunking) annotations. As for semantics, we use 44 similarity coefficients from Named Entity (NE) annotation overlaps between two texts. After some feature analysis, we found that 22 out of the 44 NE n-gram overlap features and 1 of the shallow parsing features have extremely low variance across all sentence pairs in the training data. We removed these features before training our models. 3.1.2 BEER Features Stanojevic and Simaan (2014) presents an MT evaluation metric that uses character n-gram overlaps, the Kendall tau distance of the monotonic word order (Isozaki et al., 2010; Birch and Osborne, 2010) and abstract ordering patterns from tree factorization of permutations (Zhang and Gildea, 2007). While Asiya features are agnostic to word classes, BEER differentiates between function words and non-function words when calculating its adequacy features. 3.1.3 METEOR Features METEOR first aligns the translation to its reference, then it uses the unigram mapping to see whether they match based on their surface forms, Linear Bo"
S16-1095,S15-2027,0,0.0913526,"e tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention of MT technology in the STS tasks is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics (Bic¸ici and van Genabith, 2013; Bic¸ici and Way, 2014; Bicici, 2015). 3 Approach Following the success of systems that use MT evaluation metrics, we train three regression models using an array of MT metrics based on lexical, syntactic and semantic features. 3.1 Feature Matrix Machine translation evaluation metrics utilize various degrees of lexical, syntactic and semantic information. Each metric consi"
S16-1095,P15-1150,0,0.057508,"erently. We use all four variants of METEOR: exact, stem, synonym and paraphrase. gressor (XGBoost) (Chen and He, 2015; Chen and Guestrin, 2015). They were trained using all features described in Section 3. We have released the MT metrics annotations of the STS data and implementation of systems on https://github.com/alvations/stasis /blob/master/notebooks/ARMOR.ipynb 3.1.4 ReVal Features 4 ReVal (Gupta et al., 2015) is a deep neural net based metric which uses the cosine similarity score between the Tree-based Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Tai et al., 2015) dense vector space representations of two sentences. 3.2 Models We annotated the STS 2012 to 2015 datasets with the features as described in Section 3.1 and submitted three models to the SemEval-2016 English STS Task using (i) a linear regressor (Linear), (ii) boosted tree regressor (Boosted) (Friedman, 2001) and (iii) eXtreme Gradient Boosted tree re630 Results Table 1 presents the official results for our submissions to the English STS task. The bottom part of the table presents the median and the best correlation results across all participating teams for the respective domains. Our baseli"
S16-1095,S15-2015,1,0.847418,"Missing"
S16-1095,U06-1019,0,0.0223859,"California, June 16-17, 2016. 2016 Association for Computational Linguistics baseline system ation metrics (BLEU, METEOR, ROUGE) to compute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 English STS and Twitter similarity tasks, Bertero and Fung (2015) trained a neural network classifier using (i) lexical similarity features based on WordNet (Miller, 1995), (ii) neural auto-encoders (Socher et al., 2011), syntactic features based on parse tree edit distance (Zhang and Shasha, 1989; Wan et al., 2006) and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA (Habash and Elkholy, 2008), BADGER (Parker, 2008) and MEANT (Lo et al., 2012). For the classic English STS task in SemEval2015, Tan et al. (2015) used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system (Sultan et al., 2015) achieved 0.8015. Another notable mention o"
S16-1095,W07-0404,0,0.0167353,"d Entity (NE) annotation overlaps between two texts. After some feature analysis, we found that 22 out of the 44 NE n-gram overlap features and 1 of the shallow parsing features have extremely low variance across all sentence pairs in the training data. We removed these features before training our models. 3.1.2 BEER Features Stanojevic and Simaan (2014) presents an MT evaluation metric that uses character n-gram overlaps, the Kendall tau distance of the monotonic word order (Isozaki et al., 2010; Birch and Osborne, 2010) and abstract ordering patterns from tree factorization of permutations (Zhang and Gildea, 2007). While Asiya features are agnostic to word classes, BEER differentiates between function words and non-function words when calculating its adequacy features. 3.1.3 METEOR Features METEOR first aligns the translation to its reference, then it uses the unigram mapping to see whether they match based on their surface forms, Linear Boosted XGBoost Median Best answer-answer 0.31539 0.37717 0.47716 0.48018 0.69235 headlines 0.76551 0.77183 0.78848 0.76439 0.82749 plagiarism 0.82063 0.81529 0.83212 0.78949 0.84138 postediting 0.83329 0.84528 0.84960 0.81241 0.86690 question-question 0.73987 0.66825"
S16-1095,S15-2010,0,\N,Missing
S16-1095,S14-2085,0,\N,Missing
S16-1095,W05-0909,0,\N,Missing
S16-1095,S12-1051,0,\N,Missing
W10-1001,W09-2105,1,0.77245,"Missing"
W10-1001,dias-da-silva-etal-2008-automatic,0,0.0263079,"Missing"
W10-1001,N07-1058,0,0.509808,"more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on models for literary and expository texts, given that traditional metrics like Flesch-Kincaid Level score tend to overpredict the difficulty of literary texts and underpredict the difficulty of expository texts. Heilman et al. (2008) investigate an appropriate scale of measurement for reading difficulty – nominal, ordinal, or interval – by comparing the effectiveness of statistical models for each type of data. Petersen and O"
W10-1001,W08-0909,0,0.823408,"e. With our readability assessment tool, the author is able to automatically check the complexity/readability level of the original text, as well as modified versions of such text produced as he/she applies simplification operations offered by SIMPLIFICA, until the text reaches the expected level, adequate for the target reader. In this paper we present such readability assessment tool, developed as part of the PorSimples project, and discuss its application within the authoring tool. Different from previous work, the tool does not model text difficulty according to linear grade levels (e.g., Heilman et al., 2008), but instead maps the text into the three levels of literacy defined by INAF: rudimentary, basic or advanced. Moreover, it uses a more comprehensive set of features, different learning techniques and targets a new language and application, as we discuss in Section 4. More specifically, we address the following research questions: 1. Given some training material, is it possible to detect the complexity level of Portuguese texts, which corresponds to the different literacy levels defined by INAF? 2. What is the best way to model this problem and which features are relevant? We experiment with n"
W10-1001,W08-0911,0,0.115784,"Missing"
W10-1001,D08-1020,0,0.197,"lity Assessment Recent work on readability assessment for the English language focus on: (i) the feature set used to capture the various aspects of readability, to evaluate the contribution of lexical, syntactic, semantic and discursive features; (ii) the audience of the texts the readability measurement is intended to; (iii) the genre effects on the calculation of text difficult; (iv) the type of learning technique which is more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on mode"
W10-1001,W07-1001,0,0.0352639,"Missing"
W10-1001,P05-1065,0,0.40269,"f learning technique which is more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on models for literary and expository texts, given that traditional metrics like Flesch-Kincaid Level score tend to overpredict the difficulty of literary texts and underpredict the difficulty of expository texts. Heilman et al. (2008) investigate an appropriate scale of measurement for reading difficulty – nominal, ordinal, or interval – by comparing the effectiveness of statistical models for each type"
W10-1001,E09-1027,0,\N,Missing
W11-4503,W10-1612,0,0.0614124,"Missing"
W11-4503,P98-1013,0,0.160983,"de tempo demandada, existem iniciativas que visam criar recursos léxicos automaticamente ou semiautomaticamente. As duas principais técnicas computacionais 20 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 20–29, c Cuiab´ a, MT, Brazil, October 24–26, 2011. 2011 Sociedade Brasileira de Computa¸c˜ ao utilizadas nesta tarefa são o uso de aprendizado de máquina em córpus1 e o uso de recursos léxicos já existentes para outras línguas, numa abordagem cross-linguística. A língua inglesa possui tradição na criação de RLC’s. Os principais são: FrameNet [Baker et al., 1998], PropBank [Palmer et al., 2005], WordNet de Princeton (aqui chamada de WordNet.Pr) [Fellbaum, 1998] e VerbNet [Kipper, 2005], que foram criados manualmente ou semiautomaticamente. Em especial, a VerbNet é um RLC que trata especificamente de verbos e possui informações sintático-semânticas dos mesmos, seguindo a taxonomia de classe proposta por Levin (1993). Levin definiu que os verbos que compartilham o mesmo comportamento sintático (as mesmas alternâncias sintáticas) devem, também, compartilhar comportamento semântico. Como exemplos seguem as expressões (1) e (2), retiradas do trabalho de L"
W11-4503,dias-da-silva-etal-2008-automatic,0,0.0307924,"Missing"
W11-4503,P04-2007,0,0.0543609,"Missing"
W11-4503,E03-1040,0,0.0273598,"s classes de Levin: Cançado (1996) (verbos psicológicos); Chagas de Souza (2001) (uma construção própria da língua portuguesa, a construção adversativa); Moraes (2008) (verbos de movimento e suas alternâncias e classes) e Amaral (2010) (verbos de modo de movimento). Porém, esses e outros trabalhos tratam, geralmente, de um conjunto fechado de verbos e não disponibilizam os resultados da análise em formato eletrônico 2 para que possam ser aproveitados computacionalmente. Há, também, iniciativas para a construção automática de RLC’s verbais, utilizando aprendizado de máquina, como o trabalho de Joanis and Stevenson (2003) e Sun and Korhonen (2009) para o inglês, Merlo et al. (2002) para o italiano, Ferrer 1 Neste trabalho escolheu-se o aportuguesamento da palavra corpus/corpora para córpus/córpus. Há uma iniciativa do Núcleo de Pesquisa em Semântica Lexical (NuPes), coordenado pela professora Marcia Cançado, da Universidade Federal de Minas Gerais, em disponibilizar de forma mais acessível os dados gerados nos trabalhos dos membros do grupo (Márcia Cançado, 2011, comunicação pessoal). 2 21 (2004) para o espanhol, Schulte in Walde (2006) para o alemão e Sun et al. (2010) para o francês. Todos estes trabalhos ut"
W11-4503,P02-1027,0,0.0260117,"ouza (2001) (uma construção própria da língua portuguesa, a construção adversativa); Moraes (2008) (verbos de movimento e suas alternâncias e classes) e Amaral (2010) (verbos de modo de movimento). Porém, esses e outros trabalhos tratam, geralmente, de um conjunto fechado de verbos e não disponibilizam os resultados da análise em formato eletrônico 2 para que possam ser aproveitados computacionalmente. Há, também, iniciativas para a construção automática de RLC’s verbais, utilizando aprendizado de máquina, como o trabalho de Joanis and Stevenson (2003) e Sun and Korhonen (2009) para o inglês, Merlo et al. (2002) para o italiano, Ferrer 1 Neste trabalho escolheu-se o aportuguesamento da palavra corpus/corpora para córpus/córpus. Há uma iniciativa do Núcleo de Pesquisa em Semântica Lexical (NuPes), coordenado pela professora Marcia Cançado, da Universidade Federal de Minas Gerais, em disponibilizar de forma mais acessível os dados gerados nos trabalhos dos membros do grupo (Márcia Cançado, 2011, comunicação pessoal). 2 21 (2004) para o espanhol, Schulte in Walde (2006) para o alemão e Sun et al. (2010) para o francês. Todos estes trabalhos utilizam aprendizado de máquina não supervisionado. Neste traba"
W11-4503,J05-1004,0,0.0122924,"ciativas que visam criar recursos léxicos automaticamente ou semiautomaticamente. As duas principais técnicas computacionais 20 Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, pages 20–29, c Cuiab´ a, MT, Brazil, October 24–26, 2011. 2011 Sociedade Brasileira de Computa¸c˜ ao utilizadas nesta tarefa são o uso de aprendizado de máquina em córpus1 e o uso de recursos léxicos já existentes para outras línguas, numa abordagem cross-linguística. A língua inglesa possui tradição na criação de RLC’s. Os principais são: FrameNet [Baker et al., 1998], PropBank [Palmer et al., 2005], WordNet de Princeton (aqui chamada de WordNet.Pr) [Fellbaum, 1998] e VerbNet [Kipper, 2005], que foram criados manualmente ou semiautomaticamente. Em especial, a VerbNet é um RLC que trata especificamente de verbos e possui informações sintático-semânticas dos mesmos, seguindo a taxonomia de classe proposta por Levin (1993). Levin definiu que os verbos que compartilham o mesmo comportamento sintático (as mesmas alternâncias sintáticas) devem, também, compartilhar comportamento semântico. Como exemplos seguem as expressões (1) e (2), retiradas do trabalho de Levin (1993, p. 2), que apresenta"
W11-4503,J06-2001,0,0.0464411,"Missing"
W11-4503,D09-1067,0,0.0191723,"996) (verbos psicológicos); Chagas de Souza (2001) (uma construção própria da língua portuguesa, a construção adversativa); Moraes (2008) (verbos de movimento e suas alternâncias e classes) e Amaral (2010) (verbos de modo de movimento). Porém, esses e outros trabalhos tratam, geralmente, de um conjunto fechado de verbos e não disponibilizam os resultados da análise em formato eletrônico 2 para que possam ser aproveitados computacionalmente. Há, também, iniciativas para a construção automática de RLC’s verbais, utilizando aprendizado de máquina, como o trabalho de Joanis and Stevenson (2003) e Sun and Korhonen (2009) para o inglês, Merlo et al. (2002) para o italiano, Ferrer 1 Neste trabalho escolheu-se o aportuguesamento da palavra corpus/corpora para córpus/córpus. Há uma iniciativa do Núcleo de Pesquisa em Semântica Lexical (NuPes), coordenado pela professora Marcia Cançado, da Universidade Federal de Minas Gerais, em disponibilizar de forma mais acessível os dados gerados nos trabalhos dos membros do grupo (Márcia Cançado, 2011, comunicação pessoal). 2 21 (2004) para o espanhol, Schulte in Walde (2006) para o alemão e Sun et al. (2010) para o francês. Todos estes trabalhos utilizam aprendizado de máqu"
W11-4506,W10-1001,1,0.843543,"Entretanto, essas fórmulas mostram-se insuficientes quando não conseguem, por exemplo, abranger a importância de marcadores discursivos [Williams, 2004]. A maioria dos trabalhos mais atuais sobre avaliação da inteligibilidade de textos usa métodos de aprendizado de máquina (AM) e avalia suas abordagens para textos em inglês. Em outra abordagem, um trabalho para a língua alemã [Glöckner et al., 2006] propõe avaliar a inteligibilidade via uso de várias features e de uma medida global de inteligibilidade similar ao índice Flesch. A língua portuguesa também foi contemplada em trabalhos recentes [Aluísio et al., 2010; Scarton e Aluísio, 2010; Scarton et al., 2010]. Esses trabalhos empregam a abordagem de avaliação da inteligibilidade para apoiar a simplificação de textos destinados a leitores com níveis de letramento baixos. Os trabalhos atuais, geralmente, tratam de cinco aspectos principais: a) Avaliam o conjunto de features usado para capturar os vários aspectos da inteligibilidade e a contribuição das features de vários níveis linguísticos. É o caso do trabalho de Pitler and Nenkova (2008) que, com base nos trabalhos de criação de rubricas para avaliação de redações de alunos (essay scoring) [Burstein"
W11-4506,E09-1027,0,0.0144981,"a vocabulário, sintaxe, elementos de coesão lexical e relações discursivas para medir a qualidade de um texto. Feng et al. (2010), seguindo os estudos de Pitler e Nenkova (2008), propõem o uso de várias features, que são comparadas e avaliadas em termos de seu impacto para prever uma série de livros de leitura adequados para estudantes do nível fundamental. b) Focam uma dada audiência para a qual a avaliação da inteligibilidade é destinada. É o caso de trabalhos focando em aprendizes do inglês como língua estrangeira [Schwarm and Ostendorf, 2005], pessoas com capacidade intelectual reduzida [Feng et al., 2009], pessoas com problemas cognitivos causados por Alzheimer [Roark at al., 2007], textos para adultos ou para crianças [Scarton e Aluísio, 2010] e textos para um determinado nível de letramento [Aluísio et al., 2010]. c) Tratam dos efeitos do gênero textual no cálculo do índice de inteligibilidade. É o caso dos trabalhos de Sheehan et al. (2007), que estudam modelos para textos expositivos e literários, considerando que o uso de índices simples, como FleschKincaid Level, tendem a subestimar a dificuldade dos primeiros e sobrestimar a dos últimos e Scarton et al. (2010) que apresentam os primeir"
W11-4506,C10-2032,0,0.0296253,"etramento baixos. Os trabalhos atuais, geralmente, tratam de cinco aspectos principais: a) Avaliam o conjunto de features usado para capturar os vários aspectos da inteligibilidade e a contribuição das features de vários níveis linguísticos. É o caso do trabalho de Pitler and Nenkova (2008) que, com base nos trabalhos de criação de rubricas para avaliação de redações de alunos (essay scoring) [Burstein et al., 2003], 51 propõem um framework unificado composto de features relacionadas a vocabulário, sintaxe, elementos de coesão lexical e relações discursivas para medir a qualidade de um texto. Feng et al. (2010), seguindo os estudos de Pitler e Nenkova (2008), propõem o uso de várias features, que são comparadas e avaliadas em termos de seu impacto para prever uma série de livros de leitura adequados para estudantes do nível fundamental. b) Focam uma dada audiência para a qual a avaliação da inteligibilidade é destinada. É o caso de trabalhos focando em aprendizes do inglês como língua estrangeira [Schwarm and Ostendorf, 2005], pessoas com capacidade intelectual reduzida [Feng et al., 2009], pessoas com problemas cognitivos causados por Alzheimer [Roark at al., 2007], textos para adultos ou para cria"
W11-4506,N07-1058,0,0.0120193,"edida para dificuldade de leitura – nominal, ordinal e intervalar – via comparação da efetividade de modelos estatísticos para estes tipos de dados, enquanto que Petersen and Ostendorf (2009) defenderam o uso de Support Vector Machines tanto como modelo de regressão quanto de classificação para predizer níveis de inteligibilidade. Para o português, Aluísio et al. (2010) avaliaram modelos nominal, ordinal e intervalar para textos em português (originais e simplificados) obtendo resultados similares. e) Focam-se em aplicações computacionais que utilizam métodos de avaliação de inteligibilidade. Heilman et al. (2007) usaram um avaliador de inteligibilidade em sistemas tutores inteligentes para indicar textos de leitura com o nível adequado de dificuldade para aprendizes do inglês como segunda língua. Miltsakali e Troutt (2007 e 2008) propuseram uma ferramenta automática para avaliar textos da Web que fossem adequados para adolescentes e adultos com níveis baixos de letramento, enquanto Aluísio et al. (2010) utilizaram um avaliador de inteligibilidade em um editor de simplificação para medir o nível de inteligibilidade com relação a três padrões de letramento (rudimentar, básico e pleno). Considerando os t"
W11-4506,W08-0909,0,0.0137229,"inteligibilidade. É o caso dos trabalhos de Sheehan et al. (2007), que estudam modelos para textos expositivos e literários, considerando que o uso de índices simples, como FleschKincaid Level, tendem a subestimar a dificuldade dos primeiros e sobrestimar a dos últimos e Scarton et al. (2010) que apresentam os primeiros resultados na proposta de um avaliador de inteligibilidade global, usando as features do Coh-Metrix-Port. d) Avaliam qual o modelo estatístico é mais apropriado para os índices (escalas nominais, ordinais ou intervalares) e para a precisão de métodos de aprendizado de máquina. Heilman et al. (2008) investigaram as escalas de medida para dificuldade de leitura – nominal, ordinal e intervalar – via comparação da efetividade de modelos estatísticos para estes tipos de dados, enquanto que Petersen and Ostendorf (2009) defenderam o uso de Support Vector Machines tanto como modelo de regressão quanto de classificação para predizer níveis de inteligibilidade. Para o português, Aluísio et al. (2010) avaliaram modelos nominal, ordinal e intervalar para textos em português (originais e simplificados) obtendo resultados similares. e) Focam-se em aplicações computacionais que utilizam métodos de av"
W11-4506,W08-0911,0,0.0306231,"Missing"
W11-4506,D08-1020,0,0.0518173,"Missing"
W11-4506,W07-1001,0,0.0277346,"Missing"
W11-4506,P05-1065,0,0.0123016,"2003], 51 propõem um framework unificado composto de features relacionadas a vocabulário, sintaxe, elementos de coesão lexical e relações discursivas para medir a qualidade de um texto. Feng et al. (2010), seguindo os estudos de Pitler e Nenkova (2008), propõem o uso de várias features, que são comparadas e avaliadas em termos de seu impacto para prever uma série de livros de leitura adequados para estudantes do nível fundamental. b) Focam uma dada audiência para a qual a avaliação da inteligibilidade é destinada. É o caso de trabalhos focando em aprendizes do inglês como língua estrangeira [Schwarm and Ostendorf, 2005], pessoas com capacidade intelectual reduzida [Feng et al., 2009], pessoas com problemas cognitivos causados por Alzheimer [Roark at al., 2007], textos para adultos ou para crianças [Scarton e Aluísio, 2010] e textos para um determinado nível de letramento [Aluísio et al., 2010]. c) Tratam dos efeitos do gênero textual no cálculo do índice de inteligibilidade. É o caso dos trabalhos de Sheehan et al. (2007), que estudam modelos para textos expositivos e literários, considerando que o uso de índices simples, como FleschKincaid Level, tendem a subestimar a dificuldade dos primeiros e sobrestima"
W13-1014,W06-1207,0,0.0310531,"the uses of the clitic pronoun se share the same realization at the surface form level, the use as a CONSTITUTIVE PARTICLE of pronominal verbs is the only one in which the verb and the clitic form a multiword lexical unit on its own. In the other uses, the clitic keeps a separate syntactic and/or semantic function, as presented in Table 1. The particle se is an integral part of pronominal verbs in the same way as the particles of English phrasal verbs. As future work, we would like to investigate possible semantic contributions of the se particle to the meaning of pronominal verbs, as done by Cook and Stevenson (2006), for example, who try to automatically classify the uses of the particle up in verb-particle constructions. Like in the present paper, they estimate a set of linguistic features which are in turn used to train a Support Vector Machine (SVM) classifier citecook:2006:mwe. 3 Methodology For the automatic identification of multiword verb+se occurrences, we performed corpus searches on the PLN-BR-FULL corpus (Muniz et al., 2007), which consists of news texts extracted from a major Brazilian newspaper, Folha de S˜ao Paulo, from 1994 to 2005, with 29,014,089 tokens. The corpus was first preprocessed"
W13-1014,C10-3015,1,0.771149,"Missing"
W13-1014,slavcheva-2006-semantic,0,0.0193784,"SIVE uses (Morais Nunes, 1990; Cyrino, 2007; Pereira-Santos, 2010); REFLEXIVE use (Godoy, 2012), and IN CHOATIVE use (Fonseca, 2010; Nunes-Ribeiro, 2010; Ros´ario Ribeiro, 2011). Despite none of these works concerning specifically pronominal verbs, they provided us an important theoretical basis for the analysis undertaken herein. The problem of the multifunctional use of clitic pronouns is not restricted to Portuguese. Romance languages, Hebrew, Russian, Bulgarian and others also have similar constructions. There are crosslinguistic studies regarding this matter reported in Siloni (2001) and Slavcheva (2006), showing that there are partial coincidence of verbs taking clitic pronouns to produce alternations and reflexive voice. From an NLP perspective, the problem of the ambiguity of the clitic pronoun se was studied by Martins et al. (1999) to solve a problem of categorization, that is, to decide which part-of-speech tag should be assigned to se. However, we have not found studies regarding pronominal verbs aiming at Portuguese automatic language processing. Even though in Portuguese all the uses of the clitic pronoun se share the same realization at the surface form level, the use as a CONSTITUT"
W13-1014,vincze-2012-light,0,0.0198436,"n the one hand, the annotation of uses can be semi-automatically projected on the sentences extracted from the corpus. On the other hand, the findings of this work in terms of syntactic and semantic characteristics can be used to propose features for the classifier, trying to reproduce those that can be automatically obtained (e.g., subcategorization frame) and to simulate those that cannot be easily automated (e.g., whether the subject is animate). For these future experiments, we intend to compare different learning models, based on SVM and on sequence models like conditional random fields (Vincze, 2012). As languages are different in what concerns allowed alternations, the use of clitic se in Portuguese becomes even more complex when approached from a bilingual point of view. Depending on how different the languages compared are, the classification of se adopted here may be of little use. For example, several verbs classified as reflexive in Portuguese, like vestir-se (to dress), barbear-se (to shave) and demitir-se (to resign) are not translated into a reflexive form in English (*to dress oneself, *to shave oneself and *to dismiss oneself ). Similarly, typical inchoative verb uses in Portug"
W14-3343,P04-1077,0,0.295785,"ramework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction. Feature analysis on a number of datasets showed that this feature contributed the most across all datasets. Louis and Nenkova (2013) apply pseudoreferences for summary evaluation. They use six systems classified as “best systems”, “mediocre systems” or “worst systems” to make the comparison, with ROUGE (Lin and Och, 2004) as quality score. They also experiment with a combination of the “best systems” and the “worst systems”. The use of only “best systems” led to the best results. Examples of “bad summaries” are said not to be very useful because a summary close to the worst systems outputs can mean that either it is bad or it is too different from the best systems outputs in terms of content. Albrecht and Hwa (2008) use pseudo-references to improve MT evaluation by combining them with a single human reference. They show that the use of pseudo-references imThis paper presents the use of consensus among Machine"
W14-3343,J13-2002,0,0.0248237,"od that uses sentence-level prediction models for document-level QE. They also use a pseudo-references-based feature (based in BLEU) and claim that this feature is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction. Feature analysis on a number of datasets showed that this feature contributed the most across all datasets. Louis and Nenkova (2013) apply pseudoreferences for summary evaluation. They use six systems classified as “best systems”, “mediocre systems” or “worst systems” to make the comparison, with ROUGE (Lin and Och, 2004) as quality score. They also experiment with a combination of the “best systems” and the “worst systems”. The use of only “best systems” led to the best results. Examples of “bad summaries” are said not to be very useful because a summary close to the worst systems outputs can mean that either it is bad or it is too different from the best systems outputs in terms of content. Albrecht and Hwa (2008) use ps"
W14-3343,P02-1040,0,0.0900983,"etrics are used in such settings as a way of predicting translation quality. While reference translations are not available for QE, previous work has explored the so called pseudoreferences (Soricut and Echihabi, 2010; Soricut et al., 2012; Soricut and Narsale, 2012; Shah et al., 2013). Pseudo-references are alternative translations produced by MT systems different from the system that we intend to predict quality for (Albrecht and Hwa, 2008). These can be used to provide additional features to train QE models. Such features are normally figures resulting from automatic metrics (such as BLEU, Papineni et al. (2002)) computed between pseudo-references and the output of the given MT system. Soricut and Echihabi (2010) explore pseudoreferences for document-level QE prediction to 1 http://www.statmt.org/wmt12/ 342 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 342–347, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics compared to the baselines provided. Section 4 presents some conclusions. proves the correlation with human judgements. Soricut and Echihabi (2010) claim that pseudoreferences should be produced by systems as different as po"
W14-3343,W08-0330,0,0.119763,"sets. Louis and Nenkova (2013) apply pseudoreferences for summary evaluation. They use six systems classified as “best systems”, “mediocre systems” or “worst systems” to make the comparison, with ROUGE (Lin and Och, 2004) as quality score. They also experiment with a combination of the “best systems” and the “worst systems”. The use of only “best systems” led to the best results. Examples of “bad summaries” are said not to be very useful because a summary close to the worst systems outputs can mean that either it is bad or it is too different from the best systems outputs in terms of content. Albrecht and Hwa (2008) use pseudo-references to improve MT evaluation by combining them with a single human reference. They show that the use of pseudo-references imThis paper presents the use of consensus among Machine Translation (MT) systems for the WMT14 Quality Estimation shared task. Consensus is explored here by comparing the MT system output against several alternative machine translations using standard evaluation metrics. Figures extracted from such metrics are used as features to complement baseline prediction models. The hypothesis is that knowing whether the translation of interest is similar or dissim"
W14-3343,2013.mtsummit-papers.21,1,0.94296,"MT system training data is also used as pseudo-references to compute training data-based features. The use of pseudoreferences has been shown to outperform strong baseline results. Soricut and Narsale (2012) propose a method that uses sentence-level prediction models for document-level QE. They also use a pseudo-references-based feature (based in BLEU) and claim that this feature is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction. Feature analysis on a number of datasets showed that this feature contributed the most across all datasets. Louis and Nenkova (2013) apply pseudoreferences for summary evaluation. They use six systems classified as “best systems”, “mediocre systems” or “worst systems” to make the comparison, with ROUGE (Lin and Och, 2004) as quality score. They also experiment with a combination of the “best systems” and the “worst systems”. The use of only “best systems” led to the best results. Examples of “bad su"
W14-3343,W05-0909,0,0.0928187,"onsensual information was extracted by using systems submitted to the WMT translation shared tasks of both years. Therefore, for each source sentence in the WMT12/13 data, all translations produced by the participating MT systems of that year were used as pseudo-references. The uedin system outputs for both WMT13 and WMT12 were not considered, since the datasets in Tasks 1.2 and 1.3 were created from translations generated by this system.2 The Asyia Toolkit3 (Gim´enez and M`arquez, 2010) was used to extract the automatic metrics considered as features. BLEU, TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin and Och, 2004) are used in all task variants. For Tasks 1.2 and 1.3 we also use metrics based on syntactic similarities from shallow and dependency parser information (metrics SPOc(*) and DPmHWCM c1, respectively, in Asyia). BLEU is a precision-oriented metric that compares n-grams (n=1-4 in our case) from reference documents against n-grams of the MT output, measuring how close the output of a system is to one or more references. TER (Translation Error Rate) measures the minimum number of edits required to transform the MT output into the closest reference document. METEOR (Me"
W14-3343,2006.amta-papers.25,0,0.278824,"air had 954 English sentences and 3,816 Spanish sentences. In the source file, the English sentences were repeated in batches of 954 sentences. Based on that, we assumed that in the target file each set of 954 translations in sequence corresponded to a given MT system (or human). For each system (human translation is considered as a system, since we do not know the order of the translations), we calculate the consensual information considering the other 2-3 systems available as pseudo-references. The quality scores for Task 1.2 and Task 1.3 were computed as HTER (Human Translation Error Rate (Snover et al., 2006)) and post-editing time, respectively, for both scoring and ranking. This paper describes the use of consensual information for the WMT14 QE shared task (USHEFF-consensus system), simulating a scenario where we do not know the quality of the pseudo-references, nor the characteristics of any MT systems (the system of interest or the systems which generated the pseudo-references). We participated in all variants of Task 1, sentence-level QE, for both for scoring and ranking. Section 2 explains how we extracted consensual information for all tasks. Section 3 shows our official results 343 3 The d"
W14-3343,W12-3121,0,0.0871667,"Specia Department of Computer Science, University of Sheffield Regent Court, 211 Portobello, Sheffield, S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract rank outputs from an MT system. The pseudoreferences-based features are BLEU scores extracted by comparing the output of the MT system under investigation and the output of an offthe-shelf MT system, for both the target and the source texts. The statistical MT system training data is also used as pseudo-references to compute training data-based features. The use of pseudoreferences has been shown to outperform strong baseline results. Soricut and Narsale (2012) propose a method that uses sentence-level prediction models for document-level QE. They also use a pseudo-references-based feature (based in BLEU) and claim that this feature is one of the most powerful in the framework. For QE at sentence-level, Soricut et al. (2012) use BLEU based on pseudo-references combined with other features to build the best QE system of the WMT12 QE shared task.1 Shah et al. (2013) use pseudo-references in the same way to extract a BLEU feature for sentence-level prediction. Feature analysis on a number of datasets showed that this feature contributed the most across"
W14-3343,W12-3118,0,0.192802,"Missing"
W14-3343,P13-4014,1,0.918348,"Missing"
W14-3343,P10-1063,0,\N,Missing
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W15-3040,2014.eamt-1.21,1,0.963963,"rt documents) becomes necessary. This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking. We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN). Little previous research has been done to address document-level QE. Soricut and Echihabi (2010) proposed document-aware features in order to rank machine translated documents. Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE. Finally, Scarton and Specia (2014) and Scarton (2015) introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents. Previous work led to some improvements over the baselines used. However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by Scarton et We present the results of the USHEF and USAAR-USHEF submissions for the WMT15 shared task on document-level quality estimation. The USHEF submissions explored several document and discourse-aware features."
W15-3040,W05-0909,0,0.179643,"those provided by the organisers were used. Tasks we participate in Task 3 (paragraph-level QE) in both subtasks, scoring and ranking. The evaluation for the scoring task was done using Mean Absolute Error (MAE) and the evaluation for the ranking task was done by DeltaAvg (official metrics of the competition). Data the official data of Task 3 - WMT15 QE shared task consist of 1215 paragraphs for ENDE and DE-EN, extracted from the corpora of WMT13 machine translation shared task (Bojar et al., 2013). For training, 800 paragraphs were used and, for test, 415 paragraphs were considered. METEOR (Banerjee and Lavie, 2005) was used as quality labels. Feature combination we experimented with different feature sets: • baseline (17 baseline features only) • baseline + discourse repetition features2 • baseline + document-aware features • baseline + discourse-aware features • all features. Features selected for EN-DE only: • LM probability of target document • LM perplexity of target document (with and without sentence markers) • type/token ration • average number of translations per source word in the document (threshold: prob &gt;0.2/0.5) Backward feature selection3 in order to perform feature selection, we used the"
W15-3040,W15-4916,1,0.869812,"Missing"
W15-3040,C04-1046,0,0.0396634,"eral metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU (Papineni et al., 2002)) or error rates (such as TER (Snover et al., 2006)). However, in some scenarios, human references are not available. For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing. Another example is machine translation for gisting by users of online systems. Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (Blatz et al., 2004; Specia et al., 2009). Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to train supervised machine learning models (classifiers or regressors). A number of data points need to be annotated for quality (by humans or automatically) for training, using a given quality metric. 336 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 336–341, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. al. (2015). Our approach focuses on extracting various features and building mod"
W15-3040,N15-2016,1,0.900849,"y. This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking. We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN). Little previous research has been done to address document-level QE. Soricut and Echihabi (2010) proposed document-aware features in order to rank machine translated documents. Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE. Finally, Scarton and Specia (2014) and Scarton (2015) introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents. Previous work led to some improvements over the baselines used. However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by Scarton et We present the results of the USHEF and USAAR-USHEF submissions for the WMT15 shared task on document-level quality estimation. The USHEF submissions explored several document and discourse-aware features. The USAARUSHEF subm"
W15-3040,2006.amta-papers.25,0,0.0992842,"ch to select the best features from the official baseline. Results show slight improvements over the baseline with the use of discourse features. More interestingly, we found that a model of comparable performance can be built with only three features selected by the exhaustive search procedure. 1 Introduction Evaluating the quality of Machine Translation (MT) systems outputs is a challenging topic. Several metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU (Papineni et al., 2002)) or error rates (such as TER (Snover et al., 2006)). However, in some scenarios, human references are not available. For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing. Another example is machine translation for gisting by users of online systems. Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (Blatz et al., 2004; Specia et al., 2009). Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to train supervised machine learning models (classifiers o"
W15-3040,P10-1063,0,0.0579964,"f each word or sentence individually is not as important as the quality of the review as a whole. Therefore, predicting the quality of the whole document (or paragraph, considering paragraph as short documents) becomes necessary. This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking. We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN). Little previous research has been done to address document-level QE. Soricut and Echihabi (2010) proposed document-aware features in order to rank machine translated documents. Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE. Finally, Scarton and Specia (2014) and Scarton (2015) introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents. Previous work led to some improvements over the baselines used. However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by S"
W15-3040,W12-3121,0,0.015232,"s a whole. Therefore, predicting the quality of the whole document (or paragraph, considering paragraph as short documents) becomes necessary. This paper presents the University of Sheffield (USHEF) and University of Saarland (USAAR) submissions to the Task 3 of the WMT15 QE shared task: paragraph-level scoring and ranking. We submitted systems for both language pairs: English-German (EN-DE) and German-English (DE-EN). Little previous research has been done to address document-level QE. Soricut and Echihabi (2010) proposed document-aware features in order to rank machine translated documents. Soricut and Narsale (2012) use sentence-level features and predictions to improve document-level QE. Finally, Scarton and Specia (2014) and Scarton (2015) introduced discourse-aware features, which are combined with baseline features adapted from sentence-level work, in order to predict the quality of full documents. Previous work led to some improvements over the baselines used. However, several problems remain to be addressed for improving document-level QE, such as the choice of quality label, as discussed by Scarton et We present the results of the USHEF and USAAR-USHEF submissions for the WMT15 shared task on docu"
W15-3040,W12-3102,1,0.794963,"Missing"
W15-3040,2009.eamt-1.5,1,0.80626,"en proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU (Papineni et al., 2002)) or error rates (such as TER (Snover et al., 2006)). However, in some scenarios, human references are not available. For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing. Another example is machine translation for gisting by users of online systems. Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (Blatz et al., 2004; Specia et al., 2009). Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to train supervised machine learning models (classifiers or regressors). A number of data points need to be annotated for quality (by humans or automatically) for training, using a given quality metric. 336 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 336–341, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. al. (2015). Our approach focuses on extracting various features and building models with different com"
W15-3040,P13-4014,1,0.869316,"Missing"
W15-3040,P13-1048,0,0.0414372,"umber of EDU (elementary discourse units) breaks in the source (target) document • number of RST (Rhetorical Structure Theory) Nucleus relations in the source/target document • number of RST Satellite relations in the source/target document. In order to extract the last set of features we use existing NLP tools: For identifying pronouns, we use the output of Charniak’s parser (Charniak, 2000) (we count the P RP tags). Discourse connectives are automatically extracted by the parser of Pitler and Nenkova (2009). RST trees and EDUs are extracted by the discourse parser and discourse segmenter of Joty et al. (2013). 3 toolkit (Pedregosa et al., 2011), to rank the features. Once this feature ranking is produced, we apply a backward feature selection approach. Starting with the features with lower positition in the rank, the method consists in consistently eliminate features, aiming to obtain a feature set that better fit the predictions. For both EN-DE and DE-EN, 38 features were selected. The set of features selected for both languages is: • LM probability of source document • LM perplexity of source document • average trigram frequency in quartile 1/2/3/4 of frequency in a corpus of the source language"
W15-3040,P15-4020,1,0.846447,"ge of nouns in the source and target documents • ratio of percentage of verbs in the source and target documents • ratio of percentage of pronouns in the source and target documents • number of dependencies with aligned constituents normalised by the total number of dependencies (maximum between source and target) • number of sentences (source and target should be the same). Document-level features Along with the official baseline features, we use two different sets of features. The first set contains document-aware features, based on QuEst features for sentence-level QE (Specia et al., 2013; Specia et al., 2015). The second set are features that encompass discourse information, following previous work of Scarton and Specia (2014) and Scarton (2015). 2.1 Document-aware features The 17 baseline features made available by the organisers are the same baseline features used for sentence-level QE, adapted for documentlevel.1 However, as part of the QuEst framework, other sentence-level features can be easily adapted for document-level QE. Our complete set of document-aware features include: • ratio of number of tokens in source and target (and in target and source) • absolute difference between number toke"
W15-3040,P02-1040,0,0.108728,"AARUSHEF submissions used an exhaustive search approach to select the best features from the official baseline. Results show slight improvements over the baseline with the use of discourse features. More interestingly, we found that a model of comparable performance can be built with only three features selected by the exhaustive search procedure. 1 Introduction Evaluating the quality of Machine Translation (MT) systems outputs is a challenging topic. Several metrics have been proposed so far comparing the MT outputs to human translations (references) in terms of ngrams matches (such as BLEU (Papineni et al., 2002)) or error rates (such as TER (Snover et al., 2006)). However, in some scenarios, human references are not available. For example, the use of machine translation in a workflow where good enough translations are given to humans for post-editing. Another example is machine translation for gisting by users of online systems. Quality Estimation (QE) approaches aim to predict the quality of MT outputs without relying on human references (Blatz et al., 2004; Specia et al., 2009). Features from source (original document) and target (MT outputs) and, when available, from the MT system are used to trai"
W15-3040,P09-2004,0,0.0467811,"cument-level evaluation purposes. We considered the discourse-aware features presented in Scarton and Specia (2014), which are already implemented in the QuEst framework (called herein as discourse repetition features): • word/lemma/noun repetition in the source/target document • ratio of word/lemma/noun repetition between source and target documents. Other discourse features were also explored (following the work of Scarton (2015)): • number of pronouns in the source/target document • number of discourse connectives in the source/target document • number of pronouns of each type according to Pitler and Nenkova (2009)’s classification: 1 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 337 Expansion, Temporal, Contingency, Comparison and Non-discourse • number of EDU (elementary discourse units) breaks in the source (target) document • number of RST (Rhetorical Structure Theory) Nucleus relations in the source/target document • number of RST Satellite relations in the source/target document. In order to extract the last set of features we use existing NLP tools: For identifying pronouns, we use the output of Charniak’s parser (Charniak, 2000) (we count the P RP tags). Discourse c"
W15-4916,2012.eamt-1.33,1,0.851202,"Missing"
W15-4916,C04-1046,0,0.147834,"h levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between"
W15-4916,W12-3156,0,0.103809,"sing automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an"
W15-4916,P14-1065,0,0.0528997,"Missing"
W15-4916,P10-1064,1,0.914135,"Missing"
W15-4916,P14-2047,0,0.0154408,"the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use automatic evaluation m"
W15-4916,W13-3303,0,0.0260337,"ssues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall consistent in its lexical choices, nearly as consistent as manually translated texts. Meyer and Webber (2013) and Li et al. (2014) show that the translation of connectives differs from humans to MT, and that the presence of explicit connectives correlates with higher HTER values. Guzm´an et al. (2014) explore rhetorical structure (RST) trees (Mann and Thompson, 1987) for automatic evaluation of MT into English, outperforming traditional metrics at system-level evaluation. Thus far, no previous work has investigated ways to provide a global quality score for an entire document that takes into account document structure, without access to reference translations. Previous work on document-level QE use a"
W15-4916,P02-1040,0,0.0919032,"c and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes). Carpuat and Simard (2012) report that MT output is overall"
W15-4916,P09-2004,0,0.0711715,"ndomly selected from the full corpus. For PE1 and PE2, only source (English) paragraphs with 3-8 sentences were selected (ﬁlter SNUMBER) to ensure that there is enough information beyond sentence-level to be evaluated and make the task feasible for the annotators. These paragraphs were further ﬁltered to select those with cohesive devices. Cohesive devices are linguistic units that play a role in establishing cohesion between clauses, sentences or paragraphs (Halliday and Hasan, 1976). Pronouns and discourse connectives are examples of such devices. A list of pronouns and the connectives from Pitler and Nenkova (2009) was considered for that. Finally, paragraphs were ranked according to the number of cohesive devices they contain and the top 200 paragraphs were selected (ﬁlter C-DEV). Table 3 shows the statistics of the initial corpus and the resulting selection after each ﬁlter. FULL CORPUS S-NUMBER C-DEV Number of Paragraphs 1, 215 394 200 Number of Cohesive devices 6, 488 3, 329 2, 338 Table 3: WMT13 English source corpus. For the PE1 experiment, the paragraphs in CDEV were randomised. Then, sets containing seven paragraphs each were created. For each set, the sentences of its paragraphs were also rando"
W15-4916,potet-etal-2012-collection,0,0.104329,"Missing"
W15-4916,2014.eamt-1.21,1,0.850194,"consider more information than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT"
W15-4916,2006.amta-papers.25,0,0.549835,"es on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the ﬁnal version – HTER (Snover et al., 2006). c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. There are, however, scenarios where quality prediction beyond sentence level is needed, most notably in cases when automatic translations without post-editing are required. This is the case, for example, of quality prediction for an entire product review translation in order to decide whether or not it can be published as is, so that customers speaking other languages can understand it. The quality of a document is often seen as some form of aggregation of the quali"
W15-4916,P10-1063,0,0.677539,"than sentence-level quality. This includes, for example, the topic and structure of the document and the relationship between its sentences. While certain sentences are considered perfect in isolation, their combination in context may lead to incoherent text. Conversely, while a sentence can be considered poor in isolation, when put in context, it may beneﬁt from information in surrounding sentences, leading to a document that is ﬁt for purpose. Document-level quality prediction is a rather understudied problem. Recent work has looked into document-level prediction (Scarton and Specia, 2014; Soricut and Echihabi, 2010) using automatic metrics such as BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as quality labels. However, their results highlighted issues with these metrics for the task at hand: the evaluation of the scores predicted in terms of mean error was inconclusive. In most cases, the prediction model only slightly improves over a simple baseline where the average BLEU or TER score of the training documents is assigned to all test documents. Other studies have considered document-level information in order to improve, analyse or au121 tomatically evaluate MT output (not for QE purposes)"
W15-4916,2009.eamt-1.5,1,0.889526,"ht crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 1 Introduction Quality estimation (QE) of machine translation (MT) (Blatz et al., 2004; Specia et al., 2009) is an area that focuses on predicting the quality of new, unseen machine translation data without relying on human references. This is done by training models using features extracted from source and target texts and, when available, from the MT system, along with a quality label for each instance. Most current work on QE is done at the sentence level. A popular application of sentence-level QE is to support post-editing of MT (He et al., 2010). As quality labels, Likert scores have been used for post-editing effort, as well as post-editing time and edit distance between the MT output and the"
W15-4916,D14-1025,0,0.053746,"Missing"
W15-4916,C14-2028,0,\N,Missing
W15-4916,W13-2201,1,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2391,W15-3035,0,0.0283832,"Missing"
W16-2391,N13-1090,0,0.0451646,"and Cohn, 2013). However, this resulted in a worse model when compared to the cross validation scheme. We speculate that the resulting models overfit the training data, due to its small size. The best combination, which we use in our submission, employs two Rational Quadratic (RatQuad) kernels (Rasmussen and Williams, 2006).9 After fixing this combination, the hyperparameters are optimised by maximising the model likelihood on the full training data. Embedding features (called hereafter EMBsource and EMB-target). The word embeddings used in our experiments are learned with the word2vec tool5 (Mikolov et al., 2013b). The tool produces word embeddings using the Distributed Skip-Gram or Continuous Bag-of-Words (CBOW) models. The models are trained using large amounts of monolingual data with a neural network architecture that aims at predicting the neighbours of a given word. Unlike standard neural network-based language models for predicting the next word given the context of preceding words, a CBOW model predicts the word in the middle given the representation of the surrounding words, while the Skip-Gram model learns word embedding representations that can be used to predict a word’s context in the sa"
W16-2391,P02-1040,0,0.096538,"d to train the Quality Estimation models. The use of word embeddings (combined with baseline features) and a Gaussian Process model with two kernels led to the winning submission in the shared task. 1 Introduction The task of Quality Estimation (QE) of Machine Translation (MT) consists in predicting the quality of unseen data using Machine Learning (ML) models trained on labelled data points. Such a scenario does not require reference translations and only uses information from source and target documents. Therefore, QE is different from traditional automatic evaluation metrics (such as BLEU (Papineni et al., 2002)). Sentence-level and word-level QE have been widely explored along the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). On the other hand, documentlevel QE has only recently started to be addressed, with the first shared task organised last year (Bojar et al., 2015). Document-level QE is the task of predicting the quality of an entire document and is useful for gisting applications (mainly in cases where the user does not speak the source language) and fully automated uses of MT where post-editing is not an option. Predicting the quality of docu"
W16-2391,P09-2004,0,0.0340078,"higher for more shared entities. We calculate the coherence score of the source documents and of the target documents and incorporate these as features. Systems Description Our submissions for the shared task explore different approaches in terms of features and modelling. We describe them in detail in what follows. 2.1 Discourse-aware system Pronouns, Connectives, EDUs and RST features (called hereafter PCER). Following (Scarton et al., 2015a), we use information from the Charniak parser (Charniak, 2000), the Discourse Parser from Joty et al. (2013), and the Discourse Connectives Tagger from Pitler and Nenkova (2009) as features for our discourse-aware model (these features could only be extracted for English, and thus for the source documents): • Number of pronouns; • Number of connectives (total number and number of connectives per class); Model We combine the described features with the official baseline ones provided by the shared task organisers and use them in an SVR with RBF kernel and hyperparameters optimised via grid search (the same as the official shared task baseline system). We use the SVR implementation available in the scikit-learn toolkit (Pedregosa et al., 2011).4 • Number of EDU breaks;"
W16-2391,W15-3001,1,0.910288,"unt, including document-wide issues. Moreover, defining quality labels for documents is a complex task on itself, as pointed by Scarton et al. (2015b). Little previous research has addressed this problem. Soricut and Echihabi (2010) explore pseudo-references and document-aware features for document-level ranking, using BLEU as quality label. Scarton and Specia (2014) apply pseudoreferences, document-aware and discourse-aware features for document-level quality prediction, using BLEU and TER as quality scores. Last year, a paragraph-level QE shared task was organised for the first time at WMT (Bojar et al., 2015), using METEOR as quality label. Scarton (2015) explore discourse information for paragraph-level prediction. They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system. Bic¸ici et al. (2015) apply referential translation machines for paragraphlevel QE and obtain the best overall results in the shared task. Finally, Scarton (2015), Scarton and Specia (2015) and Scarton et al. (2015b) analyse the task of document-level QE from the perspective of defining reliable labels. They als"
W16-2391,D13-1100,0,0.0261669,"Missing"
W16-2391,W12-3102,1,0.892477,"Missing"
W16-2391,A00-2018,0,0.0444391,"Missing"
W16-2391,2014.eamt-1.21,1,0.935026,"lation Quality Estimation Carolina Scarton, Daniel Beck, Kashif Shah, Karin Sim Smith and Lucia Specia Department of Computer Science University of Sheffield, UK {c.scarton,debeck1,kashif.shah,kmsimsmith1,l.specia} @sheffield.ac.uk Abstract taken into account, including document-wide issues. Moreover, defining quality labels for documents is a complex task on itself, as pointed by Scarton et al. (2015b). Little previous research has addressed this problem. Soricut and Echihabi (2010) explore pseudo-references and document-aware features for document-level ranking, using BLEU as quality label. Scarton and Specia (2014) apply pseudoreferences, document-aware and discourse-aware features for document-level quality prediction, using BLEU and TER as quality scores. Last year, a paragraph-level QE shared task was organised for the first time at WMT (Bojar et al., 2015), using METEOR as quality label. Scarton (2015) explore discourse information for paragraph-level prediction. They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system. Bic¸ici et al. (2015) apply referential translation machines fo"
W16-2391,P13-1010,0,0.0231522,"ion of all sentences; In addition to the official results of our submitted systems, we experiment with other feature combinations, such as scores from graphbased entity grid coherence models extracted from source documents and word embeddings generated for target documents. In Section 2 we describe the models used in our experiments and in Section 3 we present our results. 2 • Average LSA cosine distance of all sentences. Entity graph-based features (called hereafter GRAPH-source and GRAPH-target). We use an Entity Graph Model (Sim Smith et al., 2016), which is based on the bipartite graph of Guinaudeau and Strube (2013) and tracks the occurrence of entities throughout the document, including between non-adjacent sentences. Entities are taken as all nouns occurring in the document, as recommended by (Elsner, 2011). For our experiments, a POS tagger3 is used to identify nouns. A local coherence score is calculated directly, without any training, and represents the distribution of entities in the document. This is based on the theory that coherent texts contain salient entities. Both the sentences and entities are represented as nodes, with edges connecting the entities to the sentences they occur in. The final"
W16-2391,W15-3040,1,0.852183,"uality scores. Last year, a paragraph-level QE shared task was organised for the first time at WMT (Bojar et al., 2015), using METEOR as quality label. Scarton (2015) explore discourse information for paragraph-level prediction. They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system. Bic¸ici et al. (2015) apply referential translation machines for paragraphlevel QE and obtain the best overall results in the shared task. Finally, Scarton (2015), Scarton and Specia (2015) and Scarton et al. (2015b) analyse the task of document-level QE from the perspective of defining reliable labels. They also investigate the correlation of discourse phenomena and document-lvel translation quality. In this paper, we focus on feature engineering and the use of different ML techniques for document-level QE in the context of the WMT16 QE shared task (Task 3). We submitted two systems: In this paper we present the results of the University of Sheffield (SHEF) submissions for the WMT16 shared task on document-level Quality Estimation (Task 3). Our submission explore discourse and document-aware informatio"
W16-2391,P13-1048,0,0.0315195,"umber of shared entities into account, rating the projections higher for more shared entities. We calculate the coherence score of the source documents and of the target documents and incorporate these as features. Systems Description Our submissions for the shared task explore different approaches in terms of features and modelling. We describe them in detail in what follows. 2.1 Discourse-aware system Pronouns, Connectives, EDUs and RST features (called hereafter PCER). Following (Scarton et al., 2015a), we use information from the Charniak parser (Charniak, 2000), the Discourse Parser from Joty et al. (2013), and the Discourse Connectives Tagger from Pitler and Nenkova (2009) as features for our discourse-aware model (these features could only be extracted for English, and thus for the source documents): • Number of pronouns; • Number of connectives (total number and number of connectives per class); Model We combine the described features with the official baseline ones provided by the shared task organisers and use them in an SVR with RBF kernel and hyperparameters optimised via grid search (the same as the official shared task baseline system). We use the SVR implementation available in the sc"
W16-2391,W15-4916,1,0.900042,"Missing"
W16-2391,N15-2016,1,0.853155,"ing quality labels for documents is a complex task on itself, as pointed by Scarton et al. (2015b). Little previous research has addressed this problem. Soricut and Echihabi (2010) explore pseudo-references and document-aware features for document-level ranking, using BLEU as quality label. Scarton and Specia (2014) apply pseudoreferences, document-aware and discourse-aware features for document-level quality prediction, using BLEU and TER as quality scores. Last year, a paragraph-level QE shared task was organised for the first time at WMT (Bojar et al., 2015), using METEOR as quality label. Scarton (2015) explore discourse information for paragraph-level prediction. They also perform exhaustive search and find out that using only three features from the official baseline set leads to results comparable to those of the full baseline system. Bic¸ici et al. (2015) apply referential translation machines for paragraphlevel QE and obtain the best overall results in the shared task. Finally, Scarton (2015), Scarton and Specia (2015) and Scarton et al. (2015b) analyse the task of document-level QE from the perspective of defining reliable labels. They also investigate the correlation of discourse phen"
W16-2391,L16-1649,1,0.748434,"nce of adjacent sentences; • Average LSA Spearman rho correlation of all sentences; In addition to the official results of our submitted systems, we experiment with other feature combinations, such as scores from graphbased entity grid coherence models extracted from source documents and word embeddings generated for target documents. In Section 2 we describe the models used in our experiments and in Section 3 we present our results. 2 • Average LSA cosine distance of all sentences. Entity graph-based features (called hereafter GRAPH-source and GRAPH-target). We use an Entity Graph Model (Sim Smith et al., 2016), which is based on the bipartite graph of Guinaudeau and Strube (2013) and tracks the occurrence of entities throughout the document, including between non-adjacent sentences. Entities are taken as all nouns occurring in the document, as recommended by (Elsner, 2011). For our experiments, a POS tagger3 is used to identify nouns. A local coherence score is calculated directly, without any training, and represents the distribution of entities in the document. This is based on the theory that coherent texts contain salient entities. Both the sentences and entities are represented as nodes, with"
W16-2391,2006.amta-papers.25,0,0.032266,"language pair, extracted from the WMT08-13 translation shared task datasets. The machine translation for each source document was randomly picked from the set of all systems that participated in the translation task. The documents were evaluated by following the two-stage post-editing method described in (Scarton et al., 2015a). In the first stage, sentences are post-edited out of context, whilst in the second stage the post-edited sentences are placed in context and any remaining mistakes are corrected. The quality scores are, then, a variation of Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006) that combines results from both post-editing stages. 3.1 Results Table 1 shows the results for our experiments with discourse-aware features and SVR for the scoring sub-task. We report results of our 10-fold crossvalidation method over the training and the results on the official test set. Results in the first column (10-fold) show that both discourse feature combination lead to improvements over the baseline. However, when testing on the test set, the models do not outperform the baseline. More investigation with additional data would be necessary to draw any conclusions on the reasons behin"
W16-2391,P10-1063,0,0.024847,"Missing"
W16-2391,P15-4020,1,0.825892,"t set), the feature sets show a similar behaviour: the model using EMB-target does not perform better than the baseline. On the other hand, EMB-source and baseline + EMB-source outperform the baseline, with the later scoring second in the official results of the shared task. It is worth mentioning that EMB-source alone is able to outperform the baseline in both sub-tasks. This is an interesting finding since word embeddings are relatively easy to acquire and only require large raw corpora as external resources. Baseline We use the 17 Q U E ST++ baseline features to train our baseline systems (Specia et al., 2015). We build a baseline system with SVR and another with GP, in order to compare our systems with comparable models.10 Models using discourse features and SVR The features sets we experimented with are: • baseline + PCER + LSA + GRAPH-target + GRAPH-source; • baseline + PCER + LSA + GRAPH-target.11 Our models using discourse information were trained with SVR as described in Section 2.1. Models using word embeddings and GP The features sets we experimented with are: • baseline + EMB-source + EMB-target; • baseline + EMB-source;12 • EMB-source; Our models using word embeddings were trained using G"
W17-4760,W16-2391,1,0.679845,"Table 1 using FastText1 (Bojanowski et al., 2016) with 300 dimensions and learning rate set to 0.025. The default training settings are otherwise used. The in-domain data is the same as that used to train the SMT system that produced the translations in the QE datasets, as made available by the task organizers. For the word and phrase-level tasks, we used our word embeddings to obtain a word vector representation of 300 dimensions for each word of both the training and development sets. For the sentence-level task, the word embeddings are averaged for each sentence, as previously applied in (Scarton et al., 2016). Table 1: Statistics of the in-domain data used to train our embeddings. using the source-target word alignment by minimizing the negative log-likelihood using a `2 regularized objective as: L(W ) = − X s,t log(P r(t|s; W )) + λkW k2 (2) 3.3 where λ is the constant that controls the capacity of W with gradient descent-based optimization. We explore this approach for both word and phrase-level QE. For training, we rely on both the word-alignments and the gold QE labels (i.e. the OK/BAD labels). The former gives us the sourcetarget pairs, and the latter whether this pair is valid or not. Our as"
W17-4760,W15-3041,1,0.852685,"relied upon or should be fixed by post-editors. More recently, QE at phrase-level has emerged as a way of using quality predictions at decoding time in phrase-based Statistical MT (SMT) systems to guide the decoder such as to keep phrases which are predicted as good, and conversely to discard those which are predicted as bad (Logacheva, 2017). QE models are built based on a list of features along with a Machine Learning algorithm for either regression or classification. These features are usually extracted from the source and target texts or from the MT system that generated the translations. Shah et al. (2015) introduced a new set of features extracted using an unsupervised approach with the use of neural network: continuous-space n o exp φ(t)&gt; W φ(s) Pr(t|s; W ) = X t0 ∈T n o exp φ(t0 )&gt; W φ(s) (1) where φ denotes the word embeddings of any given word in a vocabulary V. The source words s and target words t are respectively taken from subspaces S ⊆ V and T ⊆ V. In essence, the problem can be reduced to first obtaining the corresponding word embeddings of the vocabularies of both source and target sentences using a substantially large monolingual corpus for each of the two languages, followed by us"
W17-4760,W09-0441,0,0.0419666,"23,000 sentences for training, 1,000 for development and 2,000 for test), and German→English segments on the Pharmaceutical domain (with 25,000 sentences for training, 1,000 for development and 2,000 for test). The same data is used for all three tasks: word, phrase and sentence-level prediction. For the word-level task, each token of the MT is annotated with OK or BAD labels. For the phrase-level task, phrases are segmented as given by an SMT decoder and also annotated with OK or BAD labels. Finally, for the sentence-level task, the quality label is a Human-Targeted Error Rate (HTER) score (Snover et al., 2009). 3.2 Tool 3.4 Evaluation We used the official task metrics to evaluate our results. For the word and phrase-level tasks, the metrics are F1 -BAD and F1 -OK which correspond to the F1 scores on both BAD and OK labels, and F1 multi which is the product of the two formers. For the sentence-level task, the metrics for scoring are Pearson’s correlation (primary metric), Mean Average Error (MAE) and Root Mean Squared Error (RMSE), and for ranking, Spearman’s rank correlation (primary metric) and DeltaAvg. Word Embeddings Word embeddings were used in our submissions for the three tasks. We trained i"
W17-4760,P15-4020,1,0.850897,"QE task. In grey are the results of the official baseline of the task. word embeddings trained on general purpose data, our embeddings are trained over in-domain data, as previously described. Word embeddings were averaged at sentence level in order to have a single vector representing each sentence. We then concatenated source and target in-domain embeddings with the 17 sentence-level baseline features provided by the organisers. An SVM regressor was used to train our QE model with hyper-parameters optimized via grid-search. For that we used the learning module available at QuEst++ toolkit (Specia et al., 2015). Although the sentence-level experiment is different from the approach applied for word and phrase-level tasks, our aim was to test the usability of the in-domain word embeddings. Our results are compared with the official baseline. Discussion The results of our sentence-level predictions are given in Table 4. Although the approach is rather simplistic, it achieves considerably good results by outperforming the baseline system and several other systems that participated in the shared task. For German→English, our system performed seventh out of 13 in the scoring task. For English→German, it p"
W17-4760,L16-1356,1,0.378591,"th both pessimistic approaches for en→de. One can also observe comparable performance for en→de when the surrounding context is used: the difference in terms of F1 -* scores between the full and window context is marginal. For de→en this is different: the phrase labelling based on word predictions using the window context outperforms the phrase laPhrase-level QE labelling (Task 3) While we could have chosen to predict phraselevel QE labels similarly to our word-level predictions, we opted for generating phrase-level labels from word-level labels following the labelling approaches described in Blain et al. (2016): • Optimistic: if half or more of words have a label OK, the phrase has the label OK (majority labelling). 548 English→German (2016) BMAPS-full-opti BMAPS-window-opti BMAPS-unigram-opti † BMAPS-unigram-nolabel-opti † BMAPS-unigram-opti BMAPS-full-pess BMAPS-window-pess BMAPS-unigram-pess • BMAPS-unigram-nolabel-pess • BMAPS-unigram-pess BMAPS-full-superpess BMAPS-window-superpess BMAPS-unigram-super-pess • BMAPS-unigram-nolabel-suppess • BMAPS-unigram-super-pess BASELINE English→German (2017) BMAPS-full-opti BMAPS-window-opti BMAPS-unigram-opti † BMAPS-unigram-nolabel-opti † BMAPS-unigram-opt"
W17-4760,C14-1017,0,0.3385,"ls to predict if information encoded in the source sentence is preserved in the target sentence after translation. This paper describes the SHEF submissions for the three sub-tasks of the Quality Estimation shared task of WMT17, namely: (i) a word-level prediction system using bilexical embeddings, (ii) a phrase-level labelling approach based on the word-level predictions, (iii) a sentencelevel prediction system using word embeddings and handcrafted baseline features. Results are promising for the sentence-level approach, but still very preliminary for the other two levels. 1 2 Bilinear Model Madhyastha et al. (2014) propose to use wordlevel embeddings to predict the strength of different types of lexical relationships between a pair of words, such as head-modifier relations between noun-adjective pairs. They designed a supervised framework for learning bilexical operators over distributional representations, based on learning bilinear forms W . We adapted their method to predict the strength of relationship between source and target words. This problem is formulated as a log-bilinear model, parametrized with W as follows: Introduction Quality Estimation (QE) allows the evaluation of Machine Translation ("
W18-6320,2009.mtsummit-posters.5,0,0.123004,"Missing"
W18-6320,W15-4918,1,0.909017,"Missing"
W18-6320,W16-2301,1,0.829153,"Missing"
W18-6320,2001.mtsummit-papers.20,0,0.488483,"Missing"
W18-6320,1999.mtsummit-1.42,0,0.32113,"Missing"
W18-6320,W11-2123,0,0.0202271,"Missing"
W18-6320,E06-1032,0,0.205885,"Missing"
W18-6320,N07-2020,0,0.0997368,"Missing"
W18-6320,L16-1048,0,0.0241495,"Missing"
W18-6320,2014.eamt-1.40,0,0.0438492,"Missing"
W18-6320,2000.tc-1.5,0,0.320841,"Missing"
W18-6320,W15-2402,0,0.0382638,"Missing"
W18-6320,stymne-etal-2012-eye,0,0.038532,"Missing"
W18-6320,P07-2045,1,0.0112762,"Missing"
W18-6320,1993.tmi-1.22,0,0.82749,"Missing"
W18-6320,W11-2401,0,0.033505,"Missing"
W18-6320,2012.freeopmt-1.3,0,0.127684,"Missing"
W18-6320,weiss-ahrenberg-2012-error,0,0.0760185,"Missing"
W18-6320,N16-1125,0,0.0253847,"Missing"
W18-6442,W16-3210,1,0.903985,"Missing"
W18-6442,W16-5307,0,0.0423393,"Missing"
W18-6442,W17-4746,0,0.167179,"Missing"
W18-6442,W16-2358,0,0.0764839,"Missing"
W18-6442,L18-1602,1,0.784592,"to investigate a multimodal, image-based, cross-lingual WSD that predicts the translation candidate which correctly disambiguates ambiguous words in the source sentence. Our baseline NMT system is based on the attentive encoder-decoder (Bahdanau et al., 2015) approach with a Conditional GRU (CGRU) (Cho et al., 2014) decoder and is built using NMTPY toolkit (Caglayan et al., 2017b). Our cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) applied to the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). For task 1b, we explore three approaches. The first approach concatenates the 10-best translation hypotheses from DE-CS, FR-CS and EN-CS MT systems and then re-ranks them using the imageaware multimodal cross-lingual WSD mentioned earlier (the same way as in Task 1) (Section 3.1.2). The second approach explores the consensus between the different 10-best lists. The best hypothesis is selected according to the number of times it appeared in the different 10-best lists. We followed the order of the n-best lists, meaning that the highest ranked hypothesis with the majority votes was selected. T"
W18-6442,P02-1040,0,0.101237,"e try two different types of classifiers Random Forest and Recurrent Neural Network. Re-ranking using MLT For the re-ranking approach, we first train three baseline EN-CS, DECS and FR-CS NMT models. Given a source sentence in the test set, we generate 10-best translation hypotheses using each of the three models. The three 10-best lists are concatenated to form a list of 30 translation hypotheses. We then use the trained EN-CS MLT model for cross-lingual WSD and perform re-ranking as mentioned in 3.1.2 and 3.1.3. 4 For both tasks, the initial evaluation was performed in terms of METEOR, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), with METEOR as the primary metric. Direct human assessments of translation adequacy will be used for the final evaluation by the task organizers. For task 1, our submitted systems consisted of: a) SHEF LT: re-ranking using LT model; b) SHEF MLT: re-ranking using MLT model; c) SHEF MFS: re-ranking using MFS model; and d) SHEF Baseline: our baseline text-only ensemble NMT model Table 3 shows the official evaluation results of our systems submitted to Task 1 and the baseline system provided by the organizers. For all language pairs, our systems outperform the offic"
W18-6442,W14-3348,0,0.548606,"clala1, p.madhyastha, c.scarton, l.specia}@sheffield.ac.uk Abstract that standard text-only sequence to sequence neural machine translation models (NMT) with attention are able to obtain very high performance. Building on this, for further inspection, we built our own standard NMT systems for EN-DE, ENFR and EN-CS language directions and noticed that the translation hypotheses besides the 1-best output are also of high quality. We made our systems produce 20 translation hypotheses for English descriptions in the validation set and selected the hypothesis with the highest sentencelevel METEOR (Denkowski and Lavie, 2014) score, called the Oracle, and compared this to the 1-best. In this experiment, we observed that the Oracle performs way better (11 to 13.5 METEOR points) than the 1-best output (See Table 1). This preliminary experiment motivated us to investigate re-ranking approaches. This paper describes the University of Sheffield’s submissions to the WMT18 Multimodal Machine Translation shared task. We participated in both tasks 1 and 1b. For task 1, we build on a standard sequence to sequence attention-based neural machine translation system (NMT) and investigate the utility of multimodal re-ranking app"
W18-6442,D17-1120,0,0.134251,"d Hoste, 2013) and a system that performs re-ranking using the Most Frequent Sense (MFS) baseline (Section 3.1.2). Our main goal is to investigate a multimodal, image-based, cross-lingual WSD that predicts the translation candidate which correctly disambiguates ambiguous words in the source sentence. Our baseline NMT system is based on the attentive encoder-decoder (Bahdanau et al., 2015) approach with a Conditional GRU (CGRU) (Cho et al., 2014) decoder and is built using NMTPY toolkit (Caglayan et al., 2017b). Our cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) applied to the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). For task 1b, we explore three approaches. The first approach concatenates the 10-best translation hypotheses from DE-CS, FR-CS and EN-CS MT systems and then re-ranks them using the imageaware multimodal cross-lingual WSD mentioned earlier (the same way as in Task 1) (Section 3.1.2). The second approach explores the consensus between the different 10-best lists. The best hypothesis is selected according to the number of times it appeared in the different 10-best"
W18-6442,W17-4718,1,0.738829,"task. Task 1 consists in translating source sentences in English that describe an image into German (DE) or French (FR) or Czech (CS), given the image. Task 1b consists in translating source sentences in English that describe an image into Czech, given the image and the French and German translations of the source sentence. This task poses the challenging problem of building models that use both language and image modalities. The dataset for the shared task (Specia et al., 2016) has sentences with simple language constructions and it has been observed by earlier systems (Specia et al., 2016; Elliott et al., 2017) Lang-Pair 1-best Best of 20best (Oracle) Scope/difference (Oracle - 1-best) EN-DE EN-FR EN-CS 48.36 64.91 33.87 61.85 76.87 44.71 +13.49 +11.96 +10.84 Table 1: Motivation for re-ranking. In this preliminary experiment, we observe that re-ranking of the 20-best translation hypotheses generated by a standard NMT model has the potential of improving translation by upto 10.84 to 13.49 METEOR points for the three language pairs. For a re-ranking strategy, we were inspired by how humans use images to translate image descriptions. We believe humans look at the image usually to disambiguate ambiguous"
W18-6442,2006.amta-papers.25,0,0.0636218,"ssifiers Random Forest and Recurrent Neural Network. Re-ranking using MLT For the re-ranking approach, we first train three baseline EN-CS, DECS and FR-CS NMT models. Given a source sentence in the test set, we generate 10-best translation hypotheses using each of the three models. The three 10-best lists are concatenated to form a list of 30 translation hypotheses. We then use the trained EN-CS MLT model for cross-lingual WSD and perform re-ranking as mentioned in 3.1.2 and 3.1.3. 4 For both tasks, the initial evaluation was performed in terms of METEOR, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), with METEOR as the primary metric. Direct human assessments of translation adequacy will be used for the final evaluation by the task organizers. For task 1, our submitted systems consisted of: a) SHEF LT: re-ranking using LT model; b) SHEF MLT: re-ranking using MLT model; c) SHEF MFS: re-ranking using MFS model; and d) SHEF Baseline: our baseline text-only ensemble NMT model Table 3 shows the official evaluation results of our systems submitted to Task 1 and the baseline system provided by the organizers. For all language pairs, our systems outperform the official baseline for all metrics."
W18-6442,W16-2346,1,0.902517,"Missing"
W18-6442,C16-1130,0,0.0172244,"stem that performs re-ranking using the Most Frequent Sense (MFS) baseline (Section 3.1.2). Our main goal is to investigate a multimodal, image-based, cross-lingual WSD that predicts the translation candidate which correctly disambiguates ambiguous words in the source sentence. Our baseline NMT system is based on the attentive encoder-decoder (Bahdanau et al., 2015) approach with a Conditional GRU (CGRU) (Cho et al., 2014) decoder and is built using NMTPY toolkit (Caglayan et al., 2017b). Our cross-lingual WSD models are based on neural sequence learning models for WSD (Raganato et al., 2017; Yuan et al., 2016; K˚ageb¨ack and Salomonsson, 2016) applied to the Multimodal Lexical Translation Dataset (Lala and Specia, 2018). For task 1b, we explore three approaches. The first approach concatenates the 10-best translation hypotheses from DE-CS, FR-CS and EN-CS MT systems and then re-ranks them using the imageaware multimodal cross-lingual WSD mentioned earlier (the same way as in Task 1) (Section 3.1.2). The second approach explores the consensus between the different 10-best lists. The best hypothesis is selected according to the number of times it appeared in the different 10-best lists. We followed"
W18-6463,E17-1100,0,0.04307,"Missing"
W18-6463,C18-1266,1,0.865593,"Missing"
W18-6463,W17-4763,0,0.149675,"uent but less adequate (Toral and S´anchez-Cartagena, 2017). For tasks 2 and 3, this year’s edition introduces a new task variant of predicting missing words in the translations. Thus two additional prediction types are required: (i) binary labels for gaps in the translation to indicate whether one or more tokens are missing from a certain position, and (ii) 2 Systems Description Our light-weight neural QE approach is based on simple encoders and requires no pre-training (bRNN). We compare its performance to the performance of our re-implementation of the state-ofthe-art neural QE approach of Kim et al. (2017a,b) (POSTECH), which uses a complex architecture and requires resource-intensive pre-training. 2.1 Architecture Following current best practices in neural sequence-to-sequence modelling (Sutskever et al., 2014; Bahdanau et al., 2015), our bRNN approach employs encoders using recurrent neural networks (RNNs). Encoders encode input into an internal representation used to make classification decisions. bRNN representations at a given level rely on representations from more fine-grained levels (i.e. sentences for document, and words for phrase and sentence). bRNN uses two bi-directional RNNs to l"
W18-6463,L16-1582,1,0.832991,"g gaps, a gap tag is placed after each token and in the beginning of the sentence. A gap tag will be BAD if one or more words were expected to appear in the gap, and OK otherwise. Task 2 has 18 variants, for each of them we again submitted two systems: SHEF-PT and SHEF-bRNN. The primary evaluation metric of task 2 is F1MULT: multiplication of F1-scores for the OK and BAD classes. F1-scores of OK and BAD classes are used as secondary metrics. The baseline system for the target word predictions is a Conditional Random Fields (CRF) model trained with word-level baseline features from the Marmot (Logacheva et al., 2016) toolkit. There are no baseline systems for the prediction of gaps or source word issues. Table 2 shows the official results. For prediction of target words, SHEF-PT is the best for ENDE – SMT, EN-LV – SMT and EN-LV – NMT. SHEF-bRNN is the best for EN-DE – NMT. This confirms our previous conclusion that bRNN better 3.3 Task 3: Phrase-level QE This task considers a subset of the EnglishGerman SMT data from task 1 (Section 3.1). Here, the MT output has been manually anno797 SRC PE SMT gold PT bRNN SRC PE NMT gold PT bRNN to make your content accessible to screen readers , avoid using these modes"
W18-6463,2006.amta-papers.25,0,0.0333458,"combinations). Our systems show competitive results and outperform the baseline in nearly all cases. 1 Introduction Quality Estimation (QE) predicts the quality of Machine Translation (MT) when automatic evaluation or human assessment is not possible (typically at system run-time). QE is mainly addressed as a supervised Machine Learning problem with QE models trained using labelled data. These labels differ for different tasks, for example, binary labels for fine-grained predictions (e.g. OK/BAD for words or phrases) and continuous measurements of quality for coarse-grained levels (e.g. HTER (Snover et al., 2006) for sentences). For this year’s shared task, post-edited (PE) and manually annotated data were provided. They cover four levels of predictions: sentence-level (task 1), word-level (task 2), phrase-level (task 3) and document-level (task 4), over five language pairs: English into German, Latvian, Czech and French, as well as German-English. For the first time, these data contain translations produced by neural MT (NMT) systems. Such translations are known to be more fluent but less adequate (Toral and S´anchez-Cartagena, 2017). For tasks 2 and 3, this year’s edition introduces a new task varia"
W18-6463,P15-4020,1,0.839196,"ction is HTER in all of them. For each variant in this task we submitted two systems: SHEF-PT and SHEF-bRNN. For the ranking evaluation, we rank sentences using the predicted HTER outputted by our systems. Following the shared task setup, Pearson’s r correlation coefficient is used as the primary evaluation metric for the scoring task (with Mean Absolute Error – MAE – as the secondary metric), whilst Spearman’s ρ rank correlation coefficient is used as metric for the ranking task. The task baseline systems are Support Vector Machine (SVM) models trained with 17 baseline features from QuEst++ (Specia et al., 2015). We show the official results in Table 1. Both our systems outperform the baseline for all the language pairs according to the main evaluation metric (r). SHEF-bRNN is better than SHEF-PT only for EN-DE – NMT and EN-LV – SMT. These may be cases where bRNN is able to better capture the fluency of high-quality MT by encoding it directly as sequences rather than assessing it word for word as POSTECH. On the official development set,9 EN-DE – NMT and EN-LV – SMT translations have the best overall quality (on average HTER=0.17 versus HTER=0.28 for the rest of the systems). Tasks Participation The"
