2007.mtsummit-papers.65,J93-2003,0,0.0196732,"006). Only Lee (2004) and Yang and Kirchhoff (2006) have used more than half a million sentence pairs for training, and still outperformed the word-based approach. Like Yang and Kirchhoff (2006), we use the Europarl corpus. For each studied language, we first train the model for morpheme segmentation, and then for each language pair, we train the translation system on a corpora containing more than 800 000 sentences. The discussion to follow focuses on both quantitative and qualitative aspects of the performance of the systems. 2. Methodology Since the introduction of the so-called IBM model (Brown et al., 1993), a standard statistical machine translation system divides into two parts: the translation model and the language model. Given a text S in the source language, we want to find the text T in the target language that is the most probable translation of S. Bayes’ Theorem states that the probability P (T |S) is maximized when the product of the prior probability P (T ) and the translation probability P (S|T ) is maximized. The former is defined by the language model, and the latter by the translation model. The texts S and T consist of tokens separated by whitespace characters. The tokens are the"
2007.mtsummit-papers.65,E06-1032,0,0.00515386,"ely. As in most of the recent studies, we have used the BLEU scores (Papineni et al., 2002) for quantitative evaluation. BLEU is based on the co-occurrence of n-grams. It counts how many n-grams (usually for n = 1, . . . , 4) the proposed translation has in common with the reference translations and calculates a score based on this. For a realistic evaluation, the calculation of the BLEU scores would need several reference translations made by different persons. Even when such are available, the BLEU score has been criticized, as in some cases human evaluation gives grossly different results (Callison-Burch et al., 2006; Culy and Riehemann, 2003). Even if we brush aside the criticism and the fact that we have used only one reference translation, BLEU scores have some problematic features for our study. It is clear that for morphologically rich languages, such as Finnish, it is harder to get good scores. Finnish has fewer words for the same text compared to Swedish or Danish, and thus one word includes more information on average. One mistake in one suffix of a word is enough to mark the word as an error. This does not usually prevent understanding the translation, but will drop the scores as much as more “se"
2007.mtsummit-papers.65,corston-oliver-gamon-2004-normalizing,0,0.0170312,". In addition, the unsupervised approach to morphological analysis has been found to work very well on a related task, automatic speech recognition. It seems that before us, only Sereewattana (2003) has used unsupervised segmentation to enhance SMT. However, she used only small training corpora, and studied only translations from German and French to English. Choice of the target language. When a morphologically rich language is involved, it has almost exclusively been the source language with English as the target. The two most common source languages seem to be German (Nießen and Ney, 2004; Corston-Oliver and Gamon, 2004) and Arabic (Lee, 2004; Zollmann et al., 2006). There are also studies for translating from Czech (Goldwater and McClosky, 2004), Finnish (Yang and Kirchhoff, 2006), and Spanish, Catalan and Serbian (Popovi´c and Ney, 2004). A recent exception to the direction of the translation is the English-Turkish translation system by Oflazer and ElKahlout (2007). With a morphological analyzer for Turkish and TreeTagger for English, they do the translation at the morpheme-level, just as we do. With additional tweaking, such as selective morpheme-grouping for Turkish and augmenting the training data with s"
2007.mtsummit-papers.65,2003.mtsummit-papers.10,0,0.0111975,"studies, we have used the BLEU scores (Papineni et al., 2002) for quantitative evaluation. BLEU is based on the co-occurrence of n-grams. It counts how many n-grams (usually for n = 1, . . . , 4) the proposed translation has in common with the reference translations and calculates a score based on this. For a realistic evaluation, the calculation of the BLEU scores would need several reference translations made by different persons. Even when such are available, the BLEU score has been criticized, as in some cases human evaluation gives grossly different results (Callison-Burch et al., 2006; Culy and Riehemann, 2003). Even if we brush aside the criticism and the fact that we have used only one reference translation, BLEU scores have some problematic features for our study. It is clear that for morphologically rich languages, such as Finnish, it is harder to get good scores. Finnish has fewer words for the same text compared to Swedish or Danish, and thus one word includes more information on average. One mistake in one suffix of a word is enough to mark the word as an error. This does not usually prevent understanding the translation, but will drop the scores as much as more “serious” mistakes. 4.2. Untra"
2007.mtsummit-papers.65,J01-2001,0,0.0408693,"plied framework for phrase-based machine translation and how morphs are used in the translation. 2.1. Morphological model for words Morfessor (Creutz and Lagus, 2007) is a method for finding morpheme-like units (morphs) of a language in an unsupervised manner. Morfessor can cope with languages where words can consist of multiple prefixes, stems, and suffixes concatenated together. This distinguishes Morfessor from other algorithms that pose harder restrictions on the possible structures of words, such that each word is assumed to consist of one stem optionally followed by a suffix; see, e.g., Goldsmith (2001). Using morph-based rather than word-based vocabularies has been shown to result in better performance in automatic speech recognition for highly inflecting and agglutinative languages (Hirsimäki et al., 2006; Kurimo et al., 2006). There exist a few different versions of Morfessor, which correspond to chronological development steps of the algorithm.1 In this work, we use the Morfessor Categories-MAP algorithm (Creutz and Lagus, 2005), which is formulated in a maximum a posteriori (MAP) framework. Morfessor Categories-MAP has a better segmentation accuracy with respect to a morphological gold"
2007.mtsummit-papers.65,D07-1091,0,0.134531,"Missing"
2007.mtsummit-papers.65,N03-1017,0,0.0354697,"language models to model the target language in our translation tasks. The two baseline models, 3-gram and 4-gram models, are trained with the SRI Language Modeling toolkit (Stolcke, 2002). The third is a varigram model trained with the VariKN Language Modeling toolkit based on the algorithms given by Siivola et al. (2007).2 2.3. Phrase-based statistical machine translation The first SMT models (Brown et al., 1993) estimated translation probabilities P (S|T ) for pairs of words in the source and target languages. When the framework of phrase-based statistical machine translation was proposed (Koehn et al., 2003), it was observed that the translation quality could be improved by translating sequences of words, phrases, at a time. The phrases are collected in an unsupervised manner from the training data. Moses is an open-source toolkit for phrase-based statistical machine translation (Koehn et al., 2007). Moses automatically trains and applies translation models for any language pair. The system needs a parallel corpus (the same text in two languages) for training the models. After training, Moses can be used for translating new sentences of the source language into the target language. We use the Mos"
2007.mtsummit-papers.65,2005.mtsummit-papers.11,0,0.296446,"dish) that are included in the Europarl corpus consisting of the Proceedings of the European Parliament. However, in our experiments we did not obtain higher BLEU scores for the morph model than for the standard word-based approach. Nonetheless, the proposed morph-based solution has clear benefits, as morphologically well motivated structures (phrases) are learned, and the proportion of words left untranslated is clearly reduced. 1. Introduction Statistical machine translation was applied to the direct translation between eleven European languages, all those present in the Europarl corpus, by Koehn (2005). An impressive number of 110 different translation systems were created, one for each language pair. Koehn discovered that the most difficult language to translate from or to is Finnish. Finnish is a non-Indo-European language and is well known for its extremely rich morphology. As verbs and nouns can, in theory, have hundreds and even thousands of word forms, data sparsity and out-of-vocabulary words present a huge problem even when large corpora are available. It appears that especially translating into a morphologically rich language poses an even bigger problem than translating from such"
2007.mtsummit-papers.65,N06-1062,0,0.0218322,"Missing"
2007.mtsummit-papers.65,N04-4015,0,0.220125,"morphological analysis has been found to work very well on a related task, automatic speech recognition. It seems that before us, only Sereewattana (2003) has used unsupervised segmentation to enhance SMT. However, she used only small training corpora, and studied only translations from German and French to English. Choice of the target language. When a morphologically rich language is involved, it has almost exclusively been the source language with English as the target. The two most common source languages seem to be German (Nießen and Ney, 2004; Corston-Oliver and Gamon, 2004) and Arabic (Lee, 2004; Zollmann et al., 2006). There are also studies for translating from Czech (Goldwater and McClosky, 2004), Finnish (Yang and Kirchhoff, 2006), and Spanish, Catalan and Serbian (Popovi´c and Ney, 2004). A recent exception to the direction of the translation is the English-Turkish translation system by Oflazer and ElKahlout (2007). With a morphological analyzer for Turkish and TreeTagger for English, they do the translation at the morpheme-level, just as we do. With additional tweaking, such as selective morpheme-grouping for Turkish and augmenting the training data with samples containing only"
2007.mtsummit-papers.65,J04-2003,0,0.0742268,"that Koehn (2005) did. In addition, the unsupervised approach to morphological analysis has been found to work very well on a related task, automatic speech recognition. It seems that before us, only Sereewattana (2003) has used unsupervised segmentation to enhance SMT. However, she used only small training corpora, and studied only translations from German and French to English. Choice of the target language. When a morphologically rich language is involved, it has almost exclusively been the source language with English as the target. The two most common source languages seem to be German (Nießen and Ney, 2004; Corston-Oliver and Gamon, 2004) and Arabic (Lee, 2004; Zollmann et al., 2006). There are also studies for translating from Czech (Goldwater and McClosky, 2004), Finnish (Yang and Kirchhoff, 2006), and Spanish, Catalan and Serbian (Popovi´c and Ney, 2004). A recent exception to the direction of the translation is the English-Turkish translation system by Oflazer and ElKahlout (2007). With a morphological analyzer for Turkish and TreeTagger for English, they do the translation at the morpheme-level, just as we do. With additional tweaking, such as selective morpheme-grouping for Turkish and au"
2007.mtsummit-papers.65,W07-0704,0,0.0894416,"eneralization ability is increased through more refined phrases. We aim to a system that would improve the phrasebased translation with morphology for practically any language pair, and regardless of the size of the training corpus. There are several ways that should improve our current approach. First, there might be some problems in the morph alignments, as the applied algorithms have been designed for word alignment. Thus word alignments could be used as a starting point for the alignment of morphs. Second, translations based on morphs could be rescored with a word-based language model, as Oflazer and El-Kahlout (2007) have done. Third, translations based on words and morphs could also be combined, e.g., with back-off models (Yang and Kirchhoff, 2006). Fourth, instead of using all the different morph forms, we would like to combine allomorphs of the same morphemes into equivalence classes. Factored translation models (Koehn and Hoang, 2007) could help to use them elegantly in the translation. Overall, we are confident that unsupervised morphology learning is useful in the development of the statistical machine translation framework and in improving translation quality across a variety of languages. 6. Ackno"
2007.mtsummit-papers.65,P02-1040,0,0.105124,"fies that the morph is not the last morph of the word. The latter information is necessary in order to reconstruct words from the morphs in the final output. 3. Experiments In this section, we compare our morphology-aware phrasebased statistical machine translation framework to the more traditional word-based framework and analyze the differences in the two approaches. All main experiments are run on the Moses systems on all six language pairs and with both word tokens and morph tokens. Quantitative evaluation is provided with BLEU scores (Bilingual Evaluation Understudy); see Papineni et al. (2002). The BLEU calculations are based on words regardless of the token type used in the translation, and (wholly or partially) untranslated words are included. We will first introduce our data sets, which comprise the three Nordic languages present in Europarl, and then we report on the experiments conducted. Language models with different n-gram orders are compared, since morphs are shorter than words and thus a higher order model may 2 VariKN toolkit is available at http://varikn.forge. pascal-network.org/. languages da-fi da-sv fi-sv token type word morph word morph word morph train 877 944 863"
2007.mtsummit-papers.65,popovic-ney-2004-towards,0,0.4694,"Missing"
2007.mtsummit-papers.65,E06-1006,0,0.176055,"us, only Sereewattana (2003) has used unsupervised segmentation to enhance SMT. However, she used only small training corpora, and studied only translations from German and French to English. Choice of the target language. When a morphologically rich language is involved, it has almost exclusively been the source language with English as the target. The two most common source languages seem to be German (Nießen and Ney, 2004; Corston-Oliver and Gamon, 2004) and Arabic (Lee, 2004; Zollmann et al., 2006). There are also studies for translating from Czech (Goldwater and McClosky, 2004), Finnish (Yang and Kirchhoff, 2006), and Spanish, Catalan and Serbian (Popovi´c and Ney, 2004). A recent exception to the direction of the translation is the English-Turkish translation system by Oflazer and ElKahlout (2007). With a morphological analyzer for Turkish and TreeTagger for English, they do the translation at the morpheme-level, just as we do. With additional tweaking, such as selective morpheme-grouping for Turkish and augmenting the training data with samples containing only the content words, they improve significantly the translation results. Size of the training corpora. Usage of morphology has often been seen"
2007.mtsummit-papers.65,N06-2051,0,0.0141714,"al analysis has been found to work very well on a related task, automatic speech recognition. It seems that before us, only Sereewattana (2003) has used unsupervised segmentation to enhance SMT. However, she used only small training corpora, and studied only translations from German and French to English. Choice of the target language. When a morphologically rich language is involved, it has almost exclusively been the source language with English as the target. The two most common source languages seem to be German (Nießen and Ney, 2004; Corston-Oliver and Gamon, 2004) and Arabic (Lee, 2004; Zollmann et al., 2006). There are also studies for translating from Czech (Goldwater and McClosky, 2004), Finnish (Yang and Kirchhoff, 2006), and Spanish, Catalan and Serbian (Popovi´c and Ney, 2004). A recent exception to the direction of the translation is the English-Turkish translation system by Oflazer and ElKahlout (2007). With a morphological analyzer for Turkish and TreeTagger for English, they do the translation at the morpheme-level, just as we do. With additional tweaking, such as selective morpheme-grouping for Turkish and augmenting the training data with samples containing only the content words, they"
2007.mtsummit-papers.65,P07-2045,0,\N,Missing
2007.mtsummit-papers.65,H05-1085,0,\N,Missing
2020.acl-demos.20,W19-6721,0,0.0277801,"Missing"
2020.acl-demos.20,W18-6317,0,0.042732,"Missing"
2020.acl-demos.20,W18-2709,0,0.0115905,"the quality of alignment between source and target language needs to checked. The aligned translations drive the mapping from input to the output language as a strong supervision during the training steps, and the amount of noise will have a decisive impact on the adequacy of the translations. The effect is especially severe for low resource settings, in which little data is available, and each mistake might directly influence the end result. The interest in automatic bitext (i.e. bilingual parallel corpora) filtering is constantly growing pushed by the advances in neural machine translation. Khayrallah and Koehn (2018) show that noisy training data is often more harmful for neural translation models than statistical translation models. As a consequence, international evaluation campaigns like the ones organised by WMT now feature shared tasks on data cleaning and ranking (Koehn et al., 2018, 2019). Various approaches have been proposed based on such challenges and directly benefit the development of MT engines in low-resource settings. This paper presents a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly avail"
2020.acl-demos.20,P17-4012,0,0.0146132,"score values used for training classifiers in the Finnish-English noise filtering. CharacterScoreFilters have been excluded from histograms as their values are almost always one. training sets. Next, we reorder the data with our toolkit and again create new data sets by removing data with the same proportions as previously. We then apply data provided for the WMT news translation task6 for validation and testing. In particular, we use newstest2018 as the development set and newstest2019 as our test set for both language directions. The translation models are trained with the OpenNMT toolkit (Klein et al., 2017) using RNN encoders and decoders with LSTM gates. All training sets are tokenized with the tokenizer from the mosesdecoder toolkit (Koehn et al., 2007) and segmented with BPE (Sennrich et al., 2016) using subword-nmt7 before feeding them to OpenNMT. 3.1 Ranking Following V´azquez et al. (2019), we first produce an initial filtering of the ParaCrawl corpus. For this, we use the following heuristic filters from the OpusFilter toolbox: bly because similar filters are already applied in bicleaner when preparing the original data set. Nevertheless, these steps are useful for creating data to train"
2020.acl-demos.20,W19-5404,0,0.0243301,"Missing"
2020.acl-demos.20,P07-2045,0,0.00833198,"alues are almost always one. training sets. Next, we reorder the data with our toolkit and again create new data sets by removing data with the same proportions as previously. We then apply data provided for the WMT news translation task6 for validation and testing. In particular, we use newstest2018 as the development set and newstest2019 as our test set for both language directions. The translation models are trained with the OpenNMT toolkit (Klein et al., 2017) using RNN encoders and decoders with LSTM gates. All training sets are tokenized with the tokenizer from the mosesdecoder toolkit (Koehn et al., 2007) and segmented with BPE (Sennrich et al., 2016) using subword-nmt7 before feeding them to OpenNMT. 3.1 Ranking Following V´azquez et al. (2019), we first produce an initial filtering of the ParaCrawl corpus. For this, we use the following heuristic filters from the OpusFilter toolbox: bly because similar filters are already applied in bicleaner when preparing the original data set. Nevertheless, these steps are useful for creating data to train models used in the later filtering methods. First, we train word alignment priors for the ¨ model 3 of the eflomal tool8 (Ostling and Tiedemann, 2016)"
2020.acl-demos.20,W18-6453,0,0.0163569,"lations. The effect is especially severe for low resource settings, in which little data is available, and each mistake might directly influence the end result. The interest in automatic bitext (i.e. bilingual parallel corpora) filtering is constantly growing pushed by the advances in neural machine translation. Khayrallah and Koehn (2018) show that noisy training data is often more harmful for neural translation models than statistical translation models. As a consequence, international evaluation campaigns like the ones organised by WMT now feature shared tasks on data cleaning and ranking (Koehn et al., 2018, 2019). Various approaches have been proposed based on such challenges and directly benefit the development of MT engines in low-resource settings. This paper presents a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly available parallel corpora. In contrast to tools such as bicleaner (S´anchez-Cartagena et al., 2018) and Zipporah (Xu and Koehn, 2017), that implement a single method for parallel corpus filtering, OpusFilter is designed as a toolbox that is useful for testing and using many differ"
2020.acl-demos.20,W18-6488,0,0.0249667,"Missing"
2020.acl-demos.20,P16-1162,0,0.00973415,"Next, we reorder the data with our toolkit and again create new data sets by removing data with the same proportions as previously. We then apply data provided for the WMT news translation task6 for validation and testing. In particular, we use newstest2018 as the development set and newstest2019 as our test set for both language directions. The translation models are trained with the OpenNMT toolkit (Klein et al., 2017) using RNN encoders and decoders with LSTM gates. All training sets are tokenized with the tokenizer from the mosesdecoder toolkit (Koehn et al., 2007) and segmented with BPE (Sennrich et al., 2016) using subword-nmt7 before feeding them to OpenNMT. 3.1 Ranking Following V´azquez et al. (2019), we first produce an initial filtering of the ParaCrawl corpus. For this, we use the following heuristic filters from the OpusFilter toolbox: bly because similar filters are already applied in bicleaner when preparing the original data set. Nevertheless, these steps are useful for creating data to train models used in the later filtering methods. First, we train word alignment priors for the ¨ model 3 of the eflomal tool8 (Ostling and Tiedemann, 2016) and variable-length character n-gram models for"
2020.acl-demos.20,tiedemann-2012-parallel,1,0.803308,"ces in neural machine translation. Khayrallah and Koehn (2018) show that noisy training data is often more harmful for neural translation models than statistical translation models. As a consequence, international evaluation campaigns like the ones organised by WMT now feature shared tasks on data cleaning and ranking (Koehn et al., 2018, 2019). Various approaches have been proposed based on such challenges and directly benefit the development of MT engines in low-resource settings. This paper presents a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly available parallel corpora. In contrast to tools such as bicleaner (S´anchez-Cartagena et al., 2018) and Zipporah (Xu and Koehn, 2017), that implement a single method for parallel corpus filtering, OpusFilter is designed as a toolbox that is useful for testing and using many different approaches. Below we describe the design of OpusFilter and present its application in the test case of filtering Finnish-English parallel data included in ParaCrawl. 2 OpusFilter Toolbox The OpusFilter toolbox is implemented in Python 3 and is available at https://github.c"
2020.acl-demos.20,W19-5441,1,0.817299,"Missing"
2020.acl-demos.20,D17-1319,0,0.0183955,"n models. As a consequence, international evaluation campaigns like the ones organised by WMT now feature shared tasks on data cleaning and ranking (Koehn et al., 2018, 2019). Various approaches have been proposed based on such challenges and directly benefit the development of MT engines in low-resource settings. This paper presents a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly available parallel corpora. In contrast to tools such as bicleaner (S´anchez-Cartagena et al., 2018) and Zipporah (Xu and Koehn, 2017), that implement a single method for parallel corpus filtering, OpusFilter is designed as a toolbox that is useful for testing and using many different approaches. Below we describe the design of OpusFilter and present its application in the test case of filtering Finnish-English parallel data included in ParaCrawl. 2 OpusFilter Toolbox The OpusFilter toolbox is implemented in Python 3 and is available at https://github.com/ Helsinki-NLP/OpusFilter under the permissive MIT open-source license. The main script provided by the package is opusfilter, which takes a configuration file as an input."
2020.blackboxnlp-1.13,marelli-etal-2014-sick,0,0.0607422,"16; Hewitt and Manning, 2019; Rogers et al., 2020; Tenney et al., 2019) as well as variation in their context of use. We propose to explore the impact of context variation on word representations. We specifically address representations generated by the BERT model (Devlin et al., 2019), trained using a language modeling objective, and translation models involving one or more language pairs (Artetxe and Schwenk, 2019; V´azquez et al., 2020). We run a series of controlled experiments using sentences illustrating both meaning preserving and meaning altering transformations from the SICK dataset (Marelli et al., 2014b), and examples automatically generated using a template-based method (Prasad et al., 2019). We explore the impact of specific alternations on the representations, namely passivization and negation. Examples in our datasets consist of sentences that only differ in terms of the specific alternation addressed. In order to detect the imprint of these transformations on the representations, we employ methodology inspired by work on linguistic bias detection in embedding representations (Bolukbasi et al., 2016; Lauscher et al., 2019; Ravfogel et al., 2020). Furthermore, we investigate the impact o"
2020.blackboxnlp-1.13,2020.cl-2.5,1,0.86502,"Missing"
2020.blackboxnlp-1.13,D14-1162,0,0.0953117,"Missing"
2020.blackboxnlp-1.13,N18-1202,0,0.0526859,"er agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019), or semantic phenomena such as semantic role labeling and coreference (Tenney et al., 2019; Kovaleva et al., 2019). In our work, we shift the focus from interpreting the knowledge about language encoded in the representations, to exploring the imprint of two specific transformations, passivization and negation, on word representations. The majority of the above mentioned works address representations generated by models trained with a language modeling objective, such as LSTM RNNs (Linzen et al., 2016), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Voita et al. (2019a) propose to study the representations obtained from models trained with a different objective. We take the same stance and investigate the impact of context on representations generated by BERT, and by the encoder of neural machine translation (NMT) models involving one or more language pairs. In order to detect the information related to the two studied transformations that is encoded in the representations, we employ methodology initially proposed for identifying and removing linguistic and other kinds of biases from representations. Such"
2020.blackboxnlp-1.13,K19-1007,0,0.0615277,"Missing"
2020.blackboxnlp-1.13,2020.acl-demos.14,0,0.0902401,"ing and illustrates lexical, syntactic and semantic phenomena that compositional distributional semantic models are expected to account for. PAS is one of the meaning preserving alternations in SICK, where a sentence S2 results from the passivization of an active sentence S1. We use all the 276 sentence 4 Our code and data are available at https://github. com/Helsinki-NLP/Syntactic_Debiasing 138 5 The code is available at https://github.com/ grushaprasad/RNN-Priming. 6 The complexity of the sentences also resulted in numerous syntactic analysis errors when we tried to parse them using Stanza (Qi et al., 2020). 7 The dataset was used in SemEval 2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment (Marelli et al., 2014a). zi cannot be predicted from g(xi ). The method is based on iteratively (1) training a linear classifier to predict zi from xi , followed by (2) projecting xi on the null-space of the classifier, using a projection matrix PN (W ) such that W (PN (W ) x) = 0 ∀x, where W is the weight matrix of the classifier, and N (W ) is its null-space. Through the projection step in each iteration, the informa"
2020.blackboxnlp-1.13,D18-1521,0,0.0187822,"lation (NMT) models involving one or more language pairs. In order to detect the information related to the two studied transformations that is encoded in the representations, we employ methodology initially proposed for identifying and removing linguistic and other kinds of biases from representations. Such methods fall in two main paradigms: projection and adversarial methods. Projection methods identify specific directions in word embedding space that correspond to the protected attribute, and remove them. Bolukbasi et al. (2016) identify a gender subspace by exploring gendered word lists. Zhao et al. (2018) propose to train debiased word embeddings from scratch by altering the loss of the GloVe model (Pennington et al., 2014) to concentrate specific information (e.g., about gender) in a dedicated coordinate of each vector. Dev and Phillips (2019) propose a simple linear projection method to reduce the bias in word embed137 dings. Lauscher et al. (2019) develop a variation of this method that introduces more flexibility in the formation of the debiasing vector used in the projection. Adversial methods extend the main task objective with a component that competes with the encoder trying to extract"
2020.blackboxnlp-1.13,D19-1448,0,0.0373628,"Missing"
2020.blackboxnlp-1.13,P19-1580,0,0.0294334,"heme in direct object position. In , 2 the semantic relationship of the mafia and the millionaire to the kidnapping event is the same but their syntactic roles have changed. 3 These two transformations were preferred on the basis that they do not change the words in the sentence, as opposed to other possible translations, which involve reformulations, eg. “a sewing machine” vs. “a machine made for sewing”. Related Work The analysis and interpretation of the linguistic knowledge present in contextualized representations has recently been the focus of a large amount of work (Clark et al., 2019; Voita et al., 2019b; Tenney et al., 2019; Talmor et al., 2019). The bulk of this interpretation work relies on probing tasks which serve to predict linguistic properties from the representations generated by the models (Linzen, 2018; Rogers et al., 2020). These might involve structural aspects of language, such as syntax, word order, or number agreement (Linzen et al., 2016; Hewitt and Manning, 2019; Hewitt and Liang, 2019), or semantic phenomena such as semantic role labeling and coreference (Tenney et al., 2019; Kovaleva et al., 2019). In our work, we shift the focus from interpreting the knowledge about lang"
2020.lrec-1.467,J96-1001,0,0.179916,"creating a new bitext A → B. Section 4. illustrates the use by the example of the creation of MultiParaCrawl. Finally, another alignment tool, opus-pt2dice, extracts rough probabilistic bilingual dictionaries from phrase-translation-tables created from word alignment and using SMT tools coming out of the Moses toolbox. Those dictionaries use some heuristics to filter the data and the tool also creates additional Dice scores as a symmetrised alignment value out of the conditional translation probabilities included in the original phrase tables, which is useful for bilingual lexicon extraction (Smadja et al., 1996). Other tools: The last tool category contains additional data processing tools such as opus-udpipe and opus-index. The former implements a wrapper around UDPipe (Straka and Strakov´a, 2017) to annotate OPUS data and to store the result in OPUS-conforming XML. OpusTools can use pre-trained models coming from LINDAT.7 Last but not least, opus-index is a tool for indexing OPUS corpora using the Corpus Work Bench (CWB) (Evert and Hardie, 2011). It creates all import files and runs the encoder if available to create multiparallel corpora to be queried using the CWB search engine. 4. ParaCrawl and"
2020.lrec-1.467,2005.mtsummit-papers.11,0,0.229261,"leased with different formats. Alignment tools: Sentence and word alignment can be used in various ways and these tools provide some convenient operations on top of aligned bitexts. Other processing tools: This category includes tools for annotation and indexing. In the first category, we have import tools such as moses2opus, tmx2opus and xml2opus. Export scripts include opus2moses, tmx2moses, opus2text and opus2multi. xml2opus is a simple script that adds sentence boundaries to arbitrary XML data. Sentence boundary detection is done using the tools released with the Europarl parallel corpus (Koehn, 2005) and packaged in the Perl module Lingua::Sentence. Additional tools based on UD treebank classifiers will be integrated in the future. Inline tags that add markup within sentences are not supported at the moment. moses2opus reads aligned plain text files as commonly used in machine translation with aligned sentences on the same line.6 The tool converts the data into simple standalone XML for the corpus data and the XCES Align format for standoff sentence alignment as it is used within OPUS. Currently, only bilingual input is supported. Plain text files do not contain sentence boundaries but st"
2020.lrec-1.467,tiedemann-2012-parallel,1,0.785192,"ls also includes tools for language identification and data filtering as well as tools for importing data from various sources into the OPUS format. We show the use of these tools in parallel corpus creation and data diagnostics. The latter is especially useful for the identification of potential problems and errors in the extensive data set. Using these tools, we can now monitor the validity of data sets and improve the overall quality and consistency of the data collection. Keywords: Corpus (Creation, Annotation, etc.); Machine Translation; Tools, Systems, Applications 1. Introduction OPUS (Tiedemann, 2012) is the biggest collection of openly available parallel corpora. The collection has been growing constantly over the years and is widely used in work on machine translation and cross-linguistic research. Currently it contains 57 released corpora covering over 700 languages and language variants creating more than 70,000 bitexts in the sense of aligned language pairs across all corpora in the collection. The size and popularity of OPUS makes it necessary to build an efficient infrastructure that enables the various users to obtain and access the data and this paper introduces two packages that"
2020.lrec-1.486,K17-2001,0,0.060676,"Missing"
2020.lrec-1.486,W02-0603,0,0.267554,"example, the simple substitution dictionary based Byte Pair Encoding segmentation algorithm (Gage, 1994), first proposed for NMT by Sennrich et al. (2015), has become a standard in the field. Especially in the case of multilingual models, training a single language-independent subword segmentation method is preferable to linguistic segmentation (Arivazhagan et al., 2019). In this study, we compare three existing and one novel subword segmentation method, all sharing the use of a unigram language model in a generative modeling framework. The previously published methods are Morfessor Baseline (Creutz and Lagus, 2002), Greedy Unigram Likelihood (Varjokallio et al., 2013), and SentencePiece (Kudo, 2018). The new Morfessor variant proposed in this work is called Morfessor EM+Prune. The contributions of this article are (i) a better training algorithm for Morfessor Baseline, with reduction of search error during training, and improved segmentation quality for English, Finnish and Turkish; (ii) comparing four similar segmentation methods, including a close look at the SentencePiece reference implementation, highlighting details omitted from the original article (Kudo, 2018); (iii) and showing that the proposed"
2020.lrec-1.486,J11-2002,0,0.034351,"uited for languages with agglutinative morphology. While rule-based morphological segmentation systems can achieve high quality, the large amount of human effort needed makes the approach problematic, particularly for low-resource languages. The systems are language dependent, necessitating use of multiple tools in multilingual setups. As a fast, cheap and effective alternative, data-driven segmentation can be learned in a completely unsupervised manner from raw corpora. Unsupervised morphological segmentation saw much research interest until the early 2010’s; for a survey on the methods, see Hammarström and Borin (2011). Semi-supervised segmentation with already small amounts of annotated training data was found to improve the accuracy significantly when compared to a linguistic segmentation; see Ruokolainen et al. (2016) for a survey. While this line of research has been continued in supervised and more grammatically oriented tasks (Cotterell et al., 2017), the more recent work on unsupervised segmentation is less focused on approximating a linguistically motivated segmentation. Instead, the aim has been to tune subword segmentations for particular applications. For example, the simple substitution dictiona"
2020.lrec-1.486,W10-2210,1,0.755156,"Missing"
2020.lrec-1.486,D18-2012,0,0.03975,"hard-EM. During the hard-EM, frequency based pruning of subwords begins. In the second phase, hard-EM is used for parameter estimation. At the end of each iteration, the least frequent subwords are selected as candidates for pruning. For each candidate subword, the change in likelihood when removing the subword is estimated by resegmenting all words in which the subword occurs. After each pruned subword, the parameters of the model are updated. Pruning ends when the goal lexicon size is reached or the change in likelihood no longer exceeds a given threshold. 2.3. SentencePiece SentencePiece (Kudo and Richardson, 2018; Kudo, 2018) is a subword segmentation method aimed for use in any NLP system, particularly NMT. One of its design goals is use in multilingual systems. Although (Kudo, 2018) implies a use of maximum likelihood estimation, the reference implementation2 uses the implicit Dirichlet Process prior called Bayesian EM (Liang and Klein, 2007). In the M-step, the count normalization is modified to P (z) = exp(Ψ(Cz )) ∑ exp(Ψ( z′ Cz′ )) (6) where Ψ is the digamma function. The seed lexicon is simply the e.g. one million most frequent substrings. SentencePiece uses an EM+Prune training algorithm. Each"
2020.lrec-1.486,P18-1007,0,0.448825,"1994), first proposed for NMT by Sennrich et al. (2015), has become a standard in the field. Especially in the case of multilingual models, training a single language-independent subword segmentation method is preferable to linguistic segmentation (Arivazhagan et al., 2019). In this study, we compare three existing and one novel subword segmentation method, all sharing the use of a unigram language model in a generative modeling framework. The previously published methods are Morfessor Baseline (Creutz and Lagus, 2002), Greedy Unigram Likelihood (Varjokallio et al., 2013), and SentencePiece (Kudo, 2018). The new Morfessor variant proposed in this work is called Morfessor EM+Prune. The contributions of this article are (i) a better training algorithm for Morfessor Baseline, with reduction of search error during training, and improved segmentation quality for English, Finnish and Turkish; (ii) comparing four similar segmentation methods, including a close look at the SentencePiece reference implementation, highlighting details omitted from the original article (Kudo, 2018); (iii) and showing that the proposed Morfessor EM+Prune with particular hyper-parameters yields SentencePiece. 1.1. Morpho"
2020.lrec-1.486,W10-2211,1,0.648757,"ing only the likelihood ˆ = arg min{− log P (D |θ)} θ α 0.3 0.4 0.3 0.2 0.2 – (7) θ As the tuning parameter α is no longer needed when the prior is omitted, the pruning criterion can be set to a predetermined lexicon size, without automatic tuning of α. Morfessor by default uses type-based training; to use frequency information, count dampening should be turned off. The seed lexicon should be constructed without using forced splitting. The EM+Viterbi-prune training scheme should be used, with Bayesian EM turned on. English, Finnish and Turkish data are from the Morpho Challenge 2010 data set (Kurimo et al., 2010a; Kurimo et al., 2010b). The training sets contain ca 878k, 2.9M and 617k word types, respectively. As test sets we use the union of the 10 official test set samples. For North Sámi, we use a list of ca 691k word types extracted from Den samiske tekstbanken corpus (Sametinget, 2004). and the 796 word type test set from version 2 of the data set collected by (Grönroos et al., 2015; Grönroos et al., 2016). In most experiments we use a grid search with a development set to find a suitable value for α. The exception is experiments using autotuning or lexicon size criterion, and experiments using"
2020.lrec-1.486,J16-1003,1,0.936942,"arly for low-resource languages. The systems are language dependent, necessitating use of multiple tools in multilingual setups. As a fast, cheap and effective alternative, data-driven segmentation can be learned in a completely unsupervised manner from raw corpora. Unsupervised morphological segmentation saw much research interest until the early 2010’s; for a survey on the methods, see Hammarström and Borin (2011). Semi-supervised segmentation with already small amounts of annotated training data was found to improve the accuracy significantly when compared to a linguistic segmentation; see Ruokolainen et al. (2016) for a survey. While this line of research has been continued in supervised and more grammatically oriented tasks (Cotterell et al., 2017), the more recent work on unsupervised segmentation is less focused on approximating a linguistically motivated segmentation. Instead, the aim has been to tune subword segmentations for particular applications. For example, the simple substitution dictionary based Byte Pair Encoding segmentation algorithm (Gage, 1994), first proposed for NMT by Sennrich et al. (2015), has become a standard in the field. Especially in the case of multilingual models, training"
2020.lrec-1.486,D11-1117,0,0.0357899,"steriori (MAP) estimates for parameters in models with latent variables. The EM algorithm consists of two steps. In the E-step (2), the expected value of the complete data likelihood including the latent variable is taken, and in the M-step (3), the parameters are updated to maximize the expected value of the E-step: ∫ Q(θ, θ (i−1) ) = log P (D, y |θ)P (y |D, θ (i−1) )dy y (2) i θ = arg max Q(θ, θ (i−1) ). (3) θ When applied to a (hidden) Markov model, EM is called the forward-backward algorithm. Using instead the related Viterbi algorithm (Viterbi, 1967) is sometimes referred to as hard-EM.1 Spitkovsky et al. (2011) present lateen-EM, a hybrid variant in which EM and Viterbi optimization are alternated. Virpioja (2012, Section 6.4.1.3) discusses the challenges of applying EM to learning of generative morphology. Jointly optimizing both the morph lexicon and the parameters for the morphs is intractable. If, like in Morfessor Baseline, the cost function is discontinuous when morphs are added or removed from the lexicon, there is no closed form solution to the M-step. With ML estimates for morph probabilities, EM can neither add nor remove morphs from the lexicon, because it can neither change a zero probab"
2020.sltu-1.6,P19-1120,0,0.0244691,"orms adversely. We also study the impact on cross-lingual transfer due to the target data size and number of source model layers transferred. Relatively, smaller amounts of target language data than the source language data leads to more considerable ASR performance improvements. Moreover, we find that pretrained NNLMs perform best when we transfer only the parameters of the lowest layer of the source model. Multilingual training of language models has successfully leveraged datasets from other languages to improve Neural Network Language Modeling (NNLM) performance in low-resource scenarios (Kim et al., 2019; Conneau and Lample, 2019; Conneau et al., 2019; Aharoni et al., 2019). One such method for training NNLM is the multi-taskbased approach, where multiple language corpora train the model simultaneously (Aharoni et al., 2019). Another approach is cross-lingual pretraining, where the NNLM is trained on a set of source languages followed by fine-tuning on the target language (Kim et al., 2019; Conneau and Lample, 2019; Conneau et al., 2019). The second approach, explored in this work, is favorable when re-training with the large source data is time-consuming as an existing trained source model’s"
2020.sltu-1.6,P19-4007,0,0.031474,"Missing"
2020.wmt-1.134,2020.lrec-1.467,1,0.814685,"ade available on the WMT website. In this section, we present the parallel and monolingual resources that we used for our systems. 2.1 Inuktitut–English Training data The training resources for the Inuktitut–English tasks are summarized in Table 1. Two allowed parallel resources are provided, the training part of the Nunavut Hansard (NH) corpus (Joanis et al., 2020) and the small WikiTitles corpus. Since the NH training corpus contained a significant proportion of duplicates and preliminary experiments suggested a slight adverse effect of duplicates, we removed them with the OpusFilter tools (Aulamo et al., 2020). We also cleaned the WikiTitles corpus, removing Inuktitut entries not in syllabic script and identical entries. The Inuktitut side of both training corpora was also used to create a parallel corpus for the romanized ↔ syllabic transliteration task. The romanized version was converted from the syllabic one using the uniconv + iconv pipeline proposed by the corpus providers. The NH corpus contains a large amount of unaligned data, which we used as additional monolingual corpora. We removed all sentences that were already covered by one of the parallel NH datasets. The English and Inuktitut par"
2020.wmt-1.134,W18-1207,0,0.0139709,"out-ofvocabulary words, and heavy computational costs due to large vocabularies. Moreover, such wordlevel modeling does not allow the productive recombination of morphemes and is thus unsuitable for morphologically rich languages such as Inuktitut or Sorbian. In recent years, a consensus has emerged that NMT vocabularies should consist of subwords of variable size. Various unsupervised word segmentation algorithms have been proposed, among which byte-pair encoding (BPE) (Sennrich et al., 2016), SentencePiece (Kudo and Richardson, 2018), and several variants of Morfessor (Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Gr¨onroos et al., 2018, 2020). Besides the actual word segmentation algorithm, various parameters influence the quality of the resulting translation system: 4 We use Europarl, NewsCommentary, Taoeba and WMTNews as Czech monolingual corpora, and Training, Sorbian Institute and Witaj as Sorbian monolingual corpora. 1132 • Separate word segmentation models for each language or one joint vocabulary for all languages. The joint approach scales better to multilingual models, and enables consistent segmentation of named entities and cognate Segmentation model Translation model Algorithm Training da"
2020.wmt-1.134,W17-3203,0,0.0157694,".4M CS 708k / 1931k 708k / 1931k 61.90 61.56 55.06 55.04 62.41 62.16 53.78 53.83 Table 4: Segmentation model experiments for German–Upper Sorbian. All segmentation models are joint models with 20 000 units, but trained on variable amounts of data. All translation models are OpenNMT-py Transformers with default settings with active subword sampling, trained either without (1–6) or with (7–8) additional backtranslations. words across languages, assuming they are written in the same script. • The chosen vocabulary size and the amount of training data from which the segmentation model is learned. Denkowski and Neubig (2017) recommend a vocabulary size of 32k units, trained jointly on all languages, for normal-sized datasets. In contrast, Ding et al. (2019) obtain the best results with small vocabularies of only 500 units in low-resource scenarios. Optimal vocabulary size varies thus depending on the size of the parallel and monolingual data. • If the segmentation algorithm is based on a probabilistic model (such as SentencePiece or Morfessor), it can be used to sample different segmentations for any given word. This technique is known as subword regularization (Kudo, 2018) and has been shown to improve the robus"
2020.wmt-1.134,N19-1423,0,0.00664495,"tchOut (Wang et al., 2018), and subword regularization (Kudo, 2018). Sampling fresh noise for each minibatch is important, especially in low-resource conditions where the small data set is reused for many epochs. The denoising sequence autoencoder has previously been applied to language model pretraining in BART (Lewis et al., 2019). Typical noise models for denoising sequence autoencoder apply small changes to the input side of the corpus: local reordering (Lample et al., 2018), deletions (Iyyer et al., 2015), insertions (Vaibhav et al., 2019), substitutions (Wang et al., 2018), and masking (Devlin et al., 2019). Of these, our method applies local reordering and token deletion. Taboo sampling segmentation task. Gr¨onroos et al. (2020) propose taboo sampling, a noise model extending the subword regularization idea specifically for monolingual data. It takes in monolingual text and generates two maximally different segmentations, e.g. dys + functional on the source side and dysfunction + al on the target side. During taboo sampling, all multi-character subwords used in the first segmentation have their probability temporarily set to zero, to ensure that they are not used in the second segmentation. Tra"
2020.wmt-1.134,W19-6620,0,0.0147578,"n. All segmentation models are joint models with 20 000 units, but trained on variable amounts of data. All translation models are OpenNMT-py Transformers with default settings with active subword sampling, trained either without (1–6) or with (7–8) additional backtranslations. words across languages, assuming they are written in the same script. • The chosen vocabulary size and the amount of training data from which the segmentation model is learned. Denkowski and Neubig (2017) recommend a vocabulary size of 32k units, trained jointly on all languages, for normal-sized datasets. In contrast, Ding et al. (2019) obtain the best results with small vocabularies of only 500 units in low-resource scenarios. Optimal vocabulary size varies thus depending on the size of the parallel and monolingual data. • If the segmentation algorithm is based on a probabilistic model (such as SentencePiece or Morfessor), it can be used to sample different segmentations for any given word. This technique is known as subword regularization (Kudo, 2018) and has been shown to improve the robustness of translation models. Gr¨onroos et al. (2020) tested various segmentation model configurations on a multilingual translation tas"
2020.wmt-1.134,W18-6410,1,0.895101,"Missing"
2020.wmt-1.134,2020.lrec-1.486,1,0.885032,"Missing"
2020.wmt-1.134,2020.lrec-1.312,0,0.16699,"n– German tasks can be qualified as low-resource settings, with less than 800K (deduplicated) parallel training instances for the former and 60K for the latter. For both tasks, we follow the constrained setting, which limits the allowed data to those made available on the WMT website. In this section, we present the parallel and monolingual resources that we used for our systems. 2.1 Inuktitut–English Training data The training resources for the Inuktitut–English tasks are summarized in Table 1. Two allowed parallel resources are provided, the training part of the Nunavut Hansard (NH) corpus (Joanis et al., 2020) and the small WikiTitles corpus. Since the NH training corpus contained a significant proportion of duplicates and preliminary experiments suggested a slight adverse effect of duplicates, we removed them with the OpusFilter tools (Aulamo et al., 2020). We also cleaned the WikiTitles corpus, removing Inuktitut entries not in syllabic script and identical entries. The Inuktitut side of both training corpora was also used to create a parallel corpus for the romanized ↔ syllabic transliteration task. The romanized version was converted from the syllabic one using the uniconv + iconv pipeline prop"
2020.wmt-1.134,Q18-1017,0,0.0241358,"ents presented in Tables 3 and 4 already confirmed the positive impact of subword regularization and backtranslation. Row 1 of Tables 5 and 6 provide baseline results with these two techniques. Backtranslated training instances are marked with a special token. Scheduled multi-task learning As row 2 in Table 6 shows, the mere inclusion of a German↔Czech task with language labels but without any task scheduling already increases BLEU scores by 1.5 points. However, simple transfer learning setups such as this are prone to catastrophic forgetting, especially in low-resource settings such as ours. Kiperwasser and Ballesteros (2018) propose a general strategy called scheduled multi-task learning, in which different tasks are mixed according to a task-mix schedule. Gr¨onroos et al. (2020) propose a partwise constant task-mix schedule with an arbitrary number of steps, any of which can be mixing multiple tasks. This flexibility is useful when training with a large number of heterogeneous tasks: multiple language pairs with different amounts of data, data from different domains (oversampling the in-domain data), natural vs synthetic (e.g. back-translated) data, and auxiliary tasks (e.g. autoencoder). A training schedule wit"
2020.wmt-1.134,P17-4012,0,0.0172715,"nroos et al. (2020). The Transformer contains 8 encoder and 8 decoder layers with 16 attention heads each. The hidden layer size is 1024, the filter size 4096. The minibatch varies between 7200 and 9200 tokens, depending on the task, and gradients are accumulated over 4 minibatches. All models were trained for 200 000 steps, which corresponded to 5–7 days training time on a single V100 GPU. The best savepoint was selected on the basis of development set accuracy; this measure turned out to be more stable than development set BLEU score. We use the dynamicdata branch of the OpenNMT-py toolkit (Klein et al., 2017) for our experiments.6 This branch provides the necesWitaj and Sorbian Institute corpora. We added an equivalent amount of German data from NewsCommentary and WMTNews. The Czech data also stems from NewsCommentary and WMT-News and is complemented by a subset of Czech Europarl. 6 https://github.com/Waino/OpenNMT-py The functionality of the dynamicdata branch is included by sary adaptations for the techniques introduced by Gr¨onroos et al. (2020): scheduled multi-task learning requires the ability to adjust the task mix during training, whereas subword regularization and the denoising sentence a"
2020.wmt-1.134,P18-1007,0,0.158466,"urce target language (HRL), Finnish. Among the WMT 2020 shared tasks, the German → Upper Sorbian low-resource translation task exactly corresponds to this setup, with Czech being a high-resource language closely related to Upper Sorbian. We adapt the approach proposed by Gr¨onroos et al. (2020) also to three slightly different scenarios: in the Upper Sorbian → German task, the low-resource language is on the source side, but can be complemented with Czech in the same way; for the English → Inuktitut task, no related high-resource language is available; and for Subword regularization Following Kudo (2018), each time a word is used during training, a new segmentation into subwords is sampled from the probabilistic segmentation model. Monolingual tasks In order to benefit from more easily available monolingual data and to make the model more robust to noise, they propose to include denoising sequence autoencoder tasks. A first variant applies small changes to the input side of the corpus (e.g. word deletions, substitutions and reorderings). A second variant, called taboo sampling, relies on the subword regularization idea and generates two maximally different segmentations of the source and targ"
2020.wmt-1.134,D18-2012,0,0.0118493,"are represented as atomic vocabulary items, leads to sparse statistics, issues with out-ofvocabulary words, and heavy computational costs due to large vocabularies. Moreover, such wordlevel modeling does not allow the productive recombination of morphemes and is thus unsuitable for morphologically rich languages such as Inuktitut or Sorbian. In recent years, a consensus has emerged that NMT vocabularies should consist of subwords of variable size. Various unsupervised word segmentation algorithms have been proposed, among which byte-pair encoding (BPE) (Sennrich et al., 2016), SentencePiece (Kudo and Richardson, 2018), and several variants of Morfessor (Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Gr¨onroos et al., 2018, 2020). Besides the actual word segmentation algorithm, various parameters influence the quality of the resulting translation system: 4 We use Europarl, NewsCommentary, Taoeba and WMTNews as Czech monolingual corpora, and Training, Sorbian Institute and Witaj as Sorbian monolingual corpora. 1132 • Separate word segmentation models for each language or one joint vocabulary for all languages. The joint approach scales better to multilingual models, and enables consistent segmentatio"
2020.wmt-1.134,N16-1162,0,0.0255245,"Missing"
2020.wmt-1.134,P16-1162,0,0.0245308,"t solution however, in which word forms are represented as atomic vocabulary items, leads to sparse statistics, issues with out-ofvocabulary words, and heavy computational costs due to large vocabularies. Moreover, such wordlevel modeling does not allow the productive recombination of morphemes and is thus unsuitable for morphologically rich languages such as Inuktitut or Sorbian. In recent years, a consensus has emerged that NMT vocabularies should consist of subwords of variable size. Various unsupervised word segmentation algorithms have been proposed, among which byte-pair encoding (BPE) (Sennrich et al., 2016), SentencePiece (Kudo and Richardson, 2018), and several variants of Morfessor (Ataman et al., 2017; Banerjee and Bhattacharyya, 2018; Gr¨onroos et al., 2018, 2020). Besides the actual word segmentation algorithm, various parameters influence the quality of the resulting translation system: 4 We use Europarl, NewsCommentary, Taoeba and WMTNews as Czech monolingual corpora, and Training, Sorbian Institute and Witaj as Sorbian monolingual corpora. 1132 • Separate word segmentation models for each language or one joint vocabulary for all languages. The joint approach scales better to multilingual"
2020.wmt-1.134,P15-1162,0,0.0601642,"Missing"
2020.wmt-1.134,N18-2074,0,0.0284829,"ween the transliteration and taboo tasks. For Sorbian, the monolingual tasks only help when translating towards German, but not when translating towards Sorbian. One reason for this somewhat surprising finding could be that the Sorbian monolingual data is identical with the Sorbian target of the backtranslations, so that no additional data is added with the monolingual tasks. 5 Submissions and results For the best-performing configurations, we trained two models each, one (“basic”) with the hyperparameters listed above, and an alternative one with relative position distance clipping at 4 (see Shaw et al., 2018). However, this setting did not yield any consistent accuracy gains or losses. For the Inuktitut task, we submitted single systems of settings 2 and 3 for both directions. For EN→IU, the alternative model of setting 2 obtained the best scores on the test set (10.1 BLEU / 0.301 chrF), whereas for IU→EN, the basic model of setting 3 obtained the best scores on the test set (23.0 BLEU / 0.455 chrF). Among the 11 primary submissions in both translation directions, our sub1135 EN→IU IU→EN Training steps 0–200k 0–200k Bilingual Backtranslation Noise EN Noise IU Taboo IU / Translit. rom.→syll. Taboo"
2020.wmt-1.134,tiedemann-2012-parallel,0,0.115013,"ntences with an average character cross-entropy higher than 30 on either side were removed. 2.2 Upper Sorbian–German Training data The training data for the Upper Sorbian–German tasks are summarized in Table 2. The organizers provide a parallel German–Sorbian corpus of 60k sentence pairs that we use without further filtering or processing. Moreover, we use four sources of parallel German–Czech data for both directions: the Europarl and JW300 corpora provided on OPUS, as suggested by the organizers, and additionally the Tatoeba and NewsCommentary corpora, which are also available through OPUS (Tiedemann, 2012). The German side of three datasets2 is backtranslated to Upper Sorbian using a baseline system. The Czech side of the four datasets is backtranslated to Upper Sorbian using an unsupervised character-level translation system (see below). Length filters are applied to all data from external resources (see below). The organizers provide three monolingual Sorbian corpora: Sorbian Institute, a Sorbian Web Crawl, and Witaj. All corpora are backtranslated to German using a baseline system and filtered. As monolingual German and Czech resources, we selected the NewsCommentary corpus and the 2018 and"
2020.wmt-1.134,N19-1190,0,0.0156437,"(Srivastava et al., 2014), label smoothing (Szegedy et al., 2016), SwitchOut (Wang et al., 2018), and subword regularization (Kudo, 2018). Sampling fresh noise for each minibatch is important, especially in low-resource conditions where the small data set is reused for many epochs. The denoising sequence autoencoder has previously been applied to language model pretraining in BART (Lewis et al., 2019). Typical noise models for denoising sequence autoencoder apply small changes to the input side of the corpus: local reordering (Lample et al., 2018), deletions (Iyyer et al., 2015), insertions (Vaibhav et al., 2019), substitutions (Wang et al., 2018), and masking (Devlin et al., 2019). Of these, our method applies local reordering and token deletion. Taboo sampling segmentation task. Gr¨onroos et al. (2020) propose taboo sampling, a noise model extending the subword regularization idea specifically for monolingual data. It takes in monolingual text and generates two maximally different segmentations, e.g. dys + functional on the source side and dysfunction + al on the target side. During taboo sampling, all multi-character subwords used in the first segmentation have their probability temporarily set to"
2020.wmt-1.134,D18-1100,0,0.0211107,"oder task. In the denoising autoencoder (Vincent et al., 2008; Hill et al., 2016) clean text is corrupted by sampling from a noise model, and fed in as a pseudo-source. The target is a reconstruction of the clean input. The goal of the autoencoder tasks is to use monolingual data to strengthen target language modeling in the decoder and source language understanding in the encoder. In addition, the autoencoder task acts as regularization. Noise has been used as a regularizer in many NLP techniques, including dropout (Srivastava et al., 2014), label smoothing (Szegedy et al., 2016), SwitchOut (Wang et al., 2018), and subword regularization (Kudo, 2018). Sampling fresh noise for each minibatch is important, especially in low-resource conditions where the small data set is reused for many epochs. The denoising sequence autoencoder has previously been applied to language model pretraining in BART (Lewis et al., 2019). Typical noise models for denoising sequence autoencoder apply small changes to the input side of the corpus: local reordering (Lample et al., 2018), deletions (Iyyer et al., 2015), insertions (Vaibhav et al., 2019), substitutions (Wang et al., 2018), and masking (Devlin et al., 2019). Of t"
2021.americasnlp-1.29,P19-1310,0,0.0459725,"Missing"
2021.americasnlp-1.29,2020.acl-demos.20,1,0.740639,"directed to finding shared task is aimed at developing machine transla- relevant corpora that could help with the translation tion (MT) systems for indigenous languages of the tasks, as well as to make the best out of the data Americas, all of them paired with Spanish (Mager provided by the organizers. In order to have an efet al., 2021). Needless to say, these language pairs ficient procedure to maintain and process the data pose big challenges since none of them benefits sets for all the ten languages, we utilized the Opusfrom large quantities of parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the config"
2021.americasnlp-1.29,2020.lrec-1.356,0,0.0833266,"Missing"
2021.americasnlp-1.29,2020.lrec-1.320,0,0.232022,"Missing"
2021.americasnlp-1.29,2020.coling-main.351,0,0.459236,"Missing"
2021.americasnlp-1.29,2020.lrec-1.352,0,0.702383,"parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the configuration file translations of political constitutions of various with a Python script. Latin American countries, (2) cleaning and filterFigure 1 shows a part of the applied OpusFiling the corpora to maximize their quality with the ter workflow for a single language pair, Spanish– OpusFilter toolbox (Aulamo et al., 2020), and (3) Raramuri, and restricted to the primary training contrasting different training techniques that could data. The provided training set and (concatenated) take advantage of the scarce data available. 1 https://github.com/Helsinki"
2021.americasnlp-1.29,tiedemann-2012-parallel,1,0.926026,"hem benefits sets for all the ten languages, we utilized the Opusfrom large quantities of parallel data and there is Filter toolbox2 (Aulamo et al., 2020). It provides limited monolingual data. For our participation, both ready-made and extensible methods for comwe focused our efforts mainly on three aspects: (1) bining, cleaning, and filtering parallel and monogathering additional parallel and monolingual data lingual corpora. OpusFilter uses a configuration for each language, taking advantage in particular file that lists all the steps for processing the data; of the OPUS corpus collection (Tiedemann, 2012), in order to make quick changes and extensions prothe JHU Bible corpus (McCarthy et al., 2020) and grammatically, we generated the configuration file translations of political constitutions of various with a Python script. Latin American countries, (2) cleaning and filterFigure 1 shows a part of the applied OpusFiling the corpora to maximize their quality with the ter workflow for a single language pair, Spanish– OpusFilter toolbox (Aulamo et al., 2020), and (3) Raramuri, and restricted to the primary training contrasting different training techniques that could data. The provided training se"
2021.americasnlp-1.29,W19-5441,1,0.88874,"Missing"
2021.americasnlp-1.29,W19-6804,0,0.422544,"Missing"
2021.americasnlp-1.29,2020.loresmt-1.1,0,0.313054,"Missing"
2021.americasnlp-1.29,L16-1144,0,0.334949,"Missing"
2021.americasnlp-1.29,2020.wmt-1.134,1,0.835483,"Missing"
2021.americasnlp-1.29,2020.wmt-1.139,1,0.845344,"Missing"
2021.nodalida-main.36,W17-0122,0,0.0492109,"MAUS) is a popular aligner based on its own speech recognition framework, utilizing a statistical expert system of pronunciation. 2.2 Cross-language forced alignment Forced alignment has also been successfully used across languages, e.g., when the target language does not have enough transcribed data. This task is called cross-language or cross-linguistic forced alignment (CLFA), sometimes untrained forced alignment. Kempton et al. (2011) used their own phonetic distance metric to evaluate the accuracy of three phoneme recognizers on isolated words from under-resourced language, and again in (Kempton, 2017) to a different target language. In another early experiment (DiCanio et al., 2013), tools trained on English were used to align isolated words from Yolox´ochitl Mixtec. Free conversations were aligned in (Kurtic et al., 2012), where authors tested multiple phoneme recognizers on Bosnian Serbo-Croatian. Most of the tools introduced at the start of this section have also been tried for CLFA. The authors of MAUS experimented a language-independent ’sampa’ version on a multitude of under-resourced languages by comparing word start and end boundaries (Strunk et al., 2014). Later Jones et al. (2019"
2021.nodalida-main.36,kurtic-etal-2012-corpus,0,0.036464,"s languages, e.g., when the target language does not have enough transcribed data. This task is called cross-language or cross-linguistic forced alignment (CLFA), sometimes untrained forced alignment. Kempton et al. (2011) used their own phonetic distance metric to evaluate the accuracy of three phoneme recognizers on isolated words from under-resourced language, and again in (Kempton, 2017) to a different target language. In another early experiment (DiCanio et al., 2013), tools trained on English were used to align isolated words from Yolox´ochitl Mixtec. Free conversations were aligned in (Kurtic et al., 2012), where authors tested multiple phoneme recognizers on Bosnian Serbo-Croatian. Most of the tools introduced at the start of this section have also been tried for CLFA. The authors of MAUS experimented a language-independent ’sampa’ version on a multitude of under-resourced languages by comparing word start and end boundaries (Strunk et al., 2014). Later Jones et al. (2019) compared MAUS’ language-independent and Italian versions for conversational speech in 1 https://github.com/lowerquality/gentle Kriol, finding that the Italian version surpassed the language-independent one. A unifying method"
2021.nodalida-main.36,strunk-etal-2014-untrained,0,0.0160417,"sourced language, and again in (Kempton, 2017) to a different target language. In another early experiment (DiCanio et al., 2013), tools trained on English were used to align isolated words from Yolox´ochitl Mixtec. Free conversations were aligned in (Kurtic et al., 2012), where authors tested multiple phoneme recognizers on Bosnian Serbo-Croatian. Most of the tools introduced at the start of this section have also been tried for CLFA. The authors of MAUS experimented a language-independent ’sampa’ version on a multitude of under-resourced languages by comparing word start and end boundaries (Strunk et al., 2014). Later Jones et al. (2019) compared MAUS’ language-independent and Italian versions for conversational speech in 1 https://github.com/lowerquality/gentle Kriol, finding that the Italian version surpassed the language-independent one. A unifying method was presented by Tang and Bennett (2019), who combined a larger source language and the target language with MFA to train the aligner. Finally Johnson et al. (2018) reviewed previous CLFA research and experimented on the minimum amount of data necessary for language dependent forced alignment, achieving good results with an hour of transcribed s"
2021.nodalida-main.37,2020.acl-demos.20,1,0.842361,"Missing"
2021.nodalida-main.37,W18-6315,0,0.0187219,"tion from Northern S´ami to Finnish (Pirinen et al., 2017) within the Apertium framework (Forcada et al., 2011). We also combine both methods to further augment the data. Our experiments demonstrate the positive effects of both strategies and the possibility of obtaining complementary information from different backtranslation engines. 2 Related work Using backtranslations from different sources as training data has been shown to be beneficial for improving machine translation quality. In addition to proposing training data augmentation methods that do not require reverse translation systems, Burlot and Yvon (2018) compare the effects of using statistical machine translation (SMT) and NMT based backtranslations for English→French and English→German translations. They show that both types of backtranslations improve translation quality, NMT slightly more than SMT. Poncelas et al. (2019) also produce backtranslations with SMT and NMT. They show that the translation quality of a German→English NMT system is improved when including either type of backtranslations in the training data. The greatest improvement is observed when both types of backtranslations are used. Augmenting training data with RBMT backtr"
2021.nodalida-main.37,D14-1179,0,0.0250003,"Missing"
2021.nodalida-main.37,W19-6908,0,0.0266878,"T based backtranslations for English→French and English→German translations. They show that both types of backtranslations improve translation quality, NMT slightly more than SMT. Poncelas et al. (2019) also produce backtranslations with SMT and NMT. They show that the translation quality of a German→English NMT system is improved when including either type of backtranslations in the training data. The greatest improvement is observed when both types of backtranslations are used. Augmenting training data with RBMT backtranslations has also proven to be useful for boosting translation quality. Dowling et al. (2019) use RBMT backtranslations to improve statistical machine translation performance for Scottish Gaelic→English translations. The authors show that backtranslations can be beneficial even in cases where the translation quality of the MT system used to produce the backtranslations is low. Soto et al. (2019) study the performance of NMT systems trained with augmented training data backtranslated using RBMT, SMT and NMT. They experiment with Basque→Spanish translations and show that the translation performance improves when using each type of augmented training data individually. Soto et al. (2020)"
2021.nodalida-main.37,2020.findings-emnlp.352,0,0.0350446,"as the backtranslation model. For Transformers, we use the example hyperparameters from MarianNMT 4 which replicate the setup 4 https://github.com/marian-nmt/ marian-examples/tree/master/transformer NMT RBMT UiT 19.4 12.3 YLE 4.5 10.0 Table 1: Reverse translation model (sme-fin) quality in BLEU points evaluated with the UiT test set and the YLE test set. from Vaswani et al. (2017). For subword segmentation, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with vocabulary size 8000, which has been shown to produce the best results with the data set sizes that we are dealing with (Gowda and May, 2020; Gr¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a module for syntactic transformation operations. Table 1 shows the quality of the sme-fin translation models used for backtranslations in BLEU point"
2021.nodalida-main.37,P18-4020,0,0.029041,"Missing"
2021.nodalida-main.37,D18-2012,0,0.0240889,"rection. All models using additional backtranslated training sets are trained with both RNNs and Transformers. All RNN models have the same architecture as the backtranslation model. For Transformers, we use the example hyperparameters from MarianNMT 4 which replicate the setup 4 https://github.com/marian-nmt/ marian-examples/tree/master/transformer NMT RBMT UiT 19.4 12.3 YLE 4.5 10.0 Table 1: Reverse translation model (sme-fin) quality in BLEU points evaluated with the UiT test set and the YLE test set. from Vaswani et al. (2017). For subword segmentation, we use the SentencePiece tokenizer (Kudo and Richardson, 2018) with vocabulary size 8000, which has been shown to produce the best results with the data set sizes that we are dealing with (Gowda and May, 2020; Gr¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a m"
2021.nodalida-main.37,P12-3005,0,0.0489923,"tences are empty or longer than 100 words, • The ratio of the sentence lengths in words is greater than 3, • The sentence pair contains words longer than 40 characters, • The sentence pair contains HTML elements, • The sentences have dissimilar numerals based on the “Non-zero numerals score” (V´azquez et al., 2019), • The sentences have dissimilar punctuation based on the “Terminal punctuation score” (V´azquez et al., 2019), • The sentence pair contains characters outside of the Latin script, • The sentences are not recognized to be their correct language by the langid.py language identifier (Lui and Baldwin, 2012). After filtering, 29,106 clean sentence pairs remain in the parallel data set. From this clean set, 2000 pairs are randomly selected to form a validation set and another 2000 pairs to form a test set, leaving 25,106 pairs for training. Note that all subsets are disjoint due to the initial deduplication. The additional test set consists of two news articles describing S´ami culture in Finland available in both Finnish and Northern S´ami on YLE News. It was extracted from the web and manually aligned to create a clean reference set. This test set 2 https://yle.fi/uutiset/osasto/sapmi/ https://g"
2021.nodalida-main.37,P02-1040,0,0.113675,"r¨onroos et al., 2021). We train the models until the cross-entropy of the validation set does not improve for 10 consecutive validation steps. For the RBMT backtranslations, we use Apertium with the sme-fin model by Pirinen et al. (2017). This system implements a shallow transfer-based translation engine consisting of modules for morphological analysis, disambiguation and generation, modules for lexical translation based on context rules, and a module for syntactic transformation operations. Table 1 shows the quality of the sme-fin translation models used for backtranslations in BLEU points (Papineni et al., 2002). The NMT model performs much better with UiT test data than with the YLE test data, which shows that the NMT system is strongly adapted to the UiT data, while the RBMT system has similar performance with both test sets. 4.1 Backtranslations All the 462,803 sentences of the cleaned monolingual data are translated with the sme-fin NMT and RBMT models. As the quality of the source side of the backtranslations is not as important as the quality of the target side (Sennrich et al., 2016), we keep an unfiltered version of both backtranslation data sets. To see the effect of filtering the augmented"
2021.nodalida-main.37,W17-0214,0,0.13371,"driven translation systems with automatically created synthetic data, e.g. backtranslation (Sennrich et al., 2016). In this paper, we combine both strategies in the context of neural machine translation (NMT) from Finnish to Northern S´ami. In particular, we investigate the impact of RBMT in data augmentation in comparison to standard NMT-based backtranslation. Northern S´ami is a Uralic minority language spoken in Norway, Sweden and Finland. Historically, most of the work on machine translation from and to S´ami languages is based on RBMT (Trosterud and Unhammer, 2012; Antonsen et al., 2017; Pirinen et al., 2017). Data-driven approaches such as NMT are generally more competitive, but require large amounts of training data in the form of parallel translated sentences. For minority languages, finding parallel data sets is usually more difficult than collecting monolingual data, which is also the case for Northern S´ami. A common way of leveraging monolingual data for NMT is the above mentioned backtranslation strategy, a method where monolingual data of the target language is translated automatically to the source language to create additional parallel training data. In this work, we use two reverse tra"
2021.nodalida-main.37,R19-1107,0,0.0350046,"Missing"
2021.nodalida-main.37,P16-1009,0,0.274972,"es the RBMT approach only for the in-domain test set. This suggests that the RBMT system provides general-domain knowledge that cannot be found from the relative small parallel training data. 1 Introduction Machine translation from and to minority languages is challenging because large parallel corpora are typically hard to obtain. Two strategies have proven most successful to eliminate this bottleneck: using rule-based machine translation (RBMT) systems that do not rely on large data, or training data-driven translation systems with automatically created synthetic data, e.g. backtranslation (Sennrich et al., 2016). In this paper, we combine both strategies in the context of neural machine translation (NMT) from Finnish to Northern S´ami. In particular, we investigate the impact of RBMT in data augmentation in comparison to standard NMT-based backtranslation. Northern S´ami is a Uralic minority language spoken in Norway, Sweden and Finland. Historically, most of the work on machine translation from and to S´ami languages is based on RBMT (Trosterud and Unhammer, 2012; Antonsen et al., 2017; Pirinen et al., 2017). Data-driven approaches such as NMT are generally more competitive, but require large amount"
2021.nodalida-main.37,W19-7102,0,0.0294514,"Missing"
2021.nodalida-main.37,2020.acl-main.359,0,0.0180743,"ling et al. (2019) use RBMT backtranslations to improve statistical machine translation performance for Scottish Gaelic→English translations. The authors show that backtranslations can be beneficial even in cases where the translation quality of the MT system used to produce the backtranslations is low. Soto et al. (2019) study the performance of NMT systems trained with augmented training data backtranslated using RBMT, SMT and NMT. They experiment with Basque→Spanish translations and show that the translation performance improves when using each type of augmented training data individually. Soto et al. (2020) also analyze the effects of using augmented training data backtranslated with the three different paradigms. They focus on two language pairs: a low-resource language pair, Basque→Spanish, and a high-resource language pair, German→English. In addition to showing similar results as Soto et al. (2019), they show further improvement in translation performance when all types of augmented training data are combined. 3 Data The UiT freecorpus1 contains a Finnish - Northern S´ami (fin-sme) parallel corpus with 110k sentence pairs and a distinct set of 868k monolingual Northern S´ami sentences. The U"
2021.nodalida-main.37,2012.freeopmt-1.3,0,0.116304,"Missing"
2021.nodalida-main.37,W19-5441,1,0.894614,"Missing"
C14-1111,I13-1152,0,0.0206284,"+FlatCat CatMAP CatMAP CRF+FlatCat FlatCat (First 5) Baseline CRF Baseline Baseline FlatCat FlatCat (Words) No Yes Yes – Yes Yes No No No – – – – – Yes No – 0.5057 0.5029 0.4987 0.4973 0.4912 0.4884 0.4865 0.4826 0.4821 0.4757 0.4722 0.4660 0.4582 0.4378 0.4349 0.4334 0.3483 Table 5: Information Retrieval results. Results of the method presented in this paper are hilighted using boldface. Mean Average Precision is abbreviated as MAP. Short affix removal is abbreviated as SAR. help disambiguate inflections of different lexemes that have the same surface form but should be analyzed differently (Can and Manandhar, 2013). The second direction is removal of the assumption that a morphology consists only of concatenative processes. Introducing transformations to model allomorphy in a similar manner as Kohonen et al. (2009) would allow finding the shared abstract morphemes underlying different allomorphs. This could be especially beneficial in information retrieval and machine translation applications. Acknowledgments This research has been supported by European Community’s Seventh Framework Programme (FP7/2007–2013) under grant agreement n°287678 and the Academy of Finland under the Finnish Centre of Excellence"
C14-1111,W02-0603,0,0.767845,"in, a small amount of linguistic expertise is more easily available. A well-informed native speaker of a language can often identify the different prefixes, stems, and suffixes of words. Then the question is how many annotated words makes a difference. One answer was provided by Kohonen et al. (2010), who showed that already one hundred manually segmented words provide significant improvements to the quality of the output when comparing to a linguistic gold standard. The semi-supervised approach by Kohonen et al. (2010) was based on Morfessor Baseline, the simplest of the Morfessor methods by Creutz and Lagus (2002; 2007). The statistical model of Morfessor Baseline is simply a categorical distribution of morphs—a unigram model in the terms of statistical language modeling. As the semi-supervised Morfessor Baseline outperformed all unsupervised and semisupervised methods evaluated in the Morpho Challenge competitions (Kurimo et al., 2010a) so far, the next question is how the approach works for more complex models. Another popular variant of Morfessor, Categories-MAP (CatMAP) (Creutz and Lagus, 2005), models word formation using a hidden Markov model (HMM). The context-sensitivity of the model improves"
C14-1111,W04-0106,0,0.384366,"number of bits needed to encode both the model parameters and the training data. Equivalently, the cost function L can be derived from the Maximum a Posteriori (MAP) estimate: ( ) θˆ = arg max P(θ |D) = arg min − log P(θ) − log P(D |θ) = arg min L(θ, D), (1) θ θ θ where θ are the model parameters, D is the training corpus, P(θ) is the prior of the parameters and P(D |θ) is the data likelihood. In context-independent models such as Morfessor Baseline, the parameters include only the forms and probabilities of the morphs in the lexicon of the model. Morfessor Baseline and Categories-ML (CatML) (Creutz and Lagus, 2004) use a flat lexicon, in which the forms of the morphs are encoded directly as strings: each letter requires a certain number of bits to encode. Thus longer morphs are more expensive. Encoding a long morph is worthwhile only if the morph is referred to frequently enough from the words in the training data. If a certain string, let us say segmentation, is common enough in the training data, it is cost-effective to have it as a whole in the lexicon. Splitting it into two items, segment and ation, would double the number of pointers from the data, even if those morphs were already in the lexicon."
C14-1111,J11-2002,0,0.15398,"Missing"
C14-1111,W10-2210,1,0.474499,"al segmentation, i.e., finding morphs, the surface forms of the morphemes. For language processing applications, unsupervised learning of morphology can provide decentquality analyses without resources produced by human experts. However, while morphological analyzers and large annotated corpora may be expensive to obtain, a small amount of linguistic expertise is more easily available. A well-informed native speaker of a language can often identify the different prefixes, stems, and suffixes of words. Then the question is how many annotated words makes a difference. One answer was provided by Kohonen et al. (2010), who showed that already one hundred manually segmented words provide significant improvements to the quality of the output when comparing to a linguistic gold standard. The semi-supervised approach by Kohonen et al. (2010) was based on Morfessor Baseline, the simplest of the Morfessor methods by Creutz and Lagus (2002; 2007). The statistical model of Morfessor Baseline is simply a categorical distribution of morphs—a unigram model in the terms of statistical language modeling. As the semi-supervised Morfessor Baseline outperformed all unsupervised and semisupervised methods evaluated in the"
C14-1111,W10-2211,1,0.927355,"Missing"
C14-1111,W13-3504,1,0.683156,"annotations from the lexicon cannot be selected, as such an operation would have infinite cost. 3 Experiments We compare Morfessor FlatCat1 to two previous Morfessor methods and a fully supervised discriminative segmentation method. The Morfessor methods used as references are the CatMAP2 and Baseline3 implementations by Creutz and Lagus (2005) and Virpioja et al. (2013), respectively. Virpioja et al. (2013) implements the semi-supervised method described by Kohonen et al. (2010). For a supervised discriminative model, we use a character-level conditional random field (CRF) implementation by Ruokolainen et al. (2013)4 . We use the English, Finnish and Turkish data sets from Morpho Challenge 2010 (Kurimo et al., 2010b). They include large unannotated word lists, one thousand annotated words for training, 700– 800 annotated words for parameter tuning, and 10 × 1000 annotated words for testing. For evalution, we use the BPR score by Virpioja et al. (2011). The score calculates the precision (Pre), recall (Rec), and F1 -score (F) of the predicted morph boundaries compared to a linguistic gold standard. In the presence of alternative gold standard analyses, we weight each alternative equally. We also report th"
E09-1019,D07-1090,0,0.0294361,"Missing"
E09-1019,N03-2003,0,0.302301,"Missing"
E14-2006,W10-2210,1,0.551193,"for Computational Linguistics An extension of the Viterbi algorithm is used for decoding, that is, finding the optimal segmentations for new compound forms without changing the model parameters. given the observed training data D W : θ MAP = arg max p(θ)p(D W |θ) (1) θ Thus we are maximizing the product of the model prior p(θ) and the data likelihood p(D W |θ). As usual, the cost function to minimize is set as the minus logarithm of the product: 3 3.1 Semi-supervised extensions One important feature that has been implemented in Morfessor 2.0 are the semi-supervised extensions as introduced by Kohonen et al. (2010) Morfessor Baseline tends to undersegment when the model is trained for morphological segmentation using a large corpus (Creutz and Lagus, 2005b). Oversegmentation or undersegmentation of the method are easy to control heuristically by including a weight parameter α for the likelihood in the cost function. A low α increases the priors influence, favoring small construction lexicons, while a high value increases the data likelihood influence, favoring longer constructions. In semi-supervised Morfessor, the likelihood of an annotated data set is added to the cost function. As the amount of annot"
E14-2006,2007.mtsummit-papers.65,1,0.293402,"Missing"
E14-2006,W02-0603,0,0.0428573,"d. In addition to morphological segmentation, it can handle, for example, sentence chunking. To reflect this we use the following generic terms: The smallest unit that can be split will be an atom (letter). A compound (word) is a sequence of atoms. A construction (morph) is a sequence of atoms contained inside a compound. In the morphological segmentation task, the goal is to segment words into morphemes, the smallest meaning-carrying units. Morfessor is a family of methods for unsupervised morphological segmentation. The first version of Morfessor, called Morfessor Baseline, was developed by Creutz and Lagus (2002) its software implementation, Morfessor 1.0, released by Creutz and Lagus (2005b). A number of Morfessor variants have been developed later, including Morfessor Categories-MAP (Creutz and Lagus, 2005a) and Allomorfessor (Virpioja et al., 2010). Even though these algorithms improve Morfessor Baseline in some areas, the Baseline version has stayed popular as a generally applicable morphological analyzer (Spiegler et al., 2008; Monson et al., 2010). Over the past years, Morfessor has been used for a wide range of languages and applications. The applications include large vocabulary continuous spe"
E14-4017,W02-1001,0,0.687613,"CRFs (Lafferty et al., 2001). Formally, the linear-chain CRF model distribution for label sequence y = (y1 , y2 , . . . , yT ) and a word form x = (x1 , x2 , . . . , xT ) is written as a conditional probability p (y |x; w) ∝ T Y Leveraging Unannotated Data   exp w · φ(yt−1 , yt , x, t) , t=2 (1) where t indexes the character positions, w denotes the model parameter vector, and φ the vectorvalued feature extracting function. The model parameters w are estimated discrimatively based on a training set of exemplar input-output pairs (x, y) using, for example, the averaged perceptron algorithm (Collins, 2002). Subsequent to estimation, the CRF model segments test word forms using the Viterbi algorithm (Lafferty et al., 2001). We next describe the feature set |φ| {φi (yt−1 , yt , x, t)}i=1 by defining emission and transition features. Denoting the label set {B, M, S} as Y, the emission feature set is defined as t xt υ(t) 1 d 1 2 r 0 3 i 0 4 v 0 5 e 1 6 r 0 7 s 0 Now, given a set of U functions {υu (t)}U u=1 , we define variants of the emission features in (2) as {υu (x, t)χm (x, t)1(yt = yt0 ) | ∀u ∈ 1..U , ∀m ∈ 1..M , ∀yt0 ∈ Y} . {χm (x, t)1(yt = yt0 ) |m ∈ 1..M , ∀yt0 ∈ Y} , (2) where the indicat"
E14-4017,W10-2210,1,0.925267,"d the CRFs solely on the annotated data, without any use of the available unannotated data. In this work, we extend the CRF-based approach to leverage unannotated data in a straightforward and computationally efficient manner via feature set augmentation, utilizing predictions of unsupervised segmentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu,"
E14-4017,W02-0603,0,0.834941,"t setup, including data partitions and evaluation metrics, described by Ruokolainen et al. (2013). Table 1 shows the total number of instances available for model estimation and testing. 3.2 CRF Feature Extraction and Training The substring features included in the CRF model are described in Section 2.1. We include all substrings which occur in the training data. The Morfessor and Harris (successor and predecessor variety) features employed by the semi-supervised extension are described in Section 2.2. We experimented on two variants of the Morfessor algorithm, namely, the Morfessor Baseline (Creutz and Lagus, 2002) and Morfessor Categories-MAP (Creutz and Lagus, 2005), CatMAP for short. The Baseline models were trained on word types and the perplexity thresholds of the CatMAP models were set equivalently to the reference runs in Morpho Challenge 2010 (English: 450, Finnish: 250, Turkish: 100); otherwise the default parameters were used. The Harris features do not require any hyper-parameters. The CRF model (supervised and semisupervised) is trained using the averaged perceptron algorithm (Collins, 2002). The number of passes over the training set made by the perceptron algorithm, and the maximum length"
E14-4017,W13-3512,0,0.0407979,"field model via feature set augmentation. Experiments on three diverse languages show that this straightforward semi-supervised extension greatly improves the segmentation accuracy of the purely supervised CRFs in a computationally efficient manner. 1 Introduction We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. This type of morphological analysis can be useful for alleviating language model sparsity inherent to morphologically rich languages (Hirsimäki et al., 2006; Creutz et al., 2007; Turunen and Kurimo, 2011; Luong et al., 2013). Particularly, we focus on a low-resource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. We study morphological segmentation using conditional random fields (CRFs), a discriminative model for sequential tagging and segmentation (Lafferty et al., 2001). Recently, Ruokolainen et al. (2013) showed that the CRFs can yield competitive segmentation accuracy compared to more complex, previous state-of-theart techniques. While CRFs yielded generally 84 Proceedings of the 14th Conference"
E14-4017,P08-1099,0,0.0213946,"augmentation, utilizing predictions of unsupervised segmentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage"
E14-4017,W13-3505,0,0.0213219,"mentation algorithms. Experiments on three diverse languages show that the semi-supervised extension substantially improves the segmentation accuracy of the CRFs. The extension also provides higher accuracies on all the considered data set sizes and languages compared to the semi-supervised Morfessor (Kohonen et al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our ex"
E14-4017,N09-1024,0,0.0840226,"Missing"
E14-4017,W13-3504,1,0.903186,"(Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, that is the surface forms of morphemes. We extend a recent segmentation approach based on conditional random fields from purely supervised to semi-supervised learning by exploiting available unsupervised segmentation techniques. We integrate the unsupervised techniques into the conditional random field model vi"
E14-4017,D11-1090,0,0.0357625,"t al., 2010). In addition to feature set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, that is the surface forms of morphemes. We extend a recent segmentation approach based on conditional random fields from purely"
E14-4017,P10-1040,0,0.0563713,"ture set augmentation, there exists numerous approaches for semi-supervised CRF model estimation, exemplified by minimum entropy regularization (Jiao et al., 2006), generalized expectations criteria (Mann and McCallum, 2008), and posterior regularization (He et al., 2013). In this work, we employ the feature-based approach due to its simplicity and the availability of useful unsupervised segmentation methods. Varying feature set augmentation approaches have been successfully applied in several related tasks, such as Chinese word segmentation (Wang et al., 2011; Sun and Xu, 2011) and chunking (Turian et al., 2010). The paper is organized as follows. In Section 2, we describe the CRF-based morphological segmentation approach following (Ruokolainen et al., 2013), and then show how to extend this approach to leverage unannotated data in an efficient manner. Our experimental setup and results are discussed in Sections 3 and 4, respectively. Finally, We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, that is the surface forms of morphemes. We extend a recent segmentation approach based on conditional random fields from purely supervised to semi-supervised learn"
E14-4017,I11-1035,0,\N,Missing
E14-4017,P06-1027,0,\N,Missing
J16-1003,chrupala-etal-2008-learning,0,0.0617205,"Missing"
J16-1003,W02-1001,0,0.180815,"ly sparser statistics. Subsequent to defining the label set, one can learn a segmentation model using general sequence labeling methods, such as the well-known conditional random field (CRF) framework (Lafferty, McCallum, and Pereira 2001). Denoting the word form and the corresponding label sequence as x and y, respectively, the CRFs directly model the conditional probability of the segmentation given the word form, that is, p(y |x; w). The model parameters w are estimated discriminatively from the annotated data set D using iterative learning algorithms (Lafferty, McCallum, and Pereira 2001; Collins 2002). Subsequent to estimation, the CRF model segments word forms x by using maximum a posteriori (MAP) graph inference, that is, solving an optimization problem z = arg max p (u |x; w) (3) u using the standard Viterbi search (Lafferty, McCallum, and Pereira 2001). As it turns out, the CRF model can learn to segment words with a surprisingly high accuracy from a relatively small D, that is, without utilizing any of the available unannotated word forms U. Particularly, Ruokolainen et al. (2013) showed that it is sufficient to use simple left and right substring context features that are naturally a"
J16-1003,W02-0603,0,0.294185,"Missing"
J16-1003,N09-2019,1,0.529056,"g the LSV/LPV scores from unannotated data and, subsequently, tuning the necessary threshold values on the annotated data (Çöltekin 2010). On the other hand, one could also use the LSV/LPV values as features for a classification model, in which case the threshold values can be learned discriminatively based on the available annotated data. The latter approach is essentially realized in the event the LSV/PSV scores are provided for the CRF model discussed earlier (Ruokolainen et al. 2014). As for more recent work, we first refer to the generative log-linear model of Poon, Cherry, and Toutanova (2009). Similarly to the Morfessor model family, this approach is based on defining a joint probability distribution over the unannotated word forms U and the corresponding segmentations S. The distribution is log-linear in form and is denoted as p(U, S; θ ), where θ is the model parameter vector. Again, similarly to the Morfessor framework, Poon, Cherry, and Toutanova (2009) learn a morph lexicon that is subsequently used to generate segmentations for new word forms. The learning is controlled using prior distributions on both corpus and lexicon, which penalize exceedingly complex morph lexicon (si"
J16-1003,P12-1016,0,0.064292,"e suffix -s marks the plural number. Although this is a major simplification of the diverse morphological phenomena present in languages, this type of analysis has nevertheless been of substantial interest to computational linguistics, beginning with the pioneering work on morphological learning by Harris (1955). As for automatic language processing, such segmentations have been found useful in a wide range of applications, including speech recognition (Hirsimäki et al. 2006; Narasimhan et al. 2014), information retrieval (Turunen and Kurimo 2011), machine translation (de Gispert et al. 2009; Green and DeNero 2012), and word representation learning (Luong, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufficiently large set of unannotated words in electronic form. Consequently, such methods provide an inexpensive means of acquiring a type of morphological analysis for low-resource languages as motiv"
J16-1003,C14-1111,1,0.0715702,"the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards semisupervised learning (Poon, Cherry, and Toutanova 2009; Kohonen, Virpioja, and Lagus 2010; Sirts and Goldwater 2013; Grönroos et al. 2014; Ruokolainen et al. 2014). In general, the aim of semi-supervised learning is to acquire high-performing models utilizing both unannotated as well as annotated data (Zhu and Goldberg 2009). In morphological segmentation, the annotated data sets are commonly small, on the order of a few hundred word forms. We refer to this learning setting with such a small amount of supervision as minimally supervised learning. In consequence, similar to the unsupervised methods, the minimally supervised techniques can be seen as a means of acquiring a type of morphological analysis for under-resourced langua"
J16-1003,J11-2002,0,0.106899,"the conditional random field method (Ruokolainen et al. 2013, 2014). We hope the presented discussion and empirical evaluation will be of help for future research on the considered task. The rest of the article is organized as follows. In Section 2, we provide an overview of related studies. We then provide a literature survey of published morphological segmentation methodology in Section 3. Experimental work is presented in Section 4. Finally, we provide a discussion on potential directions for future work and conclusions on the current work in Sections 5 and 6, respectively. 2. Related Work Hammarström and Borin (2011) presented a literature survey on unsupervised learning of morphology, including methods for learning morphological segmentation. Whereas the discussion provided by Hammarström and Borin focuses mainly on linguistic aspects of morphology learning, our work is strongly rooted in machine learning methodology and empirical evaluation. In addition, whereas Hammarström and Borin focus entirely on unsupervised learning, our work considers a broader range of learning paradigms. Therefore, although related, Hammarström and Borin and our current presentation are complementary in that they have differen"
J16-1003,P06-1027,0,0.250174,"itting some of the training data can improve segmentation accuracy (Virpioja, Kohonen, and Lagus 2011; Sirts and Goldwater 2013). For discriminative models, the possibly most straightforward semi-supervised learning technique is adding features derived from the unlabeled data, as exemplified by the CRF approach of Ruokolainen et al. (2014). However, discriminative, semi-supervised learning is in general a much researched field with numerous diverse techniques (Zhu and Goldberg 2009). For example, merely for the CRF model alone, there exist several proposed semi-supervised learning approaches (Jiao et al. 2006; Mann and McCallum 2008; Wang et al. 2009). On Local Search. In what follows, we will discuss a potential pitfall of some algorithms that utilize local search procedures in the parameter estimation process, as exemplified by the Morfessor model family (Creutz et al. 2007). As discussed in Section 3.3.1, the Morfessor algorithm finds a local optimum of the objective function using a local search procedure. This complicates model development because if two model variants perform differently empirically, it is uncertain whether it is because of a truly better model or merely better fit with the"
J16-1003,N03-2015,0,0.135092,"Missing"
J16-1003,W08-0704,0,0.0342654,"xes, a stem, and zero or more suffixes. The actual forms of the morphs are learned from the data and, subsequent to learning, used to generate segmentations for new word forms. In this general approach, AGs are similar to the Morfessor family (Creutz and Lagus 2007). A major difference, however, is that the morphological grammar is not hard-coded but instead specified as an input to the algorithm. This allows different grammars to be explored in a flexible manner. Prior to the work by Sirts and Goldwater, the AGs were successfully applied in a related task of segmenting utterances into words (Johnson 2008; Johnson and Goldwater 2009; Johnson and Demuth 2010). The second major difference between the Morfessor family and the AG framework is the contrast between the MAP and fully Bayesian estimation approaches. Whereas the search procedure of the Morfessor method discussed earlier returns a single model corresponding to the MAP point-estimate, AGs instead operate with full posterior distributions over all possible models. Because acquiring the posteriors analytically is intractable, inference is performed utilizing Markov chain Monte Carlo algorithms to obtain samples from the posterior distribut"
J16-1003,C10-1060,0,0.021178,"e actual forms of the morphs are learned from the data and, subsequent to learning, used to generate segmentations for new word forms. In this general approach, AGs are similar to the Morfessor family (Creutz and Lagus 2007). A major difference, however, is that the morphological grammar is not hard-coded but instead specified as an input to the algorithm. This allows different grammars to be explored in a flexible manner. Prior to the work by Sirts and Goldwater, the AGs were successfully applied in a related task of segmenting utterances into words (Johnson 2008; Johnson and Goldwater 2009; Johnson and Demuth 2010). The second major difference between the Morfessor family and the AG framework is the contrast between the MAP and fully Bayesian estimation approaches. Whereas the search procedure of the Morfessor method discussed earlier returns a single model corresponding to the MAP point-estimate, AGs instead operate with full posterior distributions over all possible models. Because acquiring the posteriors analytically is intractable, inference is performed utilizing Markov chain Monte Carlo algorithms to obtain samples from the posterior distributions of interest (Johnson 2008; Johnson and Goldwater"
J16-1003,N09-1036,0,0.145586,"g the LSV/LPV scores from unannotated data and, subsequently, tuning the necessary threshold values on the annotated data (Çöltekin 2010). On the other hand, one could also use the LSV/LPV values as features for a classification model, in which case the threshold values can be learned discriminatively based on the available annotated data. The latter approach is essentially realized in the event the LSV/PSV scores are provided for the CRF model discussed earlier (Ruokolainen et al. 2014). As for more recent work, we first refer to the generative log-linear model of Poon, Cherry, and Toutanova (2009). Similarly to the Morfessor model family, this approach is based on defining a joint probability distribution over the unannotated word forms U and the corresponding segmentations S. The distribution is log-linear in form and is denoted as p(U, S; θ ), where θ is the model parameter vector. Again, similarly to the Morfessor framework, Poon, Cherry, and Toutanova (2009) learn a morph lexicon that is subsequently used to generate segmentations for new word forms. The learning is controlled using prior distributions on both corpus and lexicon, which penalize exceedingly complex morph lexicon (si"
J16-1003,N07-1018,0,0.0868534,"Missing"
J16-1003,W10-2210,1,0.938439,"Missing"
J16-1003,W11-0301,0,0.0579217,"Missing"
J16-1003,W13-3512,0,0.107681,"Missing"
J16-1003,P08-1099,0,0.0199259,"full analysis consists of word lemma (basic form), part-of-speech, and fine-grained labels. word form auto (car) autossa (in car) autoilta (from cars) autoilta (car evening) maantie (highway) sähköauto (electric car) full analysis segmentation auto+N+Sg+Nom auto+N+Sg+Ine auto+N+Pl+Abl auto+N+Sg+Nom+# ilta+N+Sg+Nom maantie+N+Sg+Nom maa+N+Sg+Gen+# tie+N+Sg+Nom sähköauto+N+Sg+Nom sähkö+N+Sg+Nom+# auto+N+Sg+Nom auto auto+ssa auto+i+lta auto+ilta maantie maa+n+tie sähköauto sähkö+auto word forms in Table 1, where the full analyses are provided by the rule-based OMorFi analyzer developed by Pirinen (2008). Note that it is typical for word forms to have alternative analyses and/or meanings that cannot be disambiguated without sentential context. Evidently, the level of detail in the full analysis is substantially higher compared with the segmentation, as it contains lemmatization as well as morphological tagging, whereas the segmentation consists of only segment boundary positions. Consequently, because of this simplification, morphological segmentation has been amenable to unsupervised machine learning methodology, beginning with the work of Harris (1955). Meanwhile, the majority of work on le"
J16-1003,D14-1095,0,0.116197,"lish word houses with a corresponding segmentation house+s, where the segment house corresponds to the word stem and the suffix -s marks the plural number. Although this is a major simplification of the diverse morphological phenomena present in languages, this type of analysis has nevertheless been of substantial interest to computational linguistics, beginning with the pioneering work on morphological learning by Harris (1955). As for automatic language processing, such segmentations have been found useful in a wide range of applications, including speech recognition (Hirsimäki et al. 2006; Narasimhan et al. 2014), information retrieval (Turunen and Kurimo 2011), machine translation (de Gispert et al. 2009; Green and DeNero 2012), and word representation learning (Luong, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufficiently large set of unannotated words in electronic form. Consequently, such"
J16-1003,W02-0604,0,0.0657041,"ng, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufficiently large set of unannotated words in electronic form. Consequently, such methods provide an inexpensive means of acquiring a type of morphological analysis for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning setting has received further popularity because of its close relationship with the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards"
J16-1003,N09-1024,0,0.0739183,"Missing"
J16-1003,C14-1015,0,0.128294,"morphological phenomena present in languages, this type of analysis has nevertheless been of substantial interest to computational linguistics, beginning with the pioneering work on morphological learning by Harris (1955). As for automatic language processing, such segmentations have been found useful in a wide range of applications, including speech recognition (Hirsimäki et al. 2006; Narasimhan et al. 2014), information retrieval (Turunen and Kurimo 2011), machine translation (de Gispert et al. 2009; Green and DeNero 2012), and word representation learning (Luong, Socher, and Manning 2013; Qiu et al. 2014). Since the early work of Harris (1955), most research on morphological segmentation has focused on unsupervised learning, which aims to learn the segmentation from a list of unannotated (unlabeled) word forms. The unsupervised methods are appealing as they can be applied to any language for which there exists a sufficiently large set of unannotated words in electronic form. Consequently, such methods provide an inexpensive means of acquiring a type of morphological analysis for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning"
J16-1003,W13-3504,1,0.0750803,"imally Supervised Morphological Segmentation unannotated data, supervised methods that utilize solely annotated data, and semisupervised approaches that utilize both unannotated and annotated data. Second, we perform an extensive empirical evaluation of three diverse method families, including a detailed error analysis. The approaches considered in this comparison are variants of the Morfessor algorithm (Creutz and Lagus 2002, 2005, 2007; Kohonen, Virpioja, and Lagus 2010; Grönroos et al. 2014), the adaptor grammar framework (Sirts and Goldwater 2013), and the conditional random field method (Ruokolainen et al. 2013, 2014). We hope the presented discussion and empirical evaluation will be of help for future research on the considered task. The rest of the article is organized as follows. In Section 2, we provide an overview of related studies. We then provide a literature survey of published morphological segmentation methodology in Section 3. Experimental work is presented in Section 4. Finally, we provide a discussion on potential directions for future work and conclusions on the current work in Sections 5 and 6, respectively. 2. Related Work Hammarström and Borin (2011) presented a literature survey o"
J16-1003,E14-4017,1,0.845779,"Missing"
J16-1003,N01-1024,0,0.0829055,"undary positions. Consequently, because of this simplification, morphological segmentation has been amenable to unsupervised machine learning methodology, beginning with the work of Harris (1955). Meanwhile, the majority of work on learning of full morphological analysis has used supervised methodology (Chrupala, Dinu, and van Genabith 2008). Lastly, there have been numerous studies on statistical learning of intermediate forms of segmentation and full analysis (Lignos 2010; Virpioja, Kohonen, and Lagus 2010) as well as alternative morphological representations (Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001; Neuvel and Fulop 2002; Johnson and Martin 2003). As for language processing, learning segmentation can be advantageous compared with learning full analyses. In particular, learning full analysis in a supervised manner typically requires up to tens of thousands of manually annotated sentences. A low-cost alternative, therefore, could be to learn morphological segmentation from unannotated word lists and a handful of annotated examples. Importantly, segmentation analysis has been found useful in a range of applications, such as speech recognition (Hirsimäki et al. 2006; Narasimhan et al. 2014)"
J16-1003,Q13-1021,1,0.0498248,"for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning setting has received further popularity because of its close relationship with the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards semisupervised learning (Poon, Cherry, and Toutanova 2009; Kohonen, Virpioja, and Lagus 2010; Sirts and Goldwater 2013; Grönroos et al. 2014; Ruokolainen et al. 2014). In general, the aim of semi-supervised learning is to acquire high-performing models utilizing both unannotated as well as annotated data (Zhu and Goldberg 2009). In morphological segmentation, the annotated data sets are commonly small, on the order of a few hundred word forms. We refer to this learning setting with such a small amount of supervision as minimally supervise"
J16-1003,E14-2006,1,0.895787,"Missing"
J16-1003,P10-1039,0,0.125197,"pensive means of acquiring a type of morphological analysis for low-resource languages as motivated, for example, by Creutz and Lagus (2002). The unsupervised approach and learning setting has received further popularity because of its close relationship with the unsupervised word segmentation problem, which has been viewed as a realistic setting for theoretical study of language acquisition (Brent 1999; Goldwater 2006). Although development of novel unsupervised model formulations has remained a topic of active research (Poon, Cherry, and Toutanova 2009; Monson, Hollingshead, and Roark 2010; Spiegler and Flach 2010; Lee, Haghighi, and Barzilay 2011; Sirts and Goldwater 2013), recent work has also shown a growing interest towards semisupervised learning (Poon, Cherry, and Toutanova 2009; Kohonen, Virpioja, and Lagus 2010; Sirts and Goldwater 2013; Grönroos et al. 2014; Ruokolainen et al. 2014). In general, the aim of semi-supervised learning is to acquire high-performing models utilizing both unannotated as well as annotated data (Zhu and Goldberg 2009). In morphological segmentation, the annotated data sets are commonly small, on the order of a few hundred word forms. We refer to this learning setting w"
J16-1003,P12-2063,0,0.0178246,"ally Supervised Morphological Segmentation the output of the supervised CRF model, which in some cases resulted in improved accuracy over the random initialization. We searched the optimal values for each experiment for the upweighting factor, cached versus non-cached root non-terminal, and random versus CRF initialization on the development set. An AG model is stochastic and each segmentation result is just a single sample from the posterior. A common approach in such a case is to take several samples and report the average result. Maximum marginal decoding (MMD) (Johnson and Goldwater 2009; Stallard et al. 2012) that constructs a marginal distribution from several independent samples and returns their mean value has been shown to improve the sampling-based models’ results about 1–2 percentage points. Although the AG model uses sampling for training, the MMD is not applicable here because during test time the segmentations are obtained using parsing. However, we propose another way of achieving the gain in a similar range to the MMD. We train five different models and concatenate their posterior grammars into a single joint grammar, which is then used as the final model to decode the test data. Our ex"
J16-1003,D11-1090,0,0.0618176,"Missing"
J16-1003,P10-1040,0,0.0598779,"Missing"
J16-1003,W11-4632,1,0.852352,"nce segmentations using boundary precision, boundary recall, and boundary F1-score. The boundary F1-score, or F1-score for short, equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect 2 Implementation is available at http://people.csail.mit.edu/yklee/code.html. 104 Ruokolainen et al. Minimally Supervised Morphological Segmentation to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries): Precision = C(correct) , C(proposed) (4) Recall = C(correct) . C(reference) (5) We follow Virpioja et al. (2011) and use type-based macro-averages. However, we handle word forms with alternative analyses in a different fashion. Instead of penalizing algorithms that propose an incorrect number of alternative analyses, we take the best match over the alternative reference analyses (separately for precision and recall). This is because all the methods considered in the experiments provide a single segmentation per word form. Throughout the experiments, we establish statistical significance with confidence level 0.95, according to the standard one-sided Wilcoxon signed-rank test performed on 10 random subse"
J16-1003,P00-1027,0,0.14363,"ion consists of only segment boundary positions. Consequently, because of this simplification, morphological segmentation has been amenable to unsupervised machine learning methodology, beginning with the work of Harris (1955). Meanwhile, the majority of work on learning of full morphological analysis has used supervised methodology (Chrupala, Dinu, and van Genabith 2008). Lastly, there have been numerous studies on statistical learning of intermediate forms of segmentation and full analysis (Lignos 2010; Virpioja, Kohonen, and Lagus 2010) as well as alternative morphological representations (Yarowsky and Wicentowski 2000; Schone and Jurafsky 2001; Neuvel and Fulop 2002; Johnson and Martin 2003). As for language processing, learning segmentation can be advantageous compared with learning full analyses. In particular, learning full analysis in a supervised manner typically requires up to tens of thousands of manually annotated sentences. A low-cost alternative, therefore, could be to learn morphological segmentation from unannotated word lists and a handful of annotated examples. Importantly, segmentation analysis has been found useful in a range of applications, such as speech recognition (Hirsimäki et al. 200"
J16-1003,I11-1035,0,\N,Missing
N09-2019,P08-1115,0,0.00950665,"tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn a preparatory committee of the whole of the general assembly is to be established at its fifty-second session Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration. 2007). Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morphosyntactic information. Gim´enez and M`arquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-English corpus, but only standard tokens are used in decoding. Dyer et al. (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation. From a modeling perspective their approach is unwieldy: multiple analyses of the parallel text collections are merged to create a large, heterogeneous training set; a single set of models and alignments is produced; lattice translation is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) Syst"
N09-2019,W05-0826,0,0.144023,"Missing"
N09-2019,J01-2001,0,0.0126074,"d words, sometimes consisting of several parts, such as ”ulko+maa+n+kauppa+politiikka” (foreign trade policy). Due to these properties, the number of different word forms that can be observed is enormous. Morfessor (Creutz and Lagus, 2007) is a method for modeling concatenative morphology in an unsupervised manner. It tries to find morpheme-like units, morphs, that are segments of the words. Inspired by the minimum description length principle, Morfessor tries to find a concise lexicon of morphs that can effectively code the words in the training data. Unlike other unsupervised methods (e.g., Goldsmith (2001)), there is no restrictions on how many morphs a word can have. After training the model, the most likely segmentation of new words to morphs can be found using the Viterbi algorithm. There exist a few different versions of Morfessor. The baseline algorithm has been found to be very useful in automatic speech recognition of agglutinative languages (Kurimo et al., 2006). However, it 2 Full MT08 results are available at http://www.nist.gov/ speech/tests/mt/2008/doc/mt08 official results v0.html 75 often oversegments morphemes that are rare or not seen at all in the training data. Following the a"
N09-2019,H05-1085,0,0.0116237,"ions to SMT in Section 1.1, but we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming worksho"
N09-2019,P05-1071,0,0.0578425,"m multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an alternative segmentation into subword units. This scheme generates more tokens as it segments all Arabic articles which other74 wise remain attached in the MADA D2 scheme (Table 1). Translation experiments are based on the NIST MT08 Arabic-to-English translation task, including all allowed parallel data as training material (∼150M English words, and 153M or 178M Arabic words"
N09-2019,N09-1049,1,0.3452,"Missing"
N09-2019,D07-1091,0,0.00845091,", June 2009. 2009 Association for Computational Linguistics Arabic MADA D2 SAKHR English wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp fY dwrthA AlvAnyp wAlxmsyn w+ qrrt &gt;n tn$A ljnp tHDyryp jAmEp l+ AljmEyp AlEAmp fy dwrthA AlvAnyp w+ Alxmsyn w+ qrrt An tn$A ljnp tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn a preparatory committee of the whole of the general assembly is to be established at its fifty-second session Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration. 2007). Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morphosyntactic information. Gim´enez and M`arquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-English corpus, but only standard tokens are used in decoding. Dyer et al. (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation. From a modeling perspective their approach is unwieldy: multiple analyses of the parallel text collections are merged to create a large, heterogeneous training set; a s"
N09-2019,N04-1022,1,0.409778,"erged to create a large, heterogeneous training set; a single set of models and alignments is produced; lattice translation is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) System Combination (Sim et al., 2007). Nbest lists from multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an altern"
N09-2019,N06-1062,1,0.865461,"Missing"
N09-2019,J04-2003,0,0.022079,"(Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 7"
N09-2019,popovic-ney-2004-towards,0,0.0232948,"etitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 73–76, c Boulder, Colorado, June 2009. 2009 Association f"
N09-2019,P07-1040,0,0.0246309,"is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) System Combination (Sim et al., 2007). Nbest lists from multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an alternative segmentation into subword units. This scheme generates more tokens as it segments all Arabic articles which other7"
N09-2019,P06-1122,0,0.013867,"t we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL"
N09-2019,E03-1007,0,0.0278612,"yzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 73–76, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Arabic MADA D2 SAKHR English wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp"
N09-2019,2007.mtsummit-papers.65,1,0.720881,"Missing"
N09-2019,N06-2051,0,0.0194137,"evaluated. We focus on applications to SMT in Section 1.1, but we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the eval"
N09-2019,N04-4015,0,\N,Missing
N09-2019,N06-2013,0,\N,Missing
N09-5004,N06-1062,1,\N,Missing
N09-5004,W02-0603,0,\N,Missing
N09-5004,N09-2019,1,\N,Missing
N09-5004,2007.mtsummit-papers.65,1,\N,Missing
N09-5004,2005.mtsummit-papers.11,0,\N,Missing
W10-1729,W02-0603,0,0.0427161,"stical natural language processing, especially with English, but morphologically rich languages can benefit from more fine-grained information. For instance, statistical morphs discovered with unsupervised methods result in better performance in automatic speech recognition for highly-inflecting and agglutinative languages (Hirsim¨aki et al., 2006; Kurimo et al., 2006). Virpioja et al. (2007) applied morph-based models in statistical machine translation (SMT) between several language pairs without gaining improvement in BLEU score, but obtaining re2.1 Morphological models for words Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007, etc.) is a family of methods for unsupervised morphological segmentation. Morfessor does not limit the number of morphemes for each word, making it suitable for agglutinative and compounding languages. An analysis of a single word is a list of non-overlapping segments, 195 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195–200, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics morphs, stored in the model lexicon. We use both the Morfessor Baseline (Creutz and Lagus, 2005b) and the Morfes"
W10-1729,2007.mtsummit-papers.65,1,0.906619,"Missing"
W10-1729,N09-2019,1,0.886268,"Missing"
W10-1729,D07-1091,0,0.138279,"Missing"
W10-1729,P07-2045,0,0.0141692,"rfessor algorithms. While translation models trained using the morphological decompositions did not improve the BLEU scores, we show that the Minimum Bayes Risk combination with a word-based translation model produces significant improvements for the Germanto-English translation. However, we did not see improvements for the Czech-toEnglish translations. 1 2 Methods In this work, morphological analyses are conducted on the source language data, and each different analysis is applied to create a unique segmentation of words into morphemes. Translation systems are trained with the Moses toolkit (Koehn et al., 2007) from each differently segmented version of the same source language to the target language. Evaluation with BLEU is performed on both the individual systems and system combinations, using different levels of decomposition. Introduction The effect of morphological variation in languages can be alleviated by using word analysis schemes, which may include morpheme discovery, part-ofspeech tagging, or other linguistic information. Words are very convenient and even efficient representation in statistical natural language processing, especially with English, but morphologically rich languages can"
W10-1729,N04-1022,0,0.0428466,"h the segmented source language, where the maximum sentence length is increased from 80 to 100 tokens to compensate for the larger number of tokens in text. 2.3 Morphological model combination For combining individual models, we apply Minimum Bayes Risk (MBR) system combination (Sim et al., 2007). N-best lists from multiple SMT systems trained with different morphological analysis methods are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). In this work, the focus of the system combination is not to combine different translation systems (e.g., Moses and Systran), but to combine systems trained with the same translation algorithm using the same source language data with with different morphological decompositions. 3 Experiments The German-to-English and Czech-to-English parts of the ACL WMT10 shared task data were investigated. Vanilla SMT models were trained with Moses using word tokens for MBR combination and comparison purposes. Several different morphological segmentation models for German and Czech were trained with Morfess"
W10-1729,N06-1062,1,\N,Missing
W10-2210,W02-0603,1,0.907613,"d Technology Adaptive Informatics Research Centre P.O. Box 15400, FI-00076 AALTO, Finland {oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fi Abstract In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very hi"
W10-2210,W04-0107,0,0.0153865,"formatics Research Centre P.O. Box 15400, FI-00076 AALTO, Finland {oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fi Abstract In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different a"
W10-2210,N09-1024,0,0.531506,"rpioja,krista.lagus}@tkk.fi Abstract In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may need a particular bias, for example, not analyzing frequent compound wo"
W10-2210,N07-1020,0,0.0172826,"400, FI-00076 AALTO, Finland {oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fi Abstract In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may need a particular bias,"
W10-2210,J01-2001,0,0.472643,"ool of Science and Technology Adaptive Informatics Research Centre P.O. Box 15400, FI-00076 AALTO, Finland {oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fi Abstract In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages t"
W10-2210,P08-1084,0,0.0301658,"nland {oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fi Abstract In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources. Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years. Especially, methods that perform morphological segmentation have been studied extensively (Goldsmith, 2001; Creutz and Lagus, 2002; Monson et al., 2004; Bernhard, 2006; Dasgupta and Ng, 2007; Snyder and Barzilay, 2008b; Poon et al., 2009). These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval (Creutz et al., 2007; Kurimo et al., 2008). While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses. For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount may be very high. Also, different applications may need a particular bias, for example, not analyzing"
W10-2210,C08-1128,0,0.0218119,"Missing"
W10-2211,W02-0603,1,0.860784,"ervised algorithms are almost at par and the differences are not significant. For German and Finnish, the best unsupervised methods can also beat in a statistically significant way the baseline of not doing any segmentation or stemming. The best algorithms that performed well across languages are ParaMor (Monson et al., 2008), Bernhard (Bernhard, 2006), Morfessor Baseline, and McNamee (McNamee, 2008). 3.2 Evaluated algorithms This section attempts to describe very briefly some of the individual morpheme analysis algorithms that have been most successful in the evaluations. Morfessor Baseline (Creutz and Lagus, 2002): This is a public baseline algorithm based on jointly minimizing the size of the morph codebook and the encoded size of the all the word forms using the minimum description length MDL cost function. The performance is above average for all evaluated tasks in most languages. Allomorfessor (Kohonen et al., 2009; Virpioja and Kohonen, 2009): The development of this method was based on the observation that the Comparing the results to the linguistic evaluation (section 3.1.1), it seems that methods that perform well at the IR task tend to have good precision in the linguistic task, with exception"
W10-2211,N09-2019,1,0.88494,"Missing"
W10-2211,P06-1027,0,0.0127801,"parisons to linguistic gold standard between various inflected word forms. Until 2010 the Morpho Challenge has been defined only as an unsupervised learning task. However, since small samples of morphologically labeled data can be provided already for quite many languages, also the semi-supervised learning task has become of interest. Moreover, while there exists a fair amount of research and now even books on semi-supervised learning (Zhu, 2005; Abney, 2007; Zhu, 2010), it has not been as widely studied for structured classification problems like sequence segmentation and labeling (cf. e.g. (Jiao et al., 2006)). The semi-supervised learning challenge introduced for Morpho Challenge 2010 can thus be viewed as an opportunity to strengthen research in both morphology modeling as well as in semi-supervised learning for sequence segmentation and labeling in general. 3 The first Morpho Challenge in 2005 (Kurimo et al., 2006) considered unsupervised segmentation of words into morphemes. The evaluation was based on comparing the segmentation boundaries given by the competitor’s algorithm to the boundaries obtained from a gold standard analysis. From 2007 onwards, the task was changed to full morpheme analy"
W11-4632,W04-0106,1,0.813336,"dels of word-internal structure. By definition, a probabilistic generative model describes the joint distribution of morphological analyses and word forms. An essential question is whether the morphological model represents types, that is, disregarding word frequencies in corpora, or tokens, i.e. fully appreciating the word frequencies. It has been observed that for the well-known Morfessor Baseline method (Creutz and Lagus, 2002; Creutz and Lagus, 2007), training on types leads to a large improvement in performance over tokens, when evaluating against a linguistic gold standard segmentation (Creutz and Lagus, 2004; Creutz and Lagus, 2005). A similar effect for a more recent method is reported by Poon et al. (2009). However, intuitively the corpus frequencies of words should be useful information for learning the morphology. In support of this intuition, behavioral studies regarding storage and processing of multi-morphemic word forms imply that the frequency of a word form plays a role in how it is stored in the brain: as a whole or as composed of its parts (Alegre and Gordon, 1999; Taft, 2004). In addition, the optimal morphological analysis may depend on the task to which the analysis is applied. In"
W11-4632,W10-2210,1,0.92952,"study the effect of the frequency information on the task of finding segmentations close to a linguistic gold standard. We use Morfessor Baseline, which is convenient due to its fast training algorithm. However, it has a property that causes it to arrive at fewer morphemes per words on average when the size of the training data grows (Creutz and Lagus, 2007). This phenomenon, which we refer to as undersegmentation, happens also when the model is trained on token counts rather than types, but it is not inherently related to the word frequency weighting in the class of models studied. Recently, Kohonen et al. (2010) showed how the amount of segmentation can be controlled by weighting the likelihood. In their semi-supervised setting, optimizing the weight improved the results considerably. This results in state of the art performance in Morpho Challenge 2010 (Kurimo et al., 2010a). In order to evaluate the effect of the frequency information without the problem of undersegmentation, we apply a similar likelihood weighting. Another potential use for frequencies is noise reduction. Corpora often contain misspelled word forms and foreign names, but they are likely to occur very infrequently and are therefore"
W11-4632,N09-1024,0,0.0749966,"ibution of morphological analyses and word forms. An essential question is whether the morphological model represents types, that is, disregarding word frequencies in corpora, or tokens, i.e. fully appreciating the word frequencies. It has been observed that for the well-known Morfessor Baseline method (Creutz and Lagus, 2002; Creutz and Lagus, 2007), training on types leads to a large improvement in performance over tokens, when evaluating against a linguistic gold standard segmentation (Creutz and Lagus, 2004; Creutz and Lagus, 2005). A similar effect for a more recent method is reported by Poon et al. (2009). However, intuitively the corpus frequencies of words should be useful information for learning the morphology. In support of this intuition, behavioral studies regarding storage and processing of multi-morphemic word forms imply that the frequency of a word form plays a role in how it is stored in the brain: as a whole or as composed of its parts (Alegre and Gordon, 1999; Taft, 2004). In addition, the optimal morphological analysis may depend on the task to which the analysis is applied. In Morpho Challenge evaluations (Kurimo et al., 2010b), the winners of the different tasks are often diff"
W11-4632,P08-1084,0,0.020923,"value is close to using only types, but emphasizes frequent words slightly. Their approach is elegant Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 230–237 Evaluating the effect of word frequencies in a probabilistic generative model of morphology but computationally demanding. In contrast, our method is based on transforming the observed frequencies with a deterministic function, and therefore can be performed as a quick preprocessing step for existing algorithms. Another intermediate option between types and tokens is given by Snyder and Barzilay (2008). Their morphological model generates bilingual phrases instead of words, and consequently, it is trained on aligned phrases that consist up to 4–6 words. The phrase frequencies are applied to discard phrases that occur less than five times, as they are likely to cause problems because of the noisy alignment. However, training is based on phrase types. Considering the frequencies of the words in this type of data, the common words will have more weight, but not as much as if direct corpus frequency was used. We study the effect of the frequency information on the task of finding segmentations"
W11-4632,W02-0603,1,\N,Missing
W13-3504,N07-1048,1,0.774648,"Missing"
W13-3504,P12-1016,0,0.0627695,"egmentation task as a structured classification problem by assigning each character to one of four classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), end of a multicharacter morph (E), single character morph (S)}. For example, consider the English word form The CRF model has been widely used in NLP segmentation tasks, such as shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), and word segmentation (Zhao et al., 2006). Recently, CRFs were also employed successfully in morphological segmentation for Arabic by Green and DeNero (2012) as a component of an English to Arabic machine translation system. While the segmentation method of Green and DeNero (2012) and ours is very similar, our focuses and contributions differ in several ways. First, while in our work we consider the low-resource learning setting, in which a small annotated data set is available (up to 3,130 word types), their model is trained on the Arabic Treebank (Maamouri et al., 2004) constituting several times larger training set (588,244 word tokens). Second, we present empirical comparison between the CRF approach and two state-of-art methods (Poon et al.,"
W13-3504,J11-2002,0,0.0292684,"based segmentation approach with two stateof-art methods, the log-linear modeling approach presented by Poon et al. (2009) and the semisupervised Morfessor algorithm (Kohonen et al., 2010). As stated previously, the CRF-based segmentation approach differs from these methods in that it learns to predict morph boundaries from a small amount of annotated data, in contrast to learning morph lexicons from both annotated and large amounts of unannotated data. Lastly, there exists ample work on varying unsupervised (and semi-supervised) morphological segmentation methods. A useful review is given by Hammarström and Borin (2011). The fundamental difference between our approach and these techniques is that our method necessarily requires manually annotated training data. Turkish using the Morpho Challenge 2009/2010 data sets (Kurimo et al., 2009; Kurimo et al., 2010). The results are compared against two stateof-art techniques, namely the log-linear modeling approach presented by Poon et al. (2009) and the semi-supervised Morfessor algorithm (Kohonen et al., 2010). We show that when employing the same small amount of annotated training data, the CRF-based boundary prediction approach outperforms these reference method"
W13-3504,W10-2210,1,0.64668,"rface forms morphs. Thus, morphs are natural targets for the segmentation. For most languages, existing resources contain large amounts of raw unannotated text data, only small amounts of manually prepared annotated training data, and no freely available rule-based morphological analyzers. The focus of our work is on performing morphological segmentation in this low-resource scenario. Given this setting, the current state-of-art methods approach the problem by learning morph lexicons from both annotated and unannotated data using semi-supervised machine learning techniques (Poon et al., 2009; Kohonen et al., 2010). Subsequent to model training, the methods uncover morph boundaries for new word forms by generating their most likely morph sequences according to the morph lexicons. In contrast to learning morph lexicons (Poon et al., 2009; Kohonen et al., 2010), we study morphological segmentation by learning to directly predict morph boundaries based on their local substring contexts. Specifically, we apply the linearchain conditional random field model, a popular discriminative log-linear model for segmentation presented originally by Lafferty et al. (2001). Importantly, we learn the segmentation model"
W13-3504,N03-1028,0,0.0276986,"ds In this section, we describe in detail the CRFbased approach for supervised morphological segmentation. 3.1 Related work Morphological segmentation as a classification task We represent the morphological segmentation task as a structured classification problem by assigning each character to one of four classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), end of a multicharacter morph (E), single character morph (S)}. For example, consider the English word form The CRF model has been widely used in NLP segmentation tasks, such as shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), and word segmentation (Zhao et al., 2006). Recently, CRFs were also employed successfully in morphological segmentation for Arabic by Green and DeNero (2012) as a component of an English to Arabic machine translation system. While the segmentation method of Green and DeNero (2012) and ours is very similar, our focuses and contributions differ in several ways. First, while in our work we consider the low-resource learning setting, in which a small annotated data set is available (up to 3,130 word types), their model is trained on the Arabic Tr"
W13-3504,W06-0127,0,0.0843718,"entation. 3.1 Related work Morphological segmentation as a classification task We represent the morphological segmentation task as a structured classification problem by assigning each character to one of four classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), end of a multicharacter morph (E), single character morph (S)}. For example, consider the English word form The CRF model has been widely used in NLP segmentation tasks, such as shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), and word segmentation (Zhao et al., 2006). Recently, CRFs were also employed successfully in morphological segmentation for Arabic by Green and DeNero (2012) as a component of an English to Arabic machine translation system. While the segmentation method of Green and DeNero (2012) and ours is very similar, our focuses and contributions differ in several ways. First, while in our work we consider the low-resource learning setting, in which a small annotated data set is available (up to 3,130 word types), their model is trained on the Arabic Treebank (Maamouri et al., 2004) constituting several times larger training set (588,244 word t"
W13-3504,P08-1099,0,0.0754272,"Missing"
W13-3504,W03-0430,0,0.0333757,"ased approach for supervised morphological segmentation. 3.1 Related work Morphological segmentation as a classification task We represent the morphological segmentation task as a structured classification problem by assigning each character to one of four classes, namely {beginning of a multi-character morph (B), middle of a multi-character morph (M), end of a multicharacter morph (E), single character morph (S)}. For example, consider the English word form The CRF model has been widely used in NLP segmentation tasks, such as shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), and word segmentation (Zhao et al., 2006). Recently, CRFs were also employed successfully in morphological segmentation for Arabic by Green and DeNero (2012) as a component of an English to Arabic machine translation system. While the segmentation method of Green and DeNero (2012) and ours is very similar, our focuses and contributions differ in several ways. First, while in our work we consider the low-resource learning setting, in which a small annotated data set is available (up to 3,130 word types), their model is trained on the Arabic Treebank (Maamouri et al., 2004) constituting severa"
W13-3504,C04-1081,0,0.0231624,"unction f . We will next describe and motivate the feature set used in the experiments. Our feature set consists of binary indicator functions describing the position t of word x using all left and right substrings up to a maximum length δ. For example, consider the problem of deciding if the letter e in the word drivers is preceded by a morph boundary. This decision is now based on the overlapping substrings 3.4 Parameter estimation The CRF model parameters w are estimated based on an annotated training data set. Common training criteria include the maximum likelihood (Lafferty et al., 2001; Peng et al., 2004; Zhao et al., 2006), averaged structured perceptron (Collins, 2002), and max-margin (Szummer et al., 2008). In this work, we estimate the parameters using the perceptron algorithm (Collins, 2002). 31 In perceptron training, the required graph inference can be efficiently performed using the standard Viterbi algorithm. Subsequent to training, the segmentations for test instances are acquired again using Viterbi search. Compared to other training criteria, the structured perceptron has the advantage of employing only a single hyperparameter, namely the number of passes over training data, makin"
W13-3504,N09-1024,0,0.52038,"phemes and their surface forms morphs. Thus, morphs are natural targets for the segmentation. For most languages, existing resources contain large amounts of raw unannotated text data, only small amounts of manually prepared annotated training data, and no freely available rule-based morphological analyzers. The focus of our work is on performing morphological segmentation in this low-resource scenario. Given this setting, the current state-of-art methods approach the problem by learning morph lexicons from both annotated and unannotated data using semi-supervised machine learning techniques (Poon et al., 2009; Kohonen et al., 2010). Subsequent to model training, the methods uncover morph boundaries for new word forms by generating their most likely morph sequences according to the morph lexicons. In contrast to learning morph lexicons (Poon et al., 2009; Kohonen et al., 2010), we study morphological segmentation by learning to directly predict morph boundaries based on their local substring contexts. Specifically, we apply the linearchain conditional random field model, a popular discriminative log-linear model for segmentation presented originally by Lafferty et al. (2001). Importantly, we learn"
W13-3504,W02-1001,0,\N,Missing
W15-3010,D09-1075,0,0.309766,"Missing"
W15-3010,P11-1004,0,0.114421,"Missing"
W15-3010,N09-2019,1,0.917433,"Missing"
W15-3010,fishel-kirik-2010-linguistically,0,0.559354,"Missing"
W15-3010,C14-1111,1,0.868026,"Missing"
W15-3010,N06-2013,0,0.176461,"Missing"
W15-3010,P07-2045,0,0.017174,"Missing"
W15-3010,2005.mtsummit-papers.11,0,0.103752,"Missing"
W15-3010,N04-4015,0,0.086757,"Missing"
W15-3010,W15-1011,0,0.0384613,"Missing"
W15-3010,P08-1084,0,0.0455317,"Missing"
W15-3010,W11-2129,0,0.503578,"Missing"
W15-3010,2007.mtsummit-papers.65,1,0.946664,"Missing"
W15-3010,J04-2003,0,0.0761711,"Missing"
W15-3010,P03-1021,0,0.184737,"Missing"
W15-3010,P02-1040,0,0.0977139,"Missing"
W15-3052,W14-3353,0,0.0233033,"Missing"
W15-3052,W05-0909,0,0.206348,"Missing"
W15-3052,W13-2202,0,0.0566841,"were used as inputs to the LeBLEU score as such: no preprocessing was performed on the texts. 3.1 system Source Target n δ n δ English English English English French German Czech Russian French German Czech Russian English English English English 4 3 2 2 3 4 4 4 0.7 0.2 0.3 0.3 0.6 0.5 0.5 0.5 4 4 4 2 4 4 4 4 0.4 0.2 0.3 0.2 0.6 0.4 0.7 0.3 Table 1: Results of parameter optimization for each language pair and level of evaluation (segment or system). Parameter tuning We tuned the two parameters of the evaluation score on the data sets published from the WMT 2013 and 2014 shared tasks (Mach´acˇ ek and Bojar, 2013; Mach´acˇ ek and Bojar, 2014). We ran a grid search on the parameters for each language and level. We tested four values of the maximum n-gram length n (from 1 to 4) and six values of the fuzzy match threshold δ (from 0.2 to 0.8 using step size 0.1). Our WMT 2015 submission includes two versions regarding the method parameters: “default” and “optimized”. For the default submission, we selected the parameters based on the smallest rank sum over all languages, data sets (2013/2014) and levels of evaluation (system/segment). These parameters, which we set as the default parameters for our implem"
W15-3052,W14-3336,0,0.0282528,"BLEU score as such: no preprocessing was performed on the texts. 3.1 system Source Target n δ n δ English English English English French German Czech Russian French German Czech Russian English English English English 4 3 2 2 3 4 4 4 0.7 0.2 0.3 0.3 0.6 0.5 0.5 0.5 4 4 4 2 4 4 4 4 0.4 0.2 0.3 0.2 0.6 0.4 0.7 0.3 Table 1: Results of parameter optimization for each language pair and level of evaluation (segment or system). Parameter tuning We tuned the two parameters of the evaluation score on the data sets published from the WMT 2013 and 2014 shared tasks (Mach´acˇ ek and Bojar, 2013; Mach´acˇ ek and Bojar, 2014). We ran a grid search on the parameters for each language and level. We tested four values of the maximum n-gram length n (from 1 to 4) and six values of the fuzzy match threshold δ (from 0.2 to 0.8 using step size 0.1). Our WMT 2015 submission includes two versions regarding the method parameters: “default” and “optimized”. For the default submission, we selected the parameters based on the smallest rank sum over all languages, data sets (2013/2014) and levels of evaluation (system/segment). These parameters, which we set as the default parameters for our implementation, are n = 4 and δ = 0."
W15-3052,W11-2105,0,0.260751,"man translation, producing “Arbeits Geberverband” from “employers’ organization” would give no hits if the reference had the compound “Arbeitgeberverband”. A common approach to the problem of inflected word forms—as well as to the simpler issues of uppercase letters and punctuation characters—is preprocessing. For example, METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011) uses a stemmer. Popovi´c (2011) applies and combines BLEU-style scores based on part-of-speech (POS) tags as well as morphemes induced by the unsupervised method by Creutz and Lagus (2005). Also the AMBER score by Chen and Kuhn (2011) combines many BLEU variants, and in some variants, the words are heuristically segmented. Our approach is to extend the BLEU metric to work better on morphologically complex languages without using any language-specific resources. Instead of giving one point for exactly same n-gram or zero points for any difference, we include “soft” or “fuzzy” hits for word n-grams based on letter edit distance. We call the score LeBLEU; this name can be interpreted either as “Letter-edit-BLEU” or “Levenshtein-BLEU”. LeBLEU has two main parameters, n-gram length and fuzzy match threshold, that are easy to tu"
W15-3052,P02-1040,0,0.0964726,"it Rate (TER) by Snover et al. (2006) uses a shift operation that moves a contiguous sequence of words to another location, as well as a greedy search algorithm to find the minimum distance. Stanford Probabilistic Edit Distance Evaluation (SPEDE) by Wang and Manning (2012) applies a probabilistic push-down automaton that captures nonnested, limited distance word swapping. A different approach to avoid the requirement of exactly same word order in the hypothesis and reference translations is to concentrate on comparing only small parts of the full texts. For example, the popular BLEU metric by Papineni et al. (2002) considers only local ordering of words. To be precise, it calculates the geometric mean precision of the n-grams of length between one and four. As high precision is easy to obtain by providing a very short hypothesis translation, hypotheses that are shorter than the reference are penalized by a brevity penalty. BLEU, TER and many other word-based methods assume that a single word (or n-gram) is either correct or incorrect, nothing in between. This This paper describes the LeBLEU evaluation score for machine translation, submitted to WMT15 Metrics Shared Task. LeBLEU extends the popular BLEU"
W15-3052,W11-2110,0,0.043888,"Missing"
W15-3052,W11-2107,0,0.0511212,"Missing"
W15-3052,2006.amta-papers.25,0,0.0708234,"languages in which the grammatical roles are marked by morphology and not the word order, there may be many more options. An edit distance measure suitable for machine translation would require move operations. However, such measures are computationally very expensive: finding the minimum edit distance with moves is NP-hard (Shapira and Storer, 2002), making it cumbersome for evaluation and unsuitable for automatic tuning of the translation models. Possible solutions include limiting the move operations or searching only for an approximate solution. For example, Translation Edit Rate (TER) by Snover et al. (2006) uses a shift operation that moves a contiguous sequence of words to another location, as well as a greedy search algorithm to find the minimum distance. Stanford Probabilistic Edit Distance Evaluation (SPEDE) by Wang and Manning (2012) applies a probabilistic push-down automaton that captures nonnested, limited distance word swapping. A different approach to avoid the requirement of exactly same word order in the hypothesis and reference translations is to concentrate on comparing only small parts of the full texts. For example, the popular BLEU metric by Papineni et al. (2002) considers only"
W15-3052,I05-2014,0,0.124021,"Missing"
W15-3052,A94-1016,0,0.114871,"arison between longer chunks of text. The results on WMT data sets show that fuzzy n-gram matching improves correlations to human evaluation especially for highly compounding languages. 1 Introduction The quality of machine translation has improved to the level that the translation hypotheses are useful starting points for human translators for almost any language pair. In the post-editing task, the ultimate way to evaluate the machine translation quality is to measure the editing time. Editing times are naturally related to the number and types of the edits—and thus the number of keystrokes (Frederking and Nirenburg, 1994)— the post-editor needs to get the final translation from the hypothesis. If we compare the raw translation hypothesis and its post-edited version, an appropriate edit distance measure should correlate to the edit time. However, implementing such a measure is far from trivial. In automatic speech recognition, common evaluation measures are Word Error Rate (WER) and Letter Error Rate (LER) that are based on the Levenshtein edit distance (Levenshtein, 1966). LER is more reasonable measure than WER for morphologically complex languages, in which the same word can occur in many inflected and deriv"
W15-3052,W12-3107,0,0.0170167,"are computationally very expensive: finding the minimum edit distance with moves is NP-hard (Shapira and Storer, 2002), making it cumbersome for evaluation and unsuitable for automatic tuning of the translation models. Possible solutions include limiting the move operations or searching only for an approximate solution. For example, Translation Edit Rate (TER) by Snover et al. (2006) uses a shift operation that moves a contiguous sequence of words to another location, as well as a greedy search algorithm to find the minimum distance. Stanford Probabilistic Edit Distance Evaluation (SPEDE) by Wang and Manning (2012) applies a probabilistic push-down automaton that captures nonnested, limited distance word swapping. A different approach to avoid the requirement of exactly same word order in the hypothesis and reference translations is to concentrate on comparing only small parts of the full texts. For example, the popular BLEU metric by Papineni et al. (2002) considers only local ordering of words. To be precise, it calculates the geometric mean precision of the n-grams of length between one and four. As high precision is easy to obtain by providing a very short hypothesis translation, hypotheses that are"
W15-3052,W09-0403,0,0.0369885,"Missing"
W16-2312,W15-3010,1,0.811033,"anguage model and the translation phrases to cover the same span of text. An unsupervised morphological segmentation may alleviate these problems. A method based on optimizing the training data likelihood, such as Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007; Virpioja et al., 2013), ensures that common phenomena are modeled more accurately, for example by using full forms for highly-frequent words even if they consist of multiple morphemes. Data-driven methods also allow tuning the segmentation granularity, for example based on symmetry between the languages in a parallel corpus (Grönroos et al., 2015). To combine the advantages of linguistic segmentation and data-driven segmentation, we propose a hybrid approach for morphological segmentation. We optimize the segmentation in a datadriven manner, aiming for a similar granularity as the second language of the language pair, but restricting the possible set of segmentation boundaries to those between linguistic morphs. That is, the segmentation method may decide to join any of the linguistic morphs, but it cannot add new segmentation boundaries to known linguistic morphs. We show that it is possible to improve on the linguistically accurate s"
W16-2312,E14-1061,0,0.0171357,"ja et al., 2007; Fishel and Kirik, 2010; Luong et al., 2010), and also the use of system combination to combine their strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging compounds when translating into Germanic languages, and provide an extensive reading list on the topic. 2 2. Rescoring n-best lists with TheanoLM (Enarvi and Kurimo, 2016). 3. Correcting boundary markings with postprocessing predictor. Our system extends the phrase-based SMT system Moses (Koehn et al., 2007) to perform segmented translation, by adding pre-processing and post-processing steps, with no changes to the decoder."
W16-2312,N06-2013,0,0.110633,"Missing"
W16-2312,P07-2045,0,0.00885311,"ompounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging compounds when translating into Germanic languages, and provide an extensive reading list on the topic. 2 2. Rescoring n-best lists with TheanoLM (Enarvi and Kurimo, 2016). 3. Correcting boundary markings with postprocessing predictor. Our system extends the phrase-based SMT system Moses (Koehn et al., 2007) to perform segmented translation, by adding pre-processing and post-processing steps, with no changes to the decoder. The standard pre-processing steps not specified in Figure 1 consist of normalization of punctuation, tokenization, and statistical truecasing. All of these were performed with the tools included in Moses. The pre-processing steps are followed by morphological segmentation. In addition, the parallel data was cleaned and duplicate sentences were removed. Cleaning was performed after morphological segmentation, as the segmentation can increase the length in tokens of a sentence."
W16-2312,D09-1075,0,0.0206514,"Segmentation for Phrase-Based Machine Translation Stig-Arne Grönroos Sami Virpioja Department of Signal Processing and Acoustics Department of Computer Science Aalto University, Finland Aalto University, Finland stig-arne.gronroos@aalto.fi sami.virpioja@aalto.fi Mikko Kurimo Department of Signal Processing and Acoustics Aalto University, Finland mikko.kurimo@aalto.fi Abstract NLP applications than under-segmentation (Virpioja et al., 2011). In the case of SMT, linguistic morphs may provide too high granularity compared to the second language, and deteriorate alignment (Habash and Sadat, 2006; Chung and Gildea, 2009; Clifton and Sarkar, 2011). Moreover, longer sequences of units are needed in the language model and the translation phrases to cover the same span of text. An unsupervised morphological segmentation may alleviate these problems. A method based on optimizing the training data likelihood, such as Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007; Virpioja et al., 2013), ensures that common phenomena are modeled more accurately, for example by using full forms for highly-frequent words even if they consist of multiple morphemes. Data-driven methods also allow tuning the segmentation gra"
W16-2312,D10-1015,0,0.0204143,"preprocess postprocess Designer objects Design-esineitä predict boundaries 3 1 me +i +llä on ol +ta +va . design-esineitä tune ORM rejoin with Morfessor mei +llä on oltava . train Moses parallel dev set train n-gram LMs resegmentation model train TheanoLM 2 TheanoLM model Moses model tune Moses 1-best list rescore design esine +itä translate n-best list design esine +itä Figure 1: A pipeline overview of training of the system and using it for translation. Main contributions are hilighted with numbers 1-3. ORM is short for Omorfi-restricted Morfessor. oja et al., 2007; Fishel and Kirik, 2010; Luong et al., 2010), and also the use of system combination to combine their strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect"
W16-2312,P11-1004,0,0.0204223,"Based Machine Translation Stig-Arne Grönroos Sami Virpioja Department of Signal Processing and Acoustics Department of Computer Science Aalto University, Finland Aalto University, Finland stig-arne.gronroos@aalto.fi sami.virpioja@aalto.fi Mikko Kurimo Department of Signal Processing and Acoustics Aalto University, Finland mikko.kurimo@aalto.fi Abstract NLP applications than under-segmentation (Virpioja et al., 2011). In the case of SMT, linguistic morphs may provide too high granularity compared to the second language, and deteriorate alignment (Habash and Sadat, 2006; Chung and Gildea, 2009; Clifton and Sarkar, 2011). Moreover, longer sequences of units are needed in the language model and the translation phrases to cover the same span of text. An unsupervised morphological segmentation may alleviate these problems. A method based on optimizing the training data likelihood, such as Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007; Virpioja et al., 2013), ensures that common phenomena are modeled more accurately, for example by using full forms for highly-frequent words even if they consist of multiple morphemes. Data-driven methods also allow tuning the segmentation granularity, for example based"
W16-2312,K15-1017,0,0.020718,"on is detrimental, however, as longer windows of history need to be used, and useful phrases become more difficult to extract. It is therefore important to find a balance in the amount of segmentation. We consider the case that there are linguistic gold standard segmentations available for the morphologically complex target language. Even if there is no rule-based morphological analyzer for the language, a limited set of gold standard segmentations can be used for training a reasonably accurate statistical segmentation model in a supervised or semi-supervised manner (Ruokolainen et al., 2014; Cotterell et al., 2015). While using a linguistically accurate morphological segmentation in a phrase-based SMT system may sound like a good idea, there is evidence that shows otherwise. In general, oversegmentation seems to be a larger problem for 1.1 Related work Rule-based and statistical segmentation for SMT have been extensively studied in isolation (Virpi289 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 289–295, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics parallel en Test Train . monolingual fi fi preprocess we must hav"
W16-2312,P03-1021,0,0.0942991,"raining. The TheanoLM parameters were: 100 nodes in the projection layer, 300 LSTM nodes in the hidden layer, dropout rate 0.25, adam optimization with initial learning rate 0.01, and minibatch 16. 2.4 Moses configuration We used GIZA++ alignment. As decoding LMs, we used two SRILM n-gram models with modified-KN smoothing: a 3-gram and 5-gram model, trained from different data. Many Moses settings were left at their default values: phrase length 10, grow-diag-final-and alignment symmetrization, msd-bidirectional-fe reordering, and distortion limit 6. The feature weights were tuned using MERT (Och, 2003), with BLEU (Papineni et al., 2002) of the post-processed hypothesis against a development set as the metric. 20 random restarts per MERT iteration were used, with iterations repeated until convergence. The rescoring weights were tuned with a newly included script in Moses, which uses kb-MIRA instead of MERT. 2.3 Morph boundary correction One benefit of segmented translation is the ability to generate new compounds and inflections, that were not seen in the training data. However, the ability can also lead to errors, e.g when an English word frequently aligned to a compound modifier is transla"
W16-2312,W02-0603,0,0.0744088,"versity, Finland mikko.kurimo@aalto.fi Abstract NLP applications than under-segmentation (Virpioja et al., 2011). In the case of SMT, linguistic morphs may provide too high granularity compared to the second language, and deteriorate alignment (Habash and Sadat, 2006; Chung and Gildea, 2009; Clifton and Sarkar, 2011). Moreover, longer sequences of units are needed in the language model and the translation phrases to cover the same span of text. An unsupervised morphological segmentation may alleviate these problems. A method based on optimizing the training data likelihood, such as Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007; Virpioja et al., 2013), ensures that common phenomena are modeled more accurately, for example by using full forms for highly-frequent words even if they consist of multiple morphemes. Data-driven methods also allow tuning the segmentation granularity, for example based on symmetry between the languages in a parallel corpus (Grönroos et al., 2015). To combine the advantages of linguistic segmentation and data-driven segmentation, we propose a hybrid approach for morphological segmentation. We optimize the segmentation in a datadriven manner, aiming for a similar granu"
W16-2312,P02-1040,0,0.107131,"arameters were: 100 nodes in the projection layer, 300 LSTM nodes in the hidden layer, dropout rate 0.25, adam optimization with initial learning rate 0.01, and minibatch 16. 2.4 Moses configuration We used GIZA++ alignment. As decoding LMs, we used two SRILM n-gram models with modified-KN smoothing: a 3-gram and 5-gram model, trained from different data. Many Moses settings were left at their default values: phrase length 10, grow-diag-final-and alignment symmetrization, msd-bidirectional-fe reordering, and distortion limit 6. The feature weights were tuned using MERT (Och, 2003), with BLEU (Papineni et al., 2002) of the post-processed hypothesis against a development set as the metric. 20 random restarts per MERT iteration were used, with iterations repeated until convergence. The rescoring weights were tuned with a newly included script in Moses, which uses kb-MIRA instead of MERT. 2.3 Morph boundary correction One benefit of segmented translation is the ability to generate new compounds and inflections, that were not seen in the training data. However, the ability can also lead to errors, e.g when an English word frequently aligned to a compound modifier is translated using such a morph, even though"
W16-2312,N09-2019,1,0.692107,"Missing"
W16-2312,W15-1844,0,0.0230193,"terwards. Additive smoothing with smoothing constant 1.0 was applied in the Viterbi search. Prior to the Viterbi training, we flattened the tree structure so that the root nodes (word forms) link directly to the leaf nodes (morphs), thus removing any shared substrings nodes that are not actual morphs. This way all word forms are segmented independently and all the restrictions are followed. Morphological segmentation An example of the morphological segmentation is shown in Table 1. 2.1.1 Omorfi segmentation We begin the morphological segmentation by applying the segmentation tool from Omorfi (Pirinen, 2015). Hyphens removed by Omorfi are reintroduced. Omorfi outputs 5 types of intra-word boundaries, which we mark in different ways. Compound modifiers, identified by the WB or wB boundary type, are marked with a reserved symbol ‘@’ at the right edge of the morph. Suffixes, identified by a leading morph boundary MB or derivation boundary DB, are marked with a ‘+’ at the left edge. Boundaries of the type STUB (other stemmer-type boundary) are removed. This marking scheme leaves the compound head, or last stem of the word, unmarked. E.g. “yli{WB}voimai{STUB}s{MB}i{MB}a” is marked as ”yli@ voimais +i"
W16-2312,W15-3022,0,0.0200185,"rejoin with Morfessor mei +llä on oltava . train Moses parallel dev set train n-gram LMs resegmentation model train TheanoLM 2 TheanoLM model Moses model tune Moses 1-best list rescore design esine +itä translate n-best list design esine +itä Figure 1: A pipeline overview of training of the system and using it for translation. Main contributions are hilighted with numbers 1-3. ORM is short for Omorfi-restricted Morfessor. oja et al., 2007; Fishel and Kirik, 2010; Luong et al., 2010), and also the use of system combination to combine their strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging c"
W16-2312,E14-4017,1,0.775696,"sing step. Over-segmentation is detrimental, however, as longer windows of history need to be used, and useful phrases become more difficult to extract. It is therefore important to find a balance in the amount of segmentation. We consider the case that there are linguistic gold standard segmentations available for the morphologically complex target language. Even if there is no rule-based morphological analyzer for the language, a limited set of gold standard segmentations can be used for training a reasonably accurate statistical segmentation model in a supervised or semi-supervised manner (Ruokolainen et al., 2014; Cotterell et al., 2015). While using a linguistically accurate morphological segmentation in a phrase-based SMT system may sound like a good idea, there is evidence that shows otherwise. In general, oversegmentation seems to be a larger problem for 1.1 Related work Rule-based and statistical segmentation for SMT have been extensively studied in isolation (Virpi289 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 289–295, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics parallel en Test Train . monolingual fi"
W16-2312,W11-2129,0,0.0172865,"oLM model Moses model tune Moses 1-best list rescore design esine +itä translate n-best list design esine +itä Figure 1: A pipeline overview of training of the system and using it for translation. Main contributions are hilighted with numbers 1-3. ORM is short for Omorfi-restricted Morfessor. oja et al., 2007; Fishel and Kirik, 2010; Luong et al., 2010), and also the use of system combination to combine their strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging compounds when translating into Germanic languages, and provide an extensive reading list on the topic. 2 2. Rescoring n-best lists with Thea"
W16-2312,J13-4009,0,0.018542,"ir strengths has been examined (De Gispert et al., 2009; Rubino et al., 2015; Pirinen et al., 2016). Prediction of morph boundary types has been used in conjunction with compound splitting. Stymne and Cancedda (2011) apply rule-based compound splitting in the pre-processing stage, and a conditional random field with rich linguistic features for generating novel compounds in postprocessing. Coalescence of compound parts in the translation output is promoted using POS-tag features. Cap et al. (2014) extend the post-predictor to also inflect the compound modifiers e.g. to add a linking morpheme. Stymne et al. (2013) investigate several methods for splitting and merging compounds when translating into Germanic languages, and provide an extensive reading list on the topic. 2 2. Rescoring n-best lists with TheanoLM (Enarvi and Kurimo, 2016). 3. Correcting boundary markings with postprocessing predictor. Our system extends the phrase-based SMT system Moses (Koehn et al., 2007) to perform segmented translation, by adding pre-processing and post-processing steps, with no changes to the decoder. The standard pre-processing steps not specified in Figure 1 consist of normalization of punctuation, tokenization, an"
W16-2312,2007.mtsummit-papers.65,1,0.841019,"Missing"
W17-4727,W16-2341,0,0.168454,"Missing"
W17-4727,P17-2012,0,0.0262831,"decomposed generation process, in which they first output lemma, PoS, tense, person, gender, and number, from which the surface form is generated using a rule-based morphological analyzer. Neural machine translation provides another way to utilize external annotations, multi-task learning (MTL). MTL is a well established machine learning approach that aims at improving the generalization performance of a task using other related tasks (Caruana, 1998). For example, Luong et al. (2016) use autoencoding, parsing, and caption generation as auxiliary tasks to improve English-to-German translation. Eriguchi et al. (2017) combine NMT with a Recurrent Neural Network Grammar. The system learns to parse the target language as an auxiliary task when translating into English. We propose an MTL approach inspired by fac296 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 296–302 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tored translation. The output of a morphological analyzer for the target sentence is used as an auxiliary prediction target, while sharing network parameters to a larger extent than in the approach of Luong"
W17-4727,W16-2209,0,0.0420161,"t words as they are, while replacing infrequent words with a special <UNK&gt; symbol. A second character-level decoder then expands these <UNK&gt; symbols into surface forms. In addition to providing moderate length of input and output sequences together with an open vocabulary, the hybrid word-character decoder makes it simple to use labels based on the level of words, provided for example by morphological analyzers and parsers. In SMT, such tools are typically used via factored translation models (Koehn and Hoang, 2007). Factored translation has also been successfully applied in NMT. For example, Sennrich and Haddow (2016) augment the source words with four additional factors: PoS, lemma, dependency label and subwords. García-Martínez et al. (2016) use a decomposed generation process, in which they first output lemma, PoS, tense, person, gender, and number, from which the surface form is generated using a rule-based morphological analyzer. Neural machine translation provides another way to utilize external annotations, multi-task learning (MTL). MTL is a well established machine learning approach that aims at improving the generalization performance of a task using other related tasks (Caruana, 1998). For examp"
W17-4727,2007.mtsummit-papers.65,1,0.697651,"mpounding in synthetic languages can result in very large vocabularies. In statistical machine translation (SMT) large vocabularies cause sparsity issues. While continuous space representations make neural machine translation (NMT) more robust towards such sparsity, it suffers from a different set of problems related to large vocabularies. A large vocabulary bloats memory and computation requirements, while still leaving the problem of out-ofvocabulary words unsolved. Subword vocabularies have been proposed as a solution. While the benefits of using subwords in SMT have been at best moderate (Virpioja et al., 2007; Fishel and Kirik, 2010; Grönroos et al., 2015), subword decoding has become popular in NMT (Sennrich et al., 2015). A subword vocabulary of a moderate size ensures full coverage of an open vocabulary. The downside is an increase in the length of the input and output sequences. Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism. An alternative approach is the hybrid wordcharacter decoder presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infreq"
W17-4727,fishel-kirik-2010-linguistically,0,0.0154268,"languages can result in very large vocabularies. In statistical machine translation (SMT) large vocabularies cause sparsity issues. While continuous space representations make neural machine translation (NMT) more robust towards such sparsity, it suffers from a different set of problems related to large vocabularies. A large vocabulary bloats memory and computation requirements, while still leaving the problem of out-ofvocabulary words unsolved. Subword vocabularies have been proposed as a solution. While the benefits of using subwords in SMT have been at best moderate (Virpioja et al., 2007; Fishel and Kirik, 2010; Grönroos et al., 2015), subword decoding has become popular in NMT (Sennrich et al., 2015). A subword vocabulary of a moderate size ensures full coverage of an open vocabulary. The downside is an increase in the length of the input and output sequences. Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism. An alternative approach is the hybrid wordcharacter decoder presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a specia"
W17-4727,W15-3010,1,0.843286,"very large vocabularies. In statistical machine translation (SMT) large vocabularies cause sparsity issues. While continuous space representations make neural machine translation (NMT) more robust towards such sparsity, it suffers from a different set of problems related to large vocabularies. A large vocabulary bloats memory and computation requirements, while still leaving the problem of out-ofvocabulary words unsolved. Subword vocabularies have been proposed as a solution. While the benefits of using subwords in SMT have been at best moderate (Virpioja et al., 2007; Fishel and Kirik, 2010; Grönroos et al., 2015), subword decoding has become popular in NMT (Sennrich et al., 2015). A subword vocabulary of a moderate size ensures full coverage of an open vocabulary. The downside is an increase in the length of the input and output sequences. Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism. An alternative approach is the hybrid wordcharacter decoder presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a special <UNK&gt; symbol. A second"
W17-4727,C14-1111,1,0.802379,"weight 0.8 for the character-level cost. We use an ensemble procedure, in which the combined prediction is computed as the mean after the softmax layer of the predictions of 4 models. The primary system uses systems from 4 runs with different weights for the auxiliary task. The systems trained for comparison—a subword system based on Morfessor FlatCat and the systems in ablation experiments—were ensembled using 4 save points from a single run. To include an example of subword NMT, we also submit our FlatCat system. As preprocessing, the target side has been segmented using Morfessor FlatCat (Grönroos et al., 2014), which was tuned to produce a subword lexicon of approximately 60k symbols. Segmenting names into characters is applied in addition to the FlatCat segmentation. The FlatCat segmented system uses WMT 2016 data only, i.e., omits the Rapid corpus. The FlatCat subword system uses the standard HNMT decoder. It uses neither the hybrid wordcharacter decoder nor MTL. We did however use the improved beam search with penalties. 6 Results We evaluate the systems using characterF with β set to 1.0 and 2.0, and cased BLEU using the mteval-v13a.pl script. We also include Translation Error Rate (TER) result"
W17-4727,D07-1091,0,0.0612052,"er presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a special <UNK&gt; symbol. A second character-level decoder then expands these <UNK&gt; symbols into surface forms. In addition to providing moderate length of input and output sequences together with an open vocabulary, the hybrid word-character decoder makes it simple to use labels based on the level of words, provided for example by morphological analyzers and parsers. In SMT, such tools are typically used via factored translation models (Koehn and Hoang, 2007). Factored translation has also been successfully applied in NMT. For example, Sennrich and Haddow (2016) augment the source words with four additional factors: PoS, lemma, dependency label and subwords. García-Martínez et al. (2016) use a decomposed generation process, in which they first output lemma, PoS, tense, person, gender, and number, from which the surface form is generated using a rule-based morphological analyzer. Neural machine translation provides another way to utilize external annotations, multi-task learning (MTL). MTL is a well established machine learning approach that aims a"
W17-4727,P16-1100,0,0.311859,"have been proposed as a solution. While the benefits of using subwords in SMT have been at best moderate (Virpioja et al., 2007; Fishel and Kirik, 2010; Grönroos et al., 2015), subword decoding has become popular in NMT (Sennrich et al., 2015). A subword vocabulary of a moderate size ensures full coverage of an open vocabulary. The downside is an increase in the length of the input and output sequences. Long sequences cause a large increase in computation time, especially for architectures using the attention mechanism. An alternative approach is the hybrid wordcharacter decoder presented by Luong and Manning (2016). In the hybrid decoder, a word level decoder outputs frequent words as they are, while replacing infrequent words with a special <UNK&gt; symbol. A second character-level decoder then expands these <UNK&gt; symbols into surface forms. In addition to providing moderate length of input and output sequences together with an open vocabulary, the hybrid word-character decoder makes it simple to use labels based on the level of words, provided for example by morphological analyzers and parsers. In SMT, such tools are typically used via factored translation models (Koehn and Hoang, 2007). Factored transla"
W18-0208,W02-0603,0,0.061433,"he work by Mohri et al. (2008). The advantage of WFST-based recognizers is that once the search network has been constructed and optimized effectively by the WFST methods, the decoding is very fast and accurate. Moreover, Kaldi’s GMM-HMMs are improved by subspace Gaussians, word-position-dependent phones and advanced silence models. 2.2 Subword lexicon FSTs and language models The small amount of training data and the morphological complexity of Northern Sámi make it problematic to build language models (LM) using words as the basic units. We applied the data-driven Morfessor Baseline method (Creutz and Lagus, 2002, 2007) to segment the words into subword units. Because all words in the language can be composed from these subword units, this approach provides an unlimited vocabulary for ASR (Hirsimäki et al., 2006). While Morfessor was developed to find units of language that resemble the surface forms of linguistic morphemes, the current implementation includes a parameter for adjusting the level of segmentation that the method produces (Virpioja et al., 2013). The optimal level of segmentation for ASR varies between languages, but a wide range of lexicon seems to produce near-optimal results (Smit et"
W18-6410,W16-3402,0,0.0285845,"Missing"
W18-6410,P17-1181,0,0.148958,"Missing"
W18-6410,P14-2017,0,0.0597286,"model using a compatible data set.1 1.1 Related work Improving segmentation through multilingual learning has been studied before. Snyder and Barzilay (2008) propose an unsupervised, Bayesian method, which only uses parallel phrases as training data. Wicentowski (2004) present a supervised method, which requires lemmatization. The method of Naradowsky 1 and Toutanova (2011) is also unsupervised, utilizing a hidden semi-Markov model, but it requires rich features on the input data. The subtask of cognate extraction has seen much research eﬀort (Mitkov et al., 2007; Bloodgood and Strauss, 2017; Ciobanu and Dinu, 2014). Most methods are supervised, and/or require rich features. There is also work on cognate identiﬁcation from historical linguistics perspective (Rama, 2016; Kondrak, 2009), where the aim is to classify which cognate candidates truly share an etymological origin. We propose a language-agnostic, unsupervised method, which doesn’t require annotations, lemmatizers, analyzers or parsers. Our method can exploit both monolingual and parallel data, and can use cognates of any part-ofspeech. 2 Cognate Morfessor We introduce a new variant of Morfessor for cross-lingual segmentation.2 It is trained usin"
W18-6410,W02-0603,0,0.226497,"Missing"
W18-6410,N13-1073,0,0.0410491,"s for both languages is tried, and the pair of splits minimizing the cost function is selected, unless not splitting results in even lower cost. 3 Extracting cognates from parallel data from the news.20{14-17}.et corpora, using a language model ﬁltering technique. Finnish–Estonian cognates were automatically extracted from the shared task training data. As we needed a Finnish–Estonian parallel data set, we generated one by triangulation from the English–Finnish and English– Estonian parallel data. This resulted in a set of 679 252 sentence pairs (ca 12 million tokens per language). FastAlign (Dyer et al., 2013) was used for word alignment in both directions, after which the alignments were symmetrized using the grow-diag-ﬁnal-and heuristic. All aligned word pairs were extracted based on the symmetrized alignment. Words containing punctuation, and pairs aligned to each other fewer than 2 times were removed. The list of word pairs was ﬁltered based on Levenshtein distance. If either of the words consisted of 4 or fewer characters, an exact match was required. Otherwise, a Levenshtein distance up to a third of the mean of the lengths, rounding up, was allowed. This procedure resulted in a list of 40 47"
W18-6410,P17-4012,0,0.160201,"Missing"
W18-6410,W10-2210,1,0.853542,"E , D E ). The coding is redundant, as one language and the edits would be enough to reconstruct the second language. In the interest of symmetry between target languages, we ignore this redundancy. The intuition is that the changes in spelling between the cognates in a particular language pair is regular. Coding the diﬀerences in a way that reduces the cost of making a similar change in another word guides the model towards learning these patterns from the data. The coding of the edits is based on the Levenshtein (1966) algorithm. Let (wa , wb ) be 388 The semi-supervised weighting scheme of Kohonen et al. (2010) can be applied to Cognate Morfessor. A new weighting parameter edit_cost_weight is added, and multiplicatively applied to both the lexicon and corpus costs of the edits. The training algorithm is an iterative greedy local search very similar to the Morfessor Baseline algorithm. The algorithm ﬁnds an approximately minimizing solution to Eq 2. The recursive splitting algorithm from Morfessor Baseline is slightly modiﬁed. If a non-cognate is being reanalyzed, the normal algorithm is followed. Cognates are reanalyzed together. Recursive splitting is applied, with the restriction that if a morph i"
W18-6410,N01-1014,0,0.368489,"that our approach improves the translation quality particularly for Estonian, which has less resources for training the translation model. 1 Introduction Cognates are words in diﬀerent languages, which due to a shared etymological origin are represented as identical or nearly identical strings, and also refer to the same or similar concepts. Ideally the cognate pair is similar orthographically, semantically, and distributionally. Care must be taken with “false friends”, i.e. words with similar string representation but diﬀerent semantics. Following usage in Natural Language Processing, e.g. (Kondrak, 2001), we use this broader deﬁnition of the term cognate, without placing the same weight on etymological origin as in historical linguistics. Therefore we accept loan words as cognates. In any language pair written in the same alphabet, cognates can be found among names of persons, locations and other proper names. Cognates are more frequent in related languages, such as Finnish and Estonian. These additional cognates are words of any part-ofspeech, which happen to have a shared origin. In this work we set out to improve morphological segmentation for multilingual translation systems with one sour"
W18-6410,P11-1090,0,0.0573184,"Missing"
W18-6410,W15-3049,0,0.0172576,"omponent. Newstest is abbreviated nt. Both references are used in nt2017AB. We experimented with partially linking the embeddings of cognate morphs. In this experiment, we used morph embeddings concatenated from two parts: a part consisting of normal embedding of the morph, and a part that was shared between both halves of the cognate morph pair. Non-cognate morphs used an unlinked embedding also for the second part. After concatenation, the linked embeddings have the same size as the baseline embeddings. We evaluate the systems with cased BLEU using the mteval-v13a.pl script, and characterF (Popovic, 2015) with β set to 1.0. The latter was used for tuning. 6 Results Based on preliminary experiments, the Morfessor corpus cost weight α was set to 0.01, and the edit cost weight was set to 10. The most frequent edits are shown in Table 2. Table 3 shows the development set results for Estonian. Table 4 shows results for previous year’s test sets for Finnish. The tables show our main system and the two baselines: a multilingual model using joint BPE segmentation, and a monolingual model using Morfessor Baseline. Cognate Morfessor outperforms the comparable BPE system according to both measures for Es"
W18-6410,C16-1097,0,0.0555437,"unsupervised, Bayesian method, which only uses parallel phrases as training data. Wicentowski (2004) present a supervised method, which requires lemmatization. The method of Naradowsky 1 and Toutanova (2011) is also unsupervised, utilizing a hidden semi-Markov model, but it requires rich features on the input data. The subtask of cognate extraction has seen much research eﬀort (Mitkov et al., 2007; Bloodgood and Strauss, 2017; Ciobanu and Dinu, 2014). Most methods are supervised, and/or require rich features. There is also work on cognate identiﬁcation from historical linguistics perspective (Rama, 2016; Kondrak, 2009), where the aim is to classify which cognate candidates truly share an etymological origin. We propose a language-agnostic, unsupervised method, which doesn’t require annotations, lemmatizers, analyzers or parsers. Our method can exploit both monolingual and parallel data, and can use cognates of any part-ofspeech. 2 Cognate Morfessor We introduce a new variant of Morfessor for cross-lingual segmentation.2 It is trained using a bilingual corpus, so that both target languages are trained simultaneously. We allow each language to have its own subword lexicon. In essence, as a Mor"
W18-6410,P08-1084,0,0.0269002,"occur in the ends of the words. If a single letter changes in the middle of a cognate, consistent subwords that span over the location of the change are found only by chance. In order to encourage stronger consistency, we propose a segmentation model that uses automatically extracted cognates and fuzzy matching between cognate morphs. In this work we also contribute two new features to the OpenNMT translation system: Ensemble decoding, and ﬁne-tuning a pretrained model using a compatible data set.1 1.1 Related work Improving segmentation through multilingual learning has been studied before. Snyder and Barzilay (2008) propose an unsupervised, Bayesian method, which only uses parallel phrases as training data. Wicentowski (2004) present a supervised method, which requires lemmatization. The method of Naradowsky 1 and Toutanova (2011) is also unsupervised, utilizing a hidden semi-Markov model, but it requires rich features on the input data. The subtask of cognate extraction has seen much research eﬀort (Mitkov et al., 2007; Bloodgood and Strauss, 2017; Ciobanu and Dinu, 2014). Most methods are supervised, and/or require rich features. There is also work on cognate identiﬁcation from historical linguistics p"
W18-6410,W04-0109,0,0.070992,"er the location of the change are found only by chance. In order to encourage stronger consistency, we propose a segmentation model that uses automatically extracted cognates and fuzzy matching between cognate morphs. In this work we also contribute two new features to the OpenNMT translation system: Ensemble decoding, and ﬁne-tuning a pretrained model using a compatible data set.1 1.1 Related work Improving segmentation through multilingual learning has been studied before. Snyder and Barzilay (2008) propose an unsupervised, Bayesian method, which only uses parallel phrases as training data. Wicentowski (2004) present a supervised method, which requires lemmatization. The method of Naradowsky 1 and Toutanova (2011) is also unsupervised, utilizing a hidden semi-Markov model, but it requires rich features on the input data. The subtask of cognate extraction has seen much research eﬀort (Mitkov et al., 2007; Bloodgood and Strauss, 2017; Ciobanu and Dinu, 2014). Most methods are supervised, and/or require rich features. There is also work on cognate identiﬁcation from historical linguistics perspective (Rama, 2016; Kondrak, 2009), where the aim is to classify which cognate candidates truly share an ety"
W19-0302,W16-2002,0,0.0310277,"processes that result in https://sites.google.com/view/deeplo18/home 16 allomorphs, i.e. different surface morphs corresponding to the same meaning. w 7→ y; w ∈ Σ∗ , y ∈ (Σ ∪ {◦})∗ e.g. achievability 7→ achieve ◦ able ◦ ity where Σ is the alphabet of the language, and ◦ is the boundary marker. Morphological analysis yields the lemma and tags representing the morphological properties of a word. w 7→ yt; w, y ∈ Σ∗ , t ∈ τ ∗ e.g. took 7→ take PAST where τ is the set of morphological tags. Two related morphological tasks are reinflection and lemmatization. In morphological reinflection (see e.g. Cotterell et al., 2016), one or more inflected forms are given to identify the lexeme, together with the tags identifying the desired inflection. The task is to produce the correctly inflected surface form of the lexeme. wt 7→ y; w, y ∈ Σ∗ , t ∈ τ ∗ e.g. taken PAST 7→ took In lemmatization, the input is an inflected form and the output is the lemma. w, y ∈ Σ∗ w 7→ y; e.g. better 7→ good Morphological surface segmentation can be formulated in the same way as canonical segmentation, by just allowing the mapping to canonical segments to be the identity. However, this formulation fails to capture the fact that the segme"
W19-0302,P12-1016,0,0.034327,"coder setting. The strings to be reconstructed can be actual words or even random noise. Surface segmentation can alternatively be formulated as structured classification w 7→ y; w ∈ Σk , y ∈ Ωk , k ∈ N e.g. uses 7→ BM ES where Ω is the segmentation tag set. Note that there is no need to generate characters from the original alphabet, instead a small tag set Ω is used. The fact that the sequence of boundary decisions is of the same length k as the input has also been made explicit. Different tag sets Ω can be used for segmentation. The minimal sets only include two labels: BM/ME (used e.g. by Green and DeNero, 2012). Either the beginning (B) or end (E) of segments is distinguished from non-boundary time-steps in the middle (M). A more fine-grained approach BMES (used e.g. by Ruokolainen et al., 2014) uses Also known as BIES, where I stands for internal. 17 l e a + n <E> h1 h2 h3 h4 h5 h6 LSTM LSTM LSTM LSTM LSTM LSTM <B> l e a + n B M E S <E> h1 h2 h3 h4 h5 LSTM LSTM LSTM LSTM LSTM <B> B M E S + s2 s3 s4 s5 LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM e 0 STM a 0 STM n 1 SUF s2 s3 s4 s5 LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM l 1 STM e 0 STM a 0 STM n 1 SUF l 1 STM (a) seq2seq B M <E> (b) neural sequence ta"
W19-0302,C14-1111,1,0.87332,"e is used: first the features for the desired words are produced using the generative model. The final segmentation can then be decoded from the discriminative model. The idea is that the features from the generative model allow the statistical patterns found in the large unannotated data to be exploited. At the same time, the capacity of the discriminative model is freed for learning to determine when the generative model’s predictions are reliable, in essence to only correct its mistakes. 3.1 Morfessor FlatCat We produce the features for our semi-supervised training using Morfessor FlatCat (Grönroos et al., 2014). Morfessor FlatCat is a generative probabilistic method for learning morphological segmentations. It uses a prior over morph lexicons inspired by the Minimum Description Length principle (Rissanen, 1989). Morfessor FlatCat applies a simple Hidden Markov model for morphotactics, providing morph category tags (stem, prefix, suffix) in addition to the segmentation. The segmentations are more consistent compared to Morfessor Baseline, particularly when splitting compound words. Morfessor FlatCat produces morph category labels in addition to the segmentation decisions. These labels can also be use"
W19-0302,D16-1097,0,0.176148,"l. (2014) using feature set augmentation can also be applied to neural networks to effectively leverage large unannotated data. 2 Morphological processing tasks There are several related morphological tasks that can be described as mapping from one sequence to another. Morphological segmentation is the task of splitting words into morphemes, meaning-bearing sub-word units. In morphological surface segmentation, the word w is segmented into a sequence of surface morphs, substrings whose concatenation is the word w. e.g. achievability 7→ achiev ◦ abil ◦ ity Canonical morphological segmentation (Kann et al., 2016) instead yields a sequence of standardized segments. The aim is to undo morphological processes that result in https://sites.google.com/view/deeplo18/home 16 allomorphs, i.e. different surface morphs corresponding to the same meaning. w 7→ y; w ∈ Σ∗ , y ∈ (Σ ∪ {◦})∗ e.g. achievability 7→ achieve ◦ able ◦ ity where Σ is the alphabet of the language, and ◦ is the boundary marker. Morphological analysis yields the lemma and tags representing the morphological properties of a word. w 7→ yt; w, y ∈ Σ∗ , t ∈ τ ∗ e.g. took 7→ take PAST where τ is the set of morphological tags. Two related morphologi"
W19-0302,N18-1005,0,0.460579,"ion (e.g. Wang et al., 2016). We are interested to see if data-hungry neural network models are applicable to segmentation in low-resource settings, in this case for the Uralic language North Sámi. Neural sequence-to-sequence (seq2seq) models are a very versatile tool for NLP, and are used in state of the art methods for a wide variety of tasks, such as text summarization (Nallapati et al., 2016) and speech synthesis (Wang et al., 2017). Seq2seq methods are easy to apply, as you can often take e.g. existing neural machine translation software and train it with appropriately preprocessed data. Kann et al. (2018) apply the seq2seq model for low-resource morphological segmentation. However, arbitrary length sequence-to-sequence transduction is not the optimal formulation for the task of morphological surface segmentation. We return to formulating it as a a sequence tagging problem instead, and show that this can be implemented with minor modifications to an open source translation system. Moreover, we show that the semi-supervised training approach of Ruokolainen et al. (2014) using feature set augmentation can also be applied to neural networks to effectively leverage large unannotated data. 2 Morphol"
W19-0302,P17-4012,0,0.0471313,"Missing"
W19-0302,K16-1028,0,0.019041,"ained attention. For example, the workshop Deep Learning Approaches for Low-Resource NLP (DeepLo) was arranged first time in the year of writing. Neural methods have met with success in high-resource morphological segmentation (e.g. Wang et al., 2016). We are interested to see if data-hungry neural network models are applicable to segmentation in low-resource settings, in this case for the Uralic language North Sámi. Neural sequence-to-sequence (seq2seq) models are a very versatile tool for NLP, and are used in state of the art methods for a wide variety of tasks, such as text summarization (Nallapati et al., 2016) and speech synthesis (Wang et al., 2017). Seq2seq methods are easy to apply, as you can often take e.g. existing neural machine translation software and train it with appropriately preprocessed data. Kann et al. (2018) apply the seq2seq model for low-resource morphological segmentation. However, arbitrary length sequence-to-sequence transduction is not the optimal formulation for the task of morphological surface segmentation. We return to formulating it as a a sequence tagging problem instead, and show that this can be implemented with minor modifications to an open source translation system"
W19-0302,E14-4017,1,0.843392,"y to apply, as you can often take e.g. existing neural machine translation software and train it with appropriately preprocessed data. Kann et al. (2018) apply the seq2seq model for low-resource morphological segmentation. However, arbitrary length sequence-to-sequence transduction is not the optimal formulation for the task of morphological surface segmentation. We return to formulating it as a a sequence tagging problem instead, and show that this can be implemented with minor modifications to an open source translation system. Moreover, we show that the semi-supervised training approach of Ruokolainen et al. (2014) using feature set augmentation can also be applied to neural networks to effectively leverage large unannotated data. 2 Morphological processing tasks There are several related morphological tasks that can be described as mapping from one sequence to another. Morphological segmentation is the task of splitting words into morphemes, meaning-bearing sub-word units. In morphological surface segmentation, the word w is segmented into a sequence of surface morphs, substrings whose concatenation is the word w. e.g. achievability 7→ achiev ◦ abil ◦ ity Canonical morphological segmentation (Kann et a"
W19-5347,W18-6410,1,0.861487,"Missing"
W19-5347,P18-4020,0,0.0236348,"Missing"
W19-5347,P17-4012,0,0.0294692,"tence-level approaches In this section we describe our sentencelevel translation models and the experiments in the English-to-German, English-to-Finnish and Finnish-to-English translation directions. Table 3: Percentage of lines rejected by each filter for English–Finnish data sets. The strict version is the same as for English–German, and the relax version applies relaxed thresholds. 3.1 Model architectures We experimented with both NMT and rule-based systems. All of our neural sentence-level models are based on the transformer architecture (Vaswani et al., 2017). We used both the OpenNMTpy (Klein et al., 2017) and MarianNMT (Junczysplete sentences that end with proper final punctuation marks, and the filter might remove quite a bit of the useful data examples. However, our fi415 Dowmunt et al., 2018) frameworks. Our experiments focused on the following: BLEU news2018 Single model 5 save-points 5 save-points + 4 fine-tuned • Ensemble models: using ensembles with a combination of independent runs and savepoints from a single training run. 44.61 46.65 47.45 • Left-to-right and right-to-left models: Transformer models with decoding of the output in left-to-right and right-to-left order. Table 4: Englis"
W19-5347,D18-2012,0,0.0289023,"d BPE algorithm is run over the Omorfi-segmented text in order to split low-frequency morphemes. In this experiment, we compare two models for each translation direction: • One model segmented with the standard BPE algorithm (joint vocabulary size of 50 000, vocabulary frequency threshold of 50). English–Finnish and Finnish–English The problem of open-vocabulary translation is particularly acute for morphologically rich languages like Finnish. In recent NMT research, the standard approach consists of applying a word segmentation algorithm such as BPE (Sennrich et al., 2016b) or SentencePiece (Kudo and Richardson, 2018) during pre-processing. In recent WMT editions, various alternative segmentation approaches were examined for Finnish: hybrid models that back off to character-level rep¨ resentations (Ostling et al., 2017), and variants of the Morfessor unsupervised morphology algorithm (Gr¨onroos et al., 2018). This year, we exper• One model where the Finnish side is presegmented with Omorfi, and both the Omorfisegmented Finnish side and the English side are segmented with BPE (same parameters as above). All models are trained on filtered versions of Europarl, ParaCrawl, Rapid, Wikititles, newsdev2015 and ne"
W19-5347,P16-1009,0,0.234209,"sh-to-German (Section 4), and a comparison of different word segmentation approaches for Finnish (Section 3.3). The final submitted NMT systems are summarized in Section 5, while the rule-based machine translation system is described in Section 3.4. 2 Pre-processing • removing non-printing characters, • normalizing punctuation, • tokenization. In addition to these steps, we replaced a number of English contractions with the full form, e.g. “They’re” → “They are”. After the above steps, we applied a Moses truecaser model trained for individual languages, and finally a byte-pair encoding (BPE) (Sennrich et al., 2016b) segmentation using a set of codes for either language pair. For English–German, we initially pre-processed the data using only punctuation normalization and tokenization. We subsequently trained an English truecaser model using all monolingual English data as well as the English side of all parallel English–German datasets except the Rapid corpus (in which non-English characters were missing from a substantial portion of the German sentences). We also repeated the same for German. Afterwards, we used a heuristic cleanup script1 in Pre-processing, data filtering and back-translation It is we"
W19-5347,P16-1162,0,0.441043,"sh-to-German (Section 4), and a comparison of different word segmentation approaches for Finnish (Section 3.3). The final submitted NMT systems are summarized in Section 5, while the rule-based machine translation system is described in Section 3.4. 2 Pre-processing • removing non-printing characters, • normalizing punctuation, • tokenization. In addition to these steps, we replaced a number of English contractions with the full form, e.g. “They’re” → “They are”. After the above steps, we applied a Moses truecaser model trained for individual languages, and finally a byte-pair encoding (BPE) (Sennrich et al., 2016b) segmentation using a set of codes for either language pair. For English–German, we initially pre-processed the data using only punctuation normalization and tokenization. We subsequently trained an English truecaser model using all monolingual English data as well as the English side of all parallel English–German datasets except the Rapid corpus (in which non-English characters were missing from a substantial portion of the German sentences). We also repeated the same for German. Afterwards, we used a heuristic cleanup script1 in Pre-processing, data filtering and back-translation It is we"
W19-5347,P12-3005,0,0.0262377,"rovided parallel training data. This is especially true for the ParaCrawl and Rapid data sets. This is rather unexpected as a basic language identifier certainly must be part of the crawling and extraction pipeline. Nevertheless, after some random inspection of the data, we found it necessary to apply off-the-shelf language identifiers to the data for removing additional erroneous text from the training data. In particular, we applied the Compact Language Detector version 2 (CLD2) from the Google Chrome project (using the Python interface from pycld22 ), and the widely used langid.py package (Lui and Baldwin, 2012) to classify each sentence in the ParaCrawl, CommonCrawl, Rapid and Wikititles data sets. We removed all sentence pairs in which the language of one of the aligned sentences was not reliably detected. For this, we required the correct language ID from both classifiers, the reliable-flag set to “True” by CLD2 with a reliability score of 90 or more, and the detection probability of langid.py to be at least 0.9. Data filtering For data filtering we applied four types of filters: (i) rule-based heuristics, (ii) filters based on language identification, (iii) filters based on word alignment models,"
W19-5347,N19-1313,0,0.0317811,"for 1 epoch. The results for the NMT-HAN model are disappointing. The document-level model performs significantly worse than the sentence-level model. Hierarchical attention models A number of approaches have been developed to utilize the attention mechanism to capture extended context for document-level translation. We experimented with the two following models: • NMT-HAN: Sentence-level transformer model with a hierarchical attention network to capture the document-level context (Miculicich et al., 2018). • selectAttn: Selective attention model for context-aware neural machine translation (Maruf et al., 2019). For testing the selectAttn model, we used the same data with document-level information as we applied in the concatenation models. For NMTHAN we had to use a smaller training set due to lack of resources and due to the implementation not supporting data shards. For NMT-HAN we used only Europarl, NewsCommentary and Rapid for training. Table 11 summarizes the results on the development test data. Both of the tested models need to be trained on sentence-level first, before tuning the document-level components. 5 Model NMT-HAN selectAttn Sentence-level Document-level 35.03 35.26 31.73 34.75 Resu"
W19-5347,D18-1325,0,0.0246673,"uggested in the documentation with respect to optimizers, learning rates and dropout. Unfortunately, the results do not look very promising as we can see in Table 11. The document-level model does not even reach the performance of the sentence-level model even though we trained until convergence on development data with patience of 10 reporting steps, which is quite disappointing. Overall, the scores are below the standard transformer models of the other experiments, and hence, we did not try to further optimize the results using that model. For the NMT-HAN model we used the implementation of Miculicich et al. (2018) with the recommended hyperparameter values and settings. The system is based on the OpenNMT-py implementation of the transformer. The model includes 6 hidden layers on both the encoder and decoder side with a dimensionality of 512 and the multihead attention has 8 attention heads. We applied a sublayer and attention dropout of 0.1. The target and source vocabulary size is 30K. We trained the sentence-level model for 20 epochs after which we further fine-tuned the encoder side hierarchical attention for 1 epoch and the joint encoderdecoder hierarchical attention for 1 epoch. The results for th"
W19-5347,W17-4733,1,0.860954,"BPE algorithm (joint vocabulary size of 50 000, vocabulary frequency threshold of 50). English–Finnish and Finnish–English The problem of open-vocabulary translation is particularly acute for morphologically rich languages like Finnish. In recent NMT research, the standard approach consists of applying a word segmentation algorithm such as BPE (Sennrich et al., 2016b) or SentencePiece (Kudo and Richardson, 2018) during pre-processing. In recent WMT editions, various alternative segmentation approaches were examined for Finnish: hybrid models that back off to character-level rep¨ resentations (Ostling et al., 2017), and variants of the Morfessor unsupervised morphology algorithm (Gr¨onroos et al., 2018). This year, we exper• One model where the Finnish side is presegmented with Omorfi, and both the Omorfisegmented Finnish side and the English side are segmented with BPE (same parameters as above). All models are trained on filtered versions of Europarl, ParaCrawl, Rapid, Wikititles, newsdev2015 and newstest2015 as well as backtranslations. Following our experiments at WMT 5 https://flammie.github.io/ omorfi/pages/usage-examples.html# morphological-segmentation 417 2018 (Raganato et al., 2018), we also u"
W19-5347,W18-6427,0,0.0413463,"Missing"
W19-5347,W17-4811,1,0.87915,"2018. We then test our systems on both the original test set with coherent test data divided into short news documents and the shuffled test set with broken coherence. 4.1 System Baseline 2+1 3+1a 3+1b 1t+1s+1 2+2 BLEU news2018 Shuffled Coherent 38.96 36.62 33.90 34.14 36.82 38.53 38.96 37.17 34.30 34.39 37.24 39.08 Table 10: Comparison of concatenation approaches for English–German document-level translation. Concatenation models Some of the previously published approaches use concatenation of multiple source-side sentences in order to extend the context of the currently translated sentence (Tiedemann and Scherrer, 2017). In addition to the source-side concatenation model, we also tested an approach where we concatenate The results overall are rather disappointing. All but one of the concatenation models underperform and cannot beat the sentence-level baseline. Note that the concat-target model (1t+1s+1) even refers to an oracle experiment in which the reference 420 translation of the previous sentence is fed into the translation model for translating the current source sentence. As this is not very successful, we did not even try to run a proper evaluation with system output provided as target context during"
W19-5347,W15-1844,0,0.0159073,"scored n-best lists. The positive effect of beam search is further illustrated in Figure 1. All previous models were run with a beam size of 12. As we can see, the general trend is that larger beams lead to improved performance, at least until the limit of 64 in our experiments. Beam size 4 is an exception in the left-to-right models. 46.5 46.0 45.5 45.0 44.5 44.0 43.5 43.0 L2R R2L 1 2 4 8 16 Beam size 32 64 Figure 1: The effect of beam size on translation performance. All results use model ensembles and the scores are case-sensitive. imented with rule-based word segmentation based on Omorfi (Pirinen, 2015). Omorfi is a morphological analyzer for Finnish with a large-coverage lexicon. Its segmentation tool5 splits a word form into morphemes as defined by the morphological rules. In particular, it distinguishes prefixes, infixes and suffixes through different segmentation markers: Intia→ ←n ja Japani→ ←n p¨aa¨ → ←ministeri→ India GEN and Japan GEN prime minister ←t tapaa→ ←vat Tokio→ ←ssa PL meet 3 PL Tokyo INE While Omorfi provides word segmentation based on morphological principles, it does not rely on any frequency cues. Therefore, the standard BPE algorithm is run over the Omorfi-segmented te"
W19-5432,P11-2031,0,0.0380532,"g model architecture. The two Morfessor systems can be considered equivalent, as no clear winner emerges. The two official evaluation metrics BLEU and TER do not rank the systems consistently. Character-level metrics were not provided by the organizers, but follow-up experiments showed that chrF2 yields the same rankings as BLEU, whereas CharacTer deviates from BLEU and TER. The results of our submissions – and of many competitors in this shared task – lie very closely together. Before drawing any conclusions, it would therefore be useful to perform statistical significance testing. MultEval (Clark et al., 2011) provides significance scores through bootstrap resampling, but requires the output from multiple training runs of the same translation system. Unfortunately, we were not able to complete multiple training runs of our models due to time constraints. 6 Acknowledgments We would like to thank Stig-Arne Gr¨onroos for the help with Cognate Morfessor. The authors gratefully acknowledge the support of the Academy of Finland through project 314062 from the ICT 2023 call on Computation, Machine Learning and Artificial Intelligence. The authors also acknowledge CSC – IT Center for Science, Finland, for"
W19-5432,W17-4123,0,0.0483261,"Missing"
W19-5432,W10-2210,1,0.786398,"el NMT models tend to be slow due to the greater length of the sequences. 2.2 Morfessor Morfessor (Creutz and Lagus, 2002, 2007) is a method for unsupervised morphological segmentation. In contrast to the byte-pair encoding (BPE) algorithm widely adopted in neural machine translation (Sennrich et al., 2016), Morfessor defines a proper statistical model and applies maximum a posteriori estimation for the model parameters. The granularity of the segmentation (and thus size of the subword lexicon) is tunable by inserting a hyperparameter for varying the balance between prior and data likelihood (Kohonen et al., 2010). The prior can be considered as a encoding cost for the subword lexicon, and the likelihood as encoding cost for the corpus given the lexicon. In the first Morfessor variant, Morfessor Baseline (Creutz and Lagus, 2002; Virpioja et al., 2013), the statistical model is a unigram language model, i.e., the subword units are assumed to occur independently in words. Under this assumption, the probability of a sequence of tokens is simplified to be the product of the subword occurrence probabilities, which enables an efficient training algorithm. The Morfessor Baseline method has been widely tested"
W19-5432,P16-2058,0,0.0616344,"Missing"
W19-5432,P18-1007,0,0.116203,"(2017) report significant improvements over BPE segmentation for Turkish. 2.3 2.4 SentencePiece unigram model As discussed in Section 2.2, Morfessor Baseline defines a unigram language model and determines the size of its lexicon by using a prior probability for the lexicon parameters. A more straightforward approach, first proposed by Varjokallio et al. (2013) for application in ASR, is to fix the lexicon size beforehand and try to find the set of units such that they maximize likelihood of the data for a unigram model. Another heuristic search algorithm for this problem has been proposed by Kudo (2018). In addition, he proposes a subword regularization method for NMT: The unigram language model can be used to generate multiple candidate segmentations to emulate noise and segmentation errors in the data, and thus improve the Cognate Morfessor Cognate Morfessor (Gr¨onroos et al., 2018) is a variant of Morfessor designed to optimize subword segmentation for two related languages so that segmentations are consistent especially for cognates, i.e., word pairs that are similar orthographically, semantically, and distributionally. Cognate Morfessor extends the cost function of 237 ES ↔ PT CS ↔ PL E"
W19-5432,D18-2012,0,0.216413,"s reduces the amount of effort needed for a machine translation system to be able to generalize (Pourdamghani and Knight, 2017). Nevertheless, and especially since the languages offered in this shared task are to some extent morphologically complex, we assume that proper subword segmentation will be beneficial for neural machine translation (NMT) performance. In particular, we aim at consistent segmentation across both related languages. While generic subword segmentation methods such as BPE (Sennrich et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly used segmentation scheme is character-level segmentation, where every character, including the space character, is considered independently. The idea of character-level machine translation for similar languages dates back to SMT times (e.g. Tiedemann, 2009). More recently, character-level NMT has shown promising results for distant languages (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017) as well as for similar ones (Costa-juss`a et al., 2017). The advantage of character-level models is t"
W19-5432,W02-0603,0,0.130135,"con for them. For example, consider that some common inflection produces a slightly different suffix for the two languages. A joint lexicon is likely to have both suffixes as subword units. Then the suffix for language A may interfere with the segmentation of stems of language B that happen to contain the same string, and vice versa. Cognate Morfessor can avoid such problems by keeping the suffixes in separate lexicons. the segmentation algorithm is free of hyperparameters. However, character-level NMT models tend to be slow due to the greater length of the sequences. 2.2 Morfessor Morfessor (Creutz and Lagus, 2002, 2007) is a method for unsupervised morphological segmentation. In contrast to the byte-pair encoding (BPE) algorithm widely adopted in neural machine translation (Sennrich et al., 2016), Morfessor defines a proper statistical model and applies maximum a posteriori estimation for the model parameters. The granularity of the segmentation (and thus size of the subword lexicon) is tunable by inserting a hyperparameter for varying the balance between prior and data likelihood (Kohonen et al., 2010). The prior can be considered as a encoding cost for the subword lexicon, and the likelihood as enco"
W19-5432,W17-4727,1,0.906874,"Missing"
W19-5432,Q17-1026,0,0.0290544,"et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly used segmentation scheme is character-level segmentation, where every character, including the space character, is considered independently. The idea of character-level machine translation for similar languages dates back to SMT times (e.g. Tiedemann, 2009). More recently, character-level NMT has shown promising results for distant languages (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017) as well as for similar ones (Costa-juss`a et al., 2017). The advantage of character-level models is that they do not require any other type of preprocessing such as tokenization or truecasing, and that 236 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 236–244 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Morfessor Baseline (consisting of a lexicon and corpus coding costs) by three lexicon and corpus costs: one for each language, and one for edit operations that transform the cognate forms bet"
W19-5432,W18-6410,1,0.875519,"Missing"
W19-5432,P02-1040,0,0.107805,"tion toolkit – OpenNMT-py (Klein et al., 2017) –, use the same model architecture – the Transformer (Vaswani et al., 2017) –, and the same hyperparameters4 . Training data are shuffled beforehand. We set a threshold in terms of epochs for each translation direction, after which we stop model training.5 This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches. Results on the development set are shown in Table 3 and discussed in detail below. We report two word-level metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), as well as two character-level metrics, CharacTer (Wang et al., 2016) and chrF (Popovi´c, 2016). BLEU and chrF are computed with SacreBLEU (Post, 2018).6 In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, robustness of the translation. The unigram method by Kudo (2018) is implemented in the SentencePiece software (Kudo and Richardson, 2018). 2.5 Byte pair encoding In Sennrich et al. (2016) the authors adapt the byte pair encoding (BPE)"
W19-5432,C14-1111,1,0.901066,"Missing"
W19-5432,W16-2341,0,0.0699623,"Missing"
W19-5432,W18-6319,0,0.0135154,"d. We set a threshold in terms of epochs for each translation direction, after which we stop model training.5 This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches. Results on the development set are shown in Table 3 and discussed in detail below. We report two word-level metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), as well as two character-level metrics, CharacTer (Wang et al., 2016) and chrF (Popovi´c, 2016). BLEU and chrF are computed with SacreBLEU (Post, 2018).6 In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, robustness of the translation. The unigram method by Kudo (2018) is implemented in the SentencePiece software (Kudo and Richardson, 2018). 2.5 Byte pair encoding In Sennrich et al. (2016) the authors adapt the byte pair encoding (BPE) data compression algorithm (Gage, 1994) to the task of word segmentation. They use the idea of the original algorithm, iteratively replacing the most frequent pair of bytes in a seque"
W19-5432,D17-1266,0,0.0259837,"of the parameter space. Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding. 1 2 Introduction Subword segmentation Our experiments focused on four subword segmentation methods, which are summarized shortly in this section. Machine translation between closely related languages is, in principle, less challenging than translation between distantly related ones. Sharing large parts of their grammars and vocabularies reduces the amount of effort needed for a machine translation system to be able to generalize (Pourdamghani and Knight, 2017). Nevertheless, and especially since the languages offered in this shared task are to some extent morphologically complex, we assume that proper subword segmentation will be beneficial for neural machine translation (NMT) performance. In particular, we aim at consistent segmentation across both related languages. While generic subword segmentation methods such as BPE (Sennrich et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly"
W19-5432,W16-2342,0,0.0155825,"ani et al., 2017) –, and the same hyperparameters4 . Training data are shuffled beforehand. We set a threshold in terms of epochs for each translation direction, after which we stop model training.5 This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches. Results on the development set are shown in Table 3 and discussed in detail below. We report two word-level metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), as well as two character-level metrics, CharacTer (Wang et al., 2016) and chrF (Popovi´c, 2016). BLEU and chrF are computed with SacreBLEU (Post, 2018).6 In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, robustness of the translation. The unigram method by Kudo (2018) is implemented in the SentencePiece software (Kudo and Richardson, 2018). 2.5 Byte pair encoding In Sennrich et al. (2016) the authors adapt the byte pair encoding (BPE) data compression algorithm (Gage, 1994) to the task of word segmentation. They use the idea of the or"
W19-5432,W18-6425,1,0.853179,"aseline (α = 0.05) Cognate Morfessor (α = 0.01) 52.8 51.0 52.0 28.6 33.1 29.4 PT → ES Characters Morfessor Baseline (α = 0.05) Cognate Morfessor (α = 0.01) 59.1 58.6 58.4 25.5 25.1 25.3 CS → PL Characters Morfessor Baseline (α = 0.05) Cognate Morfessor (α = 0.01) 5.9 7.0 7.1 88.4 87.3 87.4 PL → CS Characters Morfessor Baseline (α = 0.05) Cognate Morfessor (α = 0.01) 6.6 7.2 7.0 80.2 79.6 79.4 4.4 SentencePiece unigram models We trained the segmentation models only on the available parallel datasets for each language pair, following the findings of our submission to the WMT18 translation task (Raganato et al., 2018). We specified a vocabulary size of 5,000 tokens for each language and we took advantage from the tokenizer integrated in the SentencePiece implementation (Kudo and Richardson, 2018) by training the models on non-tokenized data. We applied the same truecasing models as before. Results reported in Table 3 show that the models trained on SentencePiece-encoded data are consistently behind the Morfessor Baseline and Cognate Morfessor ones, except for the Spanish– Portuguese translation direction. This might be caused by the choice of vocabulary size used and the selected epoch in the table. These"
W19-5432,P16-1162,0,0.774633,"than translation between distantly related ones. Sharing large parts of their grammars and vocabularies reduces the amount of effort needed for a machine translation system to be able to generalize (Pourdamghani and Knight, 2017). Nevertheless, and especially since the languages offered in this shared task are to some extent morphologically complex, we assume that proper subword segmentation will be beneficial for neural machine translation (NMT) performance. In particular, we aim at consistent segmentation across both related languages. While generic subword segmentation methods such as BPE (Sennrich et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly used segmentation scheme is character-level segmentation, where every character, including the space character, is considered independently. The idea of character-level machine translation for similar languages dates back to SMT times (e.g. Tiedemann, 2009). More recently, character-level NMT has shown promising results for distant languages (Costa-juss`a and Fonollosa, 2016; Lee et al., 2"
W19-5432,2006.amta-papers.25,0,0.115959,"et al., 2017) –, use the same model architecture – the Transformer (Vaswani et al., 2017) –, and the same hyperparameters4 . Training data are shuffled beforehand. We set a threshold in terms of epochs for each translation direction, after which we stop model training.5 This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches. Results on the development set are shown in Table 3 and discussed in detail below. We report two word-level metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), as well as two character-level metrics, CharacTer (Wang et al., 2016) and chrF (Popovi´c, 2016). BLEU and chrF are computed with SacreBLEU (Post, 2018).6 In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, robustness of the translation. The unigram method by Kudo (2018) is implemented in the SentencePiece software (Kudo and Richardson, 2018). 2.5 Byte pair encoding In Sennrich et al. (2016) the authors adapt the byte pair encoding (BPE) data compression algorithm (Ga"
W19-5432,C18-1112,0,0.0206243,"results with the two references may vary by up to 2 points. Despite the large amounts of available training data, we chose hyperparameters resulting in rather small vocabulary sizes for all subword splitting schemes, ranging between 2800 and 8900 units per language pair. This choice was guided by three reasons: (1) the competitive performance of character-level models, (2) the desire to force the models to split words across languages, and to do so not only for rare words, and (3) the competitive performance of small vocabulary sizes in related problems such as historical text normalization (Tang et al., 2018). A general finding, shared by the other participants, is that the scores on the Slavic language pair are much lower than on the Romance language pair. We assume that the Spanish–Portuguese development and test sets are built by translating directly from one language to the other, whereas the Czech–Polish development and test sets had been translated from English independently of each other, leading to much freer translations. If this hypothesis is correct, the automatic evaluation scores for Czech–Polish may in fact underestimate the real translation quality. 4.1 4.3 The Cognate Morfessor tra"
W19-5432,2009.eamt-1.3,0,0.0287233,"icular, we aim at consistent segmentation across both related languages. While generic subword segmentation methods such as BPE (Sennrich et al., 2016), Morfessor (Creutz and Lagus, 2007; Gr¨onroos et al., 2014), or SentencePiece (Kudo and Richardson, 2018) yield improved consistency by concatenat2.1 Character segmentation For similar languages, a commonly used segmentation scheme is character-level segmentation, where every character, including the space character, is considered independently. The idea of character-level machine translation for similar languages dates back to SMT times (e.g. Tiedemann, 2009). More recently, character-level NMT has shown promising results for distant languages (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017) as well as for similar ones (Costa-juss`a et al., 2017). The advantage of character-level models is that they do not require any other type of preprocessing such as tokenization or truecasing, and that 236 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 236–244 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Morfessor Baseline (consisting of a lexicon and corp"
W19-5432,P12-3005,0,\N,Missing
W19-5432,P17-4012,0,\N,Missing
W19-5432,W17-4733,1,\N,Missing
W19-5432,P16-1009,0,\N,Missing
W19-5432,W15-1844,0,\N,Missing
