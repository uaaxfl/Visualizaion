2002.jeptalnrecital-recital.4,C94-1097,0,0.0875429,"Missing"
2002.jeptalnrecital-recital.4,P85-1011,0,0.331483,"Missing"
2005.jeptalnrecital-long.35,P89-1018,0,0.298191,"Missing"
2005.jeptalnrecital-long.35,E03-1030,0,0.0344254,"Missing"
2005.jeptalnrecital-long.35,W04-3321,0,0.020979,"Missing"
2005.jeptalnrecital-long.35,J94-1004,0,0.0686287,"Missing"
2005.jeptalnrecital-long.35,E93-1045,0,0.075874,"Missing"
2006.jeptalnrecital-poster.25,C96-2103,0,0.062027,"Missing"
2006.jeptalnrecital-poster.25,E93-1045,0,0.121524,"Missing"
2009.jeptalnrecital-court.1,P05-1038,0,0.0413146,"Missing"
2009.jeptalnrecital-court.1,A00-1031,0,0.165214,"Missing"
2009.jeptalnrecital-court.1,P04-1041,0,0.0348247,"Missing"
2009.jeptalnrecital-court.1,A00-2018,0,0.240234,"Missing"
2009.jeptalnrecital-court.1,P00-1058,0,0.0518767,"Missing"
2009.jeptalnrecital-court.1,P03-1013,0,0.0578217,"Missing"
2009.jeptalnrecital-court.1,J98-4004,0,0.172518,"Missing"
2009.jeptalnrecital-court.1,J00-4006,0,0.0159887,"Missing"
2009.jeptalnrecital-court.1,P03-1054,0,0.0322081,"Missing"
2009.jeptalnrecital-court.1,P06-1055,0,0.281177,"Missing"
2009.jeptalnrecital-court.1,D07-1066,0,0.0506424,"Missing"
2009.jeptalnrecital-court.1,schluter-van-genabith-2008-treebank,0,0.257186,"Missing"
2020.acl-main.107,N16-3003,0,0.0631558,"Missing"
2020.acl-main.107,W19-4603,0,0.0224334,"s. The annotation phases are (i) Morphology/tokenization, (ii) Translation, (iii) Preannotation Syntax, (iv) Correction, (v) Final Syntax. P.M stands for person.month 9 Related Work Research on Arabic dialects is quite extensive. Space is lacking to describe it exhaustively. In relation to our work regarding North-African dialect, we refer to the work of (Samih, 2017) who along his PhD covered an large range of topics regarding the dialect spoken specifically in Morocco and generally regarding language identification (Samih et al., 2016) in code-switching scenario for various Arabic dialects (Attia et al., 2019). Unlike NArabizi dialects, the resource situation for Arabic dialects in canonical written form can hardly be qualified as scarce given the amount of resources produced by the Linguistic Data Consortium regarding these languages, see (Diab et al., 2013) for details on those corpora. These data have been extensively covered in various NLP aspects by the former members of the Columbia Arabic NLP team, among which Mona Diab, Nizar Habash, and Owen Rambow, in their respective subsequent lines of works. Many small to medium scale linguistics resources, such as morphological lexicons or bilingual d"
2020.acl-main.107,bouamor-etal-2014-multidialectal,0,0.0267007,"rces produced by the Linguistic Data Consortium regarding these languages, see (Diab et al., 2013) for details on those corpora. These data have been extensively covered in various NLP aspects by the former members of the Columbia Arabic NLP team, among which Mona Diab, Nizar Habash, and Owen Rambow, in their respective subsequent lines of works. Many small to medium scale linguistics resources, such as morphological lexicons or bilingual dictionaries have been produced (Shoufan and Alameri, 2015). Recently, in addition to the release of a small-range parallel corpus for some Arabic dialects (Bouamor et al., 2014), a larger corpus collection was released, covering 25 city dialects in the travel domain (Bouamor et al., 2018). Regarding the specific NLP modeling challenges of processing Arabic-based languages, as part of the morphologically-rich languages, recent advances in joint models have been addressed by Zalmout and Habash (2019) that recently efficiently adapted a neural architecture to perform joint word segmentation, lemmatization, morphological analysis and POS tagging on an Arabic dialect. Recent works on cross-language learning using the whole massively multilingual pre-trained language model"
2020.acl-main.107,L18-1535,0,0.0311994,"Missing"
2020.acl-main.107,K17-3026,1,0.885235,"Missing"
2020.acl-main.107,P12-3005,0,0.108063,"Missing"
2020.acl-main.107,W19-6905,0,0.0281918,"Nivre et al., 2018), namely gender, number, tense and verbal mood. Note that instead of adding lemmas, we included French glosses for two reasons: firstly for practical reasons, as they helped manual corrections done by non-native speakers of NArabizi, and secondly because of the non-formalized nature of this language, which makes lemmatization very hard, almost akin to etymological research as in the case of garjouma/the throat which can either originate from French gorge or be of Amazigh root. Code-Switching identification Unlike other works in user-generated content for minority languages (Lynn and Scannell, 2019), we do not distinguish between inter- and intra-sentential code-switching and consider word-level codemixing as lexical borrowing. We annotate code-switching at the word level with information about the source language, regardless of the canonical-ness of spelling. Syntactic Annotations Here again we follow the Universal Dependencies 2.2 annotation scheme (Nivre et al., 2018). When facing sequences of French words with regular French syntax, we followed the UD French guidelines; otherwise, we followed the UD Arabic guidelines, following the Prague Arabic Dependency UD Treebank. Translation La"
2020.acl-main.107,W16-3905,1,0.895219,"Missing"
2020.acl-main.107,L18-1008,0,0.039715,"Missing"
2020.acl-main.107,2010.amta-workshop.1,0,0.0191298,"Introduction Until the rise of fully unsupervised techniques that would free our field from its addiction to annotated data, the question of building useful data sets for under-resourced languages at a reasonable cost is still crucial. Whether the lack of labeled data originates from being a minority language status, its almost oral-only nature or simply its programmed political disappearance, geopolitical events are a factor highlighting a language deficiency in terms of natural language processing resources that can have an important societal impact. Events such as the Haïti crisis in 2010 (Munro, 2010) and the current Algerian revolts (Nossiter, 2019)1 are massively reflected on social media, yet often in languages or dialects that are poorly re1 https://www.nytimes.com/2019/03/01/world/ africa/algeria-protests-bouteflika.html sourced, namely Haitian Creole and Algerian dialectal Arabic in these cases. No readily available parsing and machine translations systems are available for such languages. Taking as an example the Arabic dialects spoken in North-Africa, mostly from Morocco to Tunisia, sometimes called Maghribi, sometimes Darija, these idioms notoriously contain various degrees of cod"
2020.acl-main.107,P02-1040,0,0.106052,"Missing"
2020.acl-main.107,W15-3208,0,0.181696,"d/ africa/algeria-protests-bouteflika.html sourced, namely Haitian Creole and Algerian dialectal Arabic in these cases. No readily available parsing and machine translations systems are available for such languages. Taking as an example the Arabic dialects spoken in North-Africa, mostly from Morocco to Tunisia, sometimes called Maghribi, sometimes Darija, these idioms notoriously contain various degrees of code-switching with languages of former colonial powers such as French, Spanish, and, to a much lesser extent, Italian, depending on the area of usage (Habash, 2010; Cotterell et al., 2014; Saadane and Habash, 2015). They share Modern Standard Arabic (MSA) as their matrix language (Myers-Scotton, 1993), and of course present a rich morphology. In conjunction with the resource scarcity issue, the codeswitching variability displayed by these languages challenges most standard NLP pipelines, if not all. What makes these dialects especially interesting is their widespread use in user-generated content found on social media platforms, where they are generally written using a romanized version of the Arabic script, called Arabizi, which is neither standardized nor formalized. The absence of standardization for"
2020.acl-main.107,W16-5806,0,0.0131401,"4 7 3 5th p.m Costs (ke) 6 6 15 7 6 28 45 21 21 87 Table 8: Treebanking costs. The annotation phases are (i) Morphology/tokenization, (ii) Translation, (iii) Preannotation Syntax, (iv) Correction, (v) Final Syntax. P.M stands for person.month 9 Related Work Research on Arabic dialects is quite extensive. Space is lacking to describe it exhaustively. In relation to our work regarding North-African dialect, we refer to the work of (Samih, 2017) who along his PhD covered an large range of topics regarding the dialect spoken specifically in Morocco and generally regarding language identification (Samih et al., 2016) in code-switching scenario for various Arabic dialects (Attia et al., 2019). Unlike NArabizi dialects, the resource situation for Arabic dialects in canonical written form can hardly be qualified as scarce given the amount of resources produced by the Linguistic Data Consortium regarding these languages, see (Diab et al., 2013) for details on those corpora. These data have been extensively covered in various NLP aspects by the former members of the Columbia Arabic NLP team, among which Mona Diab, Nizar Habash, and Owen Rambow, in their respective subsequent lines of works. Many small to mediu"
2020.acl-main.107,C12-1149,1,0.637917,"iven language identification models to extract NArabizi samples among the whole collection of the Common-Crawl-based OSCAR corpora (Ortiz Suárez et al., 2019) as well as 2 millions sentences of additional crawled webdata, resulting in 50k NArabizi sentences of high quality, to date the largest corpus of this language. This makes this collection a valuable test bed for low-resource NLP research. 3.2 Annotation Layers Our NArabizi treebank contains 5 annotations layers: (i) tokenization, (ii) morphology, (iii) codeswitching identification, (iv) syntax and (v) translation. Tokenization Following Seddah et al. (2012) and their work on the French Social Media Bank, we decided to apply a light tokenization process where we manually tokenized only the obvious cases of wrongly detached punctuations and “missing whitespaces” (i.e. cases where two words are 4 https://github.com/ryancotterell/arabic_ dialect_annotation contracted into one token).5 Morphological Analysis This layer consists of two sets of part-of-speech tags, one following the Universal POS tagset (Petrov et al., 2011) and the other the FTB-cc tagset extended to deal with user-generated content (Seddah et al., 2012). In cases of word contractions"
2020.acl-main.107,W15-3205,0,0.0284269,"resource situation for Arabic dialects in canonical written form can hardly be qualified as scarce given the amount of resources produced by the Linguistic Data Consortium regarding these languages, see (Diab et al., 2013) for details on those corpora. These data have been extensively covered in various NLP aspects by the former members of the Columbia Arabic NLP team, among which Mona Diab, Nizar Habash, and Owen Rambow, in their respective subsequent lines of works. Many small to medium scale linguistics resources, such as morphological lexicons or bilingual dictionaries have been produced (Shoufan and Alameri, 2015). Recently, in addition to the release of a small-range parallel corpus for some Arabic dialects (Bouamor et al., 2014), a larger corpus collection was released, covering 25 city dialects in the travel domain (Bouamor et al., 2018). Regarding the specific NLP modeling challenges of processing Arabic-based languages, as part of the morphologically-rich languages, recent advances in joint models have been addressed by Zalmout and Habash (2019) that recently efficiently adapted a neural architecture to perform joint word segmentation, lemmatization, morphological analysis and POS tagging on an Ar"
2020.acl-main.107,K17-3009,0,0.0225523,"ization systems have better performances (Belinkov and Glass, 2015) but are either not maintained with the proper python packages, or come with a fee. 10 Theh, U+062B. Dev OOV 78.74 88.10 72.61 32.28 55.85 70.04 40.94 all Test OOV 80.37 87.17 73.87 32.75 57.42 69.12 43.50 Table 5: POS tagging results. 6.2 Early Parsing experiments As stated earlier in this paper, NArabizi contains a high-level of code-switching with French and is closely related to MSA. We described in Section 5 how we built a mixed treebank based on the 11 Note that we performed a set of baseline experiments with UDPipe 2.0 (Straka and Straková, 2017) as well on a previous version of this data set. It reached only 73.7 of UPOS on the test set. 12 https://github.com/VowpalWabbit/vowpal_ wabbit/wiki 13 Without this endogenous lexicon extraction step, the tagger performed slightly worse, although the difference is small. 1144 French GSD UD treebank and our Arabizi version of the Prague Arabic Dependency Treebank. We trained the UDPipe parser (Straka and Straková, 2017) on various treebanks obtained by combining different proportions of the French GSD and our PADT-based pseudo-Arabizi treebank. We ran these parsers with already annotated gold"
2020.acl-main.107,W10-1401,1,0.7322,"ances exhibit so much variations that they are not mutually understandable across geographically distant regions. As space is missing for an exhaustive description of Arabic language variations, we refer the reader to Habash (2010), Samih (2017) and especially to Saadane and Habash (2015) for a thorough account of Algerian dialectal Arabic, which is the focus of this work. In short, the key properties of North-African dialectal Arabic are: • It is a Semitic language, non codified, mostly spoken; • It has a rich-inflexion system, which qualifies this dialect as a morphologically-rich language (Tsarfaty et al., 2010), even though Saadane and Habash (2015) write that many properties present in Classical Arabic are absent from this dialect (e.g. it has simplified nominal and verbal case systems); • It displays a high degree of variability at all levels: spelling and transliteration conventions, phonology, morphology, lexicon; • It exhibits a high degree of code-switching; due to historical reasons and cultural influence of French in the media circles, the Algerian dialect, as well as Tunisian and Morocco, is known for its heavy use of French words. 2 http://almanach-treebanks.fr/NArabizi Gloss Attested form"
2020.acl-main.51,P18-1073,0,0.049295,"desired property of an analysis method is stability: when applied several times with slightly different conditions, we expect the method to return the same, or very similar, results. Insignificant changes in the initial conditions should result in insignificant changes in the output. This increases the likelihood that the uncovered effects are real and not just artifacts of the initial conditions. Recent works question the stability of word embedding algorithms, demonstrating that different training runs produce different results, especially with small underlying datasets. Antoniak and Mimno (2018) focuses on the cosine-similarity between words in the learned embedding space, showing large variability upon minor manipulations on the corpus. Wendlandt et al. (2018) make a similar argument, showing that word embeddings are unstable by looking at the 10-nearest neighbors (NN) of a word across the different embeddings, and showing that larger lists of nearest neighbors are generally more stable. In this work, we are concerned with the stability of usage-change detection algorithms, and present a metric for measuring this stability. A usagechange detection algorithm takes as input two corpor"
2020.acl-main.51,J82-2005,0,0.716984,"Missing"
2020.acl-main.51,N19-1210,0,0.0294391,"Missing"
2020.acl-main.51,P16-1141,0,0.457854,"g different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew). 1 Introduction Analyzing differences in corpora from different sources (different time periods, populations, geographic regions, news outlets, etc) is a central use case in digital humanities and computational social science. A particular methodology is to identify individual words that are used differently in the different corpora. This includes words that have their meaning changed over time periods (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016b; Kutuzov et al., 2018; Tahmasebi et al., 2018), and words that are used differently by different populations (Azarbonyad et al., 2017; Rudolph et al., 2017). It is thus desired to have an automatic, robust and simple method for detecting such potential changes in word usage and surfacing them for human analysis. In this work we present such a method. ∗ djame.seddah@inria.fr A popular method for performing the task (§4) is to train word embeddings on each corpus and then to project one space to the other using a vectorspace alignment algorithm. Then, distances between a word-form to itself in"
2020.acl-main.51,S19-1001,0,0.0209125,"Missing"
2020.acl-main.51,P19-1379,0,0.133658,"factors, rather than linguistic shift. This may serve as another motivation to move from the global measures to a local one. Recent works (Giullianelli, 2019; Martinc et al., 2019) explored the possibility of modeling diachronic and usage change using contextualized embeddings extracted from now ubiquitous Bert representations (Devlin et al., 2019). Focusing on the financial domain, Montariol and Allauzen (2020) use, on top of Bert embeddings, a clustering method that does not need to predefine the number of clusters and which leads to interesting results on that domain. Another approach from Hu et al. (2019) relies on the inclusion of example-based word sense inventories over time from the Oxford dictionary to a Bert model. Doing so provides an efficient fine-grained word sense representation and enables a seemingly accurate way to monitor word sense change over time. Most of those approaches could be easily used with our method, the inclusion of contextualized embeddings would be for example straightforward, we leave it for future work. 9 Conclusion Detecting words that are used differently in different corpora is an important use-case in corpusbased research. We present a simple and effective m"
2020.acl-main.51,K18-1021,0,0.0491753,"X correspond to embeddings of words in space A, while the rows of Y are the corresponding embeddings in space B. This optimization is solved using the Orthogonal Procrustes (OP) method (Sch¨onemann, 1966), that provides a closed form solution. Vector space alignment methods are extensively studied also outside of the area of detecting word change, primarily for aligning embedding spaces across language pairs (Xing et al., 2015; Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a). Also there, the Orthogonal Procrustes method is taken to be a top contender (Lample et al., 2018b; Kementchedjhieva et al., 2018). 4.1 Shortcomings of the alignment approach Self-contradicting objective. Note that the optimization procedure in the (linear) alignment stage attempts to project each word to itself. This includes words that changed usage, and which therefore should not be near each other in the space. While one may hope that other words and the linearity constraints will intervene, the method may succeed, by mistake, to project words that did change usage next to each other, at the expense of projecting words that did not change usage further apart than they should be. This is an inherent problem with any a"
2020.acl-main.51,W14-2517,0,0.216973,"iveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew). 1 Introduction Analyzing differences in corpora from different sources (different time periods, populations, geographic regions, news outlets, etc) is a central use case in digital humanities and computational social science. A particular methodology is to identify individual words that are used differently in the different corpora. This includes words that have their meaning changed over time periods (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016b; Kutuzov et al., 2018; Tahmasebi et al., 2018), and words that are used differently by different populations (Azarbonyad et al., 2017; Rudolph et al., 2017). It is thus desired to have an automatic, robust and simple method for detecting such potential changes in word usage and surfacing them for human analysis. In this work we present such a method. ∗ djame.seddah@inria.fr A popular method for performing the task (§4) is to train word embeddings on each corpus and then to project one space to the other using a vectorspace alignment algorithm. Th"
2020.acl-main.51,N18-1190,0,0.131702,"same, or very similar, results. Insignificant changes in the initial conditions should result in insignificant changes in the output. This increases the likelihood that the uncovered effects are real and not just artifacts of the initial conditions. Recent works question the stability of word embedding algorithms, demonstrating that different training runs produce different results, especially with small underlying datasets. Antoniak and Mimno (2018) focuses on the cosine-similarity between words in the learned embedding space, showing large variability upon minor manipulations on the corpus. Wendlandt et al. (2018) make a similar argument, showing that word embeddings are unstable by looking at the 10-nearest neighbors (NN) of a word across the different embeddings, and showing that larger lists of nearest neighbors are generally more stable. In this work, we are concerned with the stability of usage-change detection algorithms, and present a metric for measuring this stability. A usagechange detection algorithm takes as input two corpora, and returns a ranked list r of candidate words, sorted from the most likely to have changed to the least likely. For a stable algorithm, we expect different runs to r"
2020.acl-main.51,P14-1096,0,0.209763,"Missing"
2020.acl-main.51,P19-1249,0,0.0211102,"Missing"
2020.acl-main.51,2020.jeptalnrecital-taln.31,0,0.0429294,"Missing"
2020.acl-main.51,N15-1104,0,0.0840014,"as AlignCos. The alignment is performed by finding an orthogonal linear transformation Q that, when given matrices X and Y , projects X to Y while minimizng the squared loss: The rows of X correspond to embeddings of words in space A, while the rows of Y are the corresponding embeddings in space B. This optimization is solved using the Orthogonal Procrustes (OP) method (Sch¨onemann, 1966), that provides a closed form solution. Vector space alignment methods are extensively studied also outside of the area of detecting word change, primarily for aligning embedding spaces across language pairs (Xing et al., 2015; Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a). Also there, the Orthogonal Procrustes method is taken to be a top contender (Lample et al., 2018b; Kementchedjhieva et al., 2018). 4.1 Shortcomings of the alignment approach Self-contradicting objective. Note that the optimization procedure in the (linear) alignment stage attempts to project each word to itself. This includes words that changed usage, and which therefore should not be near each other in the space. While one may hope that other words and the linearity constraints will intervene, the method may succeed, by mi"
2020.acl-main.645,C18-1139,0,0.188832,"e of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2019). These transf"
2020.acl-main.645,bawden-etal-2014-correcting,0,0.0112223,"ncy parsing consists in predicting the labeled syntactic tree in order to capture the syntactic relations between words. For both of these tasks we run our experiments using the Universal Dependencies (UD)3 framework and its corresponding UD POS tag set (Petrov et al., 2012) and UD treebank collection (Nivre et al., 2018), which was used for the CoNLL 2018 shared task (Seker et al., 2018). We perform our evaluations on the four freely available French UD treebanks in UD v2.2: GSD (McDonald et al., 2013), Sequoia4 (Candito and Seddah, 2012; Candito et al., 2014), Spoken (Lacheret et al., 2014; Bawden et al., 2014)5 , and ParTUT (Sanguinetti and Bosco, 2015). A brief overview of the size and content of each treebank can be found in Table 1. Treebank #Tokens #Sentences GSD 389,363 16,342 68,615 3,099 Spoken 34,972 ParTUT 27,658 ···················· FTB 350,930 2,786 1,020 27,658 ···················· Sequoia ···················· ···················· sion of the Multi-Genre NLI (MultiNLI) corpus (Williams et al., 2018) to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is machine translated for all languages other than English. The da"
2020.acl-main.645,J92-4003,0,0.338164,"Missing"
2020.acl-main.645,W09-3821,0,0.0439594,"l model based on mBERT, UDify is trained simultaneously on 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages. We report the scores from Kondratyuk (2019) paper. Table 1: Statistics on the treebanks used in POS tagging, dependency parsing, and NER (FTB). We also evaluate our model in NER, which is a sequence labeling task predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank6 (FTB) (Abeillé et al., 2003) in its 2008 version introduced by Candito and Crabbé (2009) and with NER annotations by Sagot et al. (2012). The FTB contains more than 11 thousand entity mentions distributed among 7 different entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctua"
2020.acl-main.645,2020.acl-main.747,0,0.0712291,"Missing"
2020.acl-main.645,D18-1269,0,0.021825,"g, and NER (FTB). We also evaluate our model in NER, which is a sequence labeling task predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank6 (FTB) (Abeillé et al., 2003) in its 2008 version introduced by Candito and Crabbé (2009) and with NER annotations by Sagot et al. (2012). The FTB contains more than 11 thousand entity mentions distributed among 7 different entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctuation 6 This dataset has only been stored and used on Inria’s servers after signing the research-only agreement. 4 • UDPipe Future (Straka, 2018): An LSTMbased model ranked 3rd in dependency parsing and 6th in POS tagging at the CoNLL 2018 shared task (Seker et al., 2018). We report the scores from Kondratyuk"
2020.acl-main.645,2020.findings-emnlp.292,0,0.0196656,"B model (81.88 vs. 81.55). This might be due to the random seed used for pretraining, as each model is pretrained only once. 7210 language understanding tasks. However, even with a BASE architecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance. 7 Discussion Since the pre-publication of this work (Martin et al., 2019), many monolingual language models have appeared, e.g. (Le et al., 2019; Virtanen et al., 2019; Delobelle et al., 2020), for as much as 30 languages (Nozza et al., 2020). In almost all tested configurations they displayed better results than multilingual language models such as mBERT (Pires et al., 2019). Interestingly, Le et al. (2019) showed that using their FlauBert, a RoBERTa-based language model for French, which was trained on less but more edited data, in conjunction to CamemBERT in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing of French, highlighting thus the complementarity of both models.18 As it was the case for Engli"
2020.acl-main.645,N19-1423,0,0.496677,"ennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2019). These transfer learning methods exhibit clear advantages over more traditional task-specific approaches. In particular, they can be trained in an unsupervized manner, thereby taking advantage of the information contained in large amounts of raw text. Yet they come with implementation challenges, namely the amount of data and computational resources needed for pretraining, which can reach hundreds of gigabytes of text and require hundreds of GPUs (Yang et al., 2019; Liu et al., 2019). This has limited the availability of these stat"
2020.acl-main.645,E17-2068,0,0.0593732,"training data, architecture, training objective and optimisation setup we use for CamemBERT. 4.1 Training data Pretrained language models benefits from being trained on large datasets (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019). We therefore use the French part of the OSCAR corpus (Ortiz Suárez et al., 2019), a pre-filtered and pre-classified version of Common Crawl.7 OSCAR is a set of monolingual corpora extracted from Common Crawl snapshots. It follows the same approach as (Grave et al., 2018) by using a language classification model based on the fastText linear classifier (Grave et al., 2017; Joulin et al., 2016) pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization. 4.2 Pre-processing We segment the input text data into subword units using SentencePiece (Kudo and Richardson, 2018). SentencePiece is an extension of Byte-Pair encoding (BPE) (Sennrich et al., 2016) and WordPiece (Kudo, 2018) that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific to"
2020.acl-main.645,P19-1356,1,0.885465,"Missing"
2020.acl-main.645,D14-1162,0,0.0850598,"entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Ra"
2020.acl-main.645,N18-1202,0,0.351202,". We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al."
2020.acl-main.645,petrov-etal-2012-universal,0,0.0660713,"Natural Language Inference (NLI). We also present the baselines that we will use for comparison. 1 Released at: https://camembert-model.fr under the MIT open-source license. 7204 2 https://allennlp.org/elmo Tasks POS tagging is a low-level syntactic task, which consists in assigning to each word its corresponding grammatical category. Dependency parsing consists in predicting the labeled syntactic tree in order to capture the syntactic relations between words. For both of these tasks we run our experiments using the Universal Dependencies (UD)3 framework and its corresponding UD POS tag set (Petrov et al., 2012) and UD treebank collection (Nivre et al., 2018), which was used for the CoNLL 2018 shared task (Seker et al., 2018). We perform our evaluations on the four freely available French UD treebanks in UD v2.2: GSD (McDonald et al., 2013), Sequoia4 (Candito and Seddah, 2012; Candito et al., 2014), Spoken (Lacheret et al., 2014; Bawden et al., 2014)5 , and ParTUT (Sanguinetti and Bosco, 2015). A brief overview of the size and content of each treebank can be found in Table 1. Treebank #Tokens #Sentences GSD 389,363 16,342 68,615 3,099 Spoken 34,972 ParTUT 27,658 ···················· FTB 350,930 2,786"
2020.acl-main.645,P19-1493,0,0.0210785,"itecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance. 7 Discussion Since the pre-publication of this work (Martin et al., 2019), many monolingual language models have appeared, e.g. (Le et al., 2019; Virtanen et al., 2019; Delobelle et al., 2020), for as much as 30 languages (Nozza et al., 2020). In almost all tested configurations they displayed better results than multilingual language models such as mBERT (Pires et al., 2019). Interestingly, Le et al. (2019) showed that using their FlauBert, a RoBERTa-based language model for French, which was trained on less but more edited data, in conjunction to CamemBERT in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing of French, highlighting thus the complementarity of both models.18 As it was the case for English when BERT was first released, the availability of similar scale language models for French enabled interesting applications, such as large scale anonymization of legal texts, where Ca"
2020.acl-main.645,P16-1162,0,0.0173054,"l snapshots. It follows the same approach as (Grave et al., 2018) by using a language classification model based on the fastText linear classifier (Grave et al., 2017; Joulin et al., 2016) pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization. 4.2 Pre-processing We segment the input text data into subword units using SentencePiece (Kudo and Richardson, 2018). SentencePiece is an extension of Byte-Pair encoding (BPE) (Sennrich et al., 2016) and WordPiece (Kudo, 2018) that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers. We use a vocabulary size of 32k subword tokens. These subwords are learned on 107 sentences sampled randomly from the pretraining dataset. We do not use subword regularisation (i.e. sampling from multiple possible segmentations) for the sake of simplicity. 7 https://commoncrawl.org/about/ 4.3 Language Modeling Transformer Similar to RoBERTa and BERT, CamemBERT is a multi-layer bidirectional Transformer (Vaswani et al., 2017). Given the widesp"
2020.acl-main.645,K18-2020,0,0.0902039,"ent entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctuation 6 This dataset has only been stored and used on Inria’s servers after signing the research-only agreement. 4 • UDPipe Future (Straka, 2018): An LSTMbased model ranked 3rd in dependency parsing and 6th in POS tagging at the CoNLL 2018 shared task (Seker et al., 2018). We report the scores from Kondratyuk (2019) paper. • UDPipe Future + mBERT + Flair (Straka et al., 2019): The original UDPipe Future implementation using mBERT and Flair as feature-based contextualized word embeddings. We report the scores from Straka et al. (2019) paper. In French, no extensive work has been done on NER due to the limited availability of annotated corpora. Thus we compare our model with the only recent available baselines set by Dupont (2017), who t"
2020.acl-main.645,P81-1022,0,0.325042,"Missing"
2020.acl-main.645,P19-1527,0,0.185418,"ining set is machine translated for all languages other than English. The dataset is composed of 122k train, 2490 development and 5010 test examples for each language. As usual, NLI performance is evaluated using accuracy. Baselines In dependency parsing and POStagging we compare our model with: • mBERT: The multilingual cased version of BERT (see Section 2.1). We fine-tune mBERT on each of the treebanks with an additional layer for POS-tagging and dependency parsing, in the same conditions as our CamemBERT model. • XLMMLM-TLM : A multilingual pretrained language model from Lample and Conneau (2019), which showed better performance than mBERT on NLI. We use the version available in the Hugging’s Face transformer library (Wolf et al., 2019); like mBERT, we fine-tune it in the same conditions as our model. Genres Blogs, News Reviews, Wiki Medical, News Non-fiction, Wiki Spoken Legal, News, Wikis News • UDify (Kondratyuk, 2019): A multitask and multilingual model based on mBERT, UDify is trained simultaneously on 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages. We report the scores from Kondratyuk (2019) paper."
2020.acl-main.645,P19-1452,0,0.028162,"Missing"
2020.acl-main.645,N18-1101,0,0.0873351,"Missing"
2020.acl-main.645,D19-1077,0,0.0177636,"Straka et al. (2019) paper. In French, no extensive work has been done on NER due to the limited availability of annotated corpora. Thus we compare our model with the only recent available baselines set by Dupont (2017), who trained both CRF (Lafferty et al., 2001) and 7205 BiLSTM-CRF (Lample et al., 2016) architectures on the FTB and enhanced them using heuristics and pretrained word embeddings. Additionally, as for POS and dependency parsing, we compare our model to a fine-tuned version of mBERT for the NER task. For XNLI, we provide the scores of mBERT which has been reported for French by Wu and Dredze (2019). We report scores from XLMMLM-TLM (described above), the best model from Lample and Conneau (2019). We also report the results of XLM-R (Conneau et al., 2019). 4 CamemBERT: a French Language Model In this section, we describe the pretraining data, architecture, training objective and optimisation setup we use for CamemBERT. 4.1 Training data Pretrained language models benefits from being trained on large datasets (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019). We therefore use the French part of the OSCAR corpus (Ortiz Suárez et al., 2019), a pre-filtered and pre-classified vers"
2020.iwpt-1.16,2020.iwpt-1.21,0,0.391189,". This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and st"
2020.iwpt-1.16,2020.iwpt-1.24,0,0.181901,"dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an"
2020.iwpt-1.16,W17-6507,1,0.792227,"Missing"
2020.iwpt-1.16,2020.iwpt-1.20,0,0.157709,"Missing"
2020.iwpt-1.16,2020.iwpt-1.23,0,0.193264,"on that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependen"
2020.iwpt-1.16,2020.iwpt-1.26,0,0.254795,"Missing"
2020.iwpt-1.16,2020.iwpt-1.19,0,0.133706,"is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an enhancement originates in the de-lexicalized edge label. A wide range of parsers (graph-based biaffine, transitionbased), and pre-trained embeddings (XLM-R or mBERT or language specific BERTs) is used. Finally, several teams (Emory NLP (He and Choi, 2020), ShanghaiTech (Wang et al., 2020), ADAPT, Køpsala (Hershcovich et al., 2020), RobertNLP (Gr¨unewald and Friedrich, 2020)) do not use conversion (or only to restore de-lexicalized labels), but instead use a graph-based parser that can directly produce enhanced dependency graphs. The output of the graph-based parser is often combined with information from a standard UD parser to ensure well-formedness and connectedness of the resulting graph. 6 8 Results We include two baseline results:10 baseline1 was obtained by taking gold basic UD trees and copying these into the enhanced layer without any"
2020.iwpt-1.16,2020.iwpt-1.18,0,0.167819,"its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-"
2020.iwpt-1.16,P80-1024,0,0.722615,"Missing"
2020.iwpt-1.16,2020.iwpt-1.17,0,0.156172,"Missing"
2020.iwpt-1.16,D19-1279,0,0.19421,"pendent with dependency label L2 has an empty node i2.1 as parent which itself is an L1 dependent of i1, its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into"
2020.iwpt-1.16,D19-1277,0,0.0922493,"Missing"
2020.iwpt-1.16,2020.lrec-1.497,1,0.848313,"Missing"
2020.iwpt-1.16,W18-6012,0,0.132583,"hether the same is true for enhanced dependency parsing. The challenge is both formal and practical. First, the enhanced representation is a connected graph, possibly containing cycles, while previous work on dependency parsing mostly dealt with rooted trees. Second, as some dependency labels incorporate the lemma of certain dependents and other additional information, the set of labels to be predicted is much larger and language-dependent. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Schuster et al., 2017; Nivre et al., 2018). Moreover, most state of the art work in dependency parsing uses a graph-based approach, where the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen et al., 2014, 2015, 2019) and could even lead to state of the art performance for downstream applications as shown by the results of the Extrinsic Evaluation Parsing shared-task (Oepen et al., 2017). 151 Proceedings of the 16th Internation"
2020.iwpt-1.16,K19-2001,0,0.0535138,"Missing"
2020.iwpt-1.16,S15-2153,1,0.852196,"Missing"
2020.iwpt-1.16,S14-2008,1,0.812645,"e set of labels to be predicted is much larger and language-dependent. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Schuster et al., 2017; Nivre et al., 2018). Moreover, most state of the art work in dependency parsing uses a graph-based approach, where the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen et al., 2014, 2015, 2019) and could even lead to state of the art performance for downstream applications as shown by the results of the Extrinsic Evaluation Parsing shared-task (Oepen et al., 2017). 151 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 151–161 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics 3 Enhanced Universal Dependencies conj conj UD version 22 states that apart from the morphological and basic dependency annotation layers, strings may be annotated with an additional, enhanced, dependency layer"
2020.iwpt-1.16,2020.acl-demos.14,0,0.0565369,"node i2.1 as parent which itself is an L1 dependent of i1, its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by"
2020.iwpt-1.16,L16-1680,0,0.0883819,"Missing"
2020.iwpt-1.16,2020.iwpt-1.22,0,0.144364,"combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an enhancement originates in the de-lexicalized edge label. A wide range of parsers (graph-based biaffine, transitionbased), and pre-trained embeddings (XLM-R or mBERT or language specific BERTs) is used. Finally, several teams (Emory NLP (He and Choi, 2020), ShanghaiTech (Wang et al., 2020), ADAPT, Køpsala (Hershcovich et al., 2020), RobertNLP (Gr¨unewald and Friedrich, 2020)) do not use conversion (or only to restore de-lexicalized labels), but instead use a graph-based parser that can directly produce enhanced dependency graphs. The output of the graph-based parser is often combined with information from a standard UD parser to ensure well-formedness and connectedness of the resulting graph. 6 8 Results We include two baseline results:10 baseline1 was obtained by taking gold basic UD trees and copying these into the enhanced layer without any modifications. Baseline2 uses UDPi"
2020.iwpt-1.16,K18-2001,1,0.884896,"Missing"
2020.jeptalnrecital-taln.5,2020.findings-emnlp.292,0,0.0264053,"Missing"
2020.jeptalnrecital-taln.5,N19-1423,0,0.0614451,"Missing"
2020.jeptalnrecital-taln.5,L18-1550,0,0.0310337,"Missing"
2020.jeptalnrecital-taln.5,E17-2068,0,0.0612565,"Missing"
2020.jeptalnrecital-taln.5,P18-1031,0,0.0436608,"Missing"
2020.jeptalnrecital-taln.5,P19-1356,1,0.883155,"Missing"
2020.jeptalnrecital-taln.5,D14-1162,0,0.0850524,"Missing"
2020.jeptalnrecital-taln.5,N18-1202,0,0.122933,"Missing"
2020.jeptalnrecital-taln.5,P19-1493,0,0.0268051,"Missing"
2020.jeptalnrecital-taln.5,F12-2050,1,0.854707,"Missing"
2020.jeptalnrecital-taln.5,P19-1527,0,0.0505485,"Missing"
2020.jeptalnrecital-taln.5,P19-1452,0,0.0305722,"Missing"
2020.jeptalnrecital-taln.5,N18-1101,0,0.0357062,"Missing"
2020.lrec-1.645,N18-1090,0,0.154878,"ices, taking into account a number of issues whose classification was partially inspired by the list of topics from the Special Track on the Syntac5 http://www.spmrl.org/sancl-posters2014. html 6 This is not to say that there is no conventional, well-punctuated data on social media. For instance, many corporations and institutions employ social media managers who adhere to common editing standards. Conversely, some sentence boundaries in canonical written language are also ambiguous, e.g. in headings, tables 5242 Name ATDT (UD) Hi-En-CS TwitterAAE (TAAE) Reference (Albogamy and Ramsay, 2017) (Bhat et al., 2018) (Blodgett et al., 2018) Source Twitter Twitter Twitter TWITTIRÒ-UD (TWRO) DWT W2.0 Foreebank (Frb) Tweebank (Twb) Tweebank2 (Twb2) TDT xUGC ITU tweeDe Postwita-UD (Pst) FSMB EWT SDT CWT GUM (Cignarella et al., 2019) (Daiber and Van Der Goot, 2016) (Foster et al., 2011) (Kaljahi et al., 2015) (Kong et al., 2014) (Liu et al., 2018) (Luotolahti et al., 2015) (Martínez Alonso et al., 2016) (Pamay et al., 2015) (Rehbein et al., 2019) (Sanguinetti et al., 2018) (Seddah et al., 2012) (Silveira et al., 2014) (Wang et al., 2017) (Wang et al., 2014) (Zeldes, 2017) Twitter Twitter Twitter, sport forums"
2020.lrec-1.645,P04-3031,0,0.214172,"l constructs occurring in social media. For instance, (sequences of) hashtags and URLs are separated out into ‘sentences’ of their own whenever they occur at the beginning or at the end of a tweet and do not have any syntactic function. The above segmentation policies notwithstanding, tweeDe still features the use of parataxis for juxtaposed clauses that are not separated by punctuation. A third option besides not segmenting and segmenting manually is, of course, to segment automatically. In the spirit of maintaining a real-world scenario, Frb split their forum data into sentences using NLTK (Bird and Loper, 2004), with no post-corrections. Accordingly, the resource contains instances where multiple sentences are merged into one sentence due to punctuation errors such as a comma being used instead of a full stop, as in Example 1. Conversely, there are cases where a single sentence is split over multiple lines, resulting in multiple sentences (Example 2) that are not rejoined. (1) Combofix will start, When it is scanning don’t move the mouse cursor inside the box, can cause freezing. (2) I’m sure the devs. can give you more details on this Tokenization Tokenization problems in informal text include case"
2020.lrec-1.645,P18-1131,0,0.0314012,"Missing"
2020.lrec-1.645,W19-7803,0,0.0239954,"n ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding difficult by means of the"
2020.lrec-1.645,W16-1714,1,0.893259,"Missing"
2020.lrec-1.645,L16-1667,1,0.88063,"Missing"
2020.lrec-1.645,L16-1248,0,0.0199681,"246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the"
2020.lrec-1.645,N13-1037,0,0.108812,"generated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013), and that continue to make its 1 https://noisy-text.github.io/ This phrase is used here in a broader sense to indicate all those orthographic, lexical as well as structural choices adopted by a user, often for expressive purposes. 2 treatment a difficult task. The availability of ad hoc training resources remaining an essential factor for the analysis of these texts, in the last decade, numerous resources of this type have been developed. A good proportion of these have been annotated according to the UD scheme (Nivre et al., 2016), a dependency-based format which has achieved great popularit"
2020.lrec-1.645,N10-1060,0,0.0742547,"e popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some"
2020.lrec-1.645,P11-2008,0,0.520555,"Missing"
2020.lrec-1.645,D14-1108,0,0.30934,"e last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UG"
2020.lrec-1.645,lacheret-etal-2014-rhapsodie,0,0.0291573,"inustria’s education’ 5246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions"
2020.lrec-1.645,W16-5403,0,0.0247332,"nn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described"
2020.lrec-1.645,N18-1088,0,0.414107,"ade it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foste"
2020.lrec-1.645,W15-4301,1,0.715109,"l media, treebanks, Universal Dependencies, annotation guidelines, UGC 1. Introduction The immense popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as we"
2020.lrec-1.645,J93-2004,0,0.07288,"the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding difficult by means of the current UD scheme. Elliptical structures and missing elements In constituency-based treebanks of canonical texts such as the Penn Treebank (Marcus et al., 1993) the annotation of empty elements results from the need to keep traces of movement and long-distance dependencies, usually marked with traces and co-indexations at the lexical level in addition to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject"
2020.lrec-1.645,W16-3905,1,0.923501,"resources comprise texts from discussion forums of any kind. Only two treebanks consist of texts from different sub-domains, i.e. blogs, reviews, emails, newsgroups and question answers (EWT), and Wikinews, Wikivoyage, wikiHow, Wikipedia, interviews, Creative Commons fiction and Reddit (GUM), and one is made up of generic data automatically crawled from the web (TDT). Syntactic frameworks As regards the formalism adopted to represent the syntactic structure, dependencies are by far the most used paradigm, especially among the treebanks created from 2014 onward. As also pointed out by Martínez Alonso et al. (2016), a dependency-based annotation lends 3 A more complete table with additional information on the surveyed treebanks can be found here: http://di.unito.i t/webtreebanks. 4 https://developer.twitter.com/en/develop er-terms/agreement-and-policy#c-respect-use rs-control-and-privacy 5241 Phenomenon Lang Attested example Standard form Ergographic phenomena (encoding simplification) GA Leigh aris! Léigh arís! ˙ TR Istanbuldaki agaclar Istanbul’daki a˘gaçlar EN ppl people TR slm selam EN Happy Birthday 2 me Happy Birthday to me TR n zmn ne zaman FR je sé je sais GA gura míle go raibh míle FR tous mes"
2020.lrec-1.645,N13-1039,0,0.155761,"Missing"
2020.lrec-1.645,L18-1106,0,0.0219631,"1. Introduction The immense popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall"
2020.lrec-1.645,W15-1302,1,0.787281,"accurate cross-lingual studies, all switched tokens should be (consistently) lemmatized if the language is known to annotators. Otherwise the surface form should be used, allowing for more comprehensive lemmatization at a later date. Disfluencies Similar to spoken language, UGC often contains disfluencies such as repetitions, fillers or aborted sentences. This might be surprising, given that UGC does not pose the same pressure on cognitive processing that online spoken language production does. In UGC, however, what seems to be a performance error has in fact a completely different function (Rehbein, 2015). Here, repetitions, self-repair and hesitation markers are often used with humorous intent (Example 12) (12) “Le proposte per l’education di Confindustria” ‘The proposals for the Confinustria’s education’ 5246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity res"
2020.lrec-1.645,L16-1376,0,0.0124556,"on to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject and object of the right hand-side conjunct are annotated with the orphan or remnant13 relations (Schuster et al., 2017). Even though the Enhanced UD scheme proposes to include a ghost-token (Schuster and Manning, 2016) which will be the actual governor of the right hand-side conjuncts , nothing is prescribed regarding treatment of ellipsis without 13 det/nmod Figure 1: Pathological example with two contesting structures from two different readings of the token “-2” surrounded by at least 2 elided elements. (Adapted to UD2.5 from (Martínez Alonso et al., 2016)) Du Hengst! äh, hängst. You stallion! uh, hang2.P s.Sg . “You stallion! uh, you’re stalled.” 5. fixed det This treatment, however, loses information whenever the reparandum does not have the same grammatical function as the repair, which is sometimes t"
2020.lrec-1.645,W17-0416,0,0.0155036,"nce dependencies, usually marked with traces and co-indexations at the lexical level in addition to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject and object of the right hand-side conjunct are annotated with the orphan or remnant13 relations (Schuster et al., 2017). Even though the Enhanced UD scheme proposes to include a ghost-token (Schuster and Manning, 2016) which will be the actual governor of the right hand-side conjuncts , nothing is prescribed regarding treatment of ellipsis without 13 det/nmod Figure 1: Pathological example with two contesting structures from two different readings of the token “-2” surrounded by at least 2 elided elements. (Adapted to UD2.5 from (Martínez Alonso et al., 2016)) Du Hengst! äh, hängst. You stallion! uh, hang2.P s.Sg . “You stallion! uh, you’re stalled.” 5. fixed det This treatment, however, loses information when"
2020.lrec-1.645,C12-1149,1,0.52101,"ic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013), and that continue to make its 1 https://noisy-text.github.io/ This phrase is used here in a broader sense to indicate all those orthographic, lexical as well as structural choices adopted by a user, often for expressive purposes. 2 treatment a difficult task. The availability of ad hoc training resources remaining an essential factor for the analysis of these texts, in the last decade, numerous resources of this type have been developed. A good proportion of these have been annotated according to the UD scheme (Nivre et al., 2016), a dependency-based format which has achie"
2020.lrec-1.645,D08-1110,0,0.0232123,"d apply to INTER CS (universaldependencies.org, 2019a). In the cases of INTRA CS that are compositional and the grammar of the switched text is known to annotators, the dependency labels should represent the syntactic role each switched token plays. • Markup symbols (e.g. < >) have the UPOS SYM similar to e.g., math operators in the UD guidelines, and they are attached to the root with dep. Code switching While capturing code-switching (CS) in tweets is also a motivation for a tweet-based unit of analysis (Çetino˘glu, 2016; Lynn and Scannell, 2019), it is an emerging topic of interest in NLP (Solorio and Liu, 2008; Solorio et al., 2014; Bhat et al., 2018) and thus should be captured in treebank data. CS (switching between languages) can occur on a number of levels. CS that occurs at the sentence or clause level is referred to as inter-sentential switching (INTER) as shown between English and Irish in Example 9: (9) “Má tá AON Gaeilge agat, úsáid í! It’s Irish Language Week.” If you have ANY Irish, use it! It’s Irish Language Week. INTER switching can also be used to describe bilingual tweets where the switched text represents a translation of the previous segment: “Happy St Patrick’s Day! La Fhéile Pád"
2020.lrec-1.645,W14-3907,0,0.0224716,"iversaldependencies.org, 2019a). In the cases of INTRA CS that are compositional and the grammar of the switched text is known to annotators, the dependency labels should represent the syntactic role each switched token plays. • Markup symbols (e.g. < >) have the UPOS SYM similar to e.g., math operators in the UD guidelines, and they are attached to the root with dep. Code switching While capturing code-switching (CS) in tweets is also a motivation for a tweet-based unit of analysis (Çetino˘glu, 2016; Lynn and Scannell, 2019), it is an emerging topic of interest in NLP (Solorio and Liu, 2008; Solorio et al., 2014; Bhat et al., 2018) and thus should be captured in treebank data. CS (switching between languages) can occur on a number of levels. CS that occurs at the sentence or clause level is referred to as inter-sentential switching (INTER) as shown between English and Irish in Example 9: (9) “Má tá AON Gaeilge agat, úsáid í! It’s Irish Language Week.” If you have ANY Irish, use it! It’s Irish Language Week. INTER switching can also be used to describe bilingual tweets where the switched text represents a translation of the previous segment: “Happy St Patrick’s Day! La Fhéile Pádraig sona daoibh!” Thi"
2020.lrec-1.645,L16-1250,0,0.0251562,"fach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding diff"
2020.lrec-1.645,W19-7723,1,0.892639,"Missing"
2020.lrec-1.645,L16-1102,0,0.0250592,"Missing"
2020.lrec-1.645,I11-1100,1,0.739484,"Missing"
2020.lrec-1.645,D15-1157,1,0.88306,"Missing"
2020.lrec-1.645,L16-1262,0,0.0724564,"Missing"
2020.lrec-1.645,W15-1610,0,0.0503228,"Missing"
2020.lrec-1.645,W19-7811,1,0.885346,"Missing"
2020.lrec-1.645,L18-1279,1,0.892617,"Missing"
2020.lrec-1.645,silveira-etal-2014-gold,0,0.0780259,"Missing"
2020.lrec-1.645,D14-1122,0,0.0259874,"Missing"
2020.lrec-1.645,P17-1159,0,0.0585556,"Missing"
2021.eacl-main.189,2020.acl-main.692,0,0.0213185,"rops when the evaluation is done on a distinct language (e.g. -5.82 when evaluated on French). The trends are similar for all the domains and tasks we tested on. We conclude that the pretrained parameters at the lower layers are consistently more critical for cross-language transfer than for same-language transfer, and cannot be explained by the possibly different domain of the evaluated datasets. 3 Although other factors might play a part in out-ofdistribution, we suspect that domains plays a crucial part in transfer. Moreover, it was shown that BERT encodes out-ofthe-box domain information (Aharoni and Goldberg, 2020) 2216 Domain Analyses EN - EN EN - NEWS Cross-Language EN - FR ∆0-1 90.40 77.91 75.77 45.90 -1.41 -0.91 -2.14 -1.97 RANDOM-INIT of layers ∆2-3 ∆4-5 ∆6-7 ∆8-9 Parsing -2.33 -1.57 -1.43 -0.60 -1.38 -1.85 -0.83 -0.23 -2.42 -2.54 -1.42 -0.71 -2.75 -2.10 -1.04 -0.39 83.25 71.29 -5.82 -7.86 -2.69 -4.33 -2.42 -4.64 -0.44 -0.92 0.25 -0.11 0.94 0.33 96.83 93.09 89.67 68.93 -1.35 -0.58 -1.07 -2.38 -0.98 -0.65 -1.21 -1.07 -0.70 -0.28 -0.41 -0.14 POS -0.40 -0.04 -0.10 0.54 -0.28 -0.06 0.03 -0.04 -0.24 0.12 0.21 0.63 93.43 91.13 -3.59 -5.10 -0.88 -0.93 -1.31 -1.16 -0.56 -0.74 0.46 0.15 0.25 -0.07 -0.15 -0."
2021.eacl-main.189,2020.acl-tutorials.1,0,0.0184177,"raining. Focusing on syntax, Chi et al. (2020) recently showed that the multilingual version of BERT (mBERT) (Devlin et al., 2019), encodes linguistic properties in shared multilingual sub-spaces. Recently, Gonen et al. (2020) suggest that mBERT learns a language encoding component and an abstract cross-lingual component. In this work, we are interested in understanding the mechanism that leads mBERT to perform zero-shot cross-lingual transfer. More specifically, we ask what parts of the model and what mechanisms support cross-lingual transfer? By combining behavioral and structural analyses (Belinkov et al., 2020), we show that mBERT operates as the stacking of two modules: (1) A multilingual encoder, located in the lower part of the model, critical for cross-lingual transfer, is in charge of 2214 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2214–2231 April 19 - 23, 2021. ©2021 Association for Computational Linguistics aligning multilingual representations; and (2) a task-specific, language-agnostic predictor which has little importance for cross-lingual transfer and is dedicated to performing the downstream task. This mechanism that"
2021.eacl-main.189,D15-1075,0,0.044925,"ne-tuned model on NER with and w/o RANDOM-INIT between English (source) and Russian (target). The higher the score the greater the similarity. less of the language and task, and hints on an alignment that occurs in the lower part of the model. Interestingly, the same trend is also observed in the pretrained model, suggesting that the fine-tuning step preserves the multilingual alignment. These results do not match the findings of Singh et al. (2019), who found no language alignment across layers, although they inspected Natural Language Inference, a more “high-level task” (Dagan et al., 2005; Bowman et al., 2015). We leave the inspection of this mismatch to future work. 4.3 Better Alignment Leads to Better Cross-Lingual Transfer In the previous section we showed that fine-tuned models align the representations between parallel sentences, across languages. Moreover, we demonstrated that the lower part of the model is critical for cross-language transfer but hardly impacts the same-language performance. In this section, we show that the alignment measured plays a critical role in cross-lingual transfer. As seen in Figure 2 in the case of English to Russian (and in Figures 6-8 in the Appendix for other l"
2021.eacl-main.189,2020.acl-main.493,0,0.0195689,"ion of monolingual corpora across multiple languages, (ii) fine-tuning the model on a specific task in the source language, and (iii) using the finetuned model on a target language. The success of this approach is remarkable, and in contrast to the standard cross-lingual pipeline, the model sees neither aligned data nor task-specific annotated data in the target language at any training stage. The source of such a successful transfer is still largely unexplained. Pires et al. (2019) hypothesize that these models learn shared multilingual representations during pretraining. Focusing on syntax, Chi et al. (2020) recently showed that the multilingual version of BERT (mBERT) (Devlin et al., 2019), encodes linguistic properties in shared multilingual sub-spaces. Recently, Gonen et al. (2020) suggest that mBERT learns a language encoding component and an abstract cross-lingual component. In this work, we are interested in understanding the mechanism that leads mBERT to perform zero-shot cross-lingual transfer. More specifically, we ask what parts of the model and what mechanisms support cross-lingual transfer? By combining behavioral and structural analyses (Belinkov et al., 2020), we show that mBERT ope"
2021.eacl-main.189,P18-1198,0,0.0250893,"elates significantly with the cross-lang gap for all three tasks, both on the fine-tuned and pretrained models. The spearman correlation for the fine-tuned models are 0.76, 0.75 and 0.47 for parsing, POS and NER, respectively.5 In summary, our results show that the cross-lingual alignment is highly correlated with the cross-lingual transfer. 5 Discussion Understanding the behavior of pretrained language models is currently a fundamental challenge in NLP. A popular approach consists of probing the intermediate representations with external classifiers (Alain and Bengio, 2017; Adi et al., 2017; Conneau et al., 2018) to measure if a specific layer captures a given property. Using this technique, Tenney et al. (2019) showed that BERT encodes linguistic properties in the same order as the “classical NLP pipeline”. However, probing techniques only indirectly explain the behavior of a model and do not explain the relationship between the information captured in the representations and its effect on the task (Elazar et al., 2020). Moreover, recent works have questioned the usage of probing as an interpretation tool (Hewitt and Liang, 2019; Ravichander et al., 2020). This motivates our approach to combine a str"
2021.eacl-main.189,D19-6106,0,0.0254583,"andom-Init 4-7 Random-Init 8-11 Random-Init 0-3 2 4 6 *52.03 *34.66 8 Hidden Layer Index 10 12 Figure 2: Cross-Lingual similarity (CKA) of the representations of a fine-tuned model on NER with and w/o RANDOM-INIT between English (source) and Russian (target). The higher the score the greater the similarity. less of the language and task, and hints on an alignment that occurs in the lower part of the model. Interestingly, the same trend is also observed in the pretrained model, suggesting that the fine-tuning step preserves the multilingual alignment. These results do not match the findings of Singh et al. (2019), who found no language alignment across layers, although they inspected Natural Language Inference, a more “high-level task” (Dagan et al., 2005; Bowman et al., 2015). We leave the inspection of this mismatch to future work. 4.3 Better Alignment Leads to Better Cross-Lingual Transfer In the previous section we showed that fine-tuned models align the representations between parallel sentences, across languages. Moreover, we demonstrated that the lower part of the model is critical for cross-language transfer but hardly impacts the same-language performance. In this section, we show that the al"
2021.eacl-main.189,P11-2120,0,0.0231124,"lly diverse languages and multiple domains to support our hypothesis. 1 Introduction Zero-shot Cross-Lingual transfer aims at building models for a target language by reusing knowledge acquired from a source language. Historically, it has been tackled with a two-step standard crosslingual pipeline (Ruder et al., 2019): (1) Building a shared multilingual representation of text, typically by aligning textual representations across languages. This step can be done using feature extraction (Aone and McKee, 1993; Schultz and Waibel, 2001) as with the delexicalized approach (Zeman and Resnik, 2008; Søgaard, 2011) or using word embedding techniques (Mikolov et al., 2013; Smith et al., 2017) by projecting monolingual embeddings onto a shared multilingual embedding space, this step requiring explicit supervision signal in the target language in the form of features or parallel data. (2) Training a task-specific model using supervision on a source language on top of the shared representation. Recently, the rise of multilingual language models entailed a paradigm shift in this field. Multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019) have been shown to perform efficient"
2021.eacl-main.189,D19-1077,0,0.0323671,"ojecting monolingual embeddings onto a shared multilingual embedding space, this step requiring explicit supervision signal in the target language in the form of features or parallel data. (2) Training a task-specific model using supervision on a source language on top of the shared representation. Recently, the rise of multilingual language models entailed a paradigm shift in this field. Multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019) have been shown to perform efficient zero-shot cross-lingual transfer for many tasks and languages (Pires et al., 2019; Wu and Dredze, 2019). Such transfer relies on three-steps: (i) pretraining a mask-language model (e.g. Devlin et al. (2019)) on the concatenation of monolingual corpora across multiple languages, (ii) fine-tuning the model on a specific task in the source language, and (iii) using the finetuned model on a target language. The success of this approach is remarkable, and in contrast to the standard cross-lingual pipeline, the model sees neither aligned data nor task-specific annotated data in the target language at any training stage. The source of such a successful transfer is still largely unexplained. Pires et a"
2021.iwpt-1.15,W17-6507,1,0.881486,"notation is richer than in UD 2.7. Besides improvements in the officially released versions of the individual treebanks, a few other things have changed in comparison to the IWPT 2020 task. The English data now includes the GUM treebank (its enhanced annotation was not present in UD 2.7 but it was being prepared for UD 2.8 and it was ready in time for the shared task). As in 2020, we include two French treebanks whose enhanced annotation is still not included in the official UD releases, but the annotation is more conservative this year, omitting the extra labels for diathesis neutralization (Candito et al., 2017) and surface vs deep syntax markers. Still, some enhancements in French go slightly beyond the official UD guidelines (see below for details). In Polish, we now harmonize the relation subtypes in the three treebanks so that merging them into one dataset is no longer an issue. Finally, we omit the Chukchi treebank, which is new in UD 2.7 and has enhanced graphs, but the graphs are there only 147 conj conj obj nsubj nummod punct orphan cc orphan Sue has 5 euros , Pat 6 and Kim 3 Figure 1: A basic tree of a gapping structure. conj conj obj nsubj nummod punct nsubj obj cc nsubj obj Sue has 5 euros"
2021.iwpt-1.15,2021.iwpt-1.24,0,0.0565891,"Missing"
2021.iwpt-1.15,2020.acl-main.747,0,0.0880101,"ng task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021) and UNIPI (Attardi et al., 2021) use a method that learns weights for the scores obtained from various layers of the BERT model to be used as input for the biaffine parser. Most teams reduce the number of edge labels during training by de-lexic"
2021.iwpt-1.15,2021.iwpt-1.18,0,0.0205018,"This should be seen as a diagnostic only, and is intended to gain further insights into the capability of various systems to deal with challenging phenomena, such as the proper analysis of phenomena occurring in the context of coordination and ellipsis. 7 Approaches The predominant approach to obtaining the enhanced dependency graph is to use a biaffine function, i.e., predicting for each pair of nodes how likely it is that they are in a parent-child relation. There is wide variety in the way the final annotation graph is obtained, and ensuring that the result is valid (i.e. connected). GREW (Guillaume and Perrier, 2021) uses manually constructed rewrite rules to map basic UD into EUD, while FASTPARSE (Anderson and Gómez-Rodríguez, 2021) and NUIG (Choudhary and O’riordan, 2021) reformulate the task as a sequence-labeling task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of th"
2021.iwpt-1.15,2021.iwpt-1.17,0,0.05987,"Missing"
2021.iwpt-1.15,2021.iwpt-1.16,0,0.0440323,"Missing"
2021.iwpt-1.15,2020.lrec-1.497,1,0.896496,"Missing"
2021.iwpt-1.15,2021.iwpt-1.22,0,0.0523448,"Missing"
2021.iwpt-1.15,W18-6012,0,0.397597,"Missing"
2021.iwpt-1.15,W13-3728,0,0.40752,"nd subsequent work have shown that considerable progress has been made in multilingual dependency parsing. For enhanced dependency parsing, there are additional challenges. The enhanced representation is a connected directed graph, possibly containing cycles, while the bulk of dependency parsing work still focuses on rooted trees. The set of labels to be predicted is also much larger, as some enhanced dependency labels incorporate the lemma of certain dependents. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Nyblom et al., 2013; Schuster et al., 2017; Nivre et al., 2018). Moreover, most state-of-the-art work in dependency parsing uses a graph-based approach, where 146 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 146–157 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen"
2021.iwpt-1.15,2020.conll-shared.0,0,0.0794056,"Missing"
2021.iwpt-1.15,K19-2001,0,0.064997,"Missing"
2021.iwpt-1.15,S15-2153,1,0.846515,"Missing"
2021.iwpt-1.15,S14-2008,1,0.870583,"Missing"
2021.iwpt-1.15,2020.acl-demos.14,0,0.236593,"mpare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task. 1 2 Introduction Universal Dependencies (UD) (Nivre et al., 2020) is a framework for cross-linguistically consistent treebank annotation that has so far been applied to 114 languages. UD defines two levels of annotation, the basic trees and the enhanced graphs (EUD) (Schuster and Manning, 2016). There are several good parsers that can predict the basic trees (including tokenization and morphology) for previously unseen text (Straka et al., 2016; Qi et al., 2020). Two large shared tasks on basic UD parsing were organized at CoNLL (Zeman et al., 2017, 2018). Enhanced UD parsing attracted comparatively less attention until the shared task organized at IWPT 2020 (Bouma et al., 2020). The present paper describes a second instance of that task, organized as a part of the 17th International Conference on Parsing Technologies1 (IWPT), collocated with ACL-IJCNLP 2021. Like in the previous year, the evaluation was done on datasets covering 17 languages from four language familiies. This paper is a follow-up of the overview paper of the previous instance of the"
2021.iwpt-1.15,L16-1376,0,0.284241,"Missing"
2021.iwpt-1.15,2021.iwpt-1.23,0,0.012184,"cted). GREW (Guillaume and Perrier, 2021) uses manually constructed rewrite rules to map basic UD into EUD, while FASTPARSE (Anderson and Gómez-Rodríguez, 2021) and NUIG (Choudhary and O’riordan, 2021) reformulate the task as a sequence-labeling task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021"
2021.iwpt-1.15,L16-1680,0,0.0641786,"Missing"
2021.iwpt-1.15,2021.eacl-demos.10,0,0.427708,"Missing"
2021.iwpt-1.15,2021.iwpt-1.20,0,0.0398853,"kenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021) and UNIPI (Attardi et al., 2021) use a method that learns weights for the scores obtained from various layers of the BERT model to be used as input for the biaffine parser. Most teams reduce the number of edge labels during training by de-lexicalizing edge labels. Dependency paths involving an empty node are us"
2021.iwpt-1.15,K18-2001,1,0.752485,"Missing"
2021.naacl-main.38,2020.findings-emnlp.223,0,0.0412259,"ni, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experiments can be found in Appendix E. Model Arabic Russian Japanese Original Script → Latin Script POS LAS NER 96.4 → 94.9 98.1 → 96.0 97.4 → 95.7 82.9 → 78.8 88.4 → 84.5 88.5 → 86.9 87.8 → 80.9 88.1 → 86.0 61.5 → 55.6 Table 5: mBERT TASK -T UNED on high resource languages for POS tagging, parsing and NER."
2021.naacl-main.38,2020.osact-1.2,0,0.0231597,"cting the abilities of pretrained multilingual language models 1 Introduction to be used for low-resource languages. We dub those categories Easy, Intermediate and Hard. Language models are now a new standard to build state-of-the-art Natural Language ProcessHard languages include both stable and endaning (NLP) systems. In the past year, monolingual gered languages, but they predominantly are lanlanguage models have been released for more than guages of communities that are majorly under20 languages including Arabic, French, German, served by modern NLP. Hence, we direct our attenand Italian (Antoun et al., 2020; Martin et al., 2020; tion to these Hard languages. For those languages, de Vries et al., 2019; Cañete et al., 2020; Kuratov we show that the script they are written in can be and Arkhipov, 2019; Schweter, 2020, inter alia). a critical element in the transfer abilities of preAdditionally, large-scale multilingual models cov- trained multilingual language models. Translitering more than 100 languages are now available erating them leads to large gains in performance (XLM-R by Conneau et al. (2020) and mBERT outperforming non-contextual strong baselines. To by Devlin et al. (2019)). Still, most"
2021.naacl-main.38,E14-4029,0,0.0224963,"trained languages such as Arabic, Russian or Japanese, mBERT is not able to compete with the performance reached when using the script seen during pretraining. Transliterating the Arabic script and the Cyrillic script to Latin does not automatically improve mBERT performance as it does for Sorani, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experime"
2021.naacl-main.38,W18-2413,0,0.0172977,"compete with the performance reached when using the script seen during pretraining. Transliterating the Arabic script and the Cyrillic script to Latin does not automatically improve mBERT performance as it does for Sorani, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experiments can be found in Appendix E. Model Arabic Russian Japanese Original Script → Latin S"
2021.naacl-main.38,W19-7803,0,0.0131002,"tion about their scripts, language families, and amount of available raw data can be found in the Appendix in Table 12. 3.1 Raw Data To perform pretraining and fine-tuning on monolingual data, we use the deduplicated datasets from the OSCAR project (Ortiz Suárez et al., 2019). OSCAR is a corpus extracted from a Common Crawl Web snapshot.2 It provides a significant amount of data for all the unseen languages we work with, except for Buryat, Meadow Mari, Erzya and Livvi for which we use Wikipedia dumps and for Narabizi, Naija and Faroese, for which we use data collected by Seddah et al. (2020), Caron et al. (2019) and Biemann et al. (2007) respectively. 3.2 Non-contextual Baselines For parsing and POS tagging, we use the UDPipe future system (Straka, 2018) as our baseline. This model is a LSTM-based (Hochreiter and Schmidhuber, 1997) recurrent architecture trained with pretrained static word embedding (Mikolov et al., 2013) (hence our non-contextual characterization) along with character-level embeddings. This system was ranked in the very first positions for parsing and tagging in the CoNLL shared task 2018 (Zeman and Hajiˇc, 2018). For NER we use the LSTM-CRF model with character and word level embed"
2021.naacl-main.38,D19-1433,0,0.0229135,"z Suárez et al. (2020) showed that pretraining ELMo models (Peters et al., 2018) on less than 1GB of text leads to state-of-the-art performance while Martin et al. (2020) showed that pretraining a BERT model on as few as 4GB of diverse enough data results in state-of-the-art performance. Micheli et al. (2020) further demonstrated that decent performance was achievable with only 100MB of raw text data. Adapting large-scale models for low-resource languages Multilingual language models can be used directly on unseen languages, or they can also be adapted using unsupervised methods. For example, Han and Eisenstein (2019) successfully used unsupervised model adaptation of the English BERT model to Early Modern English for sequence labeling. Instead of fine-tuning the whole model, Pfeiffer et al. (2020) recently showed that adapter layers (Houlsby et al., 2019) can be injected into multilingual language models to provide parameter efficient task and language transfer. Still, as of today, the availability of monolingual or multilingual language models is limited to approximately 120 languages, leaving many languages without access to valuable NLP technology, although some are spoken by millions of people, includ"
2021.naacl-main.38,2020.findings-emnlp.118,0,0.325304,"Missing"
2021.naacl-main.38,2020.acl-main.747,0,0.526302,"Bambara (spoken guage models on a large amount of raw data by around 5 million people in Mali and neighborhas become a new norm to reach state-of-theing countries) are not covered by any available art performance in NLP. Still, it remains unclear language models at the time of writing. how this approach should be applied for unseen languages that are not covered by any available Even if training multilingual models that cover large-scale multilingual language model and more languages and language varieties is tempting, for which only a small amount of raw data is the curse of multilinguality (Conneau et al., 2020) generally available. In this work, by comparmakes it an impractical solution, as it would require ing multilingual and monolingual models, we to train ever larger models. Furthermore, as shown show that such models behave in multiple ways by Wu and Dredze (2020), large-scale multilingual on unseen languages. Some languages greatly language models are sub-optimal for languages that benefit from transfer learning and behave simiare under-sampled during pretraining. larly to closely related high resource languages whereas others apparently do not. Focusing In this paper, we analyze task and lang"
2021.naacl-main.38,2020.acl-main.560,0,0.0441456,"lable at https://github.com/benjami n-mlr/mbert-unseen-languages.git diate and the Easy languages. 448 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 448–462 June 6–11, 2021. ©2021 Association for Computational Linguistics • We show that Hard languages can be better addressed by transliterating them into a betterhandled script (typically Latin), providing a promising direction towards making multilingual language models useful for a new set of unseen languages. 2 Background and Motivation As Joshi et al. (2020) vividly illustrate, there is a large divergence in the coverage of languages by NLP technologies. The majority of the 6500+ of the world’s languages are not studied by the NLP community, since most have few or no annotated datasets, making systems’ development challenging. The development of such models is a matter of high importance for the inclusion of communities, the preservation of endangered languages and more generally to support the rise of tailored NLP ecosystems for such languages (Schmidt and Wiegand, 2017; Stecklow, 2018; Seddah et al., 2020). In that regard, the advent of the Uni"
2021.naacl-main.38,P17-1178,0,0.309625,"ges by NLP technologies. The majority of the 6500+ of the world’s languages are not studied by the NLP community, since most have few or no annotated datasets, making systems’ development challenging. The development of such models is a matter of high importance for the inclusion of communities, the preservation of endangered languages and more generally to support the rise of tailored NLP ecosystems for such languages (Schmidt and Wiegand, 2017; Stecklow, 2018; Seddah et al., 2020). In that regard, the advent of the Universal Dependencies project (Nivre et al., 2016) and the WikiAnn dataset (Pan et al., 2017) have greatly increased the number of covered languages by providing annotated datasets for more than 90 languages for dependency parsing and 282 languages for NER. Regarding modeling approaches, the emergence of multilingual representation models, first with static word embeddings (Ammar et al., 2016) and then with language model-based contextual representations (Devlin et al., 2019; Conneau et al., 2020) enabled transfer from high to low-resource languages, leading to significant improvements in downstream task performance (Rahimi et al., 2019; Kondratyuk and Straka, 2019). Furthermore, in t"
2021.naacl-main.38,N18-1202,0,0.0649653,"it comes to low-resource languages, one direction is to simply train contextualized embedding models on whatever data is available. Another option is to adapt/fine-tune a multilingual pretrained model to the language of interest. We briefly discuss these two options. of pretraining data seems to correlate with downstream task performance (e.g. compare BERT and RoBERTa (Liu et al., 2020)), several attempts have shown that training a model from scratch can be efficient even if the amount of data in that language is limited. Indeed, Ortiz Suárez et al. (2020) showed that pretraining ELMo models (Peters et al., 2018) on less than 1GB of text leads to state-of-the-art performance while Martin et al. (2020) showed that pretraining a BERT model on as few as 4GB of diverse enough data results in state-of-the-art performance. Micheli et al. (2020) further demonstrated that decent performance was achievable with only 100MB of raw text data. Adapting large-scale models for low-resource languages Multilingual language models can be used directly on unseen languages, or they can also be adapted using unsupervised methods. For example, Han and Eisenstein (2019) successfully used unsupervised model adaptation of the"
2021.naacl-main.38,2020.emnlp-main.617,0,0.107921,"Missing"
2021.wnut-1.22,W18-1817,0,0.0125703,"(e.g. inconsistent casing or usernames) as well as many OOVs denoting URL, mentions, hashtags or more generally named entities (NE). Most of the time, OOVs are exactly the same in the source and target sentences. 2 NMT Models In our experiments, we use three translation models. The first two models are standard NMT models that take as input BPE tokenized sentences: the model used in (Michel and Neubig, 2018a), a Seq2seq bi-LSTM architecture with global attention decoding as implemented in XNMT (Neubig et al., 2018) as well as a vanilla Transformer model as implemented in the OpenNMT toolkit (Klein et al., 2018). We also consider a char-based model, namely the char2char of Lee et al. (2017). Using char-based models which are, by nature, openvocabulary to translate UGC is intuitively appealing as these models are designed specifically to address the problem of translating OOVs and to 1 https://gitlab.inria.fr/seddah/paral lel-french-social-mediabank 2 Models parameters are detailed in the appendix. deal with noisy input (Belinkov and Bisk, 2018). As the Seq2seq model we consider in our experiments is not able to translate OOVs, we introduce, as part of our translation pipeline, a postprocessing step i"
2021.wnut-1.22,W18-6319,0,0.0231638,"ng out-of-domain data. These results seem to indicate that, counterintuitively, translating UGC does not raise any specific challenges. We however believe that they are biased by the evaluation metric used: as UGC contains many mentions, URLs emoticons, or named entities that are the same in the source and in the target sentence, B LEU scores estimated on a canonical and on a non-canonical can not be directly compared: B LEU scores on non-canonical data are artificially high as systems are rewarded for simply coping source tokens, which is the most natural 3 All B LEU scores are calculated by Post (2018)’s SacreBleu using the intl tokenization 190 A Corpus Annotated with UGC Specificities The PMUMT corpus To understand the impact of UGC peculiarities, we manually annotated 400 source sentences sampled from the PFSMB: one of the authors, fluent in French and with good knowledge of UGC, has identified spans in the sentence that differ from canonical French and characterized these specificities using the fine-grained typology of Sanguinetti et al. (2020) (see Table 4). Since the whole annotation process was done by a single person, no inter-annotator agreement can be calculated. Nevertheless, re"
2021.wnut-1.22,W04-3250,0,0.616691,"Missing"
2021.wnut-1.22,Q17-1026,0,0.0133858,"ns, hashtags or more generally named entities (NE). Most of the time, OOVs are exactly the same in the source and target sentences. 2 NMT Models In our experiments, we use three translation models. The first two models are standard NMT models that take as input BPE tokenized sentences: the model used in (Michel and Neubig, 2018a), a Seq2seq bi-LSTM architecture with global attention decoding as implemented in XNMT (Neubig et al., 2018) as well as a vanilla Transformer model as implemented in the OpenNMT toolkit (Klein et al., 2018). We also consider a char-based model, namely the char2char of Lee et al. (2017). Using char-based models which are, by nature, openvocabulary to translate UGC is intuitively appealing as these models are designed specifically to address the problem of translating OOVs and to 1 https://gitlab.inria.fr/seddah/paral lel-french-social-mediabank 2 Models parameters are detailed in the appendix. deal with noisy input (Belinkov and Bisk, 2018). As the Seq2seq model we consider in our experiments is not able to translate OOVs, we introduce, as part of our translation pipeline, a postprocessing step in which the translation hypothesis is aligned with the source and <UNK&gt; tokens a"
2021.wnut-1.22,L18-1275,0,0.0237229,"Missing"
2021.wnut-1.22,D18-1050,0,0.164885,"s studying the robustness of NMT systems by adding artificial noise to canonical corpora, PMUMT is made of attested UGC examples. Using this framework, we conduct several experiments on three out-of-the-box NMT architectures in a zero-shot scenario, to measure more precisely than what was possible before the impact of the different kinds of UGC specificities on translation quality. Surprisingly enough, our experiments (§3) on natural data show that out-of-the-box models exhibit unexpected strong robustness against several kinds of noise, questioning several results reported in the literature (Michel and Neubig, 2018a; Belinkov and Bisk, 2018). We believe that this data set and its associated evaluation framework will pave the way for a better understanding of the interactions at play in neural machine translation of noisy user-generated content contexts. This work takes a critical look at the evaluation of user-generated content (UGC) automatic translation. The well-known specificities of UGC (high rate of OOVs, rare, grammatical constructs, ...) raise many challenges for Machine Translation and has been the topic of many recent works (Rosales Núñez et al., 2019; Specia et al., 2020). Several UGC paralle"
2021.wnut-1.22,W19-6101,1,0.910707,"l results reported in the literature (Michel and Neubig, 2018a; Belinkov and Bisk, 2018). We believe that this data set and its associated evaluation framework will pave the way for a better understanding of the interactions at play in neural machine translation of noisy user-generated content contexts. This work takes a critical look at the evaluation of user-generated content (UGC) automatic translation. The well-known specificities of UGC (high rate of OOVs, rare, grammatical constructs, ...) raise many challenges for Machine Translation and has been the topic of many recent works (Rosales Núñez et al., 2019; Specia et al., 2020). Several UGC parallel corpora (Michel and Neubig, 2018a; Rosales Núñez et al., 2019) have been introduced to evaluate the robustness of MT, some of which, such as (Fujii et al., 2020), are specially annotated to identify UGC idiosyncrasies allow- 2 Testing Out-of-the-Box NMT models ing to measure the impact of a given specificity. on UGC Our analyses (§2), indeed, show that measuring the average-case performance using a standard metric 2.1 Experimental Setting on a UGC test set falls far short of giving a reliable image of the UGC translation quality: explaining Training"
2021.wnut-1.22,C12-1149,1,0.75196,"OpenSubtitles ⋄ MTNT News OpenTest 9.9 17.1 15.4 21.8 24.0 21.2 27.5 29.1 27.4 14.7 16.4 16.4 7.1 13.9 18.1 8.8 PFSMB † † MTNT News OpenTest 17.1 26.1 27.5 27.2 28.5 28.3 19.6 24.5 26.7 28.2 28.2 31.4 23.8 25.7 17.8 26.3 ⋄ Table 2: B LEU scores for our models. The † symbol indicates the UGC test sets, and ⋄ in-domain test sets. UGC Test Sets To evaluate the different NMT models, we consider two data sets of manually translated UGC: MTNT (Michel and Neubig, 2018a) and the Parallel French Social Media Bank corpus 1 (PFSMB) (Rosales Núñez et al., 2019) which extends the French Social Media Bank (Seddah et al., 2012) with English translations. These two data sets raise many challenges for MT systems: they notably contain characters that have not been seen in the training data (e.g. emojis), rare character sequences (e.g. inconsistent casing or usernames) as well as many OOVs denoting URL, mentions, hashtags or more generally named entities (NE). Most of the time, OOVs are exactly the same in the source and target sentences. 2 NMT Models In our experiments, we use three translation models. The first two models are standard NMT models that take as input BPE tokenized sentences: the model used in (Michel and"
2021.wnut-1.22,W18-1818,0,0.0128551,"ain characters that have not been seen in the training data (e.g. emojis), rare character sequences (e.g. inconsistent casing or usernames) as well as many OOVs denoting URL, mentions, hashtags or more generally named entities (NE). Most of the time, OOVs are exactly the same in the source and target sentences. 2 NMT Models In our experiments, we use three translation models. The first two models are standard NMT models that take as input BPE tokenized sentences: the model used in (Michel and Neubig, 2018a), a Seq2seq bi-LSTM architecture with global attention decoding as implemented in XNMT (Neubig et al., 2018) as well as a vanilla Transformer model as implemented in the OpenNMT toolkit (Klein et al., 2018). We also consider a char-based model, namely the char2char of Lee et al. (2017). Using char-based models which are, by nature, openvocabulary to translate UGC is intuitively appealing as these models are designed specifically to address the problem of translating OOVs and to 1 https://gitlab.inria.fr/seddah/paral lel-french-social-mediabank 2 Models parameters are detailed in the appendix. deal with noisy input (Belinkov and Bisk, 2018). As the Seq2seq model we consider in our experiments is not"
2021.wnut-1.22,P02-1040,0,0.111502,"Missing"
2021.wnut-1.22,W15-3049,0,0.0603752,"Missing"
2021.wnut-1.23,P16-2058,0,0.0218934,"or the present work. In this work, we explore two character-based translation models. The first model we consider, charCNN is a classic encoder-decoder in which the encoder uses character-based embeddings in combination with convolutional and highway layers to replace the standard look-up based word representations. The model considers, as input, a stream of words (i.e. it assumes the input has been tokenized beforehand) and tries to learn a word representation that is more robust to noise by unveiling regularities at the character level. This architecture was initially proposed by Kim et al. (2016) for language modeling; Costa-jussà and Fonollosa (2016) shows 3 Datasets how it can be used in an NMT system and report improvements up to 3 B LEU (Papineni et al., 2002) Training sets Due to the lack of a large paralpoints when translating from a morphologically- lel corpus of noisy sentences, we train our sysrich language, German, to English. tems with ‘standard’ parallel datasets, namely the The second model we consider does not rely corpora used in the WMT campaign (Bojar et al., on an explicit segmentation into words: Lee et al. 2016) and the OpenSubtitles corpus (Lison (2017) introduce"
2021.wnut-1.23,C18-1055,0,0.361613,"ct of UGC idiosyncrasies; • we demonstrate that char-based neural machine translation models are extremely sensitive to unknown and rare characters on both synthetic data and noisy user-generated content; • we show how an overlooked hyper-parameter drastically improve char-based MT models robustness to natural noise while maintaining the in-domain level of performance. 2 Character-level NMT on WMT or IWSLT tasks that consider texts that mostly qualify as canonical with very few spelling or grammatical errors. The impact of noise on charCNN and char2char has been evaluated by Belinkov and Bisk (2018) and Ebrahimi et al. (2018b). By adding noise to canonical texts (the TEDTalk dataset). Their results show that the different character levels models fail to translate even moderately noisy texts when trained on ‘clean’ data and that it is necessary to train a model on noisy data to make it robust. Note that, as explained in Section 3, there is no UGC parallel corpus large enough to train a NMT model and we must rely on the models’ ability to learn, from canonical data only, noise- and error-resistant representations of their input that are robust to the noise and errors found in UGCs. This is"
2021.wnut-1.23,L18-1275,0,0.0604981,"Missing"
2021.wnut-1.23,N13-1037,0,0.21978,"(NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti 1 https://github.com/josecar25/char_bas et al., 2020). This is why, focusing more on the ed_NMT-noisy_UGC 199 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 199–211 November 11, 2021. ©2021 Association for Computational Linguistics in-depth evalua"
2021.wnut-1.23,P16-1100,0,0.0404045,"Missing"
2021.wnut-1.23,N10-1060,0,0.25775,"enables Neural Machine Translation (NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti 1 https://github.com/josecar25/char_bas et al., 2020). This is why, focusing more on the ed_NMT-noisy_UGC 199 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 199–211 November 11, 2021. ©2021 Association for Co"
2021.wnut-1.23,2020.coling-main.521,0,0.0352267,"differences in performance can be explained 206 and because of our annotated UGC data set are able to provide a more precise view on the phenomena at stake when processing natural noisy-input. Similarly to Durrani et al. (2019) what concluded for morpho-syntactic classification tasks, our results show that a more fined-grained granularity of learning representations, characters over BPEs, provides a higher robustness to certain types of noise. Contrary to the both aforementioned works, our study is performed using annotated real-world noisy UGC, which proved crucial for our study. In line to Fujii et al. (2020) findings, where finegrained NMT granularity provided robustness advantages when processing misspelling, our results show that the best and worst translations’ specificities distribution point to a better performance of char2char for the missing diacritics category, giving insights on more specific types of misspellings that affect performance. Continuing this research track, we broaden the spectrum of studying UGC specificities, exploring the effects of the vocabulary training size and show that tuning it can achieve better results when translating noisy UGC. This simple hyper-parameter choic"
2021.wnut-1.23,W18-2709,0,0.0219605,"we thus show that these models are unable to perform an easy copy task due to their poor handling of unknown and rare characters. By adjusting the vocabulary size parameter, we drastically improve the robustness of our character-based model without causing a large drop in in-domain performance. Our contributions are as follows: • we provide an annotated data set1 that enables Neural Machine Translation (NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomen"
2021.wnut-1.23,W18-1817,0,0.0228932,"nt in the input. BPE-based models We use as our baselines two standard NMT models that consider tokenized sentences as input. The first one is a seq2seq bi-LSTM architecture with global attention decoding. The seq2seq model was trained using the XNMT toolkit (Neubig et al., 2018).5 It consists of a 2-layered Bi-LSTM layers encoder and a 2-layered Bi-LSTM decoder. It considers, as input, word embeddings of size 512 and each LSTM units has 1,024 components. Our second baseline model is a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings that are 512-dimensional, a feed-forward layers made of 2,048 units and 8 self-attention heads. Unknown Token Replacement One of the peculiarities of our UGC datasets is that they contain a many OOVs denoting URL, mentions, hashtags, or more generally named entities: for instance several sentences of the PFSMB mention the game “Flappy Bird” or the TV show “Teen Wolf”. Most of the time, these OOVs are exactly the same in the source and target sentences and consequently, the source 3 https://github.com/harvardnlp/seq2seq -attn 4 https://github.com/n"
2021.wnut-1.23,W17-3204,0,0.0465083,"Missing"
2021.wnut-1.23,I17-1003,0,0.0154061,"able 3), we used the ‘standard’ character vocabulary size (namely 300 characters) that was used in (Lee et al., 2017) and, to the best of our knowledge, in all following works. vocab. size PFSMB MTNT News OpenTest† 90 85 80 75 70 65 23.9 23.9 23.9 24.5 24.6 22.7 25.8 25.3 25.8 25.9 25.4 25.5 18.7 19.9 18.3 17.8 17.8 18.0 26.6 26.9 26.6 26.3 26.3 26.4 Table 7: B LEU results for MT of char2char with reduced vocabulary size. Systems trained on OpenSubs. † marks in-domain test set. by drop- and over-translation phenomena, two wellidentified limits of NMT (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). An analysis of Figure 3, which displays the ratio between the hypothesis and reference lengths for the different vocabulary sizes, seems to confirm this hypothesis as it appears that the vocabulary size parameter provides control over the translation hypothesis length and consequently, a way to limit these drop- and over-translation phenomena. PFSMB 1.1 Length ratios 25 19.8 B LEU score 30 PFSMB blind Transformer 29.7 char2char 1.08 1.05 OpenSubTest 1.07 1.04 1.04 1.03 0.99 1 1 0.95 0.92 0.89 0.9 0.89 0.84 300 90 85 80 75 70 Character vocabulary size 0.85 65 Figure 3: Reference/hypothesis le"
2021.wnut-1.23,Q17-1026,0,0.379022,"failure once such characters are encountered. We further confirm this behavior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation. 1 Introduction noise axis, char-based models appear to offer a natural solution to this problem (Luong and Manning, 2016; Ling et al., 2015): indeed these open vocabulary models are designed specifically to address the OOV problem. In this work, we explore the ability of out-of-thebox character-based NMT models (Lee et al., 2017) to address the challenges raised by UGC translation. While character-based models may seem promising for such task, to the best of our knowledge, they have only been tested either on data sets in which noise has been artificially added through sampling an edited word error data set (Belinkov and Bisk, 2018; Ebrahimi et al., 2018a) and on canonical data set, in which they prove to be very effective for translating morphologically-rich languages with a high number of OOVs (Luong and Manning, 2016). However, our starting-points experiments show that character-based systems are outperformed by BP"
2021.wnut-1.23,W16-3905,1,0.812529,"may contain emojis model outperforms subword-level (i.e. BPE-based) that can even replace some words (e.g. ♥ can stands translation models on two WMT’15 tasks (de-en for the verb ‘love’ in sentences such as ‘I ♥ you’). and cs-en) and gives comparable performance on UGC productivity limits the pertinence of domain two tasks (fi-en and ru-en). Lee et al. (2017) ad- adaptation methods such as fine-tuning, as there ditionally report that in a multilingual setting, the will always be new forms that will not have been character-level encoder significantly outperforms seen during training (Martínez Alonso et al., 2016). the subword-level encoder on all the language pairs. This is why we focus here on zero-shot scenarios, These two models have been originally tested as we believe they provide a clearer experimental 200 protocol when it comes to study the impact of UGC specificities on MT models. Test sets We consider in our experiments two French-English test sets made of user-generated content. These corpora differ in the domain of their contents, their collection date, and in the way sentences were filtered to ensure they are sufficiently different from canonical data. The first one, the MTNT corpus (Miche"
2021.wnut-1.23,D16-1096,0,0.0268631,"at in our first experiments (reported in Table 3), we used the ‘standard’ character vocabulary size (namely 300 characters) that was used in (Lee et al., 2017) and, to the best of our knowledge, in all following works. vocab. size PFSMB MTNT News OpenTest† 90 85 80 75 70 65 23.9 23.9 23.9 24.5 24.6 22.7 25.8 25.3 25.8 25.9 25.4 25.5 18.7 19.9 18.3 17.8 17.8 18.0 26.6 26.9 26.6 26.3 26.3 26.4 Table 7: B LEU results for MT of char2char with reduced vocabulary size. Systems trained on OpenSubs. † marks in-domain test set. by drop- and over-translation phenomena, two wellidentified limits of NMT (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). An analysis of Figure 3, which displays the ratio between the hypothesis and reference lengths for the different vocabulary sizes, seems to confirm this hypothesis as it appears that the vocabulary size parameter provides control over the translation hypothesis length and consequently, a way to limit these drop- and over-translation phenomena. PFSMB 1.1 Length ratios 25 19.8 B LEU score 30 PFSMB blind Transformer 29.7 char2char 1.08 1.05 OpenSubTest 1.07 1.04 1.04 1.03 0.99 1 1 0.95 0.92 0.89 0.9 0.89 0.84 300 90 85 80 75 70 Character vocabulary siz"
2021.wnut-1.23,D18-1050,0,0.367858,"2016). the subword-level encoder on all the language pairs. This is why we focus here on zero-shot scenarios, These two models have been originally tested as we believe they provide a clearer experimental 200 protocol when it comes to study the impact of UGC specificities on MT models. Test sets We consider in our experiments two French-English test sets made of user-generated content. These corpora differ in the domain of their contents, their collection date, and in the way sentences were filtered to ensure they are sufficiently different from canonical data. The first one, the MTNT corpus (Michel and Neubig, 2018), is a multilingual dataset that contains French sentences collected on Reddit and translated into English by professional translators. The second one, the Parallel French Social Media Bank (PFSMB)2 , introduced in (Rosales Núñez et al., 2019) and made from a subpart of the French Social Media Bank (Seddah et al., 2012), consists of comments extracted from Facebook, Twitter and Doctissimo, a health-related French forum. Table 9 shows some examples of source sentences and reference translations extracted from these two corpora and illustrates the peculiarities of UGC and difficulties of transla"
2021.wnut-1.23,W18-1818,0,0.0236772,"r2char.4 It must be noted that the charCNN extracts character n-grams for each input word and predicts a word contained in the target vocabulary or a special token, &lt;UNK&gt;, otherwise, whereas the char2char is capable of open-vocabulary translation and does not generate &lt;UNK&gt; tokens, unless an out-of-vocabulary character (char-OOV) is present in the input. BPE-based models We use as our baselines two standard NMT models that consider tokenized sentences as input. The first one is a seq2seq bi-LSTM architecture with global attention decoding. The seq2seq model was trained using the XNMT toolkit (Neubig et al., 2018).5 It consists of a 2-layered Bi-LSTM layers encoder and a 2-layered Bi-LSTM decoder. It considers, as input, word embeddings of size 512 and each LSTM units has 1,024 components. Our second baseline model is a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings that are 512-dimensional, a feed-forward layers made of 2,048 units and 8 self-attention heads. Unknown Token Replacement One of the peculiarities of our UGC datasets is that they contain a many OOVs denoting URL,"
2021.wnut-1.23,P02-1040,0,0.110824,"which the encoder uses character-based embeddings in combination with convolutional and highway layers to replace the standard look-up based word representations. The model considers, as input, a stream of words (i.e. it assumes the input has been tokenized beforehand) and tries to learn a word representation that is more robust to noise by unveiling regularities at the character level. This architecture was initially proposed by Kim et al. (2016) for language modeling; Costa-jussà and Fonollosa (2016) shows 3 Datasets how it can be used in an NMT system and report improvements up to 3 B LEU (Papineni et al., 2002) Training sets Due to the lack of a large paralpoints when translating from a morphologically- lel corpus of noisy sentences, we train our sysrich language, German, to English. tems with ‘standard’ parallel datasets, namely the The second model we consider does not rely corpora used in the WMT campaign (Bojar et al., on an explicit segmentation into words: Lee et al. 2016) and the OpenSubtitles corpus (Lison (2017) introduce the char2char model that di- et al., 2018). The former contains canonical texts rectly maps a source characters sequence to a tar- (2.2M sentences), while the latter (9.2M"
2021.wnut-1.23,W18-6319,0,0.0117434,"post-processing step in which the translation hypothesis is aligned with the source sentence and the &lt;UNK&gt; tokens replaced by their corresponding aligned source tokens. For the seq2seq the alignments between the source and translation hypothesis are computed using an IBM2 model.7 . For the charCNN model, the alignments are deduced from the attention matrix. The char2char model is an open-vocabulary system that is able to generate new words when necessary. The vanilla Transformer implementation we use is able to copy unknown symbols directly. Table 3 reports the B LEU scores, as calculated by Post (2018)’s SacreBleu of the different models we consider, both on canonical and non-canonical test sets. Contrary to the first results of Michel and Neubig (2018), the quality of UGC translation does not appear to be so bad: the drop in performance observed on non-canonical corpora is of the same order of magnitude as the drop observed when translation models are applied to out-of-domain data. For instance, the B LEU score of a Transformer model trained on OpenSubtitles has the same order of magnitude on PFSMB, MTNT and news: on all these datasets, the performance dropped by roughly 4 B LEU points com"
2021.wnut-1.23,W19-6101,1,0.929305,"unable to perform an easy copy task due to their poor handling of unknown and rare characters. By adjusting the vocabulary size parameter, we drastically improve the robustness of our character-based model without causing a large drop in in-domain performance. Our contributions are as follows: • we provide an annotated data set1 that enables Neural Machine Translation (NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical leve"
2021.wnut-1.23,C12-1149,1,0.896433,"Machine Translation (NMT) models fall far short from being able to translate noisy UserGenerated Content (UGC): the quality of their translation is often even worse than that of a traditional phrase-based system (Khayrallah and Koehn, 2018; Rosales Núñez et al., 2019). In addition to ambiguous grammatical constructs and profusion of ellipsis, the main difficulty encountered when translating UGC is the high number of out-of-vocabulary tokens (OOVs) resulting from misspelled words, emoticons, hashtags, mentions, and all specific constructs used in online forums and social medias (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Sanguinetti et al., 2020). Some of those phenomena can be perceived as noise while the others are typical markers of language variation among speakers. Moreover, a certain amount of those same phenomena operate at the lexical level (either at the character, subword or word levels) (Sanguinetti 1 https://github.com/josecar25/char_bas et al., 2020). This is why, focusing more on the ed_NMT-noisy_UGC 199 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 199–211 November 11, 2021. ©2021 Association for Computational Linguisti"
2021.wnut-1.23,N19-1190,0,0.0313898,"Missing"
2021.wnut-1.47,2020.acl-main.740,0,0.0523348,"Missing"
2021.wnut-1.47,P18-1031,0,0.013994,"f NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pretrained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings. 1 Introduction Current state-of-the-art monolingual and multilingual language models require large amounts of data to be trained, showing limited performance on low-resource languages (Howard and Ruder, 2018; Devlin et al., 2019). They lead to state-of-the-art results on most NLP tasks (Devlin et al., 2018; Raffel et al., 2020). In order to achieve high performance, these models rely on transfer learning architectures: the language models need to be trained on First version submitted on August 27th, 2021. Final on October 1st, 2021. large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on"
2021.wnut-1.47,P19-1356,1,0.880254,"Missing"
2021.wnut-1.47,2020.acl-main.560,0,0.0122073,"corpus (Pires et al., 2019); it is still a challenge for unseen languages, especially low-resource ones. However, Muller et al. (2020) achieved promising results by performing unsupervised fine-tuning on small amounts of NArabizi data. We follow their approach by comparing the performance of our pipeline in two setups: M ODEL +MLM+TASK and M ODEL +TASK. We describe these setups in more details in section 6. Low-resource languages, by definition, face a lack of textual resources – annotated or not –, which makes it difficult for the NLP community to develop models and systems adapted to them (Joshi et al., 2020). The majority of the almost-7000 languages worldwide actually fall into the “lowresource” category. This makes the development of systems for low-resource languages necessary to widen the accessibility of NLP technology. For deep learning approaches, which depend on the availability of large data sets, the solution to the low-resource problem comes from the idea of transfer learning. Early instances of cross-lingual 3.2 Tokenization & Character-based models transfer learning rely on non-contextualised word embeddings (Ammar et al., 2016). More recently, Standard language models rely on a subw"
2021.wnut-1.47,P18-1007,0,0.111319,"tokenizais still limited compared to other languages since tion struggle to represent rare words (Schick and they are naturally under-sampled during the train- Schütze, 2020). Many research projects have foing process (Wu and Dredze, 2020). cused on improving subword tokenization. For exTo improve performance on a specific low- ample, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to train a language model on it from multiple segmentations of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data set by using a character-level model, inspired by a previous wordlevel open-vocabulary system (Peters et al., 2018a). This new model gets better results than vanilla BERT on multiple tasks from the medical domain. Furthermore, the authors claim that it is more robust to noise and misspellings. In the same vein, Ma et al. (2020a) combined character-aware and subword-based"
2021.wnut-1.47,lacheret-etal-2014-rhapsodie,0,0.0199201,"Missing"
2021.wnut-1.47,2021.ccl-1.108,0,0.0614088,"Missing"
2021.wnut-1.47,2020.coling-main.4,0,0.197654,"tions of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data set by using a character-level model, inspired by a previous wordlevel open-vocabulary system (Peters et al., 2018a). This new model gets better results than vanilla BERT on multiple tasks from the medical domain. Furthermore, the authors claim that it is more robust to noise and misspellings. In the same vein, Ma et al. (2020a) combined character-aware and subword-based information to improve robustness to spelling errors. This initiated a new wave of tokenizer-free models based on characters or bytes (Tay et al., 2021; Xue et al., 2021; Clark et al., 2021). The question of knowing if character-based language models can handle high language variability since they are supposed to be resilient to noise and spelling variations is crucial when dealing with non-normalized dialects and non-canonical forms of language as found on many user-generated content platforms. This is why we focus on this work on the analysis of"
2021.wnut-1.47,P13-2017,0,0.0170191,"Missing"
2021.wnut-1.47,2020.emnlp-main.632,0,0.0871156,"Missing"
2021.wnut-1.47,2021.naacl-main.38,1,0.841079,"Missing"
2021.wnut-1.47,2021.eacl-main.189,1,0.844585,"models rely on transfer learning architectures: the language models need to be trained on First version submitted on August 27th, 2021. Final on October 1st, 2021. large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on large data sets for pre-training is a severe issue for low-resource languages, despite the emergence of large and successful multilingual pre-trained language models (Muller et al., 2021b). This is especially the case for languages with unusual morphological and structural features, which struggle to take advantage from similarities with high-resource, well represented languages such as Romance and Germanic languages. In this work, we focus on one of such highly challenging languages, namely North-African dialectal Arabic. Its Latin transcription (Arabizi) displays a high level of linguistic variability1 , on top of scarce and noisy resource availability, making it a particularly challenging language for most NLP systems relying on pre-trained multilingual models (Muller et a"
2021.wnut-1.47,2020.lrec-1.497,0,0.0328786,"Missing"
2021.wnut-1.47,N18-1202,0,0.365215,"ple, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to train a language model on it from multiple segmentations of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data set by using a character-level model, inspired by a previous wordlevel open-vocabulary system (Peters et al., 2018a). This new model gets better results than vanilla BERT on multiple tasks from the medical domain. Furthermore, the authors claim that it is more robust to noise and misspellings. In the same vein, Ma et al. (2020a) combined character-aware and subword-based information to improve robustness to spelling errors. This initiated a new wave of tokenizer-free models based on characters or bytes (Tay et al., 2021; Xue et al., 2021; Clark et al., 2021). The question of knowing if character-based language models can handle high language variability since they are supposed to be resilient to noise and"
2021.wnut-1.47,2020.emnlp-main.617,0,0.0412432,"Missing"
2021.wnut-1.47,P19-1493,0,0.0231583,"ingual language model on the lowresource language corpus, also called cross-lingual transfer learning (Muller et al., 2021a). The first option can sometimes lead to decent performance, provided that the training corpus is diverse enough (Martin et al., 2020). When following the fine-tuning approach, unsupervised methods can be implemented to facilitate the transfer of knowledge (Pfeiffer et al., 2020). The most widely used unsupervised fine-tuning task is masked language modeling (MLM). This system has proven its efficiency between languages that have already been seen in the training corpus (Pires et al., 2019); it is still a challenge for unseen languages, especially low-resource ones. However, Muller et al. (2020) achieved promising results by performing unsupervised fine-tuning on small amounts of NArabizi data. We follow their approach by comparing the performance of our pipeline in two setups: M ODEL +MLM+TASK and M ODEL +TASK. We describe these setups in more details in section 6. Low-resource languages, by definition, face a lack of textual resources – annotated or not –, which makes it difficult for the NLP community to develop models and systems adapted to them (Joshi et al., 2020). The maj"
2021.wnut-1.47,P19-1561,0,0.0271624,"s of data has always been queson downstream tasks and languages (Gururangan tioned. While splitting texts into subwords based et al., 2020). Therefore, this technique is crucial for on their frequencies works well for English, modmultilingual applications, as most of the world’s els using this kind of tokenization struggle with languages lack large amount of labeled data (Con- noise, whether it is naturally present in the data neau et al., 2019; Eisenschlos et al., 2019; Joshi (Sun et al., 2020) or artificially generated to chalet al., 2020). However the performance of multi- lenge the model (Pruthi et al., 2019). Moreover, lingual language model on low-resource languages language models that use subword-based tokenizais still limited compared to other languages since tion struggle to represent rare words (Schick and they are naturally under-sampled during the train- Schütze, 2020). Many research projects have foing process (Wu and Dredze, 2020). cused on improving subword tokenization. For exTo improve performance on a specific low- ample, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to t"
2021.wnut-1.47,W19-6101,1,0.897095,"Missing"
2021.wnut-1.47,2021.wnut-1.23,1,0.827828,"Missing"
2021.wnut-1.47,2020.acl-main.107,1,0.934396,"m cross-lingual transfer during pre-training. However, such model is still pre-trained on sentences written in a single language and was not trained to handle the presence 1 Language variability, or language variation, is a term coming from socio-linguistics where, as stated by Nordquist (2019), it refers to regional, social or contextual differences in the ways that a particular language is used. These variations in user-generated content can be characterized through their prevalent idiosyncraisies when compared to canonical texts (Seddah et al., 2012a; Sanguinetti et al., 2020). 2 Following Seddah et al. (2020), we refer to the Arabizi version of North-African Arabic dialects as NArabizi. 423 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 423–436 November 11, 2021. ©2021 Association for Computational Linguistics of multiple languages in the same sentence (codeswitching), a frequent phenomenon in NArabizi. However both monolingual and multilingual model approaches bear the risk of being limited by a subword tokenization-based vocabulary when facing out-of-domain training data language, especially in high-variability noisy scenarios (El Boukkouri"
2021.wnut-1.47,C12-1149,1,0.416138,"g is used there is no significant performance improvement from cross-lingual transfer during pre-training. However, such model is still pre-trained on sentences written in a single language and was not trained to handle the presence 1 Language variability, or language variation, is a term coming from socio-linguistics where, as stated by Nordquist (2019), it refers to regional, social or contextual differences in the ways that a particular language is used. These variations in user-generated content can be characterized through their prevalent idiosyncraisies when compared to canonical texts (Seddah et al., 2012a; Sanguinetti et al., 2020). 2 Following Seddah et al. (2020), we refer to the Arabizi version of North-African Arabic dialects as NArabizi. 423 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 423–436 November 11, 2021. ©2021 Association for Computational Linguistics of multiple languages in the same sentence (codeswitching), a frequent phenomenon in NArabizi. However both monolingual and multilingual model approaches bear the risk of being limited by a subword tokenization-based vocabulary when facing out-of-domain training data language"
2021.wnut-1.47,D19-1102,0,0.012512,"al., 2020) has spread far and wide in NLP, 2018a). This allows the model to handle any word enabling high-performance zero-shot cross-lingual unseen in the training data, working in an “opentransfer for numerous tasks and languages. The vocabulary setting,” where words are represented main idea is to exploit a large amount of unlabeled by a combination of subwords from a pre-defined data to pre-train a model using a self-supervised list. On top of alleviating the issue of out-oftask, such as masked language modeling (Lam- vocabulary words, this approach allows the model ple and Conneau, 2019; Vania et al., 2019). This to handle sequences written in a language unseen pre-trained model is then fine-tuned on a much during training, as long as it uses the same script. smaller annotated data set and used for another lan- Therefore, subword tokenization is a crucial feaguage, domain or task. Strategic knowledge shar- ture of state-of-the-art models in NLP. But its suiting has been shown to improve the performance ability for all types of data has always been queson downstream tasks and languages (Gururangan tioned. While splitting texts into subwords based et al., 2020). Therefore, this technique is crucia"
2021.wnut-1.47,2021.naacl-main.40,0,0.0281491,"hlos et al., 2019; Joshi (Sun et al., 2020) or artificially generated to chalet al., 2020). However the performance of multi- lenge the model (Pruthi et al., 2019). Moreover, lingual language model on low-resource languages language models that use subword-based tokenizais still limited compared to other languages since tion struggle to represent rare words (Schick and they are naturally under-sampled during the train- Schütze, 2020). Many research projects have foing process (Wu and Dredze, 2020). cused on improving subword tokenization. For exTo improve performance on a specific low- ample, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to train a language model on it from multiple segmentations of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data set by using a character-level model, inspired by a previous wordlevel open-vocabulary system (Peters et al., 2018a)."
2021.wnut-1.47,2020.repl4nlp-1.16,0,0.0166793,"lack large amount of labeled data (Con- noise, whether it is naturally present in the data neau et al., 2019; Eisenschlos et al., 2019; Joshi (Sun et al., 2020) or artificially generated to chalet al., 2020). However the performance of multi- lenge the model (Pruthi et al., 2019). Moreover, lingual language model on low-resource languages language models that use subword-based tokenizais still limited compared to other languages since tion struggle to represent rare words (Schick and they are naturally under-sampled during the train- Schütze, 2020). Many research projects have foing process (Wu and Dredze, 2020). cused on improving subword tokenization. For exTo improve performance on a specific low- ample, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to train a language model on it from multiple segmentations of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data se"
C12-1149,W09-3821,1,0.874376,"cDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed independently from the Google W"
C12-1149,W09-1008,1,0.895823,"Missing"
C12-1149,W11-2905,1,0.787828,"64.14 69.21 63.09 68.63 64.89 79.70 55.90 64.13 - 58.71 65.48 - 57.27 64.80 83.81 64.34 72.69 96.44 T EST LR SET LP F1 Pos acc. OOVs 70.10 70.59 71.68 71.44 70.88 71.02 79.14 75.70 15.42 19.88 31.50 24.70 54.67 71.29 58.16 73.45 56.36 72.35 64.40 78.88 32.84 24.47 38.25 23.40 5.2 55.26 60.98 66.69 - 59.23 61.79 68.50 - 57.18 61.38 67.58 84.10 54.64 70.68 74.43 96.97 50.40 29.52 22.81 4.89 Table 7: Baseline parsing results split by sub corpora and noisiness level word clustering within a PCFG-LA framework. Indeed, we have successfully applied these techniques for French out-of-domain parsing (Candito et al., 2011), as well as for parsing noisy English web data (Seddah et al., 2012). On the longer term we intend to apply our normalization and correction module before parsing. The parser will then be provided with corrected tokens, closely matching our regular training data, instead of unedited ones. This will compensate the lack of user generated content large unlabeled corpora, still lacking for French. 7 Conclusion As mentioned earlier, the French Social Media Bank shares with the Google web bank a common will to extend the traditional treebank domain towards user generated content. Although of a smal"
C12-1149,C10-2013,1,0.848791,"validation and correction by two annotators followed by an adjudication step. • Functional annotation followed by manual validation and correction by two annotators followed by and adjudication step. 5.1 Pre-annotation strategies for the tokenization and POS layers As mentioned above, we used two different strategies for tokenization and POS pre-annotation, depending on the noisiness score. For less noisy corpora (those with a noisiness score below 1), we used a slightly extended version of the tokenization and sentence splitting tools from our standard FTB-based parsing architecture, Bonsai (Candito et al., 2010). This is because we want to have a tokenization that is as close as possible from the principles underlying the FTB’s tokenization. Next, we used the POS-tagger MORFETTE (Chrupała et al., 2008) as a pre-annotator. For corpora with a high noisiness score, we used a specifically developed pre-annotation process. This is because in such corpora, spelling errors are even more frequent, but also because 15 In the Google Web Treebank, the counterpart of our tag Y is the tag GW. 2450 the original tokens rarely match sound linguistic units, as can be seen on the example in Table 4 taken from the DOCT"
C12-1149,W10-1409,1,0.828577,"ed over time and posters. The next step will involve collecting large unlabeled corpora to perform experiments with selftraining techniques (McClosky and Charniak, 2008; Foster et al., 2011b) and unsupervised 16 Simple linear regressions lead to the following results: without the normalization and correction wrapper, the slope is -4.8 and the correlation coefficient is 0.77; with the wrapper, the slope is -7.2 with a correlation coefficient as high as 0.88 (coefficients of determination are thus respectively 0.59 and 0.77). 17 For convenience, we provide also baseline results on the FTB, see (Candito and Seddah, 2010). 2453 DEV DOCTISSIMO high noisiness other JEUXVIDEOS. COM T WIT TER high noisiness other FACEBOOK high noisiness other all FTB SET LR LP F1 Pos acc. OOVs 37.22 69.68 66.56 41.20 70.19 66.46 39.11 69.94 66.51 51.72 77.96 74.56 40.47 15.56 20.46 62.07 68.06 64.14 69.21 63.09 68.63 64.89 79.70 55.90 64.13 - 58.71 65.48 - 57.27 64.80 83.81 64.34 72.69 96.44 T EST LR SET LP F1 Pos acc. OOVs 70.10 70.59 71.68 71.44 70.88 71.02 79.14 75.70 15.42 19.88 31.50 24.70 54.67 71.29 58.16 73.45 56.36 72.35 64.40 78.88 32.84 24.47 38.25 23.40 5.2 55.26 60.98 66.69 - 59.23 61.79 68.50 - 57.18 61.38 67.58 84.1"
C12-1149,F12-2024,1,0.873511,"iness score ‘Forplay have disappeared for at least 6 months, that is there is almost none.’ Parseval F-measure metric between two functionally annotated set of parses. Agreements range between 93.4 for FACEBOOK data and 97.44 for JEUXVIDEOS.COM (Table 5) and are on the same range than the DCU’s Twitter corpus agreement score (Foster et al., 2011a). Similarly to that corpus, the disagreements involve fragments, interjections and the syntactic status to assign to meta-tokens elements. We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012). This small annotation error rate comes from the fact that the same team annotated both treebanks and was thus highly trained for that task. Maybe more importantly, social media sentences tend to be shorter than their edited counterparts so once POS tagging errors are solved, the annotation task is made relatively easier. DCU ’ S DOCTISSIMO T WIT TER T WIT TERB ANK 95.05 95.40 95.8 JEUXVIDEOS. COM FACEBOOK - 97.44 93.40 - Table 5: Inter Annotator agreement 6 Preliminary experiments Experimental Protocol In the following experiments, we used the FTB -UC as training data set, in its classical s"
C12-1149,chrupala-etal-2008-learning,0,0.0250729,"Missing"
C12-1149,Y09-1013,1,0.861344,"evelopment corpus (all subcorpora but for the noisy Facebook subcorpus) n-gram sequences involving unknown tokens or occurring at an unexpectedly high frequency; then we manually selected the relevant ones and provided them manually with a corresponding “correction”. The number of “corrected tokens” obtained by applying these rules might be different from the number of original tokens. In such cases, we use 1-to-n or n-to-1 mappings. For example, the rule ni a pa → n’ y a pas explicitely states that ni is an amalgam for n’ and y, whereas pas is the correction of pa. 4. We use the MElt tagger (Denis and Sagot, 2009), trained on the FTB -UC and the Lefff lexicon (Sagot, 2010), for POS-tagging the sequence of corrected “tokens”. 5. We apply a set of 15 generic and almost language-independent manually crafted rewriting rules, originally developed for English data (see below), that aim at assigning the correct POS to tokens that belong to categories not found in MElt’s training corpus, i.e., the FTB; for example, all URLs and e-mail addresses are post-tagged as proper nouns whatever the tag provided by MElt; likewise, all smileys get the POS for interjections. 6. We assign POS tags to the original tokens bas"
C12-1149,P11-1118,0,0.0227658,"aptation. Yet, this is far from being the case as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organized by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for"
C12-1149,N10-1060,0,0.537119,"is highlights the high difficulty of automatically processing such noisy data in a MRL. KEYWORDS: Treebanking, User Generated Content, Parsing, Social Media. Proceedings of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpor"
C12-1149,I11-1100,0,0.0885155,"Missing"
C12-1149,W07-2204,1,0.926577,"Missing"
C12-1149,W01-0521,0,0.0471571,". Proceedings of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (m"
C12-1149,P11-2008,0,0.148522,"Missing"
C12-1149,C10-1045,0,0.0162317,"o say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed independently from the Google Web Treebank (Bies et al.,"
C12-1149,P08-2026,0,0.0646701,"Missing"
C12-1149,N06-1020,0,0.0516947,"of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (micro-blogging) or async"
C12-1149,P06-1043,0,0.0320051,"of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (micro-blogging) or async"
C12-1149,P06-1055,0,0.162253,"Missing"
C12-1149,N07-1051,0,0.0137649,"provided by MElt for the corrected token is assigned to the corresponding original token. This architecture is now available as part of the MElt distribution. It was also applied on English web data in the context of the SANCL shared task on parsing web data (Petrov and McDonald, 2012), with state-of-the-art results (Seddah et al., 2012). 5.2 Annotation strategy for constituency and functional annotation Parse pre-annotation was achieved using a state-of-the-art statistical parser trained on the FTB UC, provided with the manually validated tagging. The parser we used was the Berkeley parser (Petrov and Klein, 2007) adapted to French (Crabbé and Candito, 2008). Note that when the validated pos tags were discarded by the parser, in case of too many unknown word-pos pairs, those were reinserted. To assess the quality of annotation, we calculated the inter annotator agreement using the 2451 Original tokens Gold corrected “tokens” Automatically corrected and POS-tagged “tokens” sa fé o moin 6 mois qe les preliminaires sont sauté c a dire qil yen a presk pa ça fait au_moins 6 mois que les préliminaires sont sautés c’est-à-dire qu’ il y en a presque pas ça/PRO fait/V au/P+D moins/ADV 6/DET mois/NC que/PROREL l"
C12-1149,sagot-2010-lefff,1,0.839342,") n-gram sequences involving unknown tokens or occurring at an unexpectedly high frequency; then we manually selected the relevant ones and provided them manually with a corresponding “correction”. The number of “corrected tokens” obtained by applying these rules might be different from the number of original tokens. In such cases, we use 1-to-n or n-to-1 mappings. For example, the rule ni a pa → n’ y a pas explicitely states that ni is an amalgam for n’ and y, whereas pas is the correction of pa. 4. We use the MElt tagger (Denis and Sagot, 2009), trained on the FTB -UC and the Lefff lexicon (Sagot, 2010), for POS-tagging the sequence of corrected “tokens”. 5. We apply a set of 15 generic and almost language-independent manually crafted rewriting rules, originally developed for English data (see below), that aim at assigning the correct POS to tokens that belong to categories not found in MElt’s training corpus, i.e., the FTB; for example, all URLs and e-mail addresses are post-tagged as proper nouns whatever the tag provided by MElt; likewise, all smileys get the POS for interjections. 6. We assign POS tags to the original tokens based on the mappings between corrected POStagged tokens and or"
C12-1149,W10-1401,1,0.834989,"Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organized by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treeban"
C12-1149,C08-1112,0,0.0270883,"Missing"
C12-1149,W09-3820,0,0.0221679,"of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed indep"
C12-1149,I05-1006,0,\N,Missing
C12-1149,N03-1031,0,\N,Missing
C12-1149,N10-1004,0,\N,Missing
candito-etal-2014-deep,candito-etal-2010-statistical,1,\N,Missing
candito-etal-2014-deep,de-marneffe-etal-2006-generating,0,\N,Missing
candito-etal-2014-deep,J93-2004,0,\N,Missing
candito-etal-2014-deep,W09-4624,0,\N,Missing
candito-etal-2014-deep,H94-1020,0,\N,Missing
candito-etal-2014-deep,P05-1011,0,\N,Missing
candito-etal-2014-deep,P04-1041,0,\N,Missing
candito-etal-2014-deep,W00-1436,0,\N,Missing
candito-etal-2014-deep,J05-1004,0,\N,Missing
candito-etal-2014-deep,W13-3724,0,\N,Missing
candito-etal-2014-deep,abeille-barrier-2004-enriching,0,\N,Missing
D19-5539,I13-1041,0,0.0212033,"lin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 297–306 c Hon"
D19-5539,W15-4319,0,0.535767,"Missing"
D19-5539,W15-4312,0,0.0422461,"Missing"
D19-5539,W15-4318,0,0.0327642,"Missing"
D19-5539,N19-1423,0,0.0848551,"Missing"
D19-5539,N19-1419,0,0.0313704,"y of its transformerbased architecture, these three aspects respectively enable BERT to elegantly cope with out-ofvocabulary words and to include contextual information at the token and at the sentence levels, while fully taking advantage of a training corpus containing billions of words. Without listing all of them, BERT successfully improved the state-of-the-art for a number of tasks such as Name-Entity Recognition, Question Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions),"
D19-5539,P19-1356,1,0.835492,"architecture, these three aspects respectively enable BERT to elegantly cope with out-ofvocabulary words and to include contextual information at the token and at the sentence levels, while fully taking advantage of a training corpus containing billions of words. Without listing all of them, BERT successfully improved the state-of-the-art for a number of tasks such as Name-Entity Recognition, Question Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard synta"
D19-5539,C12-1097,0,0.0676726,"Missing"
D19-5539,D18-1050,0,0.0264044,"ly in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 297–306 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics Speech Tagging, Syntactic Parsing, Name-Entity Recognition and Machine Translation. In this context and as impactful as BERT was shown to be, its ability to handle noisy inputs is still an open question2 . Indeed, as highlighted above, it was trained on highly edited texts, as expected from Wikipedia and BookCorpus sources, which differ from UGC at many levels of linguistic descriptions, a"
D19-5539,N13-1037,0,0.41747,"ion Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Te"
D19-5539,N10-1060,0,0.0328086,"h as Name-Entity Recognition, Question Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The"
D19-5539,D18-1542,0,0.134757,"Missing"
D19-5539,W17-4404,0,0.127361,"Missing"
D19-5539,N18-1078,0,0.0550727,"Missing"
D19-5539,N18-1202,0,0.150892,"Missing"
D19-5539,C12-1149,1,0.821731,"ty Recognition, Question Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy"
D19-5539,P11-1038,0,0.574297,"words. In this purpose, we make three contributions: • We design a WordPiece tokenizer that enforces alignment between canonical and noisy tokens. • We enhance the BERT architecture so that the model is able to add extra tokens or remove them when normalisation requires it. • We fine-tune the overall architecture with a novel noise-specific strategy. In a few words, our paper is the first attempt to successfully design a domain transfer model based on BERT in a low resource setting. 2 Related Work There is an extensive literature on normalizing text from UGC. The first systematic attempt was Han and Baldwin (2011). They released 549 tweets with their normalized word-aligned counterparts and the first result for a normalization system on tweets. Their model was a Support-Vector-Machine for detecting noisy words. Then a lookup and ngram based system would pick the best candidate among the closest ones in terms of edit and phonetic distances. Following this work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012), purely unsupervised approach (Yang and Eisenstein, 2013), or syllables level model (Xu et al., 2015). In 2"
D19-5539,W15-4311,0,0.528984,"is work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012), purely unsupervised approach (Yang and Eisenstein, 2013), or syllables level model (Xu et al., 2015). In 2015, on the occasion of the Workshop on Noisy User-Generated Text, a shared task on lexical normalization of English tweets was organized (Baldwin et al., 2015) for which a collection of annotated tweets for training and evaluation was released. We will refer it as the lexnorm15 dataset. A wide range of approaches competed. The best approach (Supranovich and Patsepnia, 2015) used a UGC feature-based CRF model for detection and normalization. In 2016, the MoNoise model (van der Goot and van Noord, 2017) significantly improved the Stateof-the-art with a feature-based Random Forest. The model ranks candidates provided by modules such as a spelling checker (aspell), a n-gram based language model and word embeddings trained on millions of tweets. In summary, two aspects of the past literature on UGC normalization are striking. First, all the past work is based on UGC-specific resources such as lexicons or large UGC corpora. Second, most successful models are modular i"
D19-5539,P15-1089,0,0.156879,"Han and Baldwin (2011). They released 549 tweets with their normalized word-aligned counterparts and the first result for a normalization system on tweets. Their model was a Support-Vector-Machine for detecting noisy words. Then a lookup and ngram based system would pick the best candidate among the closest ones in terms of edit and phonetic distances. Following this work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012), purely unsupervised approach (Yang and Eisenstein, 2013), or syllables level model (Xu et al., 2015). In 2015, on the occasion of the Workshop on Noisy User-Generated Text, a shared task on lexical normalization of English tweets was organized (Baldwin et al., 2015) for which a collection of annotated tweets for training and evaluation was released. We will refer it as the lexnorm15 dataset. A wide range of approaches competed. The best approach (Supranovich and Patsepnia, 2015) used a UGC feature-based CRF model for detection and normalization. In 2016, the MoNoise model (van der Goot and van Noord, 2017) significantly improved the Stateof-the-art with a feature-based Random Forest. The mod"
D19-5539,D13-1007,0,0.0270193,"izing text from UGC. The first systematic attempt was Han and Baldwin (2011). They released 549 tweets with their normalized word-aligned counterparts and the first result for a normalization system on tweets. Their model was a Support-Vector-Machine for detecting noisy words. Then a lookup and ngram based system would pick the best candidate among the closest ones in terms of edit and phonetic distances. Following this work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012), purely unsupervised approach (Yang and Eisenstein, 2013), or syllables level model (Xu et al., 2015). In 2015, on the occasion of the Workshop on Noisy User-Generated Text, a shared task on lexical normalization of English tweets was organized (Baldwin et al., 2015) for which a collection of annotated tweets for training and evaluation was released. We will refer it as the lexnorm15 dataset. A wide range of approaches competed. The best approach (Supranovich and Patsepnia, 2015) used a UGC feature-based CRF model for detection and normalization. In 2016, the MoNoise model (van der Goot and van Noord, 2017) significantly improved the Stateof-the-art"
D19-5553,D16-1025,0,0.0229355,"he case for the PBSMT models, as seen in Table 3. In this way, in Figure 2, we can notice that the highest improvement caused by our phonetic normalization pipeline is present in short sentences (between 1 and 10 words). It is worth noting that this is the only case where the Transformer outperforms PBSMT in this Figure. Hence, the higher overall Transformer BLEU score over PBSMT is certainly due to a relatively high successful normalization over the shortest sentences of the Cr#pbank test set. This agrees with the documented fact that NMT is consistently better than PBSMT on short sentences (Bentivogli et al., 2016) and, in this concrete example, it seems that the Transformer can take advantage of this when we apply our normalization pipeline. Additionally, these results could be regarded as evidence supporting that our proposed method performs generally System Blind Tests MTNT Cr#pbank Large - PBSMT Raw Large - PBSMT Phon. Norm 29.3 26.7 30.5 26.9 Small - Transformer Raw Small - Transformer Phon. Norm 25.0 24.5 19.0 18.3 M&N18 Raw M&N18 UNK rep. Raw 19.3 21.9 13.3 15.4 Table 4: BLEU score results comparison on the MTNT and Cr#pbank blind test sets. The G2P phonetizer has been used for normalization.M&N1"
D19-5553,Q17-1010,0,0.0154724,"a language model to select the best correction. Several works have explored different approaches to normalize noisy UGC in various languages. For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple model based on finding, for each token of the sentence, words with similar pronunciations and selecting the best spelling alternative, using a language model. More precisely, we propose a fourstep process: 1. for each word of the input sentence, we automatically generate its pronunciation. We consider all words in the input sentence as misspelled tokens are not necessarily OOVs (e.g. “j’ai manger” — literally “I have eat” — which must be corrected to “j"
D19-5553,D18-1542,0,0.144163,"Missing"
D19-5553,W18-1817,0,0.0154222,"ents To evaluate whether our approach improve the translation quality of UGC, we have processed all of our test sets, both UGC and canonical ones with our phonetic normalization pipeline (Section 3). The corrected input sentences are then translated by a phrase-based and NMT systems.7 We evaluate translation quality using S ACRE BLEU (Post, 2018). The MT baselines models were trained using the parallel corpora described in Section 4.3. We use 3 training data configurations in our experiments: WMT, Small OpenTestand Large 410 7 In our experiments we used Moses (Koehn et al., 2007) and OpenNMT (Klein et al., 2018). Crap WMT Small Large 20.5 28.9 30.0 PBSMT MTNT News Open Crap 22.5† 20.4 22.3 13.3 26.1† 27.4† 15.4 27.5 26.9 21.2 27.3 28.6 Transformer MTNT News 21.2 28.3 28.3 27.4† 26.7 26.6 Open 16.3 31.4† 31.5† Table 2: BLEU score results for our two benchmark models for the different train-test combinations. None of the test sets are normalized. The best result for each test set is marked in bold, in-domain scores with a dag. Crap, News and Open respectively stand for the Cr#pbank, NeswTest and OpenSubTest. Crap PBSMT MTNT News Open Crap Transformer MTNT News Open WMT 20.4 20.2 21.9† 13.4 15.0 20.4 26"
D19-5553,L18-1275,0,0.0174447,"ated into English by professional translators. We used their designated test set and added a blind test set of 599 sentences we sampled from the MTNT validation set. The Cr#pbank and MTNT corpora both differ in the domain they consider, their collection date, and in the way sentences were filtered to ensure they are sufficiently different from canonical data. 4.3 Canonical Parallel Corpora To train our MT systems, we use the ‘standard’ parallel data, namely the Europarl and NewsCommentaries corpora that are used in the WMT evaluation campaign (Bojar et al., 2016) and the OpenSubtitles corpus (Lison et al., 2018). We will discuss the different training data configurations for the MT experiments more in detail in Section 5. We also use the totality of the French part of these corpora to train a 5-gram language model with Knesser-Ney smoothing (Ney et al., 1994) that is used to score possible rewritings of the input sentence and find the best normalization, as we have discussed in Section 3. 5 Machine Translation Experiments To evaluate whether our approach improve the translation quality of UGC, we have processed all of our test sets, both UGC and canonical ones with our phonetic normalization pipeline"
D19-5553,W18-6319,0,0.0149493,"part of these corpora to train a 5-gram language model with Knesser-Ney smoothing (Ney et al., 1994) that is used to score possible rewritings of the input sentence and find the best normalization, as we have discussed in Section 3. 5 Machine Translation Experiments To evaluate whether our approach improve the translation quality of UGC, we have processed all of our test sets, both UGC and canonical ones with our phonetic normalization pipeline (Section 3). The corrected input sentences are then translated by a phrase-based and NMT systems.7 We evaluate translation quality using S ACRE BLEU (Post, 2018). The MT baselines models were trained using the parallel corpora described in Section 4.3. We use 3 training data configurations in our experiments: WMT, Small OpenTestand Large 410 7 In our experiments we used Moses (Koehn et al., 2007) and OpenNMT (Klein et al., 2018). Crap WMT Small Large 20.5 28.9 30.0 PBSMT MTNT News Open Crap 22.5† 20.4 22.3 13.3 26.1† 27.4† 15.4 27.5 26.9 21.2 27.3 28.6 Transformer MTNT News 21.2 28.3 28.3 27.4† 26.7 26.6 Open 16.3 31.4† 31.5† Table 2: BLEU score results for our two benchmark models for the different train-test combinations. None of the test sets are n"
D19-5553,P12-3011,0,0.0124361,"ble) normalization of the input sentence. 4. using a language model, we compute the probability to observe each alternative spelling of the sentence (note that, by construction, the input sentence is also contained in the lattice) and find the most probable path (and therefore potential normalization) of the input sentence. Note that finding the most probable path in a lattice can be done with a complexity proportional to the size of the sentence even if the lattice encodes a number of paths that grows exponentially with the sentence size (Mohri, 2002). In our experiments we used the OpenGRM (Roark et al., 2012) and OpenFST (Allauzen et al., 2007) frameworks that provide a very efficient implementation to score a lattice with a language model. Phonetic Correction Model To automatically process phonetic writing and map UGC to their correct spelling, we propose a simThis process can be seen as a naive spellchecker, in which we only consider a reduced set of variations, 408 tailored to the specificities of UGC texts. We will now detail the first two steps discussed above. Generating the pronunciation of the input words To predict the pronunciation of an input word, i.e. its representation in the Interna"
D19-5553,W16-3905,1,0.830195,"Missing"
D19-5553,W19-6101,1,0.876109,"Missing"
D19-5553,D18-1050,0,0.106173,"cted in the gold translations but some of the specificities of UGC were kept. For instance, idiomatic expressions were mapped directly to the corresponding ones in English (e.g. “mdr” (mort de rire, litt. dying of laughter) has been translated to “lol” and letter repetitions were also kept (e.g. “ouiii” has been translated to “yesss”). For our experiments, we have divided the Cr#pbank into two sets (test and blind) containing 777 comments each. This corpus can be freely downloaded at https: //gitlab.inria.fr/seddah/parsiti. The MTNT corpus We also consider in our experiments, the MTNT corpus (Michel and Neubig, 2018), a multilingual dataset that contains French sentences collected on Reddit and translated into English by professional translators. We used their designated test set and added a blind test set of 599 sentences we sampled from the MTNT validation set. The Cr#pbank and MTNT corpora both differ in the domain they consider, their collection date, and in the way sentences were filtered to ensure they are sufficiently different from canonical data. 4.3 Canonical Parallel Corpora To train our MT systems, we use the ‘standard’ parallel data, namely the Europarl and NewsCommentaries corpora that are u"
D19-5553,W14-3605,0,0.0306349,". For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple model based on finding, for each token of the sentence, words with similar pronunciations and selecting the best spelling alternative, using a language model. More precisely, we propose a fourstep process: 1. for each word of the input sentence, we automatically generate its pronunciation. We consider all words in the input sentence as misspelled tokens are not necessarily OOVs (e.g. “j’ai manger” — literally “I have eat” — which must be corrected to “j’ai mang´e” — “I have eaten”, the French words “manger” and “mang´e” having both the same pronunciation /m˜A.ge/); 2. using these phone"
D19-5553,C16-1328,0,0.0633057,"Missing"
D19-5553,C12-1149,1,0.803661,"set OpenSubTest NeswTest #sentences #tokens ASL TTR 2.2M 9.2M 34M 64.2M 57.7M 1.19B 29.7 6.73 6.86 0.20 0.18 0.25 11,000 3,003 66,148 68,155 6.01 22.70 0.23 0.23 Corpus #sentences #tokens ASL TTR 777 1,022 13,680 20,169 17.60 19.70 0.32 0.34 777 599 12,808 8,176 16.48 13.62 0.37 0.38 UGC test set Cr#pbank MTNT UGC blind test set Cr#pbank MTNT Table 1: Statistics on the French side of the corpora used in our experiments. TTR stands for Type-to-Token Ratio, ASL for average sentence length. 2019), consists of 1,554 comments in French, translated from an extension of the French Social Media Bank (Seddah et al., 2012) annotated with the following linguistic information: Part-of-Speech tags, surface syntactic representations, as well as a normalized form whenever necessary. Comments have been translated from French to English by a native French speaker with near-native English speaker capabilities. Typographic and grammar error were corrected in the gold translations but some of the specificities of UGC were kept. For instance, idiomatic expressions were mapped directly to the corresponding ones in English (e.g. “mdr” (mort de rire, litt. dying of laughter) has been translated to “lol” and letter repetition"
D19-5553,P16-1162,0,0.0335358,"TNT News Open Crap Transformer MTNT News Open WMT 20.4 20.4 21.7† 13.4 14.6 20.7 26.5† 16.1 Small 28.0 26.3 19.8 26.2† 28.5 28.8 25.6 31.4† Large 28.3 27.7 21.6 27.4† 27.5 28.6 25.8 31.5† (b) (Espeak) phonetizer. Table 3: BLEU score results for our three benchmark models on normalized test sets. The best result for each test set is marked in bold, in-domain scores with a dag. OpenTest, for which Table 1 reports some statistics. We will denote Small and Large the two OpenSubtitles training sets used in the MT experiments. For every model, we tokenize the training data using byte-pair encoding (Sennrich et al., 2016) with a 16K vocabulary size. BLEU scores for our normalized test sets are reported in Table 3a and Table 3b, for the G2P and Espeak phonetizers. Results of the unprocessed test sets are reported in Table 2. We present some UGC examples of positive and negative results along with their normalization and translation in Table 6. 6 when the normalized text is translated using the PBSMT model. Moreover, our trained G2P phonetizer achieved the best improvement over the Cr#pbank corpus, attaining +1.5 BLEU points compared to the baseline. On the other hand, the Espeak phonetizer produces the highest"
D19-5553,D17-1145,0,0.0243793,"al. (2017) proposed lat2seq, an extension of seq2seq models (Sutskever et al., 2014) able to encode several possible input possibilities by conditioning their GRU output to several predecessors’ paths. The main issue with this model is that it is unable to predict the score of choosing a certain path by using future scores, i.e, by considering words that come after the current 407 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 407–416 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics token to be potentially normalized. Sperber et al. (2017) introduced a model based on Tree-LSTMs (Tai et al., 2015), to correct outputs of an Automatic Speech Recognition (ASR) system. On the other hand, Le et al. (2008) use lattices composed of written subword units to improve recognition rate on ASR. However, none of the aforementioned works have focused on processing noisy UGC corpora and they do not consider our main hypotheses of using phonetizers to recover correct tokens. They aim to correct known tokens such that a neural language model chooses the best output when an uncertain input is present (typically words with similar pronunciation fro"
D19-5553,W11-2159,0,0.0235102,"detection of OOVs. More recently, van der Goot and van Noord (2018) achieved state-of-the-art performance on dependency parsing of UGC using lattices. Closely related to our work, Baranes (2015) explored several normalization techniques on French UGC. In particular, to recover from typographical errors, they considered a rule-based system, SxPipe (Sagot and Boullier, 2008), that produced lattices encoding OOVs alternative spelling and used a language model to select the best correction. Several works have explored different approaches to normalize noisy UGC in various languages. For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple mo"
D19-5553,P15-1150,0,0.0145024,"Missing"
D19-5553,N13-1050,0,0.0225529,"o our work, Baranes (2015) explored several normalization techniques on French UGC. In particular, to recover from typographical errors, they considered a rule-based system, SxPipe (Sagot and Boullier, 2008), that produced lattices encoding OOVs alternative spelling and used a language model to select the best correction. Several works have explored different approaches to normalize noisy UGC in various languages. For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple model based on finding, for each token of the sentence, words with similar pronunciations and selecting the best spelling alternative, using a language model. More precisely,"
D19-5553,D18-1097,0,0.0262461,"and Boullier, 2008), that produced lattices encoding OOVs alternative spelling and used a language model to select the best correction. Several works have explored different approaches to normalize noisy UGC in various languages. For instance, Stymne (2011) use Approximate String Matching, an algorithm based on a weighted Levenshtein edit distance to generate lattices containing alternative spelling of OOVs. Wang and Ng (2013) employ a Conditional Random Field and a beam-search decoding approach to address missing punctuation and words in Chinese and English social media text. More recently, Watson et al. (2018) proposed a neural sequence-tosequence embedding enhancing FastText (Bojanowski et al., 2017) representations with wordlevel information, which achieved state-of-the-art on the QALB Arabic normalization task (Mohit et al., 2014). 3 ple model based on finding, for each token of the sentence, words with similar pronunciations and selecting the best spelling alternative, using a language model. More precisely, we propose a fourstep process: 1. for each word of the input sentence, we automatically generate its pronunciation. We consider all words in the input sentence as misspelled tokens are not"
F12-2024,abeille-barrier-2004-enriching,0,0.275854,"Missing"
F12-2024,J92-4003,0,0.0401775,"Missing"
F12-2024,W09-3821,1,0.913196,"Missing"
F12-2024,candito-etal-2010-statistical,1,0.877951,"Missing"
F12-2024,W11-2905,1,0.840916,"Missing"
F12-2024,C10-2013,1,0.920944,"Missing"
F12-2024,A00-2018,0,0.350317,"Missing"
F12-2024,Y09-1013,0,0.157743,"Missing"
F12-2024,N10-1060,0,0.0568431,"Missing"
F12-2024,W07-2204,1,0.930035,"Missing"
F12-2024,P08-1068,0,0.0834377,"Missing"
F12-2024,J93-2004,0,0.0403036,"Missing"
F12-2024,P06-1043,0,0.10224,"Missing"
F12-2024,N03-4009,0,0.070537,"Missing"
F12-2024,N10-1003,0,0.0223409,"Missing"
F12-2024,N07-1051,0,0.0583566,"Missing"
F12-2024,W10-2606,0,0.0311906,"Missing"
F12-2024,sagot-2010-lefff,0,0.149665,"Missing"
F12-2024,N03-1031,0,0.157988,"Missing"
F12-2024,villemonte-de-la-clergerie-etal-2008-passage,0,0.166869,"Missing"
F14-2031,candito-etal-2010-statistical,1,0.895459,"Missing"
F14-2031,candito-etal-2014-deep,1,0.70798,"Missing"
F14-2031,F12-2024,1,0.886133,"Missing"
F14-2031,J05-1004,0,0.189665,"Missing"
J13-1003,P05-1038,0,0.0571577,"Missing"
J13-1003,P08-1067,0,0.0549641,"Missing"
J13-1003,P03-1054,0,0.0158058,"odern Standard Arabic (Semitic) and French (Romance), the last article of this special issue, by Green et al., may be seen as an applications paper, treating the task of MWE recognition as a side effect of a joint model for parsing and MWE identiﬁcation. The key problem here is knowing what to consider a minimal unit for parsing, and how to handle parsing in realistic scenarios where MWEs have not yet been identiﬁed. The authors present two parsing models for such a task: a factored model including a factored lexicon that integrates morphological knowledge into the Stanford Parser word model (Klein and Manning 2003), and a Dirichlet Process Tree Substitution Grammar based model (Cohn, Blunsom, and Goldwater 2010). The latter can be roughly described as Data Oriented Parsing (Bod 1992; Bod, Scha, and Sima’an 2003) in a Bayesian framework, extended to include speciﬁc features that ease the extraction of tree fragments matching MWEs. Interestingly, those very different models do provide the same range of performance when confronted with predicted morphology input. Additional important challenges that are exposed in the context of this study concern the design of experiments for cross-linguistic comparison i"
J13-1003,P95-1037,0,0.302286,"Missing"
J13-1003,J93-2004,0,0.0463151,"Missing"
J13-1003,P06-1055,0,0.078628,"een, de Marneffe, and Manning 2013 Kallmeyer and Maier 2013 Fraser et al. 2013 Goldberg and Elhadad 2013 Seeker and Kuhn 2013 Seeker and Kuhn 2013 Tsarfaty et al. Parsing Morphologically Rich Languages: predicate-argument constraints allows the authors to obtain more substantial gains from morphology. Fraser et al. also focus on parsing German, though in a constituency-based setting. They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of manual treebank annotations that bring the treebank grammar performance to the level of automatically predicted states learned by Petrov et al. (2006). As in the previous study, syncretism is shown to cause ambiguity that hurts parsing performance. To combat this added ambiguity, they use external information sources. In particular, they show different ways of using information from monolingual and bilingual data sets in a re-ranking framework for improving parsing accuracy. The bilingual approach is inspired by machine translation studies and exploits the variation in marking the same grammatical functions differently across languages for increasing the conﬁdence of a disambiguation decision in one language by observing a parallel non-ambi"
J13-1003,C04-1024,0,0.0319019,"French German Hebrew Hungarian 18 Constituency-Based Dependency-Based Green, de Marneffe, and Manning 2013 Marton, Habash, and Rambow 2013 Seeker and Kuhn 2013 Green, de Marneffe, and Manning 2013 Kallmeyer and Maier 2013 Fraser et al. 2013 Goldberg and Elhadad 2013 Seeker and Kuhn 2013 Seeker and Kuhn 2013 Tsarfaty et al. Parsing Morphologically Rich Languages: predicate-argument constraints allows the authors to obtain more substantial gains from morphology. Fraser et al. also focus on parsing German, though in a constituency-based setting. They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of manual treebank annotations that bring the treebank grammar performance to the level of automatically predicted states learned by Petrov et al. (2006). As in the previous study, syncretism is shown to cause ambiguity that hurts parsing performance. To combat this added ambiguity, they use external information sources. In particular, they show different ways of using information from monolingual and bilingual data sets in a re-ranking framework for improving parsing accuracy. The bilingual approach is inspired by machine translation studies and exploits the variation in mar"
J13-1003,A97-1014,0,0.288535,"Missing"
J13-1003,W10-1401,1,0.919998,"Missing"
J13-1003,W07-2219,1,0.883448,"Missing"
J13-1003,A00-2018,0,\N,Missing
J13-1003,C10-1011,0,\N,Missing
J13-1003,P97-1003,0,\N,Missing
J13-1003,W06-2920,0,\N,Missing
J13-1003,W08-2102,0,\N,Missing
J13-1003,P05-1022,0,\N,Missing
J13-1003,P08-1109,0,\N,Missing
J13-1003,C92-3126,0,\N,Missing
J13-1003,P99-1065,0,\N,Missing
J13-1003,P03-1013,0,\N,Missing
J13-1003,D07-1096,1,\N,Missing
J13-1003,N10-1115,0,\N,Missing
K17-3026,L16-1262,0,\N,Missing
K17-3026,W14-6111,1,\N,Missing
K18-2023,N15-1184,0,0.0693335,"Missing"
K18-2023,L16-1262,0,0.0200927,"Missing"
K18-2023,N18-1202,0,0.227629,"eate a high performing graph parser, we implement a large BiLSTM-LM network (independent of the ELMoLex parser) which is highly regularized to prevent data overfitting and able to learn useful features. Our BiLSTM-LM consumes both the word and tag embedding as input, which can be formally written as: LM (word) = vi xLM i LM (word) vi = LM (U P oS) ⊕ vi LM (f air) vi ⊕ , (3) LM (char) vi . LM (f air) LM (char) In equation 3, the notations vi , vi LM (U P oS) GP (f air) and vi are the counterparts of vi , GP (char) GP (U P oS) vi and vi respectively. Note that ELMo, as proposed in Peters et al. (2018), builds only on character embeddings, automatically inferring the PoS information in the lower layers of the LSTM network. Since we have less training data to work with, we feed the PoS information explicitly which helps in easening the optimization process of our BiLSTM-LM network. Given a sequence of n LM words, xLM 1 , . . . , xn , BiLSTM-LM learns by maximizing the log likelihood of forward LSTM and backward LSTM directions, which can be defined as: n ∑ − → − → (log Pr(xLM |xLM , . . . , xLM i 1 i−1 ; Θx ; Θ LST M , Θ s )) i=1 ← − ← − LM |xLM (4) + (log Pr(xLM i i+1 , . . . , xn ; Θx ; Θ"
K18-2023,C16-1030,0,0.0323741,"is problem, ELMoLex relies on four signals from the proposed embedding layer: fi_pud sv_pud cs_pud Table 1: Treebanks to source the training data for Delexicalized Parsing of a given target treebank. GP (lex) (vi ) can be computed as follows:9 GP (lex) vi = m ∑ GP (mf ) slex mfj vmfj . (7) j=1 In equation 7, slex mfj corresponds to the softmaxnormalized weight which is a learnable parameter for each available morphological feature (in this case, it is mfj ). The general idea to perform a weighted sum to extract relevant features has been previously studied in the context of sequence labeling (Rei et al., 2016) for integrating word and character level features. Combining the distributional knowledge of words along with the semantic lexicons has been extensively studied for estimating high quality word vectors, also referred to as ‘retrofitting’ in literature (Faruqui et al., 2015). 2.4 Delexicalized Parsing We perform delexicalized “language family” parsing for treebanks with less than 50 or no train sentences (as shown in Table 1). The delexicalized version of ELMoLex throws away word-level information such as vGP (word) and vGP (char) and works with the rest. The source treebanks are concatenated"
K18-2023,L18-1292,1,0.815553,"They exploit the relevant sub-parts of a word such as suffixes or prefixes to generate word representations. They can generalize to unknown words if these unknown words follow such generalizations. Otherwise, they fail to add any improvement (Sagot and Martínez Alonso, 2017) and we may need to look for other sources to complement the information provided by characterlevel embeddings. We term this problem as linguistic naivety. ELMoLex taps into the large inventory of morphological features (gender, number, case, tense, mood, person, etc.) provided by external resources, namely the UDLexicons (Sagot, 2018) lexicon collection, which cover words with an irregular morphology as well as words not present in the training data. Essentially, these lexicons consist of ⟨word, UPoS, morphological features⟩ triplets, which we query using ⟨word, UPoS⟩ pair resulting in one or more hits. When we attempt to integrate the information from these hits, we face the challenge of disambiguation as not all the morphological features returned by the query are relevant to the focal ⟨word, UPoS⟩ pair. ELMoLex relies on attention mechanism (Bahdanau et al., 2014) to select the relevant morphological features, thereby h"
K18-2023,W17-6304,1,0.857446,"Missing"
L16-1375,abeille-barrier-2004-enriching,0,0.0129854,"h a complex wh-marker est-ce que: case (4) yes/no questions of the form est-ce que + SENT: Est-ce que Paul a déjà mangé ? lit. ’Is-it that Paul has already eaten?’ (Has Paul already eaten?) case (5) form qui/qu’ est-ce que/qui + SENT-with-gap: Qu’est-ce que Paul a mangé? lit. ’What is-it that Paul has eaten?’ (What has Paul eaten?) case (6) form qu’ est-ce que + NP: Qu’est-ce que le platine? lit. ’What is-it that platine’ (What is platine) 3. Questions in French corpora We now focus on questions in the French typical corpora usable for training statistical parsers. The French treebank (F TB) (Abeillé and Barrier, 2004)) is the most used treebank for that purpose, being both the first and the biggest. Other treebanks were developped later, in particular some out-of-domain treebanks using the same annotation scheme : the S EQUOIA treebank (Candito and Seddah, 2012), a well-edited out-of-domain small treebank, and the F RENCH S OCIAL M EDIA BANK, F SMB (Seddah et al., 2012), which originates in web forums and social media content. As already noted for English by Judge et al. (2006), questions are generally under-represented in treebanks. Indeed, this observation is confirmed the figures presented in Table 2366"
L16-1375,W09-3821,1,0.856077,"dard French counterparts. We left the web forum questions unedited so that the difficulties of handling noisy questions can be correctly assessed. SOURCE TREC 08-11 Faq GVT/NGOs CLEF03 sub-total Web In order to obtain evaluation treebanks compatible with parsers trained on the F TB, we have used as basis the F TB annotation scheme and followed as much as possible the corresponding annotation guidelines for morphology, phrase structure and functional annotation (Abeillé et al., 2003). More precisely, we started from a slight modification of this annotation scheme, referred to as the F TB - UC (Candito and Crabbé, 2009) and added specific guidelines for handling idiosyncrasies tied to question-phrase specificities.5 As far as grammatical function tags are concerned, we used an additional function label DIS for dislocated phrases. Such phrases appear either at the beginning or the end of a clause, and are coreferent with a (redundant) clitic appearing on the verb. It can occur in declarative sentences (e.g. Paul les a mangées, les fraises lit. ’Paul CL-ACC-pl has eaten, the strawberries’ (Paul has eaten the strawberries), but in the F QB it appears massively in questions of the form Qu’est-ce que NP whose par"
L16-1375,F12-2024,1,0.935337,"n size between our corpus parts (see Table 2). Let us note that the TREC part of the French Question Bank (F QB) is aligned with the first 1893 sentences of the QB. Joining those resources could prove useful for the evaluation of some syntax based machine translation system if not for the bootstrapping of such systems. 3 4. Annotation Scheme http://www-rali.iro.umontreal.ca/rali/?q=node/9 Social Welfare (CAF), IRS (Trésors public), employment agency (Pôle Emploi), National Statistics Agency (INSEE), UNESCO 4.1. Annotation Methodology and Evaluation We followed the same annotation protocol as (Candito and Seddah, 2012). Namely, two annotators working on the output of two parsers (the Berkeley parser (Petrov et al., 2006) and the first-phase parser of Charniak (2000)) fed with gold input (generated from a previous annotation phase). Resulting corrected parses were then adjudicated. To assess the quality of annotation, we calculated the interannotator agreement using the Parseval F-measure metric between two functionally annotated set of parses (Table 3). 4 5 Should this paper be accepted, we will provide more details on the annotation scheme. 2367 F QB We note that our agreement scores are higher than those"
L16-1375,candito-etal-2010-statistical,1,0.859166,"1 (cf. case 6 listed in section 2.). In order to prepare a further deep syntax annotation layer, we also annotated all long distance dependencies using functional paths, following, among others, (Schluter and van Genabith, 2008; Chrupała, 2008). The motivation lies in the need to closely follow the F TB annotation scheme, therefore avoiding empty elements and traces. Other modifications such as assigning function labels to pre-terminal and participle phrases were applied so that a dependency conversion will be less sensitive to structural ambiguities than the original conversion developed by Candito et al. (2010a). SENT VN NP-ATS # OF SENTENCES 1893 196 200 2289 285 PROWH V Qu’ est PONCT Ssub-DIS -ce ? NP CLS-SUJ CS que DET le NC platine Figure 1: Dislocated example for lit. What is-it that platine? (What is platine?) Table 2: Source of F QB sentences. The difficulties gathering question data in French entailed a relatively unbalanced corpus, compared for example to the Question Bank (QB) (Judge et al., 2006), as shown by the divergence in size between our corpus parts (see Table 2). Let us note that the TREC part of the French Question Bank (F QB) is aligned with the first 1893 sentences of the QB."
L16-1375,C10-2013,1,0.930963,"1 (cf. case 6 listed in section 2.). In order to prepare a further deep syntax annotation layer, we also annotated all long distance dependencies using functional paths, following, among others, (Schluter and van Genabith, 2008; Chrupała, 2008). The motivation lies in the need to closely follow the F TB annotation scheme, therefore avoiding empty elements and traces. Other modifications such as assigning function labels to pre-terminal and participle phrases were applied so that a dependency conversion will be less sensitive to structural ambiguities than the original conversion developed by Candito et al. (2010a). SENT VN NP-ATS # OF SENTENCES 1893 196 200 2289 285 PROWH V Qu’ est PONCT Ssub-DIS -ce ? NP CLS-SUJ CS que DET le NC platine Figure 1: Dislocated example for lit. What is-it that platine? (What is platine?) Table 2: Source of F QB sentences. The difficulties gathering question data in French entailed a relatively unbalanced corpus, compared for example to the Question Bank (QB) (Judge et al., 2006), as shown by the divergence in size between our corpus parts (see Table 2). Let us note that the TREC part of the French Question Bank (F QB) is aligned with the first 1893 sentences of the QB."
L16-1375,A00-2018,0,0.573725,"QB. Joining those resources could prove useful for the evaluation of some syntax based machine translation system if not for the bootstrapping of such systems. 3 4. Annotation Scheme http://www-rali.iro.umontreal.ca/rali/?q=node/9 Social Welfare (CAF), IRS (Trésors public), employment agency (Pôle Emploi), National Statistics Agency (INSEE), UNESCO 4.1. Annotation Methodology and Evaluation We followed the same annotation protocol as (Candito and Seddah, 2012). Namely, two annotators working on the output of two parsers (the Berkeley parser (Petrov et al., 2006) and the first-phase parser of Charniak (2000)) fed with gold input (generated from a previous annotation phase). Resulting corrected parses were then adjudicated. To assess the quality of annotation, we calculated the interannotator agreement using the Parseval F-measure metric between two functionally annotated set of parses (Table 3). 4 5 Should this paper be accepted, we will provide more details on the annotation scheme. 2367 F QB We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012; Seddah et al., 2012). This can be due to the smaller average sentenc"
L16-1375,P15-1118,0,0.0152027,"nk outside English, bringing a new genre to the existing French data set. Because statistical parsing models are notoriously biased toward the domain of their training model, the availability of a treebank made of questions for French will help building more robust parsers, useful for example in syntaxaugmented question answering system. However, we showed in this work how this data set could be used to close the question genre out-of-domain gap. Once more unlabeled question data are made available for French, complementary techniques, such as uptraining (Petrov et al., 2010) or paraphrasing (Choe and McClosky, 2015), will help to further improve question parsing for French. A large part of the F QB being aligned with the QB (Judge et al., 2006), this treebank will pave the way for crosslinguistics work. The F QB is freely available at http: //alpage.inria.fr/FrenchQuestionBank. Acknowledgment We thanks our anonymous reviewers for their comments. We are grateful to Benoit Crabbé for his remarks on a earlier version of this work and to Corentin Ribeyre for his generous help. This work was funded by the Program ""Investissements d’avenir"" managed by the Agence Nationale de la Recherche ANR-10-LABX-0083 (Labe"
L16-1375,Y09-1013,0,0.0415277,"Missing"
L16-1375,P06-1063,0,0.0209364,"Missing"
L16-1375,nivre-etal-2006-maltparser,0,0.104961,"Missing"
L16-1375,P06-1055,0,0.0267484,"QB) is aligned with the first 1893 sentences of the QB. Joining those resources could prove useful for the evaluation of some syntax based machine translation system if not for the bootstrapping of such systems. 3 4. Annotation Scheme http://www-rali.iro.umontreal.ca/rali/?q=node/9 Social Welfare (CAF), IRS (Trésors public), employment agency (Pôle Emploi), National Statistics Agency (INSEE), UNESCO 4.1. Annotation Methodology and Evaluation We followed the same annotation protocol as (Candito and Seddah, 2012). Namely, two annotators working on the output of two parsers (the Berkeley parser (Petrov et al., 2006) and the first-phase parser of Charniak (2000)) fed with gold input (generated from a previous annotation phase). Resulting corrected parses were then adjudicated. To assess the quality of annotation, we calculated the interannotator agreement using the Parseval F-measure metric between two functionally annotated set of parses (Table 3). 4 5 Should this paper be accepted, we will provide more details on the annotation scheme. 2367 F QB We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012; Seddah et al., 2012)."
L16-1375,D10-1069,0,0.0429632,"ion We introduced the first QuestionBank outside English, bringing a new genre to the existing French data set. Because statistical parsing models are notoriously biased toward the domain of their training model, the availability of a treebank made of questions for French will help building more robust parsers, useful for example in syntaxaugmented question answering system. However, we showed in this work how this data set could be used to close the question genre out-of-domain gap. Once more unlabeled question data are made available for French, complementary techniques, such as uptraining (Petrov et al., 2010) or paraphrasing (Choe and McClosky, 2015), will help to further improve question parsing for French. A large part of the F QB being aligned with the QB (Judge et al., 2006), this treebank will pave the way for crosslinguistics work. The F QB is freely available at http: //alpage.inria.fr/FrenchQuestionBank. Acknowledgment We thanks our anonymous reviewers for their comments. We are grateful to Benoit Crabbé for his remarks on a earlier version of this work and to Corentin Ribeyre for his generous help. This work was funded by the Program ""Investissements d’avenir"" managed by the Agence Nation"
L16-1375,schluter-van-genabith-2008-treebank,0,0.0605489,"Missing"
L16-1375,C12-1149,1,0.88215,"(Petrov et al., 2006) and the first-phase parser of Charniak (2000)) fed with gold input (generated from a previous annotation phase). Resulting corrected parses were then adjudicated. To assess the quality of annotation, we calculated the interannotator agreement using the Parseval F-measure metric between two functionally annotated set of parses (Table 3). 4 5 Should this paper be accepted, we will provide more details on the annotation scheme. 2367 F QB We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012; Seddah et al., 2012). This can be due to the smaller average sentence length of the F QB, and to the fact that the annotators were already trained for the task. 6 A vs B 97.54 A vs Gold 95.72 POS B vs Gold 97.21 none F TB gold none gold w/o funct with funct Bracketing Fmeasure (all sent.) 83.85 86.09 81.67 83.50 65.21 69.90 74.4 76.06 w/o funct with funct Bracketing Fmeasure (≤20 sent) 84.16 86.40 88.07 90.48 65.43 69.87 78.84 80.91 Pos accuracy (all sent.) 92.05 98.98 97.29 99.93 Table 3: Inter-annotator agreement Table 4: Baseline phrase-based results (B KY). 5. Parsability of the FQB As we said earlier, the mo"
L16-1566,abeille-barrier-2004-enriching,0,0.019001,"Missing"
L16-1566,D15-1198,0,0.0286923,"Missing"
L16-1566,C14-1133,0,0.0608899,"Missing"
L16-1566,W13-2322,0,0.0242684,"Missing"
L16-1566,P14-1133,0,0.0557857,"Missing"
L16-1566,D13-1160,0,0.0454701,"Missing"
L16-1566,W14-2403,0,0.0220893,"Missing"
L16-1566,candito-etal-2010-statistical,0,0.0652301,"Missing"
L16-1566,candito-etal-2014-deep,1,0.774948,"Missing"
L16-1566,W05-0620,0,0.0187297,"Missing"
L16-1566,W03-1006,0,0.15127,"Missing"
L16-1566,W11-2924,0,0.0721923,"Missing"
L16-1566,P14-1134,0,0.158908,"ally-oriented tasks such as question-answering systems (Berant et al., 2013). This is because many predicate-argument dependencies, such as those arising for instance in control-verb constructions, it-cleft constructions, participle clauses and so on, are lacking from the annotation schemes underlying most data set used by surface syntactic parsers. Representing such constructs in a dependency scheme often leads to graph representations, which are a real challenge to predict, as shown, for example, by the performance obtained in Abstract Meaning Representation parsing (Banarescu et al., 2013; Flanigan et al., 2014; Artzi et al., 2015) or in graph-based semantic dependency parsing (Oepen et al., 2014). Even though some recent works have made good progresses in parsing full-fledged semantic structures (Beschke et al., 2014; Berant and Liang, 2014), they mostly focus on English.1 We propose to dig into that direction by relying on the recent release of a deep syntactic graphbank for French, the D EEP FTB, (Ribeyre et al., 2014a), whose deep syntactic graphs were automatically annotated on top of the dependency version of the French Treebank (Abeillé and Barrier, 2004; Candito et al., 2010). In order to dr"
L16-1566,S14-2082,0,0.01396,"governing a token. These give a wider pobj = 1 = 1 =1 context to the parser whereas the spines are viewed as de=2 3. Graph Parsing terministic supertags bringing a vertical context. Table 2 Schémaan desoverview traits syntaxiques. Le cercle accuracy autour de of PPour marque An increasing number of works have been proposed Figure over 8.9: presents of the expected fea- son appartenance à deux types de traits: BKY (fragment d’arbres) et Head Path. the last few years to cope with graphs (Sagae and Tsujii, tures via the scores of their respective source parsers. The 2008; Flanigan et al., 2014; Martins and Almeida, 2014), feature set is shown in Figure 2. whether acyclic or not. Because it has been shown to be one of the top performers on the SemEval 2014 shared task Head S Path Spine (Oepen et al., 2014), we reuse the higher-order arc-factored (w = 4) parser of (Martins and Almeida, 2014) (TSPARSER), which NP VP takes advantage of dual decomposition methods based on AD3 (Martins et al., 2011). Another motivation for using BKY D N V PP this parser is that we want to assess whether using predicted syntactic features, which do provide additional context, is beneficial even when using a globally optimized parser"
L16-1566,D11-1022,0,0.0378718,"Missing"
L16-1566,J08-2003,0,0.0912556,"Missing"
L16-1566,S14-2008,0,0.18444,"use many predicate-argument dependencies, such as those arising for instance in control-verb constructions, it-cleft constructions, participle clauses and so on, are lacking from the annotation schemes underlying most data set used by surface syntactic parsers. Representing such constructs in a dependency scheme often leads to graph representations, which are a real challenge to predict, as shown, for example, by the performance obtained in Abstract Meaning Representation parsing (Banarescu et al., 2013; Flanigan et al., 2014; Artzi et al., 2015) or in graph-based semantic dependency parsing (Oepen et al., 2014). Even though some recent works have made good progresses in parsing full-fledged semantic structures (Beschke et al., 2014; Berant and Liang, 2014), they mostly focus on English.1 We propose to dig into that direction by relying on the recent release of a deep syntactic graphbank for French, the D EEP FTB, (Ribeyre et al., 2014a), whose deep syntactic graphs were automatically annotated on top of the dependency version of the French Treebank (Abeillé and Barrier, 2004; Candito et al., 2010). In order to draw meaningful comparisons with recent results for English, we use the same types of feat"
L16-1566,P09-2010,0,0.0281423,"coordinated heads and (iv) inverted dependencies in the case of modifying verbs or adjectives (for instance, in (the French counterpart) of ’Children born after 2010 get free tickets’, the participle born both modifies the noun Children, and has this noun as (deep) subject). Moreover, semantically empty functional words are marked as such, and “shunted off” (for instance in ’Anna a parlé à Paul’ (Anna has talked to Paul), both the auxiliary and the prepo3563 sition à are marked as empty, and Paul is directly linked to TAG-based Dependencies (FRMG) As opposed to most the verb). previous works (Øvrelid et al., 2009), we use dependencies features extracted from a hand-written wide-coverage TAGAs with the DM corpus (Oepen et al., 2014), D EEP FTB 4 de la syntaxe de traits : based3 metagrammar (Fprofonde R MG, (de au La moyen Clergerie, 2010)).syntaxiques is comparable in the sense that the semantic arguments8.3. of Prédiction approche à l’état de l’art verbs and adjectives are made explicit, but it leans a litTree Fragments (BKY) These consist of fragments of tle less towards a semantic representation (hence the “deep syntactic constituency trees. They have been extracted ussyntactic” name). In particular"
L16-1566,P06-1055,0,0.0746912,"the impact of topologically different syntactic features 5. Experiments & Discussion extracted from the surfacic syntax and their respective combinations. Both our surfacic parsers use the French Experiments Table 3 displays the baseline scores, the Treebank (Abeillé et al., 2003) in its (Seddah et al., 2013) scores for each type of features separately and our best instance, with predicted POS and morphological features. models. As we can see, our baseline is weak especially The constituency features come from the Berkeley Parser in term of recall, leading us to believe that it is indeed dif(Petrov et al., 2006) trained in a 10-fold jackkniffing ficult to recover the deep structure. Whereas the parser exsetting. Respective parsers’ performance scores are shown plores a large part of the search space, it seems to need more in Table 2. 3 2 The authors wanted to differentiate graphs for X is impossible and an impossible X: if the copula is ignored, it only remains the same impossible −→ X dependency. Tree Adjoining Grammars (Joshi and Schabes, 1997). This parser generates dependency trees after disambiguation and conversion from a shared derivation forest (Villemonte De La Clergerie, 2013b). 3564 4 ARG2"
L16-1566,W12-4625,1,0.779531,"t Test set D EEP FTB DM B EST (TSPARSER ) B EST (DYAL OG -SR) 85.18 82.92 89.70 85.66 BASELINE (TSPARSER ) BASELINE (DYAL OG -SR) 80.79 75.42 88.08 83.91 TSPARSER ( SURF.)+ RULES 80.45 - Table 4: Comparison of baselines and best LF results for D EEP FTB and DM. (D EEP FTB’s best: FRMG + PATHS + SPINES & DM’s best: BN + SPINES + PATHS). Table 4 reports a comparison between the best results for D EEP FTB and DM and the baseline for both parsers. The last row includes results from the TSPARSER, trained on the F TB surface dependencies,6 whose outputs were fed to a tree-to-graph rewriting system (Ribeyre et al., 2012), following Ribeyre (2016).7 This setup provides slightly inferior performance than the TSPARSER baseline parser and is vastly over-performed by our best setup (by almost 5pt). Despite validating our approach, this leads us to wonder if a graph-to-graph rewriting system could not be developed to push the envelope even further. This is left for future work. Interestingly enough, except for the fact that our feature set generalizes well with another parser, we see that the best model for both corpora are of the same kind: mixing dependencies information with spinal trees and head paths. Even tou"
L16-1566,S14-2012,1,0.838229,"graph representations, which are a real challenge to predict, as shown, for example, by the performance obtained in Abstract Meaning Representation parsing (Banarescu et al., 2013; Flanigan et al., 2014; Artzi et al., 2015) or in graph-based semantic dependency parsing (Oepen et al., 2014). Even though some recent works have made good progresses in parsing full-fledged semantic structures (Beschke et al., 2014; Berant and Liang, 2014), they mostly focus on English.1 We propose to dig into that direction by relying on the recent release of a deep syntactic graphbank for French, the D EEP FTB, (Ribeyre et al., 2014a), whose deep syntactic graphs were automatically annotated on top of the dependency version of the French Treebank (Abeillé and Barrier, 2004; Candito et al., 2010). In order to draw meaningful comparisons with recent results for English, we use the same types of features as Ribeyre et al. (2015), which were extracted from the DM corpus (Oepen et al., 2014), an English predicate-argument structure corpus sharing similarities with the D EEP FTB. Despite the differences of language and annotation scheme, we observe that the same combination of different topolog1 An exception is the work of Bal"
L16-1566,N15-1007,1,0.89364,"hough some recent works have made good progresses in parsing full-fledged semantic structures (Beschke et al., 2014; Berant and Liang, 2014), they mostly focus on English.1 We propose to dig into that direction by relying on the recent release of a deep syntactic graphbank for French, the D EEP FTB, (Ribeyre et al., 2014a), whose deep syntactic graphs were automatically annotated on top of the dependency version of the French Treebank (Abeillé and Barrier, 2004; Candito et al., 2010). In order to draw meaningful comparisons with recent results for English, we use the same types of features as Ribeyre et al. (2015), which were extracted from the DM corpus (Oepen et al., 2014), an English predicate-argument structure corpus sharing similarities with the D EEP FTB. Despite the differences of language and annotation scheme, we observe that the same combination of different topolog1 An exception is the work of Ballesteros et al. (2014) on deep syntacting parsing for Spanish, but their work is restricted to tree structure parsing. ical syntactic information leads to the best models for both French and English. 2. Deep Syntactic French Annotation Scheme and Corpus DM C ORPUS D EEP FTB C ORPUS T RAIN D EV T RA"
L16-1566,C08-1095,0,0.0471372,"Missing"
L16-1566,W13-4917,1,0.902976,"Missing"
L16-1566,seddah-2010-exploring,1,0.81561,"ctic constituency trees. They have been extracted ussyntactic” name). In particular it sticks to (canonical) syning the same method as in (Carreras andHead Màrquez, 2005). tactic labels subj, obj, . . . instead of using numbered seS Path Spine Spinal Elementary Trees (SPINES) They consist of the mantic labels arg1, arg2, . . . . Also, in the case of a pred(w = 4) path of the maximal projection VP of a head in a constituency NP icate modifying one of its semantic argument (e.g. an attree. They have been extracted using a spine grammar tributive adjective), both the modifier dependency and the (Seddah, 2010) and percolation table ofBKY DybroD N the head predicative dependency are kept in the deep graph: for inV PP Johansen (2004). The spines are assigned in a deterministic stance for an attributive adjective like in a perfect day, day way. is taken as the subject of perfect, and perfect as a modiP NP Constituent Head Paths (PATHS) We use F R MG depenfier of day. This choice was made in order to keep both dencies to extract the shortest path between a token and its the predicate-argument structures and the general informaD N lexical head and include the path length w (i.e. the number tion structur"
L16-1566,W13-4906,1,0.907525,"Missing"
L16-1566,W13-5706,1,0.908969,"Missing"
L18-1608,W06-2920,0,0.370592,"Missing"
L18-1608,W02-1503,0,0.157572,"tactic parsing, in both pipeline and joint settings, and presenting new opportunities in the development of UD resources for low-resource languages. Keywords: Morphology, Universal Dependencies, Morphological Analysis, Morphological Ambiguity 1. Introduction The development of the universal dependencies (UD) framework and its treebank collection (Nivre et al., 2016; Nivre et al., 2017) follows many shared tasks and multilingual evaluation campaigns in which the linguistic representation schemes across different languages vary (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Butt et al., 2002; Zeman et al., 2012). The UD treebanks collection, in contrast, obeys a single set of annotation guidelines, and respects the discrepancies between surface input tokens and the output nodes in the syntax trees (a.k.a., the two-level representation principle.)1 The UD initiative has paved the way to the development of cross-lingual models for word segmentation, part-of-speech tagging and dependency parsing (Straka and Strakov´a, 2017), as well as cross-linguistic typological investigations (Futrell et al., 2015). Recently, the CoNLL 2017 Shared Task on Multilingual UD Parsing (Zeman et al., 20"
L18-1608,coltekin-2010-freely,1,0.858208,"Missing"
L18-1608,C16-1033,1,0.861239,"4), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices format to follow the proposed CoNLL-UL format, as the HEBLEX annotations have already been adapted to the treebank counterpart (More and Tsarfaty, 2017). For Turkish, we developed a new morphological analyzer based on TRmorph (C¸o¨ ltekin, 2010).8 The analyzer follows the segmentation and morphological analysis scheme of the UD Turkish treebank v2.0 (Sulubacak et al., 2016) and Turkish-PUD treebank (Zeman et al., 2017). These treebanks have employed a different se"
L18-1608,K17-3027,1,0.780254,"004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices format to follow the proposed CoNLL-UL format, as the HEBLEX annotations have already been adapted to the treebank counterpart (More and Tsarfaty, 2017). For Turkish, we developed a new morphological analyzer based on TRmorph (C¸o¨ ltekin, 2010).8 The analyzer follows the segmentation and morphological analysis scheme of the UD Turkish treebank v2.0 (Sulubacak et al., 2016) and Turkish-PUD treebank (Zeman et al., 2017). These treebanks have employed a different segmentation approach compared to the METU-Sabancı Turkish Treebank (Oflazer et al., 2003). In addition, form and lemma representations, POS tags and morphological tag sets have changed. The existing morphological analyzers are not compatible with this new representation. Thus we intro"
L18-1608,L16-1262,1,0.908557,"Missing"
L18-1608,pasha-etal-2014-madamira,1,0.772852,"w, and Turkish. For these morphological analyzers, their pre-existing morphological analyses adhere to schemes that differ from those employed in the respective UD treebanks. These discrepancies are due to differences between the morphological theories adopted by the UD treebanks developers and those employed by the developers of the morphological analyzers. Therefore, the adapted resources we provide are non-trivial to obtain, and required careful alignment of the morphosyntactic analyses with their UD treebank counterparts. For Arabic, we adapted the morphological analyzer used in MADAMIRA (Pasha et al., 2014), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of Mo"
L18-1608,L18-1292,1,0.841663,"L format. 3.2. Converted Morphological Lexicons As a complement to the CoNLL-UL-compatible analyzers described above, we have created a set of 53 CoNLL-ULcompatible morphological lexicons covering 38 languages, based on existing freely available resources.9 The source lexicons, the conversion processes and the resulting inventory of freely available CoNLL-UL lexicons are described https://conllul.github.io 3850 6 https://camel.abudhabi.nyu.edu/calima-star/ https://github.com/habeanf/yap 8 https://github.com/coltekin/TRmorph/tree/trmorph2 9 http://pauillac.inria.fr/˜sagot/udlexicons.html 7 in (Sagot, 2018).10 Here we only provide in Table 3 two examples converted from the Lefff , the Alexina lexicon for French. The first one illustrates the 1-to-1 case, with an entry converted from the following original entry: encodent encoder v P3p, which includes the wordform (i.e. the [source and tree] token) encodent ‘encode3pl.pres.ind ’, its lemma, its Lefff POS and its Lefff morphosyntactic tag. The other example illustrates the 1-to-m case with the source token auxquels, which is analyzable as reflecting the sequence of two tree tokens a` lesquels ‘to which’. 4. Related Work and Perspective Our work ov"
L18-1608,W13-4917,1,0.923267,"Missing"
L18-1608,Q15-1026,1,0.906774,"Missing"
L18-1608,K17-3009,0,0.0605054,"Missing"
L18-1608,C16-1325,1,0.883776,"Missing"
L18-1608,W17-1320,1,0.835643,"ctive UD treebanks. These discrepancies are due to differences between the morphological theories adopted by the UD treebanks developers and those employed by the developers of the morphological analyzers. Therefore, the adapted resources we provide are non-trivial to obtain, and required careful alignment of the morphosyntactic analyses with their UD treebank counterparts. For Arabic, we adapted the morphological analyzer used in MADAMIRA (Pasha et al., 2014), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices"
L18-1608,P12-2002,1,0.826771,"y. As a result of the latter, we can maintain a two-way compatibility promise: every morphological disambiguation in a UD v2 treebank can be represented as a CoNLLUL lattice, and every possible path in a CoNLL-UL lattice can serve as the syntactic words of a UD-annotated tree. Thus, we ease the burden on morphological and syntax parser research and development, such that they are relieved of adapting lexical resources (or their analyses) to UD-compliant morphology. The representation scheme for lattices used by the SPMRL shared task datasets (Seddah et al., 2013) and which were introduced by (Tsarfaty et al., 2012; Tsarfaty, 2013),12 allowed for annotating morphological ambiguity of these same languages. Seeker and C ¸ etino˘glu (2015) extended the SPMRL representation to accommodate marking the gold and optionally a predicted morphological analysis. Our proposal extends the latter with two additions: (i) we use the UD convention of specifying a surface token spanning multiple tree tokens; and (ii) we allow the specification of multiple anchors relating lattice arcs to tree tokens, for possibly grounding more than one syntactic tree (i.e., a forest) in the morphological lattice. Since we wanted to main"
L18-1608,P13-2103,1,0.786276,"atter, we can maintain a two-way compatibility promise: every morphological disambiguation in a UD v2 treebank can be represented as a CoNLLUL lattice, and every possible path in a CoNLL-UL lattice can serve as the syntactic words of a UD-annotated tree. Thus, we ease the burden on morphological and syntax parser research and development, such that they are relieved of adapting lexical resources (or their analyses) to UD-compliant morphology. The representation scheme for lattices used by the SPMRL shared task datasets (Seddah et al., 2013) and which were introduced by (Tsarfaty et al., 2012; Tsarfaty, 2013),12 allowed for annotating morphological ambiguity of these same languages. Seeker and C ¸ etino˘glu (2015) extended the SPMRL representation to accommodate marking the gold and optionally a predicted morphological analysis. Our proposal extends the latter with two additions: (i) we use the UD convention of specifying a surface token spanning multiple tree tokens; and (ii) we allow the specification of multiple anchors relating lattice arcs to tree tokens, for possibly grounding more than one syntactic tree (i.e., a forest) in the morphological lattice. Since we wanted to maintain compatibilit"
L18-1608,zeman-etal-2012-hamledt,0,0.319438,"Missing"
L18-1608,K17-3001,1,0.904845,"Missing"
L18-1718,P13-2107,0,0.0454926,"Missing"
L18-1718,W13-4916,0,0.0300012,"Missing"
L18-1718,W09-3821,1,0.740978,"Missing"
L18-1718,F12-2024,1,0.899058,"Missing"
L18-1718,candito-etal-2010-statistical,1,0.865662,"Missing"
L18-1718,C12-1052,0,0.052222,"Missing"
L18-1718,C12-1059,0,0.0416459,"Missing"
L18-1718,J93-2004,0,0.0629965,"Missing"
L18-1718,P09-2010,0,0.0726827,"Missing"
L18-1718,L16-1375,1,0.89553,"Missing"
L18-1718,W13-4917,1,0.856651,"Missing"
L18-1718,W14-6111,1,0.908556,"Missing"
L18-1718,W13-4906,1,0.857223,"Missing"
L18-1718,E17-1034,0,0.0185843,"an iterative fashion, or as new relevant conversion needs are identified. A full manual evaluation of a converted treebank could represent an effort comparable to full re-annotation of a large part of the data. Indeed, few of the UD-conversion papers provide accuracy scores of the conversion on a manually annotated testbench. For instance, The Danish conversion of Johannsen et al. (2015), uses a small set of hand-annotated sentences that reflect specific phenomena and hard cases that is used as held-out section during the iterative development of conversion rules. The Hungarian conversion of Vincze et al. (2017) uses a hand-corrected gold standard of 1,800 sentences. When comparing the quality of the conversion with the gold standard, they consider the accuracy (87.81 UAS and 75.99 LAS) not sufficient to release the resulting treebank. We draw inspiration on their method to develop a handcorrected sample to evaluate the quality of our conversion.One of the authors of the article, an expert in dependency annotation very familiar with the UD formalism, reviewed 100 sentences from the test section and 100 sentences from the dev section manually, correcting edges and labels that were either not properly"
L18-1718,K17-3001,1,0.897083,"Missing"
N15-1007,C14-1133,0,0.186579,"kinds of syntactic features to improve predicateargument parsing. To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks. Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless of the annotation scheme and the parser used. The question is now to establish whether will this be verified in other semantic data sets? From the parsing of deep syntax treebanks a la Meaning Text Theory (Ballesteros et al., 2014), to Framenet semantic parsing (Das et al., 2014) or data-driven approaches closer to ours (Flanigan et al., 2014), it is difficult to know which models will predominate from this bubbling field and what kind of semantic data sets will benefit the most from syntax. Acknowledgements We would like to thank Kenji Sagae and André F. T. Martins for making their parsers available and for kindly answering our questions. We also thank our anonymous reviewers for their comments. This work was partly funded by the Program &quot;Investissements d’avenir&quot; managed by Agence Nationale de la Recherche ANR-10-LABX"
N15-1007,D12-1091,0,0.0319756,"Missing"
N15-1007,W13-4916,0,0.0653264,"Missing"
N15-1007,P04-1041,0,0.0918059,"Missing"
N15-1007,W05-0620,0,0.0683461,"87.66 88.08 89.13 89.43 89.7 +2.14 +1.77 +1.62 PAS, baseline +grandparent +co-parents 89.73 90.15 90.93 91.68 91.92 92.11 +1.95 +1.77 +1.18 Table 11: LF Results for T.PARSER (test set). Baseline = arc-factored + siblings Related Work A growing interest for semantic parsing has emerged over the past few years, with the availability of resources such as PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004) built on top of the Penn Treebank. The shallow semantic annotations they provide were among the targets of successful shared tasks on semantic role labeling (Surdeanu et al., 2008; Carreras and Màrquez, 2005). Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here. However, they diverge in that Propbank/Nombank annotations 72 do not form connected graphs by themselves, as they only cover argument identification and nominal predicates. The range of phenomena they describe is also limited, compared to a full predicate-argument analysis as provided by DM and PAS (Oepen et al., 2014). More importantly, as pointed out by Yi et al. (2007), being verb-specific, Propbank’s roles do not generali"
N15-1007,A00-2018,0,0.337842,"the parse, the better the output. The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set: the Wall Street journal section of the Penn treebank (P TB, (Marcus et al., 1993)) which is often stripped of its richer set of annotations (i.e. traces and functional labels are removed), while, for reasons of efficiency and availability, projective dependency trees are often given preference over richer graph structures (Nivre and Nilsson, 2005; Sagae and Tsujii, 2008). This led to the emergence of surface syntax-based parsers (Charniak, 2000; Nivre, 2003; Petrov et al., 2006) whose output cannot by themselves be used to extract full-fledged predicateargument structures. For example, control verb constructions, it-cleft structures, argument sharing in ellipsis coordination, etc. are among the phenomena requiring a graph to be properly accounted for. The dichotomy between what can usually be parsed with high accuracy and what lies in the deeper syntactic description has initiated a line of research devoted to closing the gap between surface syntax and richer structures. For most of the previous decade, the term deep syntax was used"
N15-1007,W03-1006,0,0.252425,"n and nominal predicates. The range of phenomena they describe is also limited, compared to a full predicate-argument analysis as provided by DM and PAS (Oepen et al., 2014). More importantly, as pointed out by Yi et al. (2007), being verb-specific, Propbank’s roles do not generalize well beyond the A RG 0 argument (i.e. the subject/agent role) leading to inconsistencies. However, the advent of such semantic-based resources have ignited a fruitful line of research, of which the use of heterogeneous sources of information to boost parsing performance has been investigated over the past decade (Chen and Rambow, 2003; Tsuruoka et al., 2004) with a strong regain of interest raised by the work of Moschitti et al. (2008), Henderson et al. (2008), Sagae (2009). 8 Conclusion We described the use and combination of several kinds of syntactic features to improve predicateargument parsing. To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks. Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless o"
N15-1007,W02-1001,0,0.0477273,"Xσi , . . . , Xσj . multiple edges between the same pair of nodes. It is to be noted that the pop0 action may also be used to remove words with no heads. We base our work on the the DAG parser of Sagae and Tsujii (2008) (henceforth S&T) which we extended with the set of actions displayed above (Figure 1) to cope with partially connected planar graphs, and we gave it the ability to take advantage of an extended set of features. Finally, for efficiency reasons (memory consumption and speed), we replaced the original Maxent model with an averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002). Transition-based Graphs Parsing (σ, wi |β, A) ` (σ|wi , β, A) (σ|wj |wi , β, A) ` (σ|wi , β, A ∪ (wi , r, wj )) (σ|wj |wi , β, A) ` (σ|wj , β, A ∪ (wj , r, wi )) (σ|wj |wi , β, A) ` (σ|wj |wi , β, A ∪ (wi , r, wj )) (σ|wj |wi , β, A) ` (σ|wj , wi |β, A ∪ (wj , r, wi ) (σ|wi , β, A) ` (σ, β, A) POSσ1 ,σ2 ,σ3 POSβ1 ,β2 ,β3 leftLabelσ1 ,σ2 d12 d011 Table 3: Baseline features for the parser. Table 2: Breakdown of Label Statistics. Cell values in italics not counted in the DM total. 3 Lemmaσ1 ,σ2 ,σ3 Lemmaβ1 ,β2 rightPOSσ1 ,σ2 a 4 4.1 (shift) (lR) (rR) (lA) (rA) (pop0) Figure 1: Set of transition"
N15-1007,1995.tmi-1.2,0,0.173567,"proves the recognition of long distance dependencies and elliptical constructions. We finally discuss the usefulness of our approach, when applied on a second-order model based on dual decomposition (Martins and Almeida, 2014), showing that our use of syntactic features enhances this model accuracy and provides state-of-the-art performance. 2 Deep Syntax and Underspecified Semantic Corpora DeepBank Corpus Semantic dependency graphs in the DM Corpus are the result of a two-step simplification of the underspecified logical-form meaning representations, based on Minimal Recursion Semantic (MRS, (Copestake et al., 1995; Copestake et al., 2005)), derived from the manually annotated DeepBank treebank (Flickinger et al., 2012). First, Oepen and Lønning (2006) define a conversion from original MRS formulae to variable-free Elementary Dependency Structures (EDS), which (a) maps each predication in the MRS logical-form meaning representation to a node in a dependency graph and (b) transforms argument relations represented by shared logical variables into directed dependency links between graph nodes. Then, in a second conversion step, the EDS graphs are further reduced into strict bi-lexical form, i.e. a set of d"
N15-1007,J14-1002,0,0.0348455,"Missing"
N15-1007,S14-2080,0,0.287359,"(Miyao and Tsujii, 2005; Cahill et al., 2004; Hockenmaier, 2003). Its use now spreads by misnomer to models that provide more abstract structures, capable of generalizing classical functional labels to more semantic (in a logical view) arguments, potentially capable of neutralizing diathesis distinctions and of providing accurate predicate-argument structures. Although the building of syntax-to-semantic interface seems inextricably linked to an efficient parsing stage, inspirational works on semantic role labelling (Toutanova et al., 2005) and more recently on broad coverage semantic parsing (Du et al., 2014) that provide stateof-the-art results without relying on surface syntax, lead us to question the usefulness of syntactic parses for predicate-argument structure parsing. In this study, we investigate the impact of syntactic features on a transition-based graph parser by testing on two treebanks. We take advantage of the recent release for the SemEval 2014 shared task on semantic dependency parsing, by Oepen et 64 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 64–74, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computat"
N15-1007,P14-1134,0,0.0909834,"Missing"
N15-1007,P13-2111,0,0.0274346,"orpus (dev. set). Table 9: Shared subjects coordinations eval. (dev sets). Table 8: Long-distance dependencies eval. (dev sets). based parsers to predict predicate-argument structures, especially for LDDs. Yet, compared to stateof-the-art systems, our results built on the S&T parser score lower than the top performers (Table 10). However, we are currently extending a more advanced lattice-aware transition-based parser (DSR) with beams (Villemonte De La Clergerie, 2013) that takes advantage of cutting-edge techniques (dynamic programming, averaged perceptron with early updates, etc. following (Goldberg et al., 2013; Huang et al., 2012)) 4 , which proves effective by reaching the state-of-the-art on PAS, outperforming Thomson et al. (2014) and second to the model of Martins and Almeida (2014). 5 The point here is that using the same syntactic features as our base system exhibits the same improvement over a now much stronger baseline. We can conjecture that the ambiguities added by the relative scarcity of the deep annotations is efficiently handled by a more complete exploration of the search space, made possible by beam optimization. We can also wonder whether the lower improvement brought to DM parsing"
N15-1007,W08-2122,0,0.0213079,"as provided by DM and PAS (Oepen et al., 2014). More importantly, as pointed out by Yi et al. (2007), being verb-specific, Propbank’s roles do not generalize well beyond the A RG 0 argument (i.e. the subject/agent role) leading to inconsistencies. However, the advent of such semantic-based resources have ignited a fruitful line of research, of which the use of heterogeneous sources of information to boost parsing performance has been investigated over the past decade (Chen and Rambow, 2003; Tsuruoka et al., 2004) with a strong regain of interest raised by the work of Moschitti et al. (2008), Henderson et al. (2008), Sagae (2009). 8 Conclusion We described the use and combination of several kinds of syntactic features to improve predicateargument parsing. To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks. Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless of the annotation scheme and the parser used. The question is now to establish whether will this be verified in other semantic da"
N15-1007,N12-1015,0,0.026082,"9: Shared subjects coordinations eval. (dev sets). Table 8: Long-distance dependencies eval. (dev sets). based parsers to predict predicate-argument structures, especially for LDDs. Yet, compared to stateof-the-art systems, our results built on the S&T parser score lower than the top performers (Table 10). However, we are currently extending a more advanced lattice-aware transition-based parser (DSR) with beams (Villemonte De La Clergerie, 2013) that takes advantage of cutting-edge techniques (dynamic programming, averaged perceptron with early updates, etc. following (Goldberg et al., 2013; Huang et al., 2012)) 4 , which proves effective by reaching the state-of-the-art on PAS, outperforming Thomson et al. (2014) and second to the model of Martins and Almeida (2014). 5 The point here is that using the same syntactic features as our base system exhibits the same improvement over a now much stronger baseline. We can conjecture that the ambiguities added by the relative scarcity of the deep annotations is efficiently handled by a more complete exploration of the search space, made possible by beam optimization. We can also wonder whether the lower improvement brought to DM parsing by the PTB-based syn"
N15-1007,W12-3602,0,0.0134445,"k (Flickinger et al., 2012). First, Oepen and Lønning (2006) define a conversion from original MRS formulae to variable-free Elementary Dependency Structures (EDS), which (a) maps each predication in the MRS logical-form meaning representation to a node in a dependency graph and (b) transforms argument relations represented by shared logical variables into directed dependency links between graph nodes. Then, in a second conversion step, the EDS graphs are further reduced into strict bi-lexical form, i.e. a set of directed, binary dependency relations holding exclusively between lexical units (Ivanova et al., 2012). Even though both conversion steps are, by design, lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS data set. Predicate-Argument Structure Corpus Enju Predicate-Argument Structures (PAS Corpus) are derived from the automatic HPSG-style annotation of the Penn Treebank (Miyao and Tsujii, 2004) that was primarily used for the development of the Enju parsing system (Miyao and Tsujii, 2005). The 65 PAS data set is an extraction of predicate-argument structures from the Enju HPSG treebank and contains word-to-word semantic dependencies."
N15-1007,J93-2004,0,0.0503779,"roduction For the majority of the state-of-the-art parsers that routinely reach ninety percent performance plateau in capturing tree structures, the question of what next crucially arises. Indeed, it has long been thought that the bottleneck preventing the advent of accurate syntax-to-semantic interfaces lies in the quality of the preceding phase of analysis: the better the parse, the better the output. The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set: the Wall Street journal section of the Penn treebank (P TB, (Marcus et al., 1993)) which is often stripped of its richer set of annotations (i.e. traces and functional labels are removed), while, for reasons of efficiency and availability, projective dependency trees are often given preference over richer graph structures (Nivre and Nilsson, 2005; Sagae and Tsujii, 2008). This led to the emergence of surface syntax-based parsers (Charniak, 2000; Nivre, 2003; Petrov et al., 2006) whose output cannot by themselves be used to extract full-fledged predicateargument structures. For example, control verb constructions, it-cleft structures, argument sharing in ellipsis coordinati"
N15-1007,S14-2082,0,0.303828,"framework requires graphs to be predicted. Using the DeepBank (Flickinger et al., 2012) and the PredicateArgument Structure treebank (Miyao and Tsujii, 2005) as a test field, we show how transition-based parsers, extended to handle connected graphs, benefit from the use of topologically different syntactic features such as dependencies, tree fragments, spines or syntactic paths, bringing a much needed context to the parsing models, improving notably over long distance dependencies and elided coordinate structures. By confirming this positive impact on an accurate 2nd-order graphbased parser (Martins and Almeida, 2014), we establish a new state-of-the-art on these data sets. 1 Introduction For the majority of the state-of-the-art parsers that routinely reach ninety percent performance plateau in capturing tree structures, the question of what next crucially arises. Indeed, it has long been thought that the bottleneck preventing the advent of accurate syntax-to-semantic interfaces lies in the quality of the preceding phase of analysis: the better the parse, the better the output. The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set"
N15-1007,D07-1013,0,0.0383176,"6 86.74 86.76 87.01 +1.10 +1.29 +1.57 +1.59 +1.84 88.84 89.04 89.18 89.17 89.32 89.44 89.30 89.48 89.49 85.20 85.45 85.49 85.62 85.74 85.72 85.87 85.81 85.80 86.98 87.21 87.30 87.36 87.49 87.54 87.55 87.60 87.61 +1.81 +2.04 +2.13 +2.19 +2.32 +2.37 +2.38 +2.43 +2.44 89.35 89.56 89.76 89.88 89.82 89.93 85.54 86.02 86.15 86.13 86.20 86.32 87.40 87.75 87.92 87.96 87.97 88.09 +2.23 +2.58 +2.75 +2.79 +2.80 +2.92 89.70 89.91 86.11 86.14 87.87 87.99 +2.70 +2.82 BN BN ( HPOS ) BKY PATHS BKY + SPINES SPINES + PATHS BN ( HPOS )+ SPINES BN ( HPOS )+ PATHS Results Analysis BN + PATHS BKY + PATHS Following Mcdonald and Nivre (2007), we conducted an error analysis based on the two best models and the baseline for each corpus. As shown in section 5, syntactic features greatly improve semantic parsing. However, it is interesting to explore more precisely what kind of syntactic information boosts or penalizes our predictions. We consider, among other factors, the impact in terms of distance between the head and the dependent (edge length) and the labels. We also explore several linguistic phenomena well known to be difficult to recover. 3 We tested the statistical significance between our best models and the baseline with t"
N15-1007,meyers-etal-2004-annotating,0,0.0309841,"a strong baseline, as they provide a global view to graph-based models, establishing a new state-of-the-art on these corpora. -S YNT. F EAT. +S YNT. F EAT. δ DM, baseline +grandparent +co-parents 86.99 87.66 88.08 89.13 89.43 89.7 +2.14 +1.77 +1.62 PAS, baseline +grandparent +co-parents 89.73 90.15 90.93 91.68 91.92 92.11 +1.95 +1.77 +1.18 Table 11: LF Results for T.PARSER (test set). Baseline = arc-factored + siblings Related Work A growing interest for semantic parsing has emerged over the past few years, with the availability of resources such as PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004) built on top of the Penn Treebank. The shallow semantic annotations they provide were among the targets of successful shared tasks on semantic role labeling (Surdeanu et al., 2008; Carreras and Màrquez, 2005). Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here. However, they diverge in that Propbank/Nombank annotations 72 do not form connected graphs by themselves, as they only cover argument identification and nominal predicates. The range of phenomena they describe is also li"
N15-1007,C04-1204,0,0.0284792,"into directed dependency links between graph nodes. Then, in a second conversion step, the EDS graphs are further reduced into strict bi-lexical form, i.e. a set of directed, binary dependency relations holding exclusively between lexical units (Ivanova et al., 2012). Even though both conversion steps are, by design, lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS data set. Predicate-Argument Structure Corpus Enju Predicate-Argument Structures (PAS Corpus) are derived from the automatic HPSG-style annotation of the Penn Treebank (Miyao and Tsujii, 2004) that was primarily used for the development of the Enju parsing system (Miyao and Tsujii, 2005). The 65 PAS data set is an extraction of predicate-argument structures from the Enju HPSG treebank and contains word-to-word semantic dependencies. Each dependency type is made of two elements: a coarse part-of-speech of the head predicate dependent (e.g. verb and adjective), and the argument (e.g. ARG1 and ARG2). Although both are derived from HSPG resources (a hand-crafted grammar for DM, a treebank-based one for PAS), they differ in their core linguistic choices (functional heads vs lexical head"
N15-1007,P05-1011,0,0.286881,"erb constructions, it-cleft structures, argument sharing in ellipsis coordination, etc. are among the phenomena requiring a graph to be properly accounted for. The dichotomy between what can usually be parsed with high accuracy and what lies in the deeper syntactic description has initiated a line of research devoted to closing the gap between surface syntax and richer structures. For most of the previous decade, the term deep syntax was used for rich parsing models built upon enriched versions of a constituency treebank, either with added HPSG or LFG annotation or CCG (almost) full rewrites (Miyao and Tsujii, 2005; Cahill et al., 2004; Hockenmaier, 2003). Its use now spreads by misnomer to models that provide more abstract structures, capable of generalizing classical functional labels to more semantic (in a logical view) arguments, potentially capable of neutralizing diathesis distinctions and of providing accurate predicate-argument structures. Although the building of syntax-to-semantic interface seems inextricably linked to an efficient parsing stage, inspirational works on semantic role labelling (Toutanova et al., 2005) and more recently on broad coverage semantic parsing (Du et al., 2014) that p"
N15-1007,J08-2003,0,0.151701,"edicate-argument analysis as provided by DM and PAS (Oepen et al., 2014). More importantly, as pointed out by Yi et al. (2007), being verb-specific, Propbank’s roles do not generalize well beyond the A RG 0 argument (i.e. the subject/agent role) leading to inconsistencies. However, the advent of such semantic-based resources have ignited a fruitful line of research, of which the use of heterogeneous sources of information to boost parsing performance has been investigated over the past decade (Chen and Rambow, 2003; Tsuruoka et al., 2004) with a strong regain of interest raised by the work of Moschitti et al. (2008), Henderson et al. (2008), Sagae (2009). 8 Conclusion We described the use and combination of several kinds of syntactic features to improve predicateargument parsing. To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks. Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless of the annotation scheme and the parser used. The question is now to establish whether will this be veri"
N15-1007,P05-1013,0,0.0429748,"of accurate syntax-to-semantic interfaces lies in the quality of the preceding phase of analysis: the better the parse, the better the output. The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set: the Wall Street journal section of the Penn treebank (P TB, (Marcus et al., 1993)) which is often stripped of its richer set of annotations (i.e. traces and functional labels are removed), while, for reasons of efficiency and availability, projective dependency trees are often given preference over richer graph structures (Nivre and Nilsson, 2005; Sagae and Tsujii, 2008). This led to the emergence of surface syntax-based parsers (Charniak, 2000; Nivre, 2003; Petrov et al., 2006) whose output cannot by themselves be used to extract full-fledged predicateargument structures. For example, control verb constructions, it-cleft structures, argument sharing in ellipsis coordination, etc. are among the phenomena requiring a graph to be properly accounted for. The dichotomy between what can usually be parsed with high accuracy and what lies in the deeper syntactic description has initiated a line of research devoted to closing the gap between"
N15-1007,W03-3017,0,0.0682555,"etter the output. The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set: the Wall Street journal section of the Penn treebank (P TB, (Marcus et al., 1993)) which is often stripped of its richer set of annotations (i.e. traces and functional labels are removed), while, for reasons of efficiency and availability, projective dependency trees are often given preference over richer graph structures (Nivre and Nilsson, 2005; Sagae and Tsujii, 2008). This led to the emergence of surface syntax-based parsers (Charniak, 2000; Nivre, 2003; Petrov et al., 2006) whose output cannot by themselves be used to extract full-fledged predicateargument structures. For example, control verb constructions, it-cleft structures, argument sharing in ellipsis coordination, etc. are among the phenomena requiring a graph to be properly accounted for. The dichotomy between what can usually be parsed with high accuracy and what lies in the deeper syntactic description has initiated a line of research devoted to closing the gap between surface syntax and richer structures. For most of the previous decade, the term deep syntax was used for rich par"
N15-1007,oepen-lonning-2006-discriminant,0,0.0784891,"nd-order model based on dual decomposition (Martins and Almeida, 2014), showing that our use of syntactic features enhances this model accuracy and provides state-of-the-art performance. 2 Deep Syntax and Underspecified Semantic Corpora DeepBank Corpus Semantic dependency graphs in the DM Corpus are the result of a two-step simplification of the underspecified logical-form meaning representations, based on Minimal Recursion Semantic (MRS, (Copestake et al., 1995; Copestake et al., 2005)), derived from the manually annotated DeepBank treebank (Flickinger et al., 2012). First, Oepen and Lønning (2006) define a conversion from original MRS formulae to variable-free Elementary Dependency Structures (EDS), which (a) maps each predication in the MRS logical-form meaning representation to a node in a dependency graph and (b) transforms argument relations represented by shared logical variables into directed dependency links between graph nodes. Then, in a second conversion step, the EDS graphs are further reduced into strict bi-lexical form, i.e. a set of directed, binary dependency relations holding exclusively between lexical units (Ivanova et al., 2012). Even though both conversion steps are"
N15-1007,S14-2008,0,0.154697,"and δ in Figure 2). In addition, we expanded these features with the part-of-speech of the head of a given token (HPOS). The idea is to evaluate the informativeness of more abstract syntactic features since a &lt;L ABEL , HPOS&gt; pair can be seen as generalizing many constituent subtrees. 67 5 Experiments Experimental Setup Both DM and PAS treebanks consist of texts from the P TB and which were either automatically derived from the original annotations or annotated with a hand-crafted grammar (see above). We use them in their bi-lexical dependency format, aligned at the token level as provided by Oepen et al. (2014)1 . The following split is used: sections 00-19 for training, 20 for the dev. set and 21 for test2 . All predicted parses are evaluated against the gold standard with labeled precision, recall and f-measure metrics. Results Our experiments are based on the evaluation of the combinations of the 4 main types of syntactic features described in section 4: tree fragments (BKY), predicted mate dependencies (BN) and their extension with POS heads (BN ( HPOS )), spinal elementary trees (SPINES) and head paths (PATHS). The results are shown in Tables 5 and 6. All improvements from the baseline are sign"
N15-1007,J05-1004,0,0.0874258,"is also relevant with a strong baseline, as they provide a global view to graph-based models, establishing a new state-of-the-art on these corpora. -S YNT. F EAT. +S YNT. F EAT. δ DM, baseline +grandparent +co-parents 86.99 87.66 88.08 89.13 89.43 89.7 +2.14 +1.77 +1.62 PAS, baseline +grandparent +co-parents 89.73 90.15 90.93 91.68 91.92 92.11 +1.95 +1.77 +1.18 Table 11: LF Results for T.PARSER (test set). Baseline = arc-factored + siblings Related Work A growing interest for semantic parsing has emerged over the past few years, with the availability of resources such as PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004) built on top of the Penn Treebank. The shallow semantic annotations they provide were among the targets of successful shared tasks on semantic role labeling (Surdeanu et al., 2008; Carreras and Màrquez, 2005). Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here. However, they diverge in that Propbank/Nombank annotations 72 do not form connected graphs by themselves, as they only cover argument identification and nominal predicates. The range of phenomena th"
N15-1007,P06-1055,0,0.439254,"put. The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set: the Wall Street journal section of the Penn treebank (P TB, (Marcus et al., 1993)) which is often stripped of its richer set of annotations (i.e. traces and functional labels are removed), while, for reasons of efficiency and availability, projective dependency trees are often given preference over richer graph structures (Nivre and Nilsson, 2005; Sagae and Tsujii, 2008). This led to the emergence of surface syntax-based parsers (Charniak, 2000; Nivre, 2003; Petrov et al., 2006) whose output cannot by themselves be used to extract full-fledged predicateargument structures. For example, control verb constructions, it-cleft structures, argument sharing in ellipsis coordination, etc. are among the phenomena requiring a graph to be properly accounted for. The dichotomy between what can usually be parsed with high accuracy and what lies in the deeper syntactic description has initiated a line of research devoted to closing the gap between surface syntax and richer structures. For most of the previous decade, the term deep syntax was used for rich parsing models built upon"
N15-1007,C08-1095,0,0.556292,"antic interfaces lies in the quality of the preceding phase of analysis: the better the parse, the better the output. The truth is that most of the structures used to train current parsing models are degraded versions of a more informative data set: the Wall Street journal section of the Penn treebank (P TB, (Marcus et al., 1993)) which is often stripped of its richer set of annotations (i.e. traces and functional labels are removed), while, for reasons of efficiency and availability, projective dependency trees are often given preference over richer graph structures (Nivre and Nilsson, 2005; Sagae and Tsujii, 2008). This led to the emergence of surface syntax-based parsers (Charniak, 2000; Nivre, 2003; Petrov et al., 2006) whose output cannot by themselves be used to extract full-fledged predicateargument structures. For example, control verb constructions, it-cleft structures, argument sharing in ellipsis coordination, etc. are among the phenomena requiring a graph to be properly accounted for. The dichotomy between what can usually be parsed with high accuracy and what lies in the deeper syntactic description has initiated a line of research devoted to closing the gap between surface syntax and richer"
N15-1007,W09-3813,0,0.0228927,"S (Oepen et al., 2014). More importantly, as pointed out by Yi et al. (2007), being verb-specific, Propbank’s roles do not generalize well beyond the A RG 0 argument (i.e. the subject/agent role) leading to inconsistencies. However, the advent of such semantic-based resources have ignited a fruitful line of research, of which the use of heterogeneous sources of information to boost parsing performance has been investigated over the past decade (Chen and Rambow, 2003; Tsuruoka et al., 2004) with a strong regain of interest raised by the work of Moschitti et al. (2008), Henderson et al. (2008), Sagae (2009). 8 Conclusion We described the use and combination of several kinds of syntactic features to improve predicateargument parsing. To do so, we tested our approach of injecting surface-syntax features by thoroughly evaluating their impact on one transitionbased graph parser, then validating on two more efficient parsers, over two deep syntax and semantic treebanks. Results of the syntax-enhanced semantic parsers exhibit a constant improvement, regardless of the annotation scheme and the parser used. The question is now to establish whether will this be verified in other semantic data sets? From"
N15-1007,seddah-2010-exploring,1,0.817941,"S +δ S PINES TREES H EAD PATHS 648 272 273 1305 742 731 637 265 268 27,670 3,320 2,389 Table 4: Syntactic features statistics (Counts). Figure 2: Schema of Syntactic Features Constituent Tree Fragments These consist of fragments of syntactic trees predicted by the Petrov et al. (2006) parser in a 10-way jackknife setting. They can be used as enhanced POS or as features. Spinal Elementary Trees A full set of parses was reconstructed from the tree fragments using a slightly tweaked version of the C O NLL 2009 shared task processing tools (Hajiˇc et al., 2009). We then extracted a spine grammar (Seddah, 2010) using the head percolation table of the Bikel (2002) parser, slightly modified to avoid certain determiners being marked as heads in certain configurations. The resulting spines were assigned in a deterministic way (red part in Figure 2). Predicted MATE Dependency Labels These consist of the dependency labels predicted by the MATE parser (Bohnet, 2010), trained on a Stanford surface dependency version of the Penn Treebank. We combined the labels with a distance δ = t − h where t is the token position and h the head position (brown labels and δ in Figure 2). In addition, we expanded these feat"
N15-1007,W08-2121,0,0.0728947,"arent +co-parents 86.99 87.66 88.08 89.13 89.43 89.7 +2.14 +1.77 +1.62 PAS, baseline +grandparent +co-parents 89.73 90.15 90.93 91.68 91.92 92.11 +1.95 +1.77 +1.18 Table 11: LF Results for T.PARSER (test set). Baseline = arc-factored + siblings Related Work A growing interest for semantic parsing has emerged over the past few years, with the availability of resources such as PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004) built on top of the Penn Treebank. The shallow semantic annotations they provide were among the targets of successful shared tasks on semantic role labeling (Surdeanu et al., 2008; Carreras and Màrquez, 2005). Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here. However, they diverge in that Propbank/Nombank annotations 72 do not form connected graphs by themselves, as they only cover argument identification and nominal predicates. The range of phenomena they describe is also limited, compared to a full predicate-argument analysis as provided by DM and PAS (Oepen et al., 2014). More importantly, as pointed out by Yi et al. (2007), being verb-specific, Pro"
N15-1007,S14-2027,0,0.025904,"Missing"
N15-1007,P05-1073,0,0.0433563,"ebank, either with added HPSG or LFG annotation or CCG (almost) full rewrites (Miyao and Tsujii, 2005; Cahill et al., 2004; Hockenmaier, 2003). Its use now spreads by misnomer to models that provide more abstract structures, capable of generalizing classical functional labels to more semantic (in a logical view) arguments, potentially capable of neutralizing diathesis distinctions and of providing accurate predicate-argument structures. Although the building of syntax-to-semantic interface seems inextricably linked to an efficient parsing stage, inspirational works on semantic role labelling (Toutanova et al., 2005) and more recently on broad coverage semantic parsing (Du et al., 2014) that provide stateof-the-art results without relying on surface syntax, lead us to question the usefulness of syntactic parses for predicate-argument structure parsing. In this study, we investigate the impact of syntactic features on a transition-based graph parser by testing on two treebanks. We take advantage of the recent release for the SemEval 2014 shared task on semantic dependency parsing, by Oepen et 64 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 64–74, c"
N15-1007,W13-4906,1,0.82846,"75.41 ALL ( HPOS ) 97.86 98.57 64.78 65.09 77.96 78.41 BKY + BN ( HPOS )+ PATHS +2.55 +3.00 (b) on PAS (dev. set, 636 dependencies). (b) PAS Corpus (dev. set). Table 9: Shared subjects coordinations eval. (dev sets). Table 8: Long-distance dependencies eval. (dev sets). based parsers to predict predicate-argument structures, especially for LDDs. Yet, compared to stateof-the-art systems, our results built on the S&T parser score lower than the top performers (Table 10). However, we are currently extending a more advanced lattice-aware transition-based parser (DSR) with beams (Villemonte De La Clergerie, 2013) that takes advantage of cutting-edge techniques (dynamic programming, averaged perceptron with early updates, etc. following (Goldberg et al., 2013; Huang et al., 2012)) 4 , which proves effective by reaching the state-of-the-art on PAS, outperforming Thomson et al. (2014) and second to the model of Martins and Almeida (2014). 5 The point here is that using the same syntactic features as our base system exhibits the same improvement over a now much stronger baseline. We can conjecture that the ambiguities added by the relative scarcity of the deep annotations is efficiently handled by a more"
N15-1007,N07-1069,0,0.024958,"antic role labeling (Surdeanu et al., 2008; Carreras and Màrquez, 2005). Actually, the conjoint use of such annotations with surface syntax dependencies bears some resemblance with predicate-argument structure parsing like we presented here. However, they diverge in that Propbank/Nombank annotations 72 do not form connected graphs by themselves, as they only cover argument identification and nominal predicates. The range of phenomena they describe is also limited, compared to a full predicate-argument analysis as provided by DM and PAS (Oepen et al., 2014). More importantly, as pointed out by Yi et al. (2007), being verb-specific, Propbank’s roles do not generalize well beyond the A RG 0 argument (i.e. the subject/agent role) leading to inconsistencies. However, the advent of such semantic-based resources have ignited a fruitful line of research, of which the use of heterogeneous sources of information to boost parsing performance has been investigated over the past decade (Chen and Rambow, 2003; Tsuruoka et al., 2004) with a strong regain of interest raised by the work of Moschitti et al. (2008), Henderson et al. (2008), Sagae (2009). 8 Conclusion We described the use and combination of several k"
N15-1007,W09-1201,0,\N,Missing
P19-1356,D15-1075,0,0.0312175,"x tree (e.g. ‘LR’ denotes the right child of left child of root). The authors assume that, for a given role scheme, if a TPDN can be trained well to approximate the representation learned by a neural model, then that role scheme likely specifies the compositionality implicitly learned by the model. For each BERT layer, we work with five different role schemes. Each word’s role is computed based on its left-to-right index, its right-to-left index, an ordered pair containing its left-to-right and Following McCoy et al. (2019), we train our TPDN model on the premise sentences in the SNLI corpus (Bowman et al., 2015). We initialize the filler embeddings of the TPDN with the pre-trained word embeddings from BERT’s input layer, freeze it, learn a linear projection on top of it and use a Mean Squared Error (MSE) loss function. Other trainable parameters include the role embeddings and a linear projection on top of tensor product sum to match the embedding size of BERT. Table 4 displays the MSE between representation from pretrained BERT and representation from TPDN trained to approximate BERT. We discover that BERT implicitly implements a treebased scheme, as a TPDN model following that scheme best approxima"
P19-1356,L18-1269,0,0.0653377,"e grouped into three categories. Surface tasks probe for sentence length (SentLen) and for the presence of words in the sentence (WC). Syntactic tasks test for sensitivity to word order (BShift), the depth of the syntactic tree (TreeDepth) and the sequence of toplevel constituents in the syntax tree (TopConst). Semantic tasks check for the tense (Tense), the subject (resp. direct object) number in the main clause (SubjNum, resp. ObjNum), the sensitivity to random replacement of a noun/verb (SOMO) and the random swapping of coordinated clausal conjuncts (CoordInv). We use the SentEval toolkit (Conneau and Kiela, 2018) along with the recommended hyperparameter space to search for the best probing classifier. As random encoders can surprisingly encode a lot of lexical and structural information (Zhang and Bowman, 2018), we also evaluate the untrained version of BERT, obtained by setting all model weights to a random number. Table 2 shows that BERT embeds a rich hierarchy of linguistic signals: surface information at the bottom, syntactic information in the middle, semantic information at the top. BERT has also surpassed the previously published results for two tasks: BShift and CoordInv. We find that the unt"
P19-1356,P18-1198,0,0.0981764,"Missing"
P19-1356,N19-1423,0,0.211957,"Missing"
P19-1356,P03-1054,0,0.0944559,"0.0081 0.0099 0.0201 0.0233 0.0251 0.0226 0.0179 0.0237 0.0179 0.0203 0.0221 0.0201 0.0155 0.0214 0.0284 0.0337 0.0355 0.0311 0.0249 0.0338 0.0428 0.0486 0.0507 0.0453 0.0363 0.0486 0.0362 0.0411 0.0422 0.0391 0.0319 0.0415 0.0305 0.0339 0.0348 0.0334 0.0278 0.0340 Table 4: Mean squared error between TPDN and BERT representation for a given layer and role scheme on SNLI test instances. Each number corresponds to the average across five random initializations. 27/02/2019 depparse_layer_1.svg right-to-left indices, its position in a syntactic tree (formatted version of the Stanford PCFG Parser (Klein and Manning, 2003) with no unary nodes and no labels) and an index common to all the words in the sentence (bag-of-words), which ignores its position. Additionally, we also define a role scheme based on random binary trees. The keys to the cabinet are on the table The keys to the cabinet are on the table Figure 2: Dependency parse tree induced from attention head #11 in layer #2 using gold root (‘are’) as starting node for maximum spanning tree algorithm. ﬁle:///Users/ganeshj/Downloads/todelete/depparse_layer_1.svg 1/1 Results in Table 3 show that the middle layers perform well in most cases, which supports the"
P19-1356,Q16-1037,0,0.0751865,"asks: BShift and CoordInv. We find that the untrained version of BERT corresponding to the higher layers outperforms the trained version in the task of predicting sentence length (SentLen). This could indicate that untrained models contain sufficient information to predict a basic surface feature such as sentence length, whereas training the model results in the model storing more complex information, at the expense of its ability to predict such basic surface features. 5 Subject-Verb Agreement Subject-verb agreement is a proxy task to probe whether a neural model encodes syntactic structure (Linzen et al., 2016). The task of predicting the verb number becomes harder when there are more nouns with opposite number (attractors) intervening between the subject and the verb. Goldberg (2019) has shown that BERT learns syntactic phenomenon surprisingly well using various stimuli for subject-verb agreement. We extend his work by performing the test on each layer of BERT and controlling for the number of attractors. In our study, we use the stimuli created by Linzen et al. (2016) and the SentEval toolkit (Conneau and Kiela, 2018) to build the binary classifier with the recommended hyperparameter space, using"
P19-1356,N19-1112,0,0.214179,"a contemporaneous work that introduces a novel edge probing task to investigate how contextual word representations encode sentence structure across a range of syntactic, semantic, local and long-range phenomena. They conclude that contextual word representations trained on language modeling and machine translation encode syntactic phenomena strongly, but offer comparably small improvements on semantic tasks over a non-contextual baseline. Their result using BERT model on capturing linguistic hierarchy confirms our probing task results although using a set of relatively simple probing tasks. Liu et al. (2019) is another contemporaneous work that studies the features of language captured/missed by contextualized vectors, transferability across different layers of the model and the impact of pretraining on the linguistic knowledge and transferability. They find that (i) contextualized word embeddings do not capture finegrained linguistic knowledge, (ii) higher layers of RNN to be task-specific (with no such pattern for a transformer) and (iii) pretraining on a closely related task yields better performance than language model pretraining. Hewitt and Manning (2019) is a very recent work which showed"
P19-1356,D14-1162,0,0.101356,"nd “are”) are captured accurately. Surprisingly, the predicate-argument structure seems to be partly modeled as shown by the chain of dependencies between “key”,“cabinet” and “table”. 3654 7 Related Work Peters et al. (2018) studies how the choice of neural architecture such as CNNs, Transformers and RNNs used for language model pretraining affects the downstream task accuracy and the qualitative properties of the contextualized word representations that are learned. They conclude that all architectures learn high quality representations that outperform standard word embeddings such as GloVe (Pennington et al., 2014) for challenging NLP tasks. They also show that these architectures hierarchically structure linguistic information, such that morphological, (local) syntactic and (longer range) semantic information tend to be represented in, respectively, the word embedding layer, lower contextual layers and upper layers. In our work, we observe that such hierarchy exists as well for BERT models that are not trained using the standard language modelling objective. Goldberg (2019) shows that the BERT model captures syntactic information well for subject-verb agreement. We build on this work by performing the"
P19-1356,D18-1179,0,0.151216,"e linguistic structure implicitly learned by BERT’s representations. We use the PyTorch implementation of BERT, which hosts the models trained by (Devlin et al., 2018). All our experiments are based on the bert-base-uncased variant,2 which consists of 12 layers, each having a hidden size of 768 and 12 attention heads (110M parameters). In all our experiments, we seek the activation of the first input token (‘[CLS]’) (which summarizes the information from the actual tokens using a self-attention mechanism) at every layer to compute BERT representation, unless otherwise stated. 3 Phrasal Syntax Peters et al. (2018) have shown that the representations underlying LSTM-based language models(Hochreiter and Schmidhuber, 1997) can capture phrase-level (or span-level) information.3 It remains unclear if this holds true for models not trained with a traditional language modeling objective, such as BERT. Even if it does, would the information be present in multiple layers of the model? To investigate this question we extract span representations from each layer of BERT. 2 We obtained similar results in preliminary experiments with the bert-large-uncased variant. 3 Peters et al. (2018) experimented with ELMo-styl"
P19-1356,N19-1419,0,0.0377872,"g a set of relatively simple probing tasks. Liu et al. (2019) is another contemporaneous work that studies the features of language captured/missed by contextualized vectors, transferability across different layers of the model and the impact of pretraining on the linguistic knowledge and transferability. They find that (i) contextualized word embeddings do not capture finegrained linguistic knowledge, (ii) higher layers of RNN to be task-specific (with no such pattern for a transformer) and (iii) pretraining on a closely related task yields better performance than language model pretraining. Hewitt and Manning (2019) is a very recent work which showed that we can recover parse trees from the linear transformation of contextual word representation consistently, better than with non-contextual baselines. They focused mainly on syntactic structure while our work additionally experimented with linear structures (leftto-right, right-to-left) to show that the compositionality modelling underlying BERT mimics traditional syntactic analysis. The recent burst of papers around these questions illustrates the importance of interpreting contextualized word embedding models and our work complements the growing literat"
P19-1356,W18-5431,0,0.0410636,"p of tensor product sum to match the embedding size of BERT. Table 4 displays the MSE between representation from pretrained BERT and representation from TPDN trained to approximate BERT. We discover that BERT implicitly implements a treebased scheme, as a TPDN model following that scheme best approximates BERT’s representation at most layers. This result is remarkable, as BERT encodes classical, tree-like structures despite relying purely on attention mechanisms. Motivated by this study, we perform a case study on dependency trees induced from self attention weight following the work done by Raganato and Tiedemann (2018). Figure 2 displays the dependencies inferred from an example sentence by obtaining self attention weights for every word pairs from attention head #11 in layer #2, fixing the gold root as the starting node and invoking the Chu-Liu-Edmonds algorithm (Chu and Liu, 1967). We observe that determiner-noun dependencies (“the keys”, “the cabinet” and “the table”) and subject-verb dependency (“keys” and “are”) are captured accurately. Surprisingly, the predicate-argument structure seems to be partly modeled as shown by the chain of dependencies between “key”,“cabinet” and “table”. 3654 7 Related Work"
P19-1356,W00-0726,0,0.313791,"ch layer of BERT. 2 We obtained similar results in preliminary experiments with the bert-large-uncased variant. 3 Peters et al. (2018) experimented with ELMo-style CNN and Transformer but did not report this finding for these models. Following Peters et al. (2018), for a token sequence si , . . . , sj , we compute the span representation s(si ,sj ),l at layer l by concatenating the first (hsi ,l ) and last hidden vector (hsj ,l ), along with their element-wise product and difference. We randomly pick 3000 labeled chunks and 500 spans not labeled as chunks from the CoNLL 2000 chunking dataset (Sang and Buchholz, 2000). As shown in Figure 1, we visualize the span representations obtained from multiple layers using tSNE (Maaten and Hinton, 2008), a non-linear dimensionality reduction algorithm for visualizing high-dimensional data. We observe that BERT mostly captures phrase-level information in the lower layers and that this information gets gradually diluted in higher layers. The span representations from the lower layers map chunks (e.g. ‘to demonstrate’) that project their underlying category (e.g. VP) together. We further quantify this claim by performing a k-means clustering on span representations wit"
P19-1356,W18-5446,0,0.0898822,"Missing"
P19-1356,W18-5448,0,0.0414088,"h of the syntactic tree (TreeDepth) and the sequence of toplevel constituents in the syntax tree (TopConst). Semantic tasks check for the tense (Tense), the subject (resp. direct object) number in the main clause (SubjNum, resp. ObjNum), the sensitivity to random replacement of a noun/verb (SOMO) and the random swapping of coordinated clausal conjuncts (CoordInv). We use the SentEval toolkit (Conneau and Kiela, 2018) along with the recommended hyperparameter space to search for the best probing classifier. As random encoders can surprisingly encode a lot of lexical and structural information (Zhang and Bowman, 2018), we also evaluate the untrained version of BERT, obtained by setting all model weights to a random number. Table 2 shows that BERT embeds a rich hierarchy of linguistic signals: surface information at the bottom, syntactic information in the middle, semantic information at the top. BERT has also surpassed the previously published results for two tasks: BShift and CoordInv. We find that the untrained version of BERT corresponding to the higher layers outperforms the trained version in the task of predicting sentence length (SentLen). This could indicate that untrained models contain sufficient"
S14-2012,W13-4916,0,0.0464075,"Missing"
S14-2012,C04-1204,0,0.0390746,"Missing"
S14-2012,P13-1091,0,0.0327856,"Missing"
S14-2012,S14-2008,0,0.153665,"Missing"
S14-2012,P13-2111,0,0.0604694,"c Graph Parsing with Syntactic Features Corentin Ribeyre? ◦ Eric Villemonte de la Clergerie? Djamé Seddah?  ? Alpage, INRIA ◦ Univ Paris Diderot, Sorbonne Paris Cité  Université Paris Sorbonne firstname.lastname@inria.fr Abstract wide range of languages (Nivre et al., 2007a; Seddah et al., 2013). The two systems we present both extend transition-based parsers in order to be able to generate acyclic dependency graphs. The first one follows the standard greedy search mechanism of (Nivre et al., 2007b), while the second one follows a slightly more global search strategy (Huang and Sagae, 2010; Goldberg et al., 2013) by relying on dynamic programming techniques. In addition to building graphs directly, the main originality of our work lies in the use of different kinds of syntactic features, showing that using syntax for pure deep semantic parsing improves global performance by more than two points. Although not state-of-the-art, our systems perform very honorably compared with other single systems in this shared task and pave quite an interesting way for further work. In the remainder of this paper, we present the parsers and their extensions for building graphs; we then present our syntactic features an"
S14-2012,C08-1095,0,0.524033,"een used interchangeably to test their performance in terms of Fscore. But the difference was negligeable in general. plied, etc. For various reasons, we started our experiments with two rather different transition-based parsers, which have finally converged on several aspects. In particular, the main convergence concerns the set of transitions needed to parse the three proposed annotation schemes. To be able to attach zero, one, or more heads to a word, it is necessary to clearly dissociate the addition of a dependency from the reduction of a word (i.e. its removal from the stack). Following Sagae and Tsujii (2008), as shown in Figure 1, beside the usual shift and reduce transitions of the arc-standard strategy, we introduced the new left and right attach actions for adding new dependencies (while keeping the dependent on the stack) and two reduce pop0 and pop1 actions to remove a word from the stack after attachement of its dependents. All transitions adding an edge should also satisfy the condition that the new edge does not create a cycle or multiple edges between the same pair of nodes. It is worth noting that the pop actions may also be used to remove words with no heads. 2.1 2.2 DYAL OG -SR Our se"
S14-2012,P10-1110,0,0.181802,"ransition-based Semantic Graph Parsing with Syntactic Features Corentin Ribeyre? ◦ Eric Villemonte de la Clergerie? Djamé Seddah?  ? Alpage, INRIA ◦ Univ Paris Diderot, Sorbonne Paris Cité  Université Paris Sorbonne firstname.lastname@inria.fr Abstract wide range of languages (Nivre et al., 2007a; Seddah et al., 2013). The two systems we present both extend transition-based parsers in order to be able to generate acyclic dependency graphs. The first one follows the standard greedy search mechanism of (Nivre et al., 2007b), while the second one follows a slightly more global search strategy (Huang and Sagae, 2010; Goldberg et al., 2013) by relying on dynamic programming techniques. In addition to building graphs directly, the main originality of our work lies in the use of different kinds of syntactic features, showing that using syntax for pure deep semantic parsing improves global performance by more than two points. Although not state-of-the-art, our systems perform very honorably compared with other single systems in this shared task and pave quite an interesting way for further work. In the remainder of this paper, we present the parsers and their extensions for building graphs; we then present o"
S14-2012,seddah-2010-exploring,1,0.838798,"Missing"
S14-2012,W13-4906,1,0.843025,"duce transitions of the arc-standard strategy, we introduced the new left and right attach actions for adding new dependencies (while keeping the dependent on the stack) and two reduce pop0 and pop1 actions to remove a word from the stack after attachement of its dependents. All transitions adding an edge should also satisfy the condition that the new edge does not create a cycle or multiple edges between the same pair of nodes. It is worth noting that the pop actions may also be used to remove words with no heads. 2.1 2.2 DYAL OG -SR Our second parsing system is DYAL OG -SR (Villemonte De La Clergerie, 2013), which has been developed to participate to the SPMRL’13 shared task. Coded on top of tabular logic programming system DYAL OG, it implements a transition-based parser relying on dynamic programming techniques, beams, and an averaged structured perceptron, following ideas from (Huang and Sagae, 2010; Goldberg et al., 2013). It was initially designed to follow an arcstandard parsing strategy, relying on shift and left/right reduce transitions. To deal with dependency graphs and non governed words, we first added the two attach transitions and the pop0 transition. But because there exist some o"
S14-2012,W12-3602,0,\N,Missing
S14-2012,candito-etal-2014-deep,1,\N,Missing
S14-2012,W13-4917,1,\N,Missing
S14-2012,D07-1096,0,\N,Missing
seddah-2010-exploring,W00-2037,0,\N,Missing
seddah-2010-exploring,J95-4002,0,\N,Missing
seddah-2010-exploring,J93-2004,0,\N,Missing
seddah-2010-exploring,C92-2066,0,\N,Missing
seddah-2010-exploring,J04-4004,0,\N,Missing
seddah-2010-exploring,W08-2102,0,\N,Missing
seddah-2010-exploring,W09-3815,0,\N,Missing
seddah-2010-exploring,E09-1080,0,\N,Missing
seddah-2010-exploring,W98-0131,0,\N,Missing
seddah-2010-exploring,P00-1058,0,\N,Missing
seddah-2010-exploring,P98-1091,0,\N,Missing
seddah-2010-exploring,C98-1088,0,\N,Missing
seddah-2010-exploring,A00-1031,0,\N,Missing
seddah-2010-exploring,C92-2065,0,\N,Missing
seddah-2010-exploring,P95-1037,0,\N,Missing
seddah-2010-exploring,P95-1021,0,\N,Missing
seddah-2010-exploring,J05-1004,0,\N,Missing
seddah-2010-exploring,H05-1102,0,\N,Missing
seddah-2010-exploring,J94-1004,0,\N,Missing
W00-2003,E93-1045,0,0.0815284,"Missing"
W00-2003,P98-2217,1,0.521658,"Missing"
W00-2003,C98-2212,1,\N,Missing
W06-1522,W98-0106,0,\N,Missing
W06-1522,E93-1045,0,\N,Missing
W06-1522,C96-2103,0,\N,Missing
W07-2204,P05-1022,0,0.224574,"Missing"
W07-2204,A00-2018,0,0.444658,"Missing"
W07-2204,J03-4003,0,0.0705179,"Missing"
W07-2204,W01-0521,0,0.468993,"Missing"
W07-2204,J93-2004,0,0.0282093,"Missing"
W07-2204,N06-1020,0,0.203344,"Missing"
W07-2204,P06-1043,0,0.355074,"Missing"
W07-2204,E03-1008,0,0.14938,"Missing"
W08-2311,W04-3315,0,0.29524,"shi, 1996a). The authors extend the formalism itself by a new operation, the conjoin operation, to provide derivation structures which cannot be obtained by pure (Lexicalized)TAG. Although powerful by allowing node merging and rich derivational structures, this operation leads to a difficult interpretation of the derivation tree in terms of generated languages even though the final derivation tree is actually a derivation graph. The derived tree becomes also a bit difficult to interpret for any classical phrase based linguistic theory. However, this model has been implemented among others by (Banik, 2004) for an interface syntax-semantic framework. Closer to our approach, to process elliptic coordination (Sarkar, 1997) introduces LinkSharing TAG, a more constrained formalism than Synchronous TAG (Shieber and Schabes, 1990) while belonging to the same family. The main idea is to dissociate dependency from constituency by the use of pairs of trees, one being a regular ele87 mentary trees, the other being a dependency tree. Derivations are shared thanks to a synchronization mechanism over different pairs of the same type (dependency and constituency). On the contrary, our approach builds parallel"
W08-2311,C96-1034,0,0.051075,"ing with Non Fixed Tree-Set’s Cardinality So far, we assumed that the grammar will provide the correct cardinality of a tree set (namely the correct number of unrealized elementary trees). Obviously, such an assumption cannot stand; it would lead to an exponential amount of elementary tree sets inside the grammar. In (Villemonte de La Clergerie, 2005), the author implements a proposal to handle this growing size problem using regular operators (mainly disjunction, Kleene star, interleaving and optionality) on nodes or subpart of a metagrammar tree description (Vijay-Shanker and Schabes, 1992; Candito, 1996). We argue for the use of the Kleene star and the optionality operator to cope with the potential exponential size of our MCTAG. The tree set α-aimer (Fig. 7) would then contain one main anchored tree, an optional unrealized Kleene starred tree of the form N0VN1 analysis of gapping coordination with any given number of conjuncts stands in TL-MCTAG ; its logical interpretation is simply a logical AND with n arguments. 3.4 Zeugma Construction and CUC To allow zeugma construction and CUC, we propose a set of trees that includes two different tree schemas, one of them being anchored by the coindex"
W08-2311,2005.jeptalnrecital-court.13,0,0.735717,"t adjoin into various nodes. If all nodes belong to the same elementary tree, MCTAGs are qualified as Tree-Local [TL-MCTAG], if all nodes belong to the same set, MCTAGs are Set-Local [SL-MCTAG] and Non-Local MCTAG [NL-MCTAG] otherwise. All of these MCTAG’s subclasses have a stronger generative capacity than TAG and it shall be noted that TL-TAG has the same weak and strong generative power (Weir, 1988). TL and SL-MCTAG can be parsed in a polynomial time (Boullier, 1999; Villemonte de La Clergerie, 2002) whereas NL-MCTAG’s parsing is known to be NP-Complete (Rambow and Satta, 1992). Following (Kallmeyer, 2005), we define a MCTAG, M , as a regular TAG, G, with an additional set of tree sets where each tree set is a subset of G’s elementary trees. As opposed to (Weir, 1988), (Kallmeyer, 2005) defines the MCTAG derivations to appear as the ones from the underlying TAG. This means that if a tree set γ, composed of elementary trees γi , is derived into a tree set γ ′ , the derivation tree will display every derivation instead of a link between γ ′ and γ. Thus, in order to allow more precise compositional analysis of coordination with ellipsis via the derivation tree, we adopt this view and for each tree"
W08-2311,C94-2151,0,0.822109,"Missing"
W08-2311,C96-2103,0,0.843823,"nous mechanism to the MCTAG framework . 1 Introduction We assume the reader to be familiar with the TAG framework (Joshi, 1987) and with MultiComponent TAG (MCTAG, (Weir, 1988)). We will focus on the analysis of elliptical coordination and zeugma construction in French. The main goal of this work is to build a syntax-semantic interface based on an acyclic dependency graphs obtained through MCTAG’s derivation and a simple synchronous mechanism. Knowing that pure LTAG cannot handle coordination with ellipsis without adding new notions of derivation and new operations (e.g. conjoin operation in (Sarkar and Joshi, 1996b)), we propose to use a enhanced version of MC-TAG for the processing of these structures. To the best of our knowledge, this is the first time that such a proposal is made within this framework. In this paper, we first discuss some of our examples, then we explore divergences of analysis between some elided predicates of a coordination and we finally present, using oriented synchronization links, our MC-TAG proposals going from Non-Local MCTAG (NL-MCTAG) solutions to unlexicalized Tree-Local MCTAG (TLMCTAG) ones. We conclude by showing that our proposal can deal with a wide range of coordina"
W08-2311,P00-1057,0,0.0279751,"handled by (Sarkar and Joshi, 1996a) using “node contraction” on both argument nodes and anchors. Proceedings of The Ninth International Workshop on Tree Adjoining Grammars and Related Formalisms Tübingen, Germany. June 6-8, 2008. The Use of MCTAG to Process Elliptic Coordination 83 α et S S S et α aimer S S V N0 N1 N0 V αX N N1 X={Jean|Marie|Paul|Virginie} ε [aimer] Derived Tree Derivation Tree S α −et α −aime (a) et S N Jean V aime S N Marie N V Paul ε α −Jean α −Marie α −aime (b) α −Paul α −Virginie N Virginie Figure 4: Sketch of analysis : “Jean aime Marie et Paul Virignie” et al., 1992; Schuler et al., 2000) of Tree Adjunct Grammars by allowing sets of trees, as a whole unit, to be part of a derivation step. Several types of MCTAG can be defined based on how the trees in a set adjoin into various nodes. If all nodes belong to the same elementary tree, MCTAGs are qualified as Tree-Local [TL-MCTAG], if all nodes belong to the same set, MCTAGs are Set-Local [SL-MCTAG] and Non-Local MCTAG [NL-MCTAG] otherwise. All of these MCTAG’s subclasses have a stronger generative capacity than TAG and it shall be noted that TL-TAG has the same weak and strong generative power (Weir, 1988). TL and SL-MCTAG can be"
W08-2311,W06-1522,1,0.626874,"h parts of the CET (regardless of possible adjunction, see sentence 1g where the tree anchored by ”thinks” can be an auxiliary tree of the form N0VS* which will adjoin on the root of the elementary tree N0VN1 anchored by “like”). Et P P et P N0 V N1 N0 V Paul fabrique εi Marie vend Fabriquer Vendre N1 Det N [des crepes] Marie Paul Des i Derived Tree Crepes Derivation Graph Figure 3: Derived tree and Derivation Graph for sentence 1b ple, in sentence (1e) the realized verb anchors a N0VN1 tree whereas its unrealized counterpart anchors a N0VAdj one. Therefore a tree schema copy as suggested by (Seddah and Sagot, 2006) cannot really be applied.1 In case of pure zeugma construction such as in (1e), the mismatch is even more pronounced because in French “prendre du poids” is a multi word expression meaning “to gain weight”. In LTAG this expression would lead to an initial tree with “[prendre]” as a main anchor and “du poids” as co-anchors, so the resulting tree will be similar to an intransitive N0V tree. The rightmost part of the coordination, on the contrary, can be paraphrased as “[Napoleon conquered] a lot of countries” which can be analyzed with a regular N0VN1 tree in a strictly compositional manner. He"
W08-2311,C90-3045,0,0.720155,"g node merging and rich derivational structures, this operation leads to a difficult interpretation of the derivation tree in terms of generated languages even though the final derivation tree is actually a derivation graph. The derived tree becomes also a bit difficult to interpret for any classical phrase based linguistic theory. However, this model has been implemented among others by (Banik, 2004) for an interface syntax-semantic framework. Closer to our approach, to process elliptic coordination (Sarkar, 1997) introduces LinkSharing TAG, a more constrained formalism than Synchronous TAG (Shieber and Schabes, 1990) while belonging to the same family. The main idea is to dissociate dependency from constituency by the use of pairs of trees, one being a regular ele87 mentary trees, the other being a dependency tree. Derivations are shared thanks to a synchronization mechanism over different pairs of the same type (dependency and constituency). On the contrary, our approach builds parallel derivations by simply having trees inside a same tree set and links are built explicitly for the sharing of arguments. Our methods seems to operate on two different axes (vertical vs horizontal) but further analysis will"
W08-2311,C92-1034,0,0.0736994,"ould have to be established. Dealing with Non Fixed Tree-Set’s Cardinality So far, we assumed that the grammar will provide the correct cardinality of a tree set (namely the correct number of unrealized elementary trees). Obviously, such an assumption cannot stand; it would lead to an exponential amount of elementary tree sets inside the grammar. In (Villemonte de La Clergerie, 2005), the author implements a proposal to handle this growing size problem using regular operators (mainly disjunction, Kleene star, interleaving and optionality) on nodes or subpart of a metagrammar tree description (Vijay-Shanker and Schabes, 1992; Candito, 1996). We argue for the use of the Kleene star and the optionality operator to cope with the potential exponential size of our MCTAG. The tree set α-aimer (Fig. 7) would then contain one main anchored tree, an optional unrealized Kleene starred tree of the form N0VN1 analysis of gapping coordination with any given number of conjuncts stands in TL-MCTAG ; its logical interpretation is simply a logical AND with n arguments. 3.4 Zeugma Construction and CUC To allow zeugma construction and CUC, we propose a set of trees that includes two different tree schemas, one of them being anchore"
W08-2311,C02-1028,0,0.0782926,"Missing"
W08-2311,W05-1522,0,0.181448,"Missing"
W08-2311,W90-0102,0,\N,Missing
W08-2311,W04-3316,0,\N,Missing
W09-1008,schluter-van-genabith-2008-treebank,0,\N,Missing
W09-1008,A00-2018,0,\N,Missing
W09-1008,J98-4004,0,\N,Missing
W09-1008,J04-4004,0,\N,Missing
W09-1008,W01-0521,0,\N,Missing
W09-1008,W06-1614,0,\N,Missing
W09-1008,J03-4003,0,\N,Missing
W09-1008,P03-1054,0,\N,Missing
W09-1008,P05-1038,0,\N,Missing
W09-1008,A00-1031,0,\N,Missing
W09-1008,P06-1055,0,\N,Missing
W09-1008,P05-1010,0,\N,Missing
W09-1008,P03-1013,0,\N,Missing
W09-1008,W04-3224,0,\N,Missing
W09-1008,N06-1020,0,\N,Missing
W09-3824,abeille-barrier-2004-enriching,0,0.0870136,"this question, this paper reports a set of experiments where five algorithms, first designed for the purpose of parsing English, have been adapted to French: a P CFG parser with latent annotation (Petrov et al., 2006), a Stochastic Tree Adjoining Grammar parser (Chiang, 2003), the Charniak’s lexicalized parser (Charniak, 2000) and the Bikel’s implementation of Collins’ Model 1 and 2 (Collins, 1999) described in (Bikel, 2002). To ease further comparisons, we report results on two versions of the treebank: (1) the last version made available in December 2007, hereafter F TB , and described in (Abeillé and Barrier, 2004) and the (2) L FG inspired version of (Schluter and van Genabith, 2007). The paper is structured as follows : After a brief presentation of the treebanks, we discuss the useThis paper presents preliminary investigations on the statistical parsing of French by bringing a complete evaluation on French data of the main probabilistic lexicalized and unlexicalized parsers first designed on the Penn Treebank. We adapted the parsers on the two existing treebanks of French (Abeillé et al., 2003; Schluter and van Genabith, 2007). To our knowledge, mostly all of the results reported here are state-of-th"
W09-3824,P05-1038,0,0.0683898,"zation for the parsing of French. 1 Introduction The development of large scale symbolic grammars has long been a lively topic in the French NLP community. Surprisingly, the acquisition of probabilistic grammars aiming at stochastic parsing, using either supervised or unsupervised methods, has not attracted much attention despite the availability of large manually syntactic annotated data for French. Nevertheless, the availability of the Paris 7 French Treebank (Abeillé et al., 2003), allowed (Dybro-Johansen, 2004) to carry out the extraction of a Tree Adjoining Grammar (Joshi, 1987) and led (Arun and Keller, 2005) 1 This has been made available in December 2007. 150 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 150–161, c Paris, October 2009. 2009 Association for Computational Linguistics fulness of testing different parsing frameworks over two parsing paradigms before introducing our experimental protocol and presenting our results. Finally, we discuss and compare with related works on cross-language parser adaptation, then we conclude. 2 Treebanks for French This section provides a brief overview to the corpora on which we report results: the French Treebank ("
W09-3824,H91-1060,0,0.121862,"Missing"
W09-3824,P04-1041,0,0.0395878,"Missing"
W09-3824,W08-2102,0,0.0249898,"no substitution node. Moreover, the probability model, being split between lexical anchors and tree templates, allows a very coarse grammar that contains, for example, only 83 tree templates for one treebank instantiation, namely the F TB - CC (cf. section 5). This behavior, although not documented10 , is close to Collins’ model 1, which does not use any argument adjunct distinction information, and led to results interesting enough to be integrated as the “Chiang Spinal” model in our parser set. It should be noted that, recently, the use of similar models has been independently proposed in (Carreras et al., 2008) with the purpose of getting a richer parsing model that can use non local features and in (Sangati and Zuidema, 2009) as a mean of extracting a Lexicalized Tree Substitution Grammar. In their process, the first extracted grammar is actually a spinal STIG. 4 Experimental protocol In this section, we specify the settings of the parsers for French, the evaluation protocol and the different instantiations of the treebanks we used for conducting the experiments. 4.1 Parsers settings Head Propagation table All lexicalized parsers reported in this paper use head propagation tables. Adapting them to"
W09-3824,A00-2018,0,0.849433,"set of trees. This path has the advantage of an easier reproducibility and eases verification of reported results. With the problem of the usability of the data source being solved, the question of finding one or many accurate language models for parsing French raises. Thus, to answer this question, this paper reports a set of experiments where five algorithms, first designed for the purpose of parsing English, have been adapted to French: a P CFG parser with latent annotation (Petrov et al., 2006), a Stochastic Tree Adjoining Grammar parser (Chiang, 2003), the Charniak’s lexicalized parser (Charniak, 2000) and the Bikel’s implementation of Collins’ Model 1 and 2 (Collins, 1999) described in (Bikel, 2002). To ease further comparisons, we report results on two versions of the treebank: (1) the last version made available in December 2007, hereafter F TB , and described in (Abeillé and Barrier, 2004) and the (2) L FG inspired version of (Schluter and van Genabith, 2007). The paper is structured as follows : After a brief presentation of the treebanks, we discuss the useThis paper presents preliminary investigations on the statistical parsing of French by bringing a complete evaluation on French da"
W09-3824,C02-1126,0,0.0564763,"Missing"
W09-3824,N06-1020,0,0.0337046,"as well as (Bikel, 2002)’s implementation of the Collins’ models 1 & 2 (Collins, 1999). Most of the lexicalized parsers we use in this work are well known and since their releases, almost ten years ago, their core parsing models still provide state-of-the-art performance on the standard test set for English.6 We insist on the fact that one of the goals of this work was to evaluate raw performance of well known parsing models on French annotated data. Thus, we have not considered using more complex parsing architectures that makes use of reranking (Charniak and Johnson, 2005) or self-training (McClosky et al., 2006) in order to improve the performance of a raw parsing model. Furthermore, studying and designing a set of features for a reranking parser was beyond the scope of this work. However, we did use some of these models in a non classical way, leading us to explore a Collins’ model 2 variation, named model X, and a Stochastic Tree Adjoining Grammar (Schabes, 1992; Resnik, 1992) variant7 , named Spinal Stochastic Tree Insertion Grammars (hereafter S PINAL S TIG), which was first used to validate the heuristics used by our adaptation of the Bikel’s parser to French. The next two subsections introduce"
W09-3824,P06-1055,0,0.133959,"a new released and corrected version of the treebank1 it was possible to train statistical parsers from the original set of trees. This path has the advantage of an easier reproducibility and eases verification of reported results. With the problem of the usability of the data source being solved, the question of finding one or many accurate language models for parsing French raises. Thus, to answer this question, this paper reports a set of experiments where five algorithms, first designed for the purpose of parsing English, have been adapted to French: a P CFG parser with latent annotation (Petrov et al., 2006), a Stochastic Tree Adjoining Grammar parser (Chiang, 2003), the Charniak’s lexicalized parser (Charniak, 2000) and the Bikel’s implementation of Collins’ Model 1 and 2 (Collins, 1999) described in (Bikel, 2002). To ease further comparisons, we report results on two versions of the treebank: (1) the last version made available in December 2007, hereafter F TB , and described in (Abeillé and Barrier, 2004) and the (2) L FG inspired version of (Schluter and van Genabith, 2007). The paper is structured as follows : After a brief presentation of the treebanks, we discuss the useThis paper presents"
W09-3824,D07-1066,0,0.0373258,"Missing"
W09-3824,P03-1013,0,0.012629,"vercome P CFG’s problems (a) and (b)5 . M FT 4739 28.38 2.11 6944 39 27 3.1 Lexicalized algorithms The first class of algorithms used are lexicalized parsers of (Collins, 1999; Charniak, 2000; Chiang, 2003). The insight underlying the lexicalized algorithms is to model lexical dependencies between a governor and its dependants in order to improve attachment choices. Even though it has been proven numerous times that lexicalization was useful for parsing the Wall Street Journal corpus (Collins, 1999; Charniak, 2000), the question of its relevance for other languages has been raised for German (Dubey and Keller, 2003; Kübler et al., 2006) and for French Table 1: Treebanks Properties 3 F TB A ADV C CL D ET I N P P+D P+PRO PONCT PREF PRO V Parsing Algorithms Although Probabilistic Context Free Grammars (P CFG) are a baseline formalism for probabilistic parsing, it is well known that they suffer from two problems: (a) The independence assumptions made by the model are too strong, and (b) For Natural Language Parsing, they do not take into account lexical probabilities. To date, most of the results on statistical parsing have been reported for English. Here we propose to investigate how to apply these techniq"
W09-3824,E09-1080,0,0.0120162,"lows a very coarse grammar that contains, for example, only 83 tree templates for one treebank instantiation, namely the F TB - CC (cf. section 5). This behavior, although not documented10 , is close to Collins’ model 1, which does not use any argument adjunct distinction information, and led to results interesting enough to be integrated as the “Chiang Spinal” model in our parser set. It should be noted that, recently, the use of similar models has been independently proposed in (Carreras et al., 2008) with the purpose of getting a richer parsing model that can use non local features and in (Sangati and Zuidema, 2009) as a mean of extracting a Lexicalized Tree Substitution Grammar. In their process, the first extracted grammar is actually a spinal STIG. 4 Experimental protocol In this section, we specify the settings of the parsers for French, the evaluation protocol and the different instantiations of the treebanks we used for conducting the experiments. 4.1 Parsers settings Head Propagation table All lexicalized parsers reported in this paper use head propagation tables. Adapting them to the French language requires to design French specific head propagation rules. To this end, we used those described by"
W09-3824,J98-4004,0,0.0648842,"kes advantage of the function labels annotated in the treebank. This is one of the main differences with the experiments described in (Arun and Keller, 2005) and (DybroJohansen, 2004) where the authors had to rely only on the very flat treebank structure without function labels, to annotate the arguments of a head. 3.2 Unlexicalized Parser As an instance of an unlexicalized parser, the last algorithm we use is the Berkeley unlexicalized parser (B KY) of (Petrov et al., 2006). This algorithm is an evolution of treebank transformation principles aimed at reducing P CFG independence assumptions (Johnson, 1998; Klein and Manning, 2003). Treebank transformations may be of two kinds (1) structure transformation and (2) labelling transformations. The Berkeley parser concentrates on (2) by recasting the problem of acquiring an optimal set of non terminal symbols as an semisupervised learning problem by learning a P CFG with Latent annotations (P CFG -L A): given an observed P CFG induced from the treebank, the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols. The parameters of the latent grammar are estimated from the actual tre"
W09-3824,J95-4002,0,0.210274,"arsing Algorithms Although Probabilistic Context Free Grammars (P CFG) are a baseline formalism for probabilistic parsing, it is well known that they suffer from two problems: (a) The independence assumptions made by the model are too strong, and (b) For Natural Language Parsing, they do not take into account lexical probabilities. To date, most of the results on statistical parsing have been reported for English. Here we propose to investigate how to apply these techniques to another language – French – by testing two distinct enhancements 5 Except (Chiang, 2003) which is indeed a T REE I N (Schabes and Waters, 1995) parser but which must extract a lexicalized grammar from the set of context free rules underlying a treebank. SERTION GRAMMAR 153 a lexicalized PCFG can roughly be described as a set of stochastic rules of the form: P → Ln Ln−1 ..L1 H R1 .. Rm−1 Rm where Li , H, Ri and P are all lexicalized non terminals; P inherits its head from H (Bikel, 2004). The Collins’ model 2 deterministically labels some nodes of a rule to be arguments of a given Head and the remaining nodes are considered to be modifier non terminals (hereafter MNT). In this model, given a left-hand side symbol, the head and its arg"
W09-3824,C92-2066,0,0.129055,"Missing"
W09-3824,P03-1054,0,0.0141452,"f the function labels annotated in the treebank. This is one of the main differences with the experiments described in (Arun and Keller, 2005) and (DybroJohansen, 2004) where the authors had to rely only on the very flat treebank structure without function labels, to annotate the arguments of a head. 3.2 Unlexicalized Parser As an instance of an unlexicalized parser, the last algorithm we use is the Berkeley unlexicalized parser (B KY) of (Petrov et al., 2006). This algorithm is an evolution of treebank transformation principles aimed at reducing P CFG independence assumptions (Johnson, 1998; Klein and Manning, 2003). Treebank transformations may be of two kinds (1) structure transformation and (2) labelling transformations. The Berkeley parser concentrates on (2) by recasting the problem of acquiring an optimal set of non terminal symbols as an semisupervised learning problem by learning a P CFG with Latent annotations (P CFG -L A): given an observed P CFG induced from the treebank, the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols. The parameters of the latent grammar are estimated from the actual treebank Morphology and typog"
W09-3824,schluter-van-genabith-2008-treebank,0,0.0277974,"Missing"
W09-3824,W06-1614,0,0.0574525,"(a) and (b)5 . M FT 4739 28.38 2.11 6944 39 27 3.1 Lexicalized algorithms The first class of algorithms used are lexicalized parsers of (Collins, 1999; Charniak, 2000; Chiang, 2003). The insight underlying the lexicalized algorithms is to model lexical dependencies between a governor and its dependants in order to improve attachment choices. Even though it has been proven numerous times that lexicalization was useful for parsing the Wall Street Journal corpus (Collins, 1999; Charniak, 2000), the question of its relevance for other languages has been raised for German (Dubey and Keller, 2003; Kübler et al., 2006) and for French Table 1: Treebanks Properties 3 F TB A ADV C CL D ET I N P P+D P+PRO PONCT PREF PRO V Parsing Algorithms Although Probabilistic Context Free Grammars (P CFG) are a baseline formalism for probabilistic parsing, it is well known that they suffer from two problems: (a) The independence assumptions made by the model are too strong, and (b) For Natural Language Parsing, they do not take into account lexical probabilities. To date, most of the results on statistical parsing have been reported for English. Here we propose to investigate how to apply these techniques to another languag"
W09-3824,I08-3008,0,0.0258923,"’ S MODEL 1, S PINAL S TIG, C HAR NIAK and B KY ). For this earlier experiment, our implementation of the C OLLINS MODEL 1 actually corresponds to the MODEL X without an argument adjunct distinction table. More precisely, the absence of argument nodes, used for the acquisition of subcategorization frames features, makes the M ODEL X parsing model consider all the nodes of a rule, ex15 Due to the lack of function annotation labels in this treebank, (Arun and Keller, 2005)’s argument distinction table was used for this experiment. 16 Note that the C HARNIAK’s parser has been adapted for Danish (Zeman and Resnik, 2008) ; the authors report a 80.20 F1 score for a specific instance of the Danish Treebank. PARSER Arun (acl05) Arun (this paper) Schluter (pacling07) Collins (Mx) Collins (M2) Collins (M1) Charniak Chiang (Sp) Bky F TBA RUN 80.45 81.08 81.5 79.36 77.82 82.35 80.94 84.03 M FT S CHLU 79.95 80,96 79,91 82,66 81,86 82.86 Table 6: Labeled bracket scores on Arun’s F TB version and on the M FT In order to favour a “fair” comparison between our work and (Arun and Keller, 2005), we also ran their best adaptation of the C OLLINS MODEL 2 on their treebank version using our own head rules set15 and obtained 8"
W09-3824,J93-2004,0,\N,Missing
W09-3824,J04-4004,0,\N,Missing
W10-1401,P05-1038,0,0.0195671,"arily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 on N EGRA. They showed that usi"
W10-1401,W10-1408,1,0.785091,"Missing"
W10-1401,W10-1404,0,0.236375,"s is substantial lexical data sparseness due to high morphological variation in surface forms. The question is therefore, given our finite, and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of stat"
W10-1401,H91-1060,0,0.0333452,"Missing"
W10-1401,E03-1005,0,0.0521711,"Missing"
W10-1401,W06-2920,0,0.219462,"red MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on multilingual dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) demonstrated that dependency parsing for MRLs is quite challenging. While dependency parsers are adaptable to many languages, as reflected in the multiplicity of the languages covered,1 the analysis by Nivre et al. (2007b) shows that the best result was obtained for English, followed by Catalan, and that the most difficult languages to parse were Arabic, Basque, and Greek. Nivre et al. (2007a) drew a somewhat typological conclusion, that languages with rich morphology and free word order are the hardest to parse. This was shown to be the case for both MaltParser (Nivre e"
W10-1401,W10-1409,1,0.834713,"and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of"
W10-1401,W08-2102,0,0.0218025,"Missing"
W10-1401,A00-2018,0,0.0303149,"Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologic"
W10-1401,P00-1058,0,0.0168692,"Missing"
W10-1401,W10-1406,0,0.0567829,"Missing"
W10-1401,P99-1065,0,0.261618,"Missing"
W10-1401,P97-1003,0,0.183573,"r) are reflected in the form of words, morphological information is often secondary to other syntactic factors, such as the position of words and their arrangement into phrases. German, an Indo-European language closely related to English, already exhibits some of the properties that make parsing MRLs problematic. The Semitic languages Arabic and Hebrew show an even more extreme case in terms of the richness of their morphological forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lex"
W10-1401,H05-1100,0,0.0111384,"ful in parsing English are necessarily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 o"
W10-1401,2008.jeptalnrecital-long.17,1,0.829354,"Missing"
W10-1401,P03-1013,0,0.0556766,"rted to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s"
W10-1401,P05-1039,0,0.0235903,"cal forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not"
W10-1401,P08-1109,0,0.0528707,"Missing"
W10-1401,W10-1412,1,0.240586,"various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of the SPMRL workshop for addressing such challenges. 4 Parsing MRLs: Recurring Trends The first workshop on parsing MRLs features 11"
W10-1401,E09-1038,1,0.7929,"isambiguate the morphological analyses of input forms? Should we do that prior to parsing or perhaps jointly with it?2 Representation and Modeling: Assuming that the input to our system reflects morphological information, one way or another, which types of morpho2 Most studies on parsing MRLs nowadays assume the gold standard segmentation and disambiguated morphological information as input. This is the case, for instance, for the Arabic parsing at CoNLL 2007 (Nivre et al., 2007a). This practice deludes the community as to the validity of the parsing results reported for MRLs in shared tasks. Goldberg et al. (2009), for instance, show a gap of up to 6pt F1 -score between performance on gold standard segmentation vs. raw text. One way to overcome this is to devise joint morphological and syntactic disambiguation frameworks (cf. (Goldberg and Tsarfaty, 2008)). logical information should we include in the parsing model? Inflectional and/or derivational? Case information and/or agreement features? How can valency requirements reflected in derivational morphology affect the overall syntactic structure? In tandem with the decision concerning the morphological information to include, we face genuine challenges"
W10-1401,W05-0303,0,0.043074,"Missing"
W10-1401,P08-1067,0,0.0516751,"Missing"
W10-1401,P03-1054,0,0.00472369,"rphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s parser emulation of Collins’ model 2 (Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (200"
W10-1401,W06-1614,0,0.154599,"Missing"
W10-1401,P95-1037,0,0.0299787,"Missing"
W10-1401,W10-1407,0,0.0148488,"elements into account, and thus learn the different distributions associated with morphologically marked elements in constituency structures, to improve performance. In addition to free word order, MRLs show higher degree of freedom in extraposition. Both of these phenomena can result in discontinuous structures. In constituency-based treebanks, this is either annotated as additional information which has to be recovered somehow (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that th"
W10-1401,J93-2004,0,0.0355629,". We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations. 1 Introduction The availability of large syntactically annotated corpora led to an explosion of interest in automatically inducing models for syntactic analysis and disambiguation called statistical parsers. The development of successful statistical parsing models for English focused on the Wall Street Journal Penn Treebank (PTB, (Marcus et al., 1993)) as the primary, and sometimes only, resource. Since the initial release of the Penn Treebank (PTB Marcus et Among the arguments that have been proposed to explain this performance gap are the impact of small data sets, differences in treebanks’ annotation schemes, and inadequacy of the widely used PARS E VAL evaluation metrics. None of these aspects in isolation can account for the systematic performance deterioration, but observed from a wider, crosslinguistic perspective, a picture begins to emerge – that the morphologically rich nature of some of the languages makes them inherently more s"
W10-1401,W10-1402,0,0.0399505,"Missing"
W10-1401,E06-1011,0,0.0266021,"labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the"
W10-1401,P05-1012,0,0.11798,"Missing"
W10-1401,P05-1013,0,0.0312259,"how (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with medium"
W10-1401,P09-1040,0,0.0260548,"D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare"
W10-1401,N07-1051,0,0.0143897,"markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on"
W10-1401,P06-1055,0,0.0941097,"tained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representatio"
W10-1401,D07-1066,1,0.534932,"Missing"
W10-1401,P81-1022,0,0.830446,"Missing"
W10-1401,W07-2219,1,0.909816,"Missing"
W10-1401,C08-1112,1,0.709478,"Missing"
W10-1401,W10-1405,1,0.846235,"Missing"
W10-1401,W09-3820,1,0.856092,"e other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare/unknown words is increased. One way to cope with the one of both aspects of this problem is through clustering, that is, providing an abstract representation over word forms that reflects their shared morphological and morphosyntactic aspects. This was done, for instance, in previous work on parsing German. Versley and Rehbein (2009) cluster words according to linear context features. These clusters include valency information added to verbs and morphological features such as case and number added to pre-terminal nodes. The clusters are then integrated as features in a discriminative parsing model to cope with unknown words. Their discriminative model thus obtains state-of-the-art results on parsing German. 8 Several contribution address similar challenges. For constituency-based generative parsers, the simple technique of replacing word forms with more abstract symbols is investigated by (Seddah et al., 2010; Candito and"
W10-1401,W10-1411,0,\N,Missing
W10-1401,W10-1403,0,\N,Missing
W10-1401,W10-1410,1,\N,Missing
W10-1401,W08-1008,0,\N,Missing
W10-1401,P05-1022,0,\N,Missing
W10-1401,P08-1043,1,\N,Missing
W10-1401,D07-1096,0,\N,Missing
W10-1409,P08-1037,0,0.0208222,"actic clustering to improve transition-based dependency parsing for English : using an available 30 million word corpus parsed with a constituency parser, words are represented as vectors of paths within the obtained constituency parses. Words are then clustered using a similarity metric between vectors of syntactic paths. The clusters are used as features to help a transition-based dependency parser. Note that the word representation for clustering is more complex (paths in parse trees), thus these authors have to cluster a smaller vocabulary : the top 5000 most frequent words are clustered. Agirre et al. (2008) use the same approach of replacing words by more general symbols, but these symbols are semantic classes. They test various methods to assign semantic classes (gold semantic class, most-frequent sense in sense-tagged data, or a fully unsupervised sense tagger). Though the method is very appealing, the reported improvement in parsing is rather small, especially for the fully unsupervised method. Versley and Rehbein (2009) cluster words according to linear context features, and use the clusters as features to boost discriminative German parsing for unknown words. Another approach to augment 83"
W10-1409,J92-4003,0,0.184561,"m Entropy training to learn PM and PL , we use the M ORFETTE models described in (Seddah et al., 2010), that are trained using the Averaged Sequence Perceptron algorithm (Freund and Schapire, 1999). The two classification models incorporate additional features calculated using the Lefff lexicon. Table 1 shows detailed results on dev set and test set of the F TB - UC, when M ORFETTE is trained on the F TB - UC training set. To the best of our knowledge the parts-of-speech tagging performance is state-of-the-art for French3 and the lemmatization performance has no comparable results. We use the Brown et al. (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al., 2008) and named entity recognition (Liang, 2005). The algorithm to obtain C clusters is as follows: each of the C most frequent tokens of the corpus is assigned its own distinct cluster. For the (C + 1)th most frequent token, create a (C + 1)th cluster. Then for each pair among the C + 1 resulting clusters, merge the pair that minimizes the loss in the likelihood of the corpus, according to a bigram language model defined on the clusters. Repeat this operation for the (C + 2)th mos"
W10-1409,W09-3821,1,0.669433,"leads to better probability estimates for these words. 1 Introduction Statistical parsing techniques have dramatically improved over the last 15 years, yet lexical data sparseness remains a critical problem. And the richer the morphology of a language, the sparser the treebankdriven lexicons will be for that language. Koo et al. (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. Their clusters are obtained using unsupervised clustering, which makes it possible to use a raw corpus containing several million words. Candito and Crabbé (2009) applied clustering to generative constituency parsing for French. They use a desinflection step that removes some inflection marks from word forms and then replaces them with word clusters, resulting in a significant improvement in parsing performance. Clustering words seems useful as a way of addressing the lexical data sparseness problem, since counts on clusters are more reliable and lead to better probability estimates. Clustering also appears to address the mismatch of vocabularies between the original treebank and any external, potentially out-of-domain corpus: clusters operate as an in"
W10-1409,candito-etal-2010-statistical,1,0.0468684,"al symbols. F1 <40 is the F-Measure combining labeled precision and labeled recall for sentences of less than 40 words. All other metrics are for all sentences of the dev set/test set. UAS = Unlabeled attachement score of converted constituency trees into surface dependency trees. All metrics ignore punctuation tokens. recall) both for sentences of less than 40 words, and for all sentences7 . We also use the unlabeled attachment score (UAS), obtained when converting the constituency trees output by the B KY parsers into surface dependency trees, using the conversion procedure and software of (Candito et al., 2010)8 . Punctuation tokens are ignored in all metrics. 7 Discussion Results are shown in table 3. Our hope was that using lemmatization would improve overall accuracy of unsupervised clustering, hence leading to better parsing performance. However, results using both methods are comparable. 7 Note that often for statistical constituent parsing results are given for sentences of less than 40 words, whereas for dependency parsing, there is no such limitation. The experiment D FL and D FL +C LUST &gt;200 are reproduced from the previous work (Candito and Crabbé, 2009). More precisely, this previous work"
W10-1409,chrupala-etal-2008-learning,0,0.0884213,"Missing"
W10-1409,Y09-1013,0,0.0212872,"Missing"
W10-1409,E09-1038,0,0.0872854,"ls are semantic classes. They test various methods to assign semantic classes (gold semantic class, most-frequent sense in sense-tagged data, or a fully unsupervised sense tagger). Though the method is very appealing, the reported improvement in parsing is rather small, especially for the fully unsupervised method. Versley and Rehbein (2009) cluster words according to linear context features, and use the clusters as features to boost discriminative German parsing for unknown words. Another approach to augment 83 the known vocabulary for a generative probabilistic parser is the one pursued in (Goldberg et al., 2009). Within a plain PCFG, the lexical probabilities for words that are rare or absent in the treebank are taken from an external lexical probability distribution, estimated using a lexicon and the Baulm-Welch training of an HMM tagger. This is proven useful to better parse Hebrew. 9 Conclusion and future work We have provided a thorough study of the results of parsing word clusters for French. We showed that the clustering improves performance both for unseen and rare words and for medium- to highfrequency words. For French, preprocessing words with desinflection or with tagging+lemmatisation lea"
W10-1409,P03-1054,0,0.0192231,"Missing"
W10-1409,P08-1068,0,0.684428,"e for words that are originally either unknown or low-frequency, since these words are replaced by cluster symbols that tend to have higher frequencies. Furthermore, clustering also helps significantly for medium to high frequency words, suggesting that training on word clusters leads to better probability estimates for these words. 1 Introduction Statistical parsing techniques have dramatically improved over the last 15 years, yet lexical data sparseness remains a critical problem. And the richer the morphology of a language, the sparser the treebankdriven lexicons will be for that language. Koo et al. (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. Their clusters are obtained using unsupervised clustering, which makes it possible to use a raw corpus containing several million words. Candito and Crabbé (2009) applied clustering to generative constituency parsing for French. They use a desinflection step that removes some inflection marks from word forms and then replaces them with word clusters, resulting in a significant improvement in parsing performance. Clustering words seems useful as a way of addressing the le"
W10-1409,P95-1037,0,0.554961,"Missing"
W10-1409,P05-1010,0,0.0214152,"B KY, which is a constituent parser that has been proven to perform well for French (Crabbé and Candito, 2008; Seddah et al., 2009), 1 More precisely the partition is : first 1235 sentences for test, next 1235 sentences for development, and remaining 9881 sentences for training. 77 though a little lower than a combination of a tagger plus the dependency-based MST parser (Candito et al., 2010). Though PCFG-style parsers operate on too narrow a domain of locality, splitting symbols according to structural and/or lexical properties is known to help parsing (Klein and Manning., 2003). Following (Matsuzaki et al., 2005), the B KY algorithm uses EM to estimate probabilities on symbols that are automatically augmented with latent annotations, a process which can be viewed as symbol splitting. It iteratively evaluates each such split and merges back the less beneficial ones. Crabbé and Candito (2008) show that some of the information carried by the latent annotations is lexical, since replacing words by their gold part-of-speech tag leads to worse results than the corresponding perfect tagging test, with words unchanged. This is a clear indication that lexical distinctions are used, and percolate up the parse t"
W10-1409,P06-1055,0,0.0214456,"Missing"
W10-1409,W09-3829,0,0.0844018,".60 97.21 8.73 89.67 90.07 Table 5: Tagging accuracy and UAS scores for modified terminal symbols in the dev set, grouped by ranges of frequencies in the modified training sets. The “replaced by UNKC*” line corresponds to the case where the desinflected form or the POS+lemma pair does not appear more than 200 times in the L’est Républicain corpus. unsupervised Brown clustering, which uses very local information, the higher counts lead to better estimates even for high-frequency words. 8 Related work We have already cited the previous work of Koo et al. (2008) which has directly inspired ours. Sagae and Gordon (2009) explores the use of syntactic clustering to improve transition-based dependency parsing for English : using an available 30 million word corpus parsed with a constituency parser, words are represented as vectors of paths within the obtained constituency parses. Words are then clustered using a similarity metric between vectors of syntactic paths. The clusters are used as features to help a transition-based dependency parser. Note that the word representation for clustering is more complex (paths in parse trees), thus these authors have to cluster a smaller vocabulary : the top 5000 most frequ"
W10-1409,sagot-2010-lefff,0,0.0105068,"method, without resorting to part-ofspeech tagging. We propose an alternate method here, which uses lemmas and part-of-speech tags that are output by a tagger/lemmatizer. Because counts on lemmas are more reliable, clustering over lemmas presumably produces clusters that are more reliable than those produced by clustering over desinflected forms. However, this approach does create a constraint in which automatically tagged and lemmatized text is required as input to the parser, leading to the introduction of tagging errors. Both morphological clustering methods make use of the Lefff lexicon (Sagot, 2010). Before we describe these two methods, we briefly give basic information on French inflectional morphology and on the Lefff. 4.1 French inflection and the Lefff lexicon French nouns appear in singular and plural forms, and have an intrinsic gender. The number and gender of a noun determines the number and gender of determiners, adjectives, past participles that depend on it. Hence in the general case, past participles and adjectives have four different forms. The major inflectional variation appears for finite verbs that vary for tense, mood, person and number. A regular verb may correspond t"
W10-1409,P81-1022,0,0.817009,"Missing"
W10-1409,W09-3820,0,0.0944092,"word representation for clustering is more complex (paths in parse trees), thus these authors have to cluster a smaller vocabulary : the top 5000 most frequent words are clustered. Agirre et al. (2008) use the same approach of replacing words by more general symbols, but these symbols are semantic classes. They test various methods to assign semantic classes (gold semantic class, most-frequent sense in sense-tagged data, or a fully unsupervised sense tagger). Though the method is very appealing, the reported improvement in parsing is rather small, especially for the fully unsupervised method. Versley and Rehbein (2009) cluster words according to linear context features, and use the clusters as features to boost discriminative German parsing for unknown words. Another approach to augment 83 the known vocabulary for a generative probabilistic parser is the one pursued in (Goldberg et al., 2009). Within a plain PCFG, the lexical probabilities for words that are rare or absent in the treebank are taken from an external lexical probability distribution, estimated using a lexicon and the Baulm-Welch training of an HMM tagger. This is proven useful to better parse Hebrew. 9 Conclusion and future work We have provi"
W10-1409,W10-1410,1,\N,Missing
W10-1410,P04-1041,1,0.868231,"Missing"
W10-1410,W09-3821,1,0.89873,"Missing"
W10-1410,W09-1008,1,0.89151,"Missing"
W10-1410,A00-2018,0,0.735055,"rrections (referred to as the Modified French Treebank MFT) to support grammar acquisition for PCFG-based LFG Parsing (Cahill et al., 2004) while Crabbé and Candito (2008) slightly modified the original F TB POS tagset to optimize the grammar with latent annotations extracted by the Berkeley parser (B KY, (Petrov et al., 2006)). Moreover, research oriented towards adapting more complex parsing models to French showed that lexicalized models such as Collins’ model 2 (Collins, 1999) can be tuned to cope effectively with the flatness of the annotation scheme in the F TB, with the Charniak model (Charniak, 2000) performing particularly well, but outperformed by the B KY parser on French data (Seddah et al., 2009). Focusing on the lexicon, experiments have been carried out to study the impact of different forms of word clustering on the B KY parser trained on the F TB. Candito et al. (2009) showed that using gold lemmatization provides a significant increase in performance. Obviously, less sparse lexical data which retains critical pieces of information can only help a model to perform better. This was shown in (Candito and Crabbé, 2009) where distributional word clusters were acquired from a 125 mill"
W10-1410,chrupala-etal-2008-learning,1,0.815209,"Missing"
W10-1410,Y09-1013,0,0.14934,"Missing"
W10-1410,C94-2149,0,0.129478,"Missing"
W10-1410,E09-1038,0,0.139265,"Missing"
W10-1410,P06-1055,0,0.345326,"Missing"
W10-1410,D07-1066,1,0.886037,"Missing"
W10-1410,sagot-etal-2006-lefff,0,0.0720109,"Missing"
W10-1410,W09-3820,0,0.130258,"Missing"
W10-1410,J93-2004,0,\N,Missing
W10-1410,J03-4003,0,\N,Missing
W10-4413,E03-1030,0,0.0806901,"Missing"
W10-4413,2005.jeptalnrecital-court.13,0,0.631025,"e Laurence Danlos Alpage & Univ. Paris 7 Paris, France djame.seddah@paris-sorbonne.fr benoit.sagot@inria.fr laurence.danlos@linguist.jussieu.fr Abstract In this paper1 we present an extension of MCTAGs with Local Shared Derivation (Seddah, 2008) which can handle non local elliptic coordinations. Based on a model for control verbs that makes use of so-called ghost trees, we show how this extension leads to an analysis of argument cluster coordinations that provides an adequate derivation graph. This is made possible by an original interpretation of the MCTAG derivation tree mixing the views of Kallmeyer (2005) and Weir (1988). 1 Introduction Elliptic coordinate structures are a challenge for most constituent-based syntactic theories. To model such complex phenomena, many works have argued in favor of factorized syntactic structures (Maxwell and Manning, 1996), while others have argued for distributive structures that include a certain amount of non-lexically realized elements (Beavers and Sag, 2004). Of course, the boundary between those two approaches is not sharp since one can decide to first build a factorized syntactic analysis and then construct a more distributive structure (e.g., logical or"
W10-4413,W10-4412,0,0.249106,"element. (1) Jean aimei Marie et Paul εi Virginie John lovesi Mary and Paul εi Virginia Calling this second lexically unrealized tree a ghost tree, the missing anchor can be retrieved simply because the tree it anchors is in the same MC-Set as its ghost tree. In other words, the label of the MCSet includes the anchor of its fully lexicalized tree. The application of this model to (1) is shown in Figure 1. Note that this account only requires the expressivity of Tree-Local MCTAGs and that unlike other approaches for gapping in the LTAG framework (Sarkar and Joshi, 1996; Seddah and Sagot, 2006; Lichte and Kallmeyer, 2010), this proposal for gapping does not require any special device or modification of the formalism itself. In order to model derivations that involve the elision of one syntactic verbal argument as in right node raising cases (RNR) or right subject elision coordinations, the formalism is extended with oriented links, called local shared derivation (local SD), between mandatory derivation site nodes: whenever a derivation is not realized on a given node and assuming that a local SD has been defined between this node and one possible antecedent, a derivation between those nodes is inserted in the"
W10-4413,C94-2151,0,0.0386764,"showed that, assuming the use of regular operators to handle n-ary coordinations, a broad range of coordinate structures could be processed using a TreeLocal MCTAG-based formalism named Tree Local MCTAG with Local Shared Derivations. Nevertheless, being tied to the domain of locality of a tree set, the very nature of this mechanism forbids the sharing of derivations between different tree sets, thus preventing it from analyzing non-local elliptic coordinations. In this paper, we introduce an extension of this model that can handle non-local elliptic coordination — close to unbounded ellipsis (Milward, 1994) —, which can be found in structures involving 2 See (Abeillé, 2006; Mouret, 2006) for discussions about this assumption. 101 Djamé Seddah, Benoit Sagot, Laurence Danlos α-aimer    a)               α-et ✟ N0↓ S ✟❍ ✟✟ ❍ ❍ V [aimer] b) ❍ N1↓          ✟ S↓ Sc ✟❍ ✟✟ ❍❍ N0↓ V N1↓        ε  S ✟❍ ❍ ✟ ❍ et Sc ↓ α-X N α-et ✟❍ ✟✟ ❍❍ ✟✟ α-aimer(a) ✟❍ ✟ ❍ ✟ ❍ α-Jean α-Marie ❍❍ α-aimer(b) ✟❍ ✟ ❍ ✟ ❍ α-Paul α-Virginie X={Jean|Marie|Paul|Virginie} Figure 1: Sketch of an analysis for “Jean aime Marie et Paul Virignie” The root label of α-aimer(b) is subscripted in order to av"
W10-4413,W06-1522,1,0.639988,"is anchored by an empty element. (1) Jean aimei Marie et Paul εi Virginie John lovesi Mary and Paul εi Virginia Calling this second lexically unrealized tree a ghost tree, the missing anchor can be retrieved simply because the tree it anchors is in the same MC-Set as its ghost tree. In other words, the label of the MCSet includes the anchor of its fully lexicalized tree. The application of this model to (1) is shown in Figure 1. Note that this account only requires the expressivity of Tree-Local MCTAGs and that unlike other approaches for gapping in the LTAG framework (Sarkar and Joshi, 1996; Seddah and Sagot, 2006; Lichte and Kallmeyer, 2010), this proposal for gapping does not require any special device or modification of the formalism itself. In order to model derivations that involve the elision of one syntactic verbal argument as in right node raising cases (RNR) or right subject elision coordinations, the formalism is extended with oriented links, called local shared derivation (local SD), between mandatory derivation site nodes: whenever a derivation is not realized on a given node and assuming that a local SD has been defined between this node and one possible antecedent, a derivation between th"
W10-4413,W08-2311,1,0.829275,"ill a challenge for theories based on strict atomic category coordination. In the broader context of ellipsis resolution, Dalrymple et al. (1991) propose to consider elided elements as free logical variables resolved using Higher Order Unification as the solving operation. Inspired by this approach and assuming that non-constituent coordination can be analyzed with ellipsis (Beavers and Sag, 2004),2 we consider elliptic coordination as involving parallel structures where all non lexically realized syntactic elements must be represented in a derivation structure. This path was also followed by Seddah (2008) who proposed to use the ability of Multi Component TAGs (MCTAGs) (Weir, 1988) to model such a parallelism by including conjunct trees in a same tree set. This simple proposal allows for a straightforward analysis of gapping constructions. The coverage of this account is then extended by introducing links called local shared derivations which, by allowing derivations to be shared across trees of a same set, permit to handle various elliptic coordinate structures in an efficient way. This work showed that, assuming the use of regular operators to handle n-ary coordinations, a broad range of coo"
W10-4413,W07-0402,0,0.0495269,"Missing"
W10-4413,C90-3045,0,0.712309,"Missing"
W10-4413,W05-1522,0,0.0307018,"her end of the link. This mechanism can be generalized to MCSets with more than one local shared derivation. This skteches the proof that the set of languages generated by MCTAG-LSDs is the same as that generated by MCTAGs. Therefore, MCTAG-LSDs and MCTAGs have the same weak generative capacity. Moreover, these considerations still hold while restricting GM CT AG to be TL-MCTAG. Therefore, TL-MCTAG-LSDs and TL-MCTAGs have the same weak generative power. In order to cope with very large grammar size, the use of regular operators to factorize out TAG trees has been proposed by (Villemonte de La Clergerie, 2005), and has lead to a drastic reduction of the number of trees in the grammar. The resulting formalism is called factorized TAGs and was adapted by Seddah (2008) to the MCTAG-LSD framework in order to handle n-ary coordinations. The idea is to factorize MCTAG-LSD sets that have the same underlying MCTAG set (i.e. they are identical if links are ignored). Indeed, all such MC sets can be merged into one unique tree set associated with the union of all corresponding link sets. However, as with factorized TAGs, we need to add to the resulting tree set a list of constraints, R, on the construction of"
W11-2905,W01-0521,0,0.149561,"Missing"
W11-2905,W10-1408,0,0.0471068,"Missing"
W11-2905,P08-1068,0,0.258581,"Missing"
W11-2905,J92-4003,0,0.155403,"learn a grammar from the word-clustered sentences in the training set; (iii) parse the word-clustered sentences in the test set; (iv) reintroduce the original tokens into the test sentences to obtain the final parsed output. The clustering is performed in two steps: (i) a morphological clustering is applied using the Lefff morphological lexicon (Sagot et al., 2006), where plural and feminine suffixes are removed from word forms and past/future tenses are mapped to present tense (provided this does not change the part-of-speech ambiguity of the form); (ii) an unsupervised clustering algorithm (Brown et al., 1992) is run on a large unlabeled corpus to learn clusters over the desinflected forms. Both clustering steps proved to be beneficial for parsing in-domain French text using the Berkeley parser. We apply a similar unsupervised word clustering technique to lexical domain adaptation, with the difference being that clusters are learned over a mixture of source-domain and target-domain text (hereafter mixed clusters). We test this technique when training a parser on the F TB training set as well as in self-training mode (McClosky and Charniak, 2008), where the parser is trained on both the source-domai"
W11-2905,P08-2026,0,0.575521,"guity of the form); (ii) an unsupervised clustering algorithm (Brown et al., 1992) is run on a large unlabeled corpus to learn clusters over the desinflected forms. Both clustering steps proved to be beneficial for parsing in-domain French text using the Berkeley parser. We apply a similar unsupervised word clustering technique to lexical domain adaptation, with the difference being that clusters are learned over a mixture of source-domain and target-domain text (hereafter mixed clusters). We test this technique when training a parser on the F TB training set as well as in self-training mode (McClosky and Charniak, 2008), where the parser is trained on both the source-domain training set and automatically parsed sentences from the target domain. 4 Symbols raw dfl clt-er clt-er-emea F-Measure on EMEA test set (≤ 40) No self-training 200k self-training 81.25 84.75 81.82 84.72 82.65 85.09 83.53 85.19 Table 2: F-Measure for sentences ≤ 40 tokens on the EMEA test set, both with self-training (200k autoparsed sentences from EmeaFrU) and without. freely available at CNRTL7 . Though this newspaper is less formal than Le Monde, it is still journalistic, so we consider it as being in the source domain. The mixed cluste"
W11-2905,W09-3821,1,0.958802,"the corpus, as many sentences provide general information or recommendations that are repeated in every EPAR document. In the end, the resulting preprocessed corpus (hereafter EmeaFrU) contains approximately 5.3 million tokens and 267 thousand sentences. 3 Lexical Domain Adaptation In our approach to domain adaption, we use unsupervised word clustering performed on a mixture of target-domain (biomedical) and source-domain (journalistic) text. The objective is to obtain clusters grouping together source-domain and targetdomain words, thus bridging the two vocabularies. We build on the work of Candito and Crabbé (2009), who proposed a technique to improve indomain parsing by reducing lexical data sparse2.3 Manual Bracketing Annotation To evaluate parsing performance, we manually annotated two extracts of the EmeaFrU corpus, cor3 Dev Set 574 16.2 9,346 Table 1: Statistics on the EMEA dev and test sets. alpha-lc stands for tokens converted to lowercase and containing at least one letter. Unknown tokens/types are those absent from the F TB training set. 2.2 Corpus Preprocessing 2 Test Set 544 21.5 11,679 4 We plan to make the manually-annotated corpus freely available, following a final validation step. Docume"
W11-2905,P06-1043,0,0.168543,"Missing"
W11-2905,N03-4009,0,0.0825179,"Missing"
W11-2905,C10-2013,1,0.909869,"Missing"
W11-2905,W07-2204,1,0.915983,"Missing"
W11-2905,N10-1060,0,0.231922,"Missing"
W11-2905,W10-2606,0,0.0467208,"set (Gildea, 2001; McClosky et al., 2006; Foster, 2010). However, the gap between this intrinsic evaluation methodology, which is only able to provide a ranking of some parser/treebank pairs using a given metric, and the growing need for accurate wide coverage parsers suitable for coping with an unlimited stream of new data, is currently being tackled more widely. Thus, the task of parsing out-of-domain text becomes crucial. Various techniques have been proposed to adapt existing parsing models to new genres: domain adaptation via self training (Bacchiani et al., 2006; McClosky et al., 2006; Sagae, 2010), co-training (Steedman et al., 2003), treebank and target transformation (Foster, 2010), source-domain target 2 Target Domain Corpus For our work on domain adaptation, we used the French Treebank (F TB) (Abeillé and Barrier, 2004) as the source domain corpus, which consists of 12,351 sentences from the Le Monde newspaper. For the target domain, we used biomedical texts from the European Medicines Agency, specifically the French part of the EMEA section1 of the OPUS corpus (Tiedemann, 2009). Although we chose the biomedical domain for this paper, our approach can be used for any target domain."
W11-2905,sagot-etal-2006-lefff,0,0.0230839,"Missing"
W11-2905,A97-1015,0,0.183391,"Missing"
W11-2905,N03-1031,0,0.175872,"y et al., 2006; Foster, 2010). However, the gap between this intrinsic evaluation methodology, which is only able to provide a ranking of some parser/treebank pairs using a given metric, and the growing need for accurate wide coverage parsers suitable for coping with an unlimited stream of new data, is currently being tackled more widely. Thus, the task of parsing out-of-domain text becomes crucial. Various techniques have been proposed to adapt existing parsing models to new genres: domain adaptation via self training (Bacchiani et al., 2006; McClosky et al., 2006; Sagae, 2010), co-training (Steedman et al., 2003), treebank and target transformation (Foster, 2010), source-domain target 2 Target Domain Corpus For our work on domain adaptation, we used the French Treebank (F TB) (Abeillé and Barrier, 2004) as the source domain corpus, which consists of 12,351 sentences from the Le Monde newspaper. For the target domain, we used biomedical texts from the European Medicines Agency, specifically the French part of the EMEA section1 of the OPUS corpus (Tiedemann, 2009). Although we chose the biomedical domain for this paper, our approach can be used for any target domain. 2.1 Corpus Characteristics The EMEA"
W11-2905,N07-1051,0,\N,Missing
W11-2905,I05-1006,0,\N,Missing
W11-2905,D10-1069,0,\N,Missing
W11-2905,W10-1409,1,\N,Missing
W11-2905,abeille-barrier-2004-enriching,0,\N,Missing
W12-3408,W10-1408,1,0.916496,"Missing"
W12-3408,W10-1409,1,0.884663,"ols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are collected on the training set, ranked according to their information gain with regards to the partof-speech tagging task. Hence, all the experiments are presented in two settings. In the first one, called generic, unknown words are replaced with a dummy token UNK"
W12-3408,chrupala-etal-2008-learning,0,0.220092,"Missing"
W12-3408,J05-1003,0,0.0105145,"nce (around 2.3 points better, see Table 8). Table 7: Lemmmatization Experiments In all cases reduced2 is below the other tagsets wrt. to Parseval F1 although tagging accuracy is better. We can conclude that it is too poor from an informational point of view. 4 Discussion There is relatively few works actively pursued on statistical constituency parsing for Spanish. The initial work of Cowan and Collins (2005) consisted in a thorough study of the impact of various morphological features on a lexicalized parsing model (the Collins Model 1) and on the performance gain brought by the reranker of Collins and Koo (2005) used in conjunction with the feature set developed for English. Direct comparison is difficult as they used a different test set (approximately, the concatenation of our development and test sets). They report an F-score of 85.1 on sentences of length less than 40.5 However, we are directly comparable with Chrupała (2008)6 who adapted the Collins Model 2 to Spanish. As he was focusing on wide coverage LFG grammar induction, he enriched the non terminal annotation scheme with functional paths rather than trying to obtain the optimal tagset with respect to pure parsing performance. Nevertheless"
W12-3408,H05-1100,0,0.200659,"ce techniques. We rely on accurate data-driven lemmatization and partof-speech tagging to reduce data sparseness and ease the burden on the parser. We try to see how we can improve parsing structure predictions solely by modifying the terminals and/or the preterminals of the trees. We keep the rest of the tagset as is. In order to validate our method, we perform experiments on the Cast3LB constituent treebank for Spanish (Castillan). This corpus is quite small, around 3,500 trees, and Spanish is known to have a rich verbal morphology, making the tag set quite complex and difficult to predict. Cowan and Collins (2005) and Chrupała (2008) already showed interesting results on this corpus that will provide us with a comparison for this work, especially on the lexical aspects as they used lexicalized frameworks while we choose PCFG-LAs. This paper is structured as follows. In Section 2 we describe the Cast3LB corpus in details. In Section 3 we present our experimental setup and results which we discuss and compare in Section 4. Finally, Section 5 concludes the presentation. 2 Data Set The Castillan 3LB treebank (Civit and Martì, 2004) contains 3,509 constituent trees with functional annotations. It is divided"
W12-3408,Y09-1013,1,0.835017,"strategy dubbed simple lexicon in the Berkeley parser. Rare words – words occurring less than 3 times in the training set – are replaced by a special token, which depends on the OOV handling method (generic or IG), before collecting counts. POS tagging We performed parsing experiments with three different settings regarding POS information provided as an input to the parser: (i) with no POS information, which constitutes our baseline; (ii) with gold POS information, which can be considered as a topline for a given parser setting; (iii) with POS information predicted using the MElt POS-tagger (Denis and Sagot, 2009), using three different tagsets that we describe below. MElt is a state-of-the-art sequence labeller that is trained on both an annotated corpus and an external lexicon. The standard version of MElt relies on Maximum-Entropy Markov models (MEMMs). However, in this work, we have used a multiclass perceptron instead, as it allows for much faster training with very small performance drops (see Table 2). For training purposes, we used the training section of the Cast3LB (76,931 tokens) and the Leffe lexicon (Molinero et al., 2009), which contains almost 800,000 distinct (form, category) pairs.3 We"
W12-3408,P05-1010,0,0.0162841,"iments We conducted experiments on the Cast3LB development set in order to test various treebank modifications, that can be divided in two categories: (i) modification of the preterminal symbols of the treebank by using simplified POS tagsets; (ii) modification of the terminal symbols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-"
W12-3408,R09-1049,1,0.830632,"ng; (iii) with POS information predicted using the MElt POS-tagger (Denis and Sagot, 2009), using three different tagsets that we describe below. MElt is a state-of-the-art sequence labeller that is trained on both an annotated corpus and an external lexicon. The standard version of MElt relies on Maximum-Entropy Markov models (MEMMs). However, in this work, we have used a multiclass perceptron instead, as it allows for much faster training with very small performance drops (see Table 2). For training purposes, we used the training section of the Cast3LB (76,931 tokens) and the Leffe lexicon (Molinero et al., 2009), which contains almost 800,000 distinct (form, category) pairs.3 We performed experiments using three different 1 Names generic and IG originally come from Attia et al. (2010). 2 We tried to perform 4 and 5 rounds but 3 rounds proved to be optimal on this corpus. 3 Note that MElt does not use information from the exterTAGSET baseline reduced2 Nb. of tags 106 42 Multiclass Perceptron Overall Acc. 96.34 97.42 Unk. words Acc. 91.17 93.35 Maximum-Entropy Markov model (MEMM) Overall Acc. 96.46 97.42 Unk. words Acc. 91.57 93.76 reduced3 57 97.25 92.30 97.25 92.87 Table 2: MElt POS tagging accuracy"
W12-3408,W08-1005,0,0.0710606,"rd tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are collected on the training set, ranked according to their information gain with regards to the partof-speech tagging task. Hence, all the experiments are presented in two settings. In the first one, called generic, unknown words are replaced with a dummy token UNK, while in the second one, dubbed"
W12-3408,P06-1055,0,0.111335,"riments on the Cast3LB development set in order to test various treebank modifications, that can be divided in two categories: (i) modification of the preterminal symbols of the treebank by using simplified POS tagsets; (ii) modification of the terminal symbols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are"
W12-3408,P81-1022,0,0.762867,"Missing"
W12-3408,N07-1051,0,\N,Missing
W12-4625,W11-0108,0,0.241602,"Missing"
W12-4625,candito-etal-2010-statistical,0,0.107162,"nces grouped in sets illustrating the various syntactic phenomena and their configuration. Given the annotations and the parse trees, the algorithm basically tries to generalize over the selected nodes and edges, through the following steps: 2. The tree view (right hand panel) where one can select nodes and edges (the red dotted lines); 3. The triggering part (bottom panel) of the induced transformation rule, to be then edited and completed by the transformation part. The examples described below are annotated with the CoNLL scheme used for the dependency version of the French Treebank (FTB) (Candito et al., 2010a; Abeillé et al., 2003). Our goal is to construct a new version of the FTB with deeper syntactic annotations, as a first step towards a shallow semantic representation for FTB. Hence, in the figures 6, 7 and 9, we illustrate some complex syntactic constructions and try to exhibit simple transformations, using the constraints. In the examples, we use the following color code: • Red edges for the final edges after all constraint applications, • Green edges for the initial constrained edges, 1. Extract a graph from the annotations of the first annotated parse tree • Blue edges for non constraine"
W12-4625,C10-2013,0,0.13744,"nces grouped in sets illustrating the various syntactic phenomena and their configuration. Given the annotations and the parse trees, the algorithm basically tries to generalize over the selected nodes and edges, through the following steps: 2. The tree view (right hand panel) where one can select nodes and edges (the red dotted lines); 3. The triggering part (bottom panel) of the induced transformation rule, to be then edited and completed by the transformation part. The examples described below are annotated with the CoNLL scheme used for the dependency version of the French Treebank (FTB) (Candito et al., 2010a; Abeillé et al., 2003). Our goal is to construct a new version of the FTB with deeper syntactic annotations, as a first step towards a shallow semantic representation for FTB. Hence, in the figures 6, 7 and 9, we illustrate some complex syntactic constructions and try to exhibit simple transformations, using the constraints. In the examples, we use the following color code: • Red edges for the final edges after all constraint applications, • Green edges for the initial constrained edges, 1. Extract a graph from the annotations of the first annotated parse tree • Blue edges for non constraine"
W12-4625,C08-1069,0,0.0209318,"n explosion of the number of transformation rules, difficult to create and maintain. And, when not bounding these non local phenomena, it becomes necessary to introduce recursive transformation rules that raise delicate problems of ordering when applying them, as presented in the Grew system (Bonfante et al., 2011b) based on graph rewriting rules. Many approaches have been proposed for tree or graph transformations, such as Top-Down or Bottom-Up Tree Transducers (Courcelle and Engelfriet, 2012), Tree-Walking Transducers (Boja´nczyk, 2008), Synchronous Grammars (Shieber and Schabes, 1990) and (Matsuzaki and Tsujii, 2008) for an application on annotation scheme conversion, or Graph Rewriting Systems based, for instance, on the Single PushOut model (SPO) (Löwe et al., 1993; Geiss et al., 2006). But either they are complex to implement or they suffer from the above mentioned problems (coverage, maintenance, ordering). Moreover, they are not always suited for natural language processing, especially in case of complex phenomena. Based on a preliminary experiment of scheme to scheme transformation, but motivated by more generic linguistic considerations, we propose a simple new two stage model. The first stage esse"
W12-4625,C90-3045,0,0.0812113,"re canonical cases may lead to an explosion of the number of transformation rules, difficult to create and maintain. And, when not bounding these non local phenomena, it becomes necessary to introduce recursive transformation rules that raise delicate problems of ordering when applying them, as presented in the Grew system (Bonfante et al., 2011b) based on graph rewriting rules. Many approaches have been proposed for tree or graph transformations, such as Top-Down or Bottom-Up Tree Transducers (Courcelle and Engelfriet, 2012), Tree-Walking Transducers (Boja´nczyk, 2008), Synchronous Grammars (Shieber and Schabes, 1990) and (Matsuzaki and Tsujii, 2008) for an application on annotation scheme conversion, or Graph Rewriting Systems based, for instance, on the Single PushOut model (SPO) (Löwe et al., 1993; Geiss et al., 2006). But either they are complex to implement or they suffer from the above mentioned problems (coverage, maintenance, ordering). Moreover, they are not always suited for natural language processing, especially in case of complex phenomena. Based on a preliminary experiment of scheme to scheme transformation, but motivated by more generic linguistic considerations, we propose a simple new two"
W13-4905,P05-1038,0,0.511442,"Missing"
W13-4905,D12-1133,0,0.0376615,"the pipeline systems is based on Conditional Random Fields (CRF) (Lafferty et al., 2001) and on external lexicons following (Constant and Tellier, 2012). Given a tokenized text, it jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other1 . CRF is a prominent statistical model for sequence segmenta1 Note though that we keep only the MWE segmentation, and use rather the Morfette tagger-lemmatizer, cf. section 4. 47 • Mate-tools 2, the joint POS tagger and transition-based parser with graph-based completion available in Mate-tools (Bohnet and Nivre, 2012). 2 We use the version available in the POS tagger MElt (Denis and Sagot, 2009). 3 We use the version in the platform Unitex (http://igm.univmlv.fr/˜unitex). We had to convert the DELA POS tagset to the FTB one. 4 http://igm.univ-mlv.fr/˜unitex 5 Available at http://code.google.com/p/mate-tools/. We used the Anna3.3 version. Such parsers require some preprocessing of the input text: lemmatization, POS tagging, morphology analyzer (except the joint POS tagger and transition-based parser that does not require preprocessed POS tagging). We competed for the scenario in which this information is no"
W13-4905,C10-1011,0,0.0317819,"ernor’s POS in the syntactic parse, the POS following the MWE, the POS preceding the MWE, the bigram of the POS following and preceding the MWE. e´ pargne avait ferm´e la 4 Dependency Parsers For our development, we trained 3 types of parsers, both for the pipeline and the joint architecture: veille Figure 1: French dependency tree for La caisse d’´epargne avait ferm´e la veille (The savings bank had closed the day before), containing two MWEs (in red). • MALT, a pure linear-complexity transitionbased parser (Nivre et al., 2006) • Mate-tools 1, the graph-based parser available in Mate-tools5 (Bohnet, 2010) 3 MWE Analyzer and MWE Tagger The MWE analyzer we used in the pipeline systems is based on Conditional Random Fields (CRF) (Lafferty et al., 2001) and on external lexicons following (Constant and Tellier, 2012). Given a tokenized text, it jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other1 . CRF is a prominent statistical model for sequence segmenta1 Note though that we keep only the MWE segmentation, and use rather the Morfette tagger-lemmatizer, cf. section 4. 47 • Mate-tools 2, the joint POS tagger and transition-based p"
W13-4905,C10-2013,1,0.853303,"relabeled ”dep cpd P”. At evaluation time, the output parse labels are remapped to the official annotation scheme. 8 More precisely, we based our implementation on the pseudo-code given in (McDonald, 2006). 9 http://igm.univ-mlv.fr/˜mconstan 48 plates. The MWE tagger model was trained using the Wapiti software(Lavergne et al., 2010). We used the default parameters and we forced the MaxEnt mode. Parsers For MALT (version 1.7.2), we used the arceager algorithm, and the liblinear library for training. As far as the features are concerned, we started with the feature templates given in Bonsai10 (Candito et al., 2010), and we added some templates (essentially lemma bigrams) during the development tests, that slightly improved performance. For the two Matetools parsers, we used the default feature sets and parameters proposed in the documentation. Morphological prediction Predicted lemmas, POS and morphology features are computed with Morfette version 0.3.5 (Chrupała et al., 2008; Seddah et al., 2010)11 , using 10 iterations for the tagging perceptron, 3 iterations for the lemmatization perceptron, default beam size for the decoding of the joint prediction, and the Lefff (Sagot, 2010) as external lexicon us"
W13-4905,constant-tellier-2012-evaluating,1,0.918112,"ation scheme. As shown in Figure 1, such trees contain not only syntactic dependencies, but also the grouping of tokens into MWEs, since the first component of an MWE bears dependencies to the subsequent components of the MWE with a specific label dep_cpd. At that stage, the only missing information is the POS of the MWEs, which we predict by applying a MWE tagger in a post-processing step. mo d d’ t de d cp d cp pc pd p p caisse de de la de au x tp s suj tion and labelling. External lexicons used as sources of features greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). Our lexical resources are composed of two large-coverage general-language lexicons: the Lefff2 lexicon (Sagot, 2010), which contains approx. half a million inflected word forms, among which approx. 25, 000 are MWEs; and the DELA3 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110, 000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex4 : the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. The MWE tagger we used in the joint systems takes"
W13-4905,P12-1022,1,0.924909,"mance in tasks such as machine translation (Pal et al., 2011), there has been relatively little work exploiting MWE recognition to improve parsing performance. Indeed, a classical parsing scenario is to pregroup MWEs using gold MWE annotation (Arun Djam´e Seddah Alpage Paris Sorbonne Univ INRIA and Keller, 2005). This non-realistic scenario has been shown to help parsing (Nivre and Nilsson, 2004; Eryigit et al., 2011), but the situation is quite different when switching to automatic MWE prediction. In that case, errors in MWE recognition alleviate their positive effect on parsing performance (Constant et al., 2012). While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing (Cafferkey et al., 2007; Green et al., 2011; Constant et al., 2012; Green et al., 2013), the French dataset of the SPMRL 2013 Shared Task (Seddah et al., 2013) offers one of the first opportunities to evaluate this scenario within the framework of dependency syntax. In this paper, we discuss the systems we submitted to the SPMRL 2013 shared task. We focused our participation on the French dependency parsing track using t"
W13-4905,Y09-1013,0,0.259198,"tructure comply with the French dataset annotation scheme. As shown in Figure 1, such trees contain not only syntactic dependencies, but also the grouping of tokens into MWEs, since the first component of an MWE bears dependencies to the subsequent components of the MWE with a specific label dep_cpd. At that stage, the only missing information is the POS of the MWEs, which we predict by applying a MWE tagger in a post-processing step. mo d d’ t de d cp d cp pc pd p p caisse de de la de au x tp s suj tion and labelling. External lexicons used as sources of features greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). Our lexical resources are composed of two large-coverage general-language lexicons: the Lefff2 lexicon (Sagot, 2010), which contains approx. half a million inflected word forms, among which approx. 25, 000 are MWEs; and the DELA3 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110, 000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex4 : the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names."
W13-4905,C96-1058,0,0.0712235,"-lemmatizer Morfette (Chrupała et al., 2008; Seddah et al., 2010), in order to apply a jackknifing on the training set, so that parsers are made less sensitive to tagging errors. Note that no feature pertaining to MWEs are used at this stage. 5 Reparser The reparser is an adaptation to labeled dependency parsing of the simplest6 system proposed in (Sagae and Lavie, 2006). The principle is to build an arcfactored merge of the parses produced by n input parsers, and then to find the maximum spanning tree among the resulting merged graph7 . We implemented the maximum spanning tree algorithm8 of (Eisner, 1996) devoted to projective dependency parsing. During the parse merging, each arc is unlabeled, and is given a weight, which is the frequency it appears in the n input parses. Once the maximum spanning tree is found, each arc is labeled by its most voted label among the m input parses containing such an arc (with arbitrary choice in case of ties). 6 Experiments 6.1 Settings MWE Analysis and Tagging For the MWE analyzer, we used the tool lgtagger9 (version 1.1) with its default set of feature tem6 The other more complex systems were producing equivalent scores. 7 In order to account for labeled MWE"
W13-4905,W11-3806,0,0.126514,", English, or German, a large quantity of MWE resources have been generated (Baldwin and Nam, 2010). Yet, while special treatment of complex lexical units, such as MWEs, has been shown to boost performance in tasks such as machine translation (Pal et al., 2011), there has been relatively little work exploiting MWE recognition to improve parsing performance. Indeed, a classical parsing scenario is to pregroup MWEs using gold MWE annotation (Arun Djam´e Seddah Alpage Paris Sorbonne Univ INRIA and Keller, 2005). This non-realistic scenario has been shown to help parsing (Nivre and Nilsson, 2004; Eryigit et al., 2011), but the situation is quite different when switching to automatic MWE prediction. In that case, errors in MWE recognition alleviate their positive effect on parsing performance (Constant et al., 2012). While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing (Cafferkey et al., 2007; Green et al., 2011; Constant et al., 2012; Green et al., 2013), the French dataset of the SPMRL 2013 Shared Task (Seddah et al., 2013) offers one of the first opportunities to evaluate this scenario"
W13-4905,D11-1067,0,0.217349,"Missing"
W13-4905,J13-1009,0,0.219606,"Missing"
W13-4905,P10-1052,0,0.0698905,"Missing"
W13-4905,nivre-etal-2006-maltparser,0,0.0481328,"lemma, the sequence of POS of its components, the POS of its first component, its governor’s POS in the syntactic parse, the POS following the MWE, the POS preceding the MWE, the bigram of the POS following and preceding the MWE. e´ pargne avait ferm´e la 4 Dependency Parsers For our development, we trained 3 types of parsers, both for the pipeline and the joint architecture: veille Figure 1: French dependency tree for La caisse d’´epargne avait ferm´e la veille (The savings bank had closed the day before), containing two MWEs (in red). • MALT, a pure linear-complexity transitionbased parser (Nivre et al., 2006) • Mate-tools 1, the graph-based parser available in Mate-tools5 (Bohnet, 2010) 3 MWE Analyzer and MWE Tagger The MWE analyzer we used in the pipeline systems is based on Conditional Random Fields (CRF) (Lafferty et al., 2001) and on external lexicons following (Constant and Tellier, 2012). Given a tokenized text, it jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other1 . CRF is a prominent statistical model for sequence segmenta1 Note though that we keep only the MWE segmentation, and use rather the Morfette tagger-lemmatizer"
W13-4905,2011.mtsummit-papers.23,0,0.0370541,"both pipeline architecture (MWE recognition followed by parsing), and joint architecture (MWE recognition performed by the parser). 1 Introduction As shown by the remarkable permanence over the years of specialized workshops, multiword expressions (MWEs) identification is still receiving considerable attention. For some languages, such as Arabic, French, English, or German, a large quantity of MWE resources have been generated (Baldwin and Nam, 2010). Yet, while special treatment of complex lexical units, such as MWEs, has been shown to boost performance in tasks such as machine translation (Pal et al., 2011), there has been relatively little work exploiting MWE recognition to improve parsing performance. Indeed, a classical parsing scenario is to pregroup MWEs using gold MWE annotation (Arun Djam´e Seddah Alpage Paris Sorbonne Univ INRIA and Keller, 2005). This non-realistic scenario has been shown to help parsing (Nivre and Nilsson, 2004; Eryigit et al., 2011), but the situation is quite different when switching to automatic MWE prediction. In that case, errors in MWE recognition alleviate their positive effect on parsing performance (Constant et al., 2012). While the realistic scenario of synta"
W13-4905,N06-2033,0,0.0791526,"reprocessed POS tagging). We competed for the scenario in which this information is not gold but predicted. Instead of using the predicted POS, lemma and morphological features provided by the shared task organizers, we decided to retrain the tagger-lemmatizer Morfette (Chrupała et al., 2008; Seddah et al., 2010), in order to apply a jackknifing on the training set, so that parsers are made less sensitive to tagging errors. Note that no feature pertaining to MWEs are used at this stage. 5 Reparser The reparser is an adaptation to labeled dependency parsing of the simplest6 system proposed in (Sagae and Lavie, 2006). The principle is to build an arcfactored merge of the parses produced by n input parsers, and then to find the maximum spanning tree among the resulting merged graph7 . We implemented the maximum spanning tree algorithm8 of (Eisner, 1996) devoted to projective dependency parsing. During the parse merging, each arc is unlabeled, and is given a weight, which is the frequency it appears in the n input parses. Once the maximum spanning tree is found, each arc is labeled by its most voted label among the m input parses containing such an arc (with arbitrary choice in case of ties). 6 Experiments"
W13-4905,sagot-2010-lefff,0,0.124615,"nce the first component of an MWE bears dependencies to the subsequent components of the MWE with a specific label dep_cpd. At that stage, the only missing information is the POS of the MWEs, which we predict by applying a MWE tagger in a post-processing step. mo d d’ t de d cp d cp pc pd p p caisse de de la de au x tp s suj tion and labelling. External lexicons used as sources of features greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). Our lexical resources are composed of two large-coverage general-language lexicons: the Lefff2 lexicon (Sagot, 2010), which contains approx. half a million inflected word forms, among which approx. 25, 000 are MWEs; and the DELA3 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110, 000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex4 : the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. The MWE tagger we used in the joint systems takes as input a MWE within a dependency tree, and outputs its POS. It is a pointwise classifier, based on a MaxEnt model th"
W13-4905,W10-1410,1,0.930496,"Missing"
W13-4905,chrupala-etal-2008-learning,0,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W14-6111,W13-4916,0,0.216734,"Missing"
W14-6111,H91-1060,0,0.0351043,"hological features for each input segment were not. In the raw setting, there was no gold information, i.e., morphological segmentation, POS tags and morphological features for each input token had to be predicted as part of the parsing task. To lower the entry cost, participants were provided with reasonable baseline (if not state-of-the-art) morphological predictions (either disambiguated – in most cases– or ambiguous prediction in lattice forms). As a consequence of the raw scenario, it was not possible to (only) rely on the accepted parsing metrics, labeled bracket evaluation via E VALB1 (Black et al., 1991), Leaf-Ancestor (Sampson and Babarczy, 2003) for constituents and C O NLL X’s Labeled/Unlabeled Attachment Score for dependencies (Buchholz and Marsi, 2006). When the segmentation of words into input tokens is not given, there may be discrepancies on the lexical levels, which neither E VALB and L EAF -A NCESTOR nor LAS/UAS are prepared to handle. Thus, we also used TedEval, a distance-based metric that evaluates a morphosyntactic structure as a complete whole (Tsarfaty et al., 2012b). Note that given the workload brought to the participants, we did not try to enforce function label evaluation"
W14-6111,J92-4003,0,0.188544,"description of the unlabeled-data preparation that is required in the context of parsing MRLs, and the core labeled data that is used in conjunction with it. 4.1 SPMRL Unlabeled Data Set One of the common problems when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks,"
W14-6111,W06-2920,0,0.830006,"aselines for parsing morphologically rich languages (MRLs). The goals were both to provide a focal point for researchers interested in parsing MRLs and consequently to advance the state of the art in this area of research. The shared task focused on parsing nine morphologically rich languages, from different typological language families, in both a constituent-based and a dependency-based format. The set of nine typologically diverse languages comprised data sets for Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, and Swedish. Compared to previous multilingual shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), the SPMRL shared task targeted parsing in realistic evaluation scenarios, in which the analysis of morphologically ambiguous input tokens is not known in advance. An additional novelty of the SPMRL shared task is that it allowed for both a dependency-based and a constituentbased parse representation. This setting relied on an intricate and careful data preparation process which ensured consistency between the constituent and the dependency version by aligning the two representation types at the token level and at the level of part-of-speech tags. For all languages, we pr"
W14-6111,W09-3821,0,0.0148692,"Missing"
W14-6111,W10-1409,1,0.562386,"t is used in conjunction with it. 4.1 SPMRL Unlabeled Data Set One of the common problems when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexic"
W14-6111,W13-4909,0,0.0198006,"Missing"
W14-6111,W13-4905,1,0.815015,"e Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the"
W14-6111,E09-1038,1,0.836445,"gically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabeled data and their pre-annotations wi"
W14-6111,C10-1045,0,0.0646224,"X X parsed X* X X* X X X X* X X Table 2: Unlabeled data set properties.*: made available mid-july 4.2 SPMRL Core Labeled Data Set In order to provide a faithful evaluation of the impact of these additional sets of unlabeled data, we used the exact same data sets for training and testing as in the previous edition. Specifically, we used an Arabic data set, originally provided by the LDC (Maamouri et al., 2004), in a dependency form, derived from the Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009) and in a constituency instance, following the Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the T"
W14-6111,P09-2056,0,0.0145597,"re newswire wiki (edited) balanced size (tree tokens) 120M 150M 120M 205M 160M 100M 40M 100M 24M morph X* X X+mwe X X X X X X parsed X* X X* X X X X* X X Table 2: Unlabeled data set properties.*: made available mid-july 4.2 SPMRL Core Labeled Data Set In order to provide a faithful evaluation of the impact of these additional sets of unlabeled data, we used the exact same data sets for training and testing as in the previous edition. Specifically, we used an Arabic data set, originally provided by the LDC (Maamouri et al., 2004), in a dependency form, derived from the Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009) and in a constituency instance, following the Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both insta"
W14-6111,P08-2015,0,0.0097065,"when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabele"
W14-6111,mengel-lezius-2000-xml,0,0.179115,"reebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedish data originate from (Nivre et al., 2006), we added function labels extracted from the original Swedish XML data. Note that in addition to constituency and dependency versions, the Polish, German and Swedish data sets are also available in the Tiger XML format (Mengel and Lezius, 2000), allowing a direct representation of discontinuous structures in their phrase-based structures. 5 Conclusion At the time of writing this short introduction, the shared task is ongoing, and neither results nor the final submitting teams are known. At this point, we can say that 15 teams registered for the 2014 shared task edition, indicating an increased awareness of and continued interest in the topic of the shared task. Results, cross-parser and cross-data analysis, and shared task description papers will be made available at http://www.spmrl.org/spmrl2014-sharedtask.html. Acknowledgments We"
W14-6111,nivre-etal-2006-talbanken05,0,0.0181359,"match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedish data originate from (Nivre et al., 2006), we added function labels extracted from the original Swedish XML data. Note that in addition to constituency and dependency versions, the Polish, German and Swedish data sets are also available in the Tiger XML format (Mengel and Lezius, 2000), allowing a direct representation of discontinuous structures in their phrase-based structures. 5 Conclusion At the time of writing this short introduction, the shared task is ongoing, and neither results nor the final submitting teams are known. At this point, we can say that 15 teams registered for the 2014 shared task edition, indicating an increase"
W14-6111,W11-3808,0,0.0159627,"l of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabeled data and their pre-annotations will be provided in (Seddah et al., 2014). Note that we could not ensur"
W14-6111,seeker-kuhn-2012-making,0,0.0136617,"ue, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Po"
W14-6111,W10-1401,1,0.903368,"Missing"
W14-6111,E12-1006,1,0.924838,"enario, it was not possible to (only) rely on the accepted parsing metrics, labeled bracket evaluation via E VALB1 (Black et al., 1991), Leaf-Ancestor (Sampson and Babarczy, 2003) for constituents and C O NLL X’s Labeled/Unlabeled Attachment Score for dependencies (Buchholz and Marsi, 2006). When the segmentation of words into input tokens is not given, there may be discrepancies on the lexical levels, which neither E VALB and L EAF -A NCESTOR nor LAS/UAS are prepared to handle. Thus, we also used TedEval, a distance-based metric that evaluates a morphosyntactic structure as a complete whole (Tsarfaty et al., 2012b). Note that given the workload brought to the participants, we did not try to enforce function label evaluation for constituent parsing. We hope that further shared tasks will try to generalize such an evaluation. Indeed, having predicted function labels would ease labeled T ED E VAL evaluation and favor a full parsing chain evaluation. Nevertheless, the choice of T ED E VAL allowed us to go beyond the standard cross-parser evaluation within one setting and approach cross-framework (constituent vs. dependency (Tsarfaty et al., 2012a)) and cross-language evaluation, thus pushing the envelope"
W14-6111,P13-2103,1,0.454845,"the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedi"
W14-6111,J13-1003,1,\N,Missing
W14-6111,C08-1112,1,\N,Missing
W14-6111,P12-2002,1,\N,Missing
W14-6111,W13-4917,1,\N,Missing
W14-6111,vincze-etal-2010-hungarian,0,\N,Missing
W14-6111,D07-1096,1,\N,Missing
W16-3905,W09-3821,0,0.0643458,"Missing"
W16-3905,C10-2013,0,0.0199812,"had to make bananas the main i i 20 predicate, and treat freeze as a subordinate clause. Regardless of the difficulty of the domain, it appears that a UD dependency analysis lends itself to dependency annotation in an easier way: Since non-leaf relations appear between lexical words, this representation is more robust to missing determiners, prepositions and punctuations, even phrases. Also, if we used other dependency formalisms that for instance place prepositions as heads of nouns, it would be more difficult to annotate as it is the case using the current French Treebank dependency scheme (Candito et al., 2010). Nevertheless, dependency analyses conflate functional and structural information (Silveira and Manning, 2015), and some of the structural information can be lost in cases such as the Example C, discussed above. Annotating dependencies lends itself well to noisy user-generated data. In a strict lexicalist analysis such as UD, where there are no tokens for unobserved words (e.g. dropped subjects, missing main verbs), we must build a structure from the existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Inde"
W16-3905,N13-1037,0,0.0627008,"f the main causes behind the expectable performance drops in tagging and parsing. We therefore conduct a series of automatic and manual inspections to better understand the linguistic phenomena behind UGC linguistic variability. We explore the relation between predicting performance and annotation difficulty, which is seldom explicitly addressed (Plank et al., 2015). 4 A Threefold Categorisation for UGC Idiosyncrasies Even though user-generated content does not constitute a uniform genre, many works have characterised its idiosyncrasies (Foster, 2010; Gimpel et al., 2011; Seddah et al., 2012; Eisenstein, 2013), which can be characterised on three axes, defined by the intentionality or communication needs of the word variants: 1. Encoding simplification: This axis covers ergographic phenomena, i.e.,phenomena aiming at reducing the writing effort, perceived as first glance as genuine misspell errors, and transverse phe4 In fact, in the case of L EAGUE OF L EGENDS in-game data, the normalisation step adds a significant amount of noise. A solution to this problem and more generally to the limitations of deterministic rule-based normalisation lies in the development of non-supervised or semi-supervised"
W16-3905,P11-1118,0,0.0222664,"n. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organised by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lex"
W16-3905,W07-2204,1,0.86197,"Missing"
W16-3905,I11-1100,0,0.0964601,"Missing"
W16-3905,N10-1060,0,0.65778,"a, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL s"
W16-3905,W01-0521,0,0.0192116,"UGC, and (iii) the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typo"
W16-3905,P11-2008,0,0.306203,"Missing"
W16-3905,C16-1286,0,0.0226449,"shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, while longer sentences in our data have structures closer to more canonical syntax. 5.7 The Unspoken Costs of Treebank Annotation As we all know, creating annotated data is a rewarding task, extremely useful for evaluation as well as for building feature-rich supervised models. Yet, it is time consuming and as generally said, relatively costly (Schneider, 2015) even though crowd-sourcing solutions through games with a purpose start to emerge (Guillaume et al., 2016). The dataset we presented in that paper are part of process that was initiated 5 years ago when we were confronted to the lack of syntactically-annotated out-of-domain dataset for French. The purely syntactic annotation phase for the LoL and Minecraft data is still ongoing and we expect it to be finished in the first few months of 2017. It is important to consider that such a task was made possible because of the experience we gained along the years and because we relied on a highly trained team of annotators. This training was the most important point in term of costs and had to be extended"
W16-3905,D15-1157,0,0.166833,"Missing"
W16-3905,I05-1006,0,0.0412939,"gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training"
W16-3905,P08-2026,0,0.0138217,"and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct"
W16-3905,P06-1043,0,0.0310279,"the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences,"
W16-3905,N06-1020,0,0.0421604,"the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences,"
W16-3905,C14-1166,0,0.075608,"without normalisation. The tagger (Denis and Sagot, 2012) was trained on the canonical training section of the French Treebank (Abeillé et al., 2003) instance, F TB - UC, from (Candito and Crabbé, 2009). We used an extended version of the rewriting rules used to pre-annotate the French Social Media Bank (Seddah et al., 2012). They work jointly with the tagger to provide internal cleaned versions of a token, or a sequence of, which are tagged separately. Resulting POS tags are finally merged to the original token(s). (e.g.wanna → want/VB to/TO → wanna/VB+TO). UGC tagging (Seddah et al., 2012; Nooralahzadeh et al., 2014): normalisation helps to cover some of the most frequent lexical variations and hence improves substantially the tagging accuracy. However in the case of in-game L EAGUE OF L EGENDS chat session, the normalisation is detrimental to the overall tagging performance as well as for unseen words. One obvious hypothesis is simply that the rules are applied deterministically and assign wrong PoSs while letting the pure tagging model work alone provide reasonable assumption on what would be the correct label for an out-of-domain word. Let us add that the MElt tagger makes a heavy use of features extra"
W16-3905,P14-2083,0,0.0176469,"Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lexical level, at the very definition of what constitutes a minimal unit. Plank et al. (2014) attempt to quantify how much of the domain-specific variation of POS labeling is a result of different interpretations, and how much is arguably just noise. Regarding the study of French UGC, our starting point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our syntactic analysis in terms of dependency structures. 3 Data Collection and Part-of-Speech Annotation Our dataset contains three different sources of user-generated content. Two of them are logs of multiplayer video-game chat sessions, M INECRAFT and L EAGUE OF L EGENDS1 ,"
W16-3905,W15-1617,1,0.836342,"lf-training or other semi-supervised learning techniques. Therefore, the purpose of the present work is not only to potentially provide a new dataset to be used as additional training data for domain adaptation. Rather, we provide a close inspection of the main causes behind the expectable performance drops in tagging and parsing. We therefore conduct a series of automatic and manual inspections to better understand the linguistic phenomena behind UGC linguistic variability. We explore the relation between predicting performance and annotation difficulty, which is seldom explicitly addressed (Plank et al., 2015). 4 A Threefold Categorisation for UGC Idiosyncrasies Even though user-generated content does not constitute a uniform genre, many works have characterised its idiosyncrasies (Foster, 2010; Gimpel et al., 2011; Seddah et al., 2012; Eisenstein, 2013), which can be characterised on three axes, defined by the intentionality or communication needs of the word variants: 1. Encoding simplification: This axis covers ergographic phenomena, i.e.,phenomena aiming at reducing the writing effort, perceived as first glance as genuine misspell errors, and transverse phe4 In fact, in the case of L EAGUE OF L"
W16-3905,W15-1618,0,0.0197729,"e existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, while longer sentences in our data have structures closer to more canonical syntax. 5.7 The Unspoken Costs of Treebank Annotation As we all know, creating annotated data is a rewarding task, extremely useful for evaluation as well as for building feature-rich supervised models. Yet, it is time consuming and as generally said, relatively costly (Schneider, 2015) even though crowd-sourcing solutions through games with a purpose start to emerge (Guillaume et al., 2016). The dataset we presented in that paper are part of process that was initiated 5 years ago when we were confronted to the lack of syntactically-annotated out-of-domain dataset for French. The purely syntactic annotation phase for the LoL and Minecraft data is still ongoing and we expect it to be finished in the first few months of 2017. It is important to consider that such a task was made possible because of the experience we gained along the years and because we relied on a highly trai"
W16-3905,L16-1375,1,0.910238,"g point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our syntactic analysis in terms of dependency structures. 3 Data Collection and Part-of-Speech Annotation Our dataset contains three different sources of user-generated content. Two of them are logs of multiplayer video-game chat sessions, M INECRAFT and L EAGUE OF L EGENDS1 , the last one is made of instant cooking-related web questions from M ARMITON2 , a widely popular French recipe website. This set of questions was collected during the building of the French QuestionBank (Seddah and Candito, 2016) but was not described nor analysed because of its syntactic peculiarity and was thus considered by the authors as a clear outlier. We chose to include this sample in our study because it offers a sharp contrast with video games chat logs in term of domain variation while retaining a live nature: users asks questions related to their immediate needs and expect a quick answer. The L EAGUE OF L EGENDS data set was collected by Lamy (2015) in early 2015 and consists of two types of recorded user interactions: a first part is the record of discussions occurring during an on-going game session whil"
W16-3905,C12-1149,1,0.559586,"urse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organised by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lexical level, at the very definition of what constitutes a minimal unit. Plank et al. (2014) attempt to quantify how much of the domain-specific variation of POS labeling is a result of different interpretations, and how much is arguably just noise. Regarding the study of French UGC, our starting point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our synt"
W16-3905,W15-2134,0,0.0203327,"e difficulty of the domain, it appears that a UD dependency analysis lends itself to dependency annotation in an easier way: Since non-leaf relations appear between lexical words, this representation is more robust to missing determiners, prepositions and punctuations, even phrases. Also, if we used other dependency formalisms that for instance place prepositions as heads of nouns, it would be more difficult to annotate as it is the case using the current French Treebank dependency scheme (Candito et al., 2010). Nevertheless, dependency analyses conflate functional and structural information (Silveira and Manning, 2015), and some of the structural information can be lost in cases such as the Example C, discussed above. Annotating dependencies lends itself well to noisy user-generated data. In a strict lexicalist analysis such as UD, where there are no tokens for unobserved words (e.g. dropped subjects, missing main verbs), we must build a structure from the existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, whil"
W16-3905,L16-1262,0,\N,Missing
W17-6507,W00-1436,0,0.0169751,", 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-syntactic level, the two other levels are automatically pre-annotated step by step: the annotation of a given level is rewritten to the next level using the MATE tools (Bohnet et al., 2000). Results and Error Analysis We evaluated the production of enhanced UD graphs in two settings, depending on whether the input UD trees do (PA+) or do not (PA−) contain manual disambiguation of cases (a), (b) and (c) described above. For the PA− case, we applied basic default rules instead, known to use insufficient information. Table 1 reports the F-measures (computed considering all edges or N ∪ A edges only). These results confirm the validity of our approach and highlight the consistency of the resulting graphbanks. Moreover, even if manual preannotations are required in theory, we empiric"
W17-6507,F12-2024,1,0.786077,"Missing"
W17-6507,J16-4009,0,0.0211653,"ations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-syntactic level, the two other levels are automatical"
W17-6507,candito-etal-2014-deep,1,0.801748,"are implemented through diverse and, in some few cases, multilingual graphbanks. More clearly semantic schemes seem to depend on the needs of the downstream application or impose their own constraints on the syntactic layer it is either built upon or plugged in. See for example the differences between abstract meaning representations (Knight et al., 2014), designed with Machine Translation in sight, and the U DEP L AMBDA’s logical structures, very recently proposed by Reddy et al. (2017) and evaluated on a question-answering over a knowledge base task. In this paper, we build on the work of (Candito et al., 2014; Perrier et al., 2014) to propose an extension to the current enhanced dependency framework of Schuster and Manning (2016). First, we extend the types of argumental dependencies made explicit (taking into account participles, control nouns and adjectives, non-finite verbs and more cases of infinitive verbs). Second, we neutralize syntactic alternations, in order to make linking patterns more regular for a given verb form. We believe that making explicit and normalize the predicate-argument structures, still remaining at the syntactic level, can make downstream semantic analysis more straightf"
W17-6507,P13-2017,0,0.0733126,"Missing"
W17-6507,C16-1040,1,0.896954,"Missing"
W17-6507,W16-1715,0,0.0193644,"ransitives, then the canonical labels can be made explicit as shown in figure 9. Note that the canonical function of the Passive Passive is by far the most frequent syntactic alternation, and it is fortunately rather easy to identify in a language such as French. Note that because the UD scheme uses several labels for the same argumental slot, depending on the argument’s category, the basic rule of having the passive’s subject being the canonical direct object has to be split. The nsubj:pass dependent is considered the canonical obj. The csubj:pass dependent is 8 This is already identified by Gerdes and Kahane (2016), who advocate for directly adding the semantic argument rank (1,2,3...) on top of the syntactic label. 46 4.3 nsubj:pass argument is iobj if the verb has a direct object (Fig. 9a) or obj otherwise (Fig. 9b). nsubj:pass@iobj aux:pass (a) He was given case orders nsubj:pass@obj aux:pass (b) Orders Impersonal constructions can also be viewed as syntactic alternations: in French the postverbal complement has object-like properties (in particular the pronominalization with the quantitative clitic en (of-it)). obl@nsubj obj were by Impersonal them obl@iobj case given to nsubj@expl aux him Il It nsu"
W17-6507,W13-3724,0,0.0431398,"Missing"
W17-6507,P05-1011,0,0.012789,"Missing"
W17-6507,S14-2008,0,0.203066,"Missing"
W17-6507,F14-2031,1,0.923846,"gh diverse and, in some few cases, multilingual graphbanks. More clearly semantic schemes seem to depend on the needs of the downstream application or impose their own constraints on the syntactic layer it is either built upon or plugged in. See for example the differences between abstract meaning representations (Knight et al., 2014), designed with Machine Translation in sight, and the U DEP L AMBDA’s logical structures, very recently proposed by Reddy et al. (2017) and evaluated on a question-answering over a knowledge base task. In this paper, we build on the work of (Candito et al., 2014; Perrier et al., 2014) to propose an extension to the current enhanced dependency framework of Schuster and Manning (2016). First, we extend the types of argumental dependencies made explicit (taking into account participles, control nouns and adjectives, non-finite verbs and more cases of infinitive verbs). Second, we neutralize syntactic alternations, in order to make linking patterns more regular for a given verb form. We believe that making explicit and normalize the predicate-argument structures, still remaining at the syntactic level, can make downstream semantic analysis more straightforward (as shown for in"
W17-6507,W12-3602,0,0.0290507,"nnotated corpora and given the cost of annotations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-synt"
W17-6507,petrov-etal-2012-universal,0,0.115553,"Missing"
W17-6507,D17-1009,0,0.0785386,"Missing"
W17-6507,D09-1085,0,0.0169912,"e the rise of large annotated corpora and given the cost of annotations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting"
W17-6507,L16-1376,0,0.398273,"dah@paris-sorbonne.fr bruno.guillaume@loria.fr marie.candito@linguist.univ-paris-diderot.fr Abstract released annotated versions of their treebanks, following the UD annotation scheme. Although UD has raised criticisms, both on the suitability of the scheme to meet linguistic typology (Croft et al., 2017) and on the current implementation of the UD treebanks (Gerdes and Kahane, 2016), the existence of many treebanks with same syntactic scheme does however ease crosslanguage linguistic analysis and enables parsers to generalize across languages at training time, as demonstrated by Ammar et al. (2016). The UD scheme favors dependencies between content words, in order to maximize parallelism between languages. Although this results in dependencies that are more semantic-oriented, the UD scheme lies at the surface syntax level and thus necessarily lacks abstraction over syntactic variation and does not fit all downstream applications’ needs (Schuster and Manning, 2016). This is partly why de Marneffe and Manning (2008) proposed a decade ago, in the Stanford Dependencies framework, several schemes with various semantic-oriented modifications of syntactic structures. Its graph-based, so-called"
W17-6507,P04-1041,0,\N,Missing
W17-6507,L16-1262,0,\N,Missing
W19-4705,P16-1141,0,0.333196,"ts reveal that our proposed model is able to capture subtle semantic shifts without being biased towards frequency cues and also works well when certain contextual features are absent. Our model fits the data better than current state-of-the-art dynamic word embedding models and therefore is a promising tool to study diachronic semantic changes over small time periods. 1 torical linguistics. Recently, diachronic word embeddings based on distributional hypothesis (Harris, 1954) have been used to automatically study semantic changes in a data-driven fashion from large corpora (Kim et al., 2014; Hamilton et al., 2016; Rudolph and Blei, 2018). We refer the reader to Kutuzov et al. (2018) who survey the recent methods in this field and establishes the challenges that lie ahead. Currently, we find the literature on this problem to be focused on English corpora, spanning across several decades. This has not only created a gap in extending the diachronic word embeddings for a wider scope of languages, but also to datasets spanning across few successive years which are common in digital humanities and social sciences. In this work, we study French text from Twitter collected over just five years, which provides"
W19-6101,W19-4822,0,0.287041,"aracter-based representations were more robust to synthetic and natural noise than word-based approaches. However, they did not find a substantial improvement over BPE tokenization, their BPE MT system even slightly outperforming the character-based one on 3 out of 4 of their test sets, including the one with the highest OOV rate. Similarly to all these works, we also aim at comparing the performance of PBSMT and NMT approaches, hoping that the peculiarities of UGC will help us to better understand the pros and cons of these two methods. Our approach shares several similarity with the work of Anastasopoulos (2019) that described different experiments to determine how source-side errors can impact the translation quality of NMT models. 3 Experimental Setup As the goal of this work is to compare the output of NMT and PBSMT when translating UGC corpora. Because of the lack of manually translated UGC, we consider a out-domain scenario in which our systems are trained on the canonical corpora generally used in MT evaluation campaigns and tested on UGC data. We will first describe the datasets used in this work (§3.1), then the different systems we have considered (§3.2) and finally the pre- and post-process"
W19-6101,P17-1080,0,0.0302827,"he-box NMT system in a low-resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of hidden units, batch size, ...) are correctly tuned and a proper NMT architecture is selected. The situation for other NMT approaches, such as character-based NMT, is also confusing: Wu et al. (2016) have shown that character-based methods achieve state-of-the-art performance for different language pairs; Belinkov et al. (2017) and Durrani et al. (2019) have demonstrated their systems respective abilities to retrieve good amount of morphological information leveraging on subword level features. However, Belinkov and Bisk (2018) found that these approaches are not robust to noise (both synthetic and natural) when trained only with clean corpora. On the other hand, Durrani et al. (2019) concluded that character-based representations were more robust to synthetic and natural noise than word-based approaches. However, they did not find a substantial improvement over BPE tokenization, their BPE MT system even slightly ou"
W19-6101,D16-1025,0,0.178455,"s of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media a"
W19-6101,N19-1389,0,0.0270559,"the smaller training sets when used with the neural models, our results suggest that the specificities of UGC raise new challenges for NMT systems that cannot simply be solved by feeding ours models more data. Nevertheless, Koehn and Knowles (2017) highlighted 6 challenges faced by Neural Machine Translation, one of them being the lack of data for resource poordomain. This issue is strongly emphasized when it comes to UGC which does not constitute a domain on its own and which is subjected to a degree of variability only seen in the processing historical document over a large period of times (Bollmann, 2019) or in emerging dialects which can greatly varies over geographic or socio-demographic factors (transliterated Arabic dialects for example). This is why the availability of new UGC data sets is crucial and as such the release of the Cr#pbank is a welcome, small, stone in the edifice that will help evaluating machine translation architectures in near-real conditions such as blind testing. In order to avoid common leaderboard pitfalls in such settings, we did not use the Cr#pbank’s blind test set for any of our experiments, neither did we for the MTNT validation test. Nevertheless, evaluating mo"
W19-6101,W14-4012,0,0.159211,"Missing"
W19-6101,W18-2202,0,0.140927,"atic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and ofte"
W19-6101,N19-1154,0,0.025811,"resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of hidden units, batch size, ...) are correctly tuned and a proper NMT architecture is selected. The situation for other NMT approaches, such as character-based NMT, is also confusing: Wu et al. (2016) have shown that character-based methods achieve state-of-the-art performance for different language pairs; Belinkov et al. (2017) and Durrani et al. (2019) have demonstrated their systems respective abilities to retrieve good amount of morphological information leveraging on subword level features. However, Belinkov and Bisk (2018) found that these approaches are not robust to noise (both synthetic and natural) when trained only with clean corpora. On the other hand, Durrani et al. (2019) concluded that character-based representations were more robust to synthetic and natural noise than word-based approaches. However, they did not find a substantial improvement over BPE tokenization, their BPE MT system even slightly outperforming the character-"
W19-6101,N13-1073,0,0.0310069,"hypothesis. This is done in the Moses toolkit (using the alignments produced during translation) and in OpenNMT (that uses the soft-alignments to copy the source token with the highest attention weight at every decoding step when necessary). At the time we conducted the MT experiments, the XNMT toolkit (Neubig et al., 2018) has no straightforward possibilities of replacing unknown tokens present in the test set.6 For our seq2seq NMT predictions, we performed such replacement through aligning the translation hypothesis with the source sentences (both already tokenized with BPE) with fastalign (Dyer et al., 2013) and copying the source words aligned with the &lt;UNK&gt; token. 4 Measuring noise levels as corpus divergence Several metrics have been proposed to quantify the domain drift between two corpora. In particular, the perplexity of a language model the KLdivergence between the character-level 3-gram distribution of the train and test sets were two useful measurements capable of estimating the noiselevel of UGC corpora as shown respectively by Martínez Alonso et al. (2016) and Seddah et al. (2012). We also propose a new metric to estimate the noise level tailored to the BPE tokenization. The BPE stabil"
W19-6101,N13-1037,0,0.334693,"imits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generated Content (UGC). Given the increasing importance of social medias, this type of texts has been extensively studied over the years, e.g. (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013). In this work we focus on UGC in which no grammatical, orthographic or coherence rules are respected, other than those considered by the writer. Such rule-free environment promotes a plethora of vocabulary and grammar variations, which account for the large increase of out-of-vocabulary tokens (OOVs) in UGC corpora with respect to canonical parallel training data. Translating UGC raises several challenges as it corresponds to both a low-resource scenario — producing parallel UGC corpora is very costly and often problematic due to inconsistencies between translators — and a domain adaptation s"
W19-6101,N10-1060,0,0.317459,"7) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generated Content (UGC). Given the increasing importance of social medias, this type of texts has been extensively studied over the years, e.g. (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013). In this work we focus on UGC in which no grammatical, orthographic or coherence rules are respected, other than those considered by the writer. Such rule-free environment promotes a plethora of vocabulary and grammar variations, which account for the large increase of out-of-vocabulary tokens (OOVs) in UGC corpora with respect to canonical parallel training data. Translating UGC raises several challenges as it corresponds to both a low-resource scenario — producing parallel UGC corpora is very costly and often problematic due to inconsistencies between"
W19-6101,P15-1001,0,0.02288,"ral Machine Translation systems (NMT) when translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Ko"
W19-6101,D13-1176,0,0.0522845,"a.fr Abstract This work compares the performances achieved by Phrase-Based Statistical Machine Translation systems (PBSMT) and attention-based Neural Machine Translation systems (NMT) when translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016;"
W19-6101,D15-1157,0,0.0432657,"Missing"
W19-6101,W18-1817,0,0.0972966,"ave the same thing or have gotten over it I take everything that can help me. . Table 3: Excerpts of the UGC corpora considered. Common UGC idiosyncrasies are highlighted: noncanonical contractions, spelling errors, missing elements, colloquialism, etc. See (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013) for more complete linguistic descriptions. the development set and a 0.1 label smoothing (Pereyra et al., 2017). our two UGC corpora. 3.2.3 Transformer architecture We consider a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings of 512 components, a feed-forward layers made of 2 048 units and 8 self-attention heads. It was trained using the ADAM optimizer with OpenNMT default parameters. 3.3.2 Post-processing : handling OOVs 3.3 Data processing 3.3.1 Preprocessing All of our datasets were tokenized with bytepair encoding (BPE) (Sennrich et al., 2016) using sentencepiece (Kudo and Richardson, 2018). We use a BPE vocabulary size of 16K. As a point of comparison we also train a system on Large OpenSubs with 32K BPE operations. As usual, the training corpora were cleaned so e"
W19-6101,W17-3204,0,0.31275,"T architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generate"
W19-6101,D18-2012,0,0.0201636,"., 2017). our two UGC corpora. 3.2.3 Transformer architecture We consider a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings of 512 components, a feed-forward layers made of 2 048 units and 8 self-attention heads. It was trained using the ADAM optimizer with OpenNMT default parameters. 3.3.2 Post-processing : handling OOVs 3.3 Data processing 3.3.1 Preprocessing All of our datasets were tokenized with bytepair encoding (BPE) (Sennrich et al., 2016) using sentencepiece (Kudo and Richardson, 2018). We use a BPE vocabulary size of 16K. As a point of comparison we also train a system on Large OpenSubs with 32K BPE operations. As usual, the training corpora were cleaned so each sentence has, at least, 1 token and, at most, 70 tokens. We did not perform any other pre-processing. In particular, the original case of the sentences was left unchanged in order to help disambiguate subword BPE units (see example in Figure 1) especially for Named Entities that are vastly present in Given the high number of OOVs in UGC, special care must be taken in choosing the strategy to handle them. The BPE pr"
W19-6101,I17-1003,0,0.0672878,"i et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generated Content (UGC). Given the increasing importance of social medias, this type of texts has been extensively studied over the years, e.g. (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013). In this work we focus on UGC in which no grammatical, orthographic or coherence rules are respect"
W19-6101,L18-1275,0,0.0901178,"and finally the pre- and post-processing applied (§3.3). 3.1 Data Sets Parallel corpora We train our models on two different corpora. We first consider the traditional corpus for training MT systems, namely the WMT data made of the europarl (v7) corpus2 and the newscommentaries (v10) corpus3 . We use the newsdiscussdev2015 corpus as a development set. This is exactly the setup used to train the system described in (Michel and Neubig, 2018) which will be used as a baseline throughout this work. We also consider, as a second training set, the French-English parallel portion of OpenSubtitles&apos;18 (Lison et al., 2018), a collection of crowd-sourced peer-reviewed subtitles for movies. We assume that, because it is made of informal dialogs, such as those found in popular sitcoms, sentences from OpenSubtitles will be much more similar to UGC data than WMT data, 2 3 www.statmt.org/europarl/ www.statmt.org/wmt15/training-parallel-nc-v10.tgz in part because most of it originates from social media and consists in streams of conversation. It must however be noted that UGC differs significantly from subtitles in many aspects: emotion denoted with repetitions, typographical and spelling errors, emojis, etc. To enabl"
W19-6101,D15-1166,0,0.060982,"tion systems (NMT) when translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 201"
W19-6101,W16-3905,1,0.683517,"h replacement through aligning the translation hypothesis with the source sentences (both already tokenized with BPE) with fastalign (Dyer et al., 2013) and copying the source words aligned with the &lt;UNK&gt; token. 4 Measuring noise levels as corpus divergence Several metrics have been proposed to quantify the domain drift between two corpora. In particular, the perplexity of a language model the KLdivergence between the character-level 3-gram distribution of the train and test sets were two useful measurements capable of estimating the noiselevel of UGC corpora as shown respectively by Martínez Alonso et al. (2016) and Seddah et al. (2012). We also propose a new metric to estimate the noise level tailored to the BPE tokenization. The BPE stability, BPEstab, is an indicator of how many BPE-compounded words tend to form throughout a test corpus. Formally BPEstab is defined as: 1 ∑ n_unique_neighbors(v) · freq(v) · N n_neighbors(v) (1) v∈V where N is the number of tokens in the corpus, V the BPE vocabulary, freq(v) the frequency of the token v and n_unique_neighbors(v) the number of unique tokens that surrounds the token v. Neighbors are counted only within the original word limits. Low average BPE stabili"
W19-6101,D16-1096,0,0.0327201,"hen translating User Generated Content (UGC), as encountered in social medias, from French to English. We show that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs. Our error analysis uncovers the specificities of UGC that are problematic for sequential NMT architectures and suggests new avenue for improving NMT models. Introduction1 1 Neural Machine Translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a; Cho et al., 2014) and, more specifically, attention-based models (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Mi et al., 2016) have recently become the method of choice for machine translation: many works have shown that Neural Machine Translation (NMT) outperforms classic PhraseBased Statistical Machine Translation (PBSMT) approaches over a wide array of datasets (Bentivogli et al., 2016; Dowling et al., 2018; Koehn and Knowles, 2017). Indeed, NMT provides better generalization and accuracy capabilities (Bojar et al., 2016; Bentivogli et al., 2016; Castilho et al., 2017) even if it has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017"
W19-6101,D18-1050,0,0.18939,"generally used in MT evaluation campaigns and tested on UGC data. We will first describe the datasets used in this work (§3.1), then the different systems we have considered (§3.2) and finally the pre- and post-processing applied (§3.3). 3.1 Data Sets Parallel corpora We train our models on two different corpora. We first consider the traditional corpus for training MT systems, namely the WMT data made of the europarl (v7) corpus2 and the newscommentaries (v10) corpus3 . We use the newsdiscussdev2015 corpus as a development set. This is exactly the setup used to train the system described in (Michel and Neubig, 2018) which will be used as a baseline throughout this work. We also consider, as a second training set, the French-English parallel portion of OpenSubtitles&apos;18 (Lison et al., 2018), a collection of crowd-sourced peer-reviewed subtitles for movies. We assume that, because it is made of informal dialogs, such as those found in popular sitcoms, sentences from OpenSubtitles will be much more similar to UGC data than WMT data, 2 3 www.statmt.org/europarl/ www.statmt.org/wmt15/training-parallel-nc-v10.tgz in part because most of it originates from social media and consists in streams of conversation. It"
W19-6101,E17-2045,0,0.040477,"(Bentivogli et al., 2016) and (Bojar et al., 2016), conclude that the former outperforms the latter as NMT translations require less post-editing to produce a correct translation. For instance, Castilho et al. (2017) present a detailed comparison of NMT and PBSMT and show that NMT outperforms PBSMT in terms of both fluency and translation accuracy, even if there is no improvement in terms of post-editing needs. However, other case studies, such as Koehn and Knowles (2017), have defended the idea that NMT was still outperformed by PBSMT in cross-domain and low-resource scenarios. For instance, Negri et al. (2017) showed that, when translating English to French, PBSMT outperforms NMT by a great margin in multi-domain data realistic conditions (heterogeneous training sets with different sizes). Dowling et al. (2018) also demonstrated a significant gap of performance in favor of their PBSMT system’s over an out-of-the-box NMT system in a low-resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of"
W19-6101,W18-1818,0,0.296692,"wo neural models. 3.2.1 Phrase-based Machine Translation We use the Moses (Koehn et al., 2007) toolkit as our phrase-based model, using the default features and parameters. The language model is a 5-gram language model with Knesser-Ney smoothing on the target side of the parallel data. We decided to consider only the parallel data (and not any monolingual data) so that the PBSMT and NMT systems use exactly the same data. 3.2.2 seq2seq model The first neural model we consider is a seq2seq bi-LSTM architecture with global attention decoding. The seq2seq model was trained using the XNMT toolkit (Neubig et al., 2018).5 It consists in a 2-layered Bi-LSTM layers encoder and 2-layered Bi-LSTM decoder. It considers, as input, word embeddings of 512 components and each LSTM units has 1 024 components. A dropout probability of 0.3 was introduced (Srivastava et al., 2014). The model was trained using the ADAM optimizer (Kingma and Ba, 2015) with vanilla parameters (α = 0.02, β = 0.998). Other more specific settings include keeping unchanged the learning rate (LR) for the first two epochs, a LR decay method based on the improvement of the performance on 5 4 Popular French websites devoted respectively to videogam"
W19-6101,W18-6319,0,0.0836664,"Missing"
W19-6101,2011.mtsummit-papers.27,0,0.0372684,"‘MTNT’, ‘News’ and ‘Open’ stand, respectively, for the Cr#pbank, MTNT, newstest&apos;14 and OpenSubtitlesTest test sets. substantial to out-of-domain test corpora, whereas scores on the in-domain test sets remain almost invariable regardless the chosen BPE vocabulary size. 5.2 Error Analysis The goal of this section is to analyze both quantitatively and qualitatively the output of NMT systems to explain their poor performance in translating UGC. Several works have already identified two main limits of NMT systems: translation dropping and excessive token generation, also known as over-generation (Roturier and Bensadoun, 2011; Kaljahi et al., 2015; Kaljahi and Samad, 2015; Michel and Neubig, 2018). We will analyze in detail how these two problems impact our models in the following subsections. It is also interesting to notice how performances lowered on the LargeOpenSubtitles system tokenized with 16K BPE operations for the seq2seq system. Specifically the newstest&apos;14 translation results, for which we noticed a drop of 7.2 BLEU points with respect to the SmallOpenSubtitles configuration, despite having roughly 4 times more training data. This is due to a faulty behaviour of the fastalign method, directly caused by"
W19-6101,W16-4603,0,0.311033,"et al. (2018) also demonstrated a significant gap of performance in favor of their PBSMT system’s over an out-of-the-box NMT system in a low-resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of hidden units, batch size, ...) are correctly tuned and a proper NMT architecture is selected. The situation for other NMT approaches, such as character-based NMT, is also confusing: Wu et al. (2016) have shown that character-based methods achieve state-of-the-art performance for different language pairs; Belinkov et al. (2017) and Durrani et al. (2019) have demonstrated their systems respective abilities to retrieve good amount of morphological information leveraging on subword level features. However, Belinkov and Bisk (2018) found that these approaches are not robust to noise (both synthetic and natural) when trained only with clean corpora. On the other hand, Durrani et al. (2019) concluded that character-based representations were more robust to synthetic and natural noise than word-"
W19-6101,C12-1149,1,0.441407,"has well-identified limits such as over-translating and dropping translations (Mi et al., 2016; Koehn and Knowles, 2017; Le et al., 2017). This work aims at studying how these interactions impact machine translation of noisy texts 1 We thank our anonymous reviewers for their insightful comments. This work was funded by the ANR ParSiTi project (ANR-16-CE33-0021). as generally found in social media and web forums and often denoted as User Generated Content (UGC). Given the increasing importance of social medias, this type of texts has been extensively studied over the years, e.g. (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013). In this work we focus on UGC in which no grammatical, orthographic or coherence rules are respected, other than those considered by the writer. Such rule-free environment promotes a plethora of vocabulary and grammar variations, which account for the large increase of out-of-vocabulary tokens (OOVs) in UGC corpora with respect to canonical parallel training data. Translating UGC raises several challenges as it corresponds to both a low-resource scenario — producing parallel UGC corpora is very costly and often problematic due to inconsistencies between translators — and a"
W19-6101,P16-1162,0,0.114665,"set and a 0.1 label smoothing (Pereyra et al., 2017). our two UGC corpora. 3.2.3 Transformer architecture We consider a vanilla Transformer model (Vaswani et al., 2017) using the implementation proposed in the OpenNMT framework (Klein et al., 2018). It consists of 6 layers with word embeddings of 512 components, a feed-forward layers made of 2 048 units and 8 self-attention heads. It was trained using the ADAM optimizer with OpenNMT default parameters. 3.3.2 Post-processing : handling OOVs 3.3 Data processing 3.3.1 Preprocessing All of our datasets were tokenized with bytepair encoding (BPE) (Sennrich et al., 2016) using sentencepiece (Kudo and Richardson, 2018). We use a BPE vocabulary size of 16K. As a point of comparison we also train a system on Large OpenSubs with 32K BPE operations. As usual, the training corpora were cleaned so each sentence has, at least, 1 token and, at most, 70 tokens. We did not perform any other pre-processing. In particular, the original case of the sentences was left unchanged in order to help disambiguate subword BPE units (see example in Figure 1) especially for Named Entities that are vastly present in Given the high number of OOVs in UGC, special care must be taken in"
W19-6101,P19-1021,0,0.112155,"udies, such as Koehn and Knowles (2017), have defended the idea that NMT was still outperformed by PBSMT in cross-domain and low-resource scenarios. For instance, Negri et al. (2017) showed that, when translating English to French, PBSMT outperforms NMT by a great margin in multi-domain data realistic conditions (heterogeneous training sets with different sizes). Dowling et al. (2018) also demonstrated a significant gap of performance in favor of their PBSMT system’s over an out-of-the-box NMT system in a low-resource setting (English-Irish). These conclusions have recently been questioned by Sennrich and Zhang (2019) who showed NMT could achieve good performance in low-resource scenario when all hyper-parameters (size of the bytepair encoding (BPE) vocabulary, number of hidden units, batch size, ...) are correctly tuned and a proper NMT architecture is selected. The situation for other NMT approaches, such as character-based NMT, is also confusing: Wu et al. (2016) have shown that character-based methods achieve state-of-the-art performance for different language pairs; Belinkov et al. (2017) and Durrani et al. (2019) have demonstrated their systems respective abilities to retrieve good amount of morpholo"
W19-6101,P16-1008,0,0.0155037,"nces, which demonstrates the presence of translations being dropped or shortened. Figure 2: Distribution of Cr#pbank translations length ratio w.r.t ground truth translations. Figure 1: Attention matrix for the source sentence ‘Bon je veux regardé teen wolf moi mais ce soir nsm*’ predicted by a seq2seq model. *Ok, I do want to watch Teen Wolf tonight motherf..r Our phrase-based model does not suffer from 5.2.2 Over-translation A second well-known issue with NMT is that the model sometimes repeatedly outputs tokens lacking any coherence, thus adding considerable artificial noise to the output (Tu et al., 2016). When manually inspecting the output, we noticed that this phenomenon occurred in UGC sentences that contain a rare, and often repetitive, sequence of tokens, such as those present in sentences like “ne spooooooooilez pas teen wolf non non non et non je dis non” (don’t spoooooil Teen wolf no and no I say no) in which the speaker emotion is expressed by repetitions of words or letters. The attention matrix obtained when translating such sentences with a seq2seq model often shows that the attention mechanism gets stalled due to the repetition of some BPE token (cf. the attention matrix in Figur"
