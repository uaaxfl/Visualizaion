1997.iwpt-1.14,P81-1022,0,0.0570346,"104 b ""the"" ""silver"" ""the customer bought"" ""notebook"" C a - <ID - @ ""the customer bought the notebook"" d ""The customer bought the silver"" ""The customer bought the silver notebook"" ""notebook"" Figure 3 : Backtracking Mode Prediction can be used to carry out a more informed selection between these alternatives. Words not yet read, but required for a complete analysis, can be derived from the input analyzed so far, either top-down (predicting a modifier) or bottom-up (predicting a head). Both types of prediction are common in phrase-structure based parsers, e.g. Earley-style top-down prediction (Earley, 1970) or left-corner strategies with bottom-up prediction (Kay, 1 986). Since dependency grammars, in general, do not employ non-lexical categories which can be predicted, so-called virtual words are constructed by the parser, which are later to be instantiated with lexical content as it becomes available when the analysis proceeds. Whenever an active phrase cannot attach itself to the left context, the head of this phrase may predict a virtual word as tentative head of a new phrase under which it is subordinated. The virtual word is specified with respect to its word class, morphosyntactic feature"
1997.iwpt-1.14,J95-2003,0,0.0273404,"ration of syntactic and semantic checks during the parsing process at the sentence and the text level. We now tum to text grammar specifications concerned with anaphora resolution and their realization by a special text parsing protocol. The protocol which accounts for local text coherence analysis makes use of a special actor, the centering actor, which keeps a backward-looking center (Cb ) and a preferentially ordered list of forward-looking centers (CJ ) of the previous utterance (we here assume a functional approach (Strube & Hahn, 1 996) to the well-known centering model originating from Grosz et al. (1995)). These lists are accessed to establish proper referential links between an anaphoric expres sion in the current utterance and the valid antecedent in the preceding ones. Nominal anaphora (cf. the occurrences of ""the company"" and ""these printers"" in Fig. 6) trigger a special searchNomAntecedent message. When it reaches the Cf list, possible antecedents are accessed in the given preference order. If an antecedent and the anaphor fulfill certain 6 Non-projectivities often arise, e.g. due to the fronting of a non-subject relative pronoun. As indicated by the dashed line in Fig. Sb and Sc, we em"
1997.iwpt-1.14,1993.iwpt-1.12,0,0.070874,"Missing"
1997.iwpt-1.14,P89-1013,0,0.083814,"Missing"
1997.iwpt-1.14,P97-1043,1,0.841735,"nstantiation of the PRICE slot of the corresponding SELL action, while the noun attachment leads to the corresponding instantiation of the PRICE slot of PRINTER. Robustness: Skipping Protocol. Skipping for robustnes purposes is a well known mechanism though limited in its reach (Lavie & Tomita, 1 993). But in free word-order languages as German skipping is even vital for the analysis of entirely well-formed structures, e.g., those involving scrambling or discontinuous constructions. For brevity, we will base the following explanation. on the robustness issue and refer the interested reader to Neuhaus & Broker (1997). The incompleteness of linguistic and conceptual specifications is .ubiquitous in real-world applications and, therefore, requires mechanisms for a fail-soft parsing behavior. Fig. 2 illustrates a typical ""skipping"" scenario. The currently active container addresses a searchHeadFor ( or searchModifierFor) message to its textually immediately preceding container actor. If both types of messages fail, the immediately preceding container of the active container forwards these messages - in the canonical order - to its immediately preceding container actor. If any of these two message types succe"
1997.iwpt-1.14,C96-1085,1,0.791885,"his message to its head (if any) and tests in its local site whether a dependency relation can be established by checking its corresponding valency constraints (applying SYNTAXCHECK and CONCEPTCHECK). In case of success, a headFound message is returned, the sender and the receiver are copied (to enable alternative attachments in the concurrent system, i.e., no destructive operations are carried out), and a dependency relation, indicated by a dotted line, is established between those copies which join into a phrasal relationship (for a more detailed description of the underlying protocols, cf. Neuhaus & Hahn (1996)). For illustration purposes, consider the analysis of a phrase like ""Zenon sells this printer"" covering the content of the phrase actor which textually precedes the phrase actor holding the dependency structure for ""for $2,000"". The latter actor requests its attachment as a modifier of some head. The resultant new container actor (encapsulating the dependency analysis for ''Zenon sells this printer for $2,000"" in two phrase actors) is, at the same time, the historical successor of the phrase actor covering the analysis for ""Zenon sells this printer"". The structural ambiguity inherent in the e"
1997.iwpt-1.14,E95-1033,1,0.84819,"Missing"
1997.iwpt-1.14,P96-1036,1,0.9107,"Missing"
1997.iwpt-1.14,P91-1031,0,0.0604096,"Missing"
2005.mtsummit-papers.3,C04-1117,1,0.87087,"Missing"
2005.mtsummit-papers.3,1998.amta-tutorials.5,0,0.108457,"Missing"
2005.mtsummit-papers.3,P99-1027,0,0.0809286,"Missing"
2020.acl-main.112,L16-1180,0,0.0607514,"Missing"
2020.acl-main.112,N16-1066,0,0.0287242,"lexicon is a lexical repository which encodes the affective meaning of individual words (lexical entries). Most simply, affective meaning can be encoded in terms of polarity, i.e., the distinction whether an item is considered as positive, negative, or neutral. This is the case for many well-known resources such as W ORD N ET-A FFECT (Strapparava and Valitutti, 2004), S ENTI W ORD N ET (Baccianella et al., 2010), or VADER (Hutto and Gilbert, 2014). Yet, an increasing number of researchers focus on more expressive encodings for affective states inspired by distinct lines of work in psychology (Yu et al., 2016; Buechel and Hahn, 2017; Sedoc et al., 2017; Abdul-Mageed and Ungar, 2017; Bostan and Klinger, 2018; Mohammad, 2018; Troiano et al., 2019). Psychologists, on the one hand, value such lexicons as a controlled set of stimuli for designing experiments, e.g., to investigate patterns of lexical access or the structure of memory (Hofmann et al., 2009; Monnier and Syssau, 2008). NLP researchers, on the other hand, use them to augment the emotional loading of word embeddings (Yu et al., 2017; Khosla et al., 2018), as additional input to sentence-level emotion models so that the performance of even th"
2020.acl-main.112,D17-1056,0,0.0260869,"hers focus on more expressive encodings for affective states inspired by distinct lines of work in psychology (Yu et al., 2016; Buechel and Hahn, 2017; Sedoc et al., 2017; Abdul-Mageed and Ungar, 2017; Bostan and Klinger, 2018; Mohammad, 2018; Troiano et al., 2019). Psychologists, on the one hand, value such lexicons as a controlled set of stimuli for designing experiments, e.g., to investigate patterns of lexical access or the structure of memory (Hofmann et al., 2009; Monnier and Syssau, 2008). NLP researchers, on the other hand, use them to augment the emotional loading of word embeddings (Yu et al., 2017; Khosla et al., 2018), as additional input to sentence-level emotion models so that the performance of even the most sophisticated neural network gets boosted (Mohammad and Bravo-Marquez, 2017; Mohammad et al., 2018; De Bruyne et al., 2019), or rely on them in a keyword-spotting approach when no training data is available, e.g., for studies dealing with historical language stages (Buechel et al., 2016). As with any kind of manually curated resource, the availability of emotion lexicons is heavily restricted to only a few languages whose exact number varies depending on the variables under scr"
2020.acl-main.112,cook-stevenson-2010-automatically,0,\N,Missing
2020.acl-main.112,bestgen-2008-building,0,\N,Missing
2020.acl-main.112,baccianella-etal-2010-sentiwordnet,0,\N,Missing
2020.acl-main.112,P14-2063,0,\N,Missing
2020.acl-main.112,L16-1458,0,\N,Missing
2020.acl-main.112,strapparava-valitutti-2004-wordnet,0,\N,Missing
2020.acl-main.112,S15-2078,0,\N,Missing
2020.acl-main.112,L16-1413,0,\N,Missing
2020.acl-main.112,E17-2090,0,\N,Missing
2020.acl-main.112,W16-4008,1,\N,Missing
2020.acl-main.112,P18-1017,0,\N,Missing
2020.acl-main.112,N18-1173,1,\N,Missing
2020.acl-main.112,S18-1001,0,\N,Missing
2020.acl-main.112,C18-1179,0,\N,Missing
2020.acl-main.112,C18-2003,1,\N,Missing
2020.acl-main.112,W19-6119,0,\N,Missing
2020.acl-main.112,E17-2092,1,\N,Missing
2020.acl-main.112,P17-1067,0,\N,Missing
2020.acl-main.112,N16-1095,0,\N,Missing
2020.louhi-1.5,W07-1015,0,0.0549209,"l and mostly limited to a specific medical discipline or clinical division. In addition to these pure clinical documents, other document types are also interesting for the NLP community, e.g., CPGs, which are available for a wide range of conditions. CPGs as a target for automated text analytics have been much less utilized compared to other sci&lt;document&gt; entific publications and clinical documents. Most &lt;section&gt; &lt;name&gt;Risikofaktoren&lt;/name&gt; of that work took place in the context of forma&lt;section&gt; lizing CPGs as computer-interpretable guidelines &lt;name&gt;Helicobacter pylori&lt;/name&gt; (Peleg, 2013). Bouffier and Poibeau (2007) de&lt;recommendation&gt; &lt;recommendation_creation_date scribe an approach to fill in a semi-structured value=&quot;2019-01-01&quot;/&gt; Guideline Elements Model template by segmenting &lt;recommendation_grade value=&quot;B&quot;/&gt; &lt;!-- more metadata --&gt; unstructured guidelines using linguistic patterns. &lt;text&gt;Die H. pylori-Eradikation An evaluation was run on 18 French guidelines. mit dem Ziel der Magenkarzinom Serban et al. (2007) describe the extraction and -pr¨ avention sollte bei den folgenden Risikopersonen instantiation of linguistic templates for guideline durchgef¨ uhrt werden (siehe formalization, evaluated on a D"
2020.louhi-1.5,W13-1904,0,0.0697295,"Missing"
2020.louhi-1.5,W16-5113,0,0.0172323,"mendation Yes or absent According to Oxford, SIGN, or GRADE State (checked, new, modified) & note regarding guideline updates 3.2 Becker and B¨ockmann (2017) also apply NLP to German CPGs, we consider a large superset of the CPGs used in their work and provide access to our data as a preprocessed and analyzed text corpus. 3 3.1 Automated Annotation Besides the XML version of the corpus, we created plain text versions of all recommendations of background text parts to facilitate processing by existing NLP pipelines. For preprocessing, like sentence splitting and tokenization, we used JC O R E (Hahn et al., 2016) (i.e., U IMA-based) pipelines and F RA M ED (Wermter and Hahn, 2004) models which were developed for German clinical text. We also employed the J U F IT tool (v1.1) (Hellrich et al., 2015), a filter for UMLS to create a dictionary of all German words from the UMLS (Bodenreider, 2004) (version 2019AB)2 and the Semantic Groups ANAT (Anatomical Structure), CHEM (Chemicals & Drugs), DEVI (Devices), DISO (Disorders), LIVB (Living Beings), PHYS (Physiology), and PROC (Procedures) (without advanced J U F IT rules). We have chosen only these six out of the full set of 15 UMLS Semantic Groups because"
2020.louhi-1.5,W16-4213,0,0.0186188,"redict the grade 39 Table 1: Overview of existing text corpora of German clinical language. For GGPO NC, we report the number of guidelines with the number of their individual text segments in brackets. Corpus / Data Documents F RA M ED: clinical reports and medical textbook – snippets (Wermter and Hahn, 2004) Reports from five medical domains 544 (Fette et al., 2012) Radiology reports (Bretschneider et al., 2013) 174 Transthoracic echocardiography reports 140 (Toepfer et al., 2015) Operative reports (surgery) (Lohr and Herms, 2016) 450 Discharge summaries from a dermatology depart1,696 ment (Kreuzthaler et al., 2016) Discharge summaries and clinical notes from 1,725 nephrology domain (Roller et al., 2016) Discharge summaries and clinical notes from 183 nephrology domain (Cotik et al., 2016) X-ray reports (Krebs et al., 2017) 3,000 3000PA: internistic and ICU discharge summaries ≈ 3,000 3000PA J ENA PART (Hahn et al., 2018) 1,006 JS YN CC: case examples from medical textbooks 903 (Lohr et al., 2018) (v1.1) Mixed-domain, -section, and -document type 60 (400–600 A SSESS CT corpus (Mi˜narro Gim´enez et al., 2019) chars each) Discharge summaries with osteoporosis diagnosis 1,982 (K¨onig et al., 2019) Technical"
2020.louhi-1.5,L18-1201,1,0.874336,"Missing"
2020.louhi-1.5,L18-1085,0,0.0538689,"Missing"
2020.louhi-1.5,L16-1272,0,0.026516,"corpus of 377 NGC guideline summaries. The relations are compared to structured drug product labels to assess their overlap. Some larger corpora of CPGs for the English language exist already. Hussain et al. (2009) present the Yale Guideline Recommendation Corpus (YGRC), a sample of 1,275 guideline recommendations extracted from the National Guideline Clearinghouse (NGC). Their work revealed inconsistencies in writing style and reporting of the strength of recommendations. Using a subset of YGRC, El-Rab et al. (2017) present a rule-based approach to detect procedures and drug recommendations. Read et al. (2016) describe the CREST corpus, consisting of 4,029 recommendations from 170 guidelines annotated with their respective recommendation strength and report a total number of 8,138 types within the recommendations. Large corpora of CPGs lend themselves to mining the In summary, our work is most similar to the CREST corpus (Read et al., 2016), in the sense that we provide a corpus based on CPGs consisting of medical text and metadata. However, while the number of recommendations in GGPO NC is comparable to CREST, the amount of structured metadata and background text in our corpus is much larger (see"
2020.louhi-1.5,W16-4210,0,0.0169072,"GGPO NC, we report the number of guidelines with the number of their individual text segments in brackets. Corpus / Data Documents F RA M ED: clinical reports and medical textbook – snippets (Wermter and Hahn, 2004) Reports from five medical domains 544 (Fette et al., 2012) Radiology reports (Bretschneider et al., 2013) 174 Transthoracic echocardiography reports 140 (Toepfer et al., 2015) Operative reports (surgery) (Lohr and Herms, 2016) 450 Discharge summaries from a dermatology depart1,696 ment (Kreuzthaler et al., 2016) Discharge summaries and clinical notes from 1,725 nephrology domain (Roller et al., 2016) Discharge summaries and clinical notes from 183 nephrology domain (Cotik et al., 2016) X-ray reports (Krebs et al., 2017) 3,000 3000PA: internistic and ICU discharge summaries ≈ 3,000 3000PA J ENA PART (Hahn et al., 2018) 1,006 JS YN CC: case examples from medical textbooks 903 (Lohr et al., 2018) (v1.1) Mixed-domain, -section, and -document type 60 (400–600 A SSESS CT corpus (Mi˜narro Gim´enez et al., 2019) chars each) Discharge summaries with osteoporosis diagnosis 1,982 (K¨onig et al., 2019) Technical-Laymen Corpus: social media samples 4,000 (Stomach-Intestines, Kidney) (Seiffe et al., 20"
2020.louhi-1.5,2020.lrec-1.759,0,0.0426832,"Missing"
2020.louhi-1.5,W17-6943,0,0.017382,"extraction and -pr¨ avention sollte bei den folgenden Risikopersonen instantiation of linguistic templates for guideline durchgef¨ uhrt werden (siehe formalization, evaluated on a Dutch guideline for Tabelle unten).&lt;/text&gt; breast cancer treatment. German CPGs were the fo&lt;/recommendation&gt; &lt;text&gt;Das Magenkarzinom ist eine cus of Becker and B¨ockmann (2017) who adapted multifaktorielle Erkrankung, A PACHE C TAKES to detect German UMLS conbei der die Infektion mit H. cepts and evaluated their approach on a single Gerpylori den wichtigsten Risikofaktor darstellt. Seit man breast cancer guideline. Zadrozny et al. (2017) 1994 ist H. pylori durch die outline a system which identifies contradictions and Weltgesund-heitsorganisation als Klasse I Karzinogen disagreements in English CPGs. anerkannt und wurde 2009 als Some authors have focused on extracting more solches best¨ atigt &lt;litref id=&quot; task-specific information, such as activities (Kaiser 65327&quot;/&gt;.&lt;/text&gt; &lt;/section&gt; &lt;!-- more sections --&gt; et al., 2010), process structures (Wenzina and &lt;/section&gt; Kaiser, 2013; Zhu et al., 2013; Hematialam and &lt;/document&gt; &lt;!-- more documents --&gt; Zadrozny, 2017) or negation triggers (Gindl et al., Listing 1: Text snippet from"
2020.louhi-1.5,E12-2021,0,0.0572214,"Missing"
2020.lrec-1.122,L16-1715,0,0.0167032,"or the Humanities (NEH) and the Library of Congress (LC). It aims at the creation of a national digital resource of historically significant newspapers published between 1690 and 1963, from all the U.S. states and territories. Since 2005, 15,855,607 pages from 155,857 newspaper titles were digitized, including 278,486 pages in German language.10 A similar initiative, called T ROVE, is due to the Australian Newspaper Digitisation Program (ANDP)11 where 23,407,352 newspaper pages and 2,026,782 gazette pages from 1806 to around 2007 have been digitized, amounting overall to 143 million articles (Cassidy, 2016). The British Newspaper Archive, launched in 2011 as a partnership between the British Library and Findmypast (a private company), has digitized more than 35 million pages of newspapers from 1700s until today. Upon payment, full text searching is available. However, full texts cannot be downloaded and scanned pages can only be downloaded page by page as PDF. The National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between from 1771 to 1929, yet suffers from lexical quality issues of OCRed documents (Kettunen and Pääkkönen, 2016; Kettune"
2020.lrec-1.122,P10-1015,0,0.0267087,"Missing"
2020.lrec-1.122,W19-2508,0,0.07744,"equired manual curation to check whether all 52 issues (or 65 for volume 9) had been found and to look for potential gaps in the sequence of title pages. During this step, we realized that the last page of issue 34 and the first page of issue 35 in volume 25 were missing from the BSB archive and had to be obtained from an alternative source. Another strong outlier was volume 39 where only 26 out of 52 title pages were found automatically by the script. A closer look at the entire volume then led us to replace it entirely, as already mentioned in Section 4.1. 4.3. Assessment of OCR Reul et al. (2019) have recently made available OCR4ALL ,22 a tool suite geared towards the processing of historical printings. Another larger project that deals with mass digitization of historical newspapers is OCR-D, which has a special focus on German OCR (Neudecker et al., 2019). As there is currently no mapping between PageXML and the ALTO standard, we were not able to benefit from this specialization. The same holds true for OCR-D. 22 972 https://github.com/OCR4all/OCR4all In order to assess the quality of OCR for AMZ,23 we transcribed a small random sample of pages of issue 25. This volume was chosen si"
2020.lrec-1.122,grover-etal-2008-named,0,0.0553269,"nted in this paper is available at https://github.com/ JULIELab/romantik-zeitungen. The corpus has also been archived at Zenodo and can be downloaded from https://zenodo.org/record/3708427. The current search functionality is limited to a free-text search mode, with well-known drawbacks, e.g., lack of coverage for synonyms, short forms, etc. Following previous work, e.g. by Neudecker (2016) on the Europeana newspaper corpora, one of the major next steps will be to enhance the corpus substantially by semantic metadata in terms of named entities keeping an eye on effects of corrupted OCR input (Grover et al., 2008; Alex and Burns, 2014; Kim and Cassidy, 2015; Kettunen and Ruokolainen, 2017; Bircher, 2019). 6. page 183 231 297 377 BSB 1.83 / 9.78 1.58 / 5.38 1.90 / 4.02 2.63 / 3.54 German 2.28 / 3.51 7.17 / 11.26 3.60 / 4.62 4.62 / 6.34 German_best 3.43 / 5.17 8.81 / 12.77 4.67 / 6.41 5.26 / 7.82 Table 3: Evaluation of CER/WER on a random sample of volume 25 (including punctuation errors). A large amount of those character errors can be attributed to the class of punctuation marks. A closer look at the alignment of words and characters revealed that most of them could be attributed to stains on the pape"
2020.lrec-1.122,L18-1593,0,0.0191262,"tization projects selectively transform historical newspapers in a digital format, yet advanced 5 http://projekte.thulb.uni-jena.de/ literaturportal/ 6 https://zs.thulb.uni-jena.de/receive/ jportal_jpvolume_00220472 7 https://hwk1.hebis.de 8 https://www.fes.de/bibliothek/ vorwaerts-blog/ computational analytics, such as topic modeling (Glenny et al., 2019), document clustering (Hoenen, 2018), social network analysis (Jayannavar et al., 2015; Agarwal et al., 2013; Elson et al., 2010), semantic technologies (Meroño Peñuela et al., 2015; Wang et al., 2012), information extraction or text mining (Higuchi et al., 2018; Widlöcher et al., 2015), and visualization of the analytical results they yield (Scrivner and Davis, 2017; Bradley et al., 2016; El-Assady et al., 2016) are completely out of reach. Such sophisticated techniques require cleaned textual data, common data formats and search indexes to properly address content items. The second stream of related work originates from data curators who see their main task in digitizing printed newspapers at a much larger scale. The American National Digital Newspaper Program (NDNP),9 launched in 2004, is a partnership between the National Endowment for the Humani"
2020.lrec-1.122,hinrichs-krauwer-2014-clarin,0,0.0302842,"a valuable source for examining public opinion in the 19th and 20th century. Whilst all of the above-mentioned projects concentrate on general newspapers aimed at ordinary people as primary recipients, newspapers targeting narrower defined intellectual circles still suffer from a pronounced lack of attention in digitization campaigns. Hence, researchers in such fields miss the advantages resulting from digital access, search and automated analysis tools (cf., e.g., the frameworks provided by Niekler et al. (2018), Frank and Ivanovic (2018), Pustejovsky et al. (2017), Brooke et al. (2015), or Hinrichs and Krauwer (2014) and Hinrichs et al. (2018)). More1 https://oceanicexchanges.org https://livingwithmachines.ac.uk 3 https://newseye.eu 4 https://impresso-project.ch 2 over, they face the well-known obstacles of intellectual paper work — typically, lacking completeness of the locally available newspaper collection, or, if complete volumes are available, the dilemma to be unable to sift through entire volumes by year or even decade in sufficient depth, given a specific content focus. Still, some of these newspapers have been (often only partially) digitized, possibly even with options for searchable OCRed text."
2020.lrec-1.122,L18-1114,0,0.0274003,", these projects are usually not based on open source software and do not comply with a standardized workflow. Consequently, their frameworks are neither reusable, nor are their data interoperable. In summary, such digitization projects selectively transform historical newspapers in a digital format, yet advanced 5 http://projekte.thulb.uni-jena.de/ literaturportal/ 6 https://zs.thulb.uni-jena.de/receive/ jportal_jpvolume_00220472 7 https://hwk1.hebis.de 8 https://www.fes.de/bibliothek/ vorwaerts-blog/ computational analytics, such as topic modeling (Glenny et al., 2019), document clustering (Hoenen, 2018), social network analysis (Jayannavar et al., 2015; Agarwal et al., 2013; Elson et al., 2010), semantic technologies (Meroño Peñuela et al., 2015; Wang et al., 2012), information extraction or text mining (Higuchi et al., 2018; Widlöcher et al., 2015), and visualization of the analytical results they yield (Scrivner and Davis, 2017; Bradley et al., 2016; El-Assady et al., 2016) are completely out of reach. Such sophisticated techniques require cleaned textual data, common data formats and search indexes to properly address content items. The second stream of related work originates from data c"
2020.lrec-1.122,W15-0704,0,0.0146064,"n open source software and do not comply with a standardized workflow. Consequently, their frameworks are neither reusable, nor are their data interoperable. In summary, such digitization projects selectively transform historical newspapers in a digital format, yet advanced 5 http://projekte.thulb.uni-jena.de/ literaturportal/ 6 https://zs.thulb.uni-jena.de/receive/ jportal_jpvolume_00220472 7 https://hwk1.hebis.de 8 https://www.fes.de/bibliothek/ vorwaerts-blog/ computational analytics, such as topic modeling (Glenny et al., 2019), document clustering (Hoenen, 2018), social network analysis (Jayannavar et al., 2015; Agarwal et al., 2013; Elson et al., 2010), semantic technologies (Meroño Peñuela et al., 2015; Wang et al., 2012), information extraction or text mining (Higuchi et al., 2018; Widlöcher et al., 2015), and visualization of the analytical results they yield (Scrivner and Davis, 2017; Bradley et al., 2016; El-Assady et al., 2016) are completely out of reach. Such sophisticated techniques require cleaned textual data, common data formats and search indexes to properly address content items. The second stream of related work originates from data curators who see their main task in digitizing prin"
2020.lrec-1.122,L16-1152,0,0.014929,"143 million articles (Cassidy, 2016). The British Newspaper Archive, launched in 2011 as a partnership between the British Library and Findmypast (a private company), has digitized more than 35 million pages of newspapers from 1700s until today. Upon payment, full text searching is available. However, full texts cannot be downloaded and scanned pages can only be downloaded page by page as PDF. The National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between from 1771 to 1929, yet suffers from lexical quality issues of OCRed documents (Kettunen and Pääkkönen, 2016; Kettunen et al., 2020). This collection currently contains approximately 18,6 million pages in Finnish and Swedish. The National Library’s Digital Collections are offered by a Web service, also known as D IGI.12 Part of this material is also freely downloadable from The Language Bank of Finland provided by the F IN -C LARIN consortium. Further national digital newspaper programs include DDB N EWSPAPER P ORTAL of the German Digital Library,13 A NNO (AustriaN Newspapers Online)14 and many more. What these projects have in common, is that they massively digitize primarily historical newspapers,"
2020.lrec-1.122,U15-1007,0,0.0242458,"github.com/ JULIELab/romantik-zeitungen. The corpus has also been archived at Zenodo and can be downloaded from https://zenodo.org/record/3708427. The current search functionality is limited to a free-text search mode, with well-known drawbacks, e.g., lack of coverage for synonyms, short forms, etc. Following previous work, e.g. by Neudecker (2016) on the Europeana newspaper corpora, one of the major next steps will be to enhance the corpus substantially by semantic metadata in terms of named entities keeping an eye on effects of corrupted OCR input (Grover et al., 2008; Alex and Burns, 2014; Kim and Cassidy, 2015; Kettunen and Ruokolainen, 2017; Bircher, 2019). 6. page 183 231 297 377 BSB 1.83 / 9.78 1.58 / 5.38 1.90 / 4.02 2.63 / 3.54 German 2.28 / 3.51 7.17 / 11.26 3.60 / 4.62 4.62 / 6.34 German_best 3.43 / 5.17 8.81 / 12.77 4.67 / 6.41 5.26 / 7.82 Table 3: Evaluation of CER/WER on a random sample of volume 25 (including punctuation errors). A large amount of those character errors can be attributed to the class of punctuation marks. A closer look at the alignment of words and characters revealed that most of them could be attributed to stains on the paper which were mistaken by the OCR engine as fu"
2020.lrec-1.122,L16-1689,0,0.0196766,"ng. Together with the Allgemeine Literatur-Zeitung, it will be available online as a hub for German-language Romanticism research.25 Furthermore, the code used to transform the corpus and generate all the resources we presented in this paper is available at https://github.com/ JULIELab/romantik-zeitungen. The corpus has also been archived at Zenodo and can be downloaded from https://zenodo.org/record/3708427. The current search functionality is limited to a free-text search mode, with well-known drawbacks, e.g., lack of coverage for synonyms, short forms, etc. Following previous work, e.g. by Neudecker (2016) on the Europeana newspaper corpora, one of the major next steps will be to enhance the corpus substantially by semantic metadata in terms of named entities keeping an eye on effects of corrupted OCR input (Grover et al., 2008; Alex and Burns, 2014; Kim and Cassidy, 2015; Kettunen and Ruokolainen, 2017; Bircher, 2019). 6. page 183 231 297 377 BSB 1.83 / 9.78 1.58 / 5.38 1.90 / 4.02 2.63 / 3.54 German 2.28 / 3.51 7.17 / 11.26 3.60 / 4.62 4.62 / 6.34 German_best 3.43 / 5.17 8.81 / 12.77 4.67 / 6.41 5.26 / 7.82 Table 3: Evaluation of CER/WER on a random sample of volume 25 (including punctuation"
2020.lrec-1.122,L18-1209,0,0.0133757,"e. A case study of the impresso project4 on the antiEuropean movements explores how to make newspapers a valuable source for examining public opinion in the 19th and 20th century. Whilst all of the above-mentioned projects concentrate on general newspapers aimed at ordinary people as primary recipients, newspapers targeting narrower defined intellectual circles still suffer from a pronounced lack of attention in digitization campaigns. Hence, researchers in such fields miss the advantages resulting from digital access, search and automated analysis tools (cf., e.g., the frameworks provided by Niekler et al. (2018), Frank and Ivanovic (2018), Pustejovsky et al. (2017), Brooke et al. (2015), or Hinrichs and Krauwer (2014) and Hinrichs et al. (2018)). More1 https://oceanicexchanges.org https://livingwithmachines.ac.uk 3 https://newseye.eu 4 https://impresso-project.ch 2 over, they face the well-known obstacles of intellectual paper work — typically, lacking completeness of the locally available newspaper collection, or, if complete volumes are available, the dilemma to be unable to sift through entire volumes by year or even decade in sufficient depth, given a specific content focus. Still, some of these"
2020.lrec-1.122,W19-2513,0,0.0218621,"an_best 3.43 / 5.17 8.81 / 12.77 4.67 / 6.41 5.26 / 7.82 Table 3: Evaluation of CER/WER on a random sample of volume 25 (including punctuation errors). A large amount of those character errors can be attributed to the class of punctuation marks. A closer look at the alignment of words and characters revealed that most of them could be attributed to stains on the paper which were mistaken by the OCR engine as full stops, commas, colons or semicolons. The proportion of whitespace errors (e.g., “performedby”, a much more common source of digitization errors in OCRed historical newspaper corpora (Soni et al., 2019), however, is comparatively low in our corpus. Tesseract’s24 designated best OCR model turned out to perform slightly worse than the standard model on all inputs, which shows the dependency of good results on suitable training datasets. Its most recent 4.1 version added the ALTO-XML standard to its list of supported output formats. From a computational infrastructure and resource consumption perspective, the generation of the PDF and ALTO-XML files for all 53 issues took 307 hours of elapsed 23 The impact of OCR errors for digital libraries is thoroughly discussed by Chiron et al. (2017) 24 ht"
2020.lrec-1.550,C18-1139,0,0.0268394,"Missing"
2020.lrec-1.550,D18-1001,0,0.043601,"Missing"
2020.lrec-1.550,D17-2017,0,0.160812,"tual ‘fake’ documents. Methodologically, this work is rooted in generative adversarial networks (GAN) (Goodfellow et al., 2014). In GANs, two networks, a discriminator model jointly learned with a generator model, play a game-theoretical game in which the generator attempts to generate realistic, but fake, data and the discriminator aims to distinguish between the generated fake data and the real data. This approach has already been applied to camouflage person-identifying demographic data (Li et al., 2018; Elazar and Goldberg, 2018). For the medical domain, Guan et al. (2019) and Choi et al. (2017), e.g., take clinical features (diagnoses, treatment, medication codes, etc.) as input and automatically generate synthetic textual data incorporating these features as output. Lee (2018) instead uses an encoder–decoder model, as employed in many machine translation systems, for generating chief complaints from discrete variables in Electronic Health Records (EHR), like age group, gender, and diagnosis. After being trained end-to-end on authentic records, the model can generate realistic, privacy-neutral chief complaint text. Most important from our perspective, such synthetic texts include no"
2020.lrec-1.550,N19-1423,0,0.0130736,"Missing"
2020.lrec-1.550,R19-1030,1,0.784304,"l. (2013) examined this problem with the ‘Hiding In Plain Sight’ approach: detected privacy-bearing identifiers are replaced with realistic synthetic surrogates in order to collectively render the few ‘leaked’ identifiers difficult to distinguish from the synthetic surrogates—a major advantage for pseudonymization over anonymization. Targeting English medical texts, S CRUB (Sweeney, 1996) was one of the first surrogate generation systems followed by work from Uzuner et al. (2007), Yeniterzi et al. (2010), Del´eger et al. (2014), Stubbs et al. (2015b), Stubbs and Uzuner (2015), and Chen et al. (2019). Similar procedures have been proposed for Swedish (Alfalahi et al., 2012) and Danish (Pantazos et al., 2011) clinical corpora, yet not for German ones, up until now. The most radical departure to escape from the data privacy problem is to generate fully synthetic, i.e., artificial textual ‘fake’ documents. Methodologically, this work is rooted in generative adversarial networks (GAN) (Goodfellow et al., 2014). In GANs, two networks, a discriminator model jointly learned with a generator model, play a game-theoretical game in which the generator attempts to generate realistic, but fake, data"
2020.lrec-1.550,D18-1002,0,0.0893011,"cape from the data privacy problem is to generate fully synthetic, i.e., artificial textual ‘fake’ documents. Methodologically, this work is rooted in generative adversarial networks (GAN) (Goodfellow et al., 2014). In GANs, two networks, a discriminator model jointly learned with a generator model, play a game-theoretical game in which the generator attempts to generate realistic, but fake, data and the discriminator aims to distinguish between the generated fake data and the real data. This approach has already been applied to camouflage person-identifying demographic data (Li et al., 2018; Elazar and Goldberg, 2018). For the medical domain, Guan et al. (2019) and Choi et al. (2017), e.g., take clinical features (diagnoses, treatment, medication codes, etc.) as input and automatically generate synthetic textual data incorporating these features as output. Lee (2018) instead uses an encoder–decoder model, as employed in many machine translation systems, for generating chief complaints from discrete variables in Electronic Health Records (EHR), like age group, gender, and diagnosis. After being trained end-to-end on authentic records, the model can generate realistic, privacy-neutral chief complaint text. M"
2020.lrec-1.550,P19-1584,0,0.263443,"Missing"
2020.lrec-1.550,L18-1550,0,0.0212309,"Missing"
2020.lrec-1.550,L18-1473,0,0.0584007,"Missing"
2020.lrec-1.550,P19-1027,0,0.0245219,"Missing"
2020.lrec-1.550,L18-1021,0,0.0255189,"dentified based only on three data items, namely, 5-digit ZIP (or symbolic name of the place of residence), gender, and date of birth, using 1990 U.S. Census data. 4466 blog or email raw data.4 Since this attitude also faces legal implications, a quest for the protection of individual data privacy has been raised and, in the meantime, finds active response in the most recent work of the NLP community (Li et al., 2018; Coavoux et al., 2018; Elazar and Goldberg, 2018), including the design of privacy-preserving Data Management Plans compliant with EU’s General Data Protection Regulation (GDPR) (Kamocki et al., 2018). In general, two basic approaches to eliminate privacybearing data from raw text data can be distinguished. The first one, anonymization, identifies instances of relevant privacy categories (e.g., person names or dates) and replaces sensitive strings by some artificial code (e.g., ‘xxx’). This masking approach might be appropriate to eliminate privacy-bearing data in the medical world, but is likely to be inappropriate for most NLP applications since crucial discriminative information and contextual clues will be erased by such a scrubbing procedure. The second approach, pseudonymization, pre"
2020.lrec-1.550,D09-1096,0,0.0377708,"Schmidhuber, 1997) and shares many similarities with the model proposed by Lample et al. (2016). Lee et al. (2016) enhanced N EURO NER with manually engineered features by concatenating the output of a feed-forward neural network run on a binary feature vector comprising the token’s features to the character embedding and the pre-trained token embedding of a token.20 We adopted this approach (N EURO NER+token) and decided for similar token features (see Table 6) utilizing SPAC Y and various lexicon look-ups.21 Additionally, we take the structure of emails into account (N EURO NER+token+seq). Lampert et al. (2009) define nine different zones, such as greeting, signoff, signature, new text from the sender of the email or advertising etc., and present an algorithm to automatically classify these parts. Since we presume that there is a higher likelihood 17 We revised tokenization (without messing up sentence segmentation and parsing) originating from the tools to account for entities which only span part of the token. We split tokens on ‘-/&@’ except for URLs, email addresses and punctuation marks. 18 https://spacy.io/ 19 Experiments with B ERT’s contextual embeddings (Devlin et al., 2019) are in progress"
2020.lrec-1.550,N16-1030,0,0.0604219,"mented with several systems.19 G ERMA NER and G ERMAN NER. We ran out of the box G ERMA NER (Benikova et al., 2015), a CRF-based tagger primarily developed for the task of named entity recognition (NER), and G ERMAN NER (Riedl and Pad´o, 2018), a combination of BiLSTM and CRF that utilizes character embeddings as well. N EURO NER+token(+seq). We adapted N EURO NER, a system particularly designed for clinical de-identification (Dernoncourt et al., 2017b; Dernoncourt et al., 2017a). It is based on BiLSTMs (Hochreiter and Schmidhuber, 1997) and shares many similarities with the model proposed by Lample et al. (2016). Lee et al. (2016) enhanced N EURO NER with manually engineered features by concatenating the output of a feed-forward neural network run on a binary feature vector comprising the token’s features to the character embedding and the pre-trained token embedding of a token.20 We adopted this approach (N EURO NER+token) and decided for similar token features (see Table 6) utilizing SPAC Y and various lexicon look-ups.21 Additionally, we take the structure of emails into account (N EURO NER+token+seq). Lampert et al. (2009) define nine different zones, such as greeting, signoff, signature, new tex"
2020.lrec-1.550,W16-4204,0,0.0218168,"stems.19 G ERMA NER and G ERMAN NER. We ran out of the box G ERMA NER (Benikova et al., 2015), a CRF-based tagger primarily developed for the task of named entity recognition (NER), and G ERMAN NER (Riedl and Pad´o, 2018), a combination of BiLSTM and CRF that utilizes character embeddings as well. N EURO NER+token(+seq). We adapted N EURO NER, a system particularly designed for clinical de-identification (Dernoncourt et al., 2017b; Dernoncourt et al., 2017a). It is based on BiLSTMs (Hochreiter and Schmidhuber, 1997) and shares many similarities with the model proposed by Lample et al. (2016). Lee et al. (2016) enhanced N EURO NER with manually engineered features by concatenating the output of a feed-forward neural network run on a binary feature vector comprising the token’s features to the character embedding and the pre-trained token embedding of a token.20 We adopted this approach (N EURO NER+token) and decided for similar token features (see Table 6) utilizing SPAC Y and various lexicon look-ups.21 Additionally, we take the structure of emails into account (N EURO NER+token+seq). Lampert et al. (2009) define nine different zones, such as greeting, signoff, signature, new text from the sender o"
2020.lrec-1.550,P18-2005,0,0.0277148,"l departure to escape from the data privacy problem is to generate fully synthetic, i.e., artificial textual ‘fake’ documents. Methodologically, this work is rooted in generative adversarial networks (GAN) (Goodfellow et al., 2014). In GANs, two networks, a discriminator model jointly learned with a generator model, play a game-theoretical game in which the generator attempts to generate realistic, but fake, data and the discriminator aims to distinguish between the generated fake data and the real data. This approach has already been applied to camouflage person-identifying demographic data (Li et al., 2018; Elazar and Goldberg, 2018). For the medical domain, Guan et al. (2019) and Choi et al. (2017), e.g., take clinical features (diagnoses, treatment, medication codes, etc.) as input and automatically generate synthetic textual data incorporating these features as output. Lee (2018) instead uses an encoder–decoder model, as employed in many machine translation systems, for generating chief complaints from discrete variables in Electronic Health Records (EHR), like age group, gender, and diagnosis. After being trained end-to-end on authentic records, the model can generate realistic, privacy-neu"
2020.lrec-1.550,medlock-2006-introduction,0,0.738304,"Missing"
2020.lrec-1.550,W18-7106,0,0.140825,"Missing"
2020.lrec-1.550,H05-1056,0,0.114691,"nosis. After being trained end-to-end on authentic records, the model can generate realistic, privacy-neutral chief complaint text. Most important from our perspective, such synthetic texts include none of the PHI elements that was in the training data, suggesting that such models can effectively solve the de-identification problem by introducing comparable, yet artificial documents as substitutes for authentic clinical documents, rather than artificial mentions of PHIrelevant entities (as pseudonymization does). De-identification work outside the clinical domain is rare and limited in scope. Minkov et al. (2005) aim at identifying personal names, a subclass of PHI items, within emails as a prerequisite for anonymization in informal texts using a CRF classifier. Recently, Megyesi et al. (2018) report on an anonymization study for a corpus of essays written by second language (L2) learners of Swedish. While we found no work dealing with the comprehensive anonymization or even pseudonymization of emails and Twitter-style social media data,10 anonymizing SMSes is a topic of active research. Patel et al. (2013) introduce a system capable of anonymizing SMS (Short Message Service) communication. Their stud"
2020.lrec-1.550,P17-1178,0,0.0234759,"Missing"
2020.lrec-1.550,W16-2607,0,0.0128252,"standard for BIO tags of C OD E A LLTAGS+d A3 - A1 0.958 Table 2: Cohen’s κ for BIO tags on C OD E A LLTAGS+d Prec 98.06 87.67 94.14 93.37 Prec 95.53 94.89 93.98 98.20 90.36 94.58 Rec 82.46 94.46 91.38 95.69 87.69 90.34 F1 87.98 94.43 92.50 96.80 88.26 92.28 Table 5: Weighted average of Prec(ision), Rec(all) and F1 score in respect to gold standard of C OD E A LLTAGXL 5. Recognition of Privacy-Sensitive Entities With respect to the structure of emails, we first experimented with different ways of segmenting the emails into 4470 sequences and tokenization.17 For that, we compared S O M A J O (Proisl and Uhrig, 2016), a tokenizer and sentence splitter for German and English Web and social media texts, with SPAC Y18 and found that the latter yielded better results for our task than the former. Also, taking the lines in the emails as sequences rather than the segmented sentences and keeping the tokenization as is improved performance. 5.1. Token Features typographic is punctuation character, is left punctuation mark, is right punctuation mark (e.g. ‘)’), is bracket, is quote, is currency, is digit, contains digit, is alphabetic character, contains alphabetic character, is special character (no punctuation o"
2020.lrec-1.550,P18-2020,0,0.0307553,"Missing"
2020.lrec-1.550,P16-1162,0,0.0126501,"Missing"
2020.lrec-1.550,E12-2021,0,0.0145399,"Missing"
2020.lrec-1.550,P16-1126,0,0.0289075,"comment is distributed to an often (very) large number of addressees—the recipients of an email, followers in social media, other users of a platform etc.. Hence, hitherto private communication becomes intentionally public. In response to these changes, digital (social) media communication has become a major focus of research in NLP. Yet there seems to be a lack of awareness among NLP researchers that the exploitation of natural language data from such electronic communication channels, whether for commercial, administrative or academic purposes, has to comply with binding legal regulations (Wilson et al., 2016). Dependent on each country’s legislation system, different rules for privacy protection in raw text data are enforced (cf., e.g., two recent analyses for the US (Mulligan et al., 2019) and the EU (Hoofnagle et al., 2019)). Even privacybreach incidents in a legal grey zone can be harmful for the actors involved (including NLP researchers). This dilemma is evidenced dramatically in the so-called AOL search data leak.1 In August of 2006, American Online (AOL) made a large query log collection freely accessible on the Internet for a limited time. The data were extracted over three months from the"
2020.lrec-1.564,C18-1139,0,0.0136769,"-art methods: F LAIR (Akbik et al., 2019) and B IO BERT (Lee et al., 2020). For statistical evaluation, we split the P RO G ENE corpus into a fixed set of 10 train-test partitions (with disjoint test sets) and performed a 10-fold cross-validation. The respective code is contained in the download release as well. 4.1. FLAIR ELM O (Peters et al., 2018) marked a breakthrough in contextualized embedding techniques. In this approach, word embeddings are created which depend on their particular lexical surroundings in a text rather than representing each word with a single, static embedding vector. Akbik et al. (2018) extended the approach by introducing a purely character-based technique that does not use a fixed vocabulary of words any more. This method was implemented in F LAIR (Akbik et al., 2019), an NLP framework mainly for sequence tagging and text classification using P Y T ORCH (Paszke et al., 2017). As with ELM O, B I LSTM-based language models are trained that, at test/prediction time, create vector representations for each (character) position in a given text. This is done in a forward and backward manner based on the head or the tail of the text, respectively, relative to the specific position"
2020.lrec-1.564,N19-4010,0,0.0168461,"oup describes gene/protein families which often have the token factor or protein attached as second token. Similarly, the protein enum class describes enumerations of more than one protein (see Section 3.1. for detailed descriptions). The download release of the P RO G ENE corpus contains the annotations in MMAX2 and IOB format. 4. Baseline Classifiers When we want to evaluate different methods on a given dataset, a baseline is needed for better comparison. Here, we provide a realistic multi-class classification baseline for this corpus. We decided to use two state-of-the-art methods: F LAIR (Akbik et al., 2019) and B IO BERT (Lee et al., 2020). For statistical evaluation, we split the P RO G ENE corpus into a fixed set of 10 train-test partitions (with disjoint test sets) and performed a 10-fold cross-validation. The respective code is contained in the download release as well. 4.1. FLAIR ELM O (Peters et al., 2018) marked a breakthrough in contextualized embedding techniques. In this approach, word embeddings are created which depend on their particular lexical surroundings in a text rather than representing each word with a single, static embedding vector. Akbik et al. (2018) extended the approach"
2020.lrec-1.564,N19-1423,0,0.0314677,"Missing"
2020.lrec-1.564,W17-2321,0,0.0967223,"Missing"
2020.lrec-1.564,D19-5701,0,0.0267921,"the mentions with a varying level of completeness; the GN ORM P LUS data also contain annotations for gene families or groups and domain motifs. To this point, both corpora consist of 543 abstracts, with 4,8k sentences and 126k tokens. The GN ORM P LUS corpus contains additional 151 documents from the Citation GIA test collection.5 3 https://catalog.ldc.upenn.edu/LDC2008T21 https://catalog.ldc.upenn.edu/LDC2008T20 4 https://github.com/UCDenver-ccp/CRAFT/ tree/v4.0.0 5 https://ii.nlm.nih.gov/TestCollections 4587 The P HARMAC O NER corpus evolved from the B IO NLP OST 2019 P HARMAC O NER Task (Gonzalez-Agirre et al., 2019) and is unique for several reasons. Unlike all the other English-language corpora discussed here, it features Spanish language data and primarily deals with chemical compounds and drugs, but it also carries 3,009 protein annotations. It consists of a manually classified collection of 1,000 clinical case report sections (397k tokens) derived from open access Spanish medical publications, named the Spanish Clinical Case Corpus (SPACCC).6 Relations Involving Proteins or Genes as Arguments. The IEPA corpus (Ding et al., 2002) was created to represent a diverse set of interactions between chemicals"
2020.lrec-1.564,W10-1838,1,0.741368,"classes. Thus, merging all available annotations into one large corpus and regard it as a coherent source of gene/protein annotations (Wang et al., 2009; Wang et al., 2010; Galea et al., 2018) might not be advisable. This stresses the need for large-scaled, consistently annotated and quality-checked corpora for specific entity classes. We here describe such a large-scale protein annotation campaign of M EDLINE abstracts and introduce the P RO G ENE corpus, with special emphasis on biologically reliable and annotation-wise consistent metadata. An earlier version of that corpus is described by Hahn et al. (2010) and has community-widely been referred to as the FSU-P R G E corpus (cf., e.g., Habibi et al. (2017; Dang et al. (2018)). However, the FSU-P R G E version did not offer fully detailed annotation levels but collapsed the largest part of the available annotations in the single protein/gene (PrGe) class for reasons of simplicity. In P RO G ENE, we now enrich the whole corpus with annotation levels for genes/proteins, families/groups, complexes, variants and enumerations in their original annotation format. The new corpus version is available at DOI 10.5281/zenodo.3698568. 2. A Survey of Gene/Pro"
2020.lrec-1.564,W04-1213,0,0.177194,", 2009). To alleviate this issue and to make G ENIA comparable to annotation schemes using higherlevel concepts, such as the G ENE TAG corpus (see below), G ENIA was reannotated with the GGP (gene or gene product) class (Ohta et al., 2009). This followup version contains 12k GGP annotations and 15,5k annotations of the original protein class. The G ENIA corpus also includes a subset annotated for protein-proteininteractions, the G ENIA interaction corpus that was used for the B IO NLP Shared Tasks in 2009 (Kim et al., 2009) and 2011 (Kim et al., 2011) (see below). The JNLPBA challenge corpus (Kim et al., 2004) comprises all of G ENIA version 3.02 as its training set. The corpus added another 404 documents with nearly another 97k tokens annotated for a wide variety of entities of importance in molecular biology. Taken together, the corpus features 569k tokens and 35k annotations for genes or proteins. There is a revised, cleaned version of this corpus available (Huang et al., 2019) that (according to the authors) is more consistently annotated than the original one. Another early corpus containing gene and protein annotations, also of a considerable size, is G ENE TAG (Tanabe et al., 2005). It compr"
2020.lrec-1.564,W16-3003,0,0.0270288,"on and its three sub-types, Ubiquitination, Acetylation and Deacetylation. Furthermore, the Protein modification types were modified such that they were 4588 directly linked to causal entities, which was only possible through Regulation events in previous editions. This corpus comprises 34 full texts (14 were taken over from the 2011 campaign), with 188k tokens, 12k protein and 9,4k event annotations. For the fourth edition of the B IO NLP Shared Task in 2016 the original corpus from the third round was cleaned and further augmented by assignments of U NI P ROT IDs for named entity grounding (Kim et al., 2016). In the 2019 edition of the B IO NLP Shared Task, Wang et al. (2019) introduced the Active Gene Annotation Corpus (AGAC) for the task of drug repurposing. They collected 500 M EDLINE abstracts, with slightly more than 5k sentences, using the Mesh terms “Mutation/physiopathology” and “Genetic Disease” and annotated AGAC with four annotators for eleven types of named entities, which were categorized into bio-concepts (e.g., Variation or Pathway), regulation types, and other entities (among them Disease, Enzyme, and 1,1k mentions of Gene/Protein), as well as two types of thematic relations betwe"
2020.lrec-1.564,N18-1202,0,0.00829221,"MAX2 and IOB format. 4. Baseline Classifiers When we want to evaluate different methods on a given dataset, a baseline is needed for better comparison. Here, we provide a realistic multi-class classification baseline for this corpus. We decided to use two state-of-the-art methods: F LAIR (Akbik et al., 2019) and B IO BERT (Lee et al., 2020). For statistical evaluation, we split the P RO G ENE corpus into a fixed set of 10 train-test partitions (with disjoint test sets) and performed a 10-fold cross-validation. The respective code is contained in the download release as well. 4.1. FLAIR ELM O (Peters et al., 2018) marked a breakthrough in contextualized embedding techniques. In this approach, word embeddings are created which depend on their particular lexical surroundings in a text rather than representing each word with a single, static embedding vector. Akbik et al. (2018) extended the approach by introducing a purely character-based technique that does not use a fixed vocabulary of words any more. This method was implemented in F LAIR (Akbik et al., 2019), an NLP framework mainly for sequence tagging and text classification using P Y T ORCH (Paszke et al., 2017). As with ELM O, B I LSTM-based langu"
2020.lrec-1.564,W11-1812,0,0.0890117,"ortium, 2017), NCBI G ENE (Brown et al., 2015) or KEGG (Kanehisa et al., 2012). Both tasks were particularly featured in several iterations of the B IO C RE AT I V E Challenge (Hirschman et al., 2005; Krallinger et al., 2008; Arighi et al., 2011; Mao et al., 2014). The second task aims at extracting relations between genes or proteins in terms gene/protein interactions from documents (as investigated in several iterations within the BioCreAtIvE (Krallinger et al., 2008; Leitner et al., 2010; Arighi et al., 2011; Do˘gan et al., 2017) and BioNLP relation extraction challenges (Kim et al., 2009; Pyysalo et al., 2011; Kim et al., 2011; Pyysalo et al., 2012; Kim et al., 2013; N´edellec et al., 2013; Bossy et al., 2015)). Also of interest to the biomedical NLP community are other types of gene-induced relations (as witnessed by corresponding challenge tasks) like gene–disease relations (Pyysalo et al., 2015; Wang et al., 2019) or genechemical interactions (Krallinger et al., 2017). For both kinds of tasks, annotated corpora are needed— either as repositories from which training data for classifiers can be drawn in a supervised learning mode, or as gold standards for evaluating the performance of named entit"
2020.lrec-1.564,rebholz-schuhmann-etal-2010-calbc,1,0.692367,"us, merging all available annotations into one large corpus and regard it as a coherent source of gene/protein annotations (Wang et al., 2009; Wang et al., 2010; Galea et al., 2018) might not be advisable. This stresses the need for large-scaled, consistently annotated and quality-checked corpora for specific entity classes. We here describe such a large-scale protein annotation campaign of M EDLINE abstracts and introduce the P RO G ENE corpus, with special emphasis on biologically reliable and annotation-wise consistent metadata. An earlier version of that corpus is described by Hahn et al. (2010) and has community-widely been referred to as the FSU-P R G E corpus (cf., e.g., Habibi et al. (2017; Dang et al. (2018)). However, the FSU-P R G E version did not offer fully detailed annotation levels but collapsed the largest part of the available annotations in the single protein/gene (PrGe) class for reasons of simplicity. In P RO G ENE, we now enrich the whole corpus with annotation levels for genes/proteins, families/groups, complexes, variants and enumerations in their original annotation format. The new corpus version is available at DOI 10.5281/zenodo.3698568. 2. A Survey of Gene/Pro"
2020.lrec-1.564,N19-1152,0,0.0285497,"pus with annotation levels for genes/proteins, families/groups, complexes, variants and enumerations in their original annotation format. The new corpus version is available at DOI 10.5281/zenodo.3698568. 2. A Survey of Gene/Protein Annotation In this section, we give a chronological overview of existing corpora with focus on gene or protein annotations summarized in Table 1. Unless stated otherwise, the listed corpora use a single annotation level to manually1 mark occurrences 1 Hence, we here exclude an alternative stream of work on socalled silver standards (Rebholz-Schuhmann et al., 2010; Sousa et al., 2019) where annotations are derived using automatic taggers. 4585 Name Text Type # Texts # Sentences # Tokens # Genes # Relations 30,269 35,366 23,996 n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a Named Entity Focused (Genes/Proteins) G ENIA v3.02 JNLPBA G ENE TAG P RO D I S EN O SIRIS AIM ED P ROTEINS P ENN B IO IE C RAFT v4.0 C ELL F INDER IGN GN ORM P LUS P HARMAC O NER P RO G ENE abstracts abstracts sentences abstracts abstracts abstracts abstracts full texts full texts abstracts abstracts clinical reports abstracts 2,000 2,404 20,000 2,466 105 748 2,514 97 10 543 694 1,000 3,308 20,546 2"
2020.lrec-1.564,W07-1502,1,0.673001,"g. alleles) and elliptic enumerations of proteins— much needed distinctions for professional biologists. To achieve a large coverage of biological subdomains, documents from multiple existing protein/gene corpora were reannotated. To increase coverage, new document sets were created. All documents are abstracts from P UB M ED/ MEDLINE. The final corpus consists of the union of all the documents in the different subcorpora. The annotation guidelines were primarily created by the expert biologist with support from the other annotator. The active learningbased Jena Annotation Environment (JANE) (Tomanek et al., 2007) was chosen to manage the annotation project. JANE leverages the M MAX 2 tool (M¨uller and Strube, 2006) for the annotation process which is why the primary annotation format of the corpus is the M MAX 2 format. An overview of the subcorpora is given in Table 2. The subcorpus designations are of a technical nature which is the result of the original document selection process to reach a large domain coverage. We keep the names for reference to the original data. # of Entity Annotations Percentage protein protein family or group protein complex protein variant protein enum 43,070 12,304 2,858 6"
2021.eacl-main.174,N19-4010,0,0.0239525,"Missing"
2021.eacl-main.174,C18-1139,0,0.0464303,"Missing"
2021.eacl-main.174,2020.acl-main.287,0,0.0946104,"Missing"
2021.eacl-main.174,E14-1074,0,0.0724222,"Missing"
2021.eacl-main.174,W17-4912,0,0.011988,"Section 3. The resulting lexicon comprising words with their respective formality scores subsequently serves as a gold standard for evaluation. In Section 4, we submit this lexicon to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores, and complement our investigation by an extrinsic evaluation of formality on a German-language email corpus in Section 5. 2 Related Work The relevance of ‘style’ for NLP is obvious for language output-focused core applications such as language generation (Sheikha and Inkpen, 2011; Dethlefs et al., 2014; Ficler and Goldberg, 2017), machine translation (Niu et al., 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al., 2019; Luo et al., 2019; Wang et al., 2019; Shang"
2021.eacl-main.174,N19-1320,0,0.0140822,"pplications such as language generation (Sheikha and Inkpen, 2011; Dethlefs et al., 2014; Ficler and Goldberg, 2017), machine translation (Niu et al., 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al., 2019; Luo et al., 2019; Wang et al., 2019; Shang et al., 2019; Wang et al., 2020; Zhang et al., 2020; Yi et al., 2021). 1 The lexicon is available at https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodolo"
2021.eacl-main.174,L18-1550,0,0.0233035,"ection 4.2) and the extension of the scale regarding formality levels (Section 4.3). Figure 3 illustrates the schematic workflow for our word scoring procedure, including (and marked in green) the three evaluation tasks. We adopted various approaches using a seed lexicon, actually, the entries’ word embeddings, as training data for regression models to automatically score new lexical items for their formality connotation (see, e.g., Li et al. (2017) and Buechel and Hahn (2018) for a similar scenario for automatic emotion induction). As input features we decided for FAST T EXT word embeddings (Grave et al., 2018) with their own out-of-vocabulary (OOV) functionality. We found that they performed better than getting the OOV handling from BPE MB subword embeddings (Heinzerling and Strube, 2018), based on Byte Pair Encoding (BPE) (Sennrich et al., 2016), instead, or solely utilizing pure BPE MB embeddings. We evaluated different regression models. Besides R IDGE R EGRESSION,17 which is linear regression with L2 regularization during training, we also experimented with D ENSIFIER (Rothe et al., 2016), which learns an orthogonal transformation of the embedding space, and a modified, more robust variant of t"
2021.eacl-main.174,P11-2015,0,0.0301016,"ngs of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch are summarized under the label of stylometrics and have recently found their way into NLP analytics to unveil deception (Potthast et al., 2018; Pascucci et al., 2020a) or linguistic aggression (Harpalani et al., 2011; Nogueira dos Santos et al., 2018; Pascucci et al., 2020b). The computational analysis of style according to stylometric principles, from its inception, is closely linked with lexical frequency counts. Typically, mostly function words (such as articles, pronouns, conjunctions, contractions, common abbreviations, hedging terms, also including punctuation marks) are assembled in small-sized dictionaries, together, if at all, with only a few content words (domainspecific nouns, verbs, adjectives). The frequency distributions resulting from counting these dictionary entries at the document or cor"
2021.eacl-main.174,L18-1473,0,0.0442626,"Missing"
2021.eacl-main.174,P19-1041,0,0.0115932,"nkpen, 2011; Dethlefs et al., 2014; Ficler and Goldberg, 2017), machine translation (Niu et al., 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al., 2019; Luo et al., 2019; Wang et al., 2019; Shang et al., 2019; Wang et al., 2020; Zhang et al., 2020; Yi et al., 2021). 1 The lexicon is available at https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch"
2021.eacl-main.174,N16-1095,0,0.0987695,"tic (Brooke et al., 2010) or a dynamic resource (as we do; in a later study, Brooke and Hirst (2014) proposed a dynamic acquisition method, as well, by assigning a continuous formality score to single words based on their co-occurrence frequency with a hand-picked seed set of formal, neutral and informal words), and the way how semantic similarity is computed (LSA vs. embeddings). Further, we do not induce formality levels for a near-synonym task automatically but rather crowdsource nuances of formality for a relationally unrestricted lexical inventory from human raters. Pavlick and Tetreault (2016) proposed a model of formality based on an empirical analysis of human formality perceptions. They apply their approach to analyze language use in online debate forums for multiple genres (news, blogs, emails, 2 The F(ormality)-score must not be confused with the F-score as a measure relating precision and recall. 2029 and community question answering sites). Formality assessments are solicited via Amazon Turk (following the protocol established by Lahiri (2015)) using a 7-point Likert scale, with labels ranging from −3 (Very Informal) to 3 (Very Formal). A ridge regression classifier uses 11"
2021.eacl-main.174,P17-2074,0,0.0290429,"Missing"
2021.eacl-main.174,N18-1169,0,0.0250612,"Missing"
2021.eacl-main.174,C18-1086,0,0.0195125,"Hence, we first evaluate the word scoring model (Section 4.1). Next, we assess the four main input streams of I-F OR G ER (Section 4.2) and the extension of the scale regarding formality levels (Section 4.3). Figure 3 illustrates the schematic workflow for our word scoring procedure, including (and marked in green) the three evaluation tasks. We adopted various approaches using a seed lexicon, actually, the entries’ word embeddings, as training data for regression models to automatically score new lexical items for their formality connotation (see, e.g., Li et al. (2017) and Buechel and Hahn (2018) for a similar scenario for automatic emotion induction). As input features we decided for FAST T EXT word embeddings (Grave et al., 2018) with their own out-of-vocabulary (OOV) functionality. We found that they performed better than getting the OOV handling from BPE MB subword embeddings (Heinzerling and Strube, 2018), based on Byte Pair Encoding (BPE) (Sennrich et al., 2016), instead, or solely utilizing pure BPE MB embeddings. We evaluated different regression models. Besides R IDGE R EGRESSION,17 which is linear regression with L2 regularization during training, we also experimented with D"
2021.eacl-main.174,2020.stoc-1.6,0,0.0107641,"t https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch are summarized under the label of stylometrics and have recently found their way into NLP analytics to unveil deception (Potthast et al., 2018; Pascucci et al., 2020a) or linguistic aggression (Harpalani et al., 2011; Nogueira dos Santos et al., 2018; Pascucci et al., 2020b). The computational analysis of style according to stylometric principles, from its inception, is closely linked with lexical frequency counts. Typically, mostly function words (such as articles, pronouns, conjunctions, contractions, common abbreviations, hedging terms, also including punctuation marks) are assembled in small-sized dictionaries, together, if at all, with only a few content words (domainspecific nouns, verbs, adjectives). The frequency distributions resulting from count"
2021.eacl-main.174,2020.trac-1.11,0,0.0129655,"t https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch are summarized under the label of stylometrics and have recently found their way into NLP analytics to unveil deception (Potthast et al., 2018; Pascucci et al., 2020a) or linguistic aggression (Harpalani et al., 2011; Nogueira dos Santos et al., 2018; Pascucci et al., 2020b). The computational analysis of style according to stylometric principles, from its inception, is closely linked with lexical frequency counts. Typically, mostly function words (such as articles, pronouns, conjunctions, contractions, common abbreviations, hedging terms, also including punctuation marks) are assembled in small-sized dictionaries, together, if at all, with only a few content words (domainspecific nouns, verbs, adjectives). The frequency distributions resulting from count"
2021.eacl-main.174,N15-1023,0,0.0687249,"Missing"
2021.eacl-main.174,Q16-1005,0,0.332388,"is considered as a static (Brooke et al., 2010) or a dynamic resource (as we do; in a later study, Brooke and Hirst (2014) proposed a dynamic acquisition method, as well, by assigning a continuous formality score to single words based on their co-occurrence frequency with a hand-picked seed set of formal, neutral and informal words), and the way how semantic similarity is computed (LSA vs. embeddings). Further, we do not induce formality levels for a near-synonym task automatically but rather crowdsource nuances of formality for a relationally unrestricted lexical inventory from human raters. Pavlick and Tetreault (2016) proposed a model of formality based on an empirical analysis of human formality perceptions. They apply their approach to analyze language use in online debate forums for multiple genres (news, blogs, emails, 2 The F(ormality)-score must not be confused with the F-score as a measure relating precision and recall. 2029 and community question answering sites). Formality assessments are solicited via Amazon Turk (following the protocol established by Lahiri (2015)) using a 7-point Likert scale, with labels ranging from −3 (Very Informal) to 3 (Very Formal). A ridge regression classifier uses 11"
2021.eacl-main.174,W11-0711,0,0.698606,"xical items as explicit carriers of linguistic formality as an important facet of language style. A milestone for the formal definition of formality was set up by the pioneering work of Heylighen and Dewaele (1999) who defined the F-score—close in spirit with the simple lexico-statistic frequency metrics from stylometry—as the percentage difference between deictic (article, pronouns, etc.) and non-deictic parts of speech (nouns, adjectives, etc.) in a document (F ranges between 0 and 100, with higher F indicating higher formality).2 This document-level perspective was adapted by Lahiri et al. (2011) to sentence-level formality analysis. A complementary lexical dimension for the formalization of formality was introduced by Brooke et al. (2010). They define the formality score for a word as a real number value in the range 1 to −1, with 1 representing an extremely formal word and −1 an extremely informal one, and assign a formality score to each lexical item based on standard word length, morphology-based features, lexical distribution criteria or association methods (LSA). Our work adheres to their way formality is scored in a formality lexicon and manually supplied seed sets are used (as"
2021.eacl-main.174,P18-1022,0,0.0246177,"lexicon is available at https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch are summarized under the label of stylometrics and have recently found their way into NLP analytics to unveil deception (Potthast et al., 2018; Pascucci et al., 2020a) or linguistic aggression (Harpalani et al., 2011; Nogueira dos Santos et al., 2018; Pascucci et al., 2020b). The computational analysis of style according to stylometric principles, from its inception, is closely linked with lexical frequency counts. Typically, mostly function words (such as articles, pronouns, conjunctions, contractions, common abbreviations, hedging terms, also including punctuation marks) are assembled in small-sized dictionaries, together, if at all, with only a few content words (domainspecific nouns, verbs, adjectives). The frequency distributio"
2021.eacl-main.174,P18-1080,0,0.0411025,"Missing"
2021.eacl-main.174,N18-1012,0,0.0136314,"in Section 5. 2 Related Work The relevance of ‘style’ for NLP is obvious for language output-focused core applications such as language generation (Sheikha and Inkpen, 2011; Dethlefs et al., 2014; Ficler and Goldberg, 2017), machine translation (Niu et al., 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al., 2019; Luo et al., 2019; Wang et al., 2019; Shang et al., 2019; Wang et al., 2020; Zhang et al., 2020; Yi et al., 2021). 1 The lexicon is available at https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however"
2021.eacl-main.174,D19-1499,0,0.0946923,"red to trace the human ‘stylome’, although lexical factors were found to be more relevant for style analysis than syntactic (POS sequence) patterns (van Halteren et al., 2005). Simple frequency metrics have increasingly been complemented by various forms of lexical association measures (such as information gain, mutual information), and more sophisticated probabilistic models (principal component analysis (PCA), latent semantic analysis (LSA), or other types of topic models). Comprehensive lists of criteria and metrics are provided by Sheikha and Inkpen (2010); Neal et al. (2017); Ding et al. (2019). We claim that despite their relevance for applications, such as authorship attribution and plagiarism detection, these mechanisms merely serve as easy to trace proxies for characterizing linguistic style. In our work, we will have a closer look at the style-marking semantic connotation of single lexical items as explicit carriers of linguistic formality as an important facet of language style. A milestone for the formal definition of formality was set up by the pioneering work of Heylighen and Dewaele (1999) who defined the F-score—close in spirit with the simple lexico-statistic frequency m"
2021.eacl-main.174,2020.emnlp-main.365,0,0.0409493,"Missing"
2021.eacl-main.174,N16-1091,0,0.0677172,"Missing"
2021.eacl-main.174,P18-2031,0,0.0118781,"opean Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch are summarized under the label of stylometrics and have recently found their way into NLP analytics to unveil deception (Potthast et al., 2018; Pascucci et al., 2020a) or linguistic aggression (Harpalani et al., 2011; Nogueira dos Santos et al., 2018; Pascucci et al., 2020b). The computational analysis of style according to stylometric principles, from its inception, is closely linked with lexical frequency counts. Typically, mostly function words (such as articles, pronouns, conjunctions, contractions, common abbreviations, hedging terms, also including punctuation marks) are assembled in small-sized dictionaries, together, if at all, with only a few content words (domainspecific nouns, verbs, adjectives). The frequency distributions resulting from counting these dictionary entries at the document or corpus level are already very benefic"
2021.eacl-main.174,P16-1162,0,0.00554036,"s approaches using a seed lexicon, actually, the entries’ word embeddings, as training data for regression models to automatically score new lexical items for their formality connotation (see, e.g., Li et al. (2017) and Buechel and Hahn (2018) for a similar scenario for automatic emotion induction). As input features we decided for FAST T EXT word embeddings (Grave et al., 2018) with their own out-of-vocabulary (OOV) functionality. We found that they performed better than getting the OOV handling from BPE MB subword embeddings (Heinzerling and Strube, 2018), based on Byte Pair Encoding (BPE) (Sennrich et al., 2016), instead, or solely utilizing pure BPE MB embeddings. We evaluated different regression models. Besides R IDGE R EGRESSION,17 which is linear regression with L2 regularization during training, we also experimented with D ENSIFIER (Rothe et al., 2016), which learns an orthogonal transformation of the embedding space, and a modified, more robust variant of the latter, D ENS R AY (Dufter and Sch¨utze, 2019).18 We ran a feed-forward neural network with one hidden layer combined with the boosting algorithm AdaBoost.R2 (B OOSTED FFNN) as proposed by Du and Zhang (2016).19 Further, we tested neural"
2021.eacl-main.174,W11-2826,0,0.022991,"ormal-formal scale. This workflow is described in Section 3. The resulting lexicon comprising words with their respective formality scores subsequently serves as a gold standard for evaluation. In Section 4, we submit this lexicon to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores, and complement our investigation by an extrinsic evaluation of formality on a German-language email corpus in Section 5. 2 Related Work The relevance of ‘style’ for NLP is obvious for language output-focused core applications such as language generation (Sheikha and Inkpen, 2011; Dethlefs et al., 2014; Ficler and Goldberg, 2017), machine translation (Niu et al., 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al."
2021.eacl-main.174,D19-1365,0,0.0130398,"icler and Goldberg, 2017), machine translation (Niu et al., 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al., 2019; Luo et al., 2019; Wang et al., 2019; Shang et al., 2019; Wang et al., 2020; Zhang et al., 2020; Yi et al., 2021). 1 The lexicon is available at https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch are summarized under the label of sty"
2021.eacl-main.174,2020.coling-main.203,0,0.0127105,"slation (Niu et al., 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al., 2019; Luo et al., 2019; Wang et al., 2019; Shang et al., 2019; Wang et al., 2020; Zhang et al., 2020; Yi et al., 2021). 1 The lexicon is available at https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch are summarized under the label of stylometrics and have recently found their"
2021.eacl-main.174,N18-1095,0,0.0249868,"rter.de on April 24, 2020. 6 https://de.wiktionary.org 7 https://www.openthesaurus.de 2030 Figure 1: Generic language-independent workflow for gathering words for formality scoring approaches utilizing similar sentences (in blue) and its instantiation for our use case to acquire S IM S ENT W ORDS (in green) • C OD E A LLTAG8 (Eder et al., 2020) comprising roughly 1,5M German-language emails, similar sentences. This proposal goes beyond the standard way to utilize word embeddings in order to find close semantic neighbors based on the distributional hypothesis (see, e.g., Tulkens et al. (2016); Wiegand et al. (2018a) for detecting abusive lexicalizations this way). Rather than only discovering semantically related words, we extended our scope to semantically similar sentences to identify other relevant lexical candidates in the mined sentences, like an adjective modifying an offensive noun or other vulgar, yet otherwise unrelated, words in a vulgar word’s context. On the flip side, this method admittedly gathers a considerable amount of noise (cf. Section 4 for a scoring approach to account for this problem). 3.3.1 • D ORTMUNDER C HAT KORPUS10 (Beißwenger, 2013), with more than 140,000 German-language c"
2021.eacl-main.174,P19-1482,0,0.0122167,"on (Sheikha and Inkpen, 2011; Dethlefs et al., 2014; Ficler and Goldberg, 2017), machine translation (Niu et al., 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al., 2019; Luo et al., 2019; Wang et al., 2019; Shang et al., 2019; Wang et al., 2020; Zhang et al., 2020; Yi et al., 2021). 1 The lexicon is available at https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in th"
2021.eacl-main.174,2020.acl-main.294,0,0.0119726,", 2018; Prabhumoye et al., 2018) or proper phrasing in argumentation (El Baff et al., 2020). Quite recently, the notion of ‘formality style transfer’ has received increasing attention, which captures the idea to generate a formal sentence given an informal one (et vice versa), while preserving its meaning (Shen et al., 2017; Fu et al., 2018; Rao and Tetreault, 2018; Li et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018; Lample et al., 2019; Gong et al., 2019; Dai et al., 2019; Wu et al., 2019; John et al., 2019; Luo et al., 2019; Wang et al., 2019; Shang et al., 2019; Wang et al., 2020; Zhang et al., 2020; Yi et al., 2021). 1 The lexicon is available at https://github.com/ ee-2/I-ForGer. 2028 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2028–2041 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Many efforts to cope with language style have been spent, however, in application niches, such as author identification or plagiarism detection. Most of the methodological contributions developed in this forensic branch are summarized under the label of stylometrics and have recently found their way into NLP analyt"
2021.emnlp-main.728,H05-1073,0,0.128912,"Missing"
2021.emnlp-main.728,N18-1172,0,0.062588,"Missing"
2021.emnlp-main.728,P97-1023,0,0.19749,"ch different rep1 Introduction resentation schemes. In contrast to previous work Emotion analysis in the field of NLP1 has expe- which unified some sources of heterogeneity (see rienced a remarkable evolution of representation §2), to the best of our knowledge, our approach is schemes. Starting from the early focus on polar- the first to learn a representation space for emotions ity, i.e., the main distinction between positive and that generalizes over individual languages, emotion negative feelings emerging from natural language label formats, and distinct model architectures for utterances (Hatzivassiloglou and McKeown, 1997; emotion analysis. Turney and Littman, 2003), the number and variety Technically speaking, our approach consists of a of label formats, i.e., groups of emotional target set of pre-trained prediction heads that can be easily variables and their associated value ranges, has attached to existing state-of-the-art neural models. been growing rapidly (Bostan and Klinger, 2018; Doing so, a model learns to embed language items De Bruyne et al., 2020). This development is a of a particular domain in a shared representation double-edged sword though. space that resembles an “interlingua for emotion”. O"
2021.emnlp-main.728,2020.coling-main.11,0,0.0158544,"during training and lowers space requirements by reducing a large number of format-specific models to a small number of format-agnostic ones. Although not in the center of interest of this study, our approach also often leads to small improvements in prediction quality, as experiments on 13 datasets for 6 natural languages reveal. 2 Related Work Other theories influential for NLP include Plutchik’s (2001) Wheel of Emotion (Mohammad and Turney, 2013; Abdul-Mageed and Ungar, 2017; Tafreshi and Diab, 2018; Bostan et al., 2020) and appraisal dimensions (Balahur et al., 2012; Troiano et al., 2019; Hofmann et al., 2020). Yet frequently, studies do not follow any of these established approaches but rather design a customized set of variables in an ad-hoc fashion, often driven by the availability of user-labeled data in social media, or the specifics of an application or domain which requires attention to particular emotional nuances (Bollen et al., 2011; Desmet and Hoste, 2013; Staiano and Guerini, 2014; Qadir and Riloff, 2014; Li et al., 2016; Demszky et al., 2020). Representing Emotion. At the heart of computational emotion representation lies a set of emotion variables (“classes”, “constructs”) used to cap"
2021.emnlp-main.728,C18-1187,0,0.0211904,"language in a common emotion embedding space. This broadens the range of emotional nuances said models can capture. Importantly, our method learns a representation not for a specific unit of language itself but the emotion attached to it. This differs from previous work aiming to increase the affective load of, e.g., word embeddings (see below). primarily learn to represent words (with a focus on their affective meaning though), not emotions themselves. They are thus in line with previous research aiming to increase the affective load of word embeddings (Faruqui et al., 2015; Yu et al., 2017; Khosla et al., 2018). Shantala et al. (2018) improve a dialogue system by augmenting their training data with emotion predictions from a separate system. Predicted emotion labels are fed into the dialogue model using a representation (“emotion embeddings”) learned in a supervised fashion with the remainder of the model parameters. These embeddings are specific to their architecture and training dataset, they do not generalize to other label formats. Gaonkar et al. (2020) as well as Wang and Zong (2021) learn vector representations for emotion classes from annotated text datasets to explicitly model their semantic"
2021.emnlp-main.728,D14-1181,0,0.00304144,"als encoded in word embeddings (Amir et al., 2015; Rothe et al., 2016; Li et al., 2017). Combinations of high-quality embeddings with feed-forward nets have proven to be very successful, rivaling human annotation capabilities (Buechel and Hahn, 2018b). In contrast, modeling emotion of sentences or short texts (jointly referred to as “text”) was traditionally based largely on lexical resources (Taboada et al., 2011). Later, those were combined with conventional machine learning techniques (Mohammad et al., 2013) before being widely replaced by neural end-to-end approaches (Socher et al., 2013; Kim, 2014; Abdul-Mageed and Ungar, 2017). Current state-of-the-art results are achieved by transfer learning with transformer models (Devlin et al., 2019; Zhong et al., 2019; Delbrouck et al., 2020). Our work complements these lines of research by providing a method that allows existing models to embed the emotional loading of some unit of language in a common emotion embedding space. This broadens the range of emotional nuances said models can capture. Importantly, our method learns a representation not for a specific unit of language itself but the emotion attached to it. This differs from previous w"
2021.emnlp-main.728,2021.wassa-1.7,0,0.0277844,"ring an embedding layer between models for different emotion-related tasks. They refer to these Cross-lingual approaches learn a common latent embeddings as “generalized emotion representa- representation for different languages but these reption”. Different from our work, these two studies resentations are often specific to only one pair of 9233 languages and do not generalize to other label formats (Gao et al., 2015; Abdalla and Hirst, 2017; Barnes et al., 2018). Similarly, recent work with Multilingual BERT (Devlin et al., 2019) shows strong performance in cross-lingual zero-shot transfer (Lamprinidis et al., 2021), but samples from different languages still end up in different regions of the embedding space (Pires et al., 2019). These approaches are also specific to a particular model architecture so that they do not naturally carry over to, e.g., single-word emotion prediction. Multimodal approaches to emotion analysis show some similarity to our work, as they learn a common latent representation for several modalities which can be seen as separate domains (Zadeh et al., 2017; Han et al., 2021; Poria et al., 2019). However, these representations are typically specific to a single dataset and are not m"
2021.emnlp-main.728,C16-1249,0,0.0142045,"nd Turney, 2013; Abdul-Mageed and Ungar, 2017; Tafreshi and Diab, 2018; Bostan et al., 2020) and appraisal dimensions (Balahur et al., 2012; Troiano et al., 2019; Hofmann et al., 2020). Yet frequently, studies do not follow any of these established approaches but rather design a customized set of variables in an ad-hoc fashion, often driven by the availability of user-labeled data in social media, or the specifics of an application or domain which requires attention to particular emotional nuances (Bollen et al., 2011; Desmet and Hoste, 2013; Staiano and Guerini, 2014; Qadir and Riloff, 2014; Li et al., 2016; Demszky et al., 2020). Representing Emotion. At the heart of computational emotion representation lies a set of emotion variables (“classes”, “constructs”) used to capture different facets of affective meaning. Researchers This proliferating diversity of emotion label formay choose from a multitude of approaches de- mats is the reason for the lack of comparability signed in the long and controversial history of the outlined in §1. Our work aims to unify these hetpsychology of emotion (Scherer, 2000; Hofmann erogeneous labels by learning to translate them into et al., 2020). A popular choice"
2021.emnlp-main.728,N16-1091,0,0.0221347,"to et al., 2020). A popular choice are so-called basic a shared distributional representation (see Fig. 1). 9232 Analyzing Emotion. There are several subtasks in emotion analysis that require distinct model types. Word-level prediction (or “emotion lexicon induction”) is concerned with the emotion associated with an individual word out of context. Early work exploited primarily surface patterns of word usage (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003) whereas more recent activities rely on more sophisticated statistical signals encoded in word embeddings (Amir et al., 2015; Rothe et al., 2016; Li et al., 2017). Combinations of high-quality embeddings with feed-forward nets have proven to be very successful, rivaling human annotation capabilities (Buechel and Hahn, 2018b). In contrast, modeling emotion of sentences or short texts (jointly referred to as “text”) was traditionally based largely on lexical resources (Taboada et al., 2011). Later, those were combined with conventional machine learning techniques (Mohammad et al., 2013) before being widely replaced by neural end-to-end approaches (Socher et al., 2013; Kim, 2014; Abdul-Mageed and Ungar, 2017). Current state-of-the-art re"
2021.emnlp-main.728,P18-1017,0,0.0160011,"d Surprise (BE6, for short). A subset of these excluding Surprise (BE5) is often used for emotional word datasets in psychology (“affective norms”) which are available for a wide range of languages. Figure 1: Emotional loading of heterogenous samples in common representation space with selected emotion variables (in capitals); first three principal components. Color only used as visual aid. Translations for nonEnglish items are given in Tab. 1. Affective dimensions constitute a popular alternative to basic emotions (Yu et al., 2016; Sedoc et al., 2017; Buechel and Hahn, 2017; Li et al., 2017; Mohammad, 2018). The most important ones are Valence (negative vs. positive, thus corresponding to the notion of polarity; Turney and Littman, 2003) and Arousal (calm vs. excited) (VA). These two dimensions are sometimes extended by Dominance (feeling powerless vs. empowered; VAD). 0.4 0.2 0.0 0.2 0.4 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.4 0.0 0.2 0.2 0.4 0.6 In terms of practical benefits, our method allows models to predict label formats unseen during training and lowers space requirements by reducing a large number of format-specific models to a small number of format-agnostic ones. Although not in the c"
2021.emnlp-main.728,S13-2053,0,0.0363783,"1997; Turney and Littman, 2003) whereas more recent activities rely on more sophisticated statistical signals encoded in word embeddings (Amir et al., 2015; Rothe et al., 2016; Li et al., 2017). Combinations of high-quality embeddings with feed-forward nets have proven to be very successful, rivaling human annotation capabilities (Buechel and Hahn, 2018b). In contrast, modeling emotion of sentences or short texts (jointly referred to as “text”) was traditionally based largely on lexical resources (Taboada et al., 2011). Later, those were combined with conventional machine learning techniques (Mohammad et al., 2013) before being widely replaced by neural end-to-end approaches (Socher et al., 2013; Kim, 2014; Abdul-Mageed and Ungar, 2017). Current state-of-the-art results are achieved by transfer learning with transformer models (Devlin et al., 2019; Zhong et al., 2019; Delbrouck et al., 2020). Our work complements these lines of research by providing a method that allows existing models to embed the emotional loading of some unit of language in a common emotion embedding space. This broadens the range of emotional nuances said models can capture. Importantly, our method learns a representation not for a"
2021.emnlp-main.728,P19-1493,0,0.0413102,"Missing"
2021.emnlp-main.728,2021.acl-long.184,0,0.0227255,"evious research aiming to increase the affective load of word embeddings (Faruqui et al., 2015; Yu et al., 2017; Khosla et al., 2018). Shantala et al. (2018) improve a dialogue system by augmenting their training data with emotion predictions from a separate system. Predicted emotion labels are fed into the dialogue model using a representation (“emotion embeddings”) learned in a supervised fashion with the remainder of the model parameters. These embeddings are specific to their architecture and training dataset, they do not generalize to other label formats. Gaonkar et al. (2020) as well as Wang and Zong (2021) learn vector representations for emotion classes from annotated text datasets to explicitly model their semantics and inter-relatedness. Yet again, these emotion embeddings (the class representations) do not generalize to other datasets and label formats. Han et al. (2021) propose a framework for learning a common embedding space as a means of joining information from different modalities in multimodal emotion data. While these embeddings generalize over different modalities (audio and video), they do not generalize across languages and label formats. In summary, different from these studies,"
2021.emnlp-main.728,W18-6243,0,0.019823,"al nuances. problem compared to our study. Similarly, multi-task learning can be used to fit a In more detail, Wang et al. (2020) present a model on multiple datasets potentially having difmethod for increasing the emotional content of word embeddings based on re-ordering vectors ac- ferent label formats, thus resulting in shared hidden cording to the similarity in their emotion values, representations (Tafreshi and Diab, 2018; Augenreferring to the result as “emotional embeddings”. stein et al., 2018). While representations learned with these approaches generalize across different Similarly, Xu et al. (2018) learn word embeddings label formats, they do not generalize across model that are particularly rich in affective information architectures or language domains. by sharing an embedding layer between models for different emotion-related tasks. They refer to these Cross-lingual approaches learn a common latent embeddings as “generalized emotion representa- representation for different languages but these reption”. Different from our work, these two studies resentations are often specific to only one pair of 9233 languages and do not generalize to other label formats (Gao et al., 2015; Abdalla an"
2021.emnlp-main.728,N16-1066,0,0.174946,"categories identified by Ekman (1992): Joy, Anger, Sadness, Fear, Disgust, and Surprise (BE6, for short). A subset of these excluding Surprise (BE5) is often used for emotional word datasets in psychology (“affective norms”) which are available for a wide range of languages. Figure 1: Emotional loading of heterogenous samples in common representation space with selected emotion variables (in capitals); first three principal components. Color only used as visual aid. Translations for nonEnglish items are given in Tab. 1. Affective dimensions constitute a popular alternative to basic emotions (Yu et al., 2016; Sedoc et al., 2017; Buechel and Hahn, 2017; Li et al., 2017; Mohammad, 2018). The most important ones are Valence (negative vs. positive, thus corresponding to the notion of polarity; Turney and Littman, 2003) and Arousal (calm vs. excited) (VA). These two dimensions are sometimes extended by Dominance (feeling powerless vs. empowered; VAD). 0.4 0.2 0.0 0.2 0.4 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.4 0.0 0.2 0.2 0.4 0.6 In terms of practical benefits, our method allows models to predict label formats unseen during training and lowers space requirements by reducing a large number of format-sp"
2021.emnlp-main.728,D17-1056,0,0.0246664,"of some unit of language in a common emotion embedding space. This broadens the range of emotional nuances said models can capture. Importantly, our method learns a representation not for a specific unit of language itself but the emotion attached to it. This differs from previous work aiming to increase the affective load of, e.g., word embeddings (see below). primarily learn to represent words (with a focus on their affective meaning though), not emotions themselves. They are thus in line with previous research aiming to increase the affective load of word embeddings (Faruqui et al., 2015; Yu et al., 2017; Khosla et al., 2018). Shantala et al. (2018) improve a dialogue system by augmenting their training data with emotion predictions from a separate system. Predicted emotion labels are fed into the dialogue model using a representation (“emotion embeddings”) learned in a supervised fashion with the remainder of the model parameters. These embeddings are specific to their architecture and training dataset, they do not generalize to other label formats. Gaonkar et al. (2020) as well as Wang and Zong (2021) learn vector representations for emotion classes from annotated text datasets to explicitl"
2021.emnlp-main.728,D17-1115,0,0.0190728,", recent work with Multilingual BERT (Devlin et al., 2019) shows strong performance in cross-lingual zero-shot transfer (Lamprinidis et al., 2021), but samples from different languages still end up in different regions of the embedding space (Pires et al., 2019). These approaches are also specific to a particular model architecture so that they do not naturally carry over to, e.g., single-word emotion prediction. Multimodal approaches to emotion analysis show some similarity to our work, as they learn a common latent representation for several modalities which can be seen as separate domains (Zadeh et al., 2017; Han et al., 2021; Poria et al., 2019). However, these representations are typically specific to a single dataset and are not meant to generalize further. In a recent survey on text emotion datasets, Bostan and Klinger (2018) point out naming inconsistencies between label formats. They build a joint resource that unifies twelve datasets under a common file format and annotation scheme. Annotations were unified based on the semantic closeness of their class names (e.g., merging “happy” and “Joy”). This approach is limited by its reliance on manually crafted rules which are difficult to formula"
2021.emnlp-main.728,D19-1016,0,0.0178017,"tible Mandarin dataset with basic emotions (thus, CVAT is not used in the zero-shot setting). Our method allows to freely switch between output label formats at inference time without language constraints. That is, we can predict BE5 ratings in Chinese even though there is no such training data. In terms of base models, we used the FeedForward Network developed by Buechel and Hahn (2018b) for the word datasets. This model predicts emotion ratings based on pre-trained embedding vectors (taken from Grave et al., 2018). For text datasets, we chose the B ERTbase transformer model by Devlin et al. (2019) using the implementation and pre-trained weights by Wolf et al. (2020). Both (word and text) base models use identical hyperparameter settings with or without PPH extension. For the word model, we copied the settings of the authors, whereas text model hyperparameters were tuned manually for the base model without PPH. We derived training data for the prediction heads (label mapping datasets) by combining the ratings of the word datasets en1 and en2. We used the label mapping model from Buechel and Hahn (2018a) as auxiliary label encoders. The dimensionality of the emotion space was set to 100"
A00-2043,P97-1021,0,0.319238,"irobject: {patient co-patient}&gt;i erweitern (extend) Noun Pre ~osition _ Pronoun: :: &lt;genitive attribute:~ &gt; Speicher (memory) werden_.passive &lt;{patient co-patient}&gt; mlt (with) &lt;{has-part instrument ...}&gt; Figure 3: Fragment of the Lexeme Class Hierarchy (Thus, the need for role composition in the DL language becomes evident.) The directed search in the concept graph of the domain knowledge requires sophisticated structural and topological constraints to be manageable at all. These constraints are encapsulated in a special path finding and path evaluation algorithm specified in Markert and Hahn (1997). Besides these conceptual constraints holding in the domain knowledge, we further attempt to reduce the search space for finding relation paths by two kinds of syntactic criteria. First, the search may be constrained by the type of dependency relation holding between the content words of the currently considered semantically interpretable subgraph (direct linkage), or it may be constrained by the intervening lexical material, viz. the non-content words (indirect linkage). Each of these syntactic constraints has an immediate mapping to conceptual ones. For some dependency configurations, howev"
A00-2043,C96-1024,0,0.0282097,"hatsoever, that conceptual coverage of the domain constitutes the bottleneck for any knowledge-based approach to NLP. ~ Sublanguage differences are also mirrored systematically in these data, since medical texts adhere more closely to wellestablished concept taxonomies and writing standards than magazine articles in the IT domain. 4 Related Work After a period of active research within the logicbased paradigm (e.g., Charniak and Goldman (1988), Moore (1989), Pereira and Pollack (1991)), work on semantic interpretation has almost ceased with the emergence of the empiricist movement in NLP (cf. Bos et al. (1996) for one of the more recent studies dealing with logic-based semantic interpretation in the framework of the VERBMOBIL project). Only few methodological proposals for semantic computations were made since then (e.g., higherorder colored unification as a mechanism to avoid over-generation inherent to unconstrained higherorder unification (Gardent and Kohlhase, 1996)). An issue which has lately received more focused attention are ways to cope with the tremendous complexity of semantic interpretations in the light of an exploding number of (scope) ambiguities. Within the underspecification framew"
A00-2043,P88-1011,0,0.603035,"amework for semantic interpretation in which dependency structures are mapped to conceptual representations based on a parsimonious set of interpretation schemata. Our focus is on the empirical evaluation of this approach to semantic interpretation, i.e., its quality in terms of recall and precision. Measurements are taken with respect to two real-world domains, viz. information technology test reports and medical finding reports. 1 Introduction Semantic interpretation has been an actively investigated issue on the research agenda of the logic-based paradigm of NLP in the late eighties (e.g., Charniak and Goldman (1988), Moore (1989), Pereira and Pollack (1991)). With the emergence of empirical methodologies in the early nineties, attention has almost completely shifted away from this topic. Since then, semantic issues have mainly been dealt with under a lexical perspective, viz. in terms of the resolution of lexico-semantie ambiguities (e.g., Schfitze (1998), Pedersen and Bruce (1998)) and the generation of lexical hierarchies from large text corpora (e.g., Li and Abe (1996), Hirakawa et al. (1996)) massively using statistical techniques. The research on semantic interpretation that was conducted in the pre"
A00-2043,J93-3001,0,0.0895599,"Missing"
A00-2043,P97-1050,0,0.0176779,"ling with logic-based semantic interpretation in the framework of the VERBMOBIL project). Only few methodological proposals for semantic computations were made since then (e.g., higherorder colored unification as a mechanism to avoid over-generation inherent to unconstrained higherorder unification (Gardent and Kohlhase, 1996)). An issue which has lately received more focused attention are ways to cope with the tremendous complexity of semantic interpretations in the light of an exploding number of (scope) ambiguities. Within the underspecification framework of semantic representations, e.g., DSrre (1997) proposes a polynomial algorithm which constructs packed semantic representations directly from parse forests. All the previously mentioned studies (with the exception of the experimental setup in DSrre (1997)), however, lack an empirical foundation of their various claims. Though the MUC evaluation rounds (Chinchor et al., 1993) yield the flavor of an empirical assessment of semantic structures, their scope is far too limited to count as an adequate evaluation platform for semantic interpretation. Nirenburg et al. (1996) already criticize the 'black-box' architecture underlying MUC-style eval"
A00-2043,P96-1001,0,0.181983,"actively investigated issue on the research agenda of the logic-based paradigm of NLP in the late eighties (e.g., Charniak and Goldman (1988), Moore (1989), Pereira and Pollack (1991)). With the emergence of empirical methodologies in the early nineties, attention has almost completely shifted away from this topic. Since then, semantic issues have mainly been dealt with under a lexical perspective, viz. in terms of the resolution of lexico-semantie ambiguities (e.g., Schfitze (1998), Pedersen and Bruce (1998)) and the generation of lexical hierarchies from large text corpora (e.g., Li and Abe (1996), Hirakawa et al. (1996)) massively using statistical techniques. The research on semantic interpretation that was conducted in the pre-empiricist age of NLP was mainly driven by an interest in logical formalisms as carriers for appropriate semantic representations of NL utterances. With this representational bias, computational matters - - how can semantic representation structures be properly derived from parse trees for a large variety of linguistic phenomena? - became a secondary issue. In particular, this research lacked entirely quantitative data reflecting the accuracy of the proposed s"
A00-2043,C96-1086,0,0.0294875,"investigated issue on the research agenda of the logic-based paradigm of NLP in the late eighties (e.g., Charniak and Goldman (1988), Moore (1989), Pereira and Pollack (1991)). With the emergence of empirical methodologies in the early nineties, attention has almost completely shifted away from this topic. Since then, semantic issues have mainly been dealt with under a lexical perspective, viz. in terms of the resolution of lexico-semantie ambiguities (e.g., Schfitze (1998), Pedersen and Bruce (1998)) and the generation of lexical hierarchies from large text corpora (e.g., Li and Abe (1996), Hirakawa et al. (1996)) massively using statistical techniques. The research on semantic interpretation that was conducted in the pre-empiricist age of NLP was mainly driven by an interest in logical formalisms as carriers for appropriate semantic representations of NL utterances. With this representational bias, computational matters - - how can semantic representation structures be properly derived from parse trees for a large variety of linguistic phenomena? - became a secondary issue. In particular, this research lacked entirely quantitative data reflecting the accuracy of the proposed semantic interpretation m"
A00-2043,C96-1003,0,0.0296137,"as been an actively investigated issue on the research agenda of the logic-based paradigm of NLP in the late eighties (e.g., Charniak and Goldman (1988), Moore (1989), Pereira and Pollack (1991)). With the emergence of empirical methodologies in the early nineties, attention has almost completely shifted away from this topic. Since then, semantic issues have mainly been dealt with under a lexical perspective, viz. in terms of the resolution of lexico-semantie ambiguities (e.g., Schfitze (1998), Pedersen and Bruce (1998)) and the generation of lexical hierarchies from large text corpora (e.g., Li and Abe (1996), Hirakawa et al. (1996)) massively using statistical techniques. The research on semantic interpretation that was conducted in the pre-empiricist age of NLP was mainly driven by an interest in logical formalisms as carriers for appropriate semantic representations of NL utterances. With this representational bias, computational matters - - how can semantic representation structures be properly derived from parse trees for a large variety of linguistic phenomena? - became a secondary issue. In particular, this research lacked entirely quantitative data reflecting the accuracy of the proposed s"
A00-2043,P89-1005,0,0.332628,"tation in which dependency structures are mapped to conceptual representations based on a parsimonious set of interpretation schemata. Our focus is on the empirical evaluation of this approach to semantic interpretation, i.e., its quality in terms of recall and precision. Measurements are taken with respect to two real-world domains, viz. information technology test reports and medical finding reports. 1 Introduction Semantic interpretation has been an actively investigated issue on the research agenda of the logic-based paradigm of NLP in the late eighties (e.g., Charniak and Goldman (1988), Moore (1989), Pereira and Pollack (1991)). With the emergence of empirical methodologies in the early nineties, attention has almost completely shifted away from this topic. Since then, semantic issues have mainly been dealt with under a lexical perspective, viz. in terms of the resolution of lexico-semantie ambiguities (e.g., Schfitze (1998), Pedersen and Bruce (1998)) and the generation of lexical hierarchies from large text corpora (e.g., Li and Abe (1996), Hirakawa et al. (1996)) massively using statistical techniques. The research on semantic interpretation that was conducted in the pre-empiricist ag"
A00-2043,C96-1016,0,0.0261569,"guities. Within the underspecification framework of semantic representations, e.g., DSrre (1997) proposes a polynomial algorithm which constructs packed semantic representations directly from parse forests. All the previously mentioned studies (with the exception of the experimental setup in DSrre (1997)), however, lack an empirical foundation of their various claims. Though the MUC evaluation rounds (Chinchor et al., 1993) yield the flavor of an empirical assessment of semantic structures, their scope is far too limited to count as an adequate evaluation platform for semantic interpretation. Nirenburg et al. (1996) already criticize the 'black-box' architecture underlying MUC-style evaluations, which precludes to draw serious conclusions from the shortcomings of MUC-style systems as far as single linguistic modules are concerned. More generally, in this paper the rationale underlying size (of the lexicons, knowledge or rule bases) as the major assessment category is questioned. Rather dimensions relating to the depth and breadth of the knowledge sources involved in complex system behavior should be taken more seriously into consideration. This is exactly what we intended to provide in this paper. As far"
A00-2043,J98-1004,0,0.0804924,"Missing"
buyko-etal-2010-genereg,W04-2705,0,\N,Missing
buyko-etal-2010-genereg,W09-1321,0,\N,Missing
buyko-etal-2010-genereg,W09-1401,0,\N,Missing
buyko-etal-2010-genereg,W09-1403,1,\N,Missing
buyko-etal-2010-genereg,J05-1004,0,\N,Missing
C00-1040,P88-1011,0,0.0599967,"ion rules (by the order of 30 for 200 verb concepts) instead of assigning speci c interpretation rules to each grammar item (in our case, single lexemes), and incorporate inheritance-based abstraction in the use of these schemata during the interpretation process in the knowledge base. We clearly want to point out that while this rule system covers a wide variety of standard syntactic constructions (such as genitives, prepositional phrases, various tense and modal forms), it currently does not account for quanti cational issues (like scope ambiguities) for which entirely logic-based approach (Charniak and Goldman, 1988; Moore, 1989; Pereira and Pollack, 1991) provide quite sophisticated solutions. Sondheimer et al. (1984) and Hirst (1988) treat semantic interpretation as a direct mapping from syntactic to conceptual representations. They also share with us the representation of domain knowledge using Kl-One-style terminological languages, and, hence, they make heavy use of property inheritance (or typing) mechanisms. The main di erence to our approach lies in the status of the semantic rules. Sondheimer et al. (1984) attach single interpretation rules to each role ( ller) and, hence, have to provide utterly"
C00-1040,P85-1022,0,0.128276,"f `minimal&apos; dependency subgraphs, while production rules whose speci cation is rooted in ontological categories derive a canonical conceptual interpretation from semantic interpretation structures. Con gurational descriptions of dependency graphs increase the linguistic generality of interpretation schemata, while interfacing schemata and productions to lexical and conceptual class hierarchies reduces the amount and complexity of semantic speci cations. 1 Introduction The syntax/semantics interface has always been a matter of concern for constituency-based feature grammar theories (cf., e.g., Creary and Pollard (1985), Moore (1989), Dalrymple (1992), Wedekind and Kaplan (1993)). Within the dependency grammar community, far less attention has been paid to this topic. As a consequence, there is no consensus how syntactic dependency structures might be adequately transformed into semantic interpretations (cf., Hajicova (1987), Milward (1992), Lombardo et al. (1998) for alternative proposals). In this paper, we introduce a two-layered interpretation model. In a rst pass, dependency graph structures which result from incremental parsing are immediately submitted to a semantic interpretation process. Such a proc"
C00-1040,C92-1035,0,0.0249827,"roduction rules whose speci cation is rooted in ontological categories derive a canonical conceptual interpretation from semantic interpretation structures. Con gurational descriptions of dependency graphs increase the linguistic generality of interpretation schemata, while interfacing schemata and productions to lexical and conceptual class hierarchies reduces the amount and complexity of semantic speci cations. 1 Introduction The syntax/semantics interface has always been a matter of concern for constituency-based feature grammar theories (cf., e.g., Creary and Pollard (1985), Moore (1989), Dalrymple (1992), Wedekind and Kaplan (1993)). Within the dependency grammar community, far less attention has been paid to this topic. As a consequence, there is no consensus how syntactic dependency structures might be adequately transformed into semantic interpretations (cf., Hajicova (1987), Milward (1992), Lombardo et al. (1998) for alternative proposals). In this paper, we introduce a two-layered interpretation model. In a rst pass, dependency graph structures which result from incremental parsing are immediately submitted to a semantic interpretation process. Such a process is triggered by general sche"
C00-1040,C92-4171,0,0.212499,"nd productions to lexical and conceptual class hierarchies reduces the amount and complexity of semantic speci cations. 1 Introduction The syntax/semantics interface has always been a matter of concern for constituency-based feature grammar theories (cf., e.g., Creary and Pollard (1985), Moore (1989), Dalrymple (1992), Wedekind and Kaplan (1993)). Within the dependency grammar community, far less attention has been paid to this topic. As a consequence, there is no consensus how syntactic dependency structures might be adequately transformed into semantic interpretations (cf., Hajicova (1987), Milward (1992), Lombardo et al. (1998) for alternative proposals). In this paper, we introduce a two-layered interpretation model. In a rst pass, dependency graph structures which result from incremental parsing are immediately submitted to a semantic interpretation process. Such a process is triggered by general schemata whenever a semantically interpretable subgraph of a syntactic dependency graph becomes available (cf. Section 3). As a result, lexical items and the dependency relations holding between them are directly mapped to associated conceptual entities and relations at the level of semantic repres"
C00-1040,P89-1005,0,0.0286379,"raphs, while production rules whose speci cation is rooted in ontological categories derive a canonical conceptual interpretation from semantic interpretation structures. Con gurational descriptions of dependency graphs increase the linguistic generality of interpretation schemata, while interfacing schemata and productions to lexical and conceptual class hierarchies reduces the amount and complexity of semantic speci cations. 1 Introduction The syntax/semantics interface has always been a matter of concern for constituency-based feature grammar theories (cf., e.g., Creary and Pollard (1985), Moore (1989), Dalrymple (1992), Wedekind and Kaplan (1993)). Within the dependency grammar community, far less attention has been paid to this topic. As a consequence, there is no consensus how syntactic dependency structures might be adequately transformed into semantic interpretations (cf., Hajicova (1987), Milward (1992), Lombardo et al. (1998) for alternative proposals). In this paper, we introduce a two-layered interpretation model. In a rst pass, dependency graph structures which result from incremental parsing are immediately submitted to a semantic interpretation process. Such a process is trigger"
C00-1040,A00-2043,1,0.918067,"Iw , initially,2 type(Iw ) = w:C holds (e.g., w = Festplatte&quot;, Iw = Hard-Disk.2, w:C = type(HardDisk.2) = Hard-Disk). If several conceptual correlates exist, either due to homonymy or polysemy, 1 All subsumption relations, isaW , isaF , and isaR , are considered to be transitive and re exive. 2 For instance, anaphora might necessitate changes of this initial reference assignment, cf. Strube and Hahn (1999). Figure 3: Relating Grammatical (left box) and Conceptual Knowledge (right box) each lexical ambiguity is processed independently within separate context partitions of the knowledge base (Romacker and Hahn, 2000a). 3 Interpretable Subgraphs In the parse tree from Figure 2, we can distinguish lexical nodes that have a conceptual correlate (e.g., Festplatte&quot; relating to Hard-Disk, geliefert&quot; relating to Delivery) from others that do not have such a correlate (e.g., mit&quot; (with), von&quot; (by)). Semantic interpretation capitalizes on this distinction in order to nd adequate conceptual relations between the corresponding concept instances: Direct Linkage. If two word nodes with conceptual correlates are linked by a single dependency relation, a direct linkage is given. Such a subgraph can immediately be i"
C00-1040,P84-1024,0,0.0918314,"ach grammar item (in our case, single lexemes), and incorporate inheritance-based abstraction in the use of these schemata during the interpretation process in the knowledge base. We clearly want to point out that while this rule system covers a wide variety of standard syntactic constructions (such as genitives, prepositional phrases, various tense and modal forms), it currently does not account for quanti cational issues (like scope ambiguities) for which entirely logic-based approach (Charniak and Goldman, 1988; Moore, 1989; Pereira and Pollack, 1991) provide quite sophisticated solutions. Sondheimer et al. (1984) and Hirst (1988) treat semantic interpretation as a direct mapping from syntactic to conceptual representations. They also share with us the representation of domain knowledge using Kl-One-style terminological languages, and, hence, they make heavy use of property inheritance (or typing) mechanisms. The main di erence to our approach lies in the status of the semantic rules. Sondheimer et al. (1984) attach single interpretation rules to each role ( ller) and, hence, have to provide utterly detailed speci cations re ecting the idiosyncrasies of each semantically relevant (role) attachment. Pro"
C00-1040,J99-3001,1,0.81115,", each lexical item w that has a conceptual correlate C in the domain knowledge base, w:C 2 F (mostly verbs, nouns and adjectives), gets immediately instantiated in the knowledge base, such that for any instance Iw , initially,2 type(Iw ) = w:C holds (e.g., w = Festplatte&quot;, Iw = Hard-Disk.2, w:C = type(HardDisk.2) = Hard-Disk). If several conceptual correlates exist, either due to homonymy or polysemy, 1 All subsumption relations, isaW , isaF , and isaR , are considered to be transitive and re exive. 2 For instance, anaphora might necessitate changes of this initial reference assignment, cf. Strube and Hahn (1999). Figure 3: Relating Grammatical (left box) and Conceptual Knowledge (right box) each lexical ambiguity is processed independently within separate context partitions of the knowledge base (Romacker and Hahn, 2000a). 3 Interpretable Subgraphs In the parse tree from Figure 2, we can distinguish lexical nodes that have a conceptual correlate (e.g., Festplatte&quot; relating to Hard-Disk, geliefert&quot; relating to Delivery) from others that do not have such a correlate (e.g., mit&quot; (with), von&quot; (by)). Semantic interpretation capitalizes on this distinction in order to nd adequate conceptual relations b"
C00-1040,E93-1047,0,0.028419,"ose speci cation is rooted in ontological categories derive a canonical conceptual interpretation from semantic interpretation structures. Con gurational descriptions of dependency graphs increase the linguistic generality of interpretation schemata, while interfacing schemata and productions to lexical and conceptual class hierarchies reduces the amount and complexity of semantic speci cations. 1 Introduction The syntax/semantics interface has always been a matter of concern for constituency-based feature grammar theories (cf., e.g., Creary and Pollard (1985), Moore (1989), Dalrymple (1992), Wedekind and Kaplan (1993)). Within the dependency grammar community, far less attention has been paid to this topic. As a consequence, there is no consensus how syntactic dependency structures might be adequately transformed into semantic interpretations (cf., Hajicova (1987), Milward (1992), Lombardo et al. (1998) for alternative proposals). In this paper, we introduce a two-layered interpretation model. In a rst pass, dependency graph structures which result from incremental parsing are immediately submitted to a semantic interpretation process. Such a process is triggered by general schemata whenever a semantically"
C04-1117,J90-2002,0,0.373266,"Missing"
C04-1117,P99-1067,0,0.26894,"we shifted our attention to automatic semantic validation techniques. 3.3 Automatic Semantic Validation In order to automatically validate all the generated cognate pairs, we examined the local context in which these cognates occur in non-parallel corpora of both languages involved. The basic idea that underlies this approach is that a subword that appears in a certain context should have a (true positive) cognate that occurs in a similar context, at least when (very) large corpora are taken into account. Cognate similarity can then be measured in terms of context vector comparison (cf. also Rapp (1999) or Koehn and Knight (2002)). We therefore processed the Portuguese corpus using the morpho-semantic normalization routines as discussed in Section 2. In the next step, we created a context vector for each MID, the components of which contained the relative frequencies of co-occurring MIDs in a local window of four subsequent, yet unordered MID units (a size also endorsed by Rapp (1999)). In order to compute the context vector for each Spanish subword candidate, we then constructed a seed lexicon with all the automatically created Spanish subword candidates, together with the list of Spanish a"
C04-1117,W02-0309,1,0.907839,"m an interlingua characterized by semantic identifiers. Compared to relationally richer, e.g., W ORD N ET based, interlinguas as applied for cross-language information retrieval (CLIR) (Gonzalo et al., 1999; Ruiz et al., 1999), we use a rather limited set of semantic relations and pursue a more restrictive approach to synonymy. In particular, we restrict ourselves to the specific sublanguage used in the context of the medical domain. Our claim that this interlingual approach is useful for the purpose of cross-lingual text retrieval and categorization has already been experimentally supported (Schulz et al., 2002; Mark´o et al., 2003). The quality of cross-lingual indexing fundamentally depends on the underlying lexicon and thesaurus. Its manual construction and maintenance is costly and error-prone. Therefore, machinesupported lexical acquisition techniques increasingly deserve attention. Whereas in the medical domain parallel corpora are only available for a limited number of language pairs, unrelated (i.e., nonparallel, non-aligned) corpora might provide sufficient evidence for cognate identification, at least in languages which are closely related. In this paper, we present the results of such an"
C04-1117,P98-2212,0,0.0557953,"Missing"
C04-1117,E03-1023,0,0.0360201,"Missing"
C04-1117,C98-2207,0,\N,Missing
C04-1117,W02-0902,0,\N,Missing
C04-1140,A00-1031,0,0.582938,"First, medical documents exhibit a large variety of structural features not encountered in newspaper documents (the genre problem), and, second, the understanding of medical language requires an enormous amount of a priori medical expertise (the domain problem). Hence, the question arises, how portable results are from the newspaper domain to the medical domain? We will deal with these issues, focusing on the portability of taggers, from two perspectives. We first pick up off-the-shelf technology, in our case the rule-based Brill tagger (Brill, 1995) and the statistically-based T N T tagger (Brants, 2000), both trained on newspaper data, and run it on medical text data. One may wonder how the taggers trained on newspaper language perform with medical language. Furthermore, one may ask whether it is necessary (and, if so, costly) to retrain these taggers on a medical corpus, if one were at hand? These questions seem to be of particular importance, because the use of off-the-shelf language technology for MLP applications has recently been questioned (Campbell and Johnson, 2001). Answers will be given in Section 2. Once a large annotated medical corpus becomes available, additional questions can"
C04-1140,J93-2004,0,0.0263353,"pcsak (1999)). It is both important, because there is strong demand for all kinds of computer support for health care and clinical services, which aim at improving their quality and decreasing their costs, and challenging — given the miracles of medical sublanguage, the various text genres one encounters and the enormous breadth of expertise surfacing as medical terminology. However, the development of human language technology for written language material has, up until now, almost exclusively focused on newswire or newspaper genres. This is most prominently evidenced by the P ENN T REEBANK (Marcus et al., 1993). Its value as one of the most widely used language resources mainly derives from two features. First, it supplies everyday, non-specialist document sources, such as the Wall Street Journal, and, second, it contains value-added, viz. annotated, linguistic data. Since the understanding of newspaper material does not impose particular requirements on its reader, other than the mastery of general English and common-sense knowledge, it is easy for almost everybody to deal with. This is essential for the accomplishment of the second task, viz. the annotation and reuse of part-of-speech (POS) tags a"
C04-1140,W96-0213,0,0.255858,"Missing"
C04-1140,P97-1032,0,0.0726519,"Missing"
C04-1140,A97-1014,0,0.0577724,"Missing"
C04-1140,N03-1033,0,0.0417544,"Missing"
C04-1140,wermter-hahn-2004-annotated,1,0.664597,"Missing"
C04-1140,J95-4004,0,\N,Missing
C04-1141,A00-1031,0,0.00663486,"ehen’ (‘to walk on the street’). Our goal is to consider all three types of collocations as a whole, i.e., we will not distinguish between the three different kinds of collocations. However, in order to focus our experiments, we will concentrate on a particular surface pattern in which they occur, viz. PP-verb collocations. 3 Methods and Experiments 3.1 Construction and Statistics of the Testset We used a 114-million-word German-language newspaper corpus extracted from the Web to acquire candidate PP-verb collocations. The corpus was first processed by means of the T N T partof-speech tagger (Brants, 2000). Then we ran a sentence/clause recognizer and an NP/PP chunker, both developed at the Text Knowledge Engineering Lab at Freiburg University, on the POS-tagged corpus. From the XML-marked-up tree output, PPverb complexes were automatically selected in the following way: Taking a particular PP node as a fixed point, either the preceding or the following sibling V node was taken.2 From such a PPverb combination, we extracted and counted both its various heads, in terms of Preposition-Noun-Verb (PNV) triples, and all its associated supplements, i.e., here in this case any additional lexical mater"
C04-1141,J90-1003,0,0.134291,"Missing"
C04-1141,J93-1003,0,0.0268976,"ics, a wide variety of lexical association measures have been employed for the task of (semi-)automatic collocation identification and extraction. Almost all of these measures can be grouped into one of the following three categories: frequency-based measures (e.g., based on absolute and relative co-occurrence frequencies) information-theoretic measures (e.g., mutual information, entropy) statistical measures (e.g., chi-square, t-test, log-likelihood, Dice’s coefficient) The corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties (Dunning, 1993; Manning and Sch¨utze, 1999) and their suitability for the task of collocation extraction (see Evert and Krenn (2001) and Krenn and Evert (2001) for recent evaluations). Typically, they are applied to a set of candidate lexeme pairs which were obtained from preprocessors varying in linguistic sophistication. 1 The selected measure then assigns an association score 1 On the low end, this may just be a preset numeric window span. In order to reduce the noise among the candidates, however, more elaborate linguistic processing, such as POS tagging, chunking, or even parsing, is increasingly being"
C04-1141,P01-1025,0,0.150759,"cation identification and extraction. Almost all of these measures can be grouped into one of the following three categories: frequency-based measures (e.g., based on absolute and relative co-occurrence frequencies) information-theoretic measures (e.g., mutual information, entropy) statistical measures (e.g., chi-square, t-test, log-likelihood, Dice’s coefficient) The corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties (Dunning, 1993; Manning and Sch¨utze, 1999) and their suitability for the task of collocation extraction (see Evert and Krenn (2001) and Krenn and Evert (2001) for recent evaluations). Typically, they are applied to a set of candidate lexeme pairs which were obtained from preprocessors varying in linguistic sophistication. 1 The selected measure then assigns an association score 1 On the low end, this may just be a preset numeric window span. In order to reduce the noise among the candidates, however, more elaborate linguistic processing, such as POS tagging, chunking, or even parsing, is increasingly being applied. to each candidate pair, which is computed from its joint and marginal frequencies, thus expressing the stren"
C04-1141,C96-1097,0,0.0485874,"Missing"
C04-1141,P98-2127,0,0.0116794,"Missing"
C04-1141,P99-1041,0,0.16389,"ombinability, so-called collocations. From a linguistic perspective, they can be characterized by at least three recurrent and prominent properties (Manning and Sch¨utze, 1999): Non-(or limited) compositionality. The meaning of a collocation is not a straightforward composition of the meanings of its parts. For example, the meaning of ‘red tape’ is completely different from the meaning of its components. Non-(or limited) substitutability. The parts of a collocation cannot be substituted by semantically similar words. Thus, ‘gut’ in ‘to spill gut’ cannot be substituted by ‘intestine’ (see also Lin (1999)). Non-(or limited) modifiability. Many collocations cannot be supplemented by additional lexical material. For example, the noun in ‘to kick the bucket’ cannot be modified as ‘to kick the holey/plastic/water bucket’.   Considering these observations, from a natural language processing perspective, collocations should not enter, e.g., the standard syntax-semantics pipeline so as to prevent compositional semantic readings of expressions for which this is absolutely not desired. Hence, collocations need to be identified as such and subsequently be blocked, e.g., from compositional semantic int"
C04-1141,P90-1032,0,0.0995441,"Missing"
C04-1141,J93-1007,0,0.484332,"Missing"
C04-1141,C98-2122,0,\N,Missing
C08-1012,J93-2004,0,0.0290554,"Missing"
C08-1012,H05-1105,0,0.46331,"Missing"
C08-1012,A94-1007,0,0.801356,"Missing"
C08-1012,D07-1064,0,0.667775,"Missing"
C08-1012,C04-1141,1,0.885732,"Missing"
C08-1012,P92-1003,0,0.894193,"Missing"
C08-1012,J04-4004,0,0.0239816,"nd vector (Patwardhan et al., 2003), which score the similarity of the glosses of both concepts. We applied these similarity measures to any pair of putatively coordinated nouns in the noun phrases from our data sets, A and B. To determine potential conjuncts we calculate two similarity scores relative to the structures discussed in Section 2.2: s1 = sim(N1 , N2 ) and s2 = sim(N1 , N3 ) Our final score is the maximum over both scores which is then the semantic indicator for the most plausible resolution of the coordination. 3.1.4 Bikel Parser (BP) Baseline We used the well-known Bikel Parser (Bikel, 2004) in its original version and the one used by Collins (2003). We trained both of them only with NPs extracted from the re-annotated version of WSJ (see Section 2) and converted the bracketing output of the parsers to the IO representation for NP coordinations for further evaluations. 3.2 Chunking of Conjuncts with CRFs The approach to conjunct identification presented by Buyko et al. (2007) employs Conditional Random Fields (CRF) (Lafferty et al., 2001),3 which assign a label to each token of coordinated NPs according to its function in the coordination: ‘C’ for conjuncts, ‘CC’ for conjunctions"
C08-1012,P05-1022,0,0.0851551,"Missing"
C08-1012,J03-4003,0,0.0120184,"larity of the glosses of both concepts. We applied these similarity measures to any pair of putatively coordinated nouns in the noun phrases from our data sets, A and B. To determine potential conjuncts we calculate two similarity scores relative to the structures discussed in Section 2.2: s1 = sim(N1 , N2 ) and s2 = sim(N1 , N3 ) Our final score is the maximum over both scores which is then the semantic indicator for the most plausible resolution of the coordination. 3.1.4 Bikel Parser (BP) Baseline We used the well-known Bikel Parser (Bikel, 2004) in its original version and the one used by Collins (2003). We trained both of them only with NPs extracted from the re-annotated version of WSJ (see Section 2) and converted the bracketing output of the parsers to the IO representation for NP coordinations for further evaluations. 3.2 Chunking of Conjuncts with CRFs The approach to conjunct identification presented by Buyko et al. (2007) employs Conditional Random Fields (CRF) (Lafferty et al., 2001),3 which assign a label to each token of coordinated NPs according to its function in the coordination: ‘C’ for conjuncts, ‘CC’ for conjunctions, and ‘S’ for shared elements. Since non-nested conjuncts c"
C08-1012,P07-1086,0,0.401549,"gathered lot of evidence that conjoined elements tend to be semantically similar. The important role of semantic similarity criteria for properly sorting out conjuncts was first tested by Resnik (1999). He introduced an information-content-based similarity measure that uses W ORD N ET (Fellbaum, 1998) as a lexico-semantic resource and came up with the claim that semantic similarity is helpful to achieve higher coverage in coordination resolution for coordinated noun phrases of the form ‘noun1 and noun2 noun3’ than similarity measures based on morphological information only. In a similar vein, Hogan (2007b) inspected W ORD N ET similarity and relatedness measures and investigated their role in conjunct identification. Her data reveals that several measures of semantic word similarity can indeed detect conjunct similarity. For the majority of these similarity measures, the differences between the mean similarity of coordinated elements and noncoordinated ones were statistically significant. However, it also became evident that these were only slight differences, and not all coordinated heads were semantically related as evidenced, e.g., by ‘work’/‘harmony’ in ‘hard work and harmony’. The signif"
C08-1012,P07-2038,0,0.0233293,"gathered lot of evidence that conjoined elements tend to be semantically similar. The important role of semantic similarity criteria for properly sorting out conjuncts was first tested by Resnik (1999). He introduced an information-content-based similarity measure that uses W ORD N ET (Fellbaum, 1998) as a lexico-semantic resource and came up with the claim that semantic similarity is helpful to achieve higher coverage in coordination resolution for coordinated noun phrases of the form ‘noun1 and noun2 noun3’ than similarity measures based on morphological information only. In a similar vein, Hogan (2007b) inspected W ORD N ET similarity and relatedness measures and investigated their role in conjunct identification. Her data reveals that several measures of semantic word similarity can indeed detect conjunct similarity. For the majority of these similarity measures, the differences between the mean similarity of coordinated elements and noncoordinated ones were statistically significant. However, it also became evident that these were only slight differences, and not all coordinated heads were semantically related as evidenced, e.g., by ‘work’/‘harmony’ in ‘hard work and harmony’. The signif"
C08-1012,O97-1002,0,0.055906,"ser baseline, respectively. 3.1.1 W ORD N ET Similarity (WN) Baseline Our lexico-semantic baseline comes with W ORD N ET semantic similarity scores of putatively coordinated nouns. For our experiments, we used the implementation of W ORD N ET similarity and relatedness measures provided by Ted Pedersen.2 The following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (Leacock et al., 1998)), three measures based on information content, i.e., corpus-based measures of the specificity of a concept (res (Resnik, 1999), lin (Lin, 1998), and jcn (Jiang and Conrath, 1997)). Furthermore, we used two relatedness measures, namely, lesk (Banerjee and Pedersen, 2003) and vector (Patwardhan et al., 2003), which score the similarity of the glosses of both concepts. We applied these similarity measures to any pair of putatively coordinated nouns in the noun phrases from our data sets, A and B. To determine potential conjuncts we calculate two similarity scores relative to the structures discussed in Section 2.2: s1 = sim(N1 , N2 ) and s2 = sim(N1 , N3 ) Our final score is the maximum over both scores which is then the semantic indicator for the most plausible resoluti"
C08-1012,J98-1006,0,\N,Missing
C08-1012,P07-1031,0,\N,Missing
C10-2143,P96-1042,0,0.52218,"Missing"
C10-2143,W05-0619,0,0.0656049,"Missing"
C10-2143,P09-1117,1,0.812186,"s the selection of examples depends only on the utility function. Using uLC will result in a reduction of the number of examples (i.e., sentences) selected irrespective of the sentence length so that a model learns the most from it. As a result, we observed that the selected sentences are quite long which might even cause higher annotation costs per sentence (Tomanek, 2010, Chapter 4). As for uMA there is at least a slight normalization sensitive to costs since the sum over all token-level utility scores is normalized by the length of the selected sentence. 1248 2.2 Semi-supervised AL (SeSAL) Tomanek and Hahn (2009) extendeded this standard fully supervised AL framework by a semisupervised variant (SeSAL). The selection of sentences is performed in a standard manner, i.e., similarly to the procedure in Algorithm 1. However, once selected, rather than manually annotating the complete sentence, only (uncertain) subsequences of each selected sentence are manually labeled, while the remaining (certain) ones are automatically annotated using the current version of the classifier. After the selection of an informative example p = (~x) with ~x = (x1 , . . . , xn ), the subsequences ~x0 = (xa , . . . , xb ), 1 ≤"
C10-2143,tomanek-hahn-2010-annotation,1,0.760863,"Missing"
C10-2143,D07-1051,1,0.887073,"Missing"
C10-2143,P10-1118,1,0.772477,"ute to corpus exhaustion effects since cost-sensitive AL selects for two criteria (utility and cost) and thus requires a extremely large pool to be able to pick up really advantageous examples. Consequently, applied to real-world annotation settings where the pools may be extremely large, we expect cost-sensitive approaches to be even more effective in terms of the reduction of annotation time. To be applicable in real-world scenarios, annotation costs which, in our experiments, were directly traceable in the M UC 7T corpus have to be estimated since they are not known prior to annotation. In Tomanek et al. (2010), we investigated the reading behavior during named entity annotation using eye-tracking technology. With the insights gained from this study on crucial factors influencing annotation time we were able to induce such a much needed predictive model of annotation costs. In future work, we plan to incorporate this empirically founded cost model into our approaches to cost-sensitive AL and to investigate whether our positive findings can be reproduced with estimated costs as well. Acknowledgements This work was partially funded by the EC within the CALBC (FP7-231727) project. References Conclusion"
C10-2143,D08-1112,0,0.386514,"Missing"
C16-1262,W16-2502,0,0.0142976,"percentage of correctly calculated analogies for test cases such as the frequently cited ‘king’–‘queen’ example (see Section 3). Word similarity is evaluated by calculating Spearman’s rank coefficient between embedding-derived predictions and a gold standard of human word similarity judgments. Finkelstein et al. (2002) developed a widely used test set with 353 English word pairs,2 a similar resource for German with 350 word pairs was provided by Zesch and Gurevych (2006).3 Recent work cautions that performance on such tasks is not always predictive for performance in down-stream applications (Batchkarov et al., 2016). 2.2 Diachronic Application Word embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time—words which underwent semantic shifts will be dissimilar with themselves. These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor (Kim et al., 2014; Hellrich and Hahn, 2016b), or a mapping between models for different points in time must be calculated (Kulkarni et al., 2015; Hamilton et al., 2016). The first ap"
C16-1262,P16-1141,0,0.312904,"plications (Batchkarov et al., 2016). 2.2 Diachronic Application Word embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time—words which underwent semantic shifts will be dissimilar with themselves. These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor (Kim et al., 2014; Hellrich and Hahn, 2016b), or a mapping between models for different points in time must be calculated (Kulkarni et al., 2015; Hamilton et al., 2016). The first approach cannot be performed in parallel and is thus rather time-consuming, if texts are not subsampled. We nevertheless discourage using samples instead of full corpora, as we observed extremely low reliability values between different samples (Hellrich and Hahn, 2016a). Word embeddings can also be used in diachronic studies without any kind of mapping to track clusters of similar words over time and, thus, model the evolution of topics (Kenter et al., 2015) or compare neighborhoods in embedding space for preselected words (Jo, 2016). Besides temporal variations, word embeddings c"
C16-1262,W16-2114,1,0.84271,"Mikolov et al., 2013) are probably the most influential among all embedding types (see Section 2.1). Yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models. Our investigation was performed on both historical (for the time span of 1900 to 1904) and contemporary texts (for the time span of 2005 to 2009) in two languages, English and German. It is thus a continuation of prior work, in which we investigated historical English texts only (Hellrich and Hahn, 2016a), and also influenced by the design decisions of Kim et al. (2014) and Kulkarni et al. (2015) which were the first to use word embeddings in diachronic studies. Our results cast doubt on the reproducibility of such experiments where neighborhoods between words in embedding space are taken as a computationally valid indicator for properly capturing lexical meaning (and, consequently, meaning shifts). 2 2.1 Related Work Word Embeddings Word embeddings, i.e., low (several hundred) dimensional vector word representations encoding both semantic and syntactic information, are currently one of the"
C16-1262,W14-2517,0,0.383485,"undation for trustful theories. Measuring word similarity by word neighborhoods in embedding space can be used to detect diachronic shifts or domain specific usage, by training word embeddings on suited corpora and comparing these representations. Additionally, lexical items near in the embedding space to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time or in a specific domain. These two lines of research converge in prior work to show, e.g., the increasing association of the lexical item ‘gay’ with the meaning dimension of homosexuality (Kim et al., 2014; Kulkarni et al., 2015). Neural word embeddings (Mikolov et al., 2013) are probably the most influential among all embedding types (see Section 2.1). Yet, we gathered evidence that the inherent randomness involved in their generation affects the reliability of word neighborhood judgments and demonstrate how this hampers qualitative conclusions based on such models. Our investigation was performed on both historical (for the time span of 1900 to 1904) and contemporary texts (for the time span of 2005 to 2009) in two languages, English and German. It is thus a continuation of prior work, in whi"
C16-1262,P14-2050,0,0.0377944,"tion of all word vectors before any examples are processed. Second, the order in which these examples are processed. Both can be replaced by deterministic alternatives,1 yet this would simply replace a random distortion with a fixed one, thus providing faux reliability only useful for testing purposes. A range of other word embedding algorithms was inspired by word2vec, either trying to avoid the opaqueness stemming from its neural network heritage (GloVe; still using random initialization, see Pennington et al. (2014)) or adding capabilities, like using syntactic information during training (Levy and Goldberg, 2014) or modeling multiple word senses (Bartunov et al., 2016; Panchenko, 2016). Levy et al. (2015) created SVDPPMI , a variant of the classical pointwise mutual information co-occurrence metric (see e.g., Manning and Sch¨utze (1999, pp.178–183)), by transferring pre-processing steps and hyper-parameters uncovered by the development of these algorithms, and reported similar or slightly better performance than SGNS on evaluation tasks. It is conceptually not affected by reliability problems, as there is no random initialization or relevant processing order. Word embeddings capture both syntactic and"
C16-1262,Q15-1016,0,0.529464,"and syntactic information, are currently one of the most influential methods in computational This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 2785 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2785–2796, Osaka, Japan, December 11-17 2016. linguistics. The word2vec family of algorithms, developed from heavily trimmed artificial neural networks, is a widely used and robust way to generate such embeddings (Mikolov et al., 2013; Levy et al., 2015). Its skip-gram variant predicts plausible contexts for a given word, whereas the alternative continuous bag-of-words variant tries to predict words from contexts; we focus on the former as it is generally reported to be superior (see e.g., Levy et al. (2015)). There are two strategies for managing the huge number of potential contexts a word can appear in. Skip-gram hierarchical softmax (SGHS) uses a binary tree to more efficiently represent the vocabulary, whereas skip-gram negative sampling (SGNS) updates only a limited number of word vectors during each training step. SGNS is preferred in"
C16-1262,P12-3029,0,0.0282033,"Missing"
C16-1262,L16-1421,0,0.0213473,"hich these examples are processed. Both can be replaced by deterministic alternatives,1 yet this would simply replace a random distortion with a fixed one, thus providing faux reliability only useful for testing purposes. A range of other word embedding algorithms was inspired by word2vec, either trying to avoid the opaqueness stemming from its neural network heritage (GloVe; still using random initialization, see Pennington et al. (2014)) or adding capabilities, like using syntactic information during training (Levy and Goldberg, 2014) or modeling multiple word senses (Bartunov et al., 2016; Panchenko, 2016). Levy et al. (2015) created SVDPPMI , a variant of the classical pointwise mutual information co-occurrence metric (see e.g., Manning and Sch¨utze (1999, pp.178–183)), by transferring pre-processing steps and hyper-parameters uncovered by the development of these algorithms, and reported similar or slightly better performance than SGNS on evaluation tasks. It is conceptually not affected by reliability problems, as there is no random initialization or relevant processing order. Word embeddings capture both syntactic and semantic information (and arguably also social biases, see Bolukbasi et a"
C16-1262,D14-1162,0,0.118558,"urces of randomness involved in the training of neural word embeddings: First, the random initialization of all word vectors before any examples are processed. Second, the order in which these examples are processed. Both can be replaced by deterministic alternatives,1 yet this would simply replace a random distortion with a fixed one, thus providing faux reliability only useful for testing purposes. A range of other word embedding algorithms was inspired by word2vec, either trying to avoid the opaqueness stemming from its neural network heritage (GloVe; still using random initialization, see Pennington et al. (2014)) or adding capabilities, like using syntactic information during training (Levy and Goldberg, 2014) or modeling multiple word senses (Bartunov et al., 2016; Panchenko, 2016). Levy et al. (2015) created SVDPPMI , a variant of the classical pointwise mutual information co-occurrence metric (see e.g., Manning and Sch¨utze (1999, pp.178–183)), by transferring pre-processing steps and hyper-parameters uncovered by the development of these algorithms, and reported similar or slightly better performance than SGNS on evaluation tasks. It is conceptually not affected by reliability problems, as there"
C16-1262,D15-1036,0,0.0611671,"on due to the overall similar performance and suitability for further applications. 0.4 reliability Influence of Neighborhood Size. Reliability at different top-n cut-offs is very similar for all languages and time spans under scrutiny, confirming previous observations in Hellrich and Hahn (2016a) and strengthening the suggestion to use only top-1 reliability for evaluation. Figure 2 illustrates this phenomenon with an SGNS trained on 1900– 1904 English Fiction data. We assume this to be connected with the general decrease in word2vec embedding utility for high values of n already observed by Schnabel et al. (2015). 0.3 0.2 0.1 0 10 20 30 40 50 neighborhood size n Influence of Word Frequency. Figures 3 and 4 depict 1 epoch 5 epochs 10 epochs the influence of word frequency (as percentile ranks) for English, as well as orthographically normalized German. Figure 2: Effect of neighborhood size parameter Negative sampling is overall more reliable, especially for n in reliability calculation for SGNS embeddings trained on 1900–1904 English Fiction data. words with low or medium frequency. Word frequency has a less pronounced effect on reliability for German and negative sampling is again preferable, especial"
C16-1262,W06-1104,0,0.0334227,"ore test sets than discussed here, see e.g., Baroni et al. (2014). Mikolov et al. (2013) provide an analogy test set for measuring performance as the percentage of correctly calculated analogies for test cases such as the frequently cited ‘king’–‘queen’ example (see Section 3). Word similarity is evaluated by calculating Spearman’s rank coefficient between embedding-derived predictions and a gold standard of human word similarity judgments. Finkelstein et al. (2002) developed a widely used test set with 353 English word pairs,2 a similar resource for German with 350 word pairs was provided by Zesch and Gurevych (2006).3 Recent work cautions that performance on such tasks is not always predictive for performance in down-stream applications (Batchkarov et al., 2016). 2.2 Diachronic Application Word embeddings can be used rather directly for tracking semantic changes, namely by measuring the similarity of word representations generated for one word at different points in time—words which underwent semantic shifts will be dissimilar with themselves. These models must either be trained in a continuous manner where the model for each time span is initialized with its predecessor (Kim et al., 2014; Hellrich and H"
C16-1262,P14-1023,0,\N,Missing
C18-1245,Q17-1010,0,0.0292168,"dy dealt with in the seminal work of Hatzivassiloglou and McKeown (1997). At first, the features taken into account were typically derived from co-occurrence or terminology-based similarity with a small set of seed word with known emotional scores (Turney and Littman, 2003; Esuli and Sebastiani, 2005). Nowadays, these features are almost completely replaced by word embeddings, i.e., dense, low-dimensional vector representations of words that are trained on large volumes of raw text in an unsupervised manner. W ORD 2V EC (Mikolov et al., 2013), G LOV E (Pennington et al., 2014) and FAST T EXT (Bojanowski et al., 2017) are among today’s most popular algorithms for generating embeddings. WEI algorithms constitute a natural baseline for ERM because, first, they produce the same output (emotion ratings for words according to some emotion representation format), yet their predictions are based on expressively weaker features (word embeddings instead of emotion ratings for the same word but in another format), thus constituting a harder task. Second, they form the currently prevailing paradigm for the automatic construction of emotion lexicons (K¨oper and Schulte im Walde, 2016; Shaikh et al., 2016), a problem f"
C18-1245,E17-2092,1,0.661339,"015; Yu et al., 2016b). Based on these achievements, WEI techniques have become a natural methodological choice for the automatic construction of emotion lexicons (K¨oper and Schulte im Walde, 2016; Shaikh et al., 2016). Yet, only very recently, a radically different approach to automatic emotion lexicon construction has been proposed. Instead of relying on linguistic features (such as similarity with seed words or word embeddings), the goal of emotion representation mapping (ERM) is to derive new emotional word ratings in one format based on known ratings of the same words in another format (Buechel and Hahn, 2017a). For example, ERM could use empirically gathered ratings for Basic Emotions and convert them into a Valence-Arousal-Dominance representation scheme, with greater precision than currently achievable by WEI algorithms. As a much appreciated side effect, one of the promises of ERM is to make otherwise incompatible resources (lexicons or annotated corpora, as well as tools) compatible, and incomparable systems comparable. Thus, this approach has the potential to mitigate some of the negative effects that arise from not having a community-wide standard for emotion annotation and representation ("
C18-1245,L18-1028,1,0.735186,"d use empirically gathered ratings for Basic Emotions and convert them into a Valence-Arousal-Dominance representation scheme, with greater precision than currently achievable by WEI algorithms. As a much appreciated side effect, one of the promises of ERM is to make otherwise incompatible resources (lexicons or annotated corpora, as well as tools) compatible, and incomparable systems comparable. Thus, this approach has the potential to mitigate some of the negative effects that arise from not having a community-wide standard for emotion annotation and representation (Calvo and Mac Kim, 2013; Buechel and Hahn, 2018a). We here want to contribute to this endeavor by providing a large-scale evaluation of previously proposed ERM approaches for four typologically diverse languages and report evidence that ERM clearly This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2892 Proceedings of the 27th International Conference on Computational Linguistics, pages 2892–2904 Santa Fe, New Mexico, USA, August 20-26, 2018. outperforms current state-of-the-art WEI algorithms. Furthermore, we present our own deep learning mode"
C18-1245,N18-1173,1,0.627535,"d use empirically gathered ratings for Basic Emotions and convert them into a Valence-Arousal-Dominance representation scheme, with greater precision than currently achievable by WEI algorithms. As a much appreciated side effect, one of the promises of ERM is to make otherwise incompatible resources (lexicons or annotated corpora, as well as tools) compatible, and incomparable systems comparable. Thus, this approach has the potential to mitigate some of the negative effects that arise from not having a community-wide standard for emotion annotation and representation (Calvo and Mac Kim, 2013; Buechel and Hahn, 2018a). We here want to contribute to this endeavor by providing a large-scale evaluation of previously proposed ERM approaches for four typologically diverse languages and report evidence that ERM clearly This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2892 Proceedings of the 27th International Conference on Computational Linguistics, pages 2892–2904 Santa Fe, New Mexico, USA, August 20-26, 2018. outperforms current state-of-the-art WEI algorithms. Furthermore, we present our own deep learning mode"
C18-1245,J96-2004,0,0.136048,"l publication.8 As for the word embeddings this method needs as input, we used the pre-trained FAST T EXT embeddings that Facebook Research makes available for a wide range of languages trained on the respective Wikipedias.9 This way, we hope to achieve a particularly high level of comparability across languages because, for each of them, embeddings are trained on data from the same domain and of a similar order of magnitude.10 3.4 Comparison to Human Reliability Since common metrics for Inter-Annotator Agreement (IAA), such as Cohen’s Kappa, are not applicable for real-valued emotion scores (Carletta, 1996), we will now discuss how to compare our own results against human assessments in order to put their reliability on a safe ground. One possible point of comparison that has been used in previous work (Buechel and Hahn, 2017a; Buechel and Hahn, 2018a) is inter-study reliability (ISR), i.e., the correlation between the ratings of common words in different data sets. However, this procedure comes with a number of downsides. First, the number of pairs of data sets with substantially overlapping entries is rather small since researchers focus mainly on acquiring ratings for novel words instead of g"
C18-1245,P97-1023,0,0.12956,", we present a refined evaluation methodology and gather strong evidence that our model yields results which are (almost) as reliable as human annotations, even in cross-lingual settings. Based on these results we generate new emotion ratings for 13 typologically diverse languages and claim that they have near-gold quality, at least. 1 Introduction From its inception, researchers in the field of sentiment analysis aimed at predicting the affective state that is typically associated with a given word based on a list of linguistic features, a problem referred to as word emotion induction (WEI) (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Early research activities have focused on semantic polarity (the positiveness or negativeness of a feeling) for quite a long time. But more recently this focus on binary representations has been replaced by more expressive emotion representation formats such as Basic Emotions or Valence-Arousal-Dominance. In the meantime, WEI has become an active area of research, regularly featured in shared tasks (Rosenthal et al., 2015; Yu et al., 2016b). Based on these achievements, WEI techniques have become a natural methodological choice for the automatic construction of emo"
C18-1245,L16-1413,0,0.136858,"Missing"
C18-1245,S17-1007,0,0.06527,"odologies (e.g., alternative sets of instructions or rating scales) and may thus vary substantially between different pairs of data sets. As an alternative, these shortcomings lead us to propose split-half reliability (SHR) as a new basis for our comparison. SHR is computed by splitting all individual ratings for each of the items into two groups. These individual ratings are then averaged for both groups and the Pearson correlation between the group averages is computed. The whole processes is repeated (typically 100 times) with random splits before averaging the results from each iteration (Mohammad and Bravo-Marquez, 2017). Thus, an 6 We found the usual recommendation of .2 on input and .5 on hidden layers (Srivastava et al., 2014) too high given the small number of features in our task (2 to 5). 7 In our most recent contribution featuring a large-scale evaluation of many current WEI approaches on numerous data sets, we found that among the existing ones the model proposed by Du and Zhang (2016) performs best, only beaten by our own, newly proposed model (Buechel and Hahn, 2018b). Note that even compared to this more advanced approach to WEI, the performance figures we report here for ERM still remain much high"
C18-1245,L16-1458,0,0.216123,"Missing"
C18-1245,D14-1162,0,0.081069,"as Word Emotion Induction (WEI)—is already dealt with in the seminal work of Hatzivassiloglou and McKeown (1997). At first, the features taken into account were typically derived from co-occurrence or terminology-based similarity with a small set of seed word with known emotional scores (Turney and Littman, 2003; Esuli and Sebastiani, 2005). Nowadays, these features are almost completely replaced by word embeddings, i.e., dense, low-dimensional vector representations of words that are trained on large volumes of raw text in an unsupervised manner. W ORD 2V EC (Mikolov et al., 2013), G LOV E (Pennington et al., 2014) and FAST T EXT (Bojanowski et al., 2017) are among today’s most popular algorithms for generating embeddings. WEI algorithms constitute a natural baseline for ERM because, first, they produce the same output (emotion ratings for words according to some emotion representation format), yet their predictions are based on expressively weaker features (word embeddings instead of emotion ratings for the same word but in another format), thus constituting a harder task. Second, they form the currently prevailing paradigm for the automatic construction of emotion lexicons (K¨oper and Schulte im Walde"
C18-1245,S15-2078,0,0.0169557,"typically associated with a given word based on a list of linguistic features, a problem referred to as word emotion induction (WEI) (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Early research activities have focused on semantic polarity (the positiveness or negativeness of a feeling) for quite a long time. But more recently this focus on binary representations has been replaced by more expressive emotion representation formats such as Basic Emotions or Valence-Arousal-Dominance. In the meantime, WEI has become an active area of research, regularly featured in shared tasks (Rosenthal et al., 2015; Yu et al., 2016b). Based on these achievements, WEI techniques have become a natural methodological choice for the automatic construction of emotion lexicons (K¨oper and Schulte im Walde, 2016; Shaikh et al., 2016). Yet, only very recently, a radically different approach to automatic emotion lexicon construction has been proposed. Instead of relying on linguistic features (such as similarity with seed words or word embeddings), the goal of emotion representation mapping (ERM) is to derive new emotional word ratings in one format based on known ratings of the same words in another format (Bue"
C18-1245,L16-1180,0,0.311651,"ties have focused on semantic polarity (the positiveness or negativeness of a feeling) for quite a long time. But more recently this focus on binary representations has been replaced by more expressive emotion representation formats such as Basic Emotions or Valence-Arousal-Dominance. In the meantime, WEI has become an active area of research, regularly featured in shared tasks (Rosenthal et al., 2015; Yu et al., 2016b). Based on these achievements, WEI techniques have become a natural methodological choice for the automatic construction of emotion lexicons (K¨oper and Schulte im Walde, 2016; Shaikh et al., 2016). Yet, only very recently, a radically different approach to automatic emotion lexicon construction has been proposed. Instead of relying on linguistic features (such as similarity with seed words or word embeddings), the goal of emotion representation mapping (ERM) is to derive new emotional word ratings in one format based on known ratings of the same words in another format (Buechel and Hahn, 2017a). For example, ERM could use empirically gathered ratings for Basic Emotions and convert them into a Valence-Arousal-Dominance representation scheme, with greater precision than currently achieva"
C18-1245,N16-1066,0,0.275145,"h a given word based on a list of linguistic features, a problem referred to as word emotion induction (WEI) (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Early research activities have focused on semantic polarity (the positiveness or negativeness of a feeling) for quite a long time. But more recently this focus on binary representations has been replaced by more expressive emotion representation formats such as Basic Emotions or Valence-Arousal-Dominance. In the meantime, WEI has become an active area of research, regularly featured in shared tasks (Rosenthal et al., 2015; Yu et al., 2016b). Based on these achievements, WEI techniques have become a natural methodological choice for the automatic construction of emotion lexicons (K¨oper and Schulte im Walde, 2016; Shaikh et al., 2016). Yet, only very recently, a radically different approach to automatic emotion lexicon construction has been proposed. Instead of relying on linguistic features (such as similarity with seed words or word embeddings), the goal of emotion representation mapping (ERM) is to derive new emotional word ratings in one format based on known ratings of the same words in another format (Buechel and Hahn, 20"
C18-1245,S07-1013,0,\N,Missing
C18-2003,P17-4005,0,0.0156918,"ord polarity (Hamilton et al., 2016a) or incorporates more nuanced models of emotions, such as Valence-Arousal-Dominance (Buechel and Hahn, 2018). This contribution integrates both lines of work in a unique way based on our prior research activities (Buechel et al., 2016; Buechel et al., 2017). To the best of our knowledge, only few systems share similarities with J E S EM E. Alternative websites for tracking diachronic word meaning yet offer far less diverse collections of corpora compared to J E S EM E and neither of them incorporates emotion values attached to lexical entries. For example, Arendt and Volkova (2017) provide only short term trends in word similarity in two social media corpora in their E STEEM system.2 The system3 by This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 Website available at:jeseme.org; sources available at: github.com/JULIELab/JeSemE 2 esteem.labworks.org/ 3 embvis.flovis.net/s/neighborhoods.html 10 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 10–14 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: J E S EM E"
C18-2003,N18-1173,1,0.765504,"eficial for the data-driven interpretation of literary text genres (Kim et al., 2017). Measuring affective information on the lexical level is an active field of research in computational linguistics (Liu, 2015). Yet, most contributions focus on contemporary language and are limited to shallow representations of human emotions, mainly distinguishing between positive and negative feelings. Current research in sentiment analysis either starts to include historical trends in word polarity (Hamilton et al., 2016a) or incorporates more nuanced models of emotions, such as Valence-Arousal-Dominance (Buechel and Hahn, 2018). This contribution integrates both lines of work in a unique way based on our prior research activities (Buechel et al., 2016; Buechel et al., 2017). To the best of our knowledge, only few systems share similarities with J E S EM E. Alternative websites for tracking diachronic word meaning yet offer far less diverse collections of corpora compared to J E S EM E and neither of them incorporates emotion values attached to lexical entries. For example, Arendt and Volkova (2017) provide only short term trends in word similarity in two social media corpora in their E STEEM system.2 The system3 by"
C18-2003,W16-4008,1,0.881097,"ical level is an active field of research in computational linguistics (Liu, 2015). Yet, most contributions focus on contemporary language and are limited to shallow representations of human emotions, mainly distinguishing between positive and negative feelings. Current research in sentiment analysis either starts to include historical trends in word polarity (Hamilton et al., 2016a) or incorporates more nuanced models of emotions, such as Valence-Arousal-Dominance (Buechel and Hahn, 2018). This contribution integrates both lines of work in a unique way based on our prior research activities (Buechel et al., 2016; Buechel et al., 2017). To the best of our knowledge, only few systems share similarities with J E S EM E. Alternative websites for tracking diachronic word meaning yet offer far less diverse collections of corpora compared to J E S EM E and neither of them incorporates emotion values attached to lexical entries. For example, Arendt and Volkova (2017) provide only short term trends in word similarity in two social media corpora in their E STEEM system.2 The system3 by This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons"
C18-2003,D16-1057,0,0.0725885,"connotation of words in parallel with their lexical semantics. Such a functionality is widely considered beneficial for the data-driven interpretation of literary text genres (Kim et al., 2017). Measuring affective information on the lexical level is an active field of research in computational linguistics (Liu, 2015). Yet, most contributions focus on contemporary language and are limited to shallow representations of human emotions, mainly distinguishing between positive and negative feelings. Current research in sentiment analysis either starts to include historical trends in word polarity (Hamilton et al., 2016a) or incorporates more nuanced models of emotions, such as Valence-Arousal-Dominance (Buechel and Hahn, 2018). This contribution integrates both lines of work in a unique way based on our prior research activities (Buechel et al., 2016; Buechel et al., 2017). To the best of our knowledge, only few systems share similarities with J E S EM E. Alternative websites for tracking diachronic word meaning yet offer far less diverse collections of corpora compared to J E S EM E and neither of them incorporates emotion values attached to lexical entries. For example, Arendt and Volkova (2017) provide o"
C18-2003,P16-1141,0,0.104697,"connotation of words in parallel with their lexical semantics. Such a functionality is widely considered beneficial for the data-driven interpretation of literary text genres (Kim et al., 2017). Measuring affective information on the lexical level is an active field of research in computational linguistics (Liu, 2015). Yet, most contributions focus on contemporary language and are limited to shallow representations of human emotions, mainly distinguishing between positive and negative feelings. Current research in sentiment analysis either starts to include historical trends in word polarity (Hamilton et al., 2016a) or incorporates more nuanced models of emotions, such as Valence-Arousal-Dominance (Buechel and Hahn, 2018). This contribution integrates both lines of work in a unique way based on our prior research activities (Buechel et al., 2016; Buechel et al., 2017). To the best of our knowledge, only few systems share similarities with J E S EM E. Alternative websites for tracking diachronic word meaning yet offer far less diverse collections of corpora compared to J E S EM E and neither of them incorporates emotion values attached to lexical entries. For example, Arendt and Volkova (2017) provide o"
C18-2003,C16-1262,1,0.822187,"re 1. It starts with orthographically normalizing the corpus slices, i.e., lower casing only for English and a historical spelling-aware lemmatization for German (Jurish, 2013). We then Figure 2: J E S EM E in operation I: Meaning change of “heart” reluse a modified version of H YPER ative to reference words since the 1830s in the COHA. WORDS 5 to calculate slice-specific embedding models with SVDPPMI (Levy et al., 2015). This algorithm was chosen for its superior reliability which is essential for interpreting local neighborhoods in embedding spaces as is done in the remainder of this paper (Hellrich and Hahn, 2016; Hellrich and Hahn, 2017a). Apart from word vectors, we also calculate word-based co-occurrence statistics, frequency information and emotion values for each slice (see Section 3). All this information is stored in a relational database. Compared to Hellrich and Hahn (2017b), our current version also reduces the database size from approximately 120GB to 40GB. This is achieved by storing word vectors instead of pre-computed similarity scores. Unlike the previous version, semantic similarity between most words will be computed on the fly. Only the most 4 5 tec.citius.usc.es/explorador-diacronic"
C18-2003,P17-4006,1,0.921125,"riod a scholar is investigating really available and, if so, does it cover all of the lexical items of interest? Word embeddings have been proposed as a technical vehicle to increase lexical coverage (Kim et al., 2014). However, they require locally installed software and time-consuming calculations, thus being illsuited for mostly non-technical users in the humanities. As an alternative, we here present an extended version of J E S EM E, a user-friendly open source website1 for accessing embedding-derived diachronic information on lexical meaning and emotion. The first release of J E S EM E (Hellrich and Hahn, 2017b) mainly provided time-variant diachronic lexical semantic information. Its second version, the focus of this paper, excels with the unique capability to additionally track the diachronic emotional connotation of words in parallel with their lexical semantics. Such a functionality is widely considered beneficial for the data-driven interpretation of literary text genres (Kim et al., 2017). Measuring affective information on the lexical level is an active field of research in computational linguistics (Liu, 2015). Yet, most contributions focus on contemporary language and are limited to shallo"
C18-2003,L16-1305,0,0.107797,"ddings trained on the English Google Books corpus by Hamilton et al. (2016b). The D IACHRONIC E XPLORER4 which uses sparse vector representations instead of word embeddings to calculate lexical similarity is limited to the Spanish Google Books corpus (Gamallo et al., 2018). 2 Architecture and Website J E S EM E uses five diachronic corpora: the Google Books N-Gram Corpus for German and its English fiction register (Michel et al., 2011), the Corpus of Historical American English (COHA; Davies (2012)), the Deutsches Textarchiv [‘German Text Archive’] (Geyken, 2013) and the Royal Society Corpus (Kermes et al., 2016). To ensure high embedding quality, these corpora are divided into temporal slices of similar size covering between 10 to 50 years each. J E S EM E’s processing pipeline is depicted in Figure 1. It starts with orthographically normalizing the corpus slices, i.e., lower casing only for English and a historical spelling-aware lemmatization for German (Jurish, 2013). We then Figure 2: J E S EM E in operation I: Meaning change of “heart” reluse a modified version of H YPER ative to reference words since the 1830s in the COHA. WORDS 5 to calculate slice-specific embedding models with SVDPPMI (Levy"
C18-2003,W14-2517,0,0.019903,"retation of texts in the humanities. 1 Introduction Historical, manually compiled dictionaries are central to many kinds of studies in the humanities, since they provide scholars with information about the lexical meaning of terms in former time periods. Yet, this traditional approach is limited in many ways, coverage being perhaps the most pressing issue: Is a dictionary for the specific time period a scholar is investigating really available and, if so, does it cover all of the lexical items of interest? Word embeddings have been proposed as a technical vehicle to increase lexical coverage (Kim et al., 2014). However, they require locally installed software and time-consuming calculations, thus being illsuited for mostly non-technical users in the humanities. As an alternative, we here present an extended version of J E S EM E, a user-friendly open source website1 for accessing embedding-derived diachronic information on lexical meaning and emotion. The first release of J E S EM E (Hellrich and Hahn, 2017b) mainly provided time-variant diachronic lexical semantic information. Its second version, the focus of this paper, excels with the unique capability to additionally track the diachronic emotio"
C18-2003,W17-2203,0,0.104339,"Missing"
C18-2003,Q15-1016,0,0.0142297,"2016). To ensure high embedding quality, these corpora are divided into temporal slices of similar size covering between 10 to 50 years each. J E S EM E’s processing pipeline is depicted in Figure 1. It starts with orthographically normalizing the corpus slices, i.e., lower casing only for English and a historical spelling-aware lemmatization for German (Jurish, 2013). We then Figure 2: J E S EM E in operation I: Meaning change of “heart” reluse a modified version of H YPER ative to reference words since the 1830s in the COHA. WORDS 5 to calculate slice-specific embedding models with SVDPPMI (Levy et al., 2015). This algorithm was chosen for its superior reliability which is essential for interpreting local neighborhoods in embedding spaces as is done in the remainder of this paper (Hellrich and Hahn, 2016; Hellrich and Hahn, 2017a). Apart from word vectors, we also calculate word-based co-occurrence statistics, frequency information and emotion values for each slice (see Section 3). All this information is stored in a relational database. Compared to Hellrich and Hahn (2017b), our current version also reduces the database size from approximately 120GB to 40GB. This is achieved by storing word vecto"
C86-1118,J80-3001,0,0.0556293,"Missing"
C86-1118,J80-1002,0,0.0731863,"Missing"
C86-1118,C82-1020,0,\N,Missing
C86-1118,C82-1013,0,\N,Missing
C92-1008,J91-2003,0,\N,Missing
C92-1008,J80-1002,0,\N,Missing
C92-1008,C88-2120,0,\N,Missing
C92-1008,C86-1043,0,\N,Missing
C94-1061,J92-2004,0,0.121046,"Missing"
C94-1061,C88-1030,0,0.0628626,"Missing"
C94-1061,C90-2024,0,0.0232146,"Missing"
C94-1061,C92-1024,0,0.0652411,"Missing"
C94-1061,C92-4170,0,0.0609455,"Missing"
C94-1061,C88-2121,0,0.11302,"Missing"
C94-1061,C86-1028,0,0.0765896,"Missing"
C94-1061,C88-1049,0,\N,Missing
C94-1061,C92-4171,0,\N,Missing
C94-1061,E85-1029,0,\N,Missing
C94-1061,C94-1080,1,\N,Missing
C94-1061,C90-3052,0,\N,Missing
C94-1061,J92-2001,0,\N,Missing
C94-1061,P85-1022,0,\N,Missing
C94-1061,C88-2158,0,\N,Missing
C94-1061,P92-1041,0,\N,Missing
C94-1080,C94-1061,1,0.83497,"Missing"
C94-1080,J92-2004,0,0.224963,"Missing"
C94-1080,C88-1049,0,\N,Missing
C96-1084,C92-1023,0,0.181619,"Missing"
C96-1084,P86-1004,0,\N,Missing
C96-1084,H86-1011,0,\N,Missing
C96-1084,C94-2126,0,\N,Missing
C96-1084,E95-1033,1,\N,Missing
C96-1084,J95-2003,0,\N,Missing
C96-1084,P96-1036,1,\N,Missing
C96-1085,P94-1016,0,0.0302245,"ilistic and Lee et al. (1995) for symbolic heuristics to cope with that problem). Thus, under realistic conditions, these techniques loose a lot of their theoretical appeal and compete with other approaches merely on the basis of performance measurements. Second, including semantic considerations, even if we assume efficient syntactic processing for the sake of argument, the question arises how semantic interpretations can be processed in an incremental, comparably efficient way. Though experiments have been run with packing feature structures and interleaving syntactic and semantic analyses (Dowding et al., 1994), or with the intentional underspecification of logical forms (leaving scope ambiguities of quantifiers and negations underdetermined; cf., e.g., Hobbs (1983) or Reyle (1995)), no conclusive evidences have been generated so far in favor of a general method for efficient, online semantic interpretation. As we are faced, however, with the problem to work out text interpretations incrementally and within reasonable resource bounds, we opt for a methodology that constrains the amount of ambiguous structures right at the source. Hence, the incompleteness of the algorithm trades theoretical purism f"
C96-1085,1991.iwpt-1.10,0,0.0371962,"and the lack of grammatical abstraction and formalization. Preserving the strengths of this approach (lexicalized control), but at the sane time reconciling it with current standards of lexicalized grammar specification, the PARSETALK system can be considered a unifying approach which combines procedural and declarative specifications at the grammar level in a formally disciplined way. This also distinguishes our approach from another major stream of object-oriented natural language parsing which is almost entirely concerned with implementational aspects of object-oriented programruing, e.g., Habert (1991), Lin (1993) or Yonezawa & Ohsawa (1994). The reasons why we diverge from conventional parsing methodologies, e.g., chart parsing based on Earley- or Tomita-style algorithms, are two-fold. First, at the syntactic level, any kind of chart parsing algorithm faces combinatorial problems with noncontiguous grammar specifications (accounting for discontinuous language structures) and, in particular, extra- and ungrammatical language input (cf., e.g., Magerman & Weir (1992) for probabilistic and Lee et al. (1995) for symbolic heuristics to cope with that problem). Thus, under realistic conditions, t"
C96-1085,C96-1084,1,0.833002,"e text level of analysis. In order to establish, e.g., a dependency relation the syntactic and semantic constraints relating to the head and its prospective modifier are checked in tandem. Due to this close coupling of grammatical and conceptual constraints syntactically possible though otherwise disallowed structures are filtered out as early as possible. Also, the provision of conceptual entities which are incrementally generated by the semantic interpretation process supplies the necessary anchoring points for the continuous resolution of textual anaphora and ellipses (Strube & Hahn, 1995; Hahn et al., 1996). The lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms (e.g., LTAGS (Schabes et al., 1988) or HPSG (Pollard & Sag, 1994)) is still constrained to declarative notions. Given that the control flow of text understanding is globally unpredictable and, also, needs to be purposefully adapted to critical states of the analysis 503 (e.g., cases of severe extragrammaticality), we drive lexicalization to its limits in that we also incorporate procedural control knowledge at the lexical gr,'unmar level. The specification of lexiealized communication primitive"
C96-1085,P83-1009,0,0.0516635,"ppeal and compete with other approaches merely on the basis of performance measurements. Second, including semantic considerations, even if we assume efficient syntactic processing for the sake of argument, the question arises how semantic interpretations can be processed in an incremental, comparably efficient way. Though experiments have been run with packing feature structures and interleaving syntactic and semantic analyses (Dowding et al., 1994), or with the intentional underspecification of logical forms (leaving scope ambiguities of quantifiers and negations underdetermined; cf., e.g., Hobbs (1983) or Reyle (1995)), no conclusive evidences have been generated so far in favor of a general method for efficient, online semantic interpretation. As we are faced, however, with the problem to work out text interpretations incrementally and within reasonable resource bounds, we opt for a methodology that constrains the amount of ambiguous structures right at the source. Hence, the incompleteness of the algorithm trades theoretical purism for feasibility of realistic NLP. 5 Conclusions We have presented a restricted approach to parallelism for object-oriented lexicalized parsing. Given the compl"
C96-1085,E95-1031,0,0.0304894,"Missing"
C96-1085,P93-1016,0,0.0321966,"grammatical abstraction and formalization. Preserving the strengths of this approach (lexicalized control), but at the sane time reconciling it with current standards of lexicalized grammar specification, the PARSETALK system can be considered a unifying approach which combines procedural and declarative specifications at the grammar level in a formally disciplined way. This also distinguishes our approach from another major stream of object-oriented natural language parsing which is almost entirely concerned with implementational aspects of object-oriented programruing, e.g., Habert (1991), Lin (1993) or Yonezawa & Ohsawa (1994). The reasons why we diverge from conventional parsing methodologies, e.g., chart parsing based on Earley- or Tomita-style algorithms, are two-fold. First, at the syntactic level, any kind of chart parsing algorithm faces combinatorial problems with noncontiguous grammar specifications (accounting for discontinuous language structures) and, in particular, extra- and ungrammatical language input (cf., e.g., Magerman & Weir (1992) for probabilistic and Lee et al. (1995) for symbolic heuristics to cope with that problem). Thus, under realistic conditions, these techniq"
C96-1085,P92-1006,0,0.0249764,"riented natural language parsing which is almost entirely concerned with implementational aspects of object-oriented programruing, e.g., Habert (1991), Lin (1993) or Yonezawa & Ohsawa (1994). The reasons why we diverge from conventional parsing methodologies, e.g., chart parsing based on Earley- or Tomita-style algorithms, are two-fold. First, at the syntactic level, any kind of chart parsing algorithm faces combinatorial problems with noncontiguous grammar specifications (accounting for discontinuous language structures) and, in particular, extra- and ungrammatical language input (cf., e.g., Magerman & Weir (1992) for probabilistic and Lee et al. (1995) for symbolic heuristics to cope with that problem). Thus, under realistic conditions, these techniques loose a lot of their theoretical appeal and compete with other approaches merely on the basis of performance measurements. Second, including semantic considerations, even if we assume efficient syntactic processing for the sake of argument, the question arises how semantic interpretations can be processed in an incremental, comparably efficient way. Though experiments have been run with packing feature structures and interleaving syntactic and semantic"
C96-1085,E95-1001,0,0.0229967,"e with other approaches merely on the basis of performance measurements. Second, including semantic considerations, even if we assume efficient syntactic processing for the sake of argument, the question arises how semantic interpretations can be processed in an incremental, comparably efficient way. Though experiments have been run with packing feature structures and interleaving syntactic and semantic analyses (Dowding et al., 1994), or with the intentional underspecification of logical forms (leaving scope ambiguities of quantifiers and negations underdetermined; cf., e.g., Hobbs (1983) or Reyle (1995)), no conclusive evidences have been generated so far in favor of a general method for efficient, online semantic interpretation. As we are faced, however, with the problem to work out text interpretations incrementally and within reasonable resource bounds, we opt for a methodology that constrains the amount of ambiguous structures right at the source. Hence, the incompleteness of the algorithm trades theoretical purism for feasibility of realistic NLP. 5 Conclusions We have presented a restricted approach to parallelism for object-oriented lexicalized parsing. Given the complex control struc"
C96-1085,C88-2121,0,0.252847,"arallel execution of the parsing task. This leads us to a parsing algorithm with restricted parallelism, whose experimental evaluation is briefly summarized. 3.1 The PARSETALK Model The PARSETALK grammar we use (for a survey, cf. BrOker et al. (1994)) is based on binary relations between words, e.g., dependency relations between a head and its modifier, or textual relations between an anaphor and its antecedent. Restrictions on possible relations are attached to the words, e.g., expressed as valencies in the case of dependency relations, yielding a strictly lexicalized grammar in the sense of Schabes et al. (1988). The individual behavior of words is generalized in terms of word classes which are primarily motivated by governability or phrasal distribution; additional criteria include inflection, anaphoric behavior, and possible modifiers. A word class specifies morphosyntactic features, valencies, and allowed orderings for its instances. Further abstraction is achieved by organizing word classes at different levels of speciticity in terms of inheritance hierarchies. The specification of binary constraints already provides inherent means for robust analysis, as grammatical functions describe relations"
C96-1085,J83-1005,0,0.253419,"Missing"
C96-1085,C94-1061,1,\N,Missing
C96-1085,E95-1033,1,\N,Missing
C96-1085,C92-1058,0,\N,Missing
C96-1085,C88-2158,0,\N,Missing
C98-1076,C94-1061,1,0.931278,"terminological reasoning. The 'plain' text understanding mode can be considered as tile instantiation and continuous filling 476 y M+t h i ~ Figure 1: Architectnre of the Text Learner of roles with respect to single concepts already available ill tile knowledge base. Under learning conditions, however, a set of alternative concept hypotheses has to be maintained for each nnknown item, with each hypothesis denoting a newly created conceptual interpretation tentatively associated with tile unknown item. Tile underlying methodology is summarized in Fig. 1. Tile text parser (for an overview, cf. BrSker et al. (1994)) yields information from tile grammatical constructions in which an unknown lexical item (symbolized by the black square) occurs in terms of the corresponding dependency parse tree. The kinds of syntactic collstructions (e.g., genitive, apposition, comparative), in which unknown lexical items appear, are recorded and later assessed relative to the credit they lend to a particular hypothesis. The conceptual interpretation of parse trees involving unknown lexical items in the domain knowle@e base leads to the derivation of concept hypotheses, which are further enriched by conceptual annotations"
C98-1076,C92-2082,0,0.00387056,"of the domain the texts are about, and grammatical constructions in which unknown lexical items occur. While there may be many rea+sonable interpretations when an unknown item occurs for the very first time in a text, their number rapidly decreases when more and more evidence is gathered. Our model tries to make explicit tile reasoning processes behind this learning pattern. Unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of large corpora of texts (Hin(tie, 1989; Zernik and aacobs, 1990; Hearst, 1992; Manning, 1993), we propose here a k++owledge-intensive model of concept learning from few, positive-only examples that is tightly integrated with tile non-learning mode of text understanding. Both learning and understanding build on a given core ontology in tile format of terminological assertions and, hence, make abundant use of terminological reasoning. The 'plain' text understanding mode can be considered as tile instantiation and continuous filling 476 y M+t h i ~ Figure 1: Architectnre of the Text Learner of roles with respect to single concepts already available ill tile knowledge base"
C98-1076,P89-1015,0,0.0956086,"Missing"
C98-1076,P92-1053,0,0.0264184,"Missing"
C98-1076,J91-2002,0,0.0830608,"Missing"
C98-1076,C90-1005,0,0.0387541,"Missing"
C98-1076,A92-1014,0,\N,Missing
C98-1076,P93-1032,0,\N,Missing
D07-1051,P96-1042,0,0.913855,"nerated on the basis of AL and, hence, with a poorer feature set. 2 Related Work There are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on"
D07-1051,W05-0619,0,0.284567,"al parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hachey et al. (2005) propose a committee-based AL approach where the committee’s classifiers constitute multiple views on the data by employing different feature subsets. The authors focus on (possible) negative side effects of AL on the annotations. They argue that AL annotations are cognitively more difficult to deal with for the annotators (because of the increased complexity of the selected sentences). Hence, lower annotation quality and higher per-sentence annotation times might be a concern. 487 There are controversial findings on the reusability of data annotated by means of AL for the problem of parse tre"
D07-1051,W00-1306,0,0.248876,"dological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hachey et al. (2005) propose a committee-based AL approach"
D07-1051,W01-0710,0,0.048314,"based AL approach where the committee’s classifiers constitute multiple views on the data by employing different feature subsets. The authors focus on (possible) negative side effects of AL on the annotations. They argue that AL annotations are cognitively more difficult to deal with for the annotators (because of the increased complexity of the selected sentences). Hence, lower annotation quality and higher per-sentence annotation times might be a concern. 487 There are controversial findings on the reusability of data annotated by means of AL for the problem of parse tree selection. Whereas Hwa (2001) reports positive results, Baldridge and Osborne (2004) argue that AL based on uncertainty sampling may face serious performance degradation when labeled data is reused for training a classifier different from the one employed during AL. For committee-based AL, however, there is a lack of work on reusability. Our experiments of committee-based AL for entity recognition, however, reveal that for this task at least, reusability can be guaranteed to a very large extent. 3 AL for Corpus Annotation Requirements for Practical Use AL frameworks for real-world corpus annotation should meet the followi"
D07-1051,W04-3202,0,0.511613,"ea to let the learner have control over the examples to be manually labeled so as to optimize the prediction accuracy. Accordingly, AL aims at selecting those examples with high utility for the model. AL (as well as semi-supervised methods) is typically considered as a learning protocol, i.e., to train a particular classifier. In contrast, we here propose to employ AL as a corpus annotation method. A corpus built on these premises must, however, still be reusable in a flexible way so that, e.g., training with modified or improved classifiers is feasible and reasonable on AL-generated corpora. Baldridge and Osborne (2004) have already argued that this is a highly critical requirement because the examples selected by AL are tuned to one particular classifier. The second major contribution of this paper ad486 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 486–495, Prague, June 2007. 2007 Association for Computational Linguistics dresses this issue and provides empirical evidence that corpora built with one type of classifier (based on Maximum Entropy) can reasonably be reused by another, methodologically related type"
D07-1051,J96-1002,0,0.00722345,"Missing"
D07-1051,P00-1016,0,0.666581,", with a poorer feature set. 2 Related Work There are mainly two methodological strands of AL research, viz. optimization approaches which aim at selecting those examples that optimize some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hac"
D07-1051,W01-0501,0,0.0415248,"Missing"
D07-1051,P04-1075,0,0.634643,"e some (algorithm-dependent) objective function, such as prediction variance (Cohn et al., 1996), and heuristic methods with uncertainty sampling (Lewis and Catlett, 1994) and query-by-committee (QBC) (Seung et al., 1992) just to name the most prominent ones. AL has already been applied to several NLP tasks, such as document classification (Schohn and Cohn, 2000), POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Thompson et al., 1999; Hwa, 2000), and information extraction (Lewis and Catlett, 1994; Thompson et al., 1999). In a more recent study, Shen et al. (2004) consider AL for entity recognition based on Support Vector Machines. Here, the informativeness of an example is estimated by the distance to the hyperplane of the currently learned SVM. It is assumed that an example which lies close to the hyperplane has high chances to have an effect on training. This approach is essentially limited to the SVM learning scheme as it solely relies on SVM-internal selection criteria. Hachey et al. (2005) propose a committee-based AL approach where the committee’s classifiers constitute multiple views on the data by employing different feature subsets. The autho"
D07-1051,W03-0419,0,0.0421439,"Missing"
D07-1051,W07-1502,1,0.871692,"Missing"
D10-1096,W09-1402,0,0.0353799,"Missing"
D10-1096,W09-1403,1,0.926126,"of dependency pairing or structuring (which pairs of words join in a dependency relation?) and dependency typing (how are dependency relations for a particular pair labelled?). Depending on the type of dependency theory or parser being used, various representations emerge (Miyao et al., 2007). In this paper, we explore these different representations of the dependency graphs and try, first, to pinpoint their effects on solving the overall event extraction task and, second, to further enhance the potential of JR E X, a high-performance relation and event extractor developed at the J ULIE Lab (Buyko et al., 2009). 2 Related Work In the biomedical domain, the focus has largely been on binary relations, in particular protein-protein interactions (PPIs). Accordingly, the biomedical NLP community has developed various PPIannotated corpora (e.g., LLL (N´edellec, 2005), AIM ED (Bunescu et al., 2005), B IO I NFER (Pyysalo et al., 2007)). PPI extraction does clearly not count as a solved problem, and a deeper look at its biological and representational intricacies is certainly worthwhile. The G ENIA event corpus (Kim et al., 2008) and the BioNLP 2009 Shared Task data (Kim et al., 2009) contain such detailed a"
D10-1096,cer-etal-2010-parsing,0,0.0608408,"Missing"
D10-1096,P05-1022,0,0.0190724,"oding the adjective as the head of all the nouns preceding this adjective in the noun phrase under scrutiny (see Figure 4). 5 Experiments and Results In this section, we describe the experiments and results related to event extraction tasks based on alternative dependency graph representations. For our experiments, we selected the following topperforming parsers — the first three phrase structure based and thus the origin of derivative dependency structures, the last three fully dependency based for making native dependency structures available: • C+J, Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005), with the WSJtrained parsing model. • M+C, Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005), with the selftrained biomedical parsing model from McClosky (2010). • Bikel, Bikel’s parser (Bikel, 2004) with the WSJ-trained parsing model. • GDep (Sagae and Tsujii, 2007), a native dependency parser. • MST (McDonald et al., 2005), another native dependency parser. • MALT (Nivre et al., 2007), yet another native dependency parser. The native dependency parsers were re-trained on the G ENIA Treebank (Tateisi et al., 2005) conversions.14 These conversions,15 i.e., Stanford basic, C"
D10-1096,W05-1102,0,0.0301989,"ency type for representing appositions).12 • Stanford dependencies (SD). This format was proposed by de Marneffe et al. (2006) for 11 We disregard in this study other dependency representations such as M INI PAR and L INK G RAMMAR representations. 12 For the differences between CoNLL’07 and CoNLL’08 representations, cf. http://nlp.cs.lth.se/software/ treebank_converter/ 985 semantics-sensitive applications using dependency representations, and can be obtained using the Stanford tools13 from PTB trees. The Stanford format is widely used in the biomedical domain (e.g., by Miyao et al. (2008) or Clegg and Shepherd (2005)). There are systematic differences between CoNLL’X and Stanford dependencies, e.g., as far as the representation of passive constructions, the position of auxiliary and modal verbs, or coordination representation is concerned. In particular, the representation of the passive construction and the role of the auxiliary verb therein may have considerable effects for semantics-sensitive tasks. While in SD the subject of the passive construction is represented by a special nsubj dependency label, in CD we find the same subject label as for active constructions SUB(J). On CoNLL’08 data, the logical"
D10-1096,de-marneffe-etal-2006-generating,0,0.227618,"Missing"
D10-1096,W07-2416,0,0.0944556,"types,10 while the Stanford scheme (de Marneffe et al., 2006) incorporates 48 types (so called grammatical relations or Stanford dependencies). The Link Grammar Parser (Sleator and Temperley, 1991) employs a particularly finegrained repertoire of dependency relations adding up to 106 types, whereas the well-known M INI PAR parser (Lin, 1998) relies on 59 types. Differences in dependency structure are at least as common as differences in dependency relation typing (see below). 10 Computed by using the conversion script on WSJ data (accessible via http://nlp.cs.lth.se/ pennconverter/; see also Johansson and Nugues (2007) for additional information). From the G ENIA corpus, using this script, we could only extract 29 CoNLL dependency relations. Figure 1: Example of CoNLL 2008 dependencies, as used in most of the native dependency parsers. Figure 2: Stanford dependencies, basic conversion from Penn Treebank. In general, dependency graphs can be generated by syntactic parsers in two ways. First, native dependency parsers output CoNLL’X or Stanford dependencies dependent on which representation format they have been trained on.11 Second, in a derivative dependency mode, the output of constituencybased parsers, e."
D10-1096,W09-1401,0,0.0419515,"Extraction Tasks Ekaterina Buyko and Udo Hahn Jena University Language & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit¨at Jena F¨urstengraben 30, 07743 Jena, Germany {ekaterina.buyko|udo.hahn}@uni-jena.de (both in the newspaper domain such as for ACE1 , as well as in the biological domain such as for BioCreative2 or the BioNLP Shared Task3 ), comparable in terms of analytical complexity with recent efforts directed at opinion mining (e.g., NTCIR-74 or TREC Blog tracks5 ) or the recognition of textual entailment.6 The most recent BioNLP 2009 Shared Task on Event Extraction (Kim et al., 2009) required, for a sample of 260 M EDLINE abstracts, to determine all mentioned events — to be chosen from a given set of nine event types, including “Localization”, “Binding”, “Gene Expression”, “Transcription”, “Protein Catabolism”, “Phosphorylation”, “Positive Regulation”, “Negative Regulation”, and (unspecified) “Regulation” — and link them appropriately with a priori supplied protein annotations. The demands on text analytics to deal with the complexity of this Shared Task in terms of relation diversity and specificity are unmatched by former challenges. For relation extraction in the biome"
D10-1096,N10-1004,0,0.0166834,"ts and results related to event extraction tasks based on alternative dependency graph representations. For our experiments, we selected the following topperforming parsers — the first three phrase structure based and thus the origin of derivative dependency structures, the last three fully dependency based for making native dependency structures available: • C+J, Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005), with the WSJtrained parsing model. • M+C, Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005), with the selftrained biomedical parsing model from McClosky (2010). • Bikel, Bikel’s parser (Bikel, 2004) with the WSJ-trained parsing model. • GDep (Sagae and Tsujii, 2007), a native dependency parser. • MST (McDonald et al., 2005), another native dependency parser. • MALT (Nivre et al., 2007), yet another native dependency parser. The native dependency parsers were re-trained on the G ENIA Treebank (Tateisi et al., 2005) conversions.14 These conversions,15 i.e., Stanford basic, CoNLL’07 and CoNLL’08 were produced with the currently available conversion scripts. For the Stanford dependency conversion, we used the Stanford parser tool,16 for CoNLL’07 and CoN"
D10-1096,H05-1066,0,0.0366371,"rming parsers — the first three phrase structure based and thus the origin of derivative dependency structures, the last three fully dependency based for making native dependency structures available: • C+J, Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005), with the WSJtrained parsing model. • M+C, Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005), with the selftrained biomedical parsing model from McClosky (2010). • Bikel, Bikel’s parser (Bikel, 2004) with the WSJ-trained parsing model. • GDep (Sagae and Tsujii, 2007), a native dependency parser. • MST (McDonald et al., 2005), another native dependency parser. • MALT (Nivre et al., 2007), yet another native dependency parser. The native dependency parsers were re-trained on the G ENIA Treebank (Tateisi et al., 2005) conversions.14 These conversions,15 i.e., Stanford basic, CoNLL’07 and CoNLL’08 were produced with the currently available conversion scripts. For the Stanford dependency conversion, we used the Stanford parser tool,16 for CoNLL’07 and CoNLL’08 we used the treebank-to-CoNLL conversion scripts17 available from the CoNLL’X Shared Task organizers. The phrase structure based parsers were applied with alrea"
D10-1096,P08-1006,0,0.674744,"ncy graphs for solving the event extraction tasks. While the T URKU system exploits the Stanford dependencies from the McClosky-Charniak parser (Charniak and Johnson, 2005), and the J ULIELab system uses the CoNLL-like dependencies from the GDep parser (Sagae and Tsujii, 2007),8 the T OKYO system overlays the Shared Task data with two parsing representations, viz. Enju PAS structure (Miyao and Tsujii, 2002) and GDep parser dependencies. Obviously, one might raise the question as to what extent the performance of these systems depends on the choice of the parser and its output representations. Miyao et al. (2008) already assessed the impact of different parsers for the task of biomedical relation extraction (PPI). Here we perform a similar study for the task of event extraction and focus, in particular, on the impact of various dependency representations such as Stanford and CoNLL’X dependencies and additional trimming procedures. For the experiments on which we report here, we performed experiments with the J ULIELab system. Our main goal is to investigate into the crucial role of proper representation structures for dependency graphs so that the performance gap from Shared Task results between the b"
D10-1096,D07-1111,0,0.0192982,"ons. For our experiments, we selected the following topperforming parsers — the first three phrase structure based and thus the origin of derivative dependency structures, the last three fully dependency based for making native dependency structures available: • C+J, Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005), with the WSJtrained parsing model. • M+C, Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005), with the selftrained biomedical parsing model from McClosky (2010). • Bikel, Bikel’s parser (Bikel, 2004) with the WSJ-trained parsing model. • GDep (Sagae and Tsujii, 2007), a native dependency parser. • MST (McDonald et al., 2005), another native dependency parser. • MALT (Nivre et al., 2007), yet another native dependency parser. The native dependency parsers were re-trained on the G ENIA Treebank (Tateisi et al., 2005) conversions.14 These conversions,15 i.e., Stanford basic, CoNLL’07 and CoNLL’08 were produced with the currently available conversion scripts. For the Stanford dependency conversion, we used the Stanford parser tool,16 for CoNLL’07 and CoNLL’08 we used the treebank-to-CoNLL conversion scripts17 available from the CoNLL’X Shared Task organizers."
D10-1096,I05-2038,0,0.0290153,"senting appositions).12 • Stanford dependencies (SD). This format was proposed by de Marneffe et al. (2006) for 11 We disregard in this study other dependency representations such as M INI PAR and L INK G RAMMAR representations. 12 For the differences between CoNLL’07 and CoNLL’08 representations, cf. http://nlp.cs.lth.se/software/ treebank_converter/ 985 semantics-sensitive applications using dependency representations, and can be obtained using the Stanford tools13 from PTB trees. The Stanford format is widely used in the biomedical domain (e.g., by Miyao et al. (2008) or Clegg and Shepherd (2005)). There are systematic differences between CoNLL’X and Stanford dependencies, e.g., as far as the representation of passive constructions, the position of auxiliary and modal verbs, or coordination representation is concerned. In particular, the representation of the passive construction and the role of the auxiliary verb therein may have considerable effects for semantics-sensitive tasks. While in SD the subject of the passive construction is represented by a special nsubj dependency label, in CD we find the same subject label as for active constructions SUB(J). On CoNLL’08 data, the logical"
D10-1096,J04-4004,0,\N,Missing
D10-1096,W06-2920,0,\N,Missing
D19-5103,N18-1173,1,0.8937,"ints for 2019 and has been revised down by 0.2 percentage points for 2020 and by 0.1 percentage points for 2021. The risks surrounding the euro area growth outlook remain tilted to the downside, on account of the prolonged presence of uncertainties, related to geopolitical factors, the rising threat of protectionism and vulnerabilities in emerging markets. In contrast, a growing number of researchers start focusing on more complex and informative representations of affective states, often following distinct psychological research traditions (Yu et al., 2016; Wang et al., 2016; Mohammad, 2018; Buechel and Hahn, 2018b). Figure 1: Excerpt of ECB statement from June 6, 2019. Studies applying NLP methods to various other fields seem to benefit strongly from such additional information. For example, Kim et al. (2017) examine the relationship between literary genres and emotional plot development finding that, in contrast to other, more predictive emotion categories, Joy as a common emotional category is only moderately helpful for genre classification. More closely related to us, Bollen et al. (2011) predict stock market prices based on Twitter data. They find evidence that more complex emotion measurements a"
D19-5103,W16-0423,1,0.8057,"niversal basic emotions (Ekman, 1992), such as Joy, Anger, and Sadness, is stipulated. Equally popular in psychology though is the dimensional approach which represents emotions as real-valued vectors, having components such as Valence (pleasure vs. displeasure), Arousal (calmness vs. excitement) and Dominance (being controlled by vs. having control over a social situation; Bradley and Lang 1994). Figure 2 provides an illustration of these dimensions relative to common emotional categories. In this work, we employ the VAD format because of its greater flexibility, following our previous work (Buechel et al., 2016; Händschke et al., 2018). where e(w) is defined as the vector representing the neutral emotion, if w is not covered by the lexicon, and λ denotes some term weighting function. Here, we use absolute term frequency. Given the relatively high average token number per document in our corpus, we adopted a comparably simple lexicon-based approach which models document emotion based on word frequency combined with empirical measurements of lexical1 Although less reliable for individual sentences than neural methods, lexicon-based methods still perform well on longer documents since the larger amount"
D19-5103,D14-1121,0,0.033791,"the neutral emotion, if w is not covered by the lexicon, and λ denotes some term weighting function. Here, we use absolute term frequency. Given the relatively high average token number per document in our corpus, we adopted a comparably simple lexicon-based approach which models document emotion based on word frequency combined with empirical measurements of lexical1 Although less reliable for individual sentences than neural methods, lexicon-based methods still perform well on longer documents since the larger amount of word material improves predictions based on word frequency statistics (Sap et al., 2014). 2 https://github.com/JULIELab/JEmAS 18 For our subsequent time series study, we process the CB corpus (see Section 2) using JE M AS. The result is one three-dimensional VAD value per document. As both an exploratory analysis and sanity check, we center and scale the resulting data and visualize them as scatterplot array (see Figure 3). ECB statements are higher in Valence and Dominance but lower in Arousal than Fed statements (in all cases p &lt; .001; Mann–Whitney U test). As often observed (Warriner et al., 2013), Valence and Dominance have a strong linear correlation (r = .758). Since neithe"
D19-5103,W18-3103,1,0.764695,"s (Ekman, 1992), such as Joy, Anger, and Sadness, is stipulated. Equally popular in psychology though is the dimensional approach which represents emotions as real-valued vectors, having components such as Valence (pleasure vs. displeasure), Arousal (calmness vs. excitement) and Dominance (being controlled by vs. having control over a social situation; Bradley and Lang 1994). Figure 2 provides an illustration of these dimensions relative to common emotional categories. In this work, we employ the VAD format because of its greater flexibility, following our previous work (Buechel et al., 2016; Händschke et al., 2018). where e(w) is defined as the vector representing the neutral emotion, if w is not covered by the lexicon, and λ denotes some term weighting function. Here, we use absolute term frequency. Given the relatively high average token number per document in our corpus, we adopted a comparably simple lexicon-based approach which models document emotion based on word frequency combined with empirical measurements of lexical1 Although less reliable for individual sentences than neural methods, lexicon-based methods still perform well on longer documents since the larger amount of word material improve"
D19-5103,P16-2037,0,0.0246293,"een revised up by 0.1 percentage points for 2019 and has been revised down by 0.2 percentage points for 2020 and by 0.1 percentage points for 2021. The risks surrounding the euro area growth outlook remain tilted to the downside, on account of the prolonged presence of uncertainties, related to geopolitical factors, the rising threat of protectionism and vulnerabilities in emerging markets. In contrast, a growing number of researchers start focusing on more complex and informative representations of affective states, often following distinct psychological research traditions (Yu et al., 2016; Wang et al., 2016; Mohammad, 2018; Buechel and Hahn, 2018b). Figure 1: Excerpt of ECB statement from June 6, 2019. Studies applying NLP methods to various other fields seem to benefit strongly from such additional information. For example, Kim et al. (2017) examine the relationship between literary genres and emotional plot development finding that, in contrast to other, more predictive emotion categories, Joy as a common emotional category is only moderately helpful for genre classification. More closely related to us, Bollen et al. (2011) predict stock market prices based on Twitter data. They find evidence"
D19-5103,W17-2203,0,0.041949,"of the prolonged presence of uncertainties, related to geopolitical factors, the rising threat of protectionism and vulnerabilities in emerging markets. In contrast, a growing number of researchers start focusing on more complex and informative representations of affective states, often following distinct psychological research traditions (Yu et al., 2016; Wang et al., 2016; Mohammad, 2018; Buechel and Hahn, 2018b). Figure 1: Excerpt of ECB statement from June 6, 2019. Studies applying NLP methods to various other fields seem to benefit strongly from such additional information. For example, Kim et al. (2017) examine the relationship between literary genres and emotional plot development finding that, in contrast to other, more predictive emotion categories, Joy as a common emotional category is only moderately helpful for genre classification. More closely related to us, Bollen et al. (2011) predict stock market prices based on Twitter data. They find evidence that more complex emotion measurements allow for more accurate predictions than polarity alone. The present study provides further evidence for this general observation focusing on the well-established emotional dimensions of Valence, Arous"
D19-5103,N16-1066,0,0.0284833,"GDP growth has been revised up by 0.1 percentage points for 2019 and has been revised down by 0.2 percentage points for 2020 and by 0.1 percentage points for 2021. The risks surrounding the euro area growth outlook remain tilted to the downside, on account of the prolonged presence of uncertainties, related to geopolitical factors, the rising threat of protectionism and vulnerabilities in emerging markets. In contrast, a growing number of researchers start focusing on more complex and informative representations of affective states, often following distinct psychological research traditions (Yu et al., 2016; Wang et al., 2016; Mohammad, 2018; Buechel and Hahn, 2018b). Figure 1: Excerpt of ECB statement from June 6, 2019. Studies applying NLP methods to various other fields seem to benefit strongly from such additional information. For example, Kim et al. (2017) examine the relationship between literary genres and emotional plot development finding that, in contrast to other, more predictive emotion categories, Joy as a common emotional category is only moderately helpful for genre classification. More closely related to us, Bollen et al. (2011) predict stock market prices based on Twitter data."
D19-5103,P18-1017,0,0.0319348,".1 percentage points for 2019 and has been revised down by 0.2 percentage points for 2020 and by 0.1 percentage points for 2021. The risks surrounding the euro area growth outlook remain tilted to the downside, on account of the prolonged presence of uncertainties, related to geopolitical factors, the rising threat of protectionism and vulnerabilities in emerging markets. In contrast, a growing number of researchers start focusing on more complex and informative representations of affective states, often following distinct psychological research traditions (Yu et al., 2016; Wang et al., 2016; Mohammad, 2018; Buechel and Hahn, 2018b). Figure 1: Excerpt of ECB statement from June 6, 2019. Studies applying NLP methods to various other fields seem to benefit strongly from such additional information. For example, Kim et al. (2017) examine the relationship between literary genres and emotional plot development finding that, in contrast to other, more predictive emotion categories, Joy as a common emotional category is only moderately helpful for genre classification. More closely related to us, Bollen et al. (2011) predict stock market prices based on Twitter data. They find evidence that more comple"
D19-5103,D15-1071,0,\N,Missing
D19-5103,E17-2092,1,\N,Missing
E17-2092,L16-1413,0,0.0329438,"Missing"
E17-2092,C16-1249,0,0.110325,"ders. An utterance like “Italy defeats France in the World Cup Final” may be completely neutral from the writer’s viewpoint (presumably a professional journalist), but is likely to evoke rather adverse emotions in Italian and French readers (Katz et al., 2007). In this line of work, Tang and Chen (2012) examine the relation between the sentiment of microblog posts and the sentiment of their comments (as a proxy for reader emotion). Liu et al. (2013) model the emotion of a news reader jointly with the emotion of a comment writer using a cotraining approach. This contribution was followed up by Li et al. (2016) who propose a two-view label propagation approach instead. However, to our knowledge, only Mohammad and Turney (2013) investigated the effects of these perspectives on annotation quality, finding differences in interannotator agreement (IAA) relative to the exact phrasing of the annotation task. In a similar vein to the writer-reader distinction, identifying the holder or source of an opinion or sentiment also aims at describing the affective information entailed in a sentence in more detail (Wiebe et al., 2005; Seki et al., 2009). Thus, opinion statements that can directly be attributed to t"
E17-2092,W17-0801,1,0.736602,"Missing"
E17-2092,P13-2091,0,0.0871226,"its sentiment”), there is an increasing interest in a more fine-grained approach where emotion expressed by writers is modeled separately from emotion evoked in readers. An utterance like “Italy defeats France in the World Cup Final” may be completely neutral from the writer’s viewpoint (presumably a professional journalist), but is likely to evoke rather adverse emotions in Italian and French readers (Katz et al., 2007). In this line of work, Tang and Chen (2012) examine the relation between the sentiment of microblog posts and the sentiment of their comments (as a proxy for reader emotion). Liu et al. (2013) model the emotion of a news reader jointly with the emotion of a comment writer using a cotraining approach. This contribution was followed up by Li et al. (2016) who propose a two-view label propagation approach instead. However, to our knowledge, only Mohammad and Turney (2013) investigated the effects of these perspectives on annotation quality, finding differences in interannotator agreement (IAA) relative to the exact phrasing of the annotation task. In a similar vein to the writer-reader distinction, identifying the holder or source of an opinion or sentiment also aims at describing the"
E17-2092,P10-2013,0,0.0203235,") to annotated bi-representationally as well, complewhom most of the VAD resources developed in menting dimensional VAD ratings with annotapsychology refer (see Section 2). We changed the tions according to a categorical emotion model. 9-point SAM scales to 5-point scales (see Figure Following these criteria, we composed our cor2) in order to reduce the cognitive load during depus out of several categories of the Manually cision making for crowdworkers. For the writer’s Annotated Sub-Corpus of the American National perspective, we presented a number of linguisCorpus (M ASC; Ide et al. (2008), Ide et al. (2010)) tic clues supporting the annotators in their rating and the corpus of SemEval-2007 Task 14 Affective decisions, while, for the reader’s perspective, we Text (S E 07; Strapparava and Mihalcea (2007)). asked what emotion would be evoked in an averM ASC is already annotated on various linguistic age reader (rather than asking for the rater’s perlevels. Hence, our work will allow for research sonal feelings). Both adjustments were made to at the intersection of emotion and other language establish more objective criteria for the exclusion phenomena. S E 07, on the other hand, bears annoof untrus"
E17-2092,H05-1073,0,0.677326,"small number of independent emotional dimensions (often two or three): Valence (corresponding to the concept of polarity), Arousal (degree of calmness or excitement), and Dominance2 (perceived degree of control over a situation); the VAD model. Formally, the VAD dimensions span a three-dimensional real-valued vector space as illustrated in Figure 1. Alternatively, categorical models, such as the six Basic Emotions by Ekman (1992) or the Wheel of Emotion by Plutchik (1980), conceptualize emotions as discrete states.3 In contrast to categorical models which were used early on in NLP (Ovesdotter Alm et al., 2005; Strapparava and Mihalcea, 2007), dimensional Introduction In the past years, the analysis of affective language has become one of the most productive and vivid areas in computational linguistics. In the early days, the prediction of the semantic polarity (positiveness or negativeness) was in the center of interest, but in the meantime, research activities shifted towards a more fine-grained modeling of sentiment. This includes the extension from only two to multiple polarity classes or even real-valued scores (Strapparava and Mihalcea, 2007), the aggregation of multiple aspects of an opinion"
E17-2092,W16-0404,0,0.478154,"Missing"
E17-2092,S07-1013,0,0.839543,"ategorical emotion model. 9-point SAM scales to 5-point scales (see Figure Following these criteria, we composed our cor2) in order to reduce the cognitive load during depus out of several categories of the Manually cision making for crowdworkers. For the writer’s Annotated Sub-Corpus of the American National perspective, we presented a number of linguisCorpus (M ASC; Ide et al. (2008), Ide et al. (2010)) tic clues supporting the annotators in their rating and the corpus of SemEval-2007 Task 14 Affective decisions, while, for the reader’s perspective, we Text (S E 07; Strapparava and Mihalcea (2007)). asked what emotion would be evoked in an averM ASC is already annotated on various linguistic age reader (rather than asking for the rater’s perlevels. Hence, our work will allow for research sonal feelings). Both adjustments were made to at the intersection of emotion and other language establish more objective criteria for the exclusion phenomena. S E 07, on the other hand, bears annoof untrustworthy workers. We provide the instructations according to Ekman’s six Basic Emotion tions along with our dataset. (see Section 2) on a [0, 100] scale, respectively. For each sentence, five annotato"
E17-2092,D09-1150,0,0.0235477,"Missing"
E17-2092,W16-0430,0,0.0233703,"of interest, but in the meantime, research activities shifted towards a more fine-grained modeling of sentiment. This includes the extension from only two to multiple polarity classes or even real-valued scores (Strapparava and Mihalcea, 2007), the aggregation of multiple aspects of an opinion item into a composite opinion statement for the whole item (Schouten and Frasincar, 2016), and sentiment compositionality (Socher et al., 2013). Yet, two important features of fine-grained modeling still lack appropriate resources, namely shifting towards psychologically more adequate models of emotion (Strapparava, 2016) and distinguishing between writer’s vs. reader’s perspec1 https://github.com/JULIELab/EmoBank This dimension is sometimes omitted (the VA model). 3 Both dimensional and categorical formats allow for numerical scores regarding their dimensions/categories. 2 578 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 578–585, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ● Anger 0.0 ● ● Joy Surprise Fear Sadness 1.0 0.5 −0.5 Dominance 0.5 ● Arousal 1.0 ● Disgust ● (to our know"
E17-2092,tang-chen-2012-mining,0,0.110628,"basic sentiment analysis systems in NLP to map the many different possible interpretations of a sentence’s affective meaning into a single assessment (“its sentiment”), there is an increasing interest in a more fine-grained approach where emotion expressed by writers is modeled separately from emotion evoked in readers. An utterance like “Italy defeats France in the World Cup Final” may be completely neutral from the writer’s viewpoint (presumably a professional journalist), but is likely to evoke rather adverse emotions in Italian and French readers (Katz et al., 2007). In this line of work, Tang and Chen (2012) examine the relation between the sentiment of microblog posts and the sentiment of their comments (as a proxy for reader emotion). Liu et al. (2013) model the emotion of a news reader jointly with the emotion of a comment writer using a cotraining approach. This contribution was followed up by Li et al. (2016) who propose a two-view label propagation approach instead. However, to our knowledge, only Mohammad and Turney (2013) investigated the effects of these perspectives on annotation quality, finding differences in interannotator agreement (IAA) relative to the exact phrasing of the annotat"
E17-2092,P16-2037,0,0.0189947,"eferences to other’s opinions. A related task, the detection of stance, focuses on inferring the writer’s (dis)approval towards a given issue from a piece of text (Sobhani et al., 2016). 0.0 −1.0 −0.5 −1.0 −1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: The affective space spanned by the three VAD dimensions. As an example, we here include the positions of Ekman’s six Basic Emotions as determined by Russell and Mehrabian (1977). models have only recently received increased attention in tasks such as word and document emotion prediction (see, e.g., Yu et al. (2015), K¨oper and Schulte im Walde (2016), Wang et al. (2016), Buechel and Hahn (2016)). In spite of this shift in modeling focus, VA(D)annotated corpora are surprisingly rare in number and small in size, and also tend to be restricted in reliability. A NET, for instance, comprises only 120 sentences designed for psychological research (Bradley and Lang, 2007), while Preot¸iuc-Pietro et al. (2016) created a corpus of 2,895 English Facebook posts relying on only two annotators. Yu et al. (2016) recently presented a corpus of 2,009 Chinese sentences from various online texts. As far as categorical models for emotion analysis are concerned, many studies us"
E17-2092,S16-2021,0,0.0316421,"erences in interannotator agreement (IAA) relative to the exact phrasing of the annotation task. In a similar vein to the writer-reader distinction, identifying the holder or source of an opinion or sentiment also aims at describing the affective information entailed in a sentence in more detail (Wiebe et al., 2005; Seki et al., 2009). Thus, opinion statements that can directly be attributed to the writer can be distinguished from references to other’s opinions. A related task, the detection of stance, focuses on inferring the writer’s (dis)approval towards a given issue from a piece of text (Sobhani et al., 2016). 0.0 −1.0 −0.5 −1.0 −1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: The affective space spanned by the three VAD dimensions. As an example, we here include the positions of Ekman’s six Basic Emotions as determined by Russell and Mehrabian (1977). models have only recently received increased attention in tasks such as word and document emotion prediction (see, e.g., Yu et al. (2015), K¨oper and Schulte im Walde (2016), Wang et al. (2016), Buechel and Hahn (2016)). In spite of this shift in modeling focus, VA(D)annotated corpora are surprisingly rare in number and small in size, and also tend to be res"
E17-2092,P15-2129,0,0.0610521,"e attributed to the writer can be distinguished from references to other’s opinions. A related task, the detection of stance, focuses on inferring the writer’s (dis)approval towards a given issue from a piece of text (Sobhani et al., 2016). 0.0 −1.0 −0.5 −1.0 −1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: The affective space spanned by the three VAD dimensions. As an example, we here include the positions of Ekman’s six Basic Emotions as determined by Russell and Mehrabian (1977). models have only recently received increased attention in tasks such as word and document emotion prediction (see, e.g., Yu et al. (2015), K¨oper and Schulte im Walde (2016), Wang et al. (2016), Buechel and Hahn (2016)). In spite of this shift in modeling focus, VA(D)annotated corpora are surprisingly rare in number and small in size, and also tend to be restricted in reliability. A NET, for instance, comprises only 120 sentences designed for psychological research (Bradley and Lang, 2007), while Preot¸iuc-Pietro et al. (2016) created a corpus of 2,895 English Facebook posts relying on only two annotators. Yu et al. (2016) recently presented a corpus of 2,009 Chinese sentences from various online texts. As far as categorical mo"
E17-2092,N16-1066,0,0.422126,"ly recently received increased attention in tasks such as word and document emotion prediction (see, e.g., Yu et al. (2015), K¨oper and Schulte im Walde (2016), Wang et al. (2016), Buechel and Hahn (2016)). In spite of this shift in modeling focus, VA(D)annotated corpora are surprisingly rare in number and small in size, and also tend to be restricted in reliability. A NET, for instance, comprises only 120 sentences designed for psychological research (Bradley and Lang, 2007), while Preot¸iuc-Pietro et al. (2016) created a corpus of 2,895 English Facebook posts relying on only two annotators. Yu et al. (2016) recently presented a corpus of 2,009 Chinese sentences from various online texts. As far as categorical models for emotion analysis are concerned, many studies use incompatible subsets of category systems, which limits their comparability (Buechel and Hahn, 2016; Calvo and Mac Kim, 2013). This also reflects the situation in psychology where there is still no consensus on a set of fundamental emotions (Sander and Scherer, 2009). Here, the VAD model has a major advantage: Since the dimensions are designed as being independent, results remain comparable dimension-wise even in the absence of othe"
E17-2092,D13-1170,0,\N,Missing
E95-1033,C94-1061,1,0.895913,"Missing"
E95-1033,C88-1026,0,0.0554151,"dy is presented in Hahn (1992)). 2 DG Constraints on Anaphora In this section, we present, quite informally, some constraints on intra-sentential anaphora in terms of dependency grammar (DG). We will reconsider these constraints in Section 3, where our grammar model is dealt with in more depth. We provide here a definition of d-binding and two constraints which describe the use of reflexive pronouns and anaphors (personal pronouns and definite noun phrases). These constraints cover approximately the same phenomena as the binding theory of GB (Chomsky (1981); for a computational treatment, cf. Correa (1988)). Dependency structures, by definition, refer to the sentence level of linguistic description only. The relation of dependency holds between a lexical head and one or several modifiers of that head, such that the occurrence of a head allows for the occurrence of one or several modifiers (in some 238 pre-specified linear ordering), but not vice versa. Speaking in terms of dependency structure representations, the head always precedes and, thus, (transitively) governs its associated modifiers in the dependency tree. This basic notion of government must be further refined for the description of"
E95-1033,J86-3001,0,0.0173051,"airly general linguistic tasks, such as establishing dependencies, properly arranging coordinations, and, of course, resolving anaphors. Consequently, any of these subprotocols constitutes part of the grammar specification proper. We shall illustrate the linguistic aspects of word actor-based parsing by introducing the basic data structures for text-level anaphora as acquaintances of specific word actors, and then turn to the general message-passing protocol that accounts for intra- as well as inter-sentential anaphora. Our exposition builds on the well-known focusing mechanism (Sidner, 1983; Grosz and Sidner, 1986). Accordingly, we distinguish each sentence's unique focus, a complementary list of alternate potential loci, and a history list composed of discourse elements not in the list of potential loci, but occurring in previous sentences of the current discourse segment. These data structures are realized as acquaintances of sentence delimiters to restrict the search space beyond the sentence to the relevant word actors. The protocol level of analysis encompasses the procedural interpretation of the declarative constraints given in Section 2. At that level, in the case of reflexive pronouns, the sear"
E95-1033,C92-1008,1,0.838158,"with crucial linguistic phenomena which cause considerable problems for current GB theory, 3. goes beyond GB in that it allows the treatment of anaphora at the text level of description within the same grammar formalism as is used for sentence level anaphora, and, 4. goes beyond the anaphora-centered treatment of text structure characteristic of the DRT approach in that it already accounts for the resolution of text-level ellipsis (sometimes also referred to as functional anaphora, cf. Hahn and Strube (1995)) and the interpretation of text macro structures (a preliminary study is presented in Hahn (1992)). 2 DG Constraints on Anaphora In this section, we present, quite informally, some constraints on intra-sentential anaphora in terms of dependency grammar (DG). We will reconsider these constraints in Section 3, where our grammar model is dealt with in more depth. We provide here a definition of d-binding and two constraints which describe the use of reflexive pronouns and anaphors (personal pronouns and definite noun phrases). These constraints cover approximately the same phenomena as the binding theory of GB (Chomsky (1981); for a computational treatment, cf. Correa (1988)). Dependency str"
E95-1033,C92-1023,0,0.148716,"ved (Lappin and Leass, 1994), but still is a source of lots of problems. Third, unlike our approach, even the current SG model for anaphora resolution does not incorporate conceptual knowledge and global discourse structures (for reasons discussed by Lappin and Laess). This decision might nevertheless cause trouble if more conceptually rooted text cohesion and coherence structures have to be accounted for (e.g., textual ellipses). A particular problem we have not yet solved, the plausible ranking of single antecedents from a candidate set, is dealt with in depth by Lappin and Laess (1994) and Hajicova et al. (1992). Both define salience metrics capable of ordering alternative antecedents according to structural criteria, several of which can directly be attributed to the topological structure and topic/comment annotations of the underlying dependency trees. 243 6 Conclusions We have outlined a model of anaphora resolution which is founded on a dependency-based grammar model. This model accounts for sentence-level anaphora, with constraints adapted from GB, as well as text-level anaphora, with concepts close to Grosz-Sidner-style focus models. The associated text parser is based on the actor computation"
E95-1033,P89-1032,0,0.016866,"mong concepts, e.g., (MOTnERBOARD, has-cpu, CPU) E permit. Furthermore, object.attribute denotes the value of the property attribute at object and the symbol self refers to the current lexical item. The ParseTalk specification language, in addition, incorporates topological primitives for relations within dependency trees. The relations left and head denote ""~ occurs left of y"" and ""x is head of y', resp. These primitive relations can be considered declarative equivalents to the procedural specifications used in several tree-walking algorithms for anaphora resolution, e.g., by Hobbs (1978) or Ingria and Stallard (1989). Note that in the description below tel + and rel* denote the transitive and transitive/reflexive closure of a relation rel, respectively. Major G r a m m a t i c a l Predicates The ParseTalk model of DG (Hahn et al., 1994) exploits inheritance as a major abstraction mechanism. The entire lexical system is organized as a hierarchy of lexical classes (isac denoting the subclass relation among lexical classes), with concrete lexical items forming the leave nodes of the corresponding lexicon grammar graph. Valency constraints are attached to each lexical item, on which the local computation of c"
E95-1033,J94-4002,0,0.0937059,"mar (SG), a slight theory variant of DG. In particular, they treat pronominal coreference and anaphora (i.e., reflexives and reciprocals). Our approach methodologically differs in three major aspects from that study: First, unlike the SG proposal, which is based on a second-pass algorithm operating on fully parsed clauses to determine anaphoric relationships, our proposal is basically an incremental single-pass parsing model. Most importan't, however, is that our model incorporates the text-level of anaphora resolution, a shortcoming of the original SG approach that has recently been removed (Lappin and Leass, 1994), but still is a source of lots of problems. Third, unlike our approach, even the current SG model for anaphora resolution does not incorporate conceptual knowledge and global discourse structures (for reasons discussed by Lappin and Laess). This decision might nevertheless cause trouble if more conceptually rooted text cohesion and coherence structures have to be accounted for (e.g., textual ellipses). A particular problem we have not yet solved, the plausible ranking of single antecedents from a candidate set, is dealt with in depth by Lappin and Laess (1994) and Hajicova et al. (1992). Both"
E95-1033,J90-4001,0,0.101558,"eover, conceptual criteria have to be met as in the case of nominal anaphors which must subsume their antecedents at the conceptual level. Similarly, for pronominal anaphors the selected antecedent must be permitted in those conceptual roles connecting the pronominal anaphors and its grammatical head. The DG constraints for the use of reflexives and intra-sentential anaphora cover approximately the same phenomena as GB, but the structures used by DG analysis are less complex than those of GB and do not require the formal machinery of empty categories, binding chains and complex movements (cf. Lappin and McCord (1990, p.205) for a similar argument). Hence, our proposal provides a more tractable basis for implementation. 3 guage. The concept hierarchy consists of a set of concept names ~"" = {COMPUTERSYSTEM, NOTEBOOK, MOTHERBOARD, ...} and a subclass relation isaF = {(NOTEBOOK, COMPUTERSYSTEM), (PCI-MOTHERBOARD, MOTHERBOARD), ...} C x 9r. roles C f"" x 9v is the set of relations with role names ""R = {has-part, has-cpu, ...} and denotes the established relations in the knowledge base, while R characterizes the labels of admitted conceptual relations. The relation permit C 9v x ""R x 9r characterizes the range"
E95-1033,C94-1080,1,0.840754,"horicAntecedentOf (cf. Box 3), determines the candidate set of possible antecedents for (pro)nominal anaphors, and 4 x isPotentialAnaphoricAntecedentOf y :~:~ --,3 z: (z d-binds x A z d-binds y) R e s o l u t i o n of A n a p h o r a NomAnaphorTest (defNP, ante):~ ante isac* Noun A ((defNP.features sel~agr
um) U(ante.featuressel~ag,
um) # _L) ^ ante.concept isaF* defNP.concept The ParseTaik environment builds on the actor computation model (Agha and Hewitt, 1987) as background for the procedural interpretation of lexicalized dependency specifications in terms of so-called word actors (of. Schacht et al. 1994; Hahn et al. 1994). Word actors combine objectoriented features with concurrency yielding strict lexical distribution and distributed computation in a methodologically clean way. The model assumes word actors to communicate via asynchronous message passing. An actor can send messages only to other actors it knows about, its socalled acquaintances. The arrival of a message at an actor is called an event; it triggers the execution of a method that is composed of atomic actions - among them the evaluation of grammatical predicates. As we will show, the specification of a particular message proto"
faessler-etal-2014-disclose,J06-4003,0,\N,Missing
faessler-etal-2014-disclose,petrov-etal-2012-universal,0,\N,Missing
faessler-etal-2014-disclose,wermter-hahn-2004-annotated,1,\N,Missing
H01-1067,J95-2003,0,0.0268778,"a centering mechanism is used. The discourse entities which occur in an utterance Ui constitute its set of forward-looking centers, Cf (Ui ). The elements in Cf (Ui ) are ordered to re ect relative prominence in Ui in the sense that the most highly ranked element of Cf (Ui ) is the most likely antecedent of an anaphoric expression in Ui+1 , while the remaining elements are ordered according to decreasing preference for establishing referential links. While it is usually assumed (for the English language, in particular) that grammatical roles are the major determinant for the ranking on the Cf [4], we claim that for German { a language with relatively free word order { it is the functional information structure of the sentence [27]. Accordingly, the constraints on the ordering of entries in Cf (Ui ) prefer hearer-old (either evoked or unused) elements in an utterance (i.e., those that can be related to previously introduced discourse elements or generally accessible world knowledge) over mediated (inferrable) ones, while these are preferred over hearer-new (brand-new) elements for anaphora resolution. If two elements belong to the same category, then preference is de ned in terms of li"
H01-1067,P97-1014,1,0.758415,"MED), the framework of the medSynDiKATe system [10, 9]. Our rst goal is to extract conceptually and inferentially richer forms of knowledge than those captured by standard IE systems such as evaluative assertions and comparisons [25, 24], temporal [26] and spatial information [22]. Second, we also want to dynamically enhance the set of knowledge templates through incremental taxonomy learning devices [12] so that the information extraction capability of the system is increased in a bootstrapping manner. Third, SynDiKATe is particularly sensitive to the treatment of textual reference relations [27, 6, 14]. The capability to properly deal with various forms of anaphora is a prerequisite for the soundness and validity of the knowledge bases we create as a result of the text understanding process and likewise for the feasibility of sophisticated retrieval and question answering applications based on the acquired text knowledge. 2. SYSTEM ARCHITECTURE The overall architecture of SynDiKATe, an acronym which stands for Synthesis of Distributed Knowledge Acquired from Texts"", is summarized in Figure 1. Incoming texts, Ti , are mapped into corresponding text knowledge bases, TKBi , which contain a re"
H01-1067,A00-2043,1,0.871467,"Missing"
H01-1067,J99-3001,1,0.824987,"MED), the framework of the medSynDiKATe system [10, 9]. Our rst goal is to extract conceptually and inferentially richer forms of knowledge than those captured by standard IE systems such as evaluative assertions and comparisons [25, 24], temporal [26] and spatial information [22]. Second, we also want to dynamically enhance the set of knowledge templates through incremental taxonomy learning devices [12] so that the information extraction capability of the system is increased in a bootstrapping manner. Third, SynDiKATe is particularly sensitive to the treatment of textual reference relations [27, 6, 14]. The capability to properly deal with various forms of anaphora is a prerequisite for the soundness and validity of the knowledge bases we create as a result of the text understanding process and likewise for the feasibility of sophisticated retrieval and question answering applications based on the acquired text knowledge. 2. SYSTEM ARCHITECTURE The overall architecture of SynDiKATe, an acronym which stands for Synthesis of Distributed Knowledge Acquired from Texts"", is summarized in Figure 1. Incoming texts, Ti , are mapped into corresponding text knowledge bases, TKBi , which contain a re"
H05-1106,C04-1087,0,0.0764504,"Missing"
H05-1106,P01-1025,0,0.316558,"ate is the essential building block of any term identification system. For multi-word automatic term recognition (ATR), the C-value approach (Frantzi et al., 2000; Nenadi´c et al., 2004), which aims at improving the extraction of nested terms, has been one of the most widely used techniques in recent years. Other potential association measures are mutual information (Damerau, 1993) and the whole battery of statistical and information-theoretic measures (t-test, loglikelihood, entropy) which are typically employed for the extraction of general-language collocations (Manning and Sch¨utze, 1999; Evert and Krenn, 2001). While these measures have their statistical merits in terminology identification, it is interesting to note that they only make little use of linguistic properties inherent to complex terms. 2 More linguistically oriented work on ATR by Daille (1996) or on term variation by Jacquemin (1999) builds on the deep syntactic analysis of term candidates. This includes morphological and headmodifier dependency analysis and thus presupposes accurate, high-quality parsing which, for sublanguages at least, can only be achieved by a highly domain-dependent type of grammar. As sublanguages from different"
H05-1106,P99-1044,0,0.0721294,"nt years. Other potential association measures are mutual information (Damerau, 1993) and the whole battery of statistical and information-theoretic measures (t-test, loglikelihood, entropy) which are typically employed for the extraction of general-language collocations (Manning and Sch¨utze, 1999; Evert and Krenn, 2001). While these measures have their statistical merits in terminology identification, it is interesting to note that they only make little use of linguistic properties inherent to complex terms. 2 More linguistically oriented work on ATR by Daille (1996) or on term variation by Jacquemin (1999) builds on the deep syntactic analysis of term candidates. This includes morphological and headmodifier dependency analysis and thus presupposes accurate, high-quality parsing which, for sublanguages at least, can only be achieved by a highly domain-dependent type of grammar. As sublanguages from different domains usually reveal a high degree of syntactic variability among each other (e.g., in terms of POS distribution, syntactic patterns), this property makes it difficult to port grammatical specifications to different domains. Therefore, one may wonder whether there are cross-domain linguist"
H05-1106,N01-1025,0,0.0135276,"ements and will elaborate on it in detail in Subsection 3.3. 2 A notable exception is the C-value method which incorporates a term’s likelihood of being nested in other multi-word units. 844 3 Methods and Experiments 3.1 Text Corpus We collected a biomedical training corpus of approximately 513,000 M EDLINE abstracts using the following query composed of M E SH terms from the biomedical domain: transcription factors, blood cells and human.3 We then annotated the resulting 104-million-word corpus with the G ENIA partof-speech tagger4 and identified noun phrases (NPs) with the YAM C HA chunker (Kudo and Matsumoto, 2001). We restrict our study to NP recognition (i.e., determining the extension of a noun phrase but refraining from assigning any internal constituent structure to that phrase), because the vast majority of technical or scientific terms surface as noun phrases (Justeson and Katz, 1995). We filtered out a number of stop words (determiners, pronouns, measure symbols, etc.) and also ignored noun phrases with coordination markers (“and”, “or”, etc.). 5 n-gram length bigrams trigrams quadgrams cut-off no cut-off c ≥ 10 no cut-off c≥8 no cut-off c≥6 NP term candidates tokens types 5,920,018 1,055,820 4,"
H05-1106,W02-1407,0,0.0695374,"Missing"
H05-1106,C04-1141,1,0.514087,"ishing terms from non-terms in the biomedical literature. While mining scientific literature for new terminological units and assembling those in controlled vocabularies is a task involving several components, one essential building block is to measure the degree of termhood of a candidate. In this respect, our study has shown that a criterion which incorporates a vital linguistic property of terms, viz. their limited paradigmatic modifiability, is much more powerful than linguistically more uninformed measures. This is in line with our previous work on generallanguage collocation extraction (Wermter and Hahn, 2004), in which we showed that a linguistically motivated criterion based on the limited syntagmatic modifiability of collocations outperforms alternative standard association measures as well. We also collected evidence that the superiority of the P -M od method relative to other term extraction approaches holds independent of the underlying corpus size (given a reasonable offset). This is a crucial finding because other domains might lack large volumes of free-text material but still provide sufficient corpus sizes for valid term extraction. Finally, since we only require shallow syntactic analys"
H05-1106,W02-0308,0,\N,Missing
hahn-etal-2008-semantic,J93-2004,0,\N,Missing
hahn-etal-2008-semantic,D07-1051,1,\N,Missing
hahn-etal-2008-semantic,C04-1140,1,\N,Missing
hahn-etal-2008-semantic,W07-1028,1,\N,Missing
hahn-etal-2008-semantic,W04-3111,0,\N,Missing
hahn-etal-2008-semantic,J05-1004,0,\N,Missing
hahn-etal-2008-semantic,doddington-etal-2004-automatic,0,\N,Missing
hahn-etal-2008-semantic,W07-1502,1,\N,Missing
hahn-etal-2012-iterative,D07-1051,1,\N,Missing
hahn-etal-2012-iterative,W07-1013,0,\N,Missing
hahn-etal-2012-iterative,ogren-etal-2008-constructing,0,\N,Missing
hahn-etal-2012-iterative,W07-1502,1,\N,Missing
hahn-etal-2012-iterative,W10-1833,0,\N,Missing
hahn-wermter-2004-pumping,C00-2117,0,\N,Missing
hahn-wermter-2004-pumping,C94-2174,0,\N,Missing
hahn-wermter-2004-pumping,P97-1005,0,\N,Missing
hellrich-etal-2014-collaboratively,lewin-etal-2012-centroids,1,\N,Missing
J99-3001,P87-1022,0,0.972084,"Missing"
J99-3001,C88-1021,0,0.0910327,"e comprehensive theory of discourse understanding based on linguistic, attentional, and intentional layers, the centering model can be considered the first principled attempt to deal with preference orders for plausible antecedent selection for anaphors. Its predecessors were entirely heuristic approaches to anaphora resolution. These were concerned with various criteria--beyond strictly grammatical constraints such as agreement--for the optimization of the referent selection process based on preferential choices. An elaborate description of several of these preference criteria is supplied by Carbonell and Brown (1988) who discuss, among others, heuristics involving case role filling, semantic and pragmatic alignment, syntactic parallelism, syntactic topicalization, and intersentential recency. Given such a wealth of criteria one may either try to order them a priori in terms of importance or--as was proposed by the majority of researchers in this field-define several scoring functions that compute flexible orderings on the fly. These combine the variety of available evidence, each one usually annotated by a specific weight factor, and, finally, map the weights to a single salience score (Rich and LuperFoy"
J99-3001,T75-2034,0,0.865862,"Missing"
J99-3001,C90-3063,0,0.0650446,"ap the weights to a single salience score (Rich and LuperFoy 1988; Haji~ovG KuboG and Kubo~ 1992; Lappin and Leass 1994) These heuristics helped to improve the performance of discourse-understanding systems through significant reductions of the available search-space for antecedents. Their major drawback is that they require a great deal of skilled hand-crafting that, unfortunately, usually does not scale in broader application domains. Hence, proposals were made to replace these high-level ""symbolic"" categories by statistically interpreted occurrence patterns derived from large text corpora (Dagan and Itai 1990). Preferences then reflect patterns of statistically significant lexical usage rather than introspective abstractions of linguistic patterns such as syntactic parallelism or pragmatic alignment. Among the heuristic approaches to anaphora resolution, those which consider the identification of heuristics a machine learning (ML) problem are particularly interesting, since their heuristics dynamically adapt to the textual data. Furthermore, ML procedures operate on incomplete parses (hence, they accept noisy data), which dis337 Computational Linguistics Volume 25, Number 3 tinguishes them from the"
J99-3001,C86-1119,0,0.0864974,"Missing"
J99-3001,E99-1006,1,0.82756,"Missing"
J99-3001,P83-1007,0,0.469828,"description level. Computational linguists have recognized the need to account for referential ambiguities in discourse and have developed various theories centered around the notion of discourse focus (Grosz 1977; Sidner 1983). In a seminal paper, Grosz and Sidner (1986) wrapped up the results of their research and formulated a model in which three levels of discourse coherence are distinguished--attention, intention, and discourse segment structure. While this paper gives a comprehensive picture of a complex, yet not explicitly spelled-out theory of discourse coherence, the centering model (Grosz, Joshi, and Weinstein, 1983, 1995) marked a major step in clarifying the relationship between attentional states and (local) discourse segment structure. More precisely, the centering model accounts for the interactions between local coherence and preferential choices of referring expressions. It relates differences in coherence (in part) to varying demands on inferences as required by different types of referring expressions, given a particular attentional state of the hearer in a discourse setting (Grosz, Joshi, and Weinstein 1995, 204-205). The claim is made then that the lower the inference load put on the hearer, t"
J99-3001,J95-2003,0,0.902021,"Missing"
J99-3001,J86-3001,0,0.270633,"Philadelphia, PA 19104, USA t Computational Linguistics Group, Text Understanding Lab, Werthmannplatz 1, 79085 Freiburg, Germany (~) 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 3 terns of language use and, thus, introduces the level of discourse context and further pragmatic factors as a complementary description level. Computational linguists have recognized the need to account for referential ambiguities in discourse and have developed various theories centered around the notion of discourse focus (Grosz 1977; Sidner 1983). In a seminal paper, Grosz and Sidner (1986) wrapped up the results of their research and formulated a model in which three levels of discourse coherence are distinguished--attention, intention, and discourse segment structure. While this paper gives a comprehensive picture of a complex, yet not explicitly spelled-out theory of discourse coherence, the centering model (Grosz, Joshi, and Weinstein, 1983, 1995) marked a major step in clarifying the relationship between attentional states and (local) discourse segment structure. More precisely, the centering model accounts for the interactions between local coherence and preferential choic"
J99-3001,1997.iwpt-1.14,1,0.84595,"Missing"
J99-3001,P97-1014,1,0.845678,"ense, i.e., it allows only the consideration of immediately adjacent centering structures for establishing proper referential links. In order to extend that theory to the level of global coherence, various steps have to be taken. At the referential level, mechanisms have to be introduced to account for reference relationships that extend beyond the immediately preceding utterance. Empirical evidence for such phenomena exists in the literature and we also found the need to have such a mechanism available for longer texts. The extension of functional centering to these phenomena is presented in Hahn and Strube (1997), while Walker (1998) builds upon the centering algorithm described in Brennan, Friedman, and Pollard (1987). At the level of discourse pragmatics, a richer notion than mere reference between terms is needed to account for coherence relations such as those aimed at by Rhetorical Structure Theory 340 Strube and Hahn Functional Centering (Mann and T h o m p s o n 1988). In addition, an explicit relation to basic notions from speech act theory is also missing, t h o u g h it should be considered vital for the global coherence of discourse (Grosz and Sidner 1986). In general, it might become incre"
J99-3001,C92-1023,0,0.037871,"Missing"
J99-3001,P92-1002,0,0.0129328,"al and nominal anaphora and even functional anaphora based on the proposal we have developed in this article. It does not, however, take into account several ""hard"" issues such as plural anaphora, generic definite noun phrases, propositional anaphora, and deictic forms (but see Eckert and Strube [1999] for a treatment of discourse-deictic anaphora in dialogues within a centering-type framework). These shortcomings might be traced back to the fact that the centering model, up to now, did not consider the role of the (main) verb of the utterance under scrutiny. Other cases, such as VP anaphora (Hardt 1992), temporal anaphora (Kameyama, Passonneau, and Poesio 1993; Hitzeman, Moens, and Grover 1995) have already been examined within the centering model. The particular phenomenon of paycheck anaphora is described by Hardt (1996), though he uses only a rather simplified centering model for this work. Other cases are only dealt with in the focusing framework such as propositional anaphora (Dahl and Ball 1990). Evaluations of the centering model have so far only been carried out manually. This is clearly no longer rewarding, so appropriate computational support environments have to be provided. What"
J99-3001,C96-1088,0,0.0137294,"ses, propositional anaphora, and deictic forms (but see Eckert and Strube [1999] for a treatment of discourse-deictic anaphora in dialogues within a centering-type framework). These shortcomings might be traced back to the fact that the centering model, up to now, did not consider the role of the (main) verb of the utterance under scrutiny. Other cases, such as VP anaphora (Hardt 1992), temporal anaphora (Kameyama, Passonneau, and Poesio 1993; Hitzeman, Moens, and Grover 1995) have already been examined within the centering model. The particular phenomenon of paycheck anaphora is described by Hardt (1996), though he uses only a rather simplified centering model for this work. Other cases are only dealt with in the focusing framework such as propositional anaphora (Dahl and Ball 1990). Evaluations of the centering model have so far only been carried out manually. This is clearly no longer rewarding, so appropriate computational support environments have to be provided. What we have in mind is a kind of discourse structure bank and associated workbenches comparable to grammar workbenches and parse treebanks. Aone and Bennett (1994), for example, report on a GUI-based Discourse Tagging Tool (DTT)"
J99-3001,E95-1035,0,0.0202363,"Missing"
J99-3001,C96-1094,0,0.0090448,"s theme and rheme in the sense of the functional sentence perspective (FSP) (Firbas 1974). Viewed from this perspective, the theme/rheme-hierarchy of utterance Ui is determined by the Cf(Ui_l). Elements of Ui that are contained in Cf(Ui-1) are less rhematic than those not contained in Cf(Ui-1). He then concludes that the Cb(Ui) must be the theme of the current utterance. Rambow does not exploit the information structure of utterances to determine the Cf ranking but formulates it on the basis of linear textual precedence among the relevant discourse entities. In order to analyze Turkish texts, Hoffman (1996, 1998) distinguishes between the information structure of utterances and centering, since both constructs are assigned different functions for text understanding. A hearer exploits the information structure of an utterance to update his discourse model, and he applies the centering constraints in order to connect the current utterance to the previous discourse. Hoffman describes the information structure of an utterance in terms of topic (theme) and comment (rheme). The comment is split again into focus and (back)ground (see also Vallduvi [1990] and Vallduvf and Engdahl [1996]). Based on prev"
J99-3001,P86-1031,0,0.0567532,"Missing"
J99-3001,P93-1010,0,0.0837112,"Missing"
J99-3001,C96-1021,0,0.0237166,"theory of preferential anaphora resolution, one should clearly stress the different goals behind heuristics-based systems, such as the ones just discussed, and the model of centering. Heuristic approaches combine introspectively acquired descriptive evidence and attempt to optimize reference resolution performance by proper evidence ""engineering"". This is often done in an admittedly ad hoc way, requiring tricky retuning when new evidence is added (Rich and LuperFoy 1988). On the other hand, many of these systems work in a real-world environment (Rich and LuperFoy 1988; Lappin and Leass 1994; Kennedy and Boguraev 1996) in which noisy data and incomplete, sometimes even faulty, analysis results have to be accounted for. The centering model differs from these considerations in that it aims at unfolding a unified theory of discourse coherence at the linguistic, attentional, and intentional level (Grosz and Sidner 1986); hence, the search for a more principled, theory-based solution, but also the need for (almost) perfect linguistic analyses in terms of parsing and semantic interpretation. 7. C o n c l u s i o n In this paper, we provided a novel account for ordering the forward-looking center list, a major con"
J99-3001,J94-4002,0,0.806272,"s involving case role filling, semantic and pragmatic alignment, syntactic parallelism, syntactic topicalization, and intersentential recency. Given such a wealth of criteria one may either try to order them a priori in terms of importance or--as was proposed by the majority of researchers in this field-define several scoring functions that compute flexible orderings on the fly. These combine the variety of available evidence, each one usually annotated by a specific weight factor, and, finally, map the weights to a single salience score (Rich and LuperFoy 1988; Haji~ovG KuboG and Kubo~ 1992; Lappin and Leass 1994) These heuristics helped to improve the performance of discourse-understanding systems through significant reductions of the available search-space for antecedents. Their major drawback is that they require a great deal of skilled hand-crafting that, unfortunately, usually does not scale in broader application domains. Hence, proposals were made to replace these high-level ""symbolic"" categories by statistically interpreted occurrence patterns derived from large text corpora (Dagan and Itai 1990). Preferences then reflect patterns of statistically significant lexical usage rather than introspec"
J99-3001,A88-1003,0,0.0854674,"l and Brown (1988) who discuss, among others, heuristics involving case role filling, semantic and pragmatic alignment, syntactic parallelism, syntactic topicalization, and intersentential recency. Given such a wealth of criteria one may either try to order them a priori in terms of importance or--as was proposed by the majority of researchers in this field-define several scoring functions that compute flexible orderings on the fly. These combine the variety of available evidence, each one usually annotated by a specific weight factor, and, finally, map the weights to a single salience score (Rich and LuperFoy 1988; Haji~ovG KuboG and Kubo~ 1992; Lappin and Leass 1994) These heuristics helped to improve the performance of discourse-understanding systems through significant reductions of the available search-space for antecedents. Their major drawback is that they require a great deal of skilled hand-crafting that, unfortunately, usually does not scale in broader application domains. Hence, proposals were made to replace these high-level ""symbolic"" categories by statistically interpreted occurrence patterns derived from large text corpora (Dagan and Itai 1990). Preferences then reflect patterns of statis"
J99-3001,P96-1057,1,0.647789,"appropriate for different languages. In fact, Walker, Iida, and Cote (1994) hypothesize that the Cf ranking criteria are the only language-dependent factors within the centering model. Though evidence for many additional criteria for the Cf ranking have been brought forward in the literature, to some extent consensus has emerged that grammatical roles play a major role in making ranking decisions (e.g., whether the referential expression appears as the grammatical subject, direct object, or indirect object of an utterance). Our own work on the centering model 1 (Strube and Hahn 1996; Hahn and Strube 1996) brings in evidence from German, a free-word-order language in which grammatical role information is far less predictive of the organization of centers than for fixed-word-order languages such as English. In establishing proper referential relations, we found the functional information structure of the utterances to be much more relevant. By this we mean indicators of whether or not a discourse entity in the current utterance refers to another discourse entity already introduced by previous utterances in the discourse. Borrowing terminology from Prince (1981, 1992), an entity that does refer t"
J99-3001,P98-2204,1,0.91819,"in the fact that nominal anaphors are far more constrained by conceptual criteria than pronominal ones. Thus, the chance of properly resolving a nominal anaphor, even when ranked at a lower position in the center lists, is greater than for pronominal anaphors. By shifting our evaluation criteria away from resolution success data to structural conditions reflecting the proper ordering of center lists (in particular, we focus on the most highly ranked item of the forward-looking centers), these criteria are intended to compensate for the a significant improvement in the results, is proposed in Strube (1998). 21 Mtiller, Heiner. 1974. Geschichten aus der Produktion 2. Rotbuch Verlag, Berlin. (""Liebesgeschichte,"" pages 57~2.) 330 Strube and Hahn Functional Centering Table 18 Quantitative distribution of centering transitions. Grammatical Grammatical & F A ante &gt; FA FunC 167 158 41 23 102 226 24 37 197 131 35 26 309 25 51 4 17 42 9 7 28 32 9 6 37 28 7 3 43 23 8 1 50 12 13 0 CONTINUE RETAIN SMOOTH-SHIFT ROUGH-SHIFT 31 19 15 14 31 19 17 12 32 18 15 14 32 18 16 13 36 15 18 10 CONTINUE RETAIN SMOOTH-SHIFT ROUGH-SHIFT 97 330 56 60 226 209 67 41 171 272 46 54 272 172 59 40 395 52 82 14 Transition Types N"
J99-3001,E95-1033,1,0.919915,"ketchily dealt with in the centering literature, e.g., by asserting that the entity in question ""is realized but not directly realized"" (Grosz, Joshi, and Weinstein 1995, 217). Furthermore, the distinction between these two kinds of realization is not part of the centering mechanisms but delegated to the underlying semantic theory. We will develop arguments for how to discern inferable discourse entities and relate them properly to their antecedent at the center level. The ordering constraints we supply account for all of the types of anaphora mentioned above, including (pro)nominal anaphora (Strube and Hahn 1995; Hahn and Strube 1996). This claim will be validated by a substantial body of empirical data in Section 5. Our third contribution relates to the w a y the results of centering-based anaphora resolution are usually evaluated. Basically, we argue that rather than counting resolution rates for anaphora or comparing isolated transition types holding among head positions in the center lists--preferred transition types stand for a high degree of local coherence, while less preferred ones signal that the underlying discourse might lack coherence--one should consider adjacent transition pairs and ann"
J99-3001,P96-1036,1,0.927266,"ers about the ranking criteria appropriate for different languages. In fact, Walker, Iida, and Cote (1994) hypothesize that the Cf ranking criteria are the only language-dependent factors within the centering model. Though evidence for many additional criteria for the Cf ranking have been brought forward in the literature, to some extent consensus has emerged that grammatical roles play a major role in making ranking decisions (e.g., whether the referential expression appears as the grammatical subject, direct object, or indirect object of an utterance). Our own work on the centering model 1 (Strube and Hahn 1996; Hahn and Strube 1996) brings in evidence from German, a free-word-order language in which grammatical role information is far less predictive of the organization of centers than for fixed-word-order languages such as English. In establishing proper referential relations, we found the functional information structure of the utterances to be much more relevant. By this we mean indicators of whether or not a discourse entity in the current utterance refers to another discourse entity already introduced by previous utterances in the discourse. Borrowing terminology from Prince (1981, 1992), an e"
J99-3001,J94-2006,0,0.572774,"tatus of discourse entities to determine the current discourse focus. However, a 336 Strube and Hahn Functional Centering common area of criticism of these approaches is the diversity of data structures they require. These data structures are likely to hide the underlying linguistic regularities, because they promote the mix of preference and data structure considerations in the focusing algorithms. As an example, Sidner (1983, 292ff.) distinguishes between an Actor Focus and a Discourse Focus, as well as corresponding lists, viz. Potential Actor Focus List and Potential Discourse Focus List. Suri and McCoy (1994) in their RAFT/RAPR approach use grammatical roles for ordering the focus lists and make a distinction between Subject Focus, Current Focus, and corresponding lists. Both focusing algorithms prefer an element that represents the Focus to the elements in the list when the anaphoric expression under consideration is not the agent (for Sidner) or the subject (for Suri and McCoy). Relating these approaches to our proposal, they already exhibit a weak preference for a single hearer-old (more precisely, evoked) discourse element. Dahl and Ball (1990), describing the anaphora resolution module of the"
J99-3001,P89-1031,0,0.226704,"st set for success rate evaluation. Hemingway NYT English 274 153 2785 302 233 4546 576 386 7331 3rd pers. & poss. pron. sentences words Writers FAZ 299 186 3195 320 394 8005 German 619 580 11200 5.1.2 Method. The evaluation was carried out manually by the authors, supported by a small-scale discourse annotation tool. We used the following guidelines for our evaluation: We did not assume any world knowledge as part of the anaphora resolution process. Only agreement criteria and sortal constraints were applied. We did not account for false positives and error chains, but marked the latter (see Walker 1989). We use Kameyama&apos;s (1998) specifications for dealing with complex sentences (for a description, see Section 3). Following Walker (1989), a discourse segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun whose syntactic features do not match the syntactic features of any of the preceding sentence-internal noun phrases. Also, at the beginning of a segment, anaphora resolution is preferentially performed within the same utterance. According to the preference for intersentential candidates in the original centering model, we defined the followi"
J99-3001,J94-2003,0,0.325741,"Missing"
J99-3001,J90-3001,0,\N,Missing
J99-3001,C98-2199,1,\N,Missing
kafkas-etal-2012-calbc,W04-1213,0,\N,Missing
kafkas-etal-2012-calbc,lewin-etal-2012-centroids,1,\N,Missing
kafkas-etal-2012-calbc,rebholz-schuhmann-etal-2010-calbc,1,\N,Missing
L16-1397,J96-1002,0,0.0260704,"otations Wrapper for UIMA’s XMI writer, with some additional options Consumer which generates stand-off I E X ML files as used in the M ANTRA Challenge (https://sites.google.com/site/mantraeu/clef-er-challenge) (Hellrich et al., 2014) Table 1: Overview of the JC O R E 2.0 Component Repository the actual text processing. JC O R E 2.0 contains at the time of this writing 15 AEs which deal with either morphosyntactic or semantic processing. Morpho-Syntactic Processing. Token and sentence segmentation is taken care of by a wrapper for the O PEN NLP tool suite based on Maximum Entropy (ME) models (Berger et al., 1996) and a self developed tool based on Conditional Random Fields (CRF) (Lafferty et al., 2001). In order to deal with morphological variation of words, we provide the Stanford Lemmatizer. This component takes at least tokenized and POS-tagged text and returns a dictionary form of each word. To provide the aforementioned part-of-speech (POS) tags, we supply a wrapper for O PEN NLP’s POS tagger and a self developed component (Hellrich et al., 2015). The tool set for syntactic analysis features a phrase chunker, a constituency parser (both wrappers for O PEN NLP) and a slightly modified version of t"
L16-1397,doddington-etal-2004-automatic,0,0.139864,"Missing"
L16-1397,hellrich-etal-2014-collaboratively,1,0.863299,"Missing"
L16-1397,W09-1401,0,0.11538,"Missing"
L16-1397,H05-1066,0,0.0214825,"Missing"
L16-1397,M98-1001,0,\N,Missing
L16-1404,W13-1617,0,0.0128233,"age use patterns. One of these rare examples is the study by Schwartz et al. (2013) who investigate the language in social media for the purpose of identifying personality traits of subjects by exploring their wording in a sample of 14.3M FACEBOOK messages using standard personality questionnaires, including reports for age and gender, for approximately 75,000 volunteers. 2. Related Work There is currently an enormous interest in the automatic analysis of computer-mediated natural language communication, such as blogs, chats or tweets, most notably for applications such as sentiment analysis (Balahur, 2013), opinion mining (Sokolova and Lapalme, 2011), forensic linguistics, with focus on authorship identification (MacLeod and Grant, 2012), and cyber intelligence (Iqbal et al., 2012). Besides the rapid development of dedicated text analytics software, numerous efforts have been undertaken in building up corpus resources backing up the evaluation and, if needed, training and development of this type of software. Whereas an abundance of (even annotated) linguistic data have been assembled from blogs, chats or tweets (cf., e.g., Saur´ı et al. (2014), Song et al. (2014), Scheffler (2014), Uthus and A"
L16-1404,burchardt-etal-2006-salsa,0,0.0183722,"Missing"
L16-1404,U07-1006,0,0.0329716,"ml 2 http://www.w3c.org Corpus (Lampert, 2009) which emerged as the result of a nationally spread request for submitting e-mails (E-Mail Australia) based on an 8-category content scheme; it comprises 10,000 e-mails.3 Within T REC 2006, a bilingual e-mail corpus was assembled by Web crawlers for a spam/non-spam classification task for the English (roughly 99,000 documents) and the Chinese language (almost 65,000 documents) (Cormack, 2006). Another bilingual corpus collected for the TAT author profiling system contains about 9,800 and 8,000 documents of English and Arabic e-mails, respectively (Estival et al., 2007). Two Chinese e-mail corpora (amounting to 10,000 and 74,300 sentences) are accessible from the KingLine Data Center4 (last accessed on March 10, 2016). For the German language, two e-mail corpora have been developed, up until now. The largest one, the F LAG corpus, contains approximately 120,000 sentences from the Internet Usenet Newsgroup (Becker et al., 2003), whereas Declerck and Klein (1997) earlier assembled a small-sized e-mail corpus (C OSMA) made of 160 e-mails. C OD E A LLTAG not only differs in size by orders of magnitude from both corpora but also in qualitative terms—its broader s"
L16-1404,kupietz-etal-2010-german,0,0.033494,"c linguistics, with focus on authorship identification (MacLeod and Grant, 2012), and cyber intelligence (Iqbal et al., 2012). Besides the rapid development of dedicated text analytics software, numerous efforts have been undertaken in building up corpus resources backing up the evaluation and, if needed, training and development of this type of software. Whereas an abundance of (even annotated) linguistic data have been assembled from blogs, chats or tweets (cf., e.g., Saur´ı et al. (2014), Song et al. (2014), Scheffler (2014), Uthus and Aha (2013), Beißwenger et al. (2013), Pak and Paroubek (2010)), e-mail corpora are still quite rare. Perhaps the most famous and, so far, largest among those few e-mail text corpora is the E NRON corpus (Klimt and Yang, 2004), which contains almost 620,000 Englishlanguage messages with more than 30,000 threads; it became publically available as a consequence of US court decisions. Second in size to the E NRON corpus, the W3C corpus1 was generated by Web crawlers from mailing lists and public Web pages from the World Wide Web Consortium (W3C)2 and is composed of 200,000 documents with more than 50,000 threads. For T REC 2007, another large-sized corpus c"
L16-1404,W10-0712,0,0.0652569,"Missing"
L16-1404,J93-2004,0,0.0560736,"ch as part-of-speech tags, lemma information and even parsing data (cf., e.g., Andersen et al. (2008)). The second stream of work is dominated by computational concerns in terms of (meta)language banking. NLP researchers took mainly newspaper/newswire material as a basis for in-depth annotation efforts, on a much smaller scale than the national reference corpora though, and built up value-added resources containing, e.g., extensive syntactic or semantic metadata associated with linguistic raw data. This approach is most prominently featured in the seminal work on the English P ENN T REE BANK (Marcus et al., 1993) and P ROP BANK (Palmer et al., 2005), respectively. For German, there are currently two major competing syntactic annotation efforts, namely the T IGER (Brants ¨ -D/Z Treebank (Telljohann et et al., 2004) and the T UBA al., 2004), whereas propositional information is primarily featured in the S ALSA corpus which builds on the syntactic annotations of T IGER (Burchardt et al., 2006). In basically all these efforts related to national reference corpora or metadata banks, for German and other languages, a preference for documents from skilled professional, educated writers can be observed since"
L16-1404,medlock-2006-introduction,0,0.0779208,"Missing"
L16-1404,pak-paroubek-2010-twitter,0,0.0278189,"e, 2011), forensic linguistics, with focus on authorship identification (MacLeod and Grant, 2012), and cyber intelligence (Iqbal et al., 2012). Besides the rapid development of dedicated text analytics software, numerous efforts have been undertaken in building up corpus resources backing up the evaluation and, if needed, training and development of this type of software. Whereas an abundance of (even annotated) linguistic data have been assembled from blogs, chats or tweets (cf., e.g., Saur´ı et al. (2014), Song et al. (2014), Scheffler (2014), Uthus and Aha (2013), Beißwenger et al. (2013), Pak and Paroubek (2010)), e-mail corpora are still quite rare. Perhaps the most famous and, so far, largest among those few e-mail text corpora is the E NRON corpus (Klimt and Yang, 2004), which contains almost 620,000 Englishlanguage messages with more than 30,000 threads; it became publically available as a consequence of US court decisions. Second in size to the E NRON corpus, the W3C corpus1 was generated by Web crawlers from mailing lists and public Web pages from the World Wide Web Consortium (W3C)2 and is composed of 200,000 documents with more than 50,000 threads. For T REC 2007, another large-sized corpus c"
L16-1404,J05-1004,0,0.00833207,"rmation and even parsing data (cf., e.g., Andersen et al. (2008)). The second stream of work is dominated by computational concerns in terms of (meta)language banking. NLP researchers took mainly newspaper/newswire material as a basis for in-depth annotation efforts, on a much smaller scale than the national reference corpora though, and built up value-added resources containing, e.g., extensive syntactic or semantic metadata associated with linguistic raw data. This approach is most prominently featured in the seminal work on the English P ENN T REE BANK (Marcus et al., 1993) and P ROP BANK (Palmer et al., 2005), respectively. For German, there are currently two major competing syntactic annotation efforts, namely the T IGER (Brants ¨ -D/Z Treebank (Telljohann et et al., 2004) and the T UBA al., 2004), whereas propositional information is primarily featured in the S ALSA corpus which builds on the syntactic annotations of T IGER (Burchardt et al., 2006). In basically all these efforts related to national reference corpora or metadata banks, for German and other languages, a preference for documents from skilled professional, educated writers can be observed since newspaper/ journal articles, books (i"
L16-1404,sauri-etal-2014-newsome,0,0.0221835,"Missing"
L16-1404,scheffler-2014-german,0,0.0246765,"ment analysis (Balahur, 2013), opinion mining (Sokolova and Lapalme, 2011), forensic linguistics, with focus on authorship identification (MacLeod and Grant, 2012), and cyber intelligence (Iqbal et al., 2012). Besides the rapid development of dedicated text analytics software, numerous efforts have been undertaken in building up corpus resources backing up the evaluation and, if needed, training and development of this type of software. Whereas an abundance of (even annotated) linguistic data have been assembled from blogs, chats or tweets (cf., e.g., Saur´ı et al. (2014), Song et al. (2014), Scheffler (2014), Uthus and Aha (2013), Beißwenger et al. (2013), Pak and Paroubek (2010)), e-mail corpora are still quite rare. Perhaps the most famous and, so far, largest among those few e-mail text corpora is the E NRON corpus (Klimt and Yang, 2004), which contains almost 620,000 Englishlanguage messages with more than 30,000 threads; it became publically available as a consequence of US court decisions. Second in size to the E NRON corpus, the W3C corpus1 was generated by Web crawlers from mailing lists and public Web pages from the World Wide Web Consortium (W3C)2 and is composed of 200,000 documents wi"
L16-1404,song-etal-2014-collecting,0,0.0192819,"ations such as sentiment analysis (Balahur, 2013), opinion mining (Sokolova and Lapalme, 2011), forensic linguistics, with focus on authorship identification (MacLeod and Grant, 2012), and cyber intelligence (Iqbal et al., 2012). Besides the rapid development of dedicated text analytics software, numerous efforts have been undertaken in building up corpus resources backing up the evaluation and, if needed, training and development of this type of software. Whereas an abundance of (even annotated) linguistic data have been assembled from blogs, chats or tweets (cf., e.g., Saur´ı et al. (2014), Song et al. (2014), Scheffler (2014), Uthus and Aha (2013), Beißwenger et al. (2013), Pak and Paroubek (2010)), e-mail corpora are still quite rare. Perhaps the most famous and, so far, largest among those few e-mail text corpora is the E NRON corpus (Klimt and Yang, 2004), which contains almost 620,000 Englishlanguage messages with more than 30,000 threads; it became publically available as a consequence of US court decisions. Second in size to the E NRON corpus, the W3C corpus1 was generated by Web crawlers from mailing lists and public Web pages from the World Wide Web Consortium (W3C)2 and is composed of 20"
L16-1404,telljohann-etal-2004-tuba,0,0.0542911,"Missing"
L18-1028,E17-2092,1,0.936792,"le in a specific research setting, on the flip-side, this proliferation of competing formats may seriously hamper progress in sentiment analysis for two reasons, at least. First, language resources are less reusable (if at all) as gold standards and, second, with the growing number of representation formats meaningful comparisons between predictive systems become harder (if not impossible). One way to resolve this dilemma is to develop techniques to automatically translate between such formats. This task of emotion representation mapping (E MO M AP) was introduced only very recently to NLP by Buechel and Hahn (2017b). Their work came up with an emotion-labeled corpus which, in part, is annotated with two different emotion formats both being highly predictive for each other. In a follow-up study, Buechel and Hahn (2017a) examined the potential of E MO M AP as a substitute for manual annotation, yet their comparison was restricted to only two emotion lexicons. Comparable work has (to the best of our knowledge) only been done in psychology. However, this stream of work does not target the goal of predictive modeling (Stevenson et al., 2007; Pinheiro et al., 2017). In NLP, a task related to E MO M AP is emo"
L18-1028,P97-1023,0,0.328241,"ly diverse languages and find evidence that our approach produces (near-)gold quality emotion lexicons, even in crosslingual settings. Finally, we use our models to create new lexicons for eight typologically diverse languages. Keywords: Automatic Construction of Emotion Lexicons, Representation Mapping, Models of Emotion 1. Introduction In the past two decades, the NLP-based analysis and prediction of affective states, as performed by sentiment analysis systems, has received enormous interest (Liu, 2015). Starting with simple positive-negative polarity distinctions on the word or text level (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002), research in sentiment analysis has since then shifted towards more nuanced and challenging tasks, e.g., sentiment compositionality (Socher et al., 2013), aspect-level assessments (Schouten and Frasincar, 2016) or stance detection (Sobhani et al., 2016). In parallel, psychologically more advanced and more expressive representation formats for affective states have been proposed, like Basic Emotions (Ekman, 1992) or Valence-ArousalDominance (Bradley and Lang, 1994). However, there is currently no consensus in the literature what scheme should be used as a common ground. Rat"
L18-1028,C16-1249,0,0.0163718,"t-level assessments (Schouten and Frasincar, 2016) or stance detection (Sobhani et al., 2016). In parallel, psychologically more advanced and more expressive representation formats for affective states have been proposed, like Basic Emotions (Ekman, 1992) or Valence-ArousalDominance (Bradley and Lang, 1994). However, there is currently no consensus in the literature what scheme should be used as a common ground. Rather, there are a multitude of competing formats often motivated by the needs of concrete applications or the availability of user-labeled social media data (Desmet and Hoste, 2013; Li et al., 2016). While such decisions for a specific format may be perfectly reasonable in a specific research setting, on the flip-side, this proliferation of competing formats may seriously hamper progress in sentiment analysis for two reasons, at least. First, language resources are less reusable (if at all) as gold standards and, second, with the growing number of representation formats meaningful comparisons between predictive systems become harder (if not impossible). One way to resolve this dilemma is to develop techniques to automatically translate between such formats. This task of emotion represent"
L18-1028,L16-1458,0,0.144056,"Missing"
L18-1028,W02-1011,0,0.0266593,"ce that our approach produces (near-)gold quality emotion lexicons, even in crosslingual settings. Finally, we use our models to create new lexicons for eight typologically diverse languages. Keywords: Automatic Construction of Emotion Lexicons, Representation Mapping, Models of Emotion 1. Introduction In the past two decades, the NLP-based analysis and prediction of affective states, as performed by sentiment analysis systems, has received enormous interest (Liu, 2015). Starting with simple positive-negative polarity distinctions on the word or text level (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002), research in sentiment analysis has since then shifted towards more nuanced and challenging tasks, e.g., sentiment compositionality (Socher et al., 2013), aspect-level assessments (Schouten and Frasincar, 2016) or stance detection (Sobhani et al., 2016). In parallel, psychologically more advanced and more expressive representation formats for affective states have been proposed, like Basic Emotions (Ekman, 1992) or Valence-ArousalDominance (Bradley and Lang, 1994). However, there is currently no consensus in the literature what scheme should be used as a common ground. Rather, there are a mul"
L18-1028,E17-2090,0,0.0634103,"Missing"
L18-1028,S16-2021,0,0.0243892,"sentation Mapping, Models of Emotion 1. Introduction In the past two decades, the NLP-based analysis and prediction of affective states, as performed by sentiment analysis systems, has received enormous interest (Liu, 2015). Starting with simple positive-negative polarity distinctions on the word or text level (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002), research in sentiment analysis has since then shifted towards more nuanced and challenging tasks, e.g., sentiment compositionality (Socher et al., 2013), aspect-level assessments (Schouten and Frasincar, 2016) or stance detection (Sobhani et al., 2016). In parallel, psychologically more advanced and more expressive representation formats for affective states have been proposed, like Basic Emotions (Ekman, 1992) or Valence-ArousalDominance (Bradley and Lang, 1994). However, there is currently no consensus in the literature what scheme should be used as a common ground. Rather, there are a multitude of competing formats often motivated by the needs of concrete applications or the availability of user-labeled social media data (Desmet and Hoste, 2013; Li et al., 2016). While such decisions for a specific format may be perfectly reasonable in a"
L18-1028,D13-1170,0,0.00281244,"eight typologically diverse languages. Keywords: Automatic Construction of Emotion Lexicons, Representation Mapping, Models of Emotion 1. Introduction In the past two decades, the NLP-based analysis and prediction of affective states, as performed by sentiment analysis systems, has received enormous interest (Liu, 2015). Starting with simple positive-negative polarity distinctions on the word or text level (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002), research in sentiment analysis has since then shifted towards more nuanced and challenging tasks, e.g., sentiment compositionality (Socher et al., 2013), aspect-level assessments (Schouten and Frasincar, 2016) or stance detection (Sobhani et al., 2016). In parallel, psychologically more advanced and more expressive representation formats for affective states have been proposed, like Basic Emotions (Ekman, 1992) or Valence-ArousalDominance (Bradley and Lang, 1994). However, there is currently no consensus in the literature what scheme should be used as a common ground. Rather, there are a multitude of competing formats often motivated by the needs of concrete applications or the availability of user-labeled social media data (Desmet and Hoste,"
L18-1028,N16-1066,0,0.242033,"Missing"
L18-1028,S07-1013,0,\N,Missing
L18-1201,W16-5113,0,0.47316,"allows for the sharing of corpora among scientific partners despite copyright protection, potentially mitigating some of the problems addressed in this contribution (at least for researchers located in Germany). For further information see UrhWissG (2018). 2 http://www.trec-cds.org/ 3 https://www.i2b2.org/NLP/DataSets/ 4 https://physionet.org/mimic2/ 1260 Corpus Wermter and Hahn (2004) (F RA M ED) Fette et al. (2012) Bretschneider et al. (2013b) Bretschneider et al. (2013a) Toepfer et al. (2015) Lohr and Herms (2016) Kreuzthaler and Schulz (2015) Kreuzthaler et al. (2016) Roller et al. (2016) Cotik et al. (2016) Krebs et al. (2017) Hahn et al. (2018) (3000PA) Documents Sentences Types Tokens Available – 544 6,494 – 20,729 – 100,150 – 7 7 174 4,295 3,979 28,009 7 140 450 – 22,427 – 11,008 – 266,390 7 7 1,696 – – – 7 1,725 183 3,000 3,000 27,939 2,234 – – – – – – 158,171 12,895 – – 7 7 7 7 867 24,895 32,108 312,784 3 JS YN CC (this work) Table 1: Overview of existing corpora of German clinical language. Highest value per column in bold. Another source of medical language resources in Europe derives from the “C LEF eHealth” initiative.5 Established in 2013, this series of health-related challenges led t"
L18-1201,deleger-etal-2014-annotation,0,0.0435885,"Missing"
L18-1201,faessler-etal-2014-disclose,1,0.842143,"o create in-project clinical corpora. In Table 1 we list, to the best of our knowledge, all existing German-language clinical corpora that have been described in scientific publications. Wermter and Hahn (2004) created F RA M ED, the first German-language medical corpus ever published. It consists of a mixture of approximately 300 clinical reports, textbooks and consumer texts annotated with low-level linguistic metadata (up to the level of parts of speech). F RA M ED was further used to generate in-domain machine learning models for different tasks, e.g., sentence splitting and tokenization (Faessler et al., 2014; Hellrich et al., 2015; Hahn et al., 2016). F RA M ED has also become part of the multilingual extension of N EG E X, a corpus annotated for negation expressions (Chapman et al., 2013). Bretschneider et al. (2013b) and Bretschneider et al. (2013a) introduce a corpus composed of German radiology 5 https://sites.google.com/site/clefehealth/ reports and expand clinical lexical resources with information about pathology classification. Fette et al. (2012) assembled a corpus composed of 544 clinical reports from various medical domains (echocardiography, EEG, lung function, X-ray thorax, bicycle s"
L18-1201,W16-4213,0,0.149887,"raction (IE) task. Toepfer et al. (2015) describe a corpus made of 140 transthoracic echocardiography reports for their IE experiments. Lohr and Herms (2016) collected 450 surgery reports and used these resources to build language models adapted to metadata from two German medical thesauri. A collection of almost 1,700 de-identified clinical in- and outpatient discharge summaries were assembled from a dermatology department for an unsupervised abbreviation detection procedure (Kreuzthaler and Schulz, 2015) and supervised machine learning using an SVM for abbreviation and sentence delineation (Kreuzthaler et al., 2016). Roller et al. (2016) introduce an annotation scheme for a German corpus in the nephrology domain and a similar scheme focusing on negation phenomena is presented by Cotik et al. (2016). The latter two publications use discharge summaries and clinical notes as their document base. In the most recent publications, Krebs et al. (2017) describe a corpus of 3,000 chest X-ray reports used for term extraction (in an effort to improve IE) and Hahn et al. (2018) present 3000PA, a collection of 3,000 German discharge summaries from three different German university hospitals, currently annotated with"
L18-1201,W11-0211,0,0.0310227,"s on the English language, there are plenty of resources available (although not as abundant as in other non-medical fields such as newspapers or social media). For the non-English language communities, however, less comfortable conditions apply. Only very few EU countries follow the DUA policy, such as reported for a clinical adverse drug reaction corpus for Spanish (Oronoz et al., 2015) or a comprehensive Dutch clinical corpus (Afzal et al., 2014). Some labs working on non-English languages have announced plans for releasing their resources, e.g., for French (Del´eger et al., 2014), Polish (Marciniak and Mykowiecka, 2011) or Swedish (Dalianis et al., 2009). Apparently, these plans have not yet been fully realized as, to the best of our knowledge, none of these corpora is currently DUA-available for the research community. legislation cultures. However, for the specific case of the German legal system, very recently an interesting amendment to the national copyright law (“Urheberrechtsgesetz”) has been installed by German authorities. Under certain conditions, this amendment allows for the sharing of corpora among scientific partners despite copyright protection, potentially mitigating some of the problems addr"
L18-1201,W16-4210,0,0.189442,"., 2015; Zhang et al., 2015). However, clinical language poses several domain-specific and rather tough challenges for NLP tools. Not only is the vocabulary abundant and highly specialized, but the language further deviates from standard usage in terms of spelling, typography and syntax: including, for example, short sentence fragments with paragrammatical structure, lack of punctuation, great volume and high degree of ambiguity of abbreviations, non-standard alphanumerical expressions, table-structured passages and mixed language use, including Greek and Latin forms (cf., e.g., Savkov et al. (2016)). Thus, adapting existing NLP tools to the clinical domain is arguably much more difficult than for many other domains and text genres. Furthermore, there is ample evidence that simply reusing standard NLP software trained on general language data (e.g., newspapers) results in severe losses of performance for biomedical applications (Tomanek et al., 2007; Ferraro et al., 2013; Hellrich et al., 2015). The solution we are proposing here bypasses the two major obstacles for clinical NLP—IPRs and data privacy—in the following way. First, as discussed above, without any major legal changes, real,"
L18-1201,W17-1610,0,0.0465391,"Missing"
L18-1201,truyens-van-eecke-2014-legal,0,0.0694503,"Missing"
N18-1173,P17-1067,0,0.0355173,"et-up was published by Li et al. (2017). They propose ridge regression, again using word embeddings as features. Even with this simple approach, they report to outperform many of the above methods in the VAD prediction task.6 Sentence-Level and Text-Level Prediction. Different from the word-level prediction task (the one we focus on in this contribution), the determination of emotion values for higher-level linguistic units (especially sentences and texts) is also heavily investigated. For this problem, DL approaches are meanwhile fully established as the method of choice (Wang et al., 2016b; Abdul-Mageed and Ungar, 2017; Felbo et al., 2017; Mohammad and Bravo-Marquez, 2017). 5 Personal correspondence with William L. Hamilton; See also README at https://github.com/ williamleif/socialsent 6 However, they also report extremely weak performance figures for some of their reference methods. It is important to note, however, that the methods discussed for these higher-level units cannot easily be transferred to solve the word emotion induction problem. Sentence-level and text-level architectures are either adapted to sequential input data (typical for RNN, LSTM, GRNN and related architectures) or spatially arranged"
N18-1173,S15-2102,0,0.0474061,"Missing"
N18-1173,Q17-1010,0,0.0509305,"raphic source, identifier, language they refer to, emotion representation format, and number of lexical entries they contain). Word sunshine terrorism orgasm Valence 8.1 1.6 8.0 Arousal 5.3 7.4 7.2 Dominance 5.4 2.7 5.8 Table 2: Three sample entries from Warriner et al. (2013). They use 9-point scales ranging from 1 (most negative/calm/submissive) to 9 (most positive/excited/dominant). WORD 2 VEC (with its variants S GNS and C BOW ) features an extremely trimmed down neural network (Mikolov et al., 2013). FAST T EXT is a derivative of WORD 2 VEC, also incorporating sub-word character n-grams (Bojanowski et al., 2017). Unlike the former two algorithms which fit word embeddings in a streaming fashion, G LOV E trains word vectors directly on a word co-occurrence matrix under the assumption to make more efficient use of word statistics (Pennington et al., 2014). Somewhat similar, SVDPPMI performs singular value decomposition on top of a point-wise mutual information co-occurrence matrix (Levy et al., 2015). In order to increase the reproducibility of our experiments, we rely on the following widely used, publicly available embedding models trained on very large corpora (summarized in Table 3): the S GNS model"
N18-1173,P16-1075,0,0.412784,"sive list of such data sets is presented by Buechel and Hahn (2018). For illustration, we also provide three sample entries from one of those lexicons in Table 2. As can be seen, the three affective dimensions behave complementary to each other, e.g., “terrorism” and “orgasm” display similar Arousal but opposing Valence. The task we address in this paper is to predict the values for Valence, Arousal and Dominance, given a lexical item. As is obvious from these examples, we consider emotion prediction as a regression, not as a classification problem (see arguments discussed in Buechel and Hahn (2016)). In this paper, we focus on the VAD format for the following reasons: First, note that the Valence dimension exactly corresponds to polarity (Turney and Littman, 2003). Hence, with the VAD model, emotion prediction can be seen as a generalization over classical polarity prediction. Second, to the best of our knowledge, the amount and diversity of available emotion lexicons with VAD encodings is larger than for any other format (see Table 1). Word Embeddings. Word embeddings are dense, low-dimensional vector representations of words trained on large volumes of raw text in an unsupervised mann"
N18-1173,D17-1169,0,0.0628274,"Missing"
N18-1173,L18-1550,0,0.0263872,"blicly available embedding models trained on very large corpora (summarized in Table 3): the S GNS model trained on the Google News corpus2 (G OOGLE), the FAST T EXT model trained on Common Crawl3 (C OMMON), as well as the FAST T EXT models for a wide range of languages trained on the respective Wikipedias4 (W IKI). 2 https://code.google.com/archive/p/ word2vec/ 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors. md Note that W IKI denotes multiple embedding models with different training and vocabulary sizes (see Grave et al. (2018) for further details). Additionally, we were given the opportunity to reuse the English embedding model from Sedoc et al. (2017) (G IGA), a strongly related contribution (see below). Their embeddings were trained on the English Gigaword corpus (Parker et al., 2011). Word-Level Prediction. One of the early approaches to word polarity induction which is still popular today (K¨oper and Schulte im Walde, 2016) was introduced by Turney and Littman (2003). They compute the polarity of an unseen word based on its point-wise mutual information (PMI) to a set of positive and negative seed words, respec"
N18-1173,D16-1057,0,0.324787,"d based on its point-wise mutual information (PMI) to a set of positive and negative seed words, respectively. SemEval-2015 Task 10E featured polarity induction on Twitter (Rosenthal et al., 2015). The best system relied on support vector regression (SVR) using a radial base function kernel (Amir et al., 2015). They employ the embedding vector of the target word as features. The results of their SVR-based system were beaten by the D EN SIFIER algorithm (Rothe et al., 2016). D ENSIFIER learns an orthogonal transformation of an embedding space into a subspace of strongly reduced dimensionality. Hamilton et al. (2016) developed S ENT P ROP, a graph-based, semi-supervised learning algorithm which builds up a word graph, where vertices correspond to words (of known as well as unknown polarity) and edge weights correspond to the similarity between them. The polarity information is then propagated through the graph, thus computing scores for unlabeled nodes. According to their evaluation, D ENSIFIER seems to be superior overall, yet S ENT P ROP produces competitive results 1909 ID G OOGLE C OMMON G IGA W IKI Language English English English all Method S GNS FAST T EXT C BOW FAST T EXT Corpus Google News Common"
N18-1173,P97-1023,0,0.0773125,"the largest gain on the smallest data sets, merely composed of one thousand samples. 1 Introduction Deep Learning (DL) has radically changed the rules of the game in NLP by dramatically boosting performance figures in almost all applications areas. Yet, one of the major premises of highperformance DL engines is their dependence on huge amounts of training data. As such, DL seems ill-suited for areas where training data are scarce, such as in the field of word emotion induction. We will use the terms polarity and emotion here to distinguish between research focusing on “semantic orientation” (Hatzivassiloglou and McKeown, 1997) (the positiveness or negativeness) of affective states, on the one hand, and approaches which provide predictions based on some of the many more elaborated representational systems for affective states, on the other hand. Originally, research activities focused on polarity alone. In the meantime, a shift towards more expressive representation models for emotion can be observed that heavily draws inspirations from psychological theory, e.g., Basic Emotions (Ekman, 1992) or the Valence-Arousal-Dominance model (Bradley and Lang, 1994). Though this change turned out to be really beneficial for se"
N18-1173,L16-1413,0,0.373706,"sive list of such data sets is presented by Buechel and Hahn (2018). For illustration, we also provide three sample entries from one of those lexicons in Table 2. As can be seen, the three affective dimensions behave complementary to each other, e.g., “terrorism” and “orgasm” display similar Arousal but opposing Valence. The task we address in this paper is to predict the values for Valence, Arousal and Dominance, given a lexical item. As is obvious from these examples, we consider emotion prediction as a regression, not as a classification problem (see arguments discussed in Buechel and Hahn (2016)). In this paper, we focus on the VAD format for the following reasons: First, note that the Valence dimension exactly corresponds to polarity (Turney and Littman, 2003). Hence, with the VAD model, emotion prediction can be seen as a generalization over classical polarity prediction. Second, to the best of our knowledge, the amount and diversity of available emotion lexicons with VAD encodings is larger than for any other format (see Table 1). Word Embeddings. Word embeddings are dense, low-dimensional vector representations of words trained on large volumes of raw text in an unsupervised mann"
N18-1173,Q15-1016,0,0.0571964,"(with its variants S GNS and C BOW ) features an extremely trimmed down neural network (Mikolov et al., 2013). FAST T EXT is a derivative of WORD 2 VEC, also incorporating sub-word character n-grams (Bojanowski et al., 2017). Unlike the former two algorithms which fit word embeddings in a streaming fashion, G LOV E trains word vectors directly on a word co-occurrence matrix under the assumption to make more efficient use of word statistics (Pennington et al., 2014). Somewhat similar, SVDPPMI performs singular value decomposition on top of a point-wise mutual information co-occurrence matrix (Levy et al., 2015). In order to increase the reproducibility of our experiments, we rely on the following widely used, publicly available embedding models trained on very large corpora (summarized in Table 3): the S GNS model trained on the Google News corpus2 (G OOGLE), the FAST T EXT model trained on Common Crawl3 (C OMMON), as well as the FAST T EXT models for a wide range of languages trained on the respective Wikipedias4 (W IKI). 2 https://code.google.com/archive/p/ word2vec/ 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://github.com/facebookresearch/ fastText/blob/master/pretrained-vectors."
N18-1173,P17-1001,0,0.207496,"ization over classical polarity prediction. Second, to the best of our knowledge, the amount and diversity of available emotion lexicons with VAD encodings is larger than for any other format (see Table 1). Word Embeddings. Word embeddings are dense, low-dimensional vector representations of words trained on large volumes of raw text in an unsupervised manner. The following are among 1 https://github.com/JULIELab/wordEmotions today’s most popular embedding algorithms: 1908 Source ID Language Format Bradley and Lang (1999) Warriner et al. (2013) Redondo et al. (2007) Stadthagen-Gonzalez et al. (2017) Schmidtke et al. (2014) Yu et al. (2016a) Imbir (2016) Montefinese et al. (2014) Soares et al. (2012) Moors et al. (2013) Sianipar et al. (2016) EN EN+ ES ES+ DE ZH PL IT PT NL ID English English Spanish Spanish German Chinese Polish Italian Portuguese Dutch Indonesian VAD VAD VAD VA VAD VA VAD VAD VAD VAD VAD # Entries 1,034 13,915 1,034 14,031 1,003 2,802 4,905 1,121 1,034 4,299 1,490 Table 1: Emotion lexicons used in our experiments (with their bibliographic source, identifier, language they refer to, emotion representation format, and number of lexical entries they contain). Word sunshine"
N18-1173,N15-1092,0,0.0276509,"gnments or mergers (Buechel and Hahn, 2017). As an alternative way of dealing with thus unwarranted heterogeneity, we here examine the potential of multi-task learning (MTL; Caruana (1997)) for word-level emotion prediction. In MTL for neural networks, a single model is fitted to solve multiple, independent tasks (in our case, to predict different emotional dimensions) which typically results in learning more robust and meaningful intermediate representations. MTL has been shown to greatly decrease the risk of overfitting (Baxter, 1997), work well for various NLP tasks (Setiawan et al., 2015; Liu et al., 2015; Søgaard and Goldberg, 2016; Cummins et al., 2016; Liu et al., 2017; Peng et al., 2017), and practically increases sample size, thus making it a natural choice for small-sized data sets typically found in the area of word emotion induction. 1907 Proceedings of NAACL-HLT 2018, pages 1907–1918 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work This section introduces the emotion representation format underlying our study and describes external resources we will use for evaluation before we discuss previous methodological work. Emotion Repre"
N18-1173,W17-5205,0,0.241432,"ization over classical polarity prediction. Second, to the best of our knowledge, the amount and diversity of available emotion lexicons with VAD encodings is larger than for any other format (see Table 1). Word Embeddings. Word embeddings are dense, low-dimensional vector representations of words trained on large volumes of raw text in an unsupervised manner. The following are among 1 https://github.com/JULIELab/wordEmotions today’s most popular embedding algorithms: 1908 Source ID Language Format Bradley and Lang (1999) Warriner et al. (2013) Redondo et al. (2007) Stadthagen-Gonzalez et al. (2017) Schmidtke et al. (2014) Yu et al. (2016a) Imbir (2016) Montefinese et al. (2014) Soares et al. (2012) Moors et al. (2013) Sianipar et al. (2016) EN EN+ ES ES+ DE ZH PL IT PT NL ID English English Spanish Spanish German Chinese Polish Italian Portuguese Dutch Indonesian VAD VAD VAD VA VAD VA VAD VAD VAD VAD VAD # Entries 1,034 13,915 1,034 14,031 1,003 2,802 4,905 1,121 1,034 4,299 1,490 Table 1: Emotion lexicons used in our experiments (with their bibliographic source, identifier, language they refer to, emotion representation format, and number of lexical entries they contain). Word sunshine"
N18-1173,D14-1162,0,0.0811845,"ries from Warriner et al. (2013). They use 9-point scales ranging from 1 (most negative/calm/submissive) to 9 (most positive/excited/dominant). WORD 2 VEC (with its variants S GNS and C BOW ) features an extremely trimmed down neural network (Mikolov et al., 2013). FAST T EXT is a derivative of WORD 2 VEC, also incorporating sub-word character n-grams (Bojanowski et al., 2017). Unlike the former two algorithms which fit word embeddings in a streaming fashion, G LOV E trains word vectors directly on a word co-occurrence matrix under the assumption to make more efficient use of word statistics (Pennington et al., 2014). Somewhat similar, SVDPPMI performs singular value decomposition on top of a point-wise mutual information co-occurrence matrix (Levy et al., 2015). In order to increase the reproducibility of our experiments, we rely on the following widely used, publicly available embedding models trained on very large corpora (summarized in Table 3): the S GNS model trained on the Google News corpus2 (G OOGLE), the FAST T EXT model trained on Common Crawl3 (C OMMON), as well as the FAST T EXT models for a wide range of languages trained on the respective Wikipedias4 (W IKI). 2 https://code.google.com/archi"
N18-1173,S15-2078,0,0.0421818,"reuse the English embedding model from Sedoc et al. (2017) (G IGA), a strongly related contribution (see below). Their embeddings were trained on the English Gigaword corpus (Parker et al., 2011). Word-Level Prediction. One of the early approaches to word polarity induction which is still popular today (K¨oper and Schulte im Walde, 2016) was introduced by Turney and Littman (2003). They compute the polarity of an unseen word based on its point-wise mutual information (PMI) to a set of positive and negative seed words, respectively. SemEval-2015 Task 10E featured polarity induction on Twitter (Rosenthal et al., 2015). The best system relied on support vector regression (SVR) using a radial base function kernel (Amir et al., 2015). They employ the embedding vector of the target word as features. The results of their SVR-based system were beaten by the D EN SIFIER algorithm (Rothe et al., 2016). D ENSIFIER learns an orthogonal transformation of an embedding space into a subspace of strongly reduced dimensionality. Hamilton et al. (2016) developed S ENT P ROP, a graph-based, semi-supervised learning algorithm which builds up a word graph, where vertices correspond to words (of known as well as unknown polari"
N18-1173,N16-1091,0,0.110079,"Missing"
N18-1173,E17-2090,0,0.210962,"Missing"
N18-1173,P15-1004,0,0.0260303,"g., on the basis of alignments or mergers (Buechel and Hahn, 2017). As an alternative way of dealing with thus unwarranted heterogeneity, we here examine the potential of multi-task learning (MTL; Caruana (1997)) for word-level emotion prediction. In MTL for neural networks, a single model is fitted to solve multiple, independent tasks (in our case, to predict different emotional dimensions) which typically results in learning more robust and meaningful intermediate representations. MTL has been shown to greatly decrease the risk of overfitting (Baxter, 1997), work well for various NLP tasks (Setiawan et al., 2015; Liu et al., 2015; Søgaard and Goldberg, 2016; Cummins et al., 2016; Liu et al., 2017; Peng et al., 2017), and practically increases sample size, thus making it a natural choice for small-sized data sets typically found in the area of word emotion induction. 1907 Proceedings of NAACL-HLT 2018, pages 1907–1918 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work This section introduces the emotion representation format underlying our study and describes external resources we will use for evaluation before we discuss previous methodological w"
N18-1173,P16-2038,0,0.0306375,"(Buechel and Hahn, 2017). As an alternative way of dealing with thus unwarranted heterogeneity, we here examine the potential of multi-task learning (MTL; Caruana (1997)) for word-level emotion prediction. In MTL for neural networks, a single model is fitted to solve multiple, independent tasks (in our case, to predict different emotional dimensions) which typically results in learning more robust and meaningful intermediate representations. MTL has been shown to greatly decrease the risk of overfitting (Baxter, 1997), work well for various NLP tasks (Setiawan et al., 2015; Liu et al., 2015; Søgaard and Goldberg, 2016; Cummins et al., 2016; Liu et al., 2017; Peng et al., 2017), and practically increases sample size, thus making it a natural choice for small-sized data sets typically found in the area of word emotion induction. 1907 Proceedings of NAACL-HLT 2018, pages 1907–1918 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work This section introduces the emotion representation format underlying our study and describes external resources we will use for evaluation before we discuss previous methodological work. Emotion Representation and Data Sets. Psy"
N18-1173,P16-2037,0,0.409408,"FAST T EXT C BOW FAST T EXT Corpus Google News Common Crawl Gigawords Wikipeda # Tokens # Types 11 6 1 × 10 6 × 1011 4 × 10 9 — 3 × 10 2 × 106 2 × 106 — # Dimensions 300 300 300 300 Table 3: Embedding models used for our experiments with identifier, language, embedding algorithm, training corpus, its size in the number of tokens, size of the vocabulary (types) of the resulting embedding model and its dimensionality. only when the seed lexicon or the corpus the word embeddings are trained on is very small.5 For word emotion induction, a very similar approach to S ENT P ROP has been proposed by Wang et al. (2016a). They also propagate affective information (Valence and Arousal, in this case) through a word graph with similarity weighted edges. Sedoc et al. (2017) recently proposed an approach based on signed spectral clustering where a word graph is constructed not only based on word similarity but also on the considered affective information (again, Valence and Arousal). The emotion value of a target word is then computed based on the seed words in its cluster. They report to outperform the results from Wang et al. (2016a). Contrary to the trend to graph-based methods, the best system of the IALP 20"
N18-1173,N16-1066,0,0.139041,"ence and Arousal, in this case) through a word graph with similarity weighted edges. Sedoc et al. (2017) recently proposed an approach based on signed spectral clustering where a word graph is constructed not only based on word similarity but also on the considered affective information (again, Valence and Arousal). The emotion value of a target word is then computed based on the seed words in its cluster. They report to outperform the results from Wang et al. (2016a). Contrary to the trend to graph-based methods, the best system of the IALP 2016 Shared Task on Chinese word emotion induction (Yu et al., 2016b) employed a simple feed-forward neural network (FFNN) with one hidden layer in combination with boosting (Du and Zhang, 2016). Another very recent contribution which advocates a supervised set-up was published by Li et al. (2017). They propose ridge regression, again using word embeddings as features. Even with this simple approach, they report to outperform many of the above methods in the VAD prediction task.6 Sentence-Level and Text-Level Prediction. Different from the word-level prediction task (the one we focus on in this contribution), the determination of emotion values for higher-lev"
P06-1099,P01-1025,0,0.186973,"nguistic filtering techniques, such as POS tagging and phrase chunking (e.g., Frantzi et al. (2000), Krenn and Evert (2001), Nenadi´c et al. (2004), Wermter and Hahn (2005)). 785 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 785–792, c Sydney, July 2006. 2006 Association for Computational Linguistics compared with various more sophisticated association measures (AMs such as t-test, loglikelihood, etc.). In particular, the precision/recall value comparison between the various AMs exhibits a rather inconclusive picture in Evert and Krenn (2001) and Krenn and Evert (2001) as to whether sophisticated statistical AMs are actually more viable than frequency counting. Commonly used statistical significance testing (e.g., the McNemar or the Wilcoxon sign rank tests; see (Sachs, 1984)) does not seem to provide an appropriate evaluation ground either. Although Evert and Krenn (2001) and Wermter and Hahn (2004) provide significance testing of some AMs with respect to mere frequency counting for collocation extraction, they do not differentiate whether this is due to differences in the ranking of true positives or true negatives or a combinat"
P06-1099,P99-1044,0,0.0117048,"t degree a candidate qualifies as a term or a collocation. While term mining and collocation mining, as a whole, involve almost the same analytical processing steps, such as orthographic and morphological normalization, normalization of term or collocation variation etc., it is exactly the measure which grades termhood or collocativity of a candidate on which alternative approaches diverge. 2 Related Work Although there has been a fair amount of work employing linguistically sophisticated analysis of candidate items (e.g., on CE by Lin (1998) and Lin (1999) as well as on ATR by Daille (1996), Jacquemin (1999), and Jacquemin (2001)), these approaches are limited by the difficulty to port grammatical specifications to other domains (in the case of ATR) or by the error-proneness of full general-language parsers (in the case of CE). Therefore, most recent approaches in both areas have backed off to more shallow linguistic filtering techniques, such as POS tagging and phrase chunking (e.g., Frantzi et al. (2000), Krenn and Evert (2001), Nenadi´c et al. (2004), Wermter and Hahn (2005)). 785 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,"
P06-1099,P98-2127,0,0.00648789,"vidence or association measures yield scores indicating to what degree a candidate qualifies as a term or a collocation. While term mining and collocation mining, as a whole, involve almost the same analytical processing steps, such as orthographic and morphological normalization, normalization of term or collocation variation etc., it is exactly the measure which grades termhood or collocativity of a candidate on which alternative approaches diverge. 2 Related Work Although there has been a fair amount of work employing linguistically sophisticated analysis of candidate items (e.g., on CE by Lin (1998) and Lin (1999) as well as on ATR by Daille (1996), Jacquemin (1999), and Jacquemin (2001)), these approaches are limited by the difficulty to port grammatical specifications to other domains (in the case of ATR) or by the error-proneness of full general-language parsers (in the case of CE). Therefore, most recent approaches in both areas have backed off to more shallow linguistic filtering techniques, such as POS tagging and phrase chunking (e.g., Frantzi et al. (2000), Krenn and Evert (2001), Nenadi´c et al. (2004), Wermter and Hahn (2005)). 785 Proceedings of the 21st International Conferen"
P06-1099,P99-1041,0,0.020778,"ciation measures yield scores indicating to what degree a candidate qualifies as a term or a collocation. While term mining and collocation mining, as a whole, involve almost the same analytical processing steps, such as orthographic and morphological normalization, normalization of term or collocation variation etc., it is exactly the measure which grades termhood or collocativity of a candidate on which alternative approaches diverge. 2 Related Work Although there has been a fair amount of work employing linguistically sophisticated analysis of candidate items (e.g., on CE by Lin (1998) and Lin (1999) as well as on ATR by Daille (1996), Jacquemin (1999), and Jacquemin (2001)), these approaches are limited by the difficulty to port grammatical specifications to other domains (in the case of ATR) or by the error-proneness of full general-language parsers (in the case of CE). Therefore, most recent approaches in both areas have backed off to more shallow linguistic filtering techniques, such as POS tagging and phrase chunking (e.g., Frantzi et al. (2000), Krenn and Evert (2001), Nenadi´c et al. (2004), Wermter and Hahn (2005)). 785 Proceedings of the 21st International Conference on Computati"
P06-1099,C04-1087,0,0.0257593,"Missing"
P06-1099,C04-1141,1,0.941407,"Linguistics compared with various more sophisticated association measures (AMs such as t-test, loglikelihood, etc.). In particular, the precision/recall value comparison between the various AMs exhibits a rather inconclusive picture in Evert and Krenn (2001) and Krenn and Evert (2001) as to whether sophisticated statistical AMs are actually more viable than frequency counting. Commonly used statistical significance testing (e.g., the McNemar or the Wilcoxon sign rank tests; see (Sachs, 1984)) does not seem to provide an appropriate evaluation ground either. Although Evert and Krenn (2001) and Wermter and Hahn (2004) provide significance testing of some AMs with respect to mere frequency counting for collocation extraction, they do not differentiate whether this is due to differences in the ranking of true positives or true negatives or a combination thereof.2 As for studies on ATR (e.g., Wermter and Hahn (2005) or Nenadi´c et al. (2004)), no statistical testing of the term extraction algorithms to mere frequency counting was performed. But after all, these kinds of commonly used statistical significance tests may not provide the right machinery in the first place. By design, they are rather limited (or f"
P06-1099,H05-1106,1,0.782382,"tically sophisticated analysis of candidate items (e.g., on CE by Lin (1998) and Lin (1999) as well as on ATR by Daille (1996), Jacquemin (1999), and Jacquemin (2001)), these approaches are limited by the difficulty to port grammatical specifications to other domains (in the case of ATR) or by the error-proneness of full general-language parsers (in the case of CE). Therefore, most recent approaches in both areas have backed off to more shallow linguistic filtering techniques, such as POS tagging and phrase chunking (e.g., Frantzi et al. (2000), Krenn and Evert (2001), Nenadi´c et al. (2004), Wermter and Hahn (2005)). 785 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 785–792, c Sydney, July 2006. 2006 Association for Computational Linguistics compared with various more sophisticated association measures (AMs such as t-test, loglikelihood, etc.). In particular, the precision/recall value comparison between the various AMs exhibits a rather inconclusive picture in Evert and Krenn (2001) and Krenn and Evert (2001) as to whether sophisticated statistical AMs are actually more viable than frequency counting. Commonly used statistical si"
P06-1099,C98-2122,0,\N,Missing
P08-1098,W04-3202,0,0.02149,"ure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in 867 or"
P08-1098,P96-1042,0,0.173428,"verage of a wide variety of domains in human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a"
P08-1098,W05-0619,0,0.00963254,") and Brown (right) 40000 5000 10000 15000 constituents 20000 25000 30000 35000 constituents Figure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994)"
P08-1098,J04-3001,0,0.70764,"an expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might use features extracted from parse t"
P08-1098,J93-2004,0,0.0340148,"ding classifier. As a consequence, training such a classifier which takes into account several annotation tasks will best be performed on a rich corpus annotated with respect to all inputrelevant tasks. Both kinds of annotation tasks, similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as"
P08-1098,P00-1016,0,0.0259827,"n human language technology (HLT) systems, we can expect a growing need for manual annotations to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might us"
P08-1098,J05-1004,0,0.0421395,"similar and dissimilar ones, constitute examples of what we refer to as multi-task annotation problems. Indeed, there have been efforts in creating resources annotated with respect to various annotation tasks though each of them was carried out independently of the other. In the general language UPenn annotation efforts for the WSJ sections of the Penn Treebank (Marcus et al., 1993), sentences are annotated with POS tags, parse trees, as well as discourse annotation from the Penn Discourse Treebank (Miltsakaki et al., 2008), while verbs and verb arguments are annotated with Propbank rolesets (Palmer et al., 2005). In the biomedical GENIA corpus (Ohta et al., 2002), scientific text is annotated with POS tags, parse trees, and named entities. In this paper, we introduce multi-task active learning (MTAL), an active learning paradigm for multiple annotation tasks. We propose a new AL framework where the examples to be annotated are selected so that they are as informative as possible for a set of classifiers instead of a single classifier only. This enables the creation of a single combined corpus annotated with respect to various annotation tasks, while preserving the advantages of AL with 862 respect to"
P08-1098,P07-1052,1,0.855102,"etric. It is calculated on the token-level as V Etok (t) = − c V (li , t) V (li , t) 1 X log (1) log k i=0 k k where V (lki ,t) is the ratio of k classifiers where the label li is assigned to a token t. The sentence level vote entropy V Esent is then the average over all tokens tj of sentence s. For the parsing task, the disagreement score is based on a committee of k2 = 10 instances of Dan Bikel’s reimplementation of Collins’ parser (Bickel, 2005; Collins, 1999). For each sentence in the unlabeled pool, the agreement between the committee members was calculated using the function reported by Reichart and Rappoport (2007): AF (s) = 1 N X f score(mi , ml ) (2) i,l∈[1...N ],i6=l Where mi and ml are the committee members and N = k2 ·(k22 −1) is the number of pairs of different committee members. This function calculates the agreement between the members of each pair by calculating their relative f-score and then averages the pairs’ scores. The disagreement of the committee on a sentence is simply 1 − AF (s). 4.2 Experimental settings For the NE task we employed the classifier described by Tomanek et al. (2007): The NE tagger is based on Conditional Random Fields (Lafferty et al., 2001) 5 We randomly sampled L = e"
P08-1098,W04-1221,0,0.0193363,"Missing"
P08-1098,P04-1075,0,0.0205662,"E task on WSJ (left) and Brown (right) 40000 5000 10000 15000 constituents 20000 25000 30000 35000 constituents Figure 3: Learning curves for parse task on WSJ (left) and Brown (right) ment curve of one task has a slope of (close to) zero. Future work will focus on issues related to this. 6 Related Work There is a large body of work on single-task AL approaches for many NLP tasks where the focus is mainly on better, task-specific selection protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combinati"
P08-1098,tomanek-hahn-2008-approximating,1,0.775115,"selection can be a better choice than one-sided selection in multiple annotation scenarios. Thus, considering all annotation tasks in the selection process (even if the selection protocol is as simple as the alternating selection protocol) is better than selecting only with respect to one task. Further, it should be noted that overall the more sophisticated rank combination protocol does not perform much better than the simpler alternating selection protocol in all scenarios. Finally, Figure 4 shows the disagreement curves for the two tasks on the WSJ corpus. As has already been discussed by Tomanek and Hahn (2008), disagreement curves can be used as a stopping criterion and to monitor the progress of AL-driven annotation. This is especially valuable when no annotated validation set is available (which is needed for plotting learning curves). We can see that the disagreement curves significantly flatten approximately at the same time as the learning curves do. In the context of MTAL, disagreement curves might not only be interesting as a stopping criterion but rather as a switching criterion, i.e., to identify when MTAL could be turned into one-sided selection. This would be the case if in an MTAL scena"
P08-1098,D07-1051,1,0.935507,"tions to support many kinds of application-specific training data. Creating annotated data is extremely laborintensive. The Active Learning (AL) paradigm (Cohn et al., 1996) offers a promising solution to deal with this bottleneck, by allowing the learning algorithm to control the selection of examples to be manually annotated such that the human labeling effort be minimized. AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al., 2007). However, AL is designed in such a way that it selects examples for manual annotation with respect to a single learning algorithm or classifier. Under this AL annotation policy, one has to perform a separate annotation cycle for each classifier to be trained. In the following, we will refer to the annotations supplied for a classifier as the annotations for a single annotation task. Modern HLT systems often utilize annotations resulting from different tasks. For example, a machine translation system might use features extracted from parse trees and named entity annotations. For such an applic"
P08-1098,D07-1082,0,0.0153277,"protocols and methods to quantify the usefulness score in different scenarios. As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). Further, there is some work on questions arising when AL is to be used in real-life annotation scenarios, including impaired inter-annotator agreement, stopping criteria for AL-driven annotation, and issues of reusability (Baldridge and Osborne, 2004; Hachey et al., 2005; Zhu and Hovy, 2007; Tomanek et al., 2007). Multi-task AL is methodologically related to approaches of decision combination, especially in the context of classifier combination (Ho et al., 1994) and ensemble methods (Breiman, 1996). Those approaches focus on the combination of classifiers in 867 order to improve the classification error rate for one specific classification task. In contrast, the focus of multi-task AL is on strategies to select training material for multi classifier systems where all classifiers cover different classification tasks. 7 Discussion Our treatment of MTAL within the context of the or"
P08-1098,J03-4003,0,\N,Missing
P08-1098,W03-0419,0,\N,Missing
P08-1098,P02-1016,0,\N,Missing
P09-1117,P96-1042,0,0.118276,"ow-up problem, viz. the need for human annotators to supply large amounts of “golden” annotation data on which ML systems can be trained. In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus. Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material. In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner. Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences. Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences. Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility. In the NER scenario, e.g., large portions of the tex"
P09-1117,W04-3111,0,0.0148969,"Missing"
P09-1117,D08-1112,0,0.12469,"of “golden” annotation data on which ML systems can be trained. In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus. Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material. In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner. Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences. Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences. Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility. In the NER scenario, e.g., large portions of the text do not contain any target entity mention at all. To further exploit this"
P09-1117,D07-1051,1,0.880804,"o supply large amounts of “golden” annotation data on which ML systems can be trained. In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus. Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material. In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner. Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences. Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences. Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility. In the NER scenario, e.g., large portions of the text do not contain any target entity mention at a"
P09-1117,P95-1026,0,0.0290145,"AL are variants of the Query-By-Committee approach (Seung et al., 1992) or based on uncertainty sampling (Lewis and Catlett, 1994). Query-by-Committee uses a committee of classifiers, and examples on which the classifiers disagree most regarding their predictions are considered highly informative and thus selected for annotation. Uncertainty sampling selects examples on which a single classifier is least confident. AL has been successfully applied to many NLP tasks; Settles and Craven (2008) compare the effectiveness of several AL approaches for sequence labeling tasks of NLP. Self-training (Yarowsky, 1995) is a form of semi-supervised learning. From a seed set of labeled examples a weak model is learned which subsequently gets incrementally refined. In each step, unlabeled examples on which the current model is very confident are labeled with their predictions, added to the training set, and a new model is learned. Similar to self-training, cotraining (Blum and Mitchell, 1998) augments the training set by automatically labeled examples. It is a multi-learner algorithm where the learners have independent views on the data and mutually produce labeled examples for each other. Bootstrapping approa"
P09-1117,P00-1016,0,0.0135771,"ed for human annotators to supply large amounts of “golden” annotation data on which ML systems can be trained. In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus. Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material. In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner. Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences. Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences. Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility. In the NER scenario, e.g., large portions of the text do not contain any targ"
P09-1117,W01-0501,0,0.0333929,"refined. In each step, unlabeled examples on which the current model is very confident are labeled with their predictions, added to the training set, and a new model is learned. Similar to self-training, cotraining (Blum and Mitchell, 1998) augments the training set by automatically labeled examples. It is a multi-learner algorithm where the learners have independent views on the data and mutually produce labeled examples for each other. Bootstrapping approaches often fail when applied to NLP tasks where large amounts of training material are required to achieve acceptable performance levels. Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a highperforming model from being learned. Also, the size of the seed set is an important parameter. When it is chosen too small data quality gets deteriorated quickly, when it is chosen too large no improvement over the initial model can be expected. To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. In this mode, a human is put into the co-training loop to review and, if necessar"
P10-1118,W07-1001,0,0.0330524,"sary condition for such features to be practically applicable. To account for our findings that syntactic and semantic complexity correlates with annotation performance, we added three features based on syntactic, and two based on semantic complexity measures. We decided for the use of multiple measures because there is no single agreed-upon metric for either syntactic or semantic complexity. This decision is further motivated by findings which reveal that different measures are often complementary to each other so that their combination better approximates the inherent degrees of complexity (Roark et al., 2007). As for syntactic complexity, we use two measures based on structural complexity including (a) the number of nodes of a constituency parse tree which are dominated by the annotation phrase (cf. Section 2.1), and (b) given the dependency graph of the sentence embedding the annotation phrase, we consider the distance between words for each dependency link within the annotation phrase and consider the maximum over such dis8 In preliminary experiments our set of basic features comprised additional features providing information on the usage of stop words in the annotation phrase and on the number"
P10-1118,W09-1903,0,0.032189,"Missing"
P10-1118,P09-1117,1,0.543841,"Missing"
P10-1118,tomanek-hahn-2010-annotation,1,0.831684,"tion, which are estimated as being most informative to learn an effective classification model (Cohn et al., 1996). This intentional selection bias stands in stark contrast to prevailing sampling approaches where annotation examples are randomly chosen. When different approaches to AL are compared with each other, or with standard random sampling, in terms of annotation efficiency, up until now, the AL community assumed uniform annotation costs for each linguistic unit, e.g. words. This claim, however, has been shown to be invalid in several studies (Hachey et al., 2005; Settles et al., 2008; Tomanek and Hahn, 2010). If uniformity does not hold and, hence, the number of annotated units does not indicate the true annotation efforts required for a specific sample, empirically more adequate cost models are needed. Building predictive models for annotation costs has only been addressed in few studies for now (Ringger et al., 2008; Settles et al., 2008; Arora et al., 2009). The proposed models are based on easy-to-determine, yet not so explanatory variables (such as the number of words to be annotated), indicating that accurate models of annotation costs remain a desideratum. We here, alternatively, consider"
P10-1118,W05-0619,0,0.147637,"Missing"
P10-1118,C96-2123,0,0.055485,"graph of the sentence embedding the annotation phrase, we consider the distance between words for each dependency link within the annotation phrase and consider the maximum over such dis8 In preliminary experiments our set of basic features comprised additional features providing information on the usage of stop words in the annotation phrase and on the number of paragraphs, sentences, and words in the respective annotation example. However, since we found these features did not have any significant impact on the model, we removed them. tance values as another metric for syntactic complexity. Lin (1996) has already shown that human performance on sentence processing tasks can be predicted using such a measure. Our third syntactic complexity measure is based on the probability of part-of-speech (POS) 2-grams. Given a POS 2-gram model, which we learned from the automatically POS-tagged M UC 7 corpus, the complexity of an annotation phrase is defined by Pn i=2 P (POSi |POSi−1 ) where POSi refers to the POS-tag of the i-th word of the annotation phrase. A similar measure has been used by Roark et al. (2007) who claim that complex syntactic structures correlate with infrequent or surprising combi"
P10-1118,ringger-etal-2008-assessing,0,0.0373481,"with standard random sampling, in terms of annotation efficiency, up until now, the AL community assumed uniform annotation costs for each linguistic unit, e.g. words. This claim, however, has been shown to be invalid in several studies (Hachey et al., 2005; Settles et al., 2008; Tomanek and Hahn, 2010). If uniformity does not hold and, hence, the number of annotated units does not indicate the true annotation efforts required for a specific sample, empirically more adequate cost models are needed. Building predictive models for annotation costs has only been addressed in few studies for now (Ringger et al., 2008; Settles et al., 2008; Arora et al., 2009). The proposed models are based on easy-to-determine, yet not so explanatory variables (such as the number of words to be annotated), indicating that accurate models of annotation costs remain a desideratum. We here, alternatively, consider different classes of syntactic and semantic complexity that might affect the cognitive load during the annotation process, with 1158 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1158–1167, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Lingui"
P17-4006,P16-1141,0,0.446763,"4 as well as interactive websites for exploring precompiled corpora, e.g., the “advanced” interface for Google Books (Davies, 2014) or D IAC OLLO (Jurish, 2015). Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014); Kulkarni et al. (2015); Hellrich and Hahn (2016b)), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and Hamilton et al. (2016) using SVDPPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation. 3 4 Semantic Processing The five corpora described in Section 3 were divided into multiple non-overlapping te"
P17-4006,C16-1262,1,0.913962,"ctor word representations of both semantic and syntactic information. Alternative approaches are e.g., graph-based algorithms (Biemann and Riedl, 2013) or ranking functions from information retrieval (Claveau et al., 2014). The premier example for word embeddings is skip-gram negative sampling, which is part of the word2vec family of algorithms (Mikolov et al., 2013). The random processes involved in training these embeddings lead to a lack of reliability which is dangerous during interpretation— experiments cannot be repeated without predicting severely different relationships between words (Hellrich and Hahn, 2016a, 2017). Introduction 1 Related Work http://jeseme.org https://github.com/hellrich/JeSemE 31 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 31–36 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-4006 Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (Deerwester et al., 1990)) are not affected by this problem. Levy et al. (2015) created SVDPPMI after investigating the implicit op"
P17-4006,W16-4008,1,0.908452,"corpora, e.g., W ORD S MITH3 or the UCS TOOLKIT,4 as well as interactive websites for exploring precompiled corpora, e.g., the “advanced” interface for Google Books (Davies, 2014) or D IAC OLLO (Jurish, 2015). Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014); Kulkarni et al. (2015); Hellrich and Hahn (2016b)), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and Hamilton et al. (2016) using SVDPPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation. 3 4 Semantic Processing The five corpora des"
P17-4006,Q15-1016,0,0.623693,"everely different relationships between words (Hellrich and Hahn, 2016a, 2017). Introduction 1 Related Work http://jeseme.org https://github.com/hellrich/JeSemE 31 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 31–36 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-4006 Word embeddings based on singular value decomposition (SVD; historically popular in the form of Latent Semantic Analysis (Deerwester et al., 1990)) are not affected by this problem. Levy et al. (2015) created SVDPPMI after investigating the implicit operations performed while training neural word embeddings (Levy and Goldberg, 2014). As SVDPPMI performs very similar to word2vec on evaluation tasks while avoiding reliability problems we deem it the best currently available word embedding method for applying distributional semantics in the Digital Humanities (Hamilton et al., 2016; Hellrich and Hahn, 2016a). 2.2 research. We employ five corpora, including the four largest diachronic corpora of acceptable quality for English and German. The Google Books Ngram Corpus (GB; Michel et al. (2011),"
P17-4006,P12-3029,0,0.0571097,"Missing"
P17-4006,L16-1305,0,0.123169,"Missing"
P17-4006,W14-2517,0,0.376233,"uistics (see e.g., Curzan (2009)). There exist already several tools for performing statistical analysis on user provided corpora, e.g., W ORD S MITH3 or the UCS TOOLKIT,4 as well as interactive websites for exploring precompiled corpora, e.g., the “advanced” interface for Google Books (Davies, 2014) or D IAC OLLO (Jurish, 2015). Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014); Kulkarni et al. (2015); Hellrich and Hahn (2016b)), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and Hamilton et al. (2016) using SVDPPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for auto"
P17-4006,D14-1162,0,0.0802961,"D S MITH3 or the UCS TOOLKIT,4 as well as interactive websites for exploring precompiled corpora, e.g., the “advanced” interface for Google Books (Davies, 2014) or D IAC OLLO (Jurish, 2015). Meanwhile, word embeddings and their application to diachronic semantics have become a novel state-of-the-art methodology lacking, however, off-the-shelves analysis tools easy to use for a typically non-technical audience. Most work is centered around word2vec (e.g., Kim et al. (2014); Kulkarni et al. (2015); Hellrich and Hahn (2016b)), whereas alternative approaches are rare, e.g., Jo (2016) using GloVe (Pennington et al., 2014) and Hamilton et al. (2016) using SVDPPMI . Embeddings trained on corpora specific for multiple time spans can be used for two research purposes, namely, screening the semantic evolution of lexical items over time (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016) and exploring the meaning of lexical items during a specific time span by finding their closest neighbors in embedding space. This information can then be exploited for automatic (Buechel et al., 2016) or manual (Jo, 2016) interpretation. 3 4 Semantic Processing The five corpora described in Section 3 were divided into"
P17-4006,P06-1099,1,0.599704,"to the interdisciplinary Graduate School “The Romantic Model” at Friedrich-Schiller-Universit¨at Jena (Germany). 2 Distributional Semantics Distributional semantics can be broadly conceived as a staged approach to capture the semantics of a lexical item in focus via contextual patterns. Concordances are probably the most simple scheme to examine contextual semantic effects, but leave semantic inferences entirely to the human observer. A more complex layer is reached with collocations which can be identified automatically via statistical word co-occurrence metrics (Manning and Sch¨utze, 1999; Wermter and Hahn, 2006), two of which are incorporated in J E S EM E as well: Positive pointwise mutual information (PPMI), developed by Bullinaria and Levy (2007) as an improvement over the probability ratio of normal pointwise mutual information (PMI; Church and Hanks (1990)) and Pearson’s χ2 , commonly used for testing the association between categorical variables (e.g., POS tags) and considered to be more robust than PMI when facing sparse information (Manning and Sch¨utze, 1999). The currently most sophisticated and most influential approach to distributional semantics employs word embeddings, i.e., low (usuall"
P17-4016,W13-2014,0,0.0721809,"Missing"
P17-4016,L16-1397,1,\N,Missing
P17-4016,W11-1801,0,\N,Missing
P84-1083,C82-1011,0,\N,Missing
P96-1036,P87-1022,0,0.986711,"on the forward-looking centers has emerged, one that reflects well-known regularities of fixed word order languages such as English. With the exception of Walker et al. (1990; 1994) for Japanese, Turan (1995) for Turkish, Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical roles we are aware of and has been taken from Brennan et al. (1987). Often, the distinction between complements and adjuncts is collapsed into the category ""others"" (c.f., e.g., Grosz et al. (1995)). Our work on the resolution of anaphora (Strube & Hahn, 1995; Hahn & Strube, 1996) and textual ellipsis (Hahn et al., 1996), however, is based on German, a free word order language, in which grammatical role information is far less predictive for the organization of centers. Rather, for establishing proper referential relations, the functional information structure of the utterances becomes crucial (different perspectives on functional analysis are brought forward"
P96-1036,P83-1007,0,0.955222,"order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering. 1 Introduction The centering model has evolved as a methodology for the description and explanation of the local coherence of discourse (Grosz et al., 1983; 1995), with focus on pronominal and nominal anaphora. Though several cross-linguistic studies have been carded out (cf. the enumeration in Grosz et al. (1995)), an almost canonical scheme for the ordering on the forward-looking centers has emerged, one that reflects well-known regularities of fixed word order languages such as English. With the exception of Walker et al. (1990; 1994) for Japanese, Turan (1995) for Turkish, Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-ob"
P96-1036,J95-2003,0,0.14679,"he centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering. 1 Introduction The centering model has evolved as a methodology for the description and explanation of the local coherence of discourse (Grosz et al., 1983; 1995), with focus on pronominal and nominal anaphora. Though several cross-linguistic studies have been carded out (cf. the enumeration in Grosz et al. (1995)), an almost canonical scheme for the ordering on the forward-looking centers has emerged, one that reflects well-known regularities of fixed word order languages such as English. With the exception of Walker et al. (1990; 1994) for Japanese, Turan (1995) for Turkish, Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical"
P96-1036,C96-1084,1,0.854411,"996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical roles we are aware of and has been taken from Brennan et al. (1987). Often, the distinction between complements and adjuncts is collapsed into the category ""others"" (c.f., e.g., Grosz et al. (1995)). Our work on the resolution of anaphora (Strube & Hahn, 1995; Hahn & Strube, 1996) and textual ellipsis (Hahn et al., 1996), however, is based on German, a free word order language, in which grammatical role information is far less predictive for the organization of centers. Rather, for establishing proper referential relations, the functional information structure of the utterances becomes crucial (different perspectives on functional analysis are brought forward in Dane~ (1974b) and Dahl (1974)). We share the notion of functional information structure as developed by Dane~ (1974a). He distinguishes between two crucial dichotomies, viz. given information vs. new information (constituting the information structure"
P96-1036,C92-1023,0,0.282948,"Missing"
P96-1036,P86-1031,0,0.841204,"English language, we considered all exampies from Grosz et al. (1995) and Brennan et al. (1987); for Japanese we took the data from Walker et al. (1994)). Surprisingly enough, all examples of Grosz et al. (1995) passed the test successfully. Only with respect to the troublesome Alfa Romeo driving scenario (cf. Brennan et al. (1987, p.157)) our constraints fail to properly rank the elements of the third sentence C! of that example. 7 Note also that these results were achieved without having recourse to extra constraints, e.g., the shared property constraint to account for anaphora parallelism (Kameyama, 1986). We applied our constraints to Japanese examples in the same way. Again we abandoned all extra constraints set up in these studies, e.g., the Zero Topic Assignment (ZTA) rule and the special role of empathy 7In essence, the very specific problem addressed by that example seems to be that Friedman has not been previously introduced in the local discourse segment and is only accessible via the global focus. CONTINUE - cheap cheap RETAIN expensive cheap CONTINUE RETAIN expensive expensive expensive SMOOTH-SHIFT cheap ROUGH-SHIFT expensive expensive SMOOTH-SHIFT ROUGH-SHIFT i expensNe cheap expen"
P96-1036,P96-1057,1,0.840409,"Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical roles we are aware of and has been taken from Brennan et al. (1987). Often, the distinction between complements and adjuncts is collapsed into the category ""others"" (c.f., e.g., Grosz et al. (1995)). Our work on the resolution of anaphora (Strube & Hahn, 1995; Hahn & Strube, 1996) and textual ellipsis (Hahn et al., 1996), however, is based on German, a free word order language, in which grammatical role information is far less predictive for the organization of centers. Rather, for establishing proper referential relations, the functional information structure of the utterances becomes crucial (different perspectives on functional analysis are brought forward in Dane~ (1974b) and Dahl (1974)). We share the notion of functional information structure as developed by Dane~ (1974a). He distinguishes between two crucial dichotomies, viz. given information vs. new informatio"
P96-1036,E95-1033,1,0.768722,", Turan (1995) for Turkish, Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical roles we are aware of and has been taken from Brennan et al. (1987). Often, the distinction between complements and adjuncts is collapsed into the category ""others"" (c.f., e.g., Grosz et al. (1995)). Our work on the resolution of anaphora (Strube & Hahn, 1995; Hahn & Strube, 1996) and textual ellipsis (Hahn et al., 1996), however, is based on German, a free word order language, in which grammatical role information is far less predictive for the organization of centers. Rather, for establishing proper referential relations, the functional information structure of the utterances becomes crucial (different perspectives on functional analysis are brought forward in Dane~ (1974b) and Dahl (1974)). We share the notion of functional information structure as developed by Dane~ (1974a). He distinguishes between two crucial dichotomies, viz. given informat"
P96-1036,J94-2006,0,0.878125,"Rotbuch Verlag, pp. 57-63. SA performance evaluation of the current anaphora and ellipsis resolution capacities of our system is reported in Hahn et al. (1996). 273 constraint that elliptical antecedents are ranked higher than elliptical expressions (short: ""ante > express""). For the evaluation of a centering algorithm on naturally occurring text it is necessary to specify how to deal with complex sentences. In particular, methods for the interaction between intra- and intersentential anaphora resolution have to be defined, since the centering model is concerned only with the latter case (see Suri & McCoy (1994)). We use an approach as described by Strube (1996) for the evaluation. Since most of the anaphors in these texts are nominal anaphors, the resolution of which is much more restricted than that of pronominal anaphors, the rate of success for the whole anaphora resolution process is not significant enough for a proper evaluation of the functional constraints. The reason for this lies in the fact that nominal anaphors are far more constrained by conceptual criteria than pronominal anaphors. So the chance to properly resolve a nominal anaphor, even at lower ranked positions in the center lists, i"
P96-1036,J94-2003,0,\N,Missing
P97-1014,P87-1022,0,0.504615,"and incorporation of discourse structure beyond the level of immediately adjacent utterances within the centering framework. Two recent studies deal with this topic in order to relate attentional and intentional structures on a larger scale of global discourse coherence. Passonneau (1996) proposes an algorithm for the generation of referring expressions and Walker (1996a) integrates centering into a cache model of attentional state. Both studies, among other things, deal with the supposition whether a correlation exists between particular centering transitions (which were first introduced by Brennan et al. (1987); cf. Table 1) and intentionbased discourse segments. In particular, the role of SHIFT-type transitions is examined from the perspective of whether they not only indicate a shift of the topic between two immediately successive utterances but also signal (intention-based) segment boundaries. The data in both studies reveal that only a weak correlation between the SHIFT transitions and segment boundaries can be observed. This finding precludes a reliable prediction of segment boundaries based on the occurrence of 1Our notion of referentialdiscourse segment should not be confounded with the inten"
P97-1014,J95-2003,0,0.851237,"tation has on the validity of anaphora resolution (cf. Section 5 for a discussion of evaluation results). We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied. 1 Introduction The centering model (Grosz et al., 1995) has evolved as a major methodology for computational discourse analysis. It provides simple, yet powerful data structures, constraints and rules for the local coherence of discourse. As far as anaphora resolution is concerned, e.g., the model requires to consider those discourse entities as potential antecedents for anaphoric expressions in the current utterance Ui, which are available in the forward-looking centers of the immediately preceding utterance Ui- 1. No constraints or rules are formulated, however, that account for anaphoric relationships which spread out over non-adjacent utteranc"
P97-1014,J86-3001,0,0.595101,"based discourse segments. In particular, the role of SHIFT-type transitions is examined from the perspective of whether they not only indicate a shift of the topic between two immediately successive utterances but also signal (intention-based) segment boundaries. The data in both studies reveal that only a weak correlation between the SHIFT transitions and segment boundaries can be observed. This finding precludes a reliable prediction of segment boundaries based on the occurrence of 1Our notion of referentialdiscourse segment should not be confounded with the intentional one originating from Grosz & Sidner (1986), for reasons discussed in Section 2. SHIFTS and vice versa. In order to accommodate to these empirical results divergent solutions are proposed. Passonneau suggests that the centering data structures need to be modified appropriately, while Walker concludes that the local centering data should be left as they are and further be complemented by a cache mechanism. She thus intends to extend the scope of centering in accordance with cognitively plausible limits of the attentional span. Walker, finally, claims that the content of the cache, rather than the intentional discourse segment structure,"
P97-1014,P94-1002,0,0.0171891,"h are competing models of the attentional state. Centered segmentation has also the additional advantage of restricting the search space of anaphoric antecedents to those discourse entities actually referred to in the discourse, while the cache model allows unrestricted retrieval in the main or long-term memory. Text segmentation procedures (more with an information retrieval motivation, rather than being related to reference resolution tasks) have also been proposed for a coarse-grained partitioning of texts into contiguous, nonoverlapping blocks and assigning content labels to these blocks (Hearst, 1994). The methodological basis of these studies are lexical cohesion indicators (Morris & Hirst, 1991) combined with word-level co-occurrence statistics. Since the labelling is one-dimensional, this approximates our use of preferred centers of discourse segments. These studies, however, lack the fine-grained information of the contents of Cf lists also needed for proper reference resolution. Finally, many studies on discourse segmentation highlight the role of cue words for signaling segment boundaries (cf., e.g., the discussion in Passonneau & Litman (1993)). However useful this strategy might be"
P97-1014,J91-1002,0,0.0120573,"onal advantage of restricting the search space of anaphoric antecedents to those discourse entities actually referred to in the discourse, while the cache model allows unrestricted retrieval in the main or long-term memory. Text segmentation procedures (more with an information retrieval motivation, rather than being related to reference resolution tasks) have also been proposed for a coarse-grained partitioning of texts into contiguous, nonoverlapping blocks and assigning content labels to these blocks (Hearst, 1994). The methodological basis of these studies are lexical cohesion indicators (Morris & Hirst, 1991) combined with word-level co-occurrence statistics. Since the labelling is one-dimensional, this approximates our use of preferred centers of discourse segments. These studies, however, lack the fine-grained information of the contents of Cf lists also needed for proper reference resolution. Finally, many studies on discourse segmentation highlight the role of cue words for signaling segment boundaries (cf., e.g., the discussion in Passonneau & Litman (1993)). However useful this strategy might be, we see the danger that such a surface-level description may actually hide structural regularitie"
P97-1014,P93-1020,0,0.10593,"blocks and assigning content labels to these blocks (Hearst, 1994). The methodological basis of these studies are lexical cohesion indicators (Morris & Hirst, 1991) combined with word-level co-occurrence statistics. Since the labelling is one-dimensional, this approximates our use of preferred centers of discourse segments. These studies, however, lack the fine-grained information of the contents of Cf lists also needed for proper reference resolution. Finally, many studies on discourse segmentation highlight the role of cue words for signaling segment boundaries (cf., e.g., the discussion in Passonneau & Litman (1993)). However useful this strategy might be, we see the danger that such a surface-level description may actually hide structural regularities at deeper levels of investigation illustrated by access mechanisms for centering data at different levels of discourse segmentation. 7 Acknowledgments. We like to thank our colleagues in the CLIF group for fruitful discussions and instant support, Joe Bush who polished the text as a native speaker, the three anonymous reviewers for their critical comments, and, in particular, Bonnie Webber for supplying invaluable comments to an earlier draft of this paper"
P97-1014,P95-1005,0,0.0394336,"Missing"
P97-1014,P96-1036,1,0.876436,"segment levels succeeds. (b) Close the embedded segment and open a new, parallel one: If none of the anaphoric expressions under consideration co-specify the IsReachable(ante, s, Ui ) if ante 6 C/(s, Ui-1) else if ante E C/(s - 1, Uosts_,.~,a]) else if (3v E N : ante =~tr Cp(v, UDsI.... a]) ^ v < ( s - 1)) A (-~Sv&apos; 6 N : ante A v < v&apos;) =,t,- else Cp(v&apos;,UDst~,.~ndl) Table 4: Reachability of the Anaphoric Antecedent Finally, the function Lift(s, i) (cf. Table 5) determines the appropriate discourse segment level, s, of an utter2The Cf lists in the functional centering model are totally ordered (Strobe & Hahn, 1996, p.272) and we here implicitly assume that they are accessed in the total order given. 106 C p ( 8 - 1, U[8_l.end]), then the entire C! at this segment level is checked for the given utterance. If an antecedent matches, the segment which contains Ui- 1 is ultimately closed, since Ui opens a parallel segment at the same level of embedding. Subsequent anaphora checks exclude any of the preceding parallel segments from the search for a valid antecedent and just visit the currently open one. (c) Open new, embedded segment: If there is no matching antecedent in hierarchically reachable segments, t"
P97-1014,P96-1000,0,0.602609,"nts on valid antecedents are placed by the global discourse structure previous utterances are embedded in. We want to emphasize from the beginning that our proposal considers only the referential properties underlying 104 2 Global Discourse Structure There have been only few attempts at dealing with the recognition and incorporation of discourse structure beyond the level of immediately adjacent utterances within the centering framework. Two recent studies deal with this topic in order to relate attentional and intentional structures on a larger scale of global discourse coherence. Passonneau (1996) proposes an algorithm for the generation of referring expressions and Walker (1996a) integrates centering into a cache model of attentional state. Both studies, among other things, deal with the supposition whether a correlation exists between particular centering transitions (which were first introduced by Brennan et al. (1987); cf. Table 1) and intentionbased discourse segments. In particular, the role of SHIFT-type transitions is examined from the perspective of whether they not only indicate a shift of the topic between two immediately successive utterances but also signal (intention-base"
P97-1014,J94-2006,0,0.270977,"could not find an embedding of more than seven levels. 6 Related Work There has always been an implicit relationship between the local perspective of centering and the global view of focusing on discourse structure (cf. the discussion in Grosz et al. (1995)). However, work establishing an explicit account of how both can be joined in a computational model has not been done so far. The efforts of Sidner (1983), e.g., have provided a variety of different focus data structures to be used for reference resolution. This multiplicity and the on-going growth of the number of different entities (cf. Suri & McCoy (1994)) mirrors an increase in explanatory constructs that we consider a methodological drawback to this approach because they can hardly be kept control of. Our model, due to its hierarchical nature implements a stack behavior that is also inherent to the above mentioned proposals. We refrain, however, from establishing a new data type (even worse, different types of stacks) that has to be managed on its own. There is no need for extra computations to determine the ""segment focus"", since that is implicitly given in the local centering data already available in our model. A recent attempt at introdu"
P97-1014,J96-2005,0,0.327174,"ormulated, however, that account for anaphoric relationships which spread out over non-adjacent utterances. Hence, it is unclear how discourse elements which appear in utterances preceding utterance Ui-1 are taken into consideration as potential antecedents for anaphoric expressions in Ui. The extension of the search space for antecedents is by no means a trivial enterprise. A simple linear backward search of all preceding centering structures, e.g., may not only turn out to establish illegal references but also contradicts the cognitive principles underlying the limited attention constraint (Walker, 1996b). The solution we propose starts from the observation that additional constraints on valid antecedents are placed by the global discourse structure previous utterances are embedded in. We want to emphasize from the beginning that our proposal considers only the referential properties underlying 104 2 Global Discourse Structure There have been only few attempts at dealing with the recognition and incorporation of discourse structure beyond the level of immediately adjacent utterances within the centering framework. Two recent studies deal with this topic in order to relate attentional and int"
P98-1079,C94-1061,1,0.93143,"an be considered as the instantiation and continuous filling 476 d~udrs,y~ trw ~Hyl~si~ j spaceHyputhcsis t spal.&apos;c-n I Q*mlifi~r ~,l~*Ine Q*mlity Figure 1: Architecture of the Text Learner of roles with respect to single concepts already available in the knowledge base. Under learning conditions, however, a set of alternative concept hypotheses has to be maintained for each unknown item, with each hypothesis denoting a newly created conceptual interpretation tentatively associated with the unknown item. The underlying methodology is summarized in Fig. 1. The text parser (for an overview, cf. BrSker et al. (1994)) yields information from the grammatical constructions in which an unknown lexical item (symbolized by the black square) occurs in terms of the corresponding dependency parse tree. The kinds of syntactic constructions (e.g., genitive, apposition, comparative), in which unknown lexical items appear, are recorded and later assessed relative to the credit they lend to a particular hypothesis. The conceptual interpretation of parse trees involving unknown lexical items in the domain knowledge base leads to the derivation of concept hypotheses, which are further enriched by conceptual annotations."
P98-1079,C92-2082,0,0.00456409,"dge of the domain the texts are about, and grammatical constructions in which unknown lexical items occur. While there may be many reasonable interpretations when an unknown item occurs for the very first time in a text, their number rapidly decreases when more and more evidence is gathered. Our model tries to make explicit the reasoning processes behind this learning pattern. Unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of large corpora of texts (Hindle, 1989; Zernik and Jacobs, 1990; Hearst, 1992; Manning, 1993), we propose here a knowledge-intensive model of concept learning from few, positive-only examples that is tightly integrated with the non-learning mode of text understanding. Both learning and understanding build on a given core ontology in the format of terminological assertions and, hence, make abundant use of terminological reasoning. The &apos;plain&apos; text understanding mode can be considered as the instantiation and continuous filling 476 d~udrs,y~ trw ~Hyl~si~ j spaceHyputhcsis t spal.&apos;c-n I Q*mlifi~r ~,l~*Ine Q*mlity Figure 1: Architecture of the Text Learner of roles with re"
P98-1079,P89-1015,0,0.0416932,"ources of evidence - - the prior knowledge of the domain the texts are about, and grammatical constructions in which unknown lexical items occur. While there may be many reasonable interpretations when an unknown item occurs for the very first time in a text, their number rapidly decreases when more and more evidence is gathered. Our model tries to make explicit the reasoning processes behind this learning pattern. Unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of large corpora of texts (Hindle, 1989; Zernik and Jacobs, 1990; Hearst, 1992; Manning, 1993), we propose here a knowledge-intensive model of concept learning from few, positive-only examples that is tightly integrated with the non-learning mode of text understanding. Both learning and understanding build on a given core ontology in the format of terminological assertions and, hence, make abundant use of terminological reasoning. The &apos;plain&apos; text understanding mode can be considered as the instantiation and continuous filling 476 d~udrs,y~ trw ~Hyl~si~ j spaceHyputhcsis t spal.&apos;c-n I Q*mlifi~r ~,l~*Ine Q*mlity Figure 1: Architectu"
P98-1079,P92-1053,0,0.0280858,"Missing"
P98-1079,J91-2002,0,0.0820092,"Missing"
P98-1079,C90-1005,0,0.0158696,"ence - - the prior knowledge of the domain the texts are about, and grammatical constructions in which unknown lexical items occur. While there may be many reasonable interpretations when an unknown item occurs for the very first time in a text, their number rapidly decreases when more and more evidence is gathered. Our model tries to make explicit the reasoning processes behind this learning pattern. Unlike the current mainstream in automatic linguistic knowledge acquisition, which can be characterized as quantitative, surface-oriented bulk processing of large corpora of texts (Hindle, 1989; Zernik and Jacobs, 1990; Hearst, 1992; Manning, 1993), we propose here a knowledge-intensive model of concept learning from few, positive-only examples that is tightly integrated with the non-learning mode of text understanding. Both learning and understanding build on a given core ontology in the format of terminological assertions and, hence, make abundant use of terminological reasoning. The &apos;plain&apos; text understanding mode can be considered as the instantiation and continuous filling 476 d~udrs,y~ trw ~Hyl~si~ j spaceHyputhcsis t spal.&apos;c-n I Q*mlifi~r ~,l~*Ine Q*mlity Figure 1: Architecture of the Text Learner of"
P98-1079,A92-1014,0,\N,Missing
P98-1079,P93-1032,0,\N,Missing
R19-1030,D18-1001,0,0.10908,"o early exceptions, cf. Rock (2001); Medlock (2006)). This na¨ıve perspective is beginning to change these days with the ever-growing importance of social media documents for text analytics. However, there are currently no systematic actions taken to hide personally sensitive information from down-stream applications when dealing with chat, blog, SMS or email raw data. Since this attitude also faces legal implications, a quest for the protection of individual data privacy has been raised and, in the meantime, finds active response in the most recent work of the NLP community (Li et al., 2018; Coavoux et al., 2018). 2 Related Work The main thrust of work on de-identification has been performed for clinical NLP.3 Most influential for progress in this field have been two challenge competitions within the context of the I 2 B 2 (Informatics for Integrating Biology & the Bedside) initiative4 which focused on 18 different types of Protected Health Information (PHI) categories as required by US legislation (HIPAA).5 The first of these challenge tasks was launched in 2006 for 889 hospital discharge summaries, with a total of 19,498 PHI instances of person-identifying verbal expressions (Uzuner et al., 2007). T"
R19-1030,W16-4204,0,0.0811928,"ing systems. Therefore, we also assessed frequency imbalances between different corpora: The original non-pseudonymized C ODE A LLTAGS+d corpus with hand-annotated pi entities (referred to as ORIG), the pseudonymized20 form of the original corpus (PSEUD) and pseudonymized20 C ODE A LLTAGS+d with automatically recognized pi entities (PSEUD PIR). For the latter, we retrieved the pi entities by training a model on 9/10 of ORIG and applying it to the unseen part on ten different folds. We used a system for recognizing privacybearing information (PIR) based on N EURO NER (Dernoncourt et al., 2017; Lee et al., 2016), with slight modifications of its neural network architecture from our side. Table 3 shows that the number of tokens declines for both pseudonymized corpora compared to the original corpus. Taking a closer look, the category discrepancy almost entirely results from the category ORG and, to a much lesser extent though, also from STREET and CITY names. We conclude that our substitution dictionaries obviously contain shorter names, i.e., entities consisting of fewer tokens. Contrasting ORIG and the automatically annotated PSEUD PIR, we witness an increase of pi entities. For one thing, this can"
R19-1030,P18-2005,0,0.063718,"Missing"
R19-1030,medlock-2006-introduction,0,0.605023,"lts of an evaluation study to assess the naturalness of these replacements with native speakers of German, as well as the performance of a recognizer for privacy-relevant text stretches on original and already pseudonymized data. Surprisingly, despite its high relevance for NLP operating on UGC, the topic of data privacy has long been neglected by the mainstream of NLP research. While it has always been of utmost importance for medical, i.e., clinical, NLP (Meystre, 2015), it has received almost no attention in NLP’s non-medical camp for a long time (for two early exceptions, cf. Rock (2001); Medlock (2006)). This na¨ıve perspective is beginning to change these days with the ever-growing importance of social media documents for text analytics. However, there are currently no systematic actions taken to hide personally sensitive information from down-stream applications when dealing with chat, blog, SMS or email raw data. Since this attitude also faces legal implications, a quest for the protection of individual data privacy has been raised and, in the meantime, finds active response in the most recent work of the NLP community (Li et al., 2018; Coavoux et al., 2018). 2 Related Work The main thru"
R19-1030,P16-1126,0,0.260595,"Missing"
tomanek-hahn-2008-approximating,J93-2004,0,\N,Missing
tomanek-hahn-2008-approximating,D07-1051,1,\N,Missing
tomanek-hahn-2008-approximating,W01-1605,0,\N,Missing
tomanek-hahn-2008-approximating,J96-1002,0,\N,Missing
tomanek-hahn-2008-approximating,W01-0710,0,\N,Missing
tomanek-hahn-2008-approximating,W04-3111,0,\N,Missing
tomanek-hahn-2008-approximating,W03-0419,0,\N,Missing
tomanek-hahn-2008-approximating,P96-1042,0,\N,Missing
tomanek-hahn-2008-approximating,J05-1004,0,\N,Missing
tomanek-hahn-2008-approximating,P00-1016,0,\N,Missing
tomanek-hahn-2008-approximating,hahn-etal-2008-semantic,1,\N,Missing
tomanek-hahn-2008-approximating,I08-1048,0,\N,Missing
tomanek-hahn-2008-approximating,W07-1502,1,\N,Missing
tomanek-hahn-2010-annotation,J93-2004,0,\N,Missing
tomanek-hahn-2010-annotation,D07-1051,1,\N,Missing
tomanek-hahn-2010-annotation,W07-1516,0,\N,Missing
tomanek-hahn-2010-annotation,W05-0619,0,\N,Missing
tomanek-hahn-2010-annotation,P96-1042,0,\N,Missing
tomanek-hahn-2010-annotation,P00-1016,0,\N,Missing
tomanek-hahn-2010-annotation,D08-1112,0,\N,Missing
W07-1028,W04-0711,0,0.0599876,"Missing"
W07-1502,P96-1042,0,0.722693,"all number of manually labeled examples. However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. Another approach to reducing the human labeling effort is active learning (AL) where the learner has direct influence on the examples to be manually labeled. In such a setting, those examples are taken for annotation which are assumed to be maximally useful for (classifier) training. AL approaches have already been tried for different NLP tasks (Engelson and Dagan, 1996; Hwa, 2000; Ngai and Yarowsky, 2000), though such studies usually report on simulations rather than on concrete experience with AL for real annotation efforts. In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. They conclude that one should rather invest human labor in annotation than in rule writing. Closer to our concerns is the study by Hachey et al. (2005) who apply AL to named entity (NE) annotation. There are some differences in the actual AL approach they chose"
W07-1502,W05-0619,0,0.043479,"aximally useful for (classifier) training. AL approaches have already been tried for different NLP tasks (Engelson and Dagan, 1996; Hwa, 2000; Ngai and Yarowsky, 2000), though such studies usually report on simulations rather than on concrete experience with AL for real annotation efforts. In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. They conclude that one should rather invest human labor in annotation than in rule writing. Closer to our concerns is the study by Hachey et al. (2005) who apply AL to named entity (NE) annotation. There are some differences in the actual AL approach they chose, while their main idea, viz. to apply committee-based AL to speed up real annotations, is comparable to our work. They report on negative side effects of AL on the annotations and state that AL annotations are cognitively more difficult for the annotators to deal with (because the sentences selected for annotation are more complex). 10 As a consequence, diminished annotation quality and higher per-sentence annotation times arise in their experiments. By and large, however, they conclu"
W07-1502,W00-1306,0,0.0498374,"eled examples. However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. Another approach to reducing the human labeling effort is active learning (AL) where the learner has direct influence on the examples to be manually labeled. In such a setting, those examples are taken for annotation which are assumed to be maximally useful for (classifier) training. AL approaches have already been tried for different NLP tasks (Engelson and Dagan, 1996; Hwa, 2000; Ngai and Yarowsky, 2000), though such studies usually report on simulations rather than on concrete experience with AL for real annotation efforts. In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. They conclude that one should rather invest human labor in annotation than in rule writing. Closer to our concerns is the study by Hachey et al. (2005) who apply AL to named entity (NE) annotation. There are some differences in the actual AL approach they chose, while the"
W07-1502,J93-2004,0,0.0318405,"ugh simulated and real-world annotations in the domain of immunogenetics for NE annotations. 1 Introduction The remarkable success of machine-learning methods for NLP has created, for supervised approaches at least, a profound need for annotated language corpora. Annotation of language resources, however, has become a bottleneck since it is performed, with some automatic support (pre-annotation) though, by humans. Hence, annotation is a time-costly and error-prone process. The demands for annotated language data is increasing at different levels. After the success in syntactic (Penn TreeBank (Marcus et al., 1993)) and propositional encodings (Penn PropBank (Palmer et al., 2005)), more sophisticated semantic data (such as temporal (Pustejovsky et al., 2003) or opinion annotations (Wiebe et al., 2005)) and discourse data Given this enormous need for high-quality annotations at virtually all levels the question turns up how to minimize efforts within an acceptable quality window. Currently, for most tasks several hundreds of thousands of text tokens (ranging between 200,000 to 500,000 text tokens) have to be scrutinized unless valid tagging judgments can be learned. While significant time savings have al"
W07-1502,W03-2117,0,0.0719586,"Missing"
W07-1502,P00-1016,0,0.292029,"es. However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. Another approach to reducing the human labeling effort is active learning (AL) where the learner has direct influence on the examples to be manually labeled. In such a setting, those examples are taken for annotation which are assumed to be maximally useful for (classifier) training. AL approaches have already been tried for different NLP tasks (Engelson and Dagan, 1996; Hwa, 2000; Ngai and Yarowsky, 2000), though such studies usually report on simulations rather than on concrete experience with AL for real annotation efforts. In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. They conclude that one should rather invest human labor in annotation than in rule writing. Closer to our concerns is the study by Hachey et al. (2005) who apply AL to named entity (NE) annotation. There are some differences in the actual AL approach they chose, while their main idea, viz. to appl"
W07-1502,J05-1004,0,0.0138647,"netics for NE annotations. 1 Introduction The remarkable success of machine-learning methods for NLP has created, for supervised approaches at least, a profound need for annotated language corpora. Annotation of language resources, however, has become a bottleneck since it is performed, with some automatic support (pre-annotation) though, by humans. Hence, annotation is a time-costly and error-prone process. The demands for annotated language data is increasing at different levels. After the success in syntactic (Penn TreeBank (Marcus et al., 1993)) and propositional encodings (Penn PropBank (Palmer et al., 2005)), more sophisticated semantic data (such as temporal (Pustejovsky et al., 2003) or opinion annotations (Wiebe et al., 2005)) and discourse data Given this enormous need for high-quality annotations at virtually all levels the question turns up how to minimize efforts within an acceptable quality window. Currently, for most tasks several hundreds of thousands of text tokens (ranging between 200,000 to 500,000 text tokens) have to be scrutinized unless valid tagging judgments can be learned. While significant time savings have already been reported on the basis of automatic pre-tagging (e.g., f"
W07-1502,W01-0501,0,0.0219148,"g on those items particularly relevant for the learning process. In Section 2, we review approaches to annotation cost reduction. We turn in Section 3 to the description of JANE, our AL-based annotation system, while in Section 4 we report on the experience we made using the AL component in NE annotations. 2 Related Work Reduction of efforts for training (semi-) supervised learners on annotated language data has always been an issue of concern. Semi-supervised learning provides methods to bootstrap annotated corpora from a small number of manually labeled examples. However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. Another approach to reducing the human labeling effort is active learning (AL) where the learner has direct influence on the examples to be manually labeled. In such a setting, those examples are taken for annotation which are assumed to be maximally useful for (classifier) training. AL approaches have already been tried for different NLP tasks (Engelson and Dagan, 1996; Hwa, 2000; Ngai and Yarowsky, 2000), though such studies usually re"
W07-1502,W04-1221,0,0.0124941,"ed entity mentions of this type. The entity density in our AL corpus is thus almost 15 times higher than in our GS corpus. Such a dense corpus is certainly much more appropriate for classifier training due to the tremendous increase of positive training instances. We observed comparable effects with other entity types as well, and thus conclude that the sparser entity mentions of a specific type are in texts, the more benefical AL-based annotation actually is. 10 The named enatity tagger used throughout in this section is based on Conditional Random Fields and similar to the one presented by (Settles, 2004). 14 4.2 Mind the Seed Set For AL, the sentences to be annotated in the first AL round, the seed set, have to be manually selected. As stated above, the proper choice of this set is crucial for efficient AL based annotation. One should definitely refrain from a randomly generated seed set 4.3 Portability of Corpora While we are working in the field of immunogenetics, the P ENN B IO IE corpus focuses on the subdomain of oncogenetics and provides a sound annota11 Variation events are not as sparse in P ENN B IO IE as, e.g., cytokine receptors in our subdomain. Actually, there is a variation enti"
W07-1502,W03-0419,0,0.0240228,"Missing"
W07-1502,J00-4005,0,0.0915308,"Missing"
W07-1502,D07-1051,1,\N,Missing
W07-1502,W01-1605,0,\N,Missing
W07-1505,brants-hansen-2002-developments,0,0.0214672,"nal) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation"
W07-1505,declerck-2006-synaf,0,0.0662362,"4), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation Compatibility Working Group (Meyers, 2006) began to concentrate its activities on the mutual compatibility of annotation schemata for, e.g., POS tagging, treebanking, role labeling, time"
W07-1505,C96-1079,0,0.0606984,"(cf. Figure 16) links annotated (named) entities to the ontologies and databases through appropriate attributes, viz. ontologyEntry and sdbEntry. The attribute specificType specifies the analyzed entity in a more detailed way (e.g., Organism can be specified through the species values ‘human’, ‘mouse’, ‘rat’, etc.) The subtypes are currently being developed in the bio-medical domain and cover, e.g., genes, pro39 teins, organisms, diseases, variations. This hierarchy can easily be extended or supplemented with entities from other domains. For illustration purposes, we extended it here by MUC (Grishman and Sundheim, 1996) entity types such as Person, Organization, etc. This scheme is still under construction and will soon also incorporate the representation of relationships between entities and domain-specific events. The general type Relation will then be extended with specific conceptual relations such as location, part-of, etc. The representation of events will be covered by a type which aggregates pre-defined relations between entities and the event mention. An event type such as InhibitionEvent would link the text spans in the sentence ‘protein A inhibits protein B’ in attributes agent (‘protein A’), pati"
W07-1505,W02-1706,0,0.0720681,"Missing"
W07-1505,ide-etal-2000-xces,0,0.0297293,"nres. The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web.4 For (computational) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in ter"
W07-1505,W03-0804,0,0.0538625,"those single modules which serve, by and large, the same functionality? Second, how can we build NLP systems by composing them, at the abstract level of functional specification, from these already existing component building blocks disregarding concrete implementation matters? Yet another burning issue relates to the increasing availability of multiple metadata annotations both in corpora and language processors. If alternative annotation tag sets are chosen for the same functional task a ‘data conversion’ problem is created which should be solved at the abstract specification level as well (Ide et al., 2003). Software engineering methodology points out that these requirements are best met by properly identifying input/output capabilities of constituent components and by specifying a general data model (e.g., based on UML (Rumbaugh et al., 1999)) in order to get rid of the low-level implementation (i.e., coding) layer. A particularly promising proposal along this line of thought is the Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) originating from IBM research activities.1 UIMA is but the latest attempt in a series of proposals concerned with more generic NLP e"
W07-1505,J93-2004,0,0.0430432,"gn of annotation schemata for language resources and their standardization have a long-standing tradition in the NLP community. In the very beginning, this work often focused exclusively on subdomains of text analysis such as document structure meta-information, syntactic or semantic analysis. The Text Encoding Initiative (TEI)2 provided schemata for the exchange of documents of various genres. The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web.4 For (computational) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations."
W07-1505,W06-0606,0,0.0289483,"n of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation Compatibility Working Group (Meyers, 2006) began to concentrate its activities on the mutual compatibility of annotation schemata for, e.g., POS tagging, treebanking, role labeling, time annotation, etc. The goal of these initiatives, however, has never been to design an annotation scheme for a complete 2 http://www.tei-c.org http://dublincore.org 4 http://www.w3.org/2001/sw 5 http://www.ilc.cnr.it/EAGLES96/ 6 http://bioie.ldc.upenn.edu 3 34 NLP pipeline as needed, e.g., for information extraction or text mining tasks (Hahn and Wermter, 2006). This lack is mainly due to missing standards for specifying comprehensive NLP software archi"
W07-1505,P05-1011,0,0.0136284,"ides the attribute parent, Constituent holds the attributes cat which stores the complex syntactic category of the current constituent (e.g., NP, VP), and head which links to the head word of the constituent. In order to account for multiple annotations in the constituent-based approach, we introduced corresponding constituent types which specialize Constituent. This parallels our approach which we advocate for alternatives in POS tagging and the management of alternative chunking results. Currently, the scheme supports three different constituent types, viz. PTBConstituent, GENIAConstituent (Miyao and Tsujii, 2005) and PennBIoIEConstituent. The attributes of the type PTBConstituent cover the complete repertoire of annotation items contained in the Penn Treebank, such as functional tags for form/function dicrepancies (formFuncDisc), grammatical role (gramRole), adverbials (adv) and miscellaneous tags (misc). The representation of null elements, topicalized elements and gaps with corresponding references to the lexicalized elements in a tree is reflected in attributes nullElement, tpc, map and ref, respectively. GENIAConstituent and PennBIoIEConstituent inherit from PTBConstituent all listed attributes an"
W07-1505,W06-2713,0,0.0542362,"Missing"
W07-1505,W06-2714,0,0.130035,"Missing"
W07-1505,doddington-etal-2004-automatic,0,\N,Missing
W07-1505,laprun-etal-2002-pratical,0,\N,Missing
W08-0507,N01-1008,0,0.0519831,"Missing"
W08-0507,P07-1086,0,0.0139302,"nguage. Besides this perspective on rich lexicological data, over the years a software infrastructure has emerged around W ORD N ET that was equally approved by the NLP community. This included, e.g., a lexicographic file generator, various editors and visualization tools but also meta tools relying on properly formated W ORD N ET data such as a library of similarity measures (Pedersen et al., 2004). In numerous articles the usefulness of this data and software ensemble has been demonstrated (e.g., for word sense disambiguation (Patwardhan et al., 2003), the analysis of noun phrase conjuncts (Hogan, 2007), or the resolution of coreferences (Harabagiu et al., 2001)). In our research on information extraction and text mining within the field of biomedical NLP, we similarly recognized an urgent need for a lexical resource comparable to W ORD N ET, both in scope and size. However, the direct usability of the original W ORD N ET for biomedical NLP is severely hampered by a (not so surprising) lack of coverage of the life sciences domain in the general-language English W ORD N ET as was clearly demonstrated by Burgun and Bodenreider (2001). Rather than building a B IOW ORD N ET by hand, as was done"
W08-0507,N04-3012,0,0.0153639,"cological richness in terms of definitions (glosses) and semantic relations, synonymy via synsets in particular, it has become a de facto standard for all sorts of research that rely on lexical content for the English language. Besides this perspective on rich lexicological data, over the years a software infrastructure has emerged around W ORD N ET that was equally approved by the NLP community. This included, e.g., a lexicographic file generator, various editors and visualization tools but also meta tools relying on properly formated W ORD N ET data such as a library of similarity measures (Pedersen et al., 2004). In numerous articles the usefulness of this data and software ensemble has been demonstrated (e.g., for word sense disambiguation (Patwardhan et al., 2003), the analysis of noun phrase conjuncts (Hogan, 2007), or the resolution of coreferences (Harabagiu et al., 2001)). In our research on information extraction and text mining within the field of biomedical NLP, we similarly recognized an urgent need for a lexical resource comparable to W ORD N ET, both in scope and size. However, the direct usability of the original W ORD N ET for biomedical NLP is severely hampered by a (not so surprising)"
W08-0507,W07-1028,1,0.814381,"ing, Testing, and Quality Assurance for Natural Language Processing, pages 31–39, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics etc.).2 Given these resources and their software infrastructure, our plan was to create a biomedically focused lexicological resource, the B IOW ORD N ET, whose coverage would exceed that of any of its component resources in a so far unprecedented manner. Only then, given such a huge combined resource advanced NLP tasks such as anaphora resolution seem likely to be tackled in a feasible way (Hahn et al., 1999; Casta˜no et al., 2002; Poprat and Hahn, 2007). In particular, we wanted to make direct use of available software infrastructure such as the library of similarity metrics without the need for re-programming and hence foster the reuse of existing software as is. We began our efforts on the assumption that the W ORD N ET software resources were stable and reliable. In the course of our work, it turned out that this belief was far too optimistic. We discuss the stumbling blocks that we encountered, point out an error in the W ORD N ET software with implications for research based on it, and conclude that building on the legacy of W ORD N ET"
W09-1305,W04-1204,0,0.0269125,"E NJU parser (Sagae et al., 2007). The E NJU parser generates predicateargument structures, and the system converts them into dependency structures. The system then analyzes the semantics of the sentences by matching syntactic-semantic patterns to the dependency structures. We constructed 1,123 patterns for the event extraction according to the following workflow. We first collected keywords related to gene regulation, from G ENE O NTOLOGY, I NTER P RO, W ORD N ET, and several papers about information extraction from biomedical literature (Hatzivassiloglou and Weng, 2002; Kim and Park, 2004; Huang et al., 2004). Then we collected subcategorization frames for each keyword and created patterns for the frames manually. Each pattern consists of a syntactic pattern and a semantic pattern. The syntactic patterns com39 ply with dependency structures. The system tries to match the syntactic patterns to the dependency structures of sentences in a bottom-up way, considering syntactic and semantic restrictions of syntactic patterns. Once a syntactic pattern is successfully matched to a sub-tree of the available dependency structure, its corresponding semantic pattern is assigned to the sub-tree as one of its s"
W09-1305,P07-1079,0,0.106613,"lled by a shared promoter and which thus express together. We have mapped the operon names to corresponding gene sets. Named entity recognition relies on the use of dictionaries. If the system recognizes an operon name, it then associates the operon with its genes. The system further recognizes multi-gene object names (e.g., “acrAB”), divides them into individual gene names (e.g., “acrA”, “acrB”) and associates the gene names with the multi-gene object names. Relation Identification. The system then identifies syntactic structures of sentences in an input corpus by utilizing the E NJU parser (Sagae et al., 2007). The E NJU parser generates predicateargument structures, and the system converts them into dependency structures. The system then analyzes the semantics of the sentences by matching syntactic-semantic patterns to the dependency structures. We constructed 1,123 patterns for the event extraction according to the following workflow. We first collected keywords related to gene regulation, from G ENE O NTOLOGY, I NTER P RO, W ORD N ET, and several papers about information extraction from biomedical literature (Hatzivassiloglou and Weng, 2002; Kim and Park, 2004; Huang et al., 2004). Then we colle"
W09-1305,P04-1025,0,0.163279,"Missing"
W09-1403,W08-0601,0,0.0366951,"Missing"
W09-1403,W04-1204,0,0.0252013,"g instances of semantic classes such as proteins, diseases, or drugs. For a couple of years, this focus has been complemented by analytics dealing with relation extraction, i.e., finding instances of relations which link one or more (usually two) arguments, the latter being instances of semantic classes, such as the interaction between two proteins (PPIs). PPI extraction is a complex task since cascades of molecular events are involved which are hard to sort out. Many different approaches have already been tried – pattern-based ones (e.g., by Blaschke et al. (1999), Hakenberg et al. (2005) or Huang et al. (2004)), rule-based ones (e.g., by Yakushiji et al. ˇ c et al. (2004) or Fundel et al. (2007)), (2001), Sari´ and machine learning-based ones (e.g., by Katrenko and Adriaans (2006), Sætre et al. (2007) or Airola et al. (2008)), yet without conclusive results. In the following, we present our approach to solve Task 1 within the “BioNLP’09 Shared Task on Event Extraction”.1 Task 1 “Event detection and characterization” required to determine the intended relation given a priori supplied protein annotations. Our approach considers dependency graphs as the central data structure on which various trimming"
W09-1403,D07-1111,0,0.0149066,"Binding event with two arguments only for triples (trigger, protein1 , protein2 ). For the third Level, we create for each event trigger and its associated arguments e = n × m events, for n C AUSE arguments and m T HEME arguments. 12 For Binding we extracted the shortest path between two protein mentions if we encounter a triple (trigger, protein1 , protein2 ). 4 Pipeline The event extraction pipeline consists of two major parts, a pre-processor and the dedicated event extractor. As far as pre-processing is concerned, we imported the sentence splitting, tokenization and GDep parsing results (Sagae and Tsujii, 2007) as prepared by the shared task organizers for all data sets (training, development and test). We processed this data with the OpenNLP POS tagger and Chunker, both re-trained on the GENIA corpus (Buyko et al., 2006). Additionally, we enhanced the original tokenization by one which includes hyphenization of lexical items such as in “PMA-dependent”. 13 The data was further processed with the gene normalizer G E N O(Wermter et al., 2009) and a number of regex- and dictionary-based entity taggers (covering promoters, binding sites, and transcription factors). We also enriched gene name mentions wi"
W09-1403,P04-1025,0,0.0247848,"s. For a couple of years, this focus has been complemented by analytics dealing with relation extraction, i.e., finding instances of relations which link one or more (usually two) arguments, the latter being instances of semantic classes, such as the interaction between two proteins (PPIs). PPI extraction is a complex task since cascades of molecular events are involved which are hard to sort out. Many different approaches have already been tried – pattern-based ones (e.g., by Blaschke et al. (1999), Hakenberg et al. (2005) or Huang et al. (2004)), rule-based ones (e.g., by Yakushiji et al. ˇ c et al. (2004) or Fundel et al. (2007)), (2001), Sari´ and machine learning-based ones (e.g., by Katrenko and Adriaans (2006), Sætre et al. (2007) or Airola et al. (2008)), yet without conclusive results. In the following, we present our approach to solve Task 1 within the “BioNLP’09 Shared Task on Event Extraction”.1 Task 1 “Event detection and characterization” required to determine the intended relation given a priori supplied protein annotations. Our approach considers dependency graphs as the central data structure on which various trimming operations are performed involving syntactic simplification bu"
W09-1902,W05-0619,0,0.0305953,"ally bad seed set. MIN serves to demonstrate the opposite case. For each type of seed set, we sampled ten independent versions to calculate averages over several AL runs. 13 Cost measure The success of AL is usually measured as reduction of annotation effort according to some cost measure. Traditionally, the most common cost measure considers a unit cost per annotated token, which favors AL systems that select individual tokens. In a real annotation setting, however, it is unnatural, and therefore hard for humans to annotate single, possibly isolated tokens, leading to bad annotation quality (Hachey et al., 2005; Ringger et al., 2007). When providing context, the question arises whether the annotator can label several tokens present in the context (e.g., an entire multi-token entity or even the whole sentence) at little more cost than annotating a single token. Thus, assigning a linear cost of n to a sentence where n is the sentence’s length in tokens seems to unfairly disadvantage sentenceselection AL setups. However, more work is needed to find a more realistic cost measure. At present there is no other generally accepted cost measure than unit cost per token, so we report costs using the token mea"
W09-1902,W04-3111,0,0.017281,"f neighboring tokens to the left and right of the current token. 3.2 Data sets We used three data sets in our experiments. Two of them (ACE and PB IO) are standard data sets. The third (S YN) is a synthetic set constructed to have specific characteristics. For simplicity, we consider only scenarios with two entity classes, a majority class (MAJ) and a minority class (MIN). We discarded all other entity annotations originally contained in the corpus assigning the OUTSIDE class.2 The first data set (PB IO) is based on the annotations of the P ENN B IO IE corpus for biomedical entity extraction (Kulick et al., 2004). As P ENN B IO IE makes fine-grained and subtle distinctions between various subtypes of classes irrelevant for this study, we combined several of the original classes into two entity classes: The majority class consists of the three original classes ‘gene-protein’, ‘gene-generic’, and ‘gene-rna’. The minority class consists of the original and similar classes ‘variation-type’ and 2 The OUTSIDE class marks that a token is not part of an named entity. 12 ’variation-event’. All other entity labels were replaced by the OUTSIDE class. The second data set (ACE) is based on the newswire section of"
W09-1902,W07-1516,0,0.274433,"equence learning problems including, e.g., POS tagging, and named entity recognition. Sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences. Algorithms for sequence learning obviously work on sequence data, so respective AL approaches need to select complete sequences instead of single text tokens (Settles and Craven, 2008). Furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (Ringger et al., 2007; Tomanek et al., 2007). Within such sequences, instances of different classes often co-occur. Thus, an active learner that selects uncertain examples of one class gets examples of a second class as an unintended, yet positive side effect. We call this the co-selection effect. As a result, AL for sequence labeling is not “pure” exploitative AL, but implicitly comprises an exploratory aspect which can substantially reduce the missed class problem. In scenarios where we cannot hope for such a co-selection, we are much more likely to have decreased AL performance due to missed clusters or classes"
W09-1902,D08-1112,0,0.0611269,"ce, it may mistake the instances of Xi and Xj before it has acquired enough information to discriminate between them. So, under certain situations similarity of classes can mitigate the missed class effect. 11 The co-selection effect Many NLP tasks are sequence learning problems including, e.g., POS tagging, and named entity recognition. Sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences. Algorithms for sequence learning obviously work on sequence data, so respective AL approaches need to select complete sequences instead of single text tokens (Settles and Craven, 2008). Furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (Ringger et al., 2007; Tomanek et al., 2007). Within such sequences, instances of different classes often co-occur. Thus, an active learner that selects uncertain examples of one class gets examples of a second class as an unintended, yet positive side effect. We call this the co-selection effect. As a result, AL for sequence labeling is not “pure” exploitative AL, but"
W09-1902,D07-1051,1,0.869461,"ems including, e.g., POS tagging, and named entity recognition. Sequences are consecutive text tokens constituting linguistically plausible chunks, e.g., sentences. Algorithms for sequence learning obviously work on sequence data, so respective AL approaches need to select complete sequences instead of single text tokens (Settles and Craven, 2008). Furthermore, sentence selection has been preferred over token selection in other works with the argument that the manual annotation of single, possibly isolated tokens is almost impossible or at least extremely time-consuming (Ringger et al., 2007; Tomanek et al., 2007). Within such sequences, instances of different classes often co-occur. Thus, an active learner that selects uncertain examples of one class gets examples of a second class as an unintended, yet positive side effect. We call this the co-selection effect. As a result, AL for sequence labeling is not “pure” exploitative AL, but implicitly comprises an exploratory aspect which can substantially reduce the missed class problem. In scenarios where we cannot hope for such a co-selection, we are much more likely to have decreased AL performance due to missed clusters or classes. 3 Experiments We ran"
W09-3018,P08-2017,0,0.0689635,"Missing"
W10-1838,W04-1221,0,0.028463,"ecision, and F-score values (both for the exact and overlap condition) are shared by the same parameter combinations which also performed best in Section 4.1. Hence, the use of a named entity tagger supports the evaluation results when comparing the various 4.2 Extrinsic Calibration of Parameters We employed a standard named entity tagger to assess the impact of the different merging strategies on a scenario near to a real-world application.12 12 This tagger is based on Conditional Random Fields (Lafferty et al., 2001) and employs a standard feature set used for biomedical entity recognition (Settles, 2004). 240 ACC 0.95 0.92 0.95 0.92 exactR 0.62 0.22 0.55 0.30 exactP 0.69 0.69 0.75 0.85 exactF 0.65 0.34 0.63 0.45 overlapR 0.76 0.26 0.67 0.34 overlapP 0.85 0.81 0.91 0.94 overlapF 0.81 0.39 0.78 0.50 systems SYS-1 + SYS-3 SYS-2 + SYS-5 SYS-3 + SYS-4 SYS-4 + SYS-5 threshold 0.20 0.60 0.20 0.60 Table 4: Twin pairs of taggers, contrasting the two best (in bold face) and the two worst performing pairs obtained by the confidence method. method cosine cosine cosine cosine cosine cosine cosine cosine cosine cosine confidence confidence confidence confidence ACC 0.94 0.93 0.94 0.93 0.94 0.93 0.94 0.93 0"
W10-1838,D07-1051,1,0.879072,"Missing"
W16-0423,baccianella-etal-2010-sentiwordnet,0,0.014835,"motion describes a much more complex type of affective state typically associated with phenomena such as sadness, fear or joy. Yet its exact definition and distinction from other affective phenomena is an open issue (Munezero et al., 2014). The more complex nature of emotion implies that both subtasks—sentiment analysis and emotion detection—need distinct analytic resources, especially lexicons. The number of general-language sentiment lexicons is, compared to the number of emotion lexicons, relatively large, including wellknown resources such as S ENTI W ORD N ET (Esuli and Sebastiani, 2006; Baccianella et al., 2010). Another notable resource is W ORD N ET-A FFECT which contains both, sentiment assessments (positive, negative, neutral and ambiguous) and a hierarchy of various emotion categories (Strapparava and Valitutti, 2004; Strapparava et al., 2006). Lately, however, an increasing number of emotion lexicons have been developed—within the fields of NLP (Mohammad and Turney, 2013; Staiano and Guerini, 2014), as well as cognitive psychology (Bestgen and Vincze, 2012; Warriner et al., 2013). As far as studies of sentiment in the economic area are concerned, Loughran and McDonald (2011) adapted the Harvard"
W16-0423,P07-1124,0,0.0428319,"le benefits of the additional information emotion detection might contribute in contrast to (less expressive) sentiment analysis. In a similar vein, G´en´ereux et al. (2011) investigate the impact of financial news items on the stock price of companies. They treat short financial news snippets about companies as if they were carrying implicit sentiment about the future market direction made explicit by the vocabulary they employ. They investigate how this sentiment vocabulary can be automatically extracted from texts and subsequently be used for classification. This resembles previous work by Devitt and Ahmad (2007) who explored a computable metric of positive or negative polarity in financial news text which is consistent with human judgments and can be used in a quantitative analysis of news sentiment impact on financial markets. Unlike sentiment analysis of informal communication pieces from social or mass media material, Kogan et al. (2009) were the first in company-centric text analysis to focus on official statements from enterprises on a larger scale (pioneering work was conducted by Kloptchenko et al. (2004), as mentioned above). They built up the 10-K Corpus, a collection of 54,379 annual busine"
W16-0423,esuli-sebastiani-2006-sentiwordnet,0,0.0346857,"like (bipolar) sentiments, emotion describes a much more complex type of affective state typically associated with phenomena such as sadness, fear or joy. Yet its exact definition and distinction from other affective phenomena is an open issue (Munezero et al., 2014). The more complex nature of emotion implies that both subtasks—sentiment analysis and emotion detection—need distinct analytic resources, especially lexicons. The number of general-language sentiment lexicons is, compared to the number of emotion lexicons, relatively large, including wellknown resources such as S ENTI W ORD N ET (Esuli and Sebastiani, 2006; Baccianella et al., 2010). Another notable resource is W ORD N ET-A FFECT which contains both, sentiment assessments (positive, negative, neutral and ambiguous) and a hierarchy of various emotion categories (Strapparava and Valitutti, 2004; Strapparava et al., 2006). Lately, however, an increasing number of emotion lexicons have been developed—within the fields of NLP (Mohammad and Turney, 2013; Staiano and Guerini, 2014), as well as cognitive psychology (Bestgen and Vincze, 2012; Warriner et al., 2013). As far as studies of sentiment in the economic area are concerned, Loughran and McDonald"
W16-0423,W14-2620,0,0.0183687,") finance-specific sentiment dictionary. The models they learn suggest strong correlations between financial sentiment words and risk in terms of stock return volatility. Using basically the same experimental set-up as the former study, H´ajek et al. (2013) go one step further and demonstrate that by combining qualitative sentiment information of annual reports with quantitative financial indicators (e.g. market capitalization, profitability, etc.) the resulting stock price forecasting model is more accurate than using quantitative indicators alone. These findings are further supported by 149 Kazemian et al. (2014) who present empirical data which indicate to act cautiously with respect to stock trading strategies based on sentiment analysis of linguistic data only (they propose to consider actual market returns, in addition). Interestingly, the study by H´ajek et al. (2013) points out that the change in sentiment (rather than its specific value in some point of time) seems to be an important determinant of stock price development in the long run. 3 Experimental Set-up In order to test the assumption whether enterprises can be attributed an emotional status as part of their identity as social actors, we"
W16-0423,N09-1031,0,0.104618,"licit sentiment about the future market direction made explicit by the vocabulary they employ. They investigate how this sentiment vocabulary can be automatically extracted from texts and subsequently be used for classification. This resembles previous work by Devitt and Ahmad (2007) who explored a computable metric of positive or negative polarity in financial news text which is consistent with human judgments and can be used in a quantitative analysis of news sentiment impact on financial markets. Unlike sentiment analysis of informal communication pieces from social or mass media material, Kogan et al. (2009) were the first in company-centric text analysis to focus on official statements from enterprises on a larger scale (pioneering work was conducted by Kloptchenko et al. (2004), as mentioned above). They built up the 10-K Corpus, a collection of 54,379 annual business reports (from 10,492 different publicly traded companies) published over the period from 1996 to 2006. They exploited this corpus to predict the volatility of stock returns, an established empirical measure of financial risk, using regression models. The economic assessments are derived from the distribution of unigrams and bigram"
W16-0423,strapparava-etal-2006-affective,0,0.0239544,"The more complex nature of emotion implies that both subtasks—sentiment analysis and emotion detection—need distinct analytic resources, especially lexicons. The number of general-language sentiment lexicons is, compared to the number of emotion lexicons, relatively large, including wellknown resources such as S ENTI W ORD N ET (Esuli and Sebastiani, 2006; Baccianella et al., 2010). Another notable resource is W ORD N ET-A FFECT which contains both, sentiment assessments (positive, negative, neutral and ambiguous) and a hierarchy of various emotion categories (Strapparava and Valitutti, 2004; Strapparava et al., 2006). Lately, however, an increasing number of emotion lexicons have been developed—within the fields of NLP (Mohammad and Turney, 2013; Staiano and Guerini, 2014), as well as cognitive psychology (Bestgen and Vincze, 2012; Warriner et al., 2013). As far as studies of sentiment in the economic area are concerned, Loughran and McDonald (2011) adapted the Harvard Psychosociological Dictionary to better fit the word usage of the finance domain. The resulting resource comprises six word lists (two of which refer to positive and negative words), thus forming a finance-specific sentiment lexicon. Resear"
W16-0423,I13-1097,0,0.018454,"rprises on a larger scale (pioneering work was conducted by Kloptchenko et al. (2004), as mentioned above). They built up the 10-K Corpus, a collection of 54,379 annual business reports (from 10,492 different publicly traded companies) published over the period from 1996 to 2006. They exploited this corpus to predict the volatility of stock returns, an established empirical measure of financial risk, using regression models. The economic assessments are derived from the distribution of unigrams and bigrams in the reports incorporating TF and TF-IDFbased measures for a bag-of-word (BOW) model. Wang et al. (2013), in a follow-up study, reused the 10-K Corpus by incorporating Loughran and McDonald’s (2011) finance-specific sentiment dictionary. The models they learn suggest strong correlations between financial sentiment words and risk in terms of stock return volatility. Using basically the same experimental set-up as the former study, H´ajek et al. (2013) go one step further and demonstrate that by combining qualitative sentiment information of annual reports with quantitative financial indicators (e.g. market capitalization, profitability, etc.) the resulting stock price forecasting model is more ac"
W16-0423,strapparava-valitutti-2004-wordnet,0,\N,Missing
W16-0423,P14-2070,0,\N,Missing
W16-2114,W14-4508,0,0.0208689,"d during optimization. While these commonly lead to very similar performance (LeCun et al., 2015), they cause different representations in the course of repeated experiments. Approaches to modelling changes of lexical semantics not using neural language models, e.g., Wijaya and Yeniterzi (2011), Gulordava and Baroni (2011), Mihalcea and Nastase (2012), Riedl et al. (2014) or Jatowt and Duh (2014) are, intentionally, out of the scope of this paper. In the same way, we here refrain from comparison with computational studies dealing with literary discussions related to the Romantic period (e.g., Aggarwal et al. (2014)). 108 107 1850 1900 year 1950 2000 Figure 1: Number of 5-grams per year (on the logarithmic y-axis) contained in the English fiction part of the G OOGLE B OOKS N GRAM corpus. The horizontal line indicates a constant sampling size of 10M 5-grams according to the Kim protocol. We use the P YTHON-based G ENSIM1 implementation of word2vec for our experiments; the relevant code is made available via G IT H UB.2 Due to the 5-gram nature of the corpus, a context window covering four neighboring words is used for all experiments. Only words with at least 10 occurrences in a sample are modeled. Traini"
W16-2114,W11-2508,0,0.179875,"chical softmax, uses a binary tree to efficiently represent the vocabulary, while the other, negative sampling, works by updating only a limited number of word vectors during each training step. Furthermore, artificial neural networks, in general, are known for a large number of local optima encountered during optimization. While these commonly lead to very similar performance (LeCun et al., 2015), they cause different representations in the course of repeated experiments. Approaches to modelling changes of lexical semantics not using neural language models, e.g., Wijaya and Yeniterzi (2011), Gulordava and Baroni (2011), Mihalcea and Nastase (2012), Riedl et al. (2014) or Jatowt and Duh (2014) are, intentionally, out of the scope of this paper. In the same way, we here refrain from comparison with computational studies dealing with literary discussions related to the Romantic period (e.g., Aggarwal et al. (2014)). 108 107 1850 1900 year 1950 2000 Figure 1: Number of 5-grams per year (on the logarithmic y-axis) contained in the English fiction part of the G OOGLE B OOKS N GRAM corpus. The horizontal line indicates a constant sampling size of 10M 5-grams according to the Kim protocol. We use the P YTHON-based"
W16-2114,P12-3029,0,0.0614896,"its predecessor, and, alternatively, independent training with a mapping between models for different points in time (Kulkarni et al., 2015). A comparison between these two protocols, 111 Proceedings of the 10th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH), pages 111–117, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics 3 Experimental Set-up For comparability with earlier studies (Kim et al., 2014; Kulkarni et al., 2015), we use the fiction part of the G OOGLE B OOKS N GRAM corpus (Michel et al., 2011; Lin et al., 2012). This part of the corpus is also less affected by sampling irregularities than other parts (Pechenick et al., 2015). Due to the opaque nature of G OOGLE’s corpus acquisition strategy, the influence of OCR errors on our results cannot be reasonably estimated, yet we assume that they will affect all experiments in an equal manner. The wide range of experimental parameters described in Section 2 makes it virtually impossible to test all their possible combinations, especially as repeated experiments are necessary to probe a method’s reliability. We thus concentrate on two experimental protocols—"
W16-2114,P12-2051,0,0.0315498,"tree to efficiently represent the vocabulary, while the other, negative sampling, works by updating only a limited number of word vectors during each training step. Furthermore, artificial neural networks, in general, are known for a large number of local optima encountered during optimization. While these commonly lead to very similar performance (LeCun et al., 2015), they cause different representations in the course of repeated experiments. Approaches to modelling changes of lexical semantics not using neural language models, e.g., Wijaya and Yeniterzi (2011), Gulordava and Baroni (2011), Mihalcea and Nastase (2012), Riedl et al. (2014) or Jatowt and Duh (2014) are, intentionally, out of the scope of this paper. In the same way, we here refrain from comparison with computational studies dealing with literary discussions related to the Romantic period (e.g., Aggarwal et al. (2014)). 108 107 1850 1900 year 1950 2000 Figure 1: Number of 5-grams per year (on the logarithmic y-axis) contained in the English fiction part of the G OOGLE B OOKS N GRAM corpus. The horizontal line indicates a constant sampling size of 10M 5-grams according to the Kim protocol. We use the P YTHON-based G ENSIM1 implementation of wo"
W16-2114,N13-1090,0,0.509022,"on the one hand, and, on the other hand, shifts in the meaning of already existing lexical items, a process which usually takes place over larger periods of time. Tracing semantic changes of the latter type is the main focus of our research. Meaning shift has recently been investigated with emphasis on neural language models (Kim et al., 2014; Kulkarni et al., 2015). This work is based on the assumption that the measurement of semantic change patterns can be reduced to the measurement of lexical similarity between lexical items. Neural language models, originating from the word2vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), are currently considered as state-of-the-art solutions for implementing this assumption (Schnabel et al., 2015). Within this approach, changes in similarity relations between lexical items at two different points of time are interpreted as a signal for meaning shift. Accordingly, lexical items which are very similar to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time. Both techniques were already combined in prior work to show, e.g., the increasing association of the lexical item “gay” with"
W16-2114,L16-1421,0,0.0709634,"old trade-off between training time, reliability and accuracy. It would also be interesting to replicate our experiments for other languages or points in time. Yet, the enormous corpus size for more recent years might require a reduced number of maximum epochs for these experiments. In order to improve the semantic modeling itself one could lemmatize the training material or utilize the part of speech annotations provided in the latest version of the G OOGLE corpus (Lin et al., 2012). Also, recently available neural language models with support for multiple word senses (Bartunov et al., 2016; Panchenko, 2016) could be helpful, since semantic changes can often be described as changes in the usage frequency of different word senses (Rissanen, 2008, pp.58–59). Finally, it is clearly important to test the effect of our proposed changes, based on synchronic experiments, on a system for tracking diachronic changes in word semantics. Acknowledgments This research was conducted within the Research Training Group “The Romantic Model. Variation – Scope – Relevance” (http:// www.modellromantik.uni-jena.de/) supported by grant GRK 2041/1 from the Deutsche Forschungsgemeinschaft (DFG). The first author (J.H.)"
W16-2114,riedl-etal-2014-distributed,0,0.0616677,"o less affected by sampling irregularities than other parts (Pechenick et al., 2015). Due to the opaque nature of G OOGLE’s corpus acquisition strategy, the influence of OCR errors on our results cannot be reasonably estimated, yet we assume that they will affect all experiments in an equal manner. The wide range of experimental parameters described in Section 2 makes it virtually impossible to test all their possible combinations, especially as repeated experiments are necessary to probe a method’s reliability. We thus concentrate on two experimental protocols—the one described by Kim et al. (2014) (referred to as Kim protocol) and the one from Kulkarni et al. (2015) (referred to as Kulkarni protocol), including close variations thereof. Kulkarni’s protocol operates on all 5grams occurring during five consecutive years (e.g., 1900–1904) and trains models independently of each other. Kim’s protocol operates on uniformly sized samples of 10M 5-grams for each year from 1850 onwards in a continuous fashion (years before 1900 are used for initialization only). Its constant sampling sizes result in both oversampling and undersampling as is evident from Figure 1. 109 number of 5-grams such as"
W16-2114,D15-1036,0,0.28195,"me. Tracing semantic changes of the latter type is the main focus of our research. Meaning shift has recently been investigated with emphasis on neural language models (Kim et al., 2014; Kulkarni et al., 2015). This work is based on the assumption that the measurement of semantic change patterns can be reduced to the measurement of lexical similarity between lexical items. Neural language models, originating from the word2vec algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), are currently considered as state-of-the-art solutions for implementing this assumption (Schnabel et al., 2015). Within this approach, changes in similarity relations between lexical items at two different points of time are interpreted as a signal for meaning shift. Accordingly, lexical items which are very similar to the lexical item under scrutiny can be considered as approximating its meaning at a given point in time. Both techniques were already combined in prior work to show, e.g., the increasing association of the lexical item “gay” with the meaning dimension of “homosexuality” (Kim et al., 2014; Kulkarni et al., 2015). We here investigate the accuracy and reliability of such similarity judgment"
W16-2114,W14-2517,0,\N,Missing
W16-4008,H05-1073,0,0.488757,"affective lexicon and jointly adapting it to prior language stages. We automatically construct a lexicon for word-emotion association of 18th and 19th century German which is then validated against expert ratings. Subsequently, this resource is used to identify distinct emotional patterns and trace long-term emotional trends in different genres of writing spanning several centuries. 1 Introduction For more than a decade, computational linguists have endeavored to decode affective information1 from textual documents, such as personal value judgments or emotional tone (Turney and Littman, 2003; Alm et al., 2005). Despite the achievements made so far, the majority of work in this area is limited in at least two ways. First, employing simple positive-negative polarity schemes fails to account for the diversity of affective reactions (Sander and Scherer, 2009) and, second, in contrast to the humanities where numerous contributions focus on emotion expression and elicitation (Corngold, 1998), very little work has been conducted by computational linguists to unravel affective information in historical sources. Arguably the main problem here relates to the availability of language resources for detecting a"
W16-4008,bestgen-2008-building,0,0.709484,"3 lexical entries) and replicates A NEW’s bian, 1977). methodology very closely (see K¨oper and Schulte im Walde (2016) for a more complete overview of German VAD resources). As the manual creation of affective lexicons (polarity or VAD) is expensive, their automatic extension is an active field of research since many years (Turney and Littman, 2003; Rosenthal et al., 2015). Typically, unlabeled words are attributed affective values given a set of seed words with known affect association, as well as similarity scores between seed and unlabeled words. Concerning emotions in VAD representation, Bestgen (2008) presented an algorithm based upon a k-Nearest-Neighbor methodology which expands the original lexicon by a factor of 17 (Bestgen and Vincze, 2012). Cook and Stevenson (2010) were the first to induce a polarity lexicon for non-contemporary language from historical corpora by employing a pointwise mutual information (PMI) metric to determine word similarity and the much received algorithm by Turney and Littman (2003) for polarity induction. PMI is, like latent semantic analysis (LSA; Deerwester et al. (1990)), an early form of distributional semantics which, in the meantime, has been replaced b"
W16-4008,cook-stevenson-2010-automatically,0,0.732896,"language stages (19th century and earlier) can no longer be recruited for data annotation. Prior work aiming to detect affect in historical text ignored this problem and relied on contemporary language resources instead (Acerbi et al., 2013; Bentley et al., 2014). Using word embeddings, we tackle this problem by jointly adapting a contemporary affective lexicon to historical language and expanding it in size. Collecting ratings from historical language experts, we successfully validate our method against human judgment. In contrast to previous work based on the categorical notion of polarity (Cook and Stevenson, 2010), we employ the more expressive dimensional Valence-Arousal-Dominance (VAD; Bradley and Lang (1994)) model of affect, instead. As a proof of concept, we apply this method to a collection of historical German texts, the main corpus of the ’Deutsches Textarchiv’ (DTA) [German Text Archive], in order to demonstrate the adequacy of our approach. Our data indicate that, at least for historical texts, academic writing and belles lettres, as well as respective subgenres, strongly differ in their use of affective language. Furthermore, we find statistically significant affect change patterns between 1"
W16-4008,P16-1141,0,0.0408259,"rical corpora by employing a pointwise mutual information (PMI) metric to determine word similarity and the much received algorithm by Turney and Littman (2003) for polarity induction. PMI is, like latent semantic analysis (LSA; Deerwester et al. (1990)), an early form of distributional semantics which, in the meantime, has been replaced by singular value decomposition with positive pointwise mutual information (SVDPPMI ; Levy et al. (2015)) and skip-gram negative sampling (SGNS; Mikolov et al. (2013)). Quite recently, evidence is available that the latter behaves more robust than the former (Hamilton et al., 2016). Most prior studies covering long time spans (e.g., Acerbi et al. (2013)) rely on the Google Books Ngram corpus (GBN; Michel et al. (2011), Lin et al. (2012)). However, this corpus might be problematic for Digital Humanities research because of digitization artifacts and its opaque and unbalanced sampling (Pechenick et al., 2015; Koplenig, 2016). For German, we use the DTA2 (Geyken, 2013; Jurish, 2013), which consists of books transcribed with double-keying and selected for their representativeness. The DTA aims for genre balance and provides a range of metadata for each document, e.g., autho"
W16-4008,L16-1413,0,0.100884,"Missing"
W16-4008,Q15-1016,0,0.0319412,"original lexicon by a factor of 17 (Bestgen and Vincze, 2012). Cook and Stevenson (2010) were the first to induce a polarity lexicon for non-contemporary language from historical corpora by employing a pointwise mutual information (PMI) metric to determine word similarity and the much received algorithm by Turney and Littman (2003) for polarity induction. PMI is, like latent semantic analysis (LSA; Deerwester et al. (1990)), an early form of distributional semantics which, in the meantime, has been replaced by singular value decomposition with positive pointwise mutual information (SVDPPMI ; Levy et al. (2015)) and skip-gram negative sampling (SGNS; Mikolov et al. (2013)). Quite recently, evidence is available that the latter behaves more robust than the former (Hamilton et al., 2016). Most prior studies covering long time spans (e.g., Acerbi et al. (2013)) rely on the Google Books Ngram corpus (GBN; Michel et al. (2011), Lin et al. (2012)). However, this corpus might be problematic for Digital Humanities research because of digitization artifacts and its opaque and unbalanced sampling (Pechenick et al., 2015; Koplenig, 2016). For German, we use the DTA2 (Geyken, 2013; Jurish, 2013), which consists"
W16-4008,S15-2078,0,0.0470255,"he VAD dimensions. For a more intuitive et al., 2014) is arguably the most important one for NLP purposes—it was only recently constructed (com- explanation, we display the position of six basic emotions (Ekman, 1992; Russell and Mehraprising 1,003 lexical entries) and replicates A NEW’s bian, 1977). methodology very closely (see K¨oper and Schulte im Walde (2016) for a more complete overview of German VAD resources). As the manual creation of affective lexicons (polarity or VAD) is expensive, their automatic extension is an active field of research since many years (Turney and Littman, 2003; Rosenthal et al., 2015). Typically, unlabeled words are attributed affective values given a set of seed words with known affect association, as well as similarity scores between seed and unlabeled words. Concerning emotions in VAD representation, Bestgen (2008) presented an algorithm based upon a k-Nearest-Neighbor methodology which expands the original lexicon by a factor of 17 (Bestgen and Vincze, 2012). Cook and Stevenson (2010) were the first to induce a polarity lexicon for non-contemporary language from historical corpora by employing a pointwise mutual information (PMI) metric to determine word similarity and"
W16-4008,P14-2070,0,0.0302618,"lexicon should be more suitable for a historical analysis than lexicons with contemporary information only. 3.2 Measuring Textual Emotion Building on an adapted lexicon, it is possible to (more) accurately determine the emotion values of historical texts. For this task, we use the Jena Emotion Analysis System3 (JE M AS; Buechel and Hahn (2016)) since it has been (as one of the first tools for VAD prediction) thoroughly evaluated and is, to the best of our knowledge, currently the only tool for this purpose freely available. The lexicon-based approach it employs yields reasonable performance (Staiano and Guerini, 2014; Buechel and Hahn, 2016) and is easily adaptable to other domains by replacing the lexicon—a feature most valuable for historical applications as well. Basically,4 it calculates the emotion value of a document d (a bag of words), e¯(d), as the weighted average of the emotion values of the words in d, e¯(w), as computed by Equation 1: P λ(w, d) × e¯(w) P e¯(d) := w∈d (2) w∈d λ(w, d) where e¯(w) is defined as the vector representing a neutral emotion, if w is not covered by the lexicon, and λ denotes some term weighting function. Here, we use absolute term frequency as the resulting performance"
W16-4008,W16-0430,0,0.0785068,"icense details: http:// 54 Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH), pages 54–61, Osaka, Japan, December 11-17 2016. 1.0 in the Google Books Ngram corpus (see below) and find correlations with major socio-political events (such as WWII), as well as the annual U.S. economic misery index in the 20th century. As stated above, most prior work focused on the bi-polar notion of semantic polarity, a rather simplified representation scheme given the richness of human affective states (a deficit increasingly recognized in sentiment analysis (Strapparava, 2016)). In contrast to this representationally restricted format, the VAD model of emotion (Bradley and Lang, 1994), which we employ here, is a well-established approach in psychology (Sander and Scherer, 2009) which also increasingly attracts interest in the NLP community (see among others K¨oper and Schulte im Walde (2016), Yu et al. (2016), and Wang et al. (2016)). It assumes that affective states can be characterized relative to three affective dimensions: Valence (corresponding to the concept of polarity), Arousal (the degree of calmness or excitement) and Dominance (the degree to which one fe"
W16-4008,P16-2037,0,0.0343316,"tury. As stated above, most prior work focused on the bi-polar notion of semantic polarity, a rather simplified representation scheme given the richness of human affective states (a deficit increasingly recognized in sentiment analysis (Strapparava, 2016)). In contrast to this representationally restricted format, the VAD model of emotion (Bradley and Lang, 1994), which we employ here, is a well-established approach in psychology (Sander and Scherer, 2009) which also increasingly attracts interest in the NLP community (see among others K¨oper and Schulte im Walde (2016), Yu et al. (2016), and Wang et al. (2016)). It assumes that affective states can be characterized relative to three affective dimensions: Valence (corresponding to the concept of polarity), Arousal (the degree of calmness or excitement) and Dominance (the degree to which one feels in control of a social situation). Formally, the VAD dimensions span a three-dimensional real-valued space which is illustrated in Figure 1, the prediction of such values being a multi-way regression problem (Buechel and Hahn, 2016). Thanks to the popularity of the VAD scheme in psychology, plenty of resources have already been Joy Anger developed for diffe"
W16-4008,N16-1066,0,0.387198,"c misery index in the 20th century. As stated above, most prior work focused on the bi-polar notion of semantic polarity, a rather simplified representation scheme given the richness of human affective states (a deficit increasingly recognized in sentiment analysis (Strapparava, 2016)). In contrast to this representationally restricted format, the VAD model of emotion (Bradley and Lang, 1994), which we employ here, is a well-established approach in psychology (Sander and Scherer, 2009) which also increasingly attracts interest in the NLP community (see among others K¨oper and Schulte im Walde (2016), Yu et al. (2016), and Wang et al. (2016)). It assumes that affective states can be characterized relative to three affective dimensions: Valence (corresponding to the concept of polarity), Arousal (the degree of calmness or excitement) and Dominance (the degree to which one feels in control of a social situation). Formally, the VAD dimensions span a three-dimensional real-valued space which is illustrated in Figure 1, the prediction of such values being a multi-way regression problem (Buechel and Hahn, 2016). Thanks to the popularity of the VAD scheme in psychology, plenty of resources have"
W17-0801,E17-2092,1,0.82966,"11th Linguistic Annotation Workshop, pages 1–12, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics ● ● Anger Disgust ● Fear Sadness 1.0 0.5 Arousal 0.0 ● ● Joy Surprise −0.5 Dominance 0.5 ● 0.0 −0.5 Related Work −1.0 2 1.0 tions and domains of raw text, making this the first study ever to address the impact of text understanding perspective on sentence-level emotion annotation. The results we achieved directly influenced the design and creation of E MO BANK, a novel large-scale gold standard for emotion analysis employing the VAD model for affect representation (Buechel and Hahn, 2017). −1.0 Representation Schemes for Emotion. Due to the multi-disciplinary nature of research on emotions, different representation schemes and models have emerged hampering comparison across different approaches (Buechel and Hahn, 2016). In NLP-oriented sentiment and emotion analysis, the most popular representation scheme is based on semantic polarity, the positiveness or negativeness of a word or a sentence, while slightly more sophisticated schemes include a neutral class or even rely on a multi-point polarity scale (Pang and Lee, 2008). Despite their popularity, these bi- or tri-polar schem"
W17-0801,P13-2091,0,0.159667,"the adequacy of BWS for emotional dimensions other than Valence (polarity) remains to be shown. Perspectival Understanding of Emotions. As stated above, research on the linkage of different annotation perspectives (typically reader vs. writer) is really rare. Tang and Chen (2012) examine the relation between the sentiment of microblog posts and the sentiment of their comments (as a proxy for reader emotion) using a positivenegative scheme. They examine which linguistic features are predictive for certain emotion transitions (combinations of an initial writer and a responsive reader emotion). Liu et al. (2013) model the emotion of a news reader jointly with the emotion of a comment writer using a co-training approach. This contribution was followed up by Li et al. (2016) who criticized that important assumptions underlying co-training, viz. sufficiency and independence of the two views, had actually been violated in that work. Instead, they propose a twoview label propagation approach. To the best of our knowledge, only Mohammad and Turney (2013) investigated the effects of different perspectives on annotation quality. They conducted an experiment on how to formulate the emotion annotation question"
W17-0801,P10-2013,0,0.120537,"Missing"
W17-0801,mukherjee-joshi-2014-author,0,0.131753,"nts of the predicate as well as the reader’s and author’s view on them. However, up until know, the power of this formalism is still restricted by assuming that author and reader evaluate the arguments in the same way. a multi-point scale (see Figure 2). Subjects refer to one of these figures per VAD dimension to rate their feelings as a response to a stimulus. SAM and derivatives therefrom have been used for annotating a wide range of resources for wordemotion associations in psychology (such as Warriner et al. (2013), Stadthagen-Gonzalez et al. (2016), Yao et al. (2016) and Schmidtke et al. (2014)), as well as VAD-annotated corpora in NLP; Preot¸iuc-Pietro et al. (2016) developed a corpus of 2,895 English Facebook posts (but they rely on only two annotators). Yu et al. (2016) generated a corpus of 2,009 Chinese sentences from different genres of online text. A possible alternative to SAM is Best-Worst Scaling (BSW; Louviere et al. (2015)), a method only recently introduced into NLP by Kiritchenko and Mohammad (2016). This annotation method exploits the fact that humans are typically more consistent when comparing two items relative to each other with respect to a given scale rather tha"
W17-0801,N16-1095,0,0.155645,"eption in behavioral psychology (Sander and Scherer, 2009). SAM iconically displays differences in Valence, Arousal and Dominance by a set of anthropomorphic cartoons on 2 Various (knowledge) representation formalisms have been suggested for inferring sentiment or opinions by either readers, writers or both from a piece of text. Reschke and Anand (2011) propose the concept of predicate-specific evaluativity functors which allow for inferring the writers’ evaluation of a proposition based on the evaluation of the arguments of the predicate. Using description logics as modeling language Klenner (2016) advocates the concept of polarity frames to capture polarity constraints verbs impose on their complements as well as polarity implications they project on them. Deng and Wiebe (2015) employ probabilistic soft logic for entity and event-based opinion inference from the viewpoint of the author or intra-textual entities. Rashkin et al. (2016) introduce connotation frames of (verb) predicates as a comprehensive formalism for modeling various evaluative relationships (being positive, negative or neutral) between the arguments of the predicate as well as the reader’s and author’s view on them. How"
W17-0801,H05-1073,0,0.432149,"o the concept of polarity), Arousal (a calm-excited scale) and Dominance (perceived degree of control in a (social) situation); see Figure 1 for an illustration. An even more wide-spread version of this model uses only the Valence and Arousal dimension, the VA model (Russell, 1980). For a long time, categorical models were pre−1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: The emotional space spanned by the Valence-Arousal-Dominance model. For illustration, the position of Ekman’s six Basic Emotions are included (as determined by Russell and Mehrabian (1977)). dominant in emotion analysis (Ovesdotter Alm et al., 2005; Strapparava and Mihalcea, 2007; Balahur et al., 2012). Only recently, the VA(D) model found increasing recognition (Paltoglou et al., 2013; Yu et al., 2015; Buechel and Hahn, 2016; Wang et al., 2016). When one of these dimensional models is selected, the task of emotion analysis is most often interpreted as a regression problem (predicting real-valued scores for each of the dimension) so that another set of metrics must be taken into account than those typically applied in NLP (see Section 3). Despite its growing popularity, the first largescale gold standard for dimensional models has only"
W17-0801,C16-1249,0,0.159265,"the linkage of different annotation perspectives (typically reader vs. writer) is really rare. Tang and Chen (2012) examine the relation between the sentiment of microblog posts and the sentiment of their comments (as a proxy for reader emotion) using a positivenegative scheme. They examine which linguistic features are predictive for certain emotion transitions (combinations of an initial writer and a responsive reader emotion). Liu et al. (2013) model the emotion of a news reader jointly with the emotion of a comment writer using a co-training approach. This contribution was followed up by Li et al. (2016) who criticized that important assumptions underlying co-training, viz. sufficiency and independence of the two views, had actually been violated in that work. Instead, they propose a twoview label propagation approach. To the best of our knowledge, only Mohammad and Turney (2013) investigated the effects of different perspectives on annotation quality. They conducted an experiment on how to formulate the emotion annotation question and found that asking whether a term is associated with an emotion actually resulted in higher IAA than asking whether a term evokes a certain emotion. Arguably, t"
W17-0801,P05-1015,0,0.388595,"ive that emotion is an intrinsic property of a sentence (or an alternative linguistic unit like a phrase or the entire text). In the following, we will use the terms W RITER, T EXT and R EADER to concisely refer to the respective perspectives. Data Sets. We collected two data sets, a movie review data set highly popular in sentiment analysis and a balanced corpus of general English. In this way, we can estimate the annotation quality resulting from different perspectives, also covering interactions regarding different domains. The first data set builds upon the corpus originally introduced by Pang and Lee (2005). It consists of about 10k snippets from movie reviews by professional critics collected from the website rottentomatoes.com. The data was further enriched by Socher et al. (2013) who annotated individual nodes in the constituency parse trees according to a 5-point polarity scale, forming the Stanford Sentiment Treebank (S ST) which contains 11,855 sentences. Upon closer inspection, we noticed that the S ST data have some encoding issues (e.g., Absorbing c Turpin .) that are character study by AndrA˜ not present in the original Rotten Tomatoes data set. So we decided to replicate the creation"
W17-0801,D13-1170,0,0.0044805,"ays, a simple classification according to the semantic polarity (positiveness, negativeness or neutralness) of a document was predominant, whereas in the meantime, research activities have shifted towards a more sophisticated modeling of sentiments. This includes the extension from only few basic to more varied emotional classes sometimes even assigning real-valued scores (Strapparava and Mihalcea, 2007), the aggregation of multiple aspects of an opinion item into a composite opinion statement for the whole item (Schouten and Frasincar, 2016), and sentiment compositionality on sentence level (Socher et al., 2013). 1 Proceedings of the 11th Linguistic Annotation Workshop, pages 1–12, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics ● ● Anger Disgust ● Fear Sadness 1.0 0.5 Arousal 0.0 ● ● Joy Surprise −0.5 Dominance 0.5 ● 0.0 −0.5 Related Work −1.0 2 1.0 tions and domains of raw text, making this the first study ever to address the impact of text understanding perspective on sentence-level emotion annotation. The results we achieved directly influenced the design and creation of E MO BANK, a novel large-scale gold standard for emotion analysis employing the VAD model for"
W17-0801,S07-1013,0,0.718103,"nt Perspectives of Text Understanding in Emotion Annotation Sven Buechel and Udo Hahn Jena University Language & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit¨at Jena, Jena, Germany {sven.buechel,udo.hahn}@uni-jena.de http://www.julielab.de Abstract There is also an increasing awareness of different perspectives one may take to interpret written discourse in the process of text comprehension. A typical distinction which mirrors different points of view is the one between the writer and the reader(s) of a document as exemplified by utterance (1) below (taken from Katz et al. (2007)): We here examine how different perspectives of understanding written discourse, like the reader’s, the writer’s or the text’s point of view, affect the quality of emotion annotations. We conducted a series of annotation experiments on two corpora, a popular movie review corpus and a genreand domain-balanced corpus of standard English. We found statistical evidence that the writer’s perspective yields superior annotation quality overall. However, the quality one perspective yields compared to the other(s) seems to depend on the domain the utterance originates from. Our data further suggest th"
W17-0801,W16-0404,0,0.190328,"Missing"
W17-0801,tang-chen-2012-mining,0,0.319355,"id of something because we evaluate it as dangerous, or do we evaluate something as dangerous because we are afraid?). Although BWS provided promising results for polarity (Kiritchenko and Mohammad, 2016), in this paper, we will use SAM scales. First, with this decision, there are way more studies to compare our results with and, second, the adequacy of BWS for emotional dimensions other than Valence (polarity) remains to be shown. Perspectival Understanding of Emotions. As stated above, research on the linkage of different annotation perspectives (typically reader vs. writer) is really rare. Tang and Chen (2012) examine the relation between the sentiment of microblog posts and the sentiment of their comments (as a proxy for reader emotion) using a positivenegative scheme. They examine which linguistic features are predictive for certain emotion transitions (combinations of an initial writer and a responsive reader emotion). Liu et al. (2013) model the emotion of a news reader jointly with the emotion of a comment writer using a co-training approach. This contribution was followed up by Li et al. (2016) who criticized that important assumptions underlying co-training, viz. sufficiency and independence"
W17-0801,P16-1030,0,0.0197195,"t. Reschke and Anand (2011) propose the concept of predicate-specific evaluativity functors which allow for inferring the writers’ evaluation of a proposition based on the evaluation of the arguments of the predicate. Using description logics as modeling language Klenner (2016) advocates the concept of polarity frames to capture polarity constraints verbs impose on their complements as well as polarity implications they project on them. Deng and Wiebe (2015) employ probabilistic soft logic for entity and event-based opinion inference from the viewpoint of the author or intra-textual entities. Rashkin et al. (2016) introduce connotation frames of (verb) predicates as a comprehensive formalism for modeling various evaluative relationships (being positive, negative or neutral) between the arguments of the predicate as well as the reader’s and author’s view on them. However, up until know, the power of this formalism is still restricted by assuming that author and reader evaluate the arguments in the same way. a multi-point scale (see Figure 2). Subjects refer to one of these figures per VAD dimension to rate their feelings as a response to a stimulus. SAM and derivatives therefrom have been used for annot"
W17-0801,P16-2037,0,0.40904,"his model uses only the Valence and Arousal dimension, the VA model (Russell, 1980). For a long time, categorical models were pre−1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: The emotional space spanned by the Valence-Arousal-Dominance model. For illustration, the position of Ekman’s six Basic Emotions are included (as determined by Russell and Mehrabian (1977)). dominant in emotion analysis (Ovesdotter Alm et al., 2005; Strapparava and Mihalcea, 2007; Balahur et al., 2012). Only recently, the VA(D) model found increasing recognition (Paltoglou et al., 2013; Yu et al., 2015; Buechel and Hahn, 2016; Wang et al., 2016). When one of these dimensional models is selected, the task of emotion analysis is most often interpreted as a regression problem (predicting real-valued scores for each of the dimension) so that another set of metrics must be taken into account than those typically applied in NLP (see Section 3). Despite its growing popularity, the first largescale gold standard for dimensional models has only very recently been developed as a followup to this contribution (E MO BANK; Buechel and Hahn (2017)). The results we obtained here were crucial for the design of E MO BANK regarding the choice of annot"
W17-0801,W11-0145,0,0.0315641,"er models of emotion, as well. Resources and Annotation Methods. For the VAD model, the Self-Assessment Manikin (SAM; Bradley and Lang (1994)) is the most important and to our knowledge only standardized instrument for acquiring emotion ratings based on human self-perception in behavioral psychology (Sander and Scherer, 2009). SAM iconically displays differences in Valence, Arousal and Dominance by a set of anthropomorphic cartoons on 2 Various (knowledge) representation formalisms have been suggested for inferring sentiment or opinions by either readers, writers or both from a piece of text. Reschke and Anand (2011) propose the concept of predicate-specific evaluativity functors which allow for inferring the writers’ evaluation of a proposition based on the evaluation of the arguments of the predicate. Using description logics as modeling language Klenner (2016) advocates the concept of polarity frames to capture polarity constraints verbs impose on their complements as well as polarity implications they project on them. Deng and Wiebe (2015) employ probabilistic soft logic for entity and event-based opinion inference from the viewpoint of the author or intra-textual entities. Rashkin et al. (2016) intro"
W17-0801,P15-2129,0,0.0330556,"on. An even more wide-spread version of this model uses only the Valence and Arousal dimension, the VA model (Russell, 1980). For a long time, categorical models were pre−1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: The emotional space spanned by the Valence-Arousal-Dominance model. For illustration, the position of Ekman’s six Basic Emotions are included (as determined by Russell and Mehrabian (1977)). dominant in emotion analysis (Ovesdotter Alm et al., 2005; Strapparava and Mihalcea, 2007; Balahur et al., 2012). Only recently, the VA(D) model found increasing recognition (Paltoglou et al., 2013; Yu et al., 2015; Buechel and Hahn, 2016; Wang et al., 2016). When one of these dimensional models is selected, the task of emotion analysis is most often interpreted as a regression problem (predicting real-valued scores for each of the dimension) so that another set of metrics must be taken into account than those typically applied in NLP (see Section 3). Despite its growing popularity, the first largescale gold standard for dimensional models has only very recently been developed as a followup to this contribution (E MO BANK; Buechel and Hahn (2017)). The results we obtained here were crucial for the desig"
W17-0801,N16-1066,0,0.451556,"reader evaluate the arguments in the same way. a multi-point scale (see Figure 2). Subjects refer to one of these figures per VAD dimension to rate their feelings as a response to a stimulus. SAM and derivatives therefrom have been used for annotating a wide range of resources for wordemotion associations in psychology (such as Warriner et al. (2013), Stadthagen-Gonzalez et al. (2016), Yao et al. (2016) and Schmidtke et al. (2014)), as well as VAD-annotated corpora in NLP; Preot¸iuc-Pietro et al. (2016) developed a corpus of 2,895 English Facebook posts (but they rely on only two annotators). Yu et al. (2016) generated a corpus of 2,009 Chinese sentences from different genres of online text. A possible alternative to SAM is Best-Worst Scaling (BSW; Louviere et al. (2015)), a method only recently introduced into NLP by Kiritchenko and Mohammad (2016). This annotation method exploits the fact that humans are typically more consistent when comparing two items relative to each other with respect to a given scale rather than attributing numerical ratings to the items directly. For example, deciding whether one sentence is more positive than the other is easier than scoring them (say) as 8 and 6 on a 9-"
W18-3103,Q17-1010,0,0.0257581,"Missing"
W18-3103,W16-0423,1,0.442812,"Missing"
W18-3103,C16-1201,0,0.0579372,"se search engines (e-commerce, e-marketing) or consumer search engines, market monitors, product/service recommender systems (Vandic et al., 2017; Trotman et al., 2017). This also includes customer-supplier interaction platforms (e.g., portals, helps desks, newsgroups) and transaction support systems based on natural language communication (including business chat bots) (Cui et al., 2017; Altinok, 2018). Specialized modes of information extraction and text mining in economic domains, e.g., temporal event or transaction mining have also been explored (Tao et al., 2015; Lefever and Hoste, 2016; Ding et al., 2016), as well as information aggregation from single sources (e.g., review summaries, automatic threading) (Gerani et al., 2014). The language resources behind these activities include specialized lexicons (Loughran and McDonald, 2011) and ontologies for economics (Leibniz Information Centre for Economics, 2014), the adaptation or acquisition of lexicons 1 www.orga.uni-jena.de/orga/en/Corpus.html for economic NLP (Xie et al., 2013; Moore et al., 21 With regard to the popular 10-K corpus (Kogan et al., 2009), the data set we present is significantly smaller in size (both in terms of tokens and comp"
W18-3103,L16-1287,0,0.0149776,"ic and organizational phenomena, as well as for economics, management and organization science, in general (King et al., 2010; Bromley and Sharkey, 2017). While the raw data set we assembled can be used for scientific purposes only, we also offer an embedding model trained on it which is available without any legal restrictions.1 2 This external market view is complemented by NLP-based organization/enterprise analytics, e.g., social role taking, risk prediction, fraud analysis, market share analytics, etc. (Goel et al., 2010; H´ajek and Olej, 2015; Buechel et al., 2016; Goel and Uzuner, 2016; El-Haj et al., 2016; Tsai and Wang, 2017), including competitive or business intelligence services based on NLP tooling (Chaudhuri et al., 2011; Chung, 2014). From a methodological perspective, the social interactions between these actors—customers, enterprises, and political/juridical authorities—have been studied in terms of sentiments they bring to bear (Van De Kauter et al., 2015). Evidence is collected from consumers’ and enterprises’ verbal behavior and their communication about products and services, e.g., via social media (Chen et al., 2014; Si et al., 2014; Liu, 2015; Alshahrani et al., 2018). This rese"
W18-3103,D16-1171,0,0.0290004,"ofiling customers, tracking their product/company preferences, screening customer reviews, etc. (Archak et al., 2011; Ikeda et al., 2013; Zhang and Pennacchiotti, 2013; Stavrianou and Brun, 2015; Yang et al., 2015; Sakaki et al., 2016; Pekar and Binner, 2017). Another stream is concerned with NLPbased product analytics, e.g., based on (social) media monitoring, summarizing reviews, or identifying (deceptive/fake) product descriptions or reviews (Mukherjee et al., 2012; Feng et al., 2012; Wang and Ester, 2014; Tsunoda et al., 2015; Fang and Zhan, 2015; Kessler et al., 2015; Imada et al., 2016; Chen et al., 2016; Pryzant et al., 2017). Yet, the main thrust of work is devoted to NLPbased financial (stock) market analytics, e.g., analyzing companies’ market performance indicators (trend prediction, performance forecasting, volatility prediction, etc.) and verbal statements related to market performance, competitors or future perspectives (Schumaker and Chen, 2009; Kogan et al., 2009; Nassirtoussi et al., 2014; Li et al., 2014; Qiu and Srinivasan, 2014; Kazemian et al., 2014; de Fortuny et al., 2014; Ammann et al., 2014; Wang and Hua, 2014; Nguyen and Shirai, 2015; Luss and d’Aspremont, 2015; Ding et al"
W18-3103,ide-suderman-2004-american,0,0.0849527,"DAQ, FTSE AIM 100, TECDAX) stock indices, respectively. Altogether, this adds up to 5,000 reports from 270 companies headquartered in three of the world’s most important economies. The corpus spans a time frame from 2000 up to 2015 and contains, in total, 282M tokens. We also feature JOC O in a smallscale experiment to demonstrate its potential for NLP-fueled studies in economics, business and management research. 1 Introduction A crucial prerequisite in today’s NLP research is the availability of large amounts of language data. National reference corpora such as the ANC for American English (Ide and Suderman, 2004), the BNC for British English (Burnard, 2000), and the D E R E KO for German (Kupietz and L¨ungen, 2014) assemble a collection of language data with a focus on ordinary language use covering a wide range of genres (e.g., newspaper articles, technical writing and popular fiction, letters, transcripts of court or parliament speeches, etc.). Corpora exclusively focusing on newspaper articles have been particularly influential for the development of syntactic and semantic methodologies in NLP * These authors contributed equally to this work. 20 Proceedings of the First Workshop on Economics and Na"
W18-3103,D14-1168,0,0.0202164,"ms (Vandic et al., 2017; Trotman et al., 2017). This also includes customer-supplier interaction platforms (e.g., portals, helps desks, newsgroups) and transaction support systems based on natural language communication (including business chat bots) (Cui et al., 2017; Altinok, 2018). Specialized modes of information extraction and text mining in economic domains, e.g., temporal event or transaction mining have also been explored (Tao et al., 2015; Lefever and Hoste, 2016; Ding et al., 2016), as well as information aggregation from single sources (e.g., review summaries, automatic threading) (Gerani et al., 2014). The language resources behind these activities include specialized lexicons (Loughran and McDonald, 2011) and ontologies for economics (Leibniz Information Centre for Economics, 2014), the adaptation or acquisition of lexicons 1 www.orga.uni-jena.de/orga/en/Corpus.html for economic NLP (Xie et al., 2013; Moore et al., 21 With regard to the popular 10-K corpus (Kogan et al., 2009), the data set we present is significantly smaller in size (both in terms of tokens and companies). However, the 10-K corpus only covers ARs, while we also include CSRRs allowing a wider view on organizational commun"
W18-3103,kessler-kuhn-2014-corpus,0,0.0283634,"ifferent kinds of corporations from different countries. This property makes our corpus particularly well suited for deeper economic investigations with respect to cross-index, crossindustry and cross-country comparisons. 2016; Oliveira et al., 2016; Chen et al., 2018), corpora and annotations policies (guidelines, metadata schemata, etc.) for economic NLP concerned with domain-specific text genres (business reports, auditing documents, product reviews, economic newswire, social media posts or blogs, business letters, legislation documents, etc.) (Flickinger et al., 2012; Takala et al., 2014; Kessler and Kuhn, 2014; Asooja et al., 2015; Sch¨on et al., 2018) , and dedicated tools for economic NLP (e.g., NER taggers, sublanguage parsers, pipelines for processing economic discourse) (Schumaker and Chen, 2009; Feldman et al., 2011; Hogenboom et al., 2013; Kessler and Kuhn, 2013; Lee et al., 2014; Malo et al., 2014; Weichselbraun et al., 2015; Lefever and Hoste, 2016; Ding et al., 2016; El-Haj et al., 2018). Pioneering efforts in considering texts originally produced by enterprises as a basis for economic NLP were made by Kloptchenko et al. (2004) who used sentiments in enterprises’ quarterly reports as a pr"
W18-3103,N09-1031,0,0.50937,"reviews, or identifying (deceptive/fake) product descriptions or reviews (Mukherjee et al., 2012; Feng et al., 2012; Wang and Ester, 2014; Tsunoda et al., 2015; Fang and Zhan, 2015; Kessler et al., 2015; Imada et al., 2016; Chen et al., 2016; Pryzant et al., 2017). Yet, the main thrust of work is devoted to NLPbased financial (stock) market analytics, e.g., analyzing companies’ market performance indicators (trend prediction, performance forecasting, volatility prediction, etc.) and verbal statements related to market performance, competitors or future perspectives (Schumaker and Chen, 2009; Kogan et al., 2009; Nassirtoussi et al., 2014; Li et al., 2014; Qiu and Srinivasan, 2014; Kazemian et al., 2014; de Fortuny et al., 2014; Ammann et al., 2014; Wang and Hua, 2014; Nguyen and Shirai, 2015; Luss and d’Aspremont, 2015; Ding et al., 2015; Liu et al., 2015; Feuerriegel and Prendinger, 2016; Rekabsaz et al., 2017; Xing et al., 2018; Li et al., 2018). System-wise, specialized types of search engines have been developed, for instance, enterprise search engines (e-commerce, e-marketing) or consumer search engines, market monitors, product/service recommender systems (Vandic et al., 2017; Trotman et al.,"
W18-3103,J93-2004,0,0.0667535,"Missing"
W18-3103,L18-1704,0,0.0610014,"Missing"
W18-3103,J05-1004,0,0.151805,"Missing"
W18-3103,D14-1120,0,0.0147849,"uechel et al., 2016; Goel and Uzuner, 2016; El-Haj et al., 2016; Tsai and Wang, 2017), including competitive or business intelligence services based on NLP tooling (Chaudhuri et al., 2011; Chung, 2014). From a methodological perspective, the social interactions between these actors—customers, enterprises, and political/juridical authorities—have been studied in terms of sentiments they bring to bear (Van De Kauter et al., 2015). Evidence is collected from consumers’ and enterprises’ verbal behavior and their communication about products and services, e.g., via social media (Chen et al., 2014; Si et al., 2014; Liu, 2015; Alshahrani et al., 2018). This research is complemented by studies related to reputation, expertise, credibility and trust models for agents in the economic process (as traders, sellers, advertisers) based on mining communication traces and recommendation legacy data, including fake ad/review recognition (Bar-Haim et al., 2011; Brown, 2012; Mukherjee et al., 2012; Rechenthin et al., 2013; Tang and ˇ Chen, 2014; Znidarˇ siˇc et al., 2018). Related Work The ties between NLP, economics, management, and organization science have evolved around different types of economic actors and ro"
W18-3103,W17-5212,0,0.0134026,"011; Brown, 2012; Mukherjee et al., 2012; Rechenthin et al., 2013; Tang and ˇ Chen, 2014; Znidarˇ siˇc et al., 2018). Related Work The ties between NLP, economics, management, and organization science have evolved around different types of economic actors and roles they play in an economic setting. One stream of work deals with NLP-based customer analytics by profiling customers, tracking their product/company preferences, screening customer reviews, etc. (Archak et al., 2011; Ikeda et al., 2013; Zhang and Pennacchiotti, 2013; Stavrianou and Brun, 2015; Yang et al., 2015; Sakaki et al., 2016; Pekar and Binner, 2017). Another stream is concerned with NLPbased product analytics, e.g., based on (social) media monitoring, summarizing reviews, or identifying (deceptive/fake) product descriptions or reviews (Mukherjee et al., 2012; Feng et al., 2012; Wang and Ester, 2014; Tsunoda et al., 2015; Fang and Zhan, 2015; Kessler et al., 2015; Imada et al., 2016; Chen et al., 2016; Pryzant et al., 2017). Yet, the main thrust of work is devoted to NLPbased financial (stock) market analytics, e.g., analyzing companies’ market performance indicators (trend prediction, performance forecasting, volatility prediction, etc.)"
W18-3103,takala-etal-2014-gold,0,0.0226804,"strategy balancing different kinds of corporations from different countries. This property makes our corpus particularly well suited for deeper economic investigations with respect to cross-index, crossindustry and cross-country comparisons. 2016; Oliveira et al., 2016; Chen et al., 2018), corpora and annotations policies (guidelines, metadata schemata, etc.) for economic NLP concerned with domain-specific text genres (business reports, auditing documents, product reviews, economic newswire, social media posts or blogs, business letters, legislation documents, etc.) (Flickinger et al., 2012; Takala et al., 2014; Kessler and Kuhn, 2014; Asooja et al., 2015; Sch¨on et al., 2018) , and dedicated tools for economic NLP (e.g., NER taggers, sublanguage parsers, pipelines for processing economic discourse) (Schumaker and Chen, 2009; Feldman et al., 2011; Hogenboom et al., 2013; Kessler and Kuhn, 2013; Lee et al., 2014; Malo et al., 2014; Weichselbraun et al., 2015; Lefever and Hoste, 2016; Ding et al., 2016; El-Haj et al., 2018). Pioneering efforts in considering texts originally produced by enterprises as a basis for economic NLP were made by Kloptchenko et al. (2004) who used sentiments in enterprises’ q"
W18-3103,P14-5018,0,0.0205871,"Missing"
W18-3103,P17-1157,0,0.0240567,"Missing"
W18-3103,P13-1086,0,0.0156874,"ormation extraction and text mining in economic domains, e.g., temporal event or transaction mining have also been explored (Tao et al., 2015; Lefever and Hoste, 2016; Ding et al., 2016), as well as information aggregation from single sources (e.g., review summaries, automatic threading) (Gerani et al., 2014). The language resources behind these activities include specialized lexicons (Loughran and McDonald, 2011) and ontologies for economics (Leibniz Information Centre for Economics, 2014), the adaptation or acquisition of lexicons 1 www.orga.uni-jena.de/orga/en/Corpus.html for economic NLP (Xie et al., 2013; Moore et al., 21 With regard to the popular 10-K corpus (Kogan et al., 2009), the data set we present is significantly smaller in size (both in terms of tokens and companies). However, the 10-K corpus only covers ARs, while we also include CSRRs allowing a wider view on organizational communication traces. Also, the 10-K corpus only includes reports up to the year 2006, whereas our work incorporates documents as recent as 2015. Additionally, the 10-K corpus is only based on the 10-k forms mandated by the Securities Exchange Commission (SEC) in the US. Nonetheless, US corporations’ ARs contai"
W18-3103,D15-1009,0,0.0125854,"d/review recognition (Bar-Haim et al., 2011; Brown, 2012; Mukherjee et al., 2012; Rechenthin et al., 2013; Tang and ˇ Chen, 2014; Znidarˇ siˇc et al., 2018). Related Work The ties between NLP, economics, management, and organization science have evolved around different types of economic actors and roles they play in an economic setting. One stream of work deals with NLP-based customer analytics by profiling customers, tracking their product/company preferences, screening customer reviews, etc. (Archak et al., 2011; Ikeda et al., 2013; Zhang and Pennacchiotti, 2013; Stavrianou and Brun, 2015; Yang et al., 2015; Sakaki et al., 2016; Pekar and Binner, 2017). Another stream is concerned with NLPbased product analytics, e.g., based on (social) media monitoring, summarizing reviews, or identifying (deceptive/fake) product descriptions or reviews (Mukherjee et al., 2012; Feng et al., 2012; Wang and Ester, 2014; Tsunoda et al., 2015; Fang and Zhan, 2015; Kessler et al., 2015; Imada et al., 2016; Chen et al., 2016; Pryzant et al., 2017). Yet, the main thrust of work is devoted to NLPbased financial (stock) market analytics, e.g., analyzing companies’ market performance indicators (trend prediction, perform"
W18-3103,W15-2907,0,0.0139836,"an economic setting. One stream of work deals with NLP-based customer analytics by profiling customers, tracking their product/company preferences, screening customer reviews, etc. (Archak et al., 2011; Ikeda et al., 2013; Zhang and Pennacchiotti, 2013; Stavrianou and Brun, 2015; Yang et al., 2015; Sakaki et al., 2016; Pekar and Binner, 2017). Another stream is concerned with NLPbased product analytics, e.g., based on (social) media monitoring, summarizing reviews, or identifying (deceptive/fake) product descriptions or reviews (Mukherjee et al., 2012; Feng et al., 2012; Wang and Ester, 2014; Tsunoda et al., 2015; Fang and Zhan, 2015; Kessler et al., 2015; Imada et al., 2016; Chen et al., 2016; Pryzant et al., 2017). Yet, the main thrust of work is devoted to NLPbased financial (stock) market analytics, e.g., analyzing companies’ market performance indicators (trend prediction, performance forecasting, volatility prediction, etc.) and verbal statements related to market performance, competitors or future perspectives (Schumaker and Chen, 2009; Kogan et al., 2009; Nassirtoussi et al., 2014; Li et al., 2014; Qiu and Srinivasan, 2014; Kazemian et al., 2014; de Fortuny et al., 2014; Ammann et al., 2014; W"
W18-3103,D14-1126,0,0.016052,"nd roles they play in an economic setting. One stream of work deals with NLP-based customer analytics by profiling customers, tracking their product/company preferences, screening customer reviews, etc. (Archak et al., 2011; Ikeda et al., 2013; Zhang and Pennacchiotti, 2013; Stavrianou and Brun, 2015; Yang et al., 2015; Sakaki et al., 2016; Pekar and Binner, 2017). Another stream is concerned with NLPbased product analytics, e.g., based on (social) media monitoring, summarizing reviews, or identifying (deceptive/fake) product descriptions or reviews (Mukherjee et al., 2012; Feng et al., 2012; Wang and Ester, 2014; Tsunoda et al., 2015; Fang and Zhan, 2015; Kessler et al., 2015; Imada et al., 2016; Chen et al., 2016; Pryzant et al., 2017). Yet, the main thrust of work is devoted to NLPbased financial (stock) market analytics, e.g., analyzing companies’ market performance indicators (trend prediction, performance forecasting, volatility prediction, etc.) and verbal statements related to market performance, competitors or future perspectives (Schumaker and Chen, 2009; Kogan et al., 2009; Nassirtoussi et al., 2014; Li et al., 2014; Qiu and Srinivasan, 2014; Kazemian et al., 2014; de Fortuny et al., 2014;"
W18-3103,P14-1109,0,0.0159964,"5; Fang and Zhan, 2015; Kessler et al., 2015; Imada et al., 2016; Chen et al., 2016; Pryzant et al., 2017). Yet, the main thrust of work is devoted to NLPbased financial (stock) market analytics, e.g., analyzing companies’ market performance indicators (trend prediction, performance forecasting, volatility prediction, etc.) and verbal statements related to market performance, competitors or future perspectives (Schumaker and Chen, 2009; Kogan et al., 2009; Nassirtoussi et al., 2014; Li et al., 2014; Qiu and Srinivasan, 2014; Kazemian et al., 2014; de Fortuny et al., 2014; Ammann et al., 2014; Wang and Hua, 2014; Nguyen and Shirai, 2015; Luss and d’Aspremont, 2015; Ding et al., 2015; Liu et al., 2015; Feuerriegel and Prendinger, 2016; Rekabsaz et al., 2017; Xing et al., 2018; Li et al., 2018). System-wise, specialized types of search engines have been developed, for instance, enterprise search engines (e-commerce, e-marketing) or consumer search engines, market monitors, product/service recommender systems (Vandic et al., 2017; Trotman et al., 2017). This also includes customer-supplier interaction platforms (e.g., portals, helps desks, newsgroups) and transaction support systems based on natural lan"
W19-2003,Q18-1008,0,0.121838,"ering (JULIE) Lab Friedrich-Schiller-Universit¨at Jena, Jena, Germany julielab.de Abstract strategies on accuracy in word similarity and analogy tasks was explored in several papers (e.g., Levy et al. (2015)). However, down-sampling and details of its implementation also have major effects on the stability of word embeddings (also known as ‘reliability’), i.e., the degree to which models trained independently on the same data agree on the structure of the resulting embedding space. This problem has lately raised severe concerns in the word embedding community (e.g., Hellrich and Hahn (2016b); Antoniak and Mimno (2018); Wendlandt et al. (2018)) and is also of interest to the wider machine learning community due to the influence of probabilistic—and thus unstable— methods on experimental results (Reimers and Gurevych, 2017; Henderson et al., 2018), as well as replicability and reproducibility (Ivie and Thain, 2018, pp. 63:3–4). Stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e.g., tracking lexical change (Kim et al., 2014; Kulkarni et al., 2015; Hellrich et al., 2018). Unstable word embeddings can lead to serious problems in such applic"
W19-2003,P12-1015,0,0.030059,"nt and corpus.10 In the case of subsampling, each model was trained on one of the independently drawn samples. Stability was evaluated by selecting the 1k most frequent words in each non-bootstrap subsampled corpus as anchor words and calculating j@10 (see Equation 1).11 Following Hellrich and Hahn (2016a,b), we did not only investigate stability, but also the accuracy of our models to gauge potential tradeoffs. We measured the Spearman rank correlation between cosine-based word similarity judgments and human ones with four psycholinguistic test sets, i.e., the two crowdsourced test sets MEN (Bruni et al., 2012) and MTurk (Radinsky et al., 2011), the especially strict SimLex-999 (Hill et al., 2014) and the widely used WordSim-353 (WS353; Finkelstein et al. (2002)). We also measured the percentage of correctly solved analogies (using the multiplicative formula from Levy and Goldberg (2014b)) with two test sets developed at Google (Mikolov et al., 2013a) and Microsoft Research (MSR; Mikolov et al. (2013b)). Experimental Set-up We compared five algorithm variants: G LOV E, SGNS, SVD PPMI without down-sampling, SVD PPMI with probabilistic down-sampling, and SVD W PPMI . While we could use SGNS6 and G LOV"
W19-2003,W16-2114,1,0.898688,"nguage & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit¨at Jena, Jena, Germany julielab.de Abstract strategies on accuracy in word similarity and analogy tasks was explored in several papers (e.g., Levy et al. (2015)). However, down-sampling and details of its implementation also have major effects on the stability of word embeddings (also known as ‘reliability’), i.e., the degree to which models trained independently on the same data agree on the structure of the resulting embedding space. This problem has lately raised severe concerns in the word embedding community (e.g., Hellrich and Hahn (2016b); Antoniak and Mimno (2018); Wendlandt et al. (2018)) and is also of interest to the wider machine learning community due to the influence of probabilistic—and thus unstable— methods on experimental results (Reimers and Gurevych, 2017; Henderson et al., 2018), as well as replicability and reproducibility (Ivie and Thain, 2018, pp. 63:3–4). Stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e.g., tracking lexical change (Kim et al., 2014; Kulkarni et al., 2015; Hellrich et al., 2018). Unstable word embeddings can lead to se"
W19-2003,C16-1262,1,0.936002,"Missing"
W19-2003,J90-1003,0,0.228257,"e. For example, in a token sequence . . . , wi−2 , wi−1 , wi , wi+1 , wi+2 , . . . , with wi as the currently modeled token, a window of size 1 would be concerned with wi−1 and wi+1 only. Down-sampling as described by Levy et al. (2015) increases accuracy by ignoring certain co-occurrences while populating the word-context matrix (further details are described below). A word-context matrix is also used in G LOV E, whereas SGNS directly operates on sampled cooccurrences in a streaming manner. Positive pointwise mutual information (PPMI) is a variant of pointwise mutual information (Fano, 1961; Church and Hanks, 1990), independently developed by Niwa and Nitta (1994) and Bullinaria and Levy (2007). PPMI measures the ratio between observed co-occurrences (normalized and treated as a joint probability) and the expected co-occurrences (based on normalized frequencies treated as individual probabilities) for two words i and j while ignoring all cases in which the observed co-occurrences are fewer than the expected ones: ( (i,j) 0 if PP(i)P (j) < 1 P P M I(i, j) := P (i,j) log( P (i)P (j) ) otherwise (2) Truncated SVD reduces the dimensionality of the vector space described by the PPMI wordcontext matrix M . SV"
W19-2003,W14-2517,0,0.0167113,"), yet not in Antoniak and Mimno (2018)—can thus be explained by their difference in down-sampling options, i.e., no down-sampling or probabilistic down-sampling. G LOV E’s high stability in other studies (Antoniak and Mimno, 2018; Wendlandt et al., 2018) seems to be counterbalanced by its low accuracy and also appears to be limited to training on small corpora. 6 gathered evidence that G LOV E lacks accuracy and is only stable when trained on small corpora. We thus recommend using SVD W PPMI , especially for studies targeting (qualitative) interpretations of semantic spaces (e.g., Kim et al. (2014)). Overall, SGNS provided no benefit in accuracy over SVD W PPMI and the latter seemed especially well-suited for small training corpora. The only downside of SVD W PPMI we are aware of is its relatively large memory consumption during training shared by all SVD PPMI variants. Further research could investigate the performance of SVD W PPMI with other sets of hyperparameters or scrutinize the effect of down-sampling strategies on the ill-understood geometry of embedding spaces (Mimno and Thompson, 2017). It would also be interesting to investigate the effect of down-sampling and stability on d"
W19-2003,D17-1308,0,0.0716508,"D W PPMI , especially for studies targeting (qualitative) interpretations of semantic spaces (e.g., Kim et al. (2014)). Overall, SGNS provided no benefit in accuracy over SVD W PPMI and the latter seemed especially well-suited for small training corpora. The only downside of SVD W PPMI we are aware of is its relatively large memory consumption during training shared by all SVD PPMI variants. Further research could investigate the performance of SVD W PPMI with other sets of hyperparameters or scrutinize the effect of down-sampling strategies on the ill-understood geometry of embedding spaces (Mimno and Thompson, 2017). It would also be interesting to investigate the effect of down-sampling and stability on downstream tasks in a follow-up to Wendlandt et al. (2018). Finally, the increasingly popular contextualized embedding algorithms, e.g., BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018), are also probabilistic in nature and should thus be affected by stability problems. A direct transfer of our type specific evaluation strategy is impossible. However, an indirect one could be achieved by averaging token-specific contextualized embeddings to generate type representations. Discussion We investigate"
W19-2003,P14-2050,0,0.492035,"troduction Word embedding algorithms implement the latest form of distributional semantics originating from the seminal work of Harris (1954) or Rubenstein and Goodenough (1965). They generate dense vector space representations for words based on co-occurrences within a context window. They sample word-context pairs, i.e., typically two cooccurring tokens, from a corpus and use these to generate vector representations of words and their context. Changes to the algorithm’s sampling mechanism can lead to new capabilities, e.g., processing dependency information instead of linear co-occurrences (Levy and Goldberg, 2014a), or increased performance, e.g., using word association values instead of raw co-occurrence counts (Bullinaria and Levy, 2007). Word embedding algorithms commonly downsample contexts to lessen the impact of highfrequency words (termed ‘subsampling’ in Levy et al. (2015)) or increase the relative importance of words closer to the center of a context window (called ‘dynamic context window’ in Levy et al. (2015)). The effect of using such down-sampling 18 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 18–26 c Minneapolis, USA, June 6, 2019. 2019 Assoc"
W19-2003,C94-1049,0,0.605708,", wi+2 , . . . , with wi as the currently modeled token, a window of size 1 would be concerned with wi−1 and wi+1 only. Down-sampling as described by Levy et al. (2015) increases accuracy by ignoring certain co-occurrences while populating the word-context matrix (further details are described below). A word-context matrix is also used in G LOV E, whereas SGNS directly operates on sampled cooccurrences in a streaming manner. Positive pointwise mutual information (PPMI) is a variant of pointwise mutual information (Fano, 1961; Church and Hanks, 1990), independently developed by Niwa and Nitta (1994) and Bullinaria and Levy (2007). PPMI measures the ratio between observed co-occurrences (normalized and treated as a joint probability) and the expected co-occurrences (based on normalized frequencies treated as individual probabilities) for two words i and j while ignoring all cases in which the observed co-occurrences are fewer than the expected ones: ( (i,j) 0 if PP(i)P (j) < 1 P P M I(i, j) := P (i,j) log( P (i)P (j) ) otherwise (2) Truncated SVD reduces the dimensionality of the vector space described by the PPMI wordcontext matrix M . SVD factorizes M in three special1 matrices, so that"
W19-2003,W14-1618,0,0.39203,"troduction Word embedding algorithms implement the latest form of distributional semantics originating from the seminal work of Harris (1954) or Rubenstein and Goodenough (1965). They generate dense vector space representations for words based on co-occurrences within a context window. They sample word-context pairs, i.e., typically two cooccurring tokens, from a corpus and use these to generate vector representations of words and their context. Changes to the algorithm’s sampling mechanism can lead to new capabilities, e.g., processing dependency information instead of linear co-occurrences (Levy and Goldberg, 2014a), or increased performance, e.g., using word association values instead of raw co-occurrence counts (Bullinaria and Levy, 2007). Word embedding algorithms commonly downsample contexts to lessen the impact of highfrequency words (termed ‘subsampling’ in Levy et al. (2015)) or increase the relative importance of words closer to the center of a context window (called ‘dynamic context window’ in Levy et al. (2015)). The effect of using such down-sampling 18 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 18–26 c Minneapolis, USA, June 6, 2019. 2019 Assoc"
W19-2003,padro-etal-2014-comparing,0,0.0476496,"Missing"
W19-2003,D14-1162,0,0.0857755,", 2018). Unstable word embeddings can lead to serious problems in such applications, as interpretations will depend on the luck of the draw. This might also affect high-stake fields like medical informatics where patients could be harmed as a consequence of misleading results (Coiera et al., 2018). In the light of these concerns, we here evaluate down-sampling strategies by modifying the SVD PPMI (Singular Value Decomposition of a Positive Pointwise Mutual Information matrix; Levy et al. (2015)) algorithm and comparing its results with those of two other embedding algorithms, namely, G LOV E (Pennington et al., 2014) and SGNS (Mikolov et al., 2013a,c). Our analysis is based on three corpora of different sizes and investigates effects on both accuracy and stability. The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stab"
W19-2003,N18-1202,0,0.043353,"are aware of is its relatively large memory consumption during training shared by all SVD PPMI variants. Further research could investigate the performance of SVD W PPMI with other sets of hyperparameters or scrutinize the effect of down-sampling strategies on the ill-understood geometry of embedding spaces (Mimno and Thompson, 2017). It would also be interesting to investigate the effect of down-sampling and stability on downstream tasks in a follow-up to Wendlandt et al. (2018). Finally, the increasingly popular contextualized embedding algorithms, e.g., BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018), are also probabilistic in nature and should thus be affected by stability problems. A direct transfer of our type specific evaluation strategy is impossible. However, an indirect one could be achieved by averaging token-specific contextualized embeddings to generate type representations. Discussion We investigated the effect of down-sampling strategies on word embedding stability by comparing five algorithm variants on three corpora, two of which were larger than those typically used in stability studies. We proposed a simple modification to the down-sampling strategy used for the SVD PPMI a"
W19-2003,Q15-1016,0,0.193101,"ced form of corpus linguistics, e.g., tracking lexical change (Kim et al., 2014; Kulkarni et al., 2015; Hellrich et al., 2018). Unstable word embeddings can lead to serious problems in such applications, as interpretations will depend on the luck of the draw. This might also affect high-stake fields like medical informatics where patients could be harmed as a consequence of misleading results (Coiera et al., 2018). In the light of these concerns, we here evaluate down-sampling strategies by modifying the SVD PPMI (Singular Value Decomposition of a Positive Pointwise Mutual Information matrix; Levy et al. (2015)) algorithm and comparing its results with those of two other embedding algorithms, namely, G LOV E (Pennington et al., 2014) and SGNS (Mikolov et al., 2013a,c). Our analysis is based on three corpora of different sizes and investigates effects on both accuracy and stability. The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong"
W19-2003,2018.jeptalnrecital-long.3,0,0.175309,"riedrich-Schiller-Universit¨at Jena, Jena, Germany julielab.de Abstract strategies on accuracy in word similarity and analogy tasks was explored in several papers (e.g., Levy et al. (2015)). However, down-sampling and details of its implementation also have major effects on the stability of word embeddings (also known as ‘reliability’), i.e., the degree to which models trained independently on the same data agree on the structure of the resulting embedding space. This problem has lately raised severe concerns in the word embedding community (e.g., Hellrich and Hahn (2016b); Antoniak and Mimno (2018); Wendlandt et al. (2018)) and is also of interest to the wider machine learning community due to the influence of probabilistic—and thus unstable— methods on experimental results (Reimers and Gurevych, 2017; Henderson et al., 2018), as well as replicability and reproducibility (Ivie and Thain, 2018, pp. 63:3–4). Stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e.g., tracking lexical change (Kim et al., 2014; Kulkarni et al., 2015; Hellrich et al., 2018). Unstable word embeddings can lead to serious problems in such applic"
W19-2003,N13-1090,0,0.342699,"an lead to serious problems in such applications, as interpretations will depend on the luck of the draw. This might also affect high-stake fields like medical informatics where patients could be harmed as a consequence of misleading results (Coiera et al., 2018). In the light of these concerns, we here evaluate down-sampling strategies by modifying the SVD PPMI (Singular Value Decomposition of a Positive Pointwise Mutual Information matrix; Levy et al. (2015)) algorithm and comparing its results with those of two other embedding algorithms, namely, G LOV E (Pennington et al., 2014) and SGNS (Mikolov et al., 2013a,c). Our analysis is based on three corpora of different sizes and investigates effects on both accuracy and stability. The stability of word embedding algorithms, i.e., the consistency of the word representations they reveal when trained repeatedly on the same data set, has recently raised concerns. We here compare word embedding algorithms on three corpora of different sizes, and evaluate both their stability and accuracy. We find strong evidence that down-sampling strategies (used as part of their training procedures) are particularly influential for the stability of SVD PPMI -type embeddi"
W19-2003,D17-1035,0,0.0190906,")). However, down-sampling and details of its implementation also have major effects on the stability of word embeddings (also known as ‘reliability’), i.e., the degree to which models trained independently on the same data agree on the structure of the resulting embedding space. This problem has lately raised severe concerns in the word embedding community (e.g., Hellrich and Hahn (2016b); Antoniak and Mimno (2018); Wendlandt et al. (2018)) and is also of interest to the wider machine learning community due to the influence of probabilistic—and thus unstable— methods on experimental results (Reimers and Gurevych, 2017; Henderson et al., 2018), as well as replicability and reproducibility (Ivie and Thain, 2018, pp. 63:3–4). Stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e.g., tracking lexical change (Kim et al., 2014; Kulkarni et al., 2015; Hellrich et al., 2018). Unstable word embeddings can lead to serious problems in such applications, as interpretations will depend on the luck of the draw. This might also affect high-stake fields like medical informatics where patients could be harmed as a consequence of misleading results (Coiera"
W19-2003,D16-1099,0,0.0148256,"g 0.01 for G LOV E trained on nonsubsampled WIKI, probably due to the overlap in our jackknife procedure. Finally, we tested12 the overall performance of each algorithm variant by first performing a Quade test (Quade, 1979) as a safeguard against type I performance on WIKI is roughly in-line with the data reported in Levy et al. (2015). In general, corpus size does seem to have a positive effect on accuracy. However, for MEN, MTurk and MSR the highest values are achieved with NEWS and not with WIKI. SVD PPMI variants seem to be less hampered by small training corpora, matching observations by Sahlgren and Lenci (2016). Stability is clearly positively influenced by corpus size for all probabilistic algorithm variants except G LOV E, which, in contrast, benefits from small training corpora. Also, randomly subsampling corpora has a negative effect on both accuracy and stability—this can be explained by the smaller corpus size for accuracy and the differences in training material (as subsampling was performed independently for each model) for stability. Figure 1 illustrates the stability of all tested algorithm variants. SVD W PPMI and SVD PPMI without down-sampling are the only systems which achieve perfect s"
W19-2003,C04-1146,0,0.192634,"Missing"
W19-2003,N18-1190,0,0.628754,"-Schiller-Universit¨at Jena, Jena, Germany julielab.de Abstract strategies on accuracy in word similarity and analogy tasks was explored in several papers (e.g., Levy et al. (2015)). However, down-sampling and details of its implementation also have major effects on the stability of word embeddings (also known as ‘reliability’), i.e., the degree to which models trained independently on the same data agree on the structure of the resulting embedding space. This problem has lately raised severe concerns in the word embedding community (e.g., Hellrich and Hahn (2016b); Antoniak and Mimno (2018); Wendlandt et al. (2018)) and is also of interest to the wider machine learning community due to the influence of probabilistic—and thus unstable— methods on experimental results (Reimers and Gurevych, 2017; Henderson et al., 2018), as well as replicability and reproducibility (Ivie and Thain, 2018, pp. 63:3–4). Stability is critical for studies examining the underlying semantic space as a more advanced form of corpus linguistics, e.g., tracking lexical change (Kim et al., 2014; Kulkarni et al., 2015; Hellrich et al., 2018). Unstable word embeddings can lead to serious problems in such applications, as interpretation"
W19-2501,C18-1245,1,0.896017,"as Plutchik’s (1980) wheel of emotion (Strapparava and Mihalcea, 2007; Mohammad and Turney, 2013; Bostan and Klinger, 2018). In order to illustrate the relationship between Ekman’s basic emotions and the VAD affect space the former are embedded into the latter scheme in Figure 1. The affective meaning of individual words is encoded in so-called emotion lexicons. Thanks to over two decades of efforts from psychologists and AI researchers alike, today a rich collection of empirically founded emotion lexicons is available covering both VAD and basic emotion representation for many languages (see Buechel and Hahn (2018b) for an overview). One of the best know resources of this kind are the Affective Norms for English Words (A NEW; Bradley and Lang, 1999) ● Disgust ● ● ● Joy Surprise Fear Sadness 1.0 0.5 −0.5 Dominance ● ● Anger 0.5 2.1 Related Work 0.0 2 0.0 −1.0 −0.5 −1.0 −1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: Affective space spanned by the ValenceArousal-Dominance (VAD) model, together with the position of six basic emotion categories. Entry rage orgasm relaxed Valence Arousal Dominance 2.50 8.01 7.25 6.62 7.19 2.49 4.17 5.84 7.09 Table 1: Sample Valence-Arousal-Dominance (VAD) ratings from the emotion l"
W19-2501,L18-1028,1,0.929237,"as Plutchik’s (1980) wheel of emotion (Strapparava and Mihalcea, 2007; Mohammad and Turney, 2013; Bostan and Klinger, 2018). In order to illustrate the relationship between Ekman’s basic emotions and the VAD affect space the former are embedded into the latter scheme in Figure 1. The affective meaning of individual words is encoded in so-called emotion lexicons. Thanks to over two decades of efforts from psychologists and AI researchers alike, today a rich collection of empirically founded emotion lexicons is available covering both VAD and basic emotion representation for many languages (see Buechel and Hahn (2018b) for an overview). One of the best know resources of this kind are the Affective Norms for English Words (A NEW; Bradley and Lang, 1999) ● Disgust ● ● ● Joy Surprise Fear Sadness 1.0 0.5 −0.5 Dominance ● ● Anger 0.5 2.1 Related Work 0.0 2 0.0 −1.0 −0.5 −1.0 −1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: Affective space spanned by the ValenceArousal-Dominance (VAD) model, together with the position of six basic emotion categories. Entry rage orgasm relaxed Valence Arousal Dominance 2.50 8.01 7.25 6.62 7.19 2.49 4.17 5.84 7.09 Table 1: Sample Valence-Arousal-Dominance (VAD) ratings from the emotion l"
W19-2501,D16-1057,0,0.0707685,"y in Seed Word Selection Johannes Hellrich* Sven Buechel* Udo Hahn {firstname.lastname}@uni-jena.de Jena University Language & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit¨at Jena, Jena, Germany julielab.de polarity (feelings being either positive, negative or neutral). Another major shortcoming of this area is the lack of appropriate data and methodologies for evaluation. As a result, the aptness of algorithmic contributions has so far only been assessed in terms of face validity rather than quantitative performance figures (Cook and Stevenson, 2010; Buechel et al., 2016; Hamilton et al., 2016a; Hellrich et al., 2018). To tackle those shortcomings, we first introduce adaptations of algorithms for word polarity induction to vectorial emotion annotation formats, thus enabling a more fine-grained analysis. Second, to put the evaluation of these methods on safer ground, we present two datasets of affective word ratings for English and German, respectively.1 These have been annotated by scholars in terms of language-stage-specific emotional connotations. We ran synchronic as well as diachronic experiments to compare different algorithms for modeling historical word emotions—the latter k"
W19-2501,N18-1173,1,0.911018,"as Plutchik’s (1980) wheel of emotion (Strapparava and Mihalcea, 2007; Mohammad and Turney, 2013; Bostan and Klinger, 2018). In order to illustrate the relationship between Ekman’s basic emotions and the VAD affect space the former are embedded into the latter scheme in Figure 1. The affective meaning of individual words is encoded in so-called emotion lexicons. Thanks to over two decades of efforts from psychologists and AI researchers alike, today a rich collection of empirically founded emotion lexicons is available covering both VAD and basic emotion representation for many languages (see Buechel and Hahn (2018b) for an overview). One of the best know resources of this kind are the Affective Norms for English Words (A NEW; Bradley and Lang, 1999) ● Disgust ● ● ● Joy Surprise Fear Sadness 1.0 0.5 −0.5 Dominance ● ● Anger 0.5 2.1 Related Work 0.0 2 0.0 −1.0 −0.5 −1.0 −1.0 −0.5 0.0 0.5 1.0 Valence Figure 1: Affective space spanned by the ValenceArousal-Dominance (VAD) model, together with the position of six basic emotion categories. Entry rage orgasm relaxed Valence Arousal Dominance 2.50 8.01 7.25 6.62 7.19 2.49 4.17 5.84 7.09 Table 1: Sample Valence-Arousal-Dominance (VAD) ratings from the emotion l"
W19-2501,P16-1141,0,0.0765713,"y in Seed Word Selection Johannes Hellrich* Sven Buechel* Udo Hahn {firstname.lastname}@uni-jena.de Jena University Language & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit¨at Jena, Jena, Germany julielab.de polarity (feelings being either positive, negative or neutral). Another major shortcoming of this area is the lack of appropriate data and methodologies for evaluation. As a result, the aptness of algorithmic contributions has so far only been assessed in terms of face validity rather than quantitative performance figures (Cook and Stevenson, 2010; Buechel et al., 2016; Hamilton et al., 2016a; Hellrich et al., 2018). To tackle those shortcomings, we first introduce adaptations of algorithms for word polarity induction to vectorial emotion annotation formats, thus enabling a more fine-grained analysis. Second, to put the evaluation of these methods on safer ground, we present two datasets of affective word ratings for English and German, respectively.1 These have been annotated by scholars in terms of language-stage-specific emotional connotations. We ran synchronic as well as diachronic experiments to compare different algorithms for modeling historical word emotions—the latter k"
W19-2501,W16-4008,1,0.863607,"eats Supposed Stability in Seed Word Selection Johannes Hellrich* Sven Buechel* Udo Hahn {firstname.lastname}@uni-jena.de Jena University Language & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit¨at Jena, Jena, Germany julielab.de polarity (feelings being either positive, negative or neutral). Another major shortcoming of this area is the lack of appropriate data and methodologies for evaluation. As a result, the aptness of algorithmic contributions has so far only been assessed in terms of face validity rather than quantitative performance figures (Cook and Stevenson, 2010; Buechel et al., 2016; Hamilton et al., 2016a; Hellrich et al., 2018). To tackle those shortcomings, we first introduce adaptations of algorithms for word polarity induction to vectorial emotion annotation formats, thus enabling a more fine-grained analysis. Second, to put the evaluation of these methods on safer ground, we present two datasets of affective word ratings for English and German, respectively.1 These have been annotated by scholars in terms of language-stage-specific emotional connotations. We ran synchronic as well as diachronic experiments to compare different algorithms for modeling historical wor"
W19-2501,C16-1262,1,0.895769,"Missing"
W19-2501,cook-stevenson-2010-automatically,0,0.41809,"rical Language: Quantity Beats Supposed Stability in Seed Word Selection Johannes Hellrich* Sven Buechel* Udo Hahn {firstname.lastname}@uni-jena.de Jena University Language & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit¨at Jena, Jena, Germany julielab.de polarity (feelings being either positive, negative or neutral). Another major shortcoming of this area is the lack of appropriate data and methodologies for evaluation. As a result, the aptness of algorithmic contributions has so far only been assessed in terms of face validity rather than quantitative performance figures (Cook and Stevenson, 2010; Buechel et al., 2016; Hamilton et al., 2016a; Hellrich et al., 2018). To tackle those shortcomings, we first introduce adaptations of algorithms for word polarity induction to vectorial emotion annotation formats, thus enabling a more fine-grained analysis. Second, to put the evaluation of these methods on safer ground, we present two datasets of affective word ratings for English and German, respectively.1 These have been annotated by scholars in terms of language-stage-specific emotional connotations. We ran synchronic as well as diachronic experiments to compare different algorithms for m"
W19-2501,W17-2203,0,0.0881335,"Missing"
W19-2501,L16-1413,0,0.0699051,"Missing"
W19-2501,Q15-1016,0,0.052503,"t e(s) denote its three-dimensional emotion vector corresponding to its VAD value in our seed lexicon. Furthermore, let nearest(w, k) denote the set of the k seed word most similar to a given word w. Then the predicted emotion of word w according to K NN is defined as follows: X 1 eK NN (w, k) := e(s) (1) k An alternative solution for generating low dimensional vectors is gathering all word-context pairs for a corpus in a large matrix and reducing its dimensionality with singular value decomposition (SVD), a technique very popular in the early 1990’s (Deerwester et al., 1990; Sch¨utze, 1993). Levy et al. (2015) propose SVDPPMI , a state-ofthe-art algorithm based on combining SVD with the positive pointwise mutual information (PPMI; Niwa and Nitta, 1994) word association metric. Both SGNS and SVDPPMI have been shown to be adequate for exploring historical semantics (Hamilton et al., 2016b,a). A general downside of existing embedding algorithms other than SVDPPMI is their inherent stochastic behavior during training which makes the resulting embedding models unreliable (Hellrich and Hahn, 2016; Antoniak and Mimno, 2018; Wendlandt et al., 2018). Very recently, contextualized word embeddings, such as EL"
W19-2501,P18-1017,0,0.0307582,"eties of such forms of semantic polarity, a rather simplified representation of the richness of human affective states—an observation increasingly recognized in sentiment analysis (Strapparava, 2016). In contrast to this bi-polar representation, the Valence-Arousal-Dominance (VAD) model of emotion (Bradley and Lang, 1994) is a well-established approach in psychology (Sander and Scherer, 2009) which increasingly attracts interest by NLP researchers (K¨oper and Schulte im Walde, 2016; Yu et al., 2016; Wang et al., 2016; Shaikh et al., 2016; Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016; Mohammad, 2018). The VAD model assumes that affective states can be characterized relative to Valence (corresponding to the concept of polarity), Arousal (the degree of calmness or excitement) and Dominance (perceived degree of control). Formally, VAD spans a three-dimensional real-valued space (see Figure 1) making the prediction of such values a multi-variate regression problem (Buechel and Hahn, 2016). Another popular line of emotion representation evolved around the notion of basic emotions, small sets of discrete, cross-culturally universal affective states (Scherer, 2000). Here, contributions most infl"
W19-2501,P93-1034,0,0.292856,"Missing"
W19-2501,C94-1049,0,0.0159396,"Missing"
W19-2501,L16-1180,0,0.170229,"imensional axes including “good vs. bad”. Most previous work focused on varieties of such forms of semantic polarity, a rather simplified representation of the richness of human affective states—an observation increasingly recognized in sentiment analysis (Strapparava, 2016). In contrast to this bi-polar representation, the Valence-Arousal-Dominance (VAD) model of emotion (Bradley and Lang, 1994) is a well-established approach in psychology (Sander and Scherer, 2009) which increasingly attracts interest by NLP researchers (K¨oper and Schulte im Walde, 2016; Yu et al., 2016; Wang et al., 2016; Shaikh et al., 2016; Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016; Mohammad, 2018). The VAD model assumes that affective states can be characterized relative to Valence (corresponding to the concept of polarity), Arousal (the degree of calmness or excitement) and Dominance (perceived degree of control). Formally, VAD spans a three-dimensional real-valued space (see Figure 1) making the prediction of such values a multi-variate regression problem (Buechel and Hahn, 2016). Another popular line of emotion representation evolved around the notion of basic emotions, small sets of discrete, cross-culturally u"
W19-2501,W16-0430,0,0.022822,"Heritage, Social Sciences, Humanities and Literature, pp. 1–11 c Minneapolis, MN, USA, June 7, 2019. 2019 Association for Computational Linguistics 1.0 Representing Word Emotions Quantitative models for word emotions can be traced back at least to Osgood (1953) who used questionnaires to gather human ratings for words on a wide variety of dimensional axes including “good vs. bad”. Most previous work focused on varieties of such forms of semantic polarity, a rather simplified representation of the richness of human affective states—an observation increasingly recognized in sentiment analysis (Strapparava, 2016). In contrast to this bi-polar representation, the Valence-Arousal-Dominance (VAD) model of emotion (Bradley and Lang, 1994) is a well-established approach in psychology (Sander and Scherer, 2009) which increasingly attracts interest by NLP researchers (K¨oper and Schulte im Walde, 2016; Yu et al., 2016; Wang et al., 2016; Shaikh et al., 2016; Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016; Mohammad, 2018). The VAD model assumes that affective states can be characterized relative to Valence (corresponding to the concept of polarity), Arousal (the degree of calmness or excitement) and Do"
W19-2501,L16-1458,0,0.029027,"nt genres of historical writing. Finally, we used the Turney and Littman (2003) algorithm to induce historical sentiment information which is provided as part of JeSemE.org, a website for exploring semantic change in multiple diachronic corpora (Hellrich et al., 2018). algorithms which have been tested in diachronic settings in previous work. An overview of recent work focusing on applications to contemporary language is given by Buechel and Hahn (2018c). More than a decade ago, Turney and Littman (2003) introduced a frequently used and often adopted (e.g., K¨oper and Schulte im Walde (2016); Palogiannidi et al. (2016)) algorithm. It computes a sentiment score based on the similarity of an unrated word to two sets of positive and negative seed words. Bestgen (2008) presented an algorithm which has been prominently put into practice in expanding a VAD lexicon to up to 17,350 entries (Bestgen and Vincze, 2012). Their method employs a k-Nearest-Neighbor methodology where an unrated word inherits the averaged ratings of the surrounding words. Rothe et al. (2016) presented a more recent approach to polarity induction. Based on word embeddings and a set of positive and negative paradigm words, they train an ortho"
W19-2501,P16-2037,0,0.0258719,"a wide variety of dimensional axes including “good vs. bad”. Most previous work focused on varieties of such forms of semantic polarity, a rather simplified representation of the richness of human affective states—an observation increasingly recognized in sentiment analysis (Strapparava, 2016). In contrast to this bi-polar representation, the Valence-Arousal-Dominance (VAD) model of emotion (Bradley and Lang, 1994) is a well-established approach in psychology (Sander and Scherer, 2009) which increasingly attracts interest by NLP researchers (K¨oper and Schulte im Walde, 2016; Yu et al., 2016; Wang et al., 2016; Shaikh et al., 2016; Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016; Mohammad, 2018). The VAD model assumes that affective states can be characterized relative to Valence (corresponding to the concept of polarity), Arousal (the degree of calmness or excitement) and Dominance (perceived degree of control). Formally, VAD spans a three-dimensional real-valued space (see Figure 1) making the prediction of such values a multi-variate regression problem (Buechel and Hahn, 2016). Another popular line of emotion representation evolved around the notion of basic emotions, small sets of discret"
W19-2501,N18-1202,0,0.0157797,"pose SVDPPMI , a state-ofthe-art algorithm based on combining SVD with the positive pointwise mutual information (PPMI; Niwa and Nitta, 1994) word association metric. Both SGNS and SVDPPMI have been shown to be adequate for exploring historical semantics (Hamilton et al., 2016b,a). A general downside of existing embedding algorithms other than SVDPPMI is their inherent stochastic behavior during training which makes the resulting embedding models unreliable (Hellrich and Hahn, 2016; Antoniak and Mimno, 2018; Wendlandt et al., 2018). Very recently, contextualized word embeddings, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), have started to establish themselves as a new family of algorithms for word representation. Those methods achieve enhanced performance on many downstream tasks by taking context into account, both during training and testing, to generate an individual vector representation for each individual token. This makes them unsuitable for our contribution, since we address emotion on the type level by creating emotion lexicons. 3.2 s∈nearest(w,k) PARA S IM computes the emotion of word w by comparing its similarity with a set of positive and negative paradigm words (POS"
W19-2501,N18-1190,0,0.0577038,"in the early 1990’s (Deerwester et al., 1990; Sch¨utze, 1993). Levy et al. (2015) propose SVDPPMI , a state-ofthe-art algorithm based on combining SVD with the positive pointwise mutual information (PPMI; Niwa and Nitta, 1994) word association metric. Both SGNS and SVDPPMI have been shown to be adequate for exploring historical semantics (Hamilton et al., 2016b,a). A general downside of existing embedding algorithms other than SVDPPMI is their inherent stochastic behavior during training which makes the resulting embedding models unreliable (Hellrich and Hahn, 2016; Antoniak and Mimno, 2018; Wendlandt et al., 2018). Very recently, contextualized word embeddings, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), have started to establish themselves as a new family of algorithms for word representation. Those methods achieve enhanced performance on many downstream tasks by taking context into account, both during training and testing, to generate an individual vector representation for each individual token. This makes them unsuitable for our contribution, since we address emotion on the type level by creating emotion lexicons. 3.2 s∈nearest(w,k) PARA S IM computes the emotion of word w b"
W19-2501,W16-0404,0,0.187639,"Missing"
W19-2501,N16-1066,0,0.0299757,"ngs for words on a wide variety of dimensional axes including “good vs. bad”. Most previous work focused on varieties of such forms of semantic polarity, a rather simplified representation of the richness of human affective states—an observation increasingly recognized in sentiment analysis (Strapparava, 2016). In contrast to this bi-polar representation, the Valence-Arousal-Dominance (VAD) model of emotion (Bradley and Lang, 1994) is a well-established approach in psychology (Sander and Scherer, 2009) which increasingly attracts interest by NLP researchers (K¨oper and Schulte im Walde, 2016; Yu et al., 2016; Wang et al., 2016; Shaikh et al., 2016; Buechel and Hahn, 2017; Preot¸iuc-Pietro et al., 2016; Mohammad, 2018). The VAD model assumes that affective states can be characterized relative to Valence (corresponding to the concept of polarity), Arousal (the degree of calmness or excitement) and Dominance (perceived degree of control). Formally, VAD spans a three-dimensional real-valued space (see Figure 1) making the prediction of such values a multi-variate regression problem (Buechel and Hahn, 2016). Another popular line of emotion representation evolved around the notion of basic emotions, sm"
W19-2501,S15-2078,0,0.0198636,"mat. A NEW’s popular extension by Warriner et al. (2013) comprises roughly 14k entries acquired via crowdsourcing (see Table 1 for examples). Recently, researchers started to build computational models of the relationship between VAD and discrete categories (illustrated in Figure 1) resulting in techniques to automatically translate ratings between these major representation schemes (Calvo and Kim, 2013; Buechel and Hahn, 2018a). 2.2 Predicting Word Emotions Word emotion induction—the task of predicting the affective score of unrated words—is an active research area within sentiment analysis (Rosenthal et al., 2015). Most approaches either rely on hand-coded lexical resources, such as W ORD N ET (Fellbaum, 1998), to propagate sentiment information to unkown words (Shaikh et al., 2016), or employ similarity metrics based on distributional semantics (see below). We deem the former inadequate for diachronic purposes, since almost all lexical resources typically cover contemporary language only. In the following, we focus on 2 over time. This phenomenon is known as elevation & degeneration in historical linguistics, e.g., Old English cniht ‘boy, servant’ was elevated becoming the modern knight (Bloomfield, 1"
W19-2501,N16-1091,0,0.0611191,"Missing"
W19-3513,L18-1550,0,0.0528185,"A NALYSIS (Orme, 2009)16 and thus got scores between +1 (most neutral) and −1 (most vulgar). Scores were calculated by subtracting the percentage of times the term was chosen as worst from the percentage of times the term was chosen as best. We computed the split-half reliability16 like Kiritchenko and Mohammad (2017) by randomly splitting the annotations of a tuple into two halves, calculating scores independently for these halves and measuring the correlation between the resulting two sets of scores. We got an average Pearson correlation of 0.9102 (+/ − 0.0022) over 100 trials. FAST T EXT (Grave et al., 2018) word embeddings, the latter being based on C OMMON C RAWL and W IKIPEDIA. Yet, we not only incorporated plain text corpora or computationally derived lexical items (exploiting the FAST T EXT embeddings) into our study, but also included word embeddings as a representation format based on the distributional semantics hypothesis and computationally derived from corpora (see also Tulkens et al. (2016); Wiegand et al. (2018a). Utilizing the word embeddings from the corpora mentioned above and the G ENSIM modˇ uˇrek and Sojka, 2010) we further genule (Reh˚ erated, using the lexical seeds from the"
W19-3513,N18-1173,1,0.82658,"itivity, but we intentionally kept the ‘lexical noise’, i.e., presumably neutral words. Since we planned to annotate the lexical items identified this way by crowdsourcing in a later phase, these neutral words also help counterbalance the impact of rough and vulgar expressions during assessments. In total, based on this procedure we gathered a seed lexicon with 3,300 entries. 6 7 Automatic Lexicon Extension 7.1 Regression Models In order to further extend the lexicon in a purely automatic way and also inspired by studies on automatic word emotion induction (especially by Li et al. (2017a) and Buechel and Hahn (2018)) we employed regression models to predict scores for input words. The seed words served as training and testing data for a linear regression and a ridge regression model (linear regression with L2 regularization during training).17 As features for the words we used their respective word embeddings (this, obviously, excludes lexical items from further consideration for which no embeddings exist). We experimented with different word embeddings. We built 100-dimensional word embeddings from C ODE A LLTAGXL (Krieg-Holz et al., 2016) using W ORD 2V EC (Mikolov et al., 2013) for all words occurring"
W19-3513,W18-4405,0,0.0504484,"Missing"
W19-3513,W17-1415,0,0.0132274,"na encountered range from (political, religious, ethnic, sexual) harassment, flaming, cybertrolling, and cyberbullying to extremely evaluative (derogatory, hurtful, rough, rude, offensive, abusive, vulgar, taboo, obscene) language use (for a typological clarification attempt, cf. Waseem et al. (2017)). NLP research has recently directed its attention towards these unwarranted effects of social media activities and targets the automatic recognition of toxic language for the purpose of alerting and warning (Huang et al., 2018), filtering and blocking (Yoon et al., 2010; Ghauth and Sukhur, 2015; Chernyak, 2017; Wu et al., 2018), or reformulating suspicious contents of this type by non-obtrusive paraphrases (Su et al., 2017; Nogueira dos Santos et al., 2018). Introduction With the rapid diffusion of social media in our daily lives, we currently experience (and many of us foster) a fundamental change of social communication habits. A main feature of this new era is an unprecedented degree of public exposure and visibility of individuals via (very) large and intentionally open networks of “friends” or “followers.” Blogs, chat rooms and online fora constitute even Yet, how can we distinguish sloppy col"
W19-3513,N16-1095,0,0.233384,"hered entries tagged with corresponding categories9 from the German 10 https://www.openthesaurus.de The vast potential of the German language for productive noun composition within single compounds makes this decision less restrictive than it seems; cf., e.g., the English phrase form “son of a bitch” and its German single compound equivalent “Hurensohn”. 12 We only took words with a minimum frequency of 3. 11 7 https://de.wiktionary.org The exact terms and corresponding abbreviations are: ‘vulg¨ar’, ‘vulg.’, ‘vul.’, ‘derb’ and ‘abwertend’, ‘abw.’. 9 ‘vulg.’, ‘derb’, ‘abwertend’ 8 122 Mohammad (2016, 2017) to generate 2N decision alternatives (N denotes the size of our seed lexicon) and thus came up with 6,600 4-tuples to be assessed. Tuples were produced randomly under the premise that each term has to occur only once in eight different tuples and each tuple is unique. For the annotation process proper, we used the crowdsourcing platforms F IGURE E IGHT14 and C LICKWORKER,15 where each n-tuple was assessed by five annotators. In order to get realvalued scores from the BWS annotations we applied C OUNTS A NALYSIS (Orme, 2009)16 and thus got scores between +1 (most neutral) and −1 (most vulgar)."
W19-3513,W17-1106,0,0.0195975,"n terms of the property of interest). In our case, judges had to select the most neutral and the most vulgar terms per given n-tuple. We used the BWS tool13 from Kiritchenko and 14 https://www.figure-eight.com https://www.clickworker.de 16 Again we used the scripts from Kiritchenko and Mohammad (2016, 2017). 17 For both we used the scikit-learn.org implementation using the default parameters. 15 13 http://www.saifmohammad.com/WebPages/ BestWorst.html 123 et al. (2014) with a minimum word frequency of 5 and 100 dimensions (UKP), 300-dimensional FAST T EXT word embeddings from S PINNING B YTES (Cieliebak et al., 2017) trained on German tweets (T WITTER) and, finally, FAST T EXT word embeddings (Grave et al., 2018) based on C OM MON C RAWL and W IKIPEDIA ( FAST T EXT ). We also tried to utilize embeddings generated from the German T WITTER H ATESPEECH corpora from Ross et al. (2016) and Wiegand et al. (2018b) under the assumption that they might contain a large number of rough and vulgar words. But due to their small size and their nevertheless high proportion of out-of-vocabulary words we had to exclude both of these resources from further consideration. Table 1 shows that the ridge regression model perfor"
W19-3513,P17-2074,0,0.0231261,"Missing"
W19-3513,W18-5115,0,0.011299,"ted by experiments in the future. In any case, we plan to use and iteratively extend our newly developed lexicon on text corpora with similar biases into pejorative languages (including scores for obscenity). However, merely (automatically) extending a specialized lexicon might not necessarily prove beneficial as evidenced by the results of Tulkens et al. (2016) that showed no performance boost for a system using such an extended dictionary, at least for detecting Dutch racist language. In order to by-pass the sparse data problem, methods like transfer learning might also be appropriate here (Sahlgren et al., 2018). Still, the validity of these new items and their scores have to be experimentally validated, e.g., by feeding newly found lexical material back to annotators and compare their judgments with automatically predicted ones. We are also aware of the fact that purely lexically driven approaches to account for obscene, offensive or vulgar language may not be sufficient to solve the recognition problem completely and that a broader discourse context has to be taken into account, as well as the linguistic conventions in # Lexical Items 3,300 2,046 5,700 11,046 Table 3: Decomposition of contributions"
W19-3513,P18-2031,0,0.0131916,"derogatory, hurtful, rough, rude, offensive, abusive, vulgar, taboo, obscene) language use (for a typological clarification attempt, cf. Waseem et al. (2017)). NLP research has recently directed its attention towards these unwarranted effects of social media activities and targets the automatic recognition of toxic language for the purpose of alerting and warning (Huang et al., 2018), filtering and blocking (Yoon et al., 2010; Ghauth and Sukhur, 2015; Chernyak, 2017; Wu et al., 2018), or reformulating suspicious contents of this type by non-obtrusive paraphrases (Su et al., 2017; Nogueira dos Santos et al., 2018). Introduction With the rapid diffusion of social media in our daily lives, we currently experience (and many of us foster) a fundamental change of social communication habits. A main feature of this new era is an unprecedented degree of public exposure and visibility of individuals via (very) large and intentionally open networks of “friends” or “followers.” Blogs, chat rooms and online fora constitute even Yet, how can we distinguish sloppy colloquial language we all use here and there from explicitly abusive and inacceptable wording, the topic we focus on in this paper, i.e., the kind of li"
W19-3513,W17-1101,0,0.0309302,"W ORD N ETs, W IKTIONARY, or BABEL N ET), or employing some sort of machine learning process (Wiegand et al., 2018a). Yet, the semantic core of such lexicons are (manual or automatic) categorical assignments of either bi-polar (e.g., ‘Offensive’ vs. ‘Non-Offensive’) or multi-polar categories (e.g., ‘Colloquial’ vs. ‘Rough’ vs. ‘Obscene’). As an alternative to this scheme, our work focuses on substituting discrete categorical decisions by continuous grading of the above distinctions based on Best-Worst Scaling (Louviere et al., 2015). We thus target a research desideratum already described by Schmidt and Wiegand (2017, p.3-4) in the following way: “Despite their general effectiveness, relatively little is known about the creation process and the theoretical concepts that underlie the lexical resources that have been specially compiled for hate speech detection.” 3 type of civilized discourse. Primarily (yet not only), it addresses the lexical fields of sexuality (sexual organs and activities, in particular), as well as body orifices or other specific body parts (e.g., “Fresse” (“puss”) as a negative denotation for “Gesicht/Mund” (“face/mouth”)) and scatologic expressions. One often also observes meaning tr"
W19-3513,W17-3003,0,0.0128699,"ying to extremely evaluative (derogatory, hurtful, rough, rude, offensive, abusive, vulgar, taboo, obscene) language use (for a typological clarification attempt, cf. Waseem et al. (2017)). NLP research has recently directed its attention towards these unwarranted effects of social media activities and targets the automatic recognition of toxic language for the purpose of alerting and warning (Huang et al., 2018), filtering and blocking (Yoon et al., 2010; Ghauth and Sukhur, 2015; Chernyak, 2017; Wu et al., 2018), or reformulating suspicious contents of this type by non-obtrusive paraphrases (Su et al., 2017; Nogueira dos Santos et al., 2018). Introduction With the rapid diffusion of social media in our daily lives, we currently experience (and many of us foster) a fundamental change of social communication habits. A main feature of this new era is an unprecedented degree of public exposure and visibility of individuals via (very) large and intentionally open networks of “friends” or “followers.” Blogs, chat rooms and online fora constitute even Yet, how can we distinguish sloppy colloquial language we all use here and there from explicitly abusive and inacceptable wording, the topic we focus on"
W19-3513,W17-3012,0,0.0156186,"sition, we were able to raise its coverage up to slightly more than 11,000 entries. 1 These promiscuous communication groups face a high risk of anti-social behavior by aggressive, ruthless or entirely hostile actors (Dadvar et al., 2014; Wester et al., 2016; Li et al., 2017b; Talukder and Carbunar, 2018). The phenomena encountered range from (political, religious, ethnic, sexual) harassment, flaming, cybertrolling, and cyberbullying to extremely evaluative (derogatory, hurtful, rough, rude, offensive, abusive, vulgar, taboo, obscene) language use (for a typological clarification attempt, cf. Waseem et al. (2017)). NLP research has recently directed its attention towards these unwarranted effects of social media activities and targets the automatic recognition of toxic language for the purpose of alerting and warning (Huang et al., 2018), filtering and blocking (Yoon et al., 2010; Ghauth and Sukhur, 2015; Chernyak, 2017; Wu et al., 2018), or reformulating suspicious contents of this type by non-obtrusive paraphrases (Su et al., 2017; Nogueira dos Santos et al., 2018). Introduction With the rapid diffusion of social media in our daily lives, we currently experience (and many of us foster) a fundamental"
W19-3513,W16-0413,0,0.02177,"en determine the degrees of obscenity for the full set of all acquired lexical items by letting crowdworkers comparatively assess their pejorative grade using best-worst scaling. This semi-automatically enriched lexicon already comprises 3,300 lexical items and incorporates 33,000 vulgarity ratings. Using it as a seed lexicon for fully automatic lexical acquisition, we were able to raise its coverage up to slightly more than 11,000 entries. 1 These promiscuous communication groups face a high risk of anti-social behavior by aggressive, ruthless or entirely hostile actors (Dadvar et al., 2014; Wester et al., 2016; Li et al., 2017b; Talukder and Carbunar, 2018). The phenomena encountered range from (political, religious, ethnic, sexual) harassment, flaming, cybertrolling, and cyberbullying to extremely evaluative (derogatory, hurtful, rough, rude, offensive, abusive, vulgar, taboo, obscene) language use (for a typological clarification attempt, cf. Waseem et al. (2017)). NLP research has recently directed its attention towards these unwarranted effects of social media activities and targets the automatic recognition of toxic language for the purpose of alerting and warning (Huang et al., 2018), filteri"
W19-3513,N18-1095,0,0.0930765,"the productiveness of language and thus changing almost every day. Related Work Lexicons covering offensive language are almost only available for the English language. Perhaps the earliest collection of such lexical items (including phrases and multi-word expressions) is due to Razavi et al. (2010) who manually assembled approximately 2,700 dictionary entries. More recent work on an alternative verb-centered lexicon (size is not specified) with a focus on hate speech is reported by Gitari et al. (2015). The currently largest and most up to date English lexicon of abusive words is provided by Wiegand et al. (2018a) who manually and automatically collected around 8,500 lexical items.1 Languages other than English are incorporated in H URT L EX2 (Bassignana et al., 2018) which forms a multilingual lexical resource of words that hurt for 53 languages, among them Italian, Spanish, English and German. This lexicon grew out of a manual selection of roughly 1,000 Italian hate words originally organized around 17 categories, with particular focus on derogatory words. It was further semi-automatically extended with complementary borrowings from the Italian M ULTI W ORD N ET3 and BABEL N ET.4 H URT L EX also ex"
W19-3513,W18-5119,0,0.0120993,"ange from (political, religious, ethnic, sexual) harassment, flaming, cybertrolling, and cyberbullying to extremely evaluative (derogatory, hurtful, rough, rude, offensive, abusive, vulgar, taboo, obscene) language use (for a typological clarification attempt, cf. Waseem et al. (2017)). NLP research has recently directed its attention towards these unwarranted effects of social media activities and targets the automatic recognition of toxic language for the purpose of alerting and warning (Huang et al., 2018), filtering and blocking (Yoon et al., 2010; Ghauth and Sukhur, 2015; Chernyak, 2017; Wu et al., 2018), or reformulating suspicious contents of this type by non-obtrusive paraphrases (Su et al., 2017; Nogueira dos Santos et al., 2018). Introduction With the rapid diffusion of social media in our daily lives, we currently experience (and many of us foster) a fundamental change of social communication habits. A main feature of this new era is an unprecedented degree of public exposure and visibility of individuals via (very) large and intentionally open networks of “friends” or “followers.” Blogs, chat rooms and online fora constitute even Yet, how can we distinguish sloppy colloquial language w"
W19-4025,L18-1173,0,0.0266597,"ersit¨at Jena, Jena, Germany, &lt;firstname.lastname>@uni-jena.de ◦ Faculty of Media, Bauhaus-Universit¨at Weimar, Weimar, Germany, &lt;firstname.lastname>@uni-weimar.de Abstract lems with the lack of systematic support for hierarchically structured tag labels where one label is semantically more general than another (e.g., the general tag anamnesis in relation to more specific ones like family anamnesis). Finally, and this point addresses a more general design desideratum, we encountered a substantial lack of continuous quality control mechanisms in the majority of annotation tools (the WASA tool (AlGhamdi and Diab, 2018) is one of the rare exceptions and shares several design goals with WAT-S L 2.0). This shortcoming requires annotation project managers to reach for external tools for statistical evaluation. As a consequence, shifting back and forth between annotation and evaluation environments slows down the overall progress of the entire annotation project and hampers iterative refinement of annotation guidelines. Yet, a close technical coupling of such testdevelopment cycles within one integrated platform is a particularly fruitful strategy in complex annotation campaigns. As a remedy for these problems,"
W19-4025,J08-4004,0,0.0606187,"nt environments and easy to customize. Plain text files are used as input, each line containing one segment for labeling. Results as well as logging information (e.g., time stamps) are stored in key-value files. These easy to process formats made WAT-S L 1.0 already well-suited for large-scale annotation projects and were further extended by us as described in Section 3.3. 216 Figure 2: Sublabels of anamnesis tag in a secondary drop-down menu shown when the user clicks on the superlabel anamnesis; the bold and underlined letters display the shortcuts of the labels. tation decisions. Following Artstein and Poesio (2008), we prefer it over a range of alternative measures, like Cohen’s κ (Cohen, 1960), which are overly sensitive to individual annotators’ decisions when modeling chance agreement. Based on such kind of statistical evidence, continuous quality monitoring allows annotation project managers to assess the difficulty of tasks, allowing for a swift refinement of annotation guidelines. This feature was implemented by calculating coincidence matrices for each task with DKP RO AGREEMENT (Meyer et al., 2014). Following annotator feedback during early iterations of our annotation project, we also introduce"
W19-4025,rak-etal-2014-interoperability,0,0.0230067,"ping WAT-S L 2.0, an open-source web-based annotation tool for long-segment labeling, hierarchically structured label sets and built-ins for quality control. 1 Introduction In the course of large-scale annotation campaigns on medical full-text corpora, we encountered several shortcomings of the current generation of annotation tools. Labeling long-spanning text segments (e.g., entire sentences or even paragraphs) is a major issue here that is only insufficiently supported by general purpose open-source annotation tools (M¨uller and Strube, 2006; Stenetorp et al., 2012; Bontcheva et al., 2013; Rak et al., 2014; Yimam et al., 2014) which typically aim at annotating (much) shorter text spans for entities and relations. This is especially troublesome given the increasing availability of full texts and even books as input for annotation projects. With annotation schemes becoming more and more conceptually structured, we also faced prob1 https://github.com/webis-de/wat 215 Proceedings of the 13th Linguistic Annotation Workshop, pages 215–219 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics guide (re)definition The user interface provides annotators not only with a single"
W19-4025,E12-2021,0,0.120789,"Missing"
W19-4025,E17-3004,1,0.883369,"Missing"
W19-4025,P14-5016,0,0.0541898,"Missing"
wermter-hahn-2004-annotated,brants-2000-inter,0,\N,Missing
wermter-hahn-2004-annotated,hahn-schulz-2002-towards,1,\N,Missing
