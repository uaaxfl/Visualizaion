2020.emnlp-main.428,P14-1035,0,0.0579505,"Missing"
2020.emnlp-main.428,W12-3809,0,0.0306301,"eds, while respecting social norms about the appropriate intimacy (Chaikin and Derlega, 1974; Korobov and Thorne, 2006). In this paper, we aim to quantify the intimacy expressed in language and demonstrate how this intimacy is constructed and employed across diverse social settings. While sociolinguistics and social psychology have long pointed to how people shape their language to convey social information (Labov, 1972; Brown and Levinson, 1978; Clark and Schunk, 1980; Weber, 2008; Locher and Graham, 2010), only recently, have computational models focused on making this information explicit (Choi et al., 2012; Danescu-Niculescu-Mizil et al., 2013; Bak et al., 2014). In particular, works on social status and power have shown how individuals use lexical cues and linguistic strategies like accommodation to express their perceived status in relation to others (Danescu-Niculescu-Mizil et al., 2013; Prabhakaran et al., 2014). Much like status in society, intimacy is a natural concept describing how an individual relates to their audience in their perceived interdependence, warmth, and willingness to personally share (Perlman and Fehr, 1987). Our work provides the first model of intimacy in language and"
2020.emnlp-main.428,W11-0609,0,0.0394443,"their screen names e.g., Stephen Curry), removing all emojis, and removing all URLs. After removing duplicates and self-replies, this process yielded 1.04M questions. Book questions were collected from 51,224 English books on Project Gutenberg (Hart, 1992). BookNLP (Bamman et al., 2014) is used to identify characters’ quotes and we identify 2.02M quotes ending with a question mark and having at least four words. We keep the full quote as a question, as the extended context was deemed necessary for correct interpretation. Movie questions were extracted from the Cornell movie dialogue dataset (Danescu-Niculescu-Mizil and Lee, 2011), where all dialog lines ending with a question mark and at least four words are treated as questions, which yields 53,507 questions. To test the generalizability of our model on these domains, the annotated data included 50 questions from each non-Reddit source, which were not included in the training data. Over this external dataset, our best-performing model achieved 0.6684, 0.6602, and 0.5233 Pearson’s r correlations on the intimacy ratings for book, Twitter, and movie questions, respectively. These moderatelyhigh correlations demonstrate the generalizability of our model on outer domain d"
2020.emnlp-main.428,P13-1025,0,0.199828,"ng social norms about the appropriate intimacy (Chaikin and Derlega, 1974; Korobov and Thorne, 2006). In this paper, we aim to quantify the intimacy expressed in language and demonstrate how this intimacy is constructed and employed across diverse social settings. While sociolinguistics and social psychology have long pointed to how people shape their language to convey social information (Labov, 1972; Brown and Levinson, 1978; Clark and Schunk, 1980; Weber, 2008; Locher and Graham, 2010), only recently, have computational models focused on making this information explicit (Choi et al., 2012; Danescu-Niculescu-Mizil et al., 2013; Bak et al., 2014). In particular, works on social status and power have shown how individuals use lexical cues and linguistic strategies like accommodation to express their perceived status in relation to others (Danescu-Niculescu-Mizil et al., 2013; Prabhakaran et al., 2014). Much like status in society, intimacy is a natural concept describing how an individual relates to their audience in their perceived interdependence, warmth, and willingness to personally share (Perlman and Fehr, 1987). Our work provides the first model of intimacy in language and tests its implications. In this paper,"
2020.emnlp-main.428,D14-1211,0,0.0284537,"nd social psychology have long pointed to how people shape their language to convey social information (Labov, 1972; Brown and Levinson, 1978; Clark and Schunk, 1980; Weber, 2008; Locher and Graham, 2010), only recently, have computational models focused on making this information explicit (Choi et al., 2012; Danescu-Niculescu-Mizil et al., 2013; Bak et al., 2014). In particular, works on social status and power have shown how individuals use lexical cues and linguistic strategies like accommodation to express their perceived status in relation to others (Danescu-Niculescu-Mizil et al., 2013; Prabhakaran et al., 2014). Much like status in society, intimacy is a natural concept describing how an individual relates to their audience in their perceived interdependence, warmth, and willingness to personally share (Perlman and Fehr, 1987). Our work provides the first model of intimacy in language and tests its implications. In this paper, we examine the intimacy of questions. As requests for information, questions provide a natural mechanism for studying how people shape the intimacy of their questions in response to the social context (Clark and Schunk, 1980; Jordan and Roloff, 1990). Questions serve a fundame"
2020.emnlp-main.45,C14-1079,0,0.0121403,"makes for an effective supportive message? Here, we perform the first major study of condolence in social media, examining what type of distress individuals seek support for, what linguistics factors are more likely to elicit condolence, and what types of condolence viewed as more helpful. Distress and emotional support have long been explored in work in social psychology and counseling (Burleson et al., 2009; Rack et al., 2008), frequently around bereavement and helping victims of abuse. NLP works have only recently examined emotional support in online spaces for mental and physical health (Biyani et al., 2014; Navindgi et al., 2016; Wang et al., 2015) and in communities oriented around goals like weight-loss (Manikonda et al., 2014); however, these focus on the general concept of supportiveness. In this work, we examine distress as a universal phenomenon—not just related to health and death—and examine the strategies and helpfulness of responses to this distress. This study aims to computationally identify mechanisms and strategies for delivering effective and impactful condolence on social media. Conveying condolence is often difficult for many people (Cameron et al., 2019), who fall back to comm"
2021.emnlp-main.25,D19-1410,0,0.0161021,"a certain threshold, the two texts are judged to be written by the same author. Task definition Given a collection of text pairs by multiple authors X = pt−1 {(xp11 , xp21 ), . . . , (xn−1 , xpnt )} from domain P = {p1 , p2 , . . . , pt } and labels Y = {yi , y2 , . . . , yn }, we aim to identify a function fθ that can determine whether two text samples xi and xj are written by the same author (y = 1) or different authors (y = 0). Stylometric similarity learning Our model for extracting stylistic embeddings from input texts is the same as the Sentence RoBERTa or BERT network (SBERT/SRoBERTa) (Reimers and Gurevych, 2019). For a text pair (xi , xj ), the Siamese model fθ maps both text samples into embedding vectors (zi , zj ) in the latent space such that zi = fθ (xi ) and zj = fθ (xj ). Rather than using the [cls] token as the representation, we use attention pooling to merge the last hidden states [h0 , . . . , hk ] into a single embedding vector to represent textual style. ho =AttentionPool([h0 , . . . , hk ]) z =W 1 · σ(W 2 · ho + b2 ) + b1 L(s) = L(s) =  X 1 Softplus LogSumExp |P − | i,j∈P − (3)  α · [τd − d(zi , zj )] L = L(s) + L(d) (4) where Softplus(z) = log(1 + ez ) is a continuous approximation"
2021.findings-emnlp.276,W12-1631,0,0.0166742,"n turns using gifs, including captions and metadata, and develop a new conversational model P EPE THE K ING P RAWN that selects 7 Related Work appropriate gif responses for messages through This work draws upon two strands of research from comparing encoded gif and text representations. In dialog systems and multimodal NLP. Conversa- two evaluations, we show that P EPE is able to gentional dialog systems have traditionally been built erate highly-relevant gif responses and in a largeupon large-scale dialog corpora from social me- scale RCT, we show that the gif replies from the dia platforms (Bessho et al., 2012) such as Twitter. P EPE model received significantly higher scores Our approaches are fundamentally information re- from the general public. Our work demonstrates trieval based systems that mirror the approach by the opportunity for using NLP methods to successtext-based conversational systems that retrieve ex- fully engage in multimodal conversations. 3236 9 Ethics ate gif, which is mitigated by the use of Giphy to seed our initial gifs. As this platform is curated and does not host objectively offensive gifs (e.g., overly-violent content), our initial gif set is relatively free of objectiona"
2021.findings-emnlp.276,W19-5935,0,0.0279601,"higan jurgens@umich.edu PizzaMagic: Ahhhhh!!! The EMNLP deadline is in 24 hours!! x CasualModel: Figure 1: Gif responses in conversation like the one shown above are embodied dialog that use visual imagery to convey reactions and emotions. This paper develops a system to select the appropriate gif response to messages. (PDF best viewed with Adobe Acrobat) Conversation analysis is central to NLP and multiple approaches have analyzed this dialog structure (Jurafsky et al., 1998; Pareti and Lando, 2018; Conversations are central to many online social platforms. While most conversations are text- Cohn et al., 2019) and developed conversational based, computer mediated dialog also affords al- agents to engage with people (e.g., Fang et al., 2018; Xu et al., 2020; Hong et al., 2020). Recent ternative forms of communication, such as emoji work has focused on generating open domain social or stickers like bitmoji, that allow users to exchatbots that engage in sustained conversations in press themselves (Tang and Hew, 2019; Konrad a natural way (Ram et al., 2018). Because many et al., 2020). Increasingly, these visual forms of of these systems are designed to support voicecommunication have become common in"
2021.findings-emnlp.276,N18-5020,0,0.0246989,"ion like the one shown above are embodied dialog that use visual imagery to convey reactions and emotions. This paper develops a system to select the appropriate gif response to messages. (PDF best viewed with Adobe Acrobat) Conversation analysis is central to NLP and multiple approaches have analyzed this dialog structure (Jurafsky et al., 1998; Pareti and Lando, 2018; Conversations are central to many online social platforms. While most conversations are text- Cohn et al., 2019) and developed conversational based, computer mediated dialog also affords al- agents to engage with people (e.g., Fang et al., 2018; Xu et al., 2020; Hong et al., 2020). Recent ternative forms of communication, such as emoji work has focused on generating open domain social or stickers like bitmoji, that allow users to exchatbots that engage in sustained conversations in press themselves (Tang and Hew, 2019; Konrad a natural way (Ram et al., 2018). Because many et al., 2020). Increasingly, these visual forms of of these systems are designed to support voicecommunication have become common in social based dialog, they overlook non-textual forms of media (Bourlai and Herring, 2014; Highfield and interaction used in social m"
2021.findings-emnlp.276,P19-1349,0,0.0234235,"ighfield and interaction used in social media conversations. In Leaver, 2016), with a notable use of the reaction gif (Bakhshi et al., 2016; Miltner and Highfield, 2017). parallel, multimodal NLP systems have been developed for image data, often focusing on image-toThese gifs are short video sequences that depict a text tasks such as image captioning (Melas-Kyriazi particular scene and sometimes contain text that et al., 2018; Sharma et al., 2018) and visual quesacts as a meta-commentary (Eppink, 2014). As a result, conversations become multimodal where in- tion answering (Antol et al., 2015; Huang et al., 2019; Khademi, 2020). More recent work has fodividuals reply to one another using combinations cused on the reverse text-to-image dimension, such of text and gifs (Figure 1). While conversational AI systems have been developed in a purely text- as generating an image from a description (Niu et al., 2020; Ramesh et al., 2021). Our work unites based setting, such systems do not capture the full these two strands of research by integrating imagemultimodal behavior seen online. Here, we study based communication into conversational agents. multimodal conversation by introducing new dialog models for s"
2021.findings-emnlp.276,W16-3648,0,0.0256436,"lies do receive more subsequent conversation. We speculate that the random models may have led to more conversation due to users replying to express confusion about why the particular gif was used. This result points to a need to understand what text and visual factors in gifs influence the volume of subsequent dialog and an opportunity to optimize gif models for both quality and number of conversation turns. isting messages from a large social media corpus as potential replies and rank these to select a response. Our work mirrors models that use neural networks for ranking (Yan et al., 2016; Inaba and Takahashi, 2016; Penha and Hauff, 2021, e.g.,); however, we note that many recent knowledge-grounded and open domain models use encoder-decoder methods to improve versatility and applicability (e.g., Ghazvininejad et al., 2018; Gao et al., 2019; Zhou et al., 2020). Generative approaches are likely inappropriate for gif-based conversation as gifs are more akin to mimetic artifacts that build on cultural knowledge (Eppink, 2014), making synthesizing a new gif from scratch likely less effective. All three models used here rely on joint embedding spaces for gif and text. Multiple works in NLP have been proposed"
2021.findings-emnlp.276,W98-0319,0,0.65037,"real users, we show that our model replies with gifs that are significantly better received by the community. David Jurgens University of Michigan jurgens@umich.edu PizzaMagic: Ahhhhh!!! The EMNLP deadline is in 24 hours!! x CasualModel: Figure 1: Gif responses in conversation like the one shown above are embodied dialog that use visual imagery to convey reactions and emotions. This paper develops a system to select the appropriate gif response to messages. (PDF best viewed with Adobe Acrobat) Conversation analysis is central to NLP and multiple approaches have analyzed this dialog structure (Jurafsky et al., 1998; Pareti and Lando, 2018; Conversations are central to many online social platforms. While most conversations are text- Cohn et al., 2019) and developed conversational based, computer mediated dialog also affords al- agents to engage with people (e.g., Fang et al., 2018; Xu et al., 2020; Hong et al., 2020). Recent ternative forms of communication, such as emoji work has focused on generating open domain social or stickers like bitmoji, that allow users to exchatbots that engage in sustained conversations in press themselves (Tang and Hew, 2019; Konrad a natural way (Ram et al., 2018). Because"
2021.findings-emnlp.276,2020.acl-main.643,0,0.0230251,"tion used in social media conversations. In Leaver, 2016), with a notable use of the reaction gif (Bakhshi et al., 2016; Miltner and Highfield, 2017). parallel, multimodal NLP systems have been developed for image data, often focusing on image-toThese gifs are short video sequences that depict a text tasks such as image captioning (Melas-Kyriazi particular scene and sometimes contain text that et al., 2018; Sharma et al., 2018) and visual quesacts as a meta-commentary (Eppink, 2014). As a result, conversations become multimodal where in- tion answering (Antol et al., 2015; Huang et al., 2019; Khademi, 2020). More recent work has fodividuals reply to one another using combinations cused on the reverse text-to-image dimension, such of text and gifs (Figure 1). While conversational AI systems have been developed in a purely text- as generating an image from a description (Niu et al., 2020; Ramesh et al., 2021). Our work unites based setting, such systems do not capture the full these two strands of research by integrating imagemultimodal behavior seen online. Here, we study based communication into conversational agents. multimodal conversation by introducing new dialog models for selecting gif rep"
2021.findings-emnlp.276,L18-1460,0,0.0215871,"t our model replies with gifs that are significantly better received by the community. David Jurgens University of Michigan jurgens@umich.edu PizzaMagic: Ahhhhh!!! The EMNLP deadline is in 24 hours!! x CasualModel: Figure 1: Gif responses in conversation like the one shown above are embodied dialog that use visual imagery to convey reactions and emotions. This paper develops a system to select the appropriate gif response to messages. (PDF best viewed with Adobe Acrobat) Conversation analysis is central to NLP and multiple approaches have analyzed this dialog structure (Jurafsky et al., 1998; Pareti and Lando, 2018; Conversations are central to many online social platforms. While most conversations are text- Cohn et al., 2019) and developed conversational based, computer mediated dialog also affords al- agents to engage with people (e.g., Fang et al., 2018; Xu et al., 2020; Hong et al., 2020). Recent ternative forms of communication, such as emoji work has focused on generating open domain social or stickers like bitmoji, that allow users to exchatbots that engage in sustained conversations in press themselves (Tang and Hew, 2019; Konrad a natural way (Ram et al., 2018). Because many et al., 2020). Incr"
2021.findings-emnlp.276,P18-1238,0,0.0701314,"Missing"
2021.findings-emnlp.276,2020.acl-main.166,0,0.0258948,"own above are embodied dialog that use visual imagery to convey reactions and emotions. This paper develops a system to select the appropriate gif response to messages. (PDF best viewed with Adobe Acrobat) Conversation analysis is central to NLP and multiple approaches have analyzed this dialog structure (Jurafsky et al., 1998; Pareti and Lando, 2018; Conversations are central to many online social platforms. While most conversations are text- Cohn et al., 2019) and developed conversational based, computer mediated dialog also affords al- agents to engage with people (e.g., Fang et al., 2018; Xu et al., 2020; Hong et al., 2020). Recent ternative forms of communication, such as emoji work has focused on generating open domain social or stickers like bitmoji, that allow users to exchatbots that engage in sustained conversations in press themselves (Tang and Hew, 2019; Konrad a natural way (Ram et al., 2018). Because many et al., 2020). Increasingly, these visual forms of of these systems are designed to support voicecommunication have become common in social based dialog, they overlook non-textual forms of media (Bourlai and Herring, 2014; Highfield and interaction used in social media conversation"
2021.findings-emnlp.288,2020.emnlp-main.656,0,0.0714434,"Missing"
2021.findings-emnlp.288,2020.emnlp-main.44,1,0.887221,"Missing"
2021.findings-emnlp.288,D19-1176,1,0.839088,"Missing"
2021.findings-emnlp.288,D19-1481,0,0.0429927,"Missing"
2021.findings-emnlp.288,D19-1107,0,0.0635706,"Missing"
2021.findings-emnlp.288,N19-1166,0,0.0209439,"ce, 2016). Moderated comments offer significant benefit to the study of supporting moderators and authorities in their goals of having supportive technologies that match their community’s norms. At the same time, users who made those comments may object to having them included in a dataset (Fiesler and Proferes, 2018). Therefore, we take additional measures to ensure that user privacy is protected, especially for the deleted comments. We use Reddit data through Pushshift (Baumgartner et al., 2020), an archive that has been widely used in NLP and related fields since its first release in 2015 (Hessel and Lee, 2019; Kennedy et al., 2020; Sap et al., 2020; Dinan et al., 2020, among many others). Pushshift’s collection policy explicitly states that it conforms to Reddit’s rules and user agreement with regards to data collection. In releasing our dataset, we provide only the associated identifiers of comments but not their textual content. Practitioners will need to independently fetch the texts from Pushshift by using the provided comment IDs. Releasing only IDs ensures that any users who request their data to be removed in Pushshift will also have it removed in our dataset. Additionally, in our dataset w"
2021.findings-emnlp.288,P19-1357,1,0.835507,"independently taken out of context. Pavlopoulos et al. (2020) challenged this assumption and examined if context matters in toxic language detection. While they found a significant number of human annotation labels were changed when context is additionally given, they could not find evidence that context actually improves the performance of classifiers. Our work also examines the importance of context, but we do not limit our scope to toxic language detection and investigate a broader set of community norm violation ranging from formatting issues to trolling. Beyond Incivility and Hate Speech Jurgens et al. (2019) claims “abusive behavior online falls along a spectrum, and current approaches focus only on a narrow range” and urges to expand the scope of problems in online abuse. Most work on online conversation has been focused on certain types of rule violation such as incivility and toxic language (e.g., Zhang et al., 2018; Chang and Danescu-Niculescu-Mizil, 2019; Almerekhi et al., 2020). In this work, we focus on a broader concept of community norm violation and provide a new dataset and tasks to facilitate future research in this direction. 7 Conclusion comments are made, and, moreover, have focuse"
2021.findings-emnlp.288,2020.acl-main.483,0,0.0203578,"omments offer significant benefit to the study of supporting moderators and authorities in their goals of having supportive technologies that match their community’s norms. At the same time, users who made those comments may object to having them included in a dataset (Fiesler and Proferes, 2018). Therefore, we take additional measures to ensure that user privacy is protected, especially for the deleted comments. We use Reddit data through Pushshift (Baumgartner et al., 2020), an archive that has been widely used in NLP and related fields since its first release in 2015 (Hessel and Lee, 2019; Kennedy et al., 2020; Sap et al., 2020; Dinan et al., 2020, among many others). Pushshift’s collection policy explicitly states that it conforms to Reddit’s rules and user agreement with regards to data collection. In releasing our dataset, we provide only the associated identifiers of comments but not their textual content. Practitioners will need to independently fetch the texts from Pushshift by using the provided comment IDs. Releasing only IDs ensures that any users who request their data to be removed in Pushshift will also have it removed in our dataset. Additionally, in our dataset we anonymize individual"
2021.findings-emnlp.288,2020.acl-main.486,0,0.01354,"ant benefit to the study of supporting moderators and authorities in their goals of having supportive technologies that match their community’s norms. At the same time, users who made those comments may object to having them included in a dataset (Fiesler and Proferes, 2018). Therefore, we take additional measures to ensure that user privacy is protected, especially for the deleted comments. We use Reddit data through Pushshift (Baumgartner et al., 2020), an archive that has been widely used in NLP and related fields since its first release in 2015 (Hessel and Lee, 2019; Kennedy et al., 2020; Sap et al., 2020; Dinan et al., 2020, among many others). Pushshift’s collection policy explicitly states that it conforms to Reddit’s rules and user agreement with regards to data collection. In releasing our dataset, we provide only the associated identifiers of comments but not their textual content. Practitioners will need to independently fetch the texts from Pushshift by using the provided comment IDs. Releasing only IDs ensures that any users who request their data to be removed in Pushshift will also have it removed in our dataset. Additionally, in our dataset we anonymize individual usernames and per"
2021.findings-emnlp.288,W19-3507,0,0.034109,"-sensitive content violation types that actually occur in the wild. To enable future research on detecting community-specific norm violations, we constructed a dataset that retrieves online conversation threads and comments deleted by moderators, categorized by community norm violations. We discuss ethical considerations related to protecting user privacy in §2. Additionally, we acknowledge that the dataset itself can incorporate unintentional biases. For example, it can incorporate moderators’ biases in deciding which comments are selected to be removed (Binns et al., 2017; Myers West, 2018; Shen and Rose, 2019). The unmoderated comments can include norm-violating comments that were missed by the moderators (Chandrasekharan et al., 2018). By constructing a large scale dataset that spans multiple subreddits and moderators’ teams we partially mitigate these concerns. To investigate this further, future work could incorporate an additional evaluation procedure with test sets containing held-out moderators (cf. Geva et al., 2019). Online communities establish their own norms for what is acceptable behavior. However, current NLP methods for identifying unacceptable behavior have largely overlooked the con"
2021.findings-emnlp.288,N16-2013,0,0.0347584,"paired unmoderated conversations as a control set. Each moderated conversation is matched with up to two unmoderated conversations from the same post and with most 4 Ranked by number of subscribers as of April 2021 5 Prior work has created datasets used to detect sinWe were unable to retrieve an additional 21K removed gle types of norm violations in social media mes- norm-violating comments, which were unavailable in the PushShift archive. We still include these corresponding consages (e.g. incivility, hate speech or hostility) versations in our data release as they can be useful in the task (Waseem and Hovy, 2016; Founta et al., 2018). of forecasting future norm violations. 3387 Rule Types Advertising Moderation Enforcement similar conversation lengths as the target moderated conversation. Ethical Considerations for Protecting User Privacy Our dataset focuses, in part, on comments that moderators have viewed as objectionable and therefore removed. While these moderated comments are still publicly available, their use requires additional ethical reflection and precautions to preserve the dignity and privacy of users (Townsend and Wallace, 2016). Moderated comments offer significant benefit to the study"
2021.findings-emnlp.288,P18-1125,0,0.0205568,"ves the performance of classifiers. Our work also examines the importance of context, but we do not limit our scope to toxic language detection and investigate a broader set of community norm violation ranging from formatting issues to trolling. Beyond Incivility and Hate Speech Jurgens et al. (2019) claims “abusive behavior online falls along a spectrum, and current approaches focus only on a narrow range” and urges to expand the scope of problems in online abuse. Most work on online conversation has been focused on certain types of rule violation such as incivility and toxic language (e.g., Zhang et al., 2018; Chang and Danescu-Niculescu-Mizil, 2019; Almerekhi et al., 2020). In this work, we focus on a broader concept of community norm violation and provide a new dataset and tasks to facilitate future research in this direction. 7 Conclusion comments are made, and, moreover, have focused on a relatively small set of unacceptable behaviors such as incivility. In this work, we introduce a new dataset, N ORM V IO, of 51K conversations grounded with community-specific judgements of which rule is violated. Using this data, we develop new models for detecting context-sensitive rule violations, demonstra"
2021.findings-emnlp.288,2020.acl-main.396,0,0.219554,"uch as Meta-rules and Trolling; we the performance was relatively more uniform and speculate that this decreased performance is due to additional context did not contribute as much. This the increased number of parameters from adding context encoder layer to process conversation his- result suggests that providing full text of rules may help resolve certain ambiguous comments and thus tory and future work with more examples of these violations may substantially improve performance. the model rely less on the additional context. This result for history greatly expands an analysis 5 Analysis by Pavlopoulos et al. (2020) that found minimal performance gain when adding a single prior com- How many violations do current systems ment to identify toxicity; while we too find minimal miss? In part due to their targeted focus, improvement for Incivility and Harassment norms, the P ERSPECTIVE and I NCIVIL H ATE baseline adding history does improve the recognition for models miss a substantial proportion of the total other norm violations (e.g., Format and Content) norm violations. Figure 6 shows the confusion maindicating that prior context can be useful. trices of the violation detection task, where labels While the"
2021.naacl-main.178,C18-1135,0,0.347684,", as discussed most in offline studies, could Conde-Silvestre, 2012; Sharma and Dodsworth, be an emergent byproduct of network size. These 2020). Except for a few recent simulation studtopic-based communities also do not experience ies (Reali et al., 2018), researchers have rarely exstrong levelling due to increased contact. Second, plored how the global properties of social networks emerging studies in online communities (Danescusystematically affect lexical change, although the Niculescu-Mizil et al., 2013; Stewart and Eisenweak tie model does predict an influence of social stein, 2018; Del Tredici and Fernández, 2018) fonetwork at the macro-level. In addition, while there cus exclusively on lexical change at the individual are lexicographic studies attempting to enumerate or word level. Few investigate how global netfactors that affect the acceptance of neologisms work properties affect lexical change at the com(Metcalf, 2004; Barnhart, 2007), network structures munity level. Finally, sampling offline networks are rarely taken into consideration. A key limitapresents practical difficulties, we extract complete tion of previous works has been access to a large networks for thousands of online communities, p"
2021.naacl-main.178,D19-5508,0,0.0269978,"is study overcomes. reddit_network and replication details are available in Appendix A. 2202 Lexical change in online communities The rise of social media and the proliferation of Internet speech has drawn increasing attention to lexical change in online communities, including Twitter (Eisenstein et al., 2014; Goel et al., 2016), Reddit (Altmann et al., 2011; Stewart and Eisenstein, 2018; Del Tredici and Fernández, 2018) and review sites (Danescu-Niculescu-Mizil et al., 2013). It has been shown that the usage of certain words is associated with community loyalty and norms (Zhang et al., 2017; Bhandari and Armstrong, 2019) and indicative of user behaviors (Danescu-Niculescu-Mizil et al., 2013; Noble and Fernández, 2015; Chang and Danescu-Niculescu-Mizil, 2019; Klein et al., 2019). Specifically for lexical change over time, Stewart and Eisenstein (2018) investigate the survival of lexical items in Reddit, and conclude that a word’s appearance in more diverse linguistic contexts is the strongest predictor of its survival while social dissemination is a comparatively weaker predictor. Del Tredici and Fernández (2018) examined the use of neologisms in 20 subreddit communities. Their finding that weak-tie users tend"
2021.naacl-main.179,P15-2072,0,0.170114,"ed approach to ground our work in Framing effects Studies of framing typically focus on either frame-building or frame- framing research and to enable robust evaluation. setting (Scheufele, 1999; de Vreese, 2005). We draw inspiration from a growing body of Frame-building is the process by which external NLP research that uses supervised approaches to factors, such as a journalist’s ideology or eco- detect issue-generic policy frames in news artinomic pressures, influence what frames are used; cles, a task popularized by the Media Frames frame-building studies thus treat framing as the Corpus (Card et al., 2015), which contains issuedependent variable. Frame-setting studies treat generic policy frame labels for articles across sevframes as independent variables that impact how eral issues (Boydstun et al., 2013). Using this an audience interprets and evaluates issues. corpus, prior work has detected frames with techPrior analyses of frame-building in immigration niques including logistic regression (Card et al., 2220 Frame Type Issue-Generic Policy Immigration Specific Narrative Frame Economic Capacity & Resources Morality & Ethics Fairness & Equality Legality, Constitutionality & Jurisdiction Crime"
2021.naacl-main.179,D16-1148,0,0.18433,"s resonate with audiences through interactive signals such as retweets and favorites. By jointly analyzing the production and reception of frames on Twitter, we provide an in-depth analysis of immigration framing by and on the public. Political communications research has identified numerous typologies of frames, such as issuegeneric policy, immigration-specific, and narrative. Each of these frame types can significantly shape the audience’s perceptions of an issue (Iyengar, 1991; Chong and Druckman, 2007; Lecheler et al., 2015), but prior NLP work seeking to detect frames in mass media (e.g. Card et al., 2016; Field et al., 2018; Kwak et al., 2020) has largely been limited to a single issue-generic policy typology. Multiple dimensions of framing must be considered in order to better understand the structure of immigration discourse and its effect on public opinion and attitudes. We thus create a novel dataset of immigration-related tweets containing labels for each typology to facilitate more nuanced computational analyses of framing. Framing selects particular aspects of an issue and makes them salient in communicating a message (Entman, 1993). Framing can impact how people understand issues, att"
2021.naacl-main.179,D18-1393,0,0.839791,"iences through interactive signals such as retweets and favorites. By jointly analyzing the production and reception of frames on Twitter, we provide an in-depth analysis of immigration framing by and on the public. Political communications research has identified numerous typologies of frames, such as issuegeneric policy, immigration-specific, and narrative. Each of these frame types can significantly shape the audience’s perceptions of an issue (Iyengar, 1991; Chong and Druckman, 2007; Lecheler et al., 2015), but prior NLP work seeking to detect frames in mass media (e.g. Card et al., 2016; Field et al., 2018; Kwak et al., 2020) has largely been limited to a single issue-generic policy typology. Multiple dimensions of framing must be considered in order to better understand the structure of immigration discourse and its effect on public opinion and attitudes. We thus create a novel dataset of immigration-related tweets containing labels for each typology to facilitate more nuanced computational analyses of framing. Framing selects particular aspects of an issue and makes them salient in communicating a message (Entman, 1993). Framing can impact how people understand issues, attribute responsibilit"
2021.naacl-main.179,2021.ccl-1.108,0,0.0490663,"Missing"
2021.semeval-1.31,2020.acl-main.740,0,0.0441713,"Missing"
2021.semeval-1.31,2021.semeval-1.6,0,0.14549,", 2018; Jurgens et al., 2019). NLP methods have been developed to identify these comments, often relying on deep-language models (Vidgen et al., 2019). However, the part of the message that is specifically toxic is often unknown. Such information is useful not only for validating and explaining the judgments of models (Carton et al., 2018), but can also be useful for moderators to use when making decisions and working with these models in their deployment (Carton et al., 2020; Liu et al., 2021). This paper describes our model1 and error analysis for SemEval-2021 Task 5: Toxic Spans Detection (Pavlopoulos et al., 2021). Our model uses a deep learning approach to identify which tokens are toxic. The approach 1 The code is available at https://github.com/ davidjurgens/offensive-span-detection. System Description Our core system relies on a standard RoBERTa model (Liu et al., 2019) that is trained on a sequence-to-sequence task in two phases. The first phase pretrains the model with heuristically-created spans, gathered from Reddit comments labeled for their toxicity. The second phase fine-tunes this model on the organizer-provided data. Figure 1 shows the overview of the system. All the data used in the paper"
2021.semeval-1.31,N16-3020,0,0.821543,"s motivated by two strands of prior work showing (1) that large language models can effectively serve as sequence-to-sequence (seq2seq) models and (2) that pre-training on a similar task can improve downstream performance (Phang et al., 2018; Gururangan et al., 2020). Here, we treat the toxicspan detection tasks as a seq2seq task, where given a sequence of tokens, the model outputs per-token judgments of whether the token is in the toxic span. Given the limited training data for Task 5, we increase our training data by generating a silver-standard set of span judgments from LIME explanations (Ribeiro et al., 2016) fropm a model trained to recognize toxic and non-toxic language. These additional judgments are intended to help the model learn the basic span recognition task and identify general toxic language, before fine-tuning on the Task 5 data. This paper presents our system submission to task 5: Toxic Spans Detection of the SemEval2021 competition. The competition aims at detecting the spans that make a toxic span toxic. In this paper, we demonstrate our system for detecting toxic spans, which includes expanding the toxic training set with Local Interpretable Model-Agnostic Explanations (LIME), fine"
2021.semeval-1.31,P19-1357,1,0.820384,"tion, and error analysis. We found that feeding the model with an expanded training set using Reddit comments of polarizedtoxicity and labeling with LIME on top of logistic regression classification could help RoBERTa more accurately learn to recognize toxic spans. We achieved a span-level F1 score of 0.6715 on the testing phase. Our quantitative and qualitative results show that the predictions from our system could be a good supplement to the gold training set’s annotations. 1 2 Introduction Toxic messages remain a small but persistent part of online communications (Fortuna and Nunes, 2018; Jurgens et al., 2019). NLP methods have been developed to identify these comments, often relying on deep-language models (Vidgen et al., 2019). However, the part of the message that is specifically toxic is often unknown. Such information is useful not only for validating and explaining the judgments of models (Carton et al., 2018), but can also be useful for moderators to use when making decisions and working with these models in their deployment (Carton et al., 2020; Liu et al., 2021). This paper describes our model1 and error analysis for SemEval-2021 Task 5: Toxic Spans Detection (Pavlopoulos et al., 2021). Ou"
2021.semeval-1.31,W19-3509,0,0.0161233,"toxicity and labeling with LIME on top of logistic regression classification could help RoBERTa more accurately learn to recognize toxic spans. We achieved a span-level F1 score of 0.6715 on the testing phase. Our quantitative and qualitative results show that the predictions from our system could be a good supplement to the gold training set’s annotations. 1 2 Introduction Toxic messages remain a small but persistent part of online communications (Fortuna and Nunes, 2018; Jurgens et al., 2019). NLP methods have been developed to identify these comments, often relying on deep-language models (Vidgen et al., 2019). However, the part of the message that is specifically toxic is often unknown. Such information is useful not only for validating and explaining the judgments of models (Carton et al., 2018), but can also be useful for moderators to use when making decisions and working with these models in their deployment (Carton et al., 2020; Liu et al., 2021). This paper describes our model1 and error analysis for SemEval-2021 Task 5: Toxic Spans Detection (Pavlopoulos et al., 2021). Our model uses a deep learning approach to identify which tokens are toxic. The approach 1 The code is available at https:/"
2021.semeval-1.31,2021.ccl-1.108,0,0.0845057,"Missing"
2021.semeval-1.31,2020.emnlp-demos.2,0,0.0194992,"1: Diagram of our data and architecture. The central hypothesis tested is whether pre-training a RoBERTa model on machine-generated rationales for toxicity could improve performance. Data Silver data was drawn from a sample of all Reddit comments made between January to June 2018. As social media data, these comments contain similar lexical and syntactic patterns as the social media comments data as the gold standard, which was made on the Civil Comments platform. Prior work has shown that pre-training RoBERTa models to recognize this type of social media data improve downstream performance (Nguyen et al., 2020). However, Reddit posts can vary substantially in their length. To avoid introducing confounding effects from pre-training a model on posts of substantially different lengths, we compute the Inter-quartile Range (IQR) of the lengths of Reddit comments and remove all comments identified as outliers. This process effectively removes very long or very short comments. Ultimately, the mean number of words in the training data and Reddit data are roughly similar: 35.87±34.92 words (mean and standard deviation) in the training data, compared with 36.79±30.57 in the Reddit data. 264 Generating Heurist"
2021.teachingnlp-1.10,D14-1082,0,0.291067,"—or dramatically slow training time. Second, students gain familiarity with how to use pre-trained embeddings in downstream applications. The dependency parser makes use of these embeddings for its initial word representations and the assignment provides an optional exercise to have students try embeddings from different sources (e.g., Twitter-based embeddings) or no pre-training at all in order to see how these affect Dependency parsing is increasingly the popular parsing formalism in practice. This assignment provides a practice exercise in implementing the shift-reduce dependency parser of Chen and Manning (2014). This parser is a two-layer feed-forward neural network, which students implement in PyTorch, providing practice in developing deep learning models and exposure to developing parser models. 1 Introduction Deep learning methods are ubiquitous in nearly all areas of NLP. However, some applications for these models require extensive training or data that make implementing the model in a classroom infeasible without additional computational support. This homework introduces a simple-yet-powerful network for performing dependency parsing using the shift-reduce parser of Chen and Manning (2014). Th"
2021.teachingnlp-1.10,P19-1339,0,0.0171923,"roduce additional design components and build intuition. 4 Potential Extensions Prior parts of the course examine word vectors and this parsing exercise includes an optional extension to allow students to see their effect in practice. Here, we provide students with multiple pre-trained vectors from different domains (e.g., Twitter and Wikipedia) and ask them to report on convergence times and accuracy. Students may also optionally freeze these embeddings to test how well their information can generalize. Since training data are known to contain biases that affect downstream performance (e.g., Garimella et al., 2019), one additional extension could be to test how particular embeddings perform better or worse on parsing text from specific groups. After implementing the model, the assignment has students explore the parsing outputs and intermediate state, which provides some grounding for how shift-reduce works in practice. However, due to the focus on learning PyTorch, the parsing component of the exercise is less in-depth; a more parsing-focus variant of this assignment could have students perform the CoNLL-to-training-data conversion in order to see how dependency trees can be turned into a sequence of s"
2021.teachingnlp-1.10,P14-1002,0,0.0297958,"hop on Teaching NLP, pages 62–64 June 10–11, 2021. ©2021 Association for Computational Linguistics performance and convergence time. This learning objective helps bridge the conceptual material to later pre-trained language models like BERT if they have not been introduced earlier. Third, students should gain a basic familiarity with dependency parsing and how a shift-reduce parser works. Shift-reduce parsing is a classic technique (Aho and Ullman, 1973) and has been widely adopted for multiple parsing tasks beyond syntax, such as semantic parsing (Misra and Artzi, 2016) or discourse parsing (Ji and Eisenstein, 2014). This assignment helps students understand the basic structures for parsing (e.g., the stack and buffer) to see how neural approaches can be used in practice. The concepts in the homework are connected to textbook material in Chapter 14 of Speech & Language Processing (Jurafsky and Martin, 2021, 3rd ed.), which provides additional examples and definitions. 3 In some iterations, we have asked the students to report on an error introduced from a non-projective parse of their choice, though some students found it difficult to come up with an example of their own. Second, students are asked to ex"
2021.teachingnlp-1.10,D16-1183,0,0.024797,"us setting; 62 Proceedings of the Fifth Workshop on Teaching NLP, pages 62–64 June 10–11, 2021. ©2021 Association for Computational Linguistics performance and convergence time. This learning objective helps bridge the conceptual material to later pre-trained language models like BERT if they have not been introduced earlier. Third, students should gain a basic familiarity with dependency parsing and how a shift-reduce parser works. Shift-reduce parsing is a classic technique (Aho and Ullman, 1973) and has been widely adopted for multiple parsing tasks beyond syntax, such as semantic parsing (Misra and Artzi, 2016) or discourse parsing (Ji and Eisenstein, 2014). This assignment helps students understand the basic structures for parsing (e.g., the stack and buffer) to see how neural approaches can be used in practice. The concepts in the homework are connected to textbook material in Chapter 14 of Speech & Language Processing (Jurafsky and Martin, 2021, 3rd ed.), which provides additional examples and definitions. 3 In some iterations, we have asked the students to report on an error introduced from a non-projective parse of their choice, though some students found it difficult to come up with an example"
2021.teachingnlp-1.19,J15-4004,0,0.00952553,"dents to practice their performance optimizing skills. Third, the lexical semantics portion of the homework exposes students to the uses and limitations of word vectors. Through training the vectors, students understand how statistical regularities in cooccurrence can be used to learn meaning. Qualitative and quantitative evaluations show students what their model has learned (e.g., using vector analogies) and introduce them to concepts of polysemy, fostering a larger discussion on what can be captured in a vector representation. 3 Homework Description scores for the subset of the SimLex-999 (Hill et al., 2015) present in their training corpus, which is uploaded to Kaggle InClass1 to see how their vectors compare with others; this leaderboard helps students identify a bug in their code (via a low-scoring submission) and occasionally prompts students to think about how to improve/extend their code to attain a higher score. Potential Extensions The word2vec method has been extended in numerous ways in NLP to improve its vectors (e.g., Ling et al., 2015; Yu and Dredze, 2014; Tissier et al., 2017). This assignment includes descriptions of other possible extensions that students can explore, such as impl"
2021.teachingnlp-1.19,N15-1142,0,0.0176644,"emy, fostering a larger discussion on what can be captured in a vector representation. 3 Homework Description scores for the subset of the SimLex-999 (Hill et al., 2015) present in their training corpus, which is uploaded to Kaggle InClass1 to see how their vectors compare with others; this leaderboard helps students identify a bug in their code (via a low-scoring submission) and occasionally prompts students to think about how to improve/extend their code to attain a higher score. Potential Extensions The word2vec method has been extended in numerous ways in NLP to improve its vectors (e.g., Ling et al., 2015; Yu and Dredze, 2014; Tissier et al., 2017). This assignment includes descriptions of other possible extensions that students can explore, such as implementing dropout, adding learning rate decay, or making use of external knowledge during training. Typically, a single extension to word2vec is included as a part of the homework to help ground the concept in code but without increasing the difficulty of the assignment. Students who are interested in deepening their understanding can use these as starting points to see how to develop their own NLP methods as a part of a course project. This ass"
2021.teachingnlp-1.19,N19-1062,0,0.0121888,"a single extension to word2vec is included as a part of the homework to help ground the concept in code but without increasing the difficulty of the assignment. Students who are interested in deepening their understanding can use these as starting points to see how to develop their own NLP methods as a part of a course project. This assignment also provides multiple possibilities for examining the latent biases learned in word vectors. Prior work has established that pretrained vectors often encode gender and racial biases based on the corpora they are trained on (e.g., Caliskan et al., 2017; Manzini et al., 2019). In a future extension, this assignment could be adapted to use Wikipedia biographies as a base corpus and have students identify how occupations become more associated with gendered words during training (Garg et al., 2018). Once this bias is discovered, students can discuss various methods for mitigating it (e.g., Bolukbasi et al., 2016; Zhao et al., 2017) and how their method might be adapted to avoid other forms of bias. This extension can help students critically think about what is and is not being captured in pretrained vectors and models. The homework has students implement two core a"
2021.teachingnlp-1.19,D17-1024,0,0.0210706,"at can be captured in a vector representation. 3 Homework Description scores for the subset of the SimLex-999 (Hill et al., 2015) present in their training corpus, which is uploaded to Kaggle InClass1 to see how their vectors compare with others; this leaderboard helps students identify a bug in their code (via a low-scoring submission) and occasionally prompts students to think about how to improve/extend their code to attain a higher score. Potential Extensions The word2vec method has been extended in numerous ways in NLP to improve its vectors (e.g., Ling et al., 2015; Yu and Dredze, 2014; Tissier et al., 2017). This assignment includes descriptions of other possible extensions that students can explore, such as implementing dropout, adding learning rate decay, or making use of external knowledge during training. Typically, a single extension to word2vec is included as a part of the homework to help ground the concept in code but without increasing the difficulty of the assignment. Students who are interested in deepening their understanding can use these as starting points to see how to develop their own NLP methods as a part of a course project. This assignment also provides multiple possibilities"
2021.teachingnlp-1.19,P14-2089,0,0.0201723,"rger discussion on what can be captured in a vector representation. 3 Homework Description scores for the subset of the SimLex-999 (Hill et al., 2015) present in their training corpus, which is uploaded to Kaggle InClass1 to see how their vectors compare with others; this leaderboard helps students identify a bug in their code (via a low-scoring submission) and occasionally prompts students to think about how to improve/extend their code to attain a higher score. Potential Extensions The word2vec method has been extended in numerous ways in NLP to improve its vectors (e.g., Ling et al., 2015; Yu and Dredze, 2014; Tissier et al., 2017). This assignment includes descriptions of other possible extensions that students can explore, such as implementing dropout, adding learning rate decay, or making use of external knowledge during training. Typically, a single extension to word2vec is included as a part of the homework to help ground the concept in code but without increasing the difficulty of the assignment. Students who are interested in deepening their understanding can use these as starting points to see how to develop their own NLP methods as a part of a course project. This assignment also provides"
2021.teachingnlp-1.19,D17-1323,0,0.0126896,"iple possibilities for examining the latent biases learned in word vectors. Prior work has established that pretrained vectors often encode gender and racial biases based on the corpora they are trained on (e.g., Caliskan et al., 2017; Manzini et al., 2019). In a future extension, this assignment could be adapted to use Wikipedia biographies as a base corpus and have students identify how occupations become more associated with gendered words during training (Garg et al., 2018). Once this bias is discovered, students can discuss various methods for mitigating it (e.g., Bolukbasi et al., 2016; Zhao et al., 2017) and how their method might be adapted to avoid other forms of bias. This extension can help students critically think about what is and is not being captured in pretrained vectors and models. The homework has students implement two core aspects of the word2vec algorithm using numpy for the numeric portions, and then evaluate with two downstream tasks. The first aspect has students perform the commonly-used text preprocessing steps that turn a raw text corpus into self-supervised training examples. This step includes removing lowfrequency tokens and subsampling tokens based on their frequency."
2021.wnut-1.35,N19-1063,0,0.0570979,"Missing"
D15-1088,D10-1100,0,0.0245476,"lete, manually-annotated list of all characters in 58 literary works. Second, we propose a new technique for character detection based 769 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 769–774, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. works without relying on dialogue, unlike the methods of Elson et al. (2010) and He et al. (2013). Lee and Yeung (2012) build social networks by recognizing characters from explicit markers (e.g., kinship) and implicit markers (e.g., physical collocation). Similarly, Agarwal and Rambow (2010) build character networks using tree kernels on parse trees to identify interacting agents. In the two most-related works, Bamman et al. (2014) and Ardanuy and Sporleder (2014), character names are extracted and clustered under a set of constraints. In the BookNLP system developed by Bamman et al. (2014), NER-identified names are retained and merged based on animacy, determined through dependencies with ”sentient” lemmas from a small dictionary (including for example, say and smile), and gender, assigned through pronomial resolution and a dictionary of genderspecific honorifics. Ardanuy and Sp"
D15-1088,I13-1171,0,0.0272845,"cter detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014). To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all coreferent expressions. Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the difficulty of the task. He et al. (2013) propose an alternate approach for identifying speaker references in novels, using a probabilistic model to identify which character is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task pro"
D15-1088,W14-0905,0,0.181622,"ases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction netHow many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examination of literary character detection, with three key contributions. Fir"
D15-1088,P14-1035,0,0.354135,"owever, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction netHow many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities"
D15-1088,P10-1015,0,0.407663,"Piper2 , Derek Ruths1 1 School of Computer Science 2 Department of Languages, Literatures, and Cultures McGill University, Montreal, QC, Canada hardik.vala@mail.mcgill.ca, jurgens@cs.mcgill.ca andrew.piper@mcgill.ca, derek.ruths@mcgill.ca Abstract on inducing character prototypes, and in comparisons with three state-of-the-art methods, demonstrate superior performance, achieving significant improvements in F1 over the next-best method. Third, as practical applications, we analyze literary trends in character density over 20 decades and revisit the character-based literary hypothesis tested by Elson et al. (2010). Characters are fundamental to literary analysis. Current approaches are heavily reliant on NER to identify characters, causing many to be overlooked. We propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. 1 2 Introduction Related Work Character detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014). To account for var"
D15-1088,P13-1120,0,0.0209974,"ph. Next, the seventh step attempts to identify characters whose names may not be recognized by NER. For example, many minor characters do not appear as named entities and instead have general role-based referents such as “the governor” or “the archbishop.” However, despite the lack of proper names, such characters behave and interact in similar ways as major characters, including having dialogue. Therefore, to discover such characters, we adopt a bootstrapping technique aimed at uncovering prototypical character behaviors from the novels themselves, inspired by the semantic predicate work of Flati and Navigli (2013). The Project Gutenberg fiction corpus was dependency parsed to identify all verbs in a dependency relation with nouns, where each noun was categorized as (a) a Detecting Characters We propose an eight stage pipeline for detecting characters, which builds a graph where nodes are names and edges connect names belonging to the same character. The vertices in the graph are initially populated by running NER over the corpus and also incorporating names following an honorific. Second, coreference resolution is run to identify names that occur together in a coreference chain and edges are added wher"
D15-1088,P13-1129,0,0.365892,"lity in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all coreferent expressions. Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the difficulty of the task. He et al. (2013) propose an alternate approach for identifying speaker references in novels, using a probabilistic model to identify which character is speaking. However, to account for the multiple aliases used to refer to a character, the authors first manually constructed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction netHow many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open"
D15-1088,W15-0704,0,0.0584975,"sed in this work and underscores the need for automated methods. Two approaches mined social interaction netHow many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examination of literary character detection, with three key contributions. First, we formalize the task"
D15-1088,Y12-1022,0,0.0829648,"structed a list of characters and their aliases, which is the task proposed in this work and underscores the need for automated methods. Two approaches mined social interaction netHow many literary characters appear in a novel? Despite the seeming simplicity of the question, precisely identifying which characters appear in a story remains an open question in literary and narrative analysis. Characters form the core of many computational analyses, from inferring prototypical character types (Bamman et al., 2014) to identifying the structure of social networks in literature (Elson et al., 2010; Lee and Yeung, 2012; Agarwal et al., 2013; Ardanuy and Sporleder, 2014; Jayannavar et al., 2015). These current approaches have largely assumed that characters can be reliably identified in text using standard techniques such as Named Entity Recognition (NER) and that the variations in how a character is named can be found through coreference resolution. However, such treatment of character identity often overlooks minor characters that serve to enrich the social structure and serve as foils for the identities of major characters (Eder et al., 2010). This work provides a comprehensive examination of literary cha"
D15-1088,P14-5010,0,0.00497194,"hypothesis tested by Elson et al. (2010). Characters are fundamental to literary analysis. Current approaches are heavily reliant on NER to identify characters, causing many to be overlooked. We propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. 1 2 Introduction Related Work Character detection has primarily been performed in the context of mining literary social networks. Elson et al. (2010) extract character mentions from conversational segments, using the Stanford CoreNLP NER system to discover character names (Manning et al., 2014). To account for variability in character naming, alternate forms of a name are generated using the method of Davis et al. (2003) and merged together as a single character. Furthermore, the set of aliases for a character is expanded by creating coreference chains originating from these proper names and merging all coreferent expressions. Agarwal et al. (2013) also rely on the CoreNLP NER and coreference resolution systems for character detection; however for literary analysis, they use gold character mentions that have been marked and resolved by a team of trained annotators, highlighting the"
D18-1004,C14-1079,0,0.102398,"ine resources. Our study asks what effect does this gender signaling have on individuals receiving support and disparagement? computational studies of gender disparity in online behavior (e.g., Lam et al., 2011; Magno and Weber, 2014; Garimella and Mihalcea, 2016; Li et al., 2018); our work here examines this disparity along a new dimension, support, and unlike prior work, examines disparity along the full spectrum of both pro-social (supportive) and anti-social (unsupportive) behaviors. Prior works have also examined the language of support in online support forums for health-related issues (Biyani et al., 2014; Wang et al., 2012; De Choudhury and De, 2014; Althoff et al., 2016; De Choudhury and Kiciman, 2017), often with the aim of improving people’s access. Here, we aim to study support in general, everyday interactions, drawing upon theories of how support is expressed in language (Cutrona and Suhr, 1992; Wright et al., 2003). Our investigation provides four main contributions. First, we introduce a new task of rating the supportiveness of a message and provide an accompanying dataset of 9,032 post-reply pairs with annotations (§2). Second, using this data, we develop a new computational model fo"
D18-1004,Q16-1033,0,0.0223038,"g have on individuals receiving support and disparagement? computational studies of gender disparity in online behavior (e.g., Lam et al., 2011; Magno and Weber, 2014; Garimella and Mihalcea, 2016; Li et al., 2018); our work here examines this disparity along a new dimension, support, and unlike prior work, examines disparity along the full spectrum of both pro-social (supportive) and anti-social (unsupportive) behaviors. Prior works have also examined the language of support in online support forums for health-related issues (Biyani et al., 2014; Wang et al., 2012; De Choudhury and De, 2014; Althoff et al., 2016; De Choudhury and Kiciman, 2017), often with the aim of improving people’s access. Here, we aim to study support in general, everyday interactions, drawing upon theories of how support is expressed in language (Cutrona and Suhr, 1992; Wright et al., 2003). Our investigation provides four main contributions. First, we introduce a new task of rating the supportiveness of a message and provide an accompanying dataset of 9,032 post-reply pairs with annotations (§2). Second, using this data, we develop a new computational model for automatically identifying supportive and unsupportive replies (§3)"
D18-1004,J08-4004,0,0.0608855,"Missing"
D18-1004,D14-1213,0,0.0656898,"Missing"
D18-1004,P13-1025,0,0.0813566,"Missing"
D18-1004,P82-1020,0,0.833834,"Missing"
D18-1004,D15-1162,0,0.0136359,".52 0.43 0.42 0.40 0.29 0.26 the most important features, followed closely by lexicons for emotion: Anger in LIWC, Disgust in NRC, and the positive sentiment in Liu et al. (2005), all of which were motivated by theory. These results confirm that our theory-inspired features are both salient for supportiveness and effective as features. Table 3: Support classification performance training fold using the 5 nearest neighbors, taking care to avoid contamination of the test set. The classifier is implemented using Scikit-learn (Pedregosa et al., 2011) and syntactic processing was done using spaCy (Honnibal and Johnson, 2015). Word vectors are the publicly released GoogleNews word2vec vectors (Mikolov et al., 2013). Three works have examined related tasks where Biyani et al. (2014) and Khanpour et al. (2018) classify posts in online cancer support groups as providing informational or emotional support and Wang et al. (2012) classify the degree of support along these dimensions. Here, we solve a more general task that includes unsupportive comments and is in the general domain. Evaluation We compare our full model for predicting support against three models: our 14 features for detecting support strategies from Tab"
D18-1004,P15-2079,0,0.0259789,"teaching were considered less supportive. We observed that in several cases these strategies 35 we include lexicons from argumentation for capturing explanatory replies (Teufel, 2000). Support may be given in response to stressors, which change in nature throughout a person’s lifetime (Vaux, 1985; Segrin, 2003). To potentially capture variation in the language of support based on the posting individuals, we include features known to be associated with age such as elongation and capitalization (Goswami et al., 2009; Barbieri, 2008), grammatical differences in sentence construction and length (Hovy and Søgaard, 2015), and a lexicon for age of acquisition (Kuperman et al., 2012). Data-driven features include (1) lexical features capturing the presence of n-grams, their relative frequency, (2) grammatical features from dependency-parsed triples, which are also backed off to parts of speech, (3) word lexicons for formality, sentiment, and subjectivity, (4) style features such as word and sentence length, complexity, and use of contractions, and (5) the average word vector for the sentence. In total, our model includes 23,903 features, the bulk of which are n-grams and dependency triples. A detailed listing o"
D18-1004,D15-1240,0,0.0676502,"Missing"
D18-1004,W16-4301,0,0.161361,"s of disparagement. 1 Figure 1: In this fictitious example, KatieZ22 receives a supportive reply from PizzaMagic. In choosing their names, each user has chosen a particular gender performance, signaling female and genderanonymity, respectively. In online settings, such gender performances evoke stereotypes that affect how others interact and provide access to online resources. Our study asks what effect does this gender signaling have on individuals receiving support and disparagement? computational studies of gender disparity in online behavior (e.g., Lam et al., 2011; Magno and Weber, 2014; Garimella and Mihalcea, 2016; Li et al., 2018); our work here examines this disparity along a new dimension, support, and unlike prior work, examines disparity along the full spectrum of both pro-social (supportive) and anti-social (unsupportive) behaviors. Prior works have also examined the language of support in online support forums for health-related issues (Biyani et al., 2014; Wang et al., 2012; De Choudhury and De, 2014; Althoff et al., 2016; De Choudhury and Kiciman, 2017), often with the aim of improving people’s access. Here, we aim to study support in general, everyday interactions, drawing upon theories of ho"
D18-1004,W16-5614,0,0.0554851,"Missing"
D18-1004,W17-1601,0,0.0614148,"∧ R:{name P:{name ∧ P:♀L ∧ R:♀name P:{name ∧ P:♀L ∧ R:{name S UP. −2.391∗∗∗ 0.561∗∗∗ 0.450∗∗∗ 1.263∗∗∗ 0.249∗∗∗ −0.057∗∗∗ 0.914∗∗∗ 0.082 −0.103∗∗∗ −0.033 −0.004 0.031∗ 0.209 −0.331∗∗ −0.343 0.329 0.067 0.024 U NSUP. −3.107∗∗∗ 0.288∗∗∗ 0.330∗∗∗ 0.290∗∗∗ −0.153∗∗∗ −0.036∗∗ 0.212∗∗ 0.131 −0.037 0.026 0.036∗ −0.003 −0.202 −0.228∗ 0.376 0.312 0.178 0.203 7 Ethical Considerations The use of gender as a variable in NLP requires that we also discuss ethical considerations resulting from this work, as it directly relates to identity and the dignity of persons being studied. Following the guidelines of Larson (2017) for using gender in NLP, our use of gender is intentional and central to this study on gender disparities in received support. We base our notion of gender as one of linguistic performance (DeFrancisco et al., 2013), in which individuals adapt their style and name to emphasize or de-emphasize certain aspects of their gender identity (Eckert, 2008). Accordingly, we have opted represent gender performance along a graded scale, though we recognize that this representation does not capture nonbinary gender identities. The gender inference methods introduced here raise ethical considerations as th"
D18-1004,Q16-1005,0,0.0267459,"the language of support draws upon multiple lexical and stylistic cues. We base on classifier on theory-inspired and datadriven features. The first set consists of the operationalized linguistic strategies for expressing support, shown in Table 2. Further, in constructing our feature set, we build upon past linguistic analyses of related social-situated language. Wellman and Wortley (1990) note that the availability of support is related to social distance, which is in part expressed linguistically through the degree of formality (Hovy, 1987; Sigley, 1997). Therefore, we include features from Pavlick and Tetreault (2016), which examined linguistic markers of formality. Advice giving is a core component of many theories of support (MacGeorge et al., 2011) and such advice is frequently wrapped in politeness language (Feng et al., 2013), e.g., hedging suggestions rather than imposing direction, which provides face-saving opportunities for the person receiving support (Clark and Schunk, 1980). Therefore, we include the feature set of DanescuNiculescu-Mizil et al. (2013), which though focused on requests, provides many general lexical patterns for politeness. Beyond these, we include features motivated by observat"
D18-1004,P12-2018,0,0.0285526,"al. (2018) classify posts in online cancer support groups as providing informational or emotional support and Wang et al. (2012) classify the degree of support along these dimensions. Here, we solve a more general task that includes unsupportive comments and is in the general domain. Evaluation We compare our full model for predicting support against three models: our 14 features for detecting support strategies from Table 2, a model trained on the subset of unigram features (4,352), and a model trained on bigram features (8,897), the latter of which is known to be a strong lexical baseline (Wang and Manning, 2012). All models were tested using five-fold cross-validation with Macro-F1 for evaluation and including baselines for labeling instances at random or choosing the most frequent. Our full model obtains substantial improvements over all baselines and models, as shown in Table 3. Further, the simple support strategy features provide a large and statically-significant improvement over the two baselines. The model using support strategy features performs similarity to the unigram model, despite having two orders of magnitude fewer features. A follow-up analysis on cross-platform performance, described"
D18-1004,W16-5603,0,0.0300368,"er identities. The gender inference methods introduced here raise ethical considerations as they ultimately enable automatic identification of gender for any person on the basis of name or writing (Hamidi et al., 2018). Such technology could be used to unfairly identify and target persons of either gender for malicious behavior or may harm through misgendering individuals. Ultimately, we decided that such risk was acceptable given the positive impact of our study on revealing gender disparity. We hope to also use our method to better support privacy-preserving behavior (Allen and Wiles, 2016; Reddy and Knight, 2016) by helping individuals identify and change names or statements that would indicate a particular gender. Further, we hope that when used in combination with our support classifier and a larger context of gendered interactions (Voigt et al., 2018), these technologies can identify healthy communities that are supportive of all people. Table 8: Regression coefficients for Reddit when the gender identity of the replier is known. The post author’s identity is denoted with a P and replier’s with R. respect to rates of giving supportive or unsupportive comments. We only observe significant interactio"
D18-1004,L18-1445,1,0.799533,"to unfairly identify and target persons of either gender for malicious behavior or may harm through misgendering individuals. Ultimately, we decided that such risk was acceptable given the positive impact of our study on revealing gender disparity. We hope to also use our method to better support privacy-preserving behavior (Allen and Wiles, 2016; Reddy and Knight, 2016) by helping individuals identify and change names or statements that would indicate a particular gender. Further, we hope that when used in combination with our support classifier and a larger context of gendered interactions (Voigt et al., 2018), these technologies can identify healthy communities that are supportive of all people. Table 8: Regression coefficients for Reddit when the gender identity of the replier is known. The post author’s identity is denoted with a P and replier’s with R. respect to rates of giving supportive or unsupportive comments. We only observe significant interactions indicating (1) replying users with female names are less likely to leave supportive replies to posting users with female names and (2) replying users with male names are i) more likely to leave supportive replies to other users with male names"
D19-1176,W18-5105,0,0.0447521,"Missing"
D19-1176,J08-4004,0,0.232989,"heme often co-exists with “Attribution of Stereotype,” but is distinct in that its focus is on redefining the target’s sense of identity. 2.3 Annotation Results Three annotators familiar with the theoretical background and prior research on microagressions performed an open coding procedure after examining the S ELF MA data to codify the typology and determine annotation guidelines. All three labeled 200 instances of the dataset to estimate agreement. Despite the difficulty of the task, annotators had moderate agreement, as shown in Table 2. While lower than what is considered high agreement (Artstein and Poesio, 2008), given the potentiallysubjective nature of MA S and criticism for lack of objectivity (Lilienfeld, 2017), we view this result as a strongly positive sign that reliable annotation is possible despite the challenge. This moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had κ val2 The full description of themes, sub-themes, and examples can be found in Suppl"
D19-1176,P19-1243,1,0.835717,"towards others. Second, our current focus is on gender-based MA S. This choice was motivated by the observation that gender-based MA S are the largest category in the S ELF MA data and, given that prior studies have shown substantial gender disparity online, with women receiving more negative behaviors (Duggan, 2017), this choice has the potential for highest impact. Our work builds upon a growing body of literature focused on identifying and mitigating gender disparity through computational means (e.g., Magno and Weber, 2014; Garimella and Mihalcea, 2016; Li et al., 2018; Field et al., 2019; Field and Tsvetkov, 2019). Further, our focus on gender also allows us to reliably recruit crowdworkers across the gender spectrum, whereas other social categories such as race or religion are more difficult to recruit in a balanced proportion through traditional mechanisms. However, despite this current focus, both the typology and annotation approach are designed for 6 Offensiveness 6 Offensiveness Category Attributive Institutionalized Teaming Othering 4 2 4 2 0 -4 -2 0 2 4 0 -4 Discrepancy -2 0 Discrepancy Figure 4: Offensiveness (y-axis) vs Discrepancy (x-axis) of perceived offensiveness between annotator gender."
D19-1176,W16-4301,0,0.0304306,"ethical concerns of having crowdworkers generate toxic statements towards others. Second, our current focus is on gender-based MA S. This choice was motivated by the observation that gender-based MA S are the largest category in the S ELF MA data and, given that prior studies have shown substantial gender disparity online, with women receiving more negative behaviors (Duggan, 2017), this choice has the potential for highest impact. Our work builds upon a growing body of literature focused on identifying and mitigating gender disparity through computational means (e.g., Magno and Weber, 2014; Garimella and Mihalcea, 2016; Li et al., 2018; Field et al., 2019; Field and Tsvetkov, 2019). Further, our focus on gender also allows us to reliably recruit crowdworkers across the gender spectrum, whereas other social categories such as race or religion are more difficult to recruit in a balanced proportion through traditional mechanisms. However, despite this current focus, both the typology and annotation approach are designed for 6 Offensiveness 6 Offensiveness Category Attributive Institutionalized Teaming Othering 4 2 4 2 0 -4 -2 0 2 4 0 -4 Discrepancy -2 0 Discrepancy Figure 4: Offensiveness (y-axis) vs Discrepan"
D19-1176,W17-2902,0,0.0761968,"sufficiently represented in our corpus, and (3) comprehensive over all distinct microaggression types in the corpus. Four key themes were identified in our analysis of the MA S data: Attributive, Institutionalized, Teaming, and Othering (see Table 1). We discuss each of these next.2 The Attributive theme covers instances where a microaggression attributes a stereotype to an individual based on their identity. These stereotypes may have inherently negative connotations (“lazy”), but may also be neutral (“liking pink”) or positive (“strong”), which complements recent work on benevolent sexism (Jha and Mamidi, 2017). The Institutionalized theme reflects larger institutionalized biases, such as in employment or law enforcement. The Teaming theme is derived from the term forced teaming, coined by de Becker (1997) to describe a strategy of abuse where the abusers frames themselves as being on the same team as the victim. The Othering theme covers MA S which revolve around framing the target in relation to some “othered” group. This theme often co-exists with “Attribution of Stereotype,” but is distinct in that its focus is on redefining the target’s sense of identity. 2.3 Annotation Results Three annotators"
D19-1176,N18-2099,0,0.027849,"[SelfMA] [Random] 6 6 Offensiveness Offensiveness representative of all types of MA S. However, our crowdsourcing approach does provide an effective way to surface MA S and our dual objective approach, which uses both annotator discrepancy and offensiveness, provides complementary views into what statements could be perceived as MA S. Additional iterations of this procedure are likely to improve microaggression recognition and substantially increase the seize of the corpus. We note that one option is to have workers generate examples, rather than rate (e.g., Xu et al., 2013; Su et al., 2016; Jiang et al., 2018); however, such a process raises ethical concerns of having crowdworkers generate toxic statements towards others. Second, our current focus is on gender-based MA S. This choice was motivated by the observation that gender-based MA S are the largest category in the S ELF MA data and, given that prior studies have shown substantial gender disparity online, with women receiving more negative behaviors (Duggan, 2017), this choice has the potential for highest impact. Our work builds upon a growing body of literature focused on identifying and mitigating gender disparity through computational mean"
D19-1176,P19-1357,1,0.772049,"lasting harmful impacts on their targets. Qualitative interviews suggest that the subtlety of MA S may cause even greater levels of situational stress than overt aggression (Sue, 2010; Nadal et al., 2014). Introduction Toxicity and offensiveness are not always expressed with toxic language. While a substantial community effort has rightfully focused on identifying, preventing, and mitigating overtly toxic, profane, and hateful language (Schmidt and Wiegand, 2017), offensiveness spans a far larger spectrum that includes comments with more implicit and subtle signals that are no less offensive (Jurgens et al., 2019). One significant class of subtle-but-offensive comments includes microaggressions (Sue et al., 2007, MA S), defined in Merriam-Webster as “a comment or action that Despite a public effort to recognize and reduce—if not eliminate—their occurrence (Kim, 2013; Neff, 2015), there has been no computational work to detect and analyze MA S at scale. Instead, much of the recent work has focused on explicitly toxic language (e.g., Waseem et al., 2017), with surveys of the area also overlooking this important and challenging task of recognizing this subtle toxicity (van Aken et al., 2018; Salminen et a"
D19-1176,passonneau-etal-2012-masc,0,0.0219217,"sible despite the challenge. This moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had κ val2 The full description of themes, sub-themes, and examples can be found in Supplementary Material §2. Figure 2: The distribution of four axes of discrimination (gender, race, sexuality, and other) in each subtheme. ues as low as 0.59, and Word Sense Disambiguation (Passonneau et al., 2012), which reported α values for some words below 0.30 at determining meaning. The final dataset was determined by first retaining all posts where at least two annotators agreed (183 posts). And where there were no agreement (17 posts), the annotators determine the labels through a follow-up adjudication process.3 After this process, another 1,100 posts were singly annotated, distributed across the three annotators, for a total of 1,300 posts. Relative percentages of each theme and examples comments are shown in Table 3. We also show the distribution of different axes of discrimination among the"
D19-1176,Q16-1005,0,0.0406361,"erceived offensiveness to pick the next batch of posts for the second round of annotation. The classifier was given, as positive examples, posts with ≥ 0.25 discrepancy8 between the averaged perceived offensiveness of the dominant group and the marginalized groups. Other posts are considered negative examples. The feature sets that we used are motivated by prior work on gender bias and power dynamics. The following seven feature categories are used: (1) unigram and bigram features to capture lexical patterns, (2) two categories of formal and informal words derived from the formality corpus of Pavlick and Tetreault (2016), (3) the gendered occupations lexicon of Bolukbasi et al. (2016), grouped across definitional and stereotypical gender (male, female, neutral), (4) the gender stereotype lexicon of Fast et al. (2016), (5) the gender lexicon for social media from (Sap et al., 2014), (6) a manually-compiled corpus of gendered words, extended from seed list from https://www.hrc.org/resources/glossary-of-terms and (7) a manually-compiled sentiment lexicon, inspired by LIWC. To facilitate reproducibility, all lexicons will be available in the software release. 8 The threshold was chosen empirically to ensure enoug"
D19-1176,D17-1244,0,0.0207354,"d moderate agreement, as shown in Table 2. While lower than what is considered high agreement (Artstein and Poesio, 2008), given the potentiallysubjective nature of MA S and criticism for lack of objectivity (Lilienfeld, 2017), we view this result as a strongly positive sign that reliable annotation is possible despite the challenge. This moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had κ val2 The full description of themes, sub-themes, and examples can be found in Supplementary Material §2. Figure 2: The distribution of four axes of discrimination (gender, race, sexuality, and other) in each subtheme. ues as low as 0.59, and Word Sense Disambiguation (Passonneau et al., 2012), which reported α values for some words below 0.30 at determining meaning. The final dataset was determined by first retaining all posts where at least two annotators agreed (183 posts). And where there were no agreement (17 posts), the annotators determine the labels through a follow-up adj"
D19-1176,P16-1030,0,0.0490384,"determine annotation guidelines. All three labeled 200 instances of the dataset to estimate agreement. Despite the difficulty of the task, annotators had moderate agreement, as shown in Table 2. While lower than what is considered high agreement (Artstein and Poesio, 2008), given the potentiallysubjective nature of MA S and criticism for lack of objectivity (Lilienfeld, 2017), we view this result as a strongly positive sign that reliable annotation is possible despite the challenge. This moderate agreement is on-par with other difficult annotation tasks, such as those for connotation frames (Rashkin et al., 2016), which had 0.52 percentage agreement for rating the polarity of the sentence towards a target, dimensions of social relationships (Rashid and Blanco, 2017), which had κ val2 The full description of themes, sub-themes, and examples can be found in Supplementary Material §2. Figure 2: The distribution of four axes of discrimination (gender, race, sexuality, and other) in each subtheme. ues as low as 0.59, and Word Sense Disambiguation (Passonneau et al., 2012), which reported α values for some words below 0.30 at determining meaning. The final dataset was determined by first retaining all posts"
D19-1176,D14-1121,0,0.0185744,"s are considered negative examples. The feature sets that we used are motivated by prior work on gender bias and power dynamics. The following seven feature categories are used: (1) unigram and bigram features to capture lexical patterns, (2) two categories of formal and informal words derived from the formality corpus of Pavlick and Tetreault (2016), (3) the gendered occupations lexicon of Bolukbasi et al. (2016), grouped across definitional and stereotypical gender (male, female, neutral), (4) the gender stereotype lexicon of Fast et al. (2016), (5) the gender lexicon for social media from (Sap et al., 2014), (6) a manually-compiled corpus of gendered words, extended from seed list from https://www.hrc.org/resources/glossary-of-terms and (7) a manually-compiled sentiment lexicon, inspired by LIWC. To facilitate reproducibility, all lexicons will be available in the software release. 8 The threshold was chosen empirically to ensure enough number of positive examples. Experimental Setup We designed the annotation task with two purposes in mind: (1) We want to test our hypothesis that there is discrepancy in how annotators of different genders perceive MA S. By comparing the distribution of discrepa"
D19-1176,D17-1323,0,0.0307307,"timent tools can label these comments as being positive. As a result, applications using such techniques to promote inoffensive content may potentially promote MA S. Such biased online content is then used 1664 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1664–1674, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics as training data in NLP tools—dialogue systems, question answering, and others—thereby perpetuating and amplifying biases (Zhao et al., 2017). Computational modeling of MA S is challenging for many reasons. MA S are subjective, context-sensitive, and expressed subtly in language (as shown in Figure 1); there are no welldefined annotation guidelines, no corpus of MA S for training and evaluating a model, and prior computational approaches to detecting overtly offensive speech are likely not suitable for classifying MA S. Moreover, they are diluted in the ocean of social media content, and it would be infeasible to create a corpus of diverse types of MA S just by crowdsourcing annotations of randomly sampled social media posts. This"
D19-1176,W17-1101,0,0.0264363,"unconsciously expresses a prejudiced attitude toward a member of a marginalized group such as a racial minority.” Though subtle, MA S have been shown to have lasting harmful impacts on their targets. Qualitative interviews suggest that the subtlety of MA S may cause even greater levels of situational stress than overt aggression (Sue, 2010; Nadal et al., 2014). Introduction Toxicity and offensiveness are not always expressed with toxic language. While a substantial community effort has rightfully focused on identifying, preventing, and mitigating overtly toxic, profane, and hateful language (Schmidt and Wiegand, 2017), offensiveness spans a far larger spectrum that includes comments with more implicit and subtle signals that are no less offensive (Jurgens et al., 2019). One significant class of subtle-but-offensive comments includes microaggressions (Sue et al., 2007, MA S), defined in Merriam-Webster as “a comment or action that Despite a public effort to recognize and reduce—if not eliminate—their occurrence (Kim, 2013; Neff, 2015), there has been no computational work to detect and analyze MA S at scale. Instead, much of the recent work has focused on explicitly toxic language (e.g., Waseem et al., 2017"
D19-1176,D16-1054,0,0.0712136,"Missing"
D19-1176,L18-1445,1,0.855654,"ost. They are then asked rate if they found the response offensive and if so, to what degree was it offensive on a seven-point Likert scale. This scale of offensiveness allows us to capture cases where a microaggression is perceived as offensive by both annotator groups (genders), but to different degrees. To test our hypothesis and quantify how certain posts could be perceived differently by annotators of different genders, each annotation task includes a demographic question.6 Data To focus the data on interactions where gender may be a salient variable, Reddit data was drawn from RtGender (Voigt et al., 2018), which has a post-and-reply interactions where the gender of each author has been inferred with high confidence. After preliminary analyses, the initial data was randomly selected from 28 subreddits based on their focus around gender issues (e.g,. Relationships, AskWomen).7 For the first round of annotation, we used these four subsets of posts to be annotated for of6 Following best practices in gender elicitation (Jaroszewski et al., 2018), we include a third option of “nonbinary, genderqueer, or otherwise”, which has a freeform text box. We opted not to ask about cis or trans status in the d"
D19-1176,W17-3012,0,0.0246784,"and Wiegand, 2017), offensiveness spans a far larger spectrum that includes comments with more implicit and subtle signals that are no less offensive (Jurgens et al., 2019). One significant class of subtle-but-offensive comments includes microaggressions (Sue et al., 2007, MA S), defined in Merriam-Webster as “a comment or action that Despite a public effort to recognize and reduce—if not eliminate—their occurrence (Kim, 2013; Neff, 2015), there has been no computational work to detect and analyze MA S at scale. Instead, much of the recent work has focused on explicitly toxic language (e.g., Waseem et al., 2017), with surveys of the area also overlooking this important and challenging task of recognizing this subtle toxicity (van Aken et al., 2018; Salminen et al., 2018; Fortuna and Nunes, 2018). Indeed, as Figure 1 suggests, current popular tools for toxic language detection do not recognize the toxicity of MA S and further, sentiment tools can label these comments as being positive. As a result, applications using such techniques to promote inoffensive content may potentially promote MA S. Such biased online content is then used 1664 Proceedings of the 2019 Conference on Empirical Methods in Natura"
D19-1176,W13-2515,0,0.0152818,"0 2 4 2 4 Discrepancy Discrepancy [SelfMA] [Random] 6 6 Offensiveness Offensiveness representative of all types of MA S. However, our crowdsourcing approach does provide an effective way to surface MA S and our dual objective approach, which uses both annotator discrepancy and offensiveness, provides complementary views into what statements could be perceived as MA S. Additional iterations of this procedure are likely to improve microaggression recognition and substantially increase the seize of the corpus. We note that one option is to have workers generate examples, rather than rate (e.g., Xu et al., 2013; Su et al., 2016; Jiang et al., 2018); however, such a process raises ethical concerns of having crowdworkers generate toxic statements towards others. Second, our current focus is on gender-based MA S. This choice was motivated by the observation that gender-based MA S are the largest category in the S ELF MA data and, given that prior studies have shown substantial gender disparity online, with women receiving more negative behaviors (Duggan, 2017), this choice has the potential for highest impact. Our work builds upon a growing body of literature focused on identifying and mitigating gende"
jurgens-2014-analysis,W04-2807,0,\N,Missing
jurgens-2014-analysis,W08-1206,0,\N,Missing
jurgens-2014-analysis,W00-0103,0,\N,Missing
jurgens-2014-analysis,W09-2402,0,\N,Missing
jurgens-2014-analysis,N06-2015,0,\N,Missing
jurgens-2014-analysis,J13-3003,0,\N,Missing
jurgens-2014-analysis,S13-2049,1,\N,Missing
jurgens-2014-analysis,J06-1003,0,\N,Missing
jurgens-2014-analysis,P09-1002,0,\N,Missing
jurgens-2014-analysis,biemann-2012-turk,0,\N,Missing
jurgens-2014-analysis,S01-1001,0,\N,Missing
jurgens-2014-analysis,N13-1062,1,\N,Missing
jurgens-2014-analysis,P13-2127,0,\N,Missing
jurgens-2014-analysis,S12-1027,1,\N,Missing
jurgens-2014-analysis,E14-4042,0,\N,Missing
jurgens-2014-analysis,S13-2040,1,\N,Missing
jurgens-2014-analysis,ide-suderman-2004-american,0,\N,Missing
L16-1028,W10-1803,0,0.0238967,"rative centered around characters and their relationships, and then explores differences in various works through these abstractions. Such works rely on the accurate identification of characters and their mentions, but state-of-the-art methods for automatically detecting and resolving character mentions perform poorly (Vala et al., 2015) and there is sparse availability of manually annotated datasets. Moreover, the growing application of computational methods in researching narrative works has been met with a growing demand for annotated literary corpora, especially for character-driven work (Agarwal et al., 2010; He et al., 2013). Previous works have used gold mentions (Agarwal et al., 2013; Lee and Yeung, 2012), or restrict to using only highly prevalent characters (Ardanuy and Sporleder, 2014), for which current automatic systems are more accurate, underscoring the need for efficient ways of building annotated literary corpora. To address the scarcity of such corpora, we propose an annotation scheme and system for the task of character resolution, i.e. linking each mention of characters to the referent characters (a prerequisite to this task is detecting which mentions refer to characters). This pa"
L16-1028,W12-2513,0,0.335289,"ns with a character, usually, and once he stands up on his feet and begins to move, all I can do is trot along behind him with a paper and pencil trying to keep up long enough to put down what he says and does.” — William Faulkner Character is fundamental to literary analysis, forming the basis of much computational research in literary domains. They are crucial to investigating various literary aspects: Social networks embedded in narratives, where characters form the nodes and their interactions and relationships form the edges, have been useful in determining character importance and role (Agarwal et al., 2012), understanding genre (Ardanuy and Sporleder, 2014), investigating the impact of setting on the social world described in novels (Jayannavar et al., 2015; Elson et al., 2010; Moretti, 2005), and even generating story text (Sack, 2012). Previous work has also attempted to induce character archetypes using the text associated to character mentions in large corpora (Bamman et al., 2014b; Bamman et al., 2014a). Elsner (2012) outlines representations of narrative centered around characters and their relationships, and then explores differences in various works through these abstractions. Such works"
L16-1028,I13-1171,0,0.0126868,"ferences in various works through these abstractions. Such works rely on the accurate identification of characters and their mentions, but state-of-the-art methods for automatically detecting and resolving character mentions perform poorly (Vala et al., 2015) and there is sparse availability of manually annotated datasets. Moreover, the growing application of computational methods in researching narrative works has been met with a growing demand for annotated literary corpora, especially for character-driven work (Agarwal et al., 2010; He et al., 2013). Previous works have used gold mentions (Agarwal et al., 2013; Lee and Yeung, 2012), or restrict to using only highly prevalent characters (Ardanuy and Sporleder, 2014), for which current automatic systems are more accurate, underscoring the need for efficient ways of building annotated literary corpora. To address the scarcity of such corpora, we propose an annotation scheme and system for the task of character resolution, i.e. linking each mention of characters to the referent characters (a prerequisite to this task is detecting which mentions refer to characters). This paper offers the following contributions: • A comprehensive annotation scheme for"
L16-1028,W14-0905,0,0.0786551,"ands up on his feet and begins to move, all I can do is trot along behind him with a paper and pencil trying to keep up long enough to put down what he says and does.” — William Faulkner Character is fundamental to literary analysis, forming the basis of much computational research in literary domains. They are crucial to investigating various literary aspects: Social networks embedded in narratives, where characters form the nodes and their interactions and relationships form the edges, have been useful in determining character importance and role (Agarwal et al., 2012), understanding genre (Ardanuy and Sporleder, 2014), investigating the impact of setting on the social world described in novels (Jayannavar et al., 2015; Elson et al., 2010; Moretti, 2005), and even generating story text (Sack, 2012). Previous work has also attempted to induce character archetypes using the text associated to character mentions in large corpora (Bamman et al., 2014b; Bamman et al., 2014a). Elsner (2012) outlines representations of narrative centered around characters and their relationships, and then explores differences in various works through these abstractions. Such works rely on the accurate identification of characters"
L16-1028,P14-1035,0,0.0929064,"us literary aspects: Social networks embedded in narratives, where characters form the nodes and their interactions and relationships form the edges, have been useful in determining character importance and role (Agarwal et al., 2012), understanding genre (Ardanuy and Sporleder, 2014), investigating the impact of setting on the social world described in novels (Jayannavar et al., 2015; Elson et al., 2010; Moretti, 2005), and even generating story text (Sack, 2012). Previous work has also attempted to induce character archetypes using the text associated to character mentions in large corpora (Bamman et al., 2014b; Bamman et al., 2014a). Elsner (2012) outlines representations of narrative centered around characters and their relationships, and then explores differences in various works through these abstractions. Such works rely on the accurate identification of characters and their mentions, but state-of-the-art methods for automatically detecting and resolving character mentions perform poorly (Vala et al., 2015) and there is sparse availability of manually annotated datasets. Moreover, the growing application of computational methods in researching narrative works has been met with a growing demand"
L16-1028,day-etal-2008-corpus,0,0.0348198,"o enter the name for a new entity tag, which may collide with existing names, increasing the likelihood of duplicate entries and confusion among all the annotators. C HARLES automatically generates unique and meaningful tag names. Moreover, it allows users to disambiguate tags using a search feature which displays tagged instances under a particular tag with their surrounding contexts. Finally, C HARLES’ built-in chat system allows annotators to confer with each other on the identity of particular tags. These features help keep the character list duplicate-free. (The EDNA plugin for Callisto (Day et al., 2008) is another web tool designed for cross-document entity coreference resolution but is less flexible than CROMER (Girardi et al., 2014).) The annotation scheme for alias detection and character resolution most closely resembles the annotation guidelines set out by the Automatic Content Extraction program for the entity recognition problem (Doddington et al., 2004). However, characters are a type of entity that require special care in recognizing. They must be distinguished from noncharacter entities mention in text that may carry an animate signal (e.g. figurative and hypothetical persons), whi"
L16-1028,doddington-etal-2004-automatic,0,0.0733408,"th their surrounding contexts. Finally, C HARLES’ built-in chat system allows annotators to confer with each other on the identity of particular tags. These features help keep the character list duplicate-free. (The EDNA plugin for Callisto (Day et al., 2008) is another web tool designed for cross-document entity coreference resolution but is less flexible than CROMER (Girardi et al., 2014).) The annotation scheme for alias detection and character resolution most closely resembles the annotation guidelines set out by the Automatic Content Extraction program for the entity recognition problem (Doddington et al., 2004). However, characters are a type of entity that require special care in recognizing. They must be distinguished from noncharacter entities mention in text that may carry an animate signal (e.g. figurative and hypothetical persons), which usually requires a pragmatic understanding of the text. Finally, there has been recent work in developing automated methods for character detection, but Vala et al. (2015) show they all achieve limited performance, underscoring the need for high-quality, manually annotated corpora generated using comprehensive schemes and efficient annotation tools. 6. Conclus"
L16-1028,E12-1065,0,0.398794,"in narratives, where characters form the nodes and their interactions and relationships form the edges, have been useful in determining character importance and role (Agarwal et al., 2012), understanding genre (Ardanuy and Sporleder, 2014), investigating the impact of setting on the social world described in novels (Jayannavar et al., 2015; Elson et al., 2010; Moretti, 2005), and even generating story text (Sack, 2012). Previous work has also attempted to induce character archetypes using the text associated to character mentions in large corpora (Bamman et al., 2014b; Bamman et al., 2014a). Elsner (2012) outlines representations of narrative centered around characters and their relationships, and then explores differences in various works through these abstractions. Such works rely on the accurate identification of characters and their mentions, but state-of-the-art methods for automatically detecting and resolving character mentions perform poorly (Vala et al., 2015) and there is sparse availability of manually annotated datasets. Moreover, the growing application of computational methods in researching narrative works has been met with a growing demand for annotated literary corpora, especi"
L16-1028,P10-1015,0,0.361421,"to put down what he says and does.” — William Faulkner Character is fundamental to literary analysis, forming the basis of much computational research in literary domains. They are crucial to investigating various literary aspects: Social networks embedded in narratives, where characters form the nodes and their interactions and relationships form the edges, have been useful in determining character importance and role (Agarwal et al., 2012), understanding genre (Ardanuy and Sporleder, 2014), investigating the impact of setting on the social world described in novels (Jayannavar et al., 2015; Elson et al., 2010; Moretti, 2005), and even generating story text (Sack, 2012). Previous work has also attempted to induce character archetypes using the text associated to character mentions in large corpora (Bamman et al., 2014b; Bamman et al., 2014a). Elsner (2012) outlines representations of narrative centered around characters and their relationships, and then explores differences in various works through these abstractions. Such works rely on the accurate identification of characters and their mentions, but state-of-the-art methods for automatically detecting and resolving character mentions perform poor"
L16-1028,girardi-etal-2014-cromer,0,0.123944,"ebAnno support this task without prior knowledge of all the unique entities (i.e. the tag set for entities must be pre-loaded and is fixed during annotation). In our case, using either system would require all characters be known prior to the resolution of character mentions. C HARLES is more flexible and facilitates the management of a single global list of characters across annotators, while permitting annotators to dynamically add new characters as they encounter them during the annotation process. The tool has built-in measures that ensure the list is duplicatefree and consistent. CROMER (Girardi et al., 2014) is a web-based annotation tool specifically designed for cross-document entity coreference resolution. Unlike brat and WebAnno, it provides annotators the ability to enter new entities as they see fit. But the workflow is designed such that entity tags are defined prior to the annotation of documents, unlike C HARLES which intertwines the process of annotating documents and defining new characters, resulting in a more seamless annotation experience. Moreover, CROMER requires the user to enter the name for a new entity tag, which may collide with existing names, increasing the likelihood of du"
L16-1028,P13-1129,0,0.524583,"characters and their relationships, and then explores differences in various works through these abstractions. Such works rely on the accurate identification of characters and their mentions, but state-of-the-art methods for automatically detecting and resolving character mentions perform poorly (Vala et al., 2015) and there is sparse availability of manually annotated datasets. Moreover, the growing application of computational methods in researching narrative works has been met with a growing demand for annotated literary corpora, especially for character-driven work (Agarwal et al., 2010; He et al., 2013). Previous works have used gold mentions (Agarwal et al., 2013; Lee and Yeung, 2012), or restrict to using only highly prevalent characters (Ardanuy and Sporleder, 2014), for which current automatic systems are more accurate, underscoring the need for efficient ways of building annotated literary corpora. To address the scarcity of such corpora, we propose an annotation scheme and system for the task of character resolution, i.e. linking each mention of characters to the referent characters (a prerequisite to this task is detecting which mentions refer to characters). This paper offers the fol"
L16-1028,W15-0704,0,0.0136364,"g to keep up long enough to put down what he says and does.” — William Faulkner Character is fundamental to literary analysis, forming the basis of much computational research in literary domains. They are crucial to investigating various literary aspects: Social networks embedded in narratives, where characters form the nodes and their interactions and relationships form the edges, have been useful in determining character importance and role (Agarwal et al., 2012), understanding genre (Ardanuy and Sporleder, 2014), investigating the impact of setting on the social world described in novels (Jayannavar et al., 2015; Elson et al., 2010; Moretti, 2005), and even generating story text (Sack, 2012). Previous work has also attempted to induce character archetypes using the text associated to character mentions in large corpora (Bamman et al., 2014b; Bamman et al., 2014a). Elsner (2012) outlines representations of narrative centered around characters and their relationships, and then explores differences in various works through these abstractions. Such works rely on the accurate identification of characters and their mentions, but state-of-the-art methods for automatically detecting and resolving character m"
L16-1028,Y12-1022,0,0.413315,"rks through these abstractions. Such works rely on the accurate identification of characters and their mentions, but state-of-the-art methods for automatically detecting and resolving character mentions perform poorly (Vala et al., 2015) and there is sparse availability of manually annotated datasets. Moreover, the growing application of computational methods in researching narrative works has been met with a growing demand for annotated literary corpora, especially for character-driven work (Agarwal et al., 2010; He et al., 2013). Previous works have used gold mentions (Agarwal et al., 2013; Lee and Yeung, 2012), or restrict to using only highly prevalent characters (Ardanuy and Sporleder, 2014), for which current automatic systems are more accurate, underscoring the need for efficient ways of building annotated literary corpora. To address the scarcity of such corpora, we propose an annotation scheme and system for the task of character resolution, i.e. linking each mention of characters to the referent characters (a prerequisite to this task is detecting which mentions refer to characters). This paper offers the following contributions: • A comprehensive annotation scheme for detecting and resolvin"
L16-1028,W11-1902,0,0.0217629,"duplicates) and the accuracy of the character annotations, judged on the same 7 chapters5 annotated by an expert, are presented in Table 2 (The inter-annotator agreement was 0.972). Precision Recall F1-score Manual 0.962 0.989 0.975 Automatic 0.437 0.714 0.542 Table 2: Accuracy results for manual character annotation versus an automated method (Stanford’s coreference resolution system), on 7 chapters from Pride and Prejudice (as compared to an expert annotator). Table 2 compares the accuracies of the manual annotation and an automated method, namely the Stanford coreference resolution system (Lee et al., 2011), showing the former yields superior performance. Most errors of the manual approach can be attributed to human error and aliases that are indefinite plural pronouns, such as Everyone in the following, Everyone in the room said how well she looked. where it is unclear which characters are present in the room. 5. Related Works The brat (Stenetorp et al., 2012) and WebAnno6 (Yimam et al., 2013) systems are designed to be general-purpose tools and facilitate the manual annotation for a number of common linguistic tasks (including entity recognition, relation extraction, and coreference resolution"
L16-1028,E12-2021,0,0.183309,"Missing"
L16-1028,P13-4001,0,0.101562,"Missing"
L18-1445,N15-1084,0,0.0182101,"social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with comments given in response to the source texts. We collect such data from a variety of contexts, including: • Facebook (Politicians): Responses to Facebook posts from members of the U.S. House and Senate • Facebook (Public Figures): Responses to Facebook posts from other public figures, e.g., television hosts, journalists, an"
L18-1445,P06-1005,0,0.0356371,"tion are all public; however, to protect the anonymity of Facebook users in our dataset we remove all identifying user information as well as Facebook-internal information such as User IDs and Post IDs, replacing these with randomized ID numbers. Therefore users whose comments appear multiple times in our dataset may be compared, but without revealing their identity. We also only report commenter first names, since this is less identifying but still allows for running genderidentification algorithms. As a baseline for convenience we provide masculine/feminine ratios for these first names from Bergsma and Lin (2006). We collect posts and their associated top-level comments for the categories of speakers described below. In each case we find the page for the speaker with a novel method for finding gender-labeled speakers from Wikipedia. Specifically, our method takes as input a Wikipedia category page such as https://en.wikipedia.org/wiki/ Category:American_female_tennis_players, and for each name listed runs a search for public pages using Facebook’s Graph API. If an exact match for the name appears in the top three results, and the category of the page matches a relevant category (for instance, ”Public"
L18-1445,P05-1054,0,0.231484,"ource and responder may know one another and have an ongoing interaction afterwards. 2. Responses to Gender Here we aim to encourage research on responses to gender. Contrasting with language about or portraying a given gender which address abstract representations of social categories, responses to gender are directed towards an individual person. We know that social characteristics of the addressee influence linguistic behavior; existing computational work has shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often"
L18-1445,W17-3001,0,0.0230235,"nce, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,535 1,453,512 Response Wor"
L18-1445,P16-1080,0,0.0474796,"Missing"
L18-1445,W17-2902,0,0.116166,"about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,535 1,453,512 Response Word Count 376,114,950 123,753,913 15,549,984 6,606,087 44,537,612 Table 1: Basic statistics about the subcorpora within RtGender. Jha and Mamidi, 2017). Nevertheless, biased responses to social categories like gender can lead to marginalization (Sue, 2010) and negatively impact a person’s self-esteem and ability through mechanisms such as stereotype threat (Spencer et al., 1999). Perhaps most related to our work, Fu et al. (2016) analyze questions directed at men and women tennis players, finding that questions directed at men tend to be more about the game while questions directed at women are more likely to stray to topics about their appearance and off-court relationships. Tsou et al. (2014) similarly find comments on TED talks are more l"
L18-1445,D15-1130,0,0.0450648,"Missing"
L18-1445,D17-1247,0,0.0271873,"nstruction of identity and social categories like gender; social issues such as gender bias, in turn, often take form in language. Linguistic datasets have been used both to debunk gender-biased myths — for example, contrary to stereotype women are not actually more talkative than men (Mehl et al., 2007) — and to identify social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with"
L18-1445,D13-1170,0,0.00495629,"Missing"
L18-1445,N12-1084,0,0.027848,"shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,"
N13-1062,J08-4004,0,0.231558,"everal methods have been proposed to gather sense annotations using large numbers of untrained annotators, with mixed results. We propose three new annotation methodologies for gathering word senses where untrained annotators are allowed to use multiple labels and weight the senses. Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling. 1 Introduction Word sense annotation is regarded as one of the most difficult annotation tasks (Artstein and Poesio, 2008) and building manually-annotated corpora with highquality sense labels is often a time- and resourceconsuming task. As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Rece"
N13-1062,biemann-2012-turk,0,0.0909136,"Missing"
N13-1062,brown-etal-2010-number,0,0.0357018,"ann and Nygaard, 2010; Passonneau et al., 2012b; We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators"
N13-1062,burchardt-etal-2006-salsa,0,0.0705238,"Missing"
N13-1062,W02-0805,0,0.046284,"ks have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropri"
N13-1062,E12-1085,0,0.0258081,"Missing"
N13-1062,P09-1002,0,0.142467,"en contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity explicit, will annotators agree? Furthermore, we adopt the goal of Erk et al. (2009), which enabled annotators to weight each sense by its applicability to the given context, thereby quantifying the ambiguity. 556 Proceedings of NAACL-HLT 2013, pages 556–562, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics This paper provides the following contributions. Firs"
N13-1062,W11-0404,0,0.0785517,"Missing"
N13-1062,N06-2015,0,0.262911,"multiple labels and weight the senses. Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling. 1 Introduction Word sense annotation is regarded as one of the most difficult annotation tasks (Artstein and Poesio, 2008) and building manually-annotated corpora with highquality sense labels is often a time- and resourceconsuming task. As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; We revisit the task of crowdsourcing word sense annotations, focusing on two key"
N13-1062,E99-1046,0,0.154882,"he reference labeling. First, we measure the ability of the Turkers individually by evaluating their IAA with the reference labeling. Second, many studies using crowdsourcing combine the results into a single answer, thereby leveraging the wisdom of the crowds (Surowiecki, 2005) to smooth over inconsistencies in the data. Therefore, in the second experiment, we evaluate different methods of combining Turker responses into a single sense labeling, referred to as an aggregate labeling, and comparing that with the reference labeling. Third, we measure the replicability of the Turker annotations (Kilgarriff, 1999) using a sampling methodol558 ogy. Two equally-sized sets of Turker annotations are created by randomly sampling without replacement from the full set of annotations for each item. IAA is calculated between the aggregate labelings computed from each set. This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of Erk et al. (2009), where three annotators rated 50 instances each for eight words. For clarity, we refer to"
N13-1062,W06-2503,0,0.0234822,"ering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the"
N13-1062,W04-0807,0,0.0247831,"eport the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of Erk et al. (2009), where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3. Contexts were drawn evenly from the SemCor (Miller et al., 1993) and SENSEVAL-3 lexical substitution (Mihalcea et al., 2004) corpora. GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels. For each annotation task, Turkers were presented with an identical set of annotation guidelines, followed by methodology-specific instructions.1 To increase the familiarity with the task, four instances were shown per task, with all instances using the same target word. Unlike Passonneau et al. (2012b), we did not require a Turker to annotate all contexts for a single word; however many Turkers did complete the majority of instances. Both the Likert, Select, and Rate tasks used ten Turke"
N13-1062,H93-1061,0,0.428031,"from each set. This sampling is repeated 50 times and we report the mean IAA as a measure of the expected degree of replicability when annotating using different groups of Turkers. For the reference sense labeling, we use a subset of the GWS dataset of Erk et al. (2009), where three annotators rated 50 instances each for eight words. For clarity, we refer to these individuals as the GWS annotators. Given a word usage in a sentence, GWS annotators rated the applicability of all WordNet 3.0 senses using the same Likert scale as described in Section 3. Contexts were drawn evenly from the SemCor (Miller et al., 1993) and SENSEVAL-3 lexical substitution (Mihalcea et al., 2004) corpora. GWS annotators were apt to use multiple senses, with nearly all instances having multiple labels. For each annotation task, Turkers were presented with an identical set of annotation guidelines, followed by methodology-specific instructions.1 To increase the familiarity with the task, four instances were shown per task, with all instances using the same target word. Unlike Passonneau et al. (2012b), we did not require a Turker to annotate all contexts for a single word; however many Turkers did complete the majority of insta"
N13-1062,W04-2807,0,0.128176,"rdNet 3.0 0.409 0.349 Krippendorff’s α Krippendorff’s α MASC, single phase reported in Passonneau et al. (2010) GWS with Likert Replicability GWS with Erk et al. (2009) annotators † Not all words achieved this agreement. Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case of agreement. However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement. A re-annotation by Palmer et al. (2004) produced a similar pair-wise agreement of 71.0. ? Tou et al. (1999) perform a re-annotation test of the same data using student annotators, finding substantially lower agreement  Excludes agreement for argument.n, which was not annotated / IAA ranges for 37 words; no corpus-wide IAA is provided. ‡ Table 2: IAA for sense-annotated corpora fore produce noisy aggregate solutions. When win.v and different.a are excluded, the agreement between aggregate Likert and MaxDiff solutions is 0.649. While this IAA is still moderate, it suggests that Turkers can still produce similar annotations even when"
N13-1062,passonneau-etal-2006-inter,0,0.145615,"Missing"
N13-1062,W09-2402,0,0.0922119,"Missing"
N13-1062,passonneau-etal-2010-word,0,0.0680151,"d weight the senses. Our findings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling. 1 Introduction Word sense annotation is regarded as one of the most difficult annotation tasks (Artstein and Poesio, 2008) and building manually-annotated corpora with highquality sense labels is often a time- and resourceconsuming task. As a result, nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotati"
N13-1062,passonneau-etal-2012-masc,0,0.334997,"nearly all sense-tagged corpora in wide-spread use are created using trained annotators (Hovy et al., 2006; Passonneau et al., 2010), which results in a knowledge acquisition bottleneck for training systems that require sense labels (Gale et al., 1992). In other NLP areas, this bottleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing. Recently, several works have proposed gathering sense annotations using crowdsourcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement s"
N13-1062,W08-1206,0,0.0571825,"urcing (Snow et al., 2008; Biemann and Nygaard, 2010; Passonneau et al., 2012b; We revisit the task of crowdsourcing word sense annotations, focusing on two key aspects: (1) the annotation methodology itself, and (2) the restriction to single sense assignment. First, the choice in sense inventory plays an important role in gathering high-quality annotations; fine-grained inventories such as WordNet often contain several related senses for polysemous words, which untrained annotators find difficult to correctly apply in a given context (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007; Rumshisky and Batiukova, 2008; Brown et al., 2010). However, many agreement studies have restricted annotators to using a single sense, which can significantly lower inter-annotator agreement (IAA) in the presence of ambiguous or polysemous usages; indeed, multiple studies have shown that when allowed, annotators readily assign multiple senses to a single usage (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b). Therefore, we focus on annotation methodologies that enable workers to use as many labels as they feel appropriate, asking the question: if allowed to make labeling ambiguity expl"
N13-1062,rumshisky-etal-2012-word,0,0.120934,"Missing"
N13-1062,D08-1027,0,0.66915,"Missing"
N13-1062,W04-0811,0,0.0465131,"Missing"
N13-1062,W99-0502,0,0.083236,"ase reported in Passonneau et al. (2010) GWS with Likert Replicability GWS with Erk et al. (2009) annotators † Not all words achieved this agreement. Kilgarriff (2002) uses a multi-stage agreement procedure where two annotators rate each item, and in the case of disagreement, a third annotator is added. If the third annotator agrees with either of the first two, the instance is marked as a case of agreement. However, the unadjudicated agreement for the dataset was 67.3 measured using pair-wise agreement. A re-annotation by Palmer et al. (2004) produced a similar pair-wise agreement of 71.0. ? Tou et al. (1999) perform a re-annotation test of the same data using student annotators, finding substantially lower agreement  Excludes agreement for argument.n, which was not annotated / IAA ranges for 37 words; no corpus-wide IAA is provided. ‡ Table 2: IAA for sense-annotated corpora fore produce noisy aggregate solutions. When win.v and different.a are excluded, the agreement between aggregate Likert and MaxDiff solutions is 0.649. While this IAA is still moderate, it suggests that Turkers can still produce similar annotations even when using different annotation methodologies. For the third experiment,"
N13-1062,S01-1004,0,\N,Missing
N15-1169,baccianella-etal-2010-sentiwordnet,0,0.0145331,"onstruction procedure is accurate and has a significant impact on a WordNet-based algorithm encountering novel lemmas. 1 Introduction Semantic knowledge bases are an essential, enabling component of many NLP applications. A notable example is WordNet (Fellbaum, 1998), which encodes a taxonomy of concepts and semantic relations between them. As a result, WordNet has enabled a wide variety of NLP techniques such as Word Sense Disambiguation (Agirre et al., 2014), information retrieval (Varelas et al., 2005), semantic similarity (Pedersen et al., 2004; B¨ar et al., 2013), and sentiment analysis (Baccianella et al., 2010). However, semantic knowledge bases such as WordNet are expensive to produce; as a result, their scope and domain are often constrained by the resources available and may omit highly-specific concepts or lemmas, as well as new terminology that emerges after their construction. For example, WordNet does not contain the nouns “stepmom,” “broadband,” and “prequel.” Because of the coverage limitations of WordNet, several approaches have attempted to enrich WordNet with new relations and concepts. One group of approaches has enriched WordNet by aligning its structure with that of other resources su"
N15-1169,P13-4021,0,0.0369554,"Missing"
N15-1169,S12-1004,0,0.0169502,"in order to extend WordNet with new synsets. However, structurebased approaches are limited only to the concepts appearing in Wikipedia article titles, which almost always correspond to noun concepts. Distributional and probabilistic approaches are also limited to OOV terms for which it is possible to gather enough statistics. As Wiktionary contains all parts of speech and our method is independent of word frequency, neither limitation applies to this work. Other related work has attempted to tap resources such as Wikipedia for automatically constructing new ontologies (Suchanek et al., 2007; Dandala et al., 2012; Moro and Navigli, 2012; Meyer and Gurevych, 2012), extending existing ones through either alignment-based methods (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014) or inferring the positions of new senses by their shared attributes which are extracted from text (Reisinger and Pas¸ca, 2009). Extension and alignment approaches based on Wikipedia are limited mainly to noun concepts in Wikipedia; furthermore, these techniques cannot be directly applied to Wiktionary because its lack of taxonomic structure would prevent adding most OOV data to the existing WordNet taxonomy. 6 Conclusion"
N15-1169,S14-2003,1,0.815024,"lt, the median error is likely an overestimate of the expected error for the C ROWN construction process. 4.2 Application-based evaluation Semantic similarity is one of the core features of many NLP applications. The second evaluation measures the performance improvement of using C ROWN instead of WordNet for measuring semantic similarity when faced with slang or OOV lemmas. Notably, prior semantic similarity benchmarks such as SimLex-999 (Hill et al., 2014) and the ESL test questions (Turney, 2001) have largely omitted these types of words. However, the recent dataset of SemEval-2014 Task 3 (Jurgens et al., 2014) includes similarity judgments between a WordNet sense and a word not defined in WordNet’s vocabulary or with a slang interpretation not present in WordNet. 1462 Table 3: The Pearson correlation performance of ADW when using the WordNet and C ROWN semantic networks on the word-to-sense test dataset of SemEval-2014 Task 3. We also show results for the string-based baseline system (GST) and for the best participating system in the word-to-sense comparison type of Task 3. 4.2.1 Methodology Semantic similarity was measured using the similarity algorithm of Pilehvar et al. (2013), ADW,3 which first"
N15-1169,S14-2072,0,0.0136046,"ge is due only to the differences between the two networks. Performance is measured using Pearson correlation with the gold standard judgments. 4.2.2 Results Of the 60 OOV lemmas and 38 OOV slang terms in the test data, 51 and 26 were contained in C ROWN, respectively. Table 3 shows the Pearson correlation performance of ADW in the two settings for all lemmas in the dataset, and for three subsets of the dataset: OOV, slang, and regular lemmas, the latter of which are in WordNet; the bottom rows show the performance of the Task’s best participating system for the word-to-sense comparison type (Kashyap et al., 2014) and the most competi3 https://github.com/pilehvar/ADW 4 http://wordnet.princeton.edu/glosstag.shtml tive baseline, based on Greedy String Tiling (GST) (Wise, 1996). ADW sees large performance improvements in the OOV and slang words when using C ROWN instead of WordNet, which are both statistically significant at p<0.01. The overall improvement of ADW would place it as the fifth best system in this comparison type of Task 3. The performance on regular in-WordNet and OOV lemmas is approximately equal, indicating the high accuracy of OOV hypernym attachment in C ROWN. Notably, on OOV and Slang,"
N15-1169,P14-5010,0,0.00416277,"ove Wiktionary markup. The extracted texts were then partitioned into two sets: (1) those expressing a lexicalization, e.g., “1337” is an alternative spelling of “elite” and (2) those indicating a definition. Novel lexicalizations that are not already handled by the WordNet morphological analyzer (Morphy) were added to the lexicalization exception lists in C ROWN. Definitions are processed using two methods to identify a set of candidate lemmas whose senses might be identical or near to the appropriate hypernym synset. First, candidates are obtained by parsing the gloss with Stanford CoreNLP (Manning et al., 2014) and extract1460 ing the head word and all other words joined to it by a conjunction. Second, additional candidates are collected from the first hyperlinked term or phrase in the gloss, which is similar to the approach of Navigli and Velardi (2010) for hypernym extraction in Wikipedia. Candidates are then filtered to ensure that (1) they have the same part of speech as the definition’s term and (2) they are defined in WordNet, which is necessary for the attachment. 3.2 Structural and Lexical Attachment Three types of structural or lexical heuristics were used to attach OOV lemmas when the appr"
N15-1169,Q13-1013,0,0.0786187,"rticle titles, which almost always correspond to noun concepts. Distributional and probabilistic approaches are also limited to OOV terms for which it is possible to gather enough statistics. As Wiktionary contains all parts of speech and our method is independent of word frequency, neither limitation applies to this work. Other related work has attempted to tap resources such as Wikipedia for automatically constructing new ontologies (Suchanek et al., 2007; Dandala et al., 2012; Moro and Navigli, 2012; Meyer and Gurevych, 2012), extending existing ones through either alignment-based methods (Matuschek and Gurevych, 2013; Pilehvar and Navigli, 2014) or inferring the positions of new senses by their shared attributes which are extracted from text (Reisinger and Pas¸ca, 2009). Extension and alignment approaches based on Wikipedia are limited mainly to noun concepts in Wikipedia; furthermore, these techniques cannot be directly applied to Wiktionary because its lack of taxonomic structure would prevent adding most OOV data to the existing WordNet taxonomy. 6 Conclusion This work has introduced C ROWN version 1.0, a new extension of WordNet that merges sense definitions from Wiktionary to add new hypernym and ant"
N15-1169,I11-1099,0,0.0344856,"ectly applied. 3 Extending WordNet C ROWN is created by identifying lemmas that are out of vocabulary (OOV) in WordNet but have one or more associated glosses in Wiktionary. A new synset is created for that lemma and a hypernym relation is added to the appropriate WordNet synset. The C ROWN attachment process rates hypernym candidates using two methods. First, where possible, we exploit structural or morphological information to identify highly-probable candidates. Second, following previous work on resource alignment showing that lexical overlap accurately measures gloss semantic similarity (Meyer and Gurevych, 2011; Navigli and Ponzetto, 2012), candidates are found by measuring the similarity of the Wiktionary gloss with the glosses of synsets found by a constrained search of the WordNet graph. We note that attaching OOV lemmas by first aligning WordNet and Wiktionary is not possible due to relation sparsity within Wiktionary, where most OOV words would not be connected to the aligned network. Following, we first describe the Wiktionary preprocessing steps and then detail both OOV attachment methods. 3.1 Preprocessing Wiktionary was parsed using JWKTL (Zesch et al., 2008) to extract the text associated"
N15-1169,miller-gurevych-2014-wordnet,0,0.302185,"Missing"
N15-1169,P10-1134,0,0.0363326,"already handled by the WordNet morphological analyzer (Morphy) were added to the lexicalization exception lists in C ROWN. Definitions are processed using two methods to identify a set of candidate lemmas whose senses might be identical or near to the appropriate hypernym synset. First, candidates are obtained by parsing the gloss with Stanford CoreNLP (Manning et al., 2014) and extract1460 ing the head word and all other words joined to it by a conjunction. Second, additional candidates are collected from the first hyperlinked term or phrase in the gloss, which is similar to the approach of Navigli and Velardi (2010) for hypernym extraction in Wikipedia. Candidates are then filtered to ensure that (1) they have the same part of speech as the definition’s term and (2) they are defined in WordNet, which is necessary for the attachment. 3.2 Structural and Lexical Attachment Three types of structural or lexical heuristics were used to attach OOV lemmas when the appropriate data was available. First, Wikisaurus or Wiktionary synonym relations create sets of mutually-synonymous lemmas, which may contain OOV lemmas. The common hypernym of these lemmas is estimated by computing the most frequent hypernym synset f"
N15-1169,P14-1044,1,0.77811,"in are often constrained by the resources available and may omit highly-specific concepts or lemmas, as well as new terminology that emerges after their construction. For example, WordNet does not contain the nouns “stepmom,” “broadband,” and “prequel.” Because of the coverage limitations of WordNet, several approaches have attempted to enrich WordNet with new relations and concepts. One group of approaches has enriched WordNet by aligning its structure with that of other resources such as Wikipedia or Wiktionary (RuizCasado et al., 2005; Navigli and Ponzetto, 2012; Miller and Gurevych, 2014; Pilehvar and Navigli, 2014). However, because these approaches identify corresponding lemmas with identical lexicalizations, they are often unable to directly add novel lemmas to the existing taxonomic structure. The second group of approaches performs taxonomy induction to learn hypernymy relationships between words (Moro and Navigli, 2012; Meyer and Gurevych, 2012). However, these approaches often produce separate taxonomies from WordNet, which are also generally not readily accessible as resources. We introduce a new resource C ROWN (CommunityenRiched Open WordNet) that extends the existing WordNet taxonomy, more tha"
N15-1169,P13-1132,1,0.78939,"emEval-2014 Task 3 (Jurgens et al., 2014) includes similarity judgments between a WordNet sense and a word not defined in WordNet’s vocabulary or with a slang interpretation not present in WordNet. 1462 Table 3: The Pearson correlation performance of ADW when using the WordNet and C ROWN semantic networks on the word-to-sense test dataset of SemEval-2014 Task 3. We also show results for the string-based baseline system (GST) and for the best participating system in the word-to-sense comparison type of Task 3. 4.2.1 Methodology Semantic similarity was measured using the similarity algorithm of Pilehvar et al. (2013), ADW,3 which first represents a given linguistic item (such as a word or a concept) using random walks over the WordNet semantic network, where random walks are initialized from the synsets associated with that item. The similarity between two linguistic items is accordingly computed in terms of the similarity of their corresponding representations. ADW is an ideal candidate for measuring the impact of C ROWN for two reasons. First, the algorithm obtains state-of-the-art performance on both wordbased and sense-based benchmarks using only WordNet as a knowledge source. Second, the method is bo"
N15-1169,W08-0507,0,0.0921622,"ly significant at p<0.01. The overall improvement of ADW would place it as the fifth best system in this comparison type of Task 3. The performance on regular in-WordNet and OOV lemmas is approximately equal, indicating the high accuracy of OOV hypernym attachment in C ROWN. Notably, on OOV and Slang, the unsupervised ADW, when coupled with the additional information in C ROWN , produces competitive results with the best performing system, which is a multi-feature supervised system utilizing extensive external dictionaries and distributional methods. 5 Related Work Most related is the work of Poprat et al. (2008), who attempted to automatically build an extension of WordNet with biomedical terminology; however, they were unsuccessful in constructing the resource. Other work has attempted to leverage distributional similarity techniques (Snow et al., 2006) or exploit the structured information in Wikipedia (Ruiz-Casado et al., 2005; Toral et al., 2008; Ponzetto and Navigli, 2009; Yamada et al., 2011) in order to extend WordNet with new synsets. However, structurebased approaches are limited only to the concepts appearing in Wikipedia article titles, which almost always correspond to noun concepts. Dist"
N15-1169,P09-1070,0,0.194356,"Missing"
N15-1169,P06-1101,0,0.338597,"rnym attachment in C ROWN. Notably, on OOV and Slang, the unsupervised ADW, when coupled with the additional information in C ROWN , produces competitive results with the best performing system, which is a multi-feature supervised system utilizing extensive external dictionaries and distributional methods. 5 Related Work Most related is the work of Poprat et al. (2008), who attempted to automatically build an extension of WordNet with biomedical terminology; however, they were unsuccessful in constructing the resource. Other work has attempted to leverage distributional similarity techniques (Snow et al., 2006) or exploit the structured information in Wikipedia (Ruiz-Casado et al., 2005; Toral et al., 2008; Ponzetto and Navigli, 2009; Yamada et al., 2011) in order to extend WordNet with new synsets. However, structurebased approaches are limited only to the concepts appearing in Wikipedia article titles, which almost always correspond to noun concepts. Distributional and probabilistic approaches are also limited to OOV terms for which it is possible to gather enough statistics. As Wiktionary contains all parts of speech and our method is independent of word frequency, neither limitation applies to t"
N15-1169,toral-etal-2008-named,0,0.380664,"additional information in C ROWN , produces competitive results with the best performing system, which is a multi-feature supervised system utilizing extensive external dictionaries and distributional methods. 5 Related Work Most related is the work of Poprat et al. (2008), who attempted to automatically build an extension of WordNet with biomedical terminology; however, they were unsuccessful in constructing the resource. Other work has attempted to leverage distributional similarity techniques (Snow et al., 2006) or exploit the structured information in Wikipedia (Ruiz-Casado et al., 2005; Toral et al., 2008; Ponzetto and Navigli, 2009; Yamada et al., 2011) in order to extend WordNet with new synsets. However, structurebased approaches are limited only to the concepts appearing in Wikipedia article titles, which almost always correspond to noun concepts. Distributional and probabilistic approaches are also limited to OOV terms for which it is possible to gather enough statistics. As Wiktionary contains all parts of speech and our method is independent of word frequency, neither limitation applies to this work. Other related work has attempted to tap resources such as Wikipedia for automatically c"
N15-1169,I11-1098,0,0.518362,"Missing"
N15-1169,zesch-etal-2008-extracting,0,0.045225,"Missing"
N15-1169,J14-1003,0,\N,Missing
P10-4006,W07-0607,0,0.182621,"tics. Word space algorithms have been proposed as an automated approach for developing meaningfully comparable semantic representations based on word distributions in text. Many of the well known algorithms, such as Latent Semantic Analysis (Landauer and Dumais, 1997) and Hyperspace Analogue to Language (Burgess and Lund, 1997), have been shown to approximate human judgements of word similarity in addition to providing computational models for other psychological and linguistic phenomena. More recent approaches have extended this approach to model phenomena such as child language acquisition (Baroni et al., 2007) or semantic priming (Jones et al., 2006). In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natural language processing and cognitive psychology. For a recent survey of word space approaches and applications, see (Turney and Pantel, 2010). The package is written in Java and defines a standardized Java interface for word space algorithms. While other word space frameworks exist, e.g. (Widdows and Ferraro, 2008), the focus of this framework is to ease the development of new algorithms and the comparison against existing models. Comp"
P10-4006,P98-2127,0,0.0519115,"irt.” 3.4 Measurements The choice of similarity function for the vector space is the least standardized across approaches. Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al., 2004). To facilitate exploration of the similarity function parameter space, the S-Space Package provides support for multiple similarity functions: cosine similarity, Euclidean distance, KL divergence, Jaccard Index, Pearson product-moment correlation, Spearman’s rank correlation, and Lin Similarity (Dekang, 1998) 3.2 Data Structures and Utilities The S-Space Package provides efficient implementations for matrices, vectors, and specialized data structures such as multi-maps and tries. Implementations are modeled after the java.util library and offer concurrent implementations when multi-threading is required. In addition, the libraries provide support for converting between multiple matrix formats, enabling interaction with external matrix-based programs. The package also provides support for parsing different corpora formats, such as XML or email threads. 3.5 Clustering 3.3 Global Operation Utilities"
P10-4006,J07-2002,0,0.161659,"Missing"
P10-4006,W04-2406,0,0.157812,"only those features that best distinguish word meanings. Other models approximate the full set of features to enable better scalability. Global vector space operations are applied to the entire space once the initial word features have been computed. Common operations include altering feature weights and dimensionality reducApproximation Models Random Indexing (Sahlgren et al., 2008) Reflective Random Indexing (Cohen et al., 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al., 2006) Incremental Semantic Analysis (Baroni et al., 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010) Table 1: Algorithms in the S-Space Package tion. These operations are designed to improve word similarity by changing the feature space itself. 3 The S-Space Framework The S-Space framework is designed to be extensible, simple to use, and scalable. We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for multi-threading. Each word space algorithm is designed to run as a stand alone program and also to be used as a library class. 3.1 Reference Algorithms The package provides reference implementati"
P10-4006,W09-4302,1,0.833177,"Missing"
P10-4006,S10-1080,1,0.850002,"Missing"
P10-4006,C04-1146,0,0.0583691,"Missing"
P10-4006,widdows-ferraro-2008-semantic,0,0.0133121,"linguistic phenomena. More recent approaches have extended this approach to model phenomena such as child language acquisition (Baroni et al., 2007) or semantic priming (Jones et al., 2006). In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natural language processing and cognitive psychology. For a recent survey of word space approaches and applications, see (Turney and Pantel, 2010). The package is written in Java and defines a standardized Java interface for word space algorithms. While other word space frameworks exist, e.g. (Widdows and Ferraro, 2008), the focus of this framework is to ease the development of new algorithms and the comparison against existing models. Compared to existing frameworks, the S-Space Package supports a much wider variety of algorithms and provides significantly more reusable developer utilities for word spaces, such as tokenizing and filtering, sparse vectors and matrices, specialized data structures, and seamless integration with external programs for dimensionality reduction and clustering. We hope that the release of this framework will greatly facilitate other researchers in their efforts to develop and vali"
P10-4006,C98-2122,0,\N,Missing
P13-1132,N09-1003,0,0.0486211,"Missing"
P13-1132,S12-1051,0,0.29581,"ky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, which enables meaningful similarity comparisons across different scales of text or lexical levels. Second, by operating at the sense level, a unified approach is able to identify the semantic similarities that exist independently of the text’s lexical forms and any semantic ambiguity therein. For example, consider the sentences: t1. A manager fired the worker. t2. An emplo"
P13-1132,S12-1059,0,0.0305138,"Missing"
P13-1132,C10-1005,0,0.0137954,"Missing"
P13-1132,J12-1003,0,0.0112526,"the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely"
P13-1132,P11-2087,0,0.0296674,", word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic simila"
P13-1132,J06-1003,0,0.937818,"2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense infor"
P13-1132,W05-1203,0,0.293606,"vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional features. As can be seen from Table 3, our alignmentbased disambiguatio"
P13-1132,J13-3008,1,0.741191,"Missing"
P13-1132,P08-2067,0,0.0268098,"an and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Halawi et al., 2012), while document-based similarity methods require more linguistic features, which often makes them inapplicable at the word or microtext level (Salton et al., 1975; Maguitman et al., 2005; Elsayed et al., 2008; Turney and Pantel, 2010). Despite the potential advantages, few approaches to semantic similarity operate at the sense level due to the challenge in sense-tagging text (Navigli, 2009); for example, none of the top four systems in the recent SemEval-2012 task on textual similarity compared semantic representations that incorporated sense information (Agirre et al., 2012). We propose a unified approach to semantic similarity across multiple representation levels from senses to documents, which offers two significant advantages. First, the method is applicable independently of the input type, w"
P13-1132,J12-1005,0,0.0406973,"Missing"
P13-1132,N06-2015,0,0.0347272,"(Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs and adjectives; (2) sense groupings included in the OntoNotes project4 (Hovy et al., 2006) for nouns and verbs. Following the evaluation methodology of Snow et al. (2007), we combine the Senseval-2 and OntoNotes datasets into a third dataset. Snow et al. (2007) considered sense grouping as a binary classification task whereby for each word every possible pairing of senses has to be classified 4 Sense groupings belong to a pre-version 1.0: http:// cemantix.org/download/sense/ontonotes-sense-groups.tar.gz Onto Noun Verb 0.406 0.522 0.421 0.544 0.418 0.531 0.370 0.455 0.218 0.396 Noun 0.450 0.483 0.478 NA NA SE-2 Verb 0.465 0.482 0.473 NA NA Adj 0.484 0.531 0.501 0.473 0.371 Onto + SE"
P13-1132,D07-1061,0,0.146501,"andom walks beginning at that node will produce a frequency distribution over the nodes in the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala,"
P13-1132,O97-1002,0,0.0927604,"icks the most relevant sense of the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our"
P13-1132,S01-1004,0,0.0108658,"t::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs and adjectives; (2) sense groupings included in the OntoNotes project4 (Hovy et al., 2006) for nouns and verbs. Following the evaluation methodology of Snow et al. (2007), we combine the Senseval-2 and OntoNotes datasets into a third dataset. Snow et al. (2007) considered sense grouping as a binary classification task whereby for each word every possible pairing of senses has to be classified 4 Sense groupings belong to a pre-version 1.0: http:// cemantix.org/download/sense/ontonotes-sense-groups.tar.gz Onto Noun Verb 0.406 0.522 0.421 0.544 0.418 0.531 0.370 0.455"
P13-1132,P98-2127,0,0.0658275,"f the word according to the resulting probability vector. As the lexical knowledge base of UKB, we used the same semantic network as that utilized by our approach for calculating semantic signatures. Table 3 lists the performance values of the two above-mentioned systems on the three training sets in terms of Pearson correlation. In addition, we present in the table correlation scores for four other similarity measures reported by B¨ar et al. (2012): • Pairwise Word Similarity that comprises of a set of WordNet-based similarity measures proposed by Resnik (1995), Jiang and Conrath (1997), and Lin (1998b). The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. • Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) where the highdimensional vectors are obtained on WordNet, Wikipedia and Wiktionary. • Distributional Thesaurus where a similarity score is computed similarly to that of Lin (1998a) using a distributional thesaurus obtained from a 10M dependency-parsed sentences of English newswire. • Character n-grams which were also used as one of our additional feat"
P13-1132,magnini-cavaglia-2000-integrating,0,0.0196462,"Missing"
P13-1132,W06-2503,0,0.0213638,"my of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of 1347 Approach Correlation ADWCos 0.825 Agirre et al. (2009) 0.830 Hughes and Ramage (2007) 0.838 Zesch et al. (2008) 0.840 ADWJac 0.841 ADWW O 0.868 Method RCos RW O RJac SVM ODE Table 6: Spearman’s ρ correlation coefficients with human judgments on the RG-65 dataset. ADWJac , ADWW O , and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources s"
P13-1132,P06-1014,1,0.932281,"tories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of 1347 Approach Correlation ADWCos 0.825 Agirre et al. (2009) 0.830 Hughes and Ramage (2007) 0.838 Zesch et al. (2008) 0.840 ADWJac 0.841 ADWW O 0.868 Method RCos RW O RJac SVM ODE Table 6: Spearman’s ρ correlation coefficients with human judgments on the RG-65 dataset. ADWJac , ADWW O , and ADWCos correspond to results with the Jaccard, Weighted Overlap and Cosine signature comparison measures respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic sig"
P13-1132,N04-3012,0,0.0975539,"respectively. word senses (Agirre and Lopez, 2003; McCarthy, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarsegrained sense distinctions of the Oxford Dictionary of English (ODE). The approach leverages semantic similarities in gloss definitions and the hierarchical relations between senses in the ODE to cluster WordNet senses. As current state of the art, Snow et al. (2007) developed a supervised SVM classifier that utilized, as its features, several earlier sense relatedness techniques such as those implemented in the WordNet::Similarity package (Pedersen et al., 2004). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavagli`a, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). 5.2 Experimental Setup We benchmark the accuracy of our similarity measure in grouping word senses against those of Navigli (2006) and Snow et al. (2007) on two datasets of manually-labeled sense groupings of WordNet senses: (1) sense groupings provided as a part of the Senseval-2 English Lexical Sample WSD task (Kilgarriff, 2001) which includes nouns, verbs"
P13-1132,N13-1130,1,0.864485,"Missing"
P13-1132,W09-3204,0,0.101622,"the graph visited during the walk. To extend beyond a single sense, the random walk may be initialized and restarted from a set of senses (seed nodes), rather than just one; this multi-seed walk produces a multinomial distribution over all the senses in WordNet with higher probability assigned to senses that are frequently visited from the seeds. Prior work has demonstrated that multinomials generated from random walks over WordNet can be successfully applied to linguistic tasks such as word similarity (Hughes and Ramage, 2007; Agirre et al., 2009), paraphrase recognition, textual entailment (Ramage et al., 2009), and pseudoword generation (Pilehvar and Navigli, 2013). Formally, we define the semantic signature of a lexical item as the multinomial distribution generated from the random walks over WordNet 3.0 where the set of seed nodes is the set of senses present in the item. This representation encompasses both when the item is itself a single sense and when the item is a sense-tagged sentence. To construct each semantic signature, we use the iterative method for calculating topic-sensitive PageRank (Haveliwala, 2002). Let M be the adjacency matrix for the WordNet network, where edges connect senses"
P13-1132,2003.mtsummit-papers.42,0,0.0659828,"ot include the mistakes made when the Jaccard measure was used as they vary with the k value. For the similarity judgment evaluation, we used as benchmark the RG-65 dataset created by Rubenstein and Goodenough (1965). The dataset contains 65 word pairs judged by 51 human subjects on a scale of 0 to 4 according to their semantic similarity. Ideally, a measure’s similarity judgments are expected to be highly correlated with those of humans. To be consistent with the previous literature (Hughes and Ramage, 2007; Agirre et al., 2009), we used Spearman’s rank correlation in our experiment. that of Rapp (2003) uses word senses, an approach that is outperformed by our method. The errors produced by our system were largely the result of sense locality in the WordNet network. Table 5 highlights the incorrect responses. The synonym mistakes reveal cases where senses of the two words are close in WordNet, indicating some relatedness. For example, percentage may be interpreted colloquially as monetary value (e.g., “give me my percentage”) and elicits the synonym of profit in the economic domain, which ADW incorrectly selects as a synonym. 4.1 4.3 Experimental Setup Our alignment-based sense disambiguatio"
P13-1132,D07-1107,0,0.352211,"ation can achieve state-of-the-art performance on three similarity tasks, each operating at a different lexical level: (1) surpassing the highest scores on the SemEval-2012 task on textual similarity (Agirre et al., 2012) that compares sentences, (2) achieving a near-perfect performance on the TOEFL synonym selection task proposed by Landauer and Dumais (1997), which measures word pair similarity, and also obtaining state-of-the-art performance in terms of the correlation with human judgments on the RG-65 dataset (Rubenstein and Goodenough, 1965), and finally (3) surpassing the performance of Snow et al. (2007) in a sensecoarsening task that measures sense similarity. 2 A Unified Semantic Representation We propose a representation of any lexical item as a distribution over a set of word senses, referred to as the item’s semantic signature. We begin with a formal description of the representation at the sense level (Section 2.1). Following this, we describe our alignment-based disambiguation algorithm which enables us to produce sense-based semantic signatures for those lexical items (e.g., words or sentences) which are not sense annotated (Section 2.2). Finally, we propose three methods for comparin"
P13-1132,J11-2003,0,0.012897,"t multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 1 Introduction Semantic similarity is a core technique for many topics in Natural Language Processing such as Textual Entailment (Berant et al., 2012), Semantic Role Labeling (F¨urstenau and Lapata, 2012), and Question Answering (Surdeanu et al., 2011). For example, textual similarity enables relevant documents to be identified for information retrieval (Hliaoutakis et al., 2006), while identifying similar words enables tasks such as paraphrasing (Glickman and Dagan, 2003), lexical substitution (McCarthy and Navigli, 2009), lexical simplification (Biran et al., 2011), and Web search result clustering (Di Marco and Navigli, 2013). Approaches to semantic similarity have often operated at separate levels: methods for word similarity are rarely applied to documents or even single sentences (Budanitsky and Hirst, 2006; Radinsky et al., 2011; Hal"
P13-1132,S12-1060,0,0.0804335,"Missing"
P13-1132,P95-1026,0,0.288603,"of the context words in both texts. To find this maximum we use an alignment procedure which, for each word type wi in item T1 , assigns wi to the sense that has the maximal similarity to any sense of the word types in the compared text T2 . Algorithm 1 formalizes the alignment process, which produces a sense disambiguated representation as a result. Senses are compared in terms of their semantic signatures, denoted as function R. We consider multiple definitions of R, defined later in Section 2.3. As a part of the disambiguation procedure, we leverage the one sense per discourse heuristic of Yarowsky (1995); given all the word types in two compared lexical items, each type is assigned a single sense, even if it is used multiple times. Additionally, if the same word type appears in both sentences, both will always be mapped to the same sense. Although such a sense assignment is potentially incorrect, assigning both types to the same sense results in a representation that does no worse than a surface-level comparison. We illustrate the alignment-based disambiguation procedure using the two example sentences t1 and t2 given in Section 1. Figure 1(a) illustrates example alignments of the first sense"
P13-1132,agirre-de-lacalle-2004-publicly,0,\N,Missing
P13-1132,C98-2122,0,\N,Missing
P14-1122,J08-4004,0,0.0131135,"Missing"
P14-1122,J06-1003,0,0.00591089,"ated to validate conceptconcept and concept-image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional"
P14-1122,W02-0817,0,0.0697425,"an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated video gamelike mechanics for annotation. First, Herda˘gdelen and Baroni (2012) validate automatically a"
P14-1122,eom-etal-2012-using,0,0.0152073,"has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task. While prior efforts in NLP have incorporated games for performing annotation and validation (Siorpaes and Hepp, 2008b; Herda˘gdelen and Baroni, 2012; Poes"
P14-1122,P14-1089,1,0.859176,"Missing"
P14-1122,P09-2053,0,0.542226,"Missing"
P14-1122,N06-1058,0,0.0155935,"ing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. Th"
P14-1122,P10-1023,1,0.342133,"propose a cost-effective method of validating and extending knowledge bases using video games with a purpose. Two video games were created to validate conceptconcept and concept-image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build"
P14-1122,D12-1128,1,0.698726,"Missing"
P14-1122,W11-0122,0,0.00823163,"knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using"
P14-1122,P13-1132,1,0.654556,"image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify nove"
P14-1122,P06-1101,0,0.0307866,"tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream app"
P14-1122,W13-0215,0,0.254243,"te for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated video gamelike mechanics for annotation. First, Herda˘gdelen and Baroni (2012) validate automatically acquired common sense relations using a slot machin"
P14-1122,D08-1056,0,0.0232994,"e most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game’s objectives, which potentially decreases motivation for answering correctly. Several works have proposed adapting existing word-based board game designs to create or validate common sense knowledge. von Ahn et al. (2006) generate common sense facts by using a game similar to TabooTM , where one player must list facts about a computer-selected lemma and a second player must guess the original lemma having seen only the facts. Similarly, Vickrey et al. (2008) gather free associations to a target word with the constraint, similar to TabooTM , where players cannot enter a small set of banned words. Vickrey et al. (2008) also present two games similar to the ScattergoriesTM , where players are given a category and then must list things in that category. The two variants differ in the constraints imposed on the players, such as beginning all items with a specific letter. For all three games, two players play the same game under time limits and then are rewarded if their answers match. Last, three two-player games have focused on validating and extendi"
P17-2009,D11-1145,0,0.0532572,"Missing"
P17-2009,P16-2096,0,0.0351595,". Furthermore, in a case study using Twitter for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Be"
P17-2009,W16-6212,0,0.269445,"development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essentia"
P17-2009,P15-1120,0,0.0212281,"tudies employ off-the-shelf LID systems without considering how they were trained. We aim to create a socially-representative corpus for LID that captures the variation within a language, such as orthography, dialect, formality, topic, and spelling. Motivated by the recent language survey of Twitter (Trampus, 2016), we next describe how we construct this corpus for 70 languages along three dimensions: geography, social and topical diversity, and multilinguality. Geographic Diversity We create a large-scale dataset of geographically-diverse text by bootstrapping with a people-centric approach (Bamman, 2015) that treats location and languagesspoken as demographic attributes to be inferred for authors. By inferring both for Twitter users and then collecting documents from monolingual users, we ensure that we capture regional variation in a language, rather than focusing on a particular aspect of linguistic variety. Individuals’ locations are inferred using the method of Compton et al. (2014) as implemented by Jurgens et al. (2015). The method first identifies the individuals who have reliable ground truth locations from geotagged tweets and then infers the locations of other individuals as the geo"
P17-2009,W12-2108,0,0.0272177,"16; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and Briti"
P17-2009,D16-1120,0,0.0307458,"Missing"
P17-2009,D14-1069,0,0.044983,"enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification"
P17-2009,W16-5801,0,0.0440216,"Missing"
P17-2009,N13-1097,0,0.0695273,"Missing"
P17-2009,W14-3356,0,0.055626,"Missing"
P17-2009,W16-5806,0,0.00821164,"Missing"
P17-2009,P12-3005,0,0.0943464,"epresented populations, enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language i"
P17-2009,W14-1303,0,0.0375212,"Missing"
P17-2009,D16-1217,0,0.0683736,"Missing"
P17-2009,Q14-1003,0,0.100672,"hich is roughly three epochs. Comparison Systems We compare against two broad-coverage LID systems, langid.py (Lui and Baldwin, 2012) and CLD2 (McCandless, 2010), both of which have been widely used for Twitter within in the NLP community. CLD2 is trained on web page text, while langid.py was trained on newswire, JRC-Acquis, web pages, and Wikipedia. As neither was designed for Twitter, we preprocess text to remove user mentions, hashtags, and URLs for a more fair comparison. For multilingual documents, we substitute langid.py (Lui and Baldwin, 2012) with its extension, Polyglot, described in Lui et al. (2014) and designed for that particular task. Equitable LID Classifier We introduce E QUI LID, and evaluate it on monolingual and multilingual tweet-length text. Model Character-based neural network architectures are particularly suitable for LID, as they facilitate modeling nuanced orthographic and phonological properties of languages (Jaech et al., 2016; Samih et al., 2016), e.g., capturing regular morpheme occurrences within the words of a language. Further, character-based methods significantly reduce the model complexity compared to word-based methods; the latter require separate neural represe"
P17-2009,W14-3907,0,0.0299577,"rage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and British English are composed of local dialects that vary across city and socioecon"
P17-2009,D13-1084,0,0.131253,"Missing"
P17-2009,W14-5307,0,0.0170451,"Missing"
P17-2009,W15-5401,0,0.0296329,"Missing"
P17-2009,P17-1180,0,0.0156764,"level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and British English are composed of local dialects that vary across city and socioeconomic development level ("
P17-2009,P13-1018,0,\N,Missing
P19-1357,C10-2011,0,0.0151241,"ify social norm violations that would be considered insults. Second, new methods should identify disparity in treatment of social groups. For example, in a study of the respectfulness of police language, Voigt et al. (2017) found that officers were consistently less likely to use respectful language with black community members than with white community members—a disparity in a positive social dimension. As NLP solutions have been developed for other social dimensions of language such as politeness (Danescu-NiculescuMizil et al., 2013; Munkova et al., 2013; Chhaya et al., 2018) and formality (Brooke et al., 2010; Sheikha and Inkpen, 2011; Pavlick and Tetreault, 2016), these methods could be readily adapted for identifying such systematic bias for additional social categories and settings. 2018). Such community-specific norms and context are important to take into account, as NLP researchers are doubling down on context-sensitive approaches to define (e.g., Chandrasekharan and Gilbert, 2019) and detect abuse (e.g., Gao and Huang, 2017). However, not all community norms are socially acceptable within the broader world. Even behavior considered harmful in one community might be celebrated in another, e."
P19-1357,D18-1386,0,0.0691711,"versation. Recent work has shown that it is possible to predict whether a conversation will become toxic on Wikipedia (Zhang et al., 2018) and whether bullying will occur on Instagram (Liu et al., 2018). These predictable abuse trajectories open the door to developing new models for preemptive interventions that directly mitigate harm. Third, messages that are not intended as offensive create opportunities to nudge authors towards correcting their text if the offense is pointed out. This strategy builds upon recent work on explainable ML for identifying which parts of a message are offensive (Carton et al., 2018; Noever, 2018), and work on paraphrase and style transfer for suggesting an appropriate inoffensive alternative (Santos et al., 2018; Prabhumoye et al., 2018). For example, parts of a message could be paraphrased to adjust the level of politeness in order to minimize any cumulative disparity towards one social group (Sennrich et al., 2016). 4 Justice Frameworks for NLP Martin Luther King Jr. wrote that the biggest obstacle to Black freedom is the “white moderate, 3661 who is more devoted to ‘order’ than to justice, who prefers a negative peace which is the absence of tension to a positive pea"
P19-1357,C18-1093,0,0.0413728,"adition for establishing evaluating metrics, defining data guidelines, and, more broadly, bringing together researchers. The broad nature of abusive behavior creates significant challenges for the shared task paradigm. Here, we outline three opportunities for new shared tasks in this area. First, new NLP shared tasks should develop annotation guidelines accurately define what constitutes abusive behavior in the target community. Recent works have begun to make progress in this area by modeling the context in which a comment is made through user and community-level features (Qian et al., 2018; Mishra et al., 2018; Ribeiro et al., 2018), yet often the norms in these settings are implicit making it difficult to transfer the techniques and models to other settings. As one potential solution, Chandrasekharan et al. (2018) studied community norms on Reddit in a large-scale, datadriven manner, and released a dataset of over 40K removed comments from Reddit labeled according to the specific type of norm being violated (Chan3660 drasekharan and Gilbert, 2019). Second, new NLP shared tasks must address the data scarcity faced by abuse detection research while minimizing harm caused by the data. Constant exposu"
P19-1357,R13-1065,0,0.0193705,"Missing"
P19-1357,W17-0802,0,0.0279729,"ad of a negative articulation—an evergrowing list of prohibited behaviors—we should use a positive phrasing (e.g., “you will be able to”) of capabilities in an online community. Such effort naturally extends our proposal for detecting community-specific abuse to one of promoting community norms. Accordingly, NLP technologies can be developed to identify positive behaviors and ensure individuals are able to fulfill these capabilities. Several recent works have made strides in this direction by examining positive behaviors such as how constructive conversations are (Kolhatkar and Taboada, 2017; Napoles et al., 2017), whether dialog on contentious topics can exist without devolving into squabbling (Tan et al., 2016), or the level of support given between community members (Wang and Jurgens, 2018). Second, once we have adequately articulated what people in a community should be able to do, we must address how the community handles transgressions. The notion of restorative justice is a useful theoretical tool for thinking about how wrongdoing should be handled. Restorative justice theory emphasizes repair and uses a process in which stakeholders, including victims and transgressors, decide together on conse"
P19-1357,Q16-1005,0,0.0198667,"ered insults. Second, new methods should identify disparity in treatment of social groups. For example, in a study of the respectfulness of police language, Voigt et al. (2017) found that officers were consistently less likely to use respectful language with black community members than with white community members—a disparity in a positive social dimension. As NLP solutions have been developed for other social dimensions of language such as politeness (Danescu-NiculescuMizil et al., 2013; Munkova et al., 2013; Chhaya et al., 2018) and formality (Brooke et al., 2010; Sheikha and Inkpen, 2011; Pavlick and Tetreault, 2016), these methods could be readily adapted for identifying such systematic bias for additional social categories and settings. 2018). Such community-specific norms and context are important to take into account, as NLP researchers are doubling down on context-sensitive approaches to define (e.g., Chandrasekharan and Gilbert, 2019) and detect abuse (e.g., Gao and Huang, 2017). However, not all community norms are socially acceptable within the broader world. Even behavior considered harmful in one community might be celebrated in another, e.g., Reddit’s r/fatpeoplehate (Chandrasekharan et al., 20"
P19-1357,P18-1080,0,0.0151022,"lying will occur on Instagram (Liu et al., 2018). These predictable abuse trajectories open the door to developing new models for preemptive interventions that directly mitigate harm. Third, messages that are not intended as offensive create opportunities to nudge authors towards correcting their text if the offense is pointed out. This strategy builds upon recent work on explainable ML for identifying which parts of a message are offensive (Carton et al., 2018; Noever, 2018), and work on paraphrase and style transfer for suggesting an appropriate inoffensive alternative (Santos et al., 2018; Prabhumoye et al., 2018). For example, parts of a message could be paraphrased to adjust the level of politeness in order to minimize any cumulative disparity towards one social group (Sennrich et al., 2016). 4 Justice Frameworks for NLP Martin Luther King Jr. wrote that the biggest obstacle to Black freedom is the “white moderate, 3661 who is more devoted to ‘order’ than to justice, who prefers a negative peace which is the absence of tension to a positive peace which is the presence of justice” (King, 1963). Analogously, by focusing only on classifying individual unacceptable speech acts, NLP risks being the same k"
P19-1357,N18-2019,0,0.0522123,"Missing"
P19-1357,N16-3020,0,0.0263776,"g. The notion of procedural justice explains that people are more likely to comply with a community’s rules if they believe the authorities are legitimate (Tyler and Huo, 2002; Sherman, 2003). For NLP, it means that our systems for detecting non-compliance must be transparent and fair. People will comply only if they accept the legitimacy of both the platform and the algorithms it employs. Therefore, abuse detection methods are needed to justify why a particular act was a violation to build legitimacy; a natural starting point for NLP in building legitimacy is recent work from explainable ML (Ribeiro et al., 2016; Lei et al., 2016; Carton et al., 2018). 5 Conclusion Abusive behavior online affects a substantial amount of the population. The NLP community has proposed computational methods to help mitigate this problem, yet has also struggled to move beyond the most obvious tasks in abuse detection. Here, we propose a new strategy for NLP to tackling online abuse in three ways. First, expanding our purview for abuse detection to include both extreme behaviors and the more subtle— but still offensive—behaviors like microaggressions and condescension. Second, NLP must develop methods that go beyond react"
P19-1357,P18-2031,0,0.0155833,"2018) and whether bullying will occur on Instagram (Liu et al., 2018). These predictable abuse trajectories open the door to developing new models for preemptive interventions that directly mitigate harm. Third, messages that are not intended as offensive create opportunities to nudge authors towards correcting their text if the offense is pointed out. This strategy builds upon recent work on explainable ML for identifying which parts of a message are offensive (Carton et al., 2018; Noever, 2018), and work on paraphrase and style transfer for suggesting an appropriate inoffensive alternative (Santos et al., 2018; Prabhumoye et al., 2018). For example, parts of a message could be paraphrased to adjust the level of politeness in order to minimize any cumulative disparity towards one social group (Sennrich et al., 2016). 4 Justice Frameworks for NLP Martin Luther King Jr. wrote that the biggest obstacle to Black freedom is the “white moderate, 3661 who is more devoted to ‘order’ than to justice, who prefers a negative peace which is the absence of tension to a positive peace which is the presence of justice” (King, 1963). Analogously, by focusing only on classifying individual unacceptable speech acts,"
P19-1357,W17-1101,0,0.0884984,"utes Abuse The classifications we adopt and computationally enforce have real and lasting consequences by defining both what is and what is not abuse (Bowker and Star, 2000). Abusive behavior is an omnibus term that often includes harassment, threats, racial slurs, sexism, unexpected pornographic content, and insults—all of which can be directed at other users or at whole communities (Davidson et al., 2017; Nobata et al., 2016). However, NLP has largely considered a far narrower scope of what constitutes abuse through its selection of which types of behavior to recognize (Waseem et al., 2017; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). We argue that NLP needs to expand its computational efforts to recognize two additional general types of abuse: (a) infrequent and physically dangerous abuse, and (b) more common but subtle abuse. Additionally, we need to develop methods that respect community norms in classification decisions. These categories of abuse and the importance of community norms have been noted elsewhere (Liu et al., 2018; Guberman and Hemphill, 2017; Salminen et al., 2018; Blackwell et al., 2017) but have not yet received the same level of attention in NLP. Who has a right to speak and"
P19-1357,N16-1005,0,0.0228191,"hird, messages that are not intended as offensive create opportunities to nudge authors towards correcting their text if the offense is pointed out. This strategy builds upon recent work on explainable ML for identifying which parts of a message are offensive (Carton et al., 2018; Noever, 2018), and work on paraphrase and style transfer for suggesting an appropriate inoffensive alternative (Santos et al., 2018; Prabhumoye et al., 2018). For example, parts of a message could be paraphrased to adjust the level of politeness in order to minimize any cumulative disparity towards one social group (Sennrich et al., 2016). 4 Justice Frameworks for NLP Martin Luther King Jr. wrote that the biggest obstacle to Black freedom is the “white moderate, 3661 who is more devoted to ‘order’ than to justice, who prefers a negative peace which is the absence of tension to a positive peace which is the presence of justice” (King, 1963). Analogously, by focusing only on classifying individual unacceptable speech acts, NLP risks being the same kind of obstacle as the white moderate: Instead of seeking the absence of certain types of speech, we should seek the presence of equitable participation. We argue that NLP should cons"
P19-1357,W11-2826,0,0.0130459,"tions that would be considered insults. Second, new methods should identify disparity in treatment of social groups. For example, in a study of the respectfulness of police language, Voigt et al. (2017) found that officers were consistently less likely to use respectful language with black community members than with white community members—a disparity in a positive social dimension. As NLP solutions have been developed for other social dimensions of language such as politeness (Danescu-NiculescuMizil et al., 2013; Munkova et al., 2013; Chhaya et al., 2018) and formality (Brooke et al., 2010; Sheikha and Inkpen, 2011; Pavlick and Tetreault, 2016), these methods could be readily adapted for identifying such systematic bias for additional social categories and settings. 2018). Such community-specific norms and context are important to take into account, as NLP researchers are doubling down on context-sensitive approaches to define (e.g., Chandrasekharan and Gilbert, 2019) and detect abuse (e.g., Gao and Huang, 2017). However, not all community norms are socially acceptable within the broader world. Even behavior considered harmful in one community might be celebrated in another, e.g., Reddit’s r/fatpeopleha"
P19-1357,D18-1004,1,0.843991,"h effort naturally extends our proposal for detecting community-specific abuse to one of promoting community norms. Accordingly, NLP technologies can be developed to identify positive behaviors and ensure individuals are able to fulfill these capabilities. Several recent works have made strides in this direction by examining positive behaviors such as how constructive conversations are (Kolhatkar and Taboada, 2017; Napoles et al., 2017), whether dialog on contentious topics can exist without devolving into squabbling (Tan et al., 2016), or the level of support given between community members (Wang and Jurgens, 2018). Second, once we have adequately articulated what people in a community should be able to do, we must address how the community handles transgressions. The notion of restorative justice is a useful theoretical tool for thinking about how wrongdoing should be handled. Restorative justice theory emphasizes repair and uses a process in which stakeholders, including victims and transgressors, decide together on consequences. A restorative process may produce a punishment, such as banning, but can include consequences such as apology and reconciliation (Braithwaite, 2002). Just responses consider"
P19-1357,W17-3012,0,0.229675,"thinking What Constitutes Abuse The classifications we adopt and computationally enforce have real and lasting consequences by defining both what is and what is not abuse (Bowker and Star, 2000). Abusive behavior is an omnibus term that often includes harassment, threats, racial slurs, sexism, unexpected pornographic content, and insults—all of which can be directed at other users or at whole communities (Davidson et al., 2017; Nobata et al., 2016). However, NLP has largely considered a far narrower scope of what constitutes abuse through its selection of which types of behavior to recognize (Waseem et al., 2017; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). We argue that NLP needs to expand its computational efforts to recognize two additional general types of abuse: (a) infrequent and physically dangerous abuse, and (b) more common but subtle abuse. Additionally, we need to develop methods that respect community norms in classification decisions. These categories of abuse and the importance of community norms have been noted elsewhere (Liu et al., 2018; Guberman and Hemphill, 2017; Salminen et al., 2018; Blackwell et al., 2017) but have not yet received the same level of attention in NLP. Wh"
P19-1357,W17-3009,0,0.0423889,"n-word), where a human-looking bot account would reply with a fixed comment about the harm such language caused and an appeal to empathy, leading to long-term behavior change in the offenders. Identifying how to best respond to abusive behavior—or whether to respond at all—are important computational next steps for this NLP strategy and one that likely needs to be done in collaboration with researchers from fields such as Psychology. Prior work has shown counter speech to be effective for limiting the effects of hate speech (Schieb and Preuss, 2016; Mathew et al., 2018; Stroud and Cox, 2018). Wright et al. (2017) notes that real-world examples of bystanders intervening can be found online, thereby providing a potential source of training data but methods are needed to reliably identify such counter speech examples. Second, interventions that occur after a point of escalation may have little positive effect in some circumstances. For example, when two individuals have already begun insulting one another, both have already become upset and must lose face to reconcile (Rubin et al., 1994). At this point, deescalation may prevent further abuse but does little for restoring the situation to a constructive"
P19-1357,P18-1125,0,0.0508408,"that occur after a point of escalation may have little positive effect in some circumstances. For example, when two individuals have already begun insulting one another, both have already become upset and must lose face to reconcile (Rubin et al., 1994). At this point, deescalation may prevent further abuse but does little for restoring the situation to a constructive dialog (Gottman, 1999). However, interventions that occur before the point of abuse can serve to shift the conversation. Recent work has shown that it is possible to predict whether a conversation will become toxic on Wikipedia (Zhang et al., 2018) and whether bullying will occur on Instagram (Liu et al., 2018). These predictable abuse trajectories open the door to developing new models for preemptive interventions that directly mitigate harm. Third, messages that are not intended as offensive create opportunities to nudge authors towards correcting their text if the offense is pointed out. This strategy builds upon recent work on explainable ML for identifying which parts of a message are offensive (Carton et al., 2018; Noever, 2018), and work on paraphrase and style transfer for suggesting an appropriate inoffensive alternative (Santo"
P19-1357,W17-3002,0,\N,Missing
P19-1625,P13-2037,0,0.0269534,"constraints (Pfaff, 1979; Poplack, 1980), audience design (Gumperz, 1977; Bell, 1984), or even to evoke a specific perception of the speaker’s identity (Niedzielski, 1999; Schmid, 2001). In common social situations, many of these factors are in play, yet we often do not have an idea of how they interact. Here, we present a large scale study of code switching in Nigeria between English and Naij´a, the widelyspoken Nigerian creole, to quantify which factors predict switching. Computational studies of code switching have largely focused on linguistic aspects of switching (Solorio and Liu, 2008; Adel et al., 2013; Vyas et al., 2014; Hartmann et al., 2018). However, several recent works have begun to examine the contextual factors that influence switching behavior, ∗ Authors contributed equally. David Jurgens School of Information University of Michigan jurgens@umich.edu finding that the topic driving a discussion spurs on language variation (Shoemark et al., 2017; Stewart et al., 2018) and that individuals are sensitive to the scope of their audience when choosing a language (Papalexakis et al., 2014; Pavalanathan and Eisenstein, 2015). Given that the social context is known to be strongly influential"
Q14-1035,J08-4004,0,0.066065,"ncrete noun senses, whereas Puzzle Racer annotates all parts of speech and both concrete and abstract senses. Furthermore, Puzzle Racer’s output enables new visual games for tasks using word senses such as Word Sense Disambiguation, frame detection, and selectional preference acquisition. The second game, Ka-boom!, performs Word Sense Disambiguation (WSD) to identify the meaning of a word in context by players interacting with pictures. Sense annotation is regarded to be one of the most challenging NLP annotation tasks (Fellbaum et al., 1998; Edmonds and Kilgarriff, 2002; Palmer et al., 2007; Artstein and Poesio, 2008), so we view it as a challenging application for testing the limits of visual NLP games. Our work provides the following four contributions. First, we present a new game-centric design methodology for NLP games with a purpose. Second, we demonstrate with the first game that video games can produce linguistic annotations equal in quality to those of experts and at a cost reduction from gathering the same annotations via crowdsourcing; with the second game we show that video games provide a statistically significant performance improvement over a current state-of-the-art nonvideo game with a pur"
Q14-1035,P98-1013,0,0.163072,"stroy, causing them to lose interest. Extensibility Ka-boom! contains two core mechanics: (1) instructions on which pictures should be destroyed and which should be spared, and (2) series of images shown to the player during game play. As with Puzzle Racer, the Ka-boom! mechanics can be modified to extend the game to new types of annotation. For example, instructions could display picture examples and ask players to destroy either similar or opposite-meaning ideas in order to annotate synonyms or antonyms. In another setting, images can be associated with semantic frames (e.g., from FrameNet (Baker et al., 1998)) and players must spare images showing the frame of the game’s 458 sentence in order to provide frame annotations. 6 Ka-boom! Annotation Analysis Ka-boom! is intended to provide a complementary and more-enjoyable method for sense annotation using only pictures. To test its effectiveness, we perform a direct comparison with the state-of-the-art GWAP for sense annotation, Wordrobe (Venhuizen et al., 2013), which is not a video game. 6.1 Experimental Setup Organizers of the Wordrobe project (Venhuizen et al., 2013) provided a data set of 111 recentlyannotated contexts having between one and nine"
Q14-1035,W02-0817,0,0.1261,"Missing"
Q14-1035,D09-1046,0,0.0234013,"layers and 16,000 images. Two experiments were performed. First, we directly compared the quality of the game-based annotations with those of crowdsourcing. Second, we compared the difference in quality between expertbased gold standard images and the highest-ranked images rated by players. 4.1 Experimental Setup To test the potential of our approach, we selected a range of 23 polysemous noun, verb, and adjective lemmas, shown in Table 1. Lemmas had 4-10 senses each, for a total of 132 senses. Many lemmas have both abstract and concrete senses and some are known to have highly-related senses (Erk and McCarthy, 2009). Hence, given their potential annotation difficulty, we view performance on these lemmas as a lower bound. For all lemmas, during the image generation process (Sec. 3.2) annotators were able to produce queries for all but one sense, expect2v ;2 this produced 1356 gold images in G and 16,656 unrated images 2 The sense expect2v has the definition, “consider obligatory; request and expect.” Annotators were able to formulate many queries that could have potentially shown images of this definition, but the images results of such queries were consistently unrelated to the meaning. interest1n : a se"
Q14-1035,P13-1120,1,0.830288,"on-related mechanics: (1) an initial set of instructions on how players are to interact with images, (2) multiple series of images shown during game play, and (3) an open-ended question at the end of the game. These mechanics can be easily extended to other types of annotation where players must choose between several concepts shown as options in the puzzle gates. For example, the instructions could show players a phrase such as “a bowl of *” and ask players to race over images of things that might fit the “*” argument in order to obtain selectional preference annotations of the phrase (`a la Flati and Navigli (2013)); the lemmas or senses associated with the selected images can be aggregated to identify the types of arguments preferred by players for the game’s provided phrase. Similarly, the instructions could be changed to provide a set of keywords or phrases (instead of images associated with a sense) and ask players to navigate over images of the words in order to perform image labeling. Nouns Verbs Adjectives disc4n : a flat circular plate circular plate dish plate argument, arm, atmosphere, bank, difficulty, disc, interest, paper, party, shelter activate, add, climb, eat, encounter expect, rule, sm"
Q14-1035,P09-2053,0,0.456664,"Missing"
Q14-1035,W11-0404,0,0.0483446,"is crowdsourcing word sense annotations. Despite initial success in performing WSD using crowdsourcing (Snow et al., 2008), many approaches noted the difficulty of performing WSD with untrained annotators, especially as the degree of polysemy increases or when word senses are related. Several approaches have attempted to make the task more suitable for untrained annotators by (1) using the crowd itself to define the sense inventory (Biemann and Nygaard, 2010), thereby ensuring the crowd understands the sense distinctions, (2) modifying the questions to explicitly model annotator uncertainty (Hong and Baker, 2011; Jurgens, 2013), or (3) using sophisticated methods to aggregate multiple annotations (Passonneau et al., 2012; Passonneau and Carpenter, 2013). In all cases, annotation was purely text based, in contrast to our work. 3 Game 1: Puzzle Racer The first game was designed to fill an important need for enabling engaging NLP games: image representations of concepts, specifically WordNet senses. Our goals are two-fold: (1) to overcome the limits of current sense-image libraries, which have focused largely on concrete nouns and (2) to provide a general game platform for annotation tasks that need to"
Q14-1035,N13-1062,1,0.571609,"sense annotations. Despite initial success in performing WSD using crowdsourcing (Snow et al., 2008), many approaches noted the difficulty of performing WSD with untrained annotators, especially as the degree of polysemy increases or when word senses are related. Several approaches have attempted to make the task more suitable for untrained annotators by (1) using the crowd itself to define the sense inventory (Biemann and Nygaard, 2010), thereby ensuring the crowd understands the sense distinctions, (2) modifying the questions to explicitly model annotator uncertainty (Hong and Baker, 2011; Jurgens, 2013), or (3) using sophisticated methods to aggregate multiple annotations (Passonneau et al., 2012; Passonneau and Carpenter, 2013). In all cases, annotation was purely text based, in contrast to our work. 3 Game 1: Puzzle Racer The first game was designed to fill an important need for enabling engaging NLP games: image representations of concepts, specifically WordNet senses. Our goals are two-fold: (1) to overcome the limits of current sense-image libraries, which have focused largely on concrete nouns and (2) to provide a general game platform for annotation tasks that need to associate lexica"
Q14-1035,H93-1061,0,0.533758,"Sense disambiguation accuracies fluent English speakers and were free to recruit other players. A total of 19 players participated. Unlike Puzzle Racer, players were not compensated. Each context was seen in at least six games. WSD performance is measured using the traditional precision and recall definitions and the F1 measure of the two (Navigli, 2009); because all items are annotated, precision and recall are equivalent and we report performance as accuracy. Performance is measured relative to two baselines: (1) a baseline that picks the sense of the lemma that is most frequent in SemCor (Miller et al., 1993), denoted as MFS, and (2) a baseline equivalent to performance if players had randomly clicked on images, denoted as Random.5 6.2 Results Two analyses were performed. Because Kaboom! continuously revises the annotation during gameplay based on which pictures players spare, the first analysis assesses how the accuracy changes 5 This baseline is similar to random sense selection but takes into account differences in the number of pictures per sense. 459 with respect to the length of one Ka-boom! game. The second analysis measures the accuracy with respect to the number of games played per contex"
Q14-1035,W13-2323,0,0.0161494,"roaches noted the difficulty of performing WSD with untrained annotators, especially as the degree of polysemy increases or when word senses are related. Several approaches have attempted to make the task more suitable for untrained annotators by (1) using the crowd itself to define the sense inventory (Biemann and Nygaard, 2010), thereby ensuring the crowd understands the sense distinctions, (2) modifying the questions to explicitly model annotator uncertainty (Hong and Baker, 2011; Jurgens, 2013), or (3) using sophisticated methods to aggregate multiple annotations (Passonneau et al., 2012; Passonneau and Carpenter, 2013). In all cases, annotation was purely text based, in contrast to our work. 3 Game 1: Puzzle Racer The first game was designed to fill an important need for enabling engaging NLP games: image representations of concepts, specifically WordNet senses. Our goals are two-fold: (1) to overcome the limits of current sense-image libraries, which have focused largely on concrete nouns and (2) to provide a general game platform for annotation tasks that need to associate lexical items with images. Following, we first describe the design, annotation process, and extensibility of the game, and then discus"
Q14-1035,J14-4005,1,0.812208,"contest period. The difference in collection time reflects an important difference in the current resources: while crowdsourcing has established platforms with on-demand workers, no central platforms exist for games with a purpose with an analogous pool of game players. However, although the current games were released in a limited fashion, later game releases to larger venues such as Facebook may attract more players and significantly decrease both collection times and overall annotation cost. 5 Game 2: Ka-boom! Building large-scale sense-annotated corpora is a long-standing objective (see (Pilehvar and Navigli, 2014)) and has sparked significant interest in developing effective crowdsourcing annotation and GWAP strategies (cf. Sec. 2). Therefore, we propose a second video game, Ka-boom!, that produces sense annotations from game play. A live demonstration of the game is available online.4 Design and Game Play Ka-boom! is an action game in the style of the popular Fruit Ninja game: pictures are tossed on screen from the boundaries of the screen, which the player must then selectively destroy in order to score points. The game’s challenge stems from rapidly identifying which pictures should be destroyed or"
Q14-1035,D08-1027,0,0.470986,"Missing"
Q14-1035,P14-1122,1,0.71668,"Missing"
Q14-1035,W13-0215,0,0.0858123,"ed by linguistic experts or trained annotators. However, such effort is often very time- and cost-intensive, and as a result creating large-scale annotated datasets remains a longstanding bottleneck for many areas of NLP. As an alternative to requiring expert-based annotations, many studies used untrained, online workers, commonly known as crowdsourcing. When Within NLP, gamified annotation tasks include anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009) and disambiguation (Seemakurty et al., 2010; Venhuizen et al., 2013). The games’ interfaces typically incorporate common game elements such as scores, leaderboards, or difficulty levels. However, the game itself remains largely text-based, with a strong resemblance to a traditional annotation task, and little resemblance to games most people actively play. In the current work, we propose a radical shift in NLP-focused GWAP design, building graphical, dynamic games that achieve the same result as traditional annotation. Rather than embellish an annota449 Transactions of the Association for Computational Linguistics, 2 (2014) 449–463. Action Editor: Mirella Lapa"
Q14-1035,J13-3003,0,\N,Missing
Q14-1035,C98-1013,0,\N,Missing
Q18-1028,N12-1009,0,0.0606437,"Missing"
Q18-1028,N13-1067,0,0.64401,"Missing"
Q18-1028,W12-3202,1,0.843103,"a variance inflation factor of < 10 for all variables. 400 their work as C OMPARISON OR C ONTRAST or E X TENDS , with both having significant positive effects. 8 The Growth of Rapid Discovery Science As scientific fields evolve, new subfields initially emerge around methods or technologies which become a focus of collective puzzle-solving and continual improvement (Moody, 2004). NLP has witnessed the emergence of several such subfields from the early grammar based approaches in the 1950s1970s, to the statistical revolution in the 1990s, to the recent deep learning models (Sp¨arck Jones, 2001; Anderson et al., 2012). Collins (1994) proposed that a field can undergo a particular shift, referring to it as rapid discovery science, when the field (a) reaches high consensus on research topics as well as methods and technologies, and (b) then develops genealogies of methods and technologies that continually improve on one another. Over time, there is increased consensus on core approaches, and the field’s periphery is extended to new research puzzles rather than contesting prior efforts. Collins claims this shift characterizes natural sciences, but not many social sciences, which are instead more likely to eng"
Q18-1028,N12-1073,0,0.05741,"es with the most-frequent arguments seen for each paths. Each dependency path feature value reflects the similarity of (i) the average word vector for that path’s arguments with (ii) the vector of the path’s argument in a given context, if the path is present. average similarity of an instance’s arguments with the class’s preferences for all observed syntactic relationships (i.e., how similar are the syntacticallyrelated words to the function’s preferences). Our work differs from dependency-based features from prior work that use separate features for each unique dependency path and argument (Athar and Teufel, 2012; Abu-Jbara et al., 2013); in contrast, we use a single feature for each path with distributed representation for its arguments, which allows our features to generalize to similar words that are unseen in the training data. 3.2 Experimental Setup Models All models were trained using a Random Forest classifier, which is robust to overfitting even with large numbers of features (Fern´andez-Delgado et al., 2014). After limited grid search over possible configurations,5 we set parameter values as follows. The number of random trees is 2500 and we required each leaf to match at least 5 instances. T"
Q18-1028,bird-etal-2008-acl,0,0.0719884,"UTURE class to indicate that authors have forward-looking references for how their work might be applied later; these references are important for establishing a temporal lineage between works, and as we show later in §4, are the most frequent citation type in papers’ Conclusion sections. Our adapted scheme enables us to conduct detailed analyses of the narrative structure of papers, venue citation pattern and evolution, and modeling the evolution of the whole field. 2.2 Annotation guidelines were created using a pilot study of 10 papers sampled from the ACL Anthology Reference Corpus (ARC) (Bird et al., 2008). 2 2.1 Classification Scheme Our classification captures the broad thematic functions a citation can serve in the discourse, e.g., pro1 For notational clarity, we use the term reference for the work that is cited and citation for the mention of it in the text. 392 Annotation Process and Dataset Another potential theme is citation sentiment (Athar, 2014; Kumar, 2016), but we omit this theme from our field-scale analysis because researchers have shown that negative sentiment is rare in practice (Chubin and Moitra, 1975; Vinkler, 1998; Case and Higgins, 2000) and can be quite subjective to class"
Q18-1028,W99-0629,0,0.0781822,"e rise in expected incom9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in the 1990s was an increase in reusable technologies and evaluations, like the BNC (Leech, 1992) and the Penn Treebank (Marcus et al., 1993). More broadly, our work points to the future of NLP"
Q18-1028,P97-1003,0,0.075739,"m9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in the 1990s was an increase in reusable technologies and evaluations, like the BNC (Leech, 1992) and the Penn Treebank (Marcus et al., 1993). More broadly, our work points to the future of NLP as a quickly mov"
Q18-1028,councill-etal-2008-parscit,0,0.0313905,"by two trained annotators with expertise in NLP using the Brat tool (Stenetorp et al., 2012) and were then fully adjudicated to ensure quality. Following best practices for annotating citations (Athar, 2014), annotators saw an extended context before and after the citing sentence, provided from the output of ParsCit. Annotators were instructed to skip any instances whose context was corrupted or whose citance text did not match the regular citation style for ACL venues.3 The citation scheme was applied to a random sample of 52 papers drawn from the ARC. Each paper was processed using ParsCit (Councill et al., 2008) to extract citations and their references. As expected from prior studies (Teufel et al., 2006a; Dong and 3 A small number of citation instances in our sample occurred in contexts where the surrounding text was malformed, which we attribute to being OCR errors, the citation being in the middle of a math-related context whose symbols were not converted, or where the citation occurred within a table or figure whose structure was treated as the surrounding text. In all cases, we viewed in the instance as unsuitable for use as a training example since it contained little meaningful context. These"
Q18-1028,I11-1070,0,0.0470054,"Missing"
Q18-1028,P07-1028,0,0.0195428,"ormances and results is likely to be a C OMPARE OR C ONTRAST, whereas one describing methodology is more likely to be U SES. We quantify this thematic framing by using features based on topic models, computed over the sentence containing the citation and also over the paragraph containing the citing sentence. For each type of context, a topic model is trained over 321,129 respective contexts from the ARC. Table 5 shows example topics. Prototypical Argument Features We also explored richer grammatical features, drawing on selectional preferences reflecting expectations for predicate arguments (Erk, 2007). We construct a prototype for each citation function by identifying the frequent arguments seen in different syntactic positions. For example, E XTENDS citations occur frequently as objects of verbs such as “follow” and “use”, whereas U SES citations have techniques or artifact words as dependents; Table 6 shows more examples. Each class’s selectional preferences are represented using a vector for the argument at each relation type, constructed by summing the vectors of all words appearing in it. Each function is represented as a separate feature whose value is the 395 Function M OTIVATION M"
Q18-1028,J89-4001,0,0.029991,"ortionally more non-methodological citations.9 In the second trend, authors are more likely to use and compare against the same set of papers, as shown in Figure 6 by the rise in expected incom9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in the 1990s wa"
Q18-1028,D08-1038,0,0.140784,"Missing"
Q18-1028,P14-5010,0,0.00248399,"depth of the decision tree as 10 or n, where n is the number of features; minimum leaf size in decision tree [2, . . . , 10]; number of topics [50, 100, 250, 500]; and whether to use Smote (Chawla et al., 2002). System Macro F1 This work 0.530 without topic features 0.502 without selectional prefs. 0.464 without bootstrapped pats. 0.457 without any novel features 0.474 Abu-Jbara et al. (2013) 0.410 Teufel (2000) 0.273 Dong and Sch¨afer (2011) 0.233 Majority-Class 0.092 Random 0.138 classifier is implemented using SciKit (Pedregosa et al., 2011) and syntactic processing was done using CoreNLP (Manning et al., 2014). Selectional preferences used pretrained 300-dimensional GloVe vectors from the 840B token Common Crawl (Pennington et al., 2014). The topic model features used an LDA with 100 topics. Data Annotated data is crucial for developing high accuracy for rare citation classes. Therefore, we integrate portions of the dataset of Teufel (2010),6 which has fine-grained citation function labeled for ACL-related documents using the annotation scheme of Teufel et al. (2006b). We map their 12 function classes into our six classes (see Appendix A). When combining the two datasets, we omit the data labeled w"
Q18-1028,J93-2004,0,0.0640913,"a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in the 1990s was an increase in reusable technologies and evaluations, like the BNC (Leech, 1992) and the Penn Treebank (Marcus et al., 1993). More broadly, our work points to the future of NLP as a quickly moving field of high consensus and suggests that artifacts that facilitate consensus such as shared tasks and open source research software will be necessary to continue this trend. 9 Conclusion pus annotated with citation function and by developing a state-of-the-art classifier for revealing scientific framing. In doing so, we demonstrate the importance of novel unsupervised features related to topic models and argument structure, and label all the citations for an entire field. We then show that citation framing reveals salien"
Q18-1028,D14-1162,0,0.0800752,"Missing"
Q18-1028,P83-1021,0,0.551973,"hors chose to include proportionally more non-methodological citations.9 In the second trend, authors are more likely to use and compare against the same set of papers, as shown in Figure 6 by the rise in expected incom9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in"
Q18-1028,C88-2128,0,0.308164,"ited space, authors chose to include proportionally more non-methodological citations.9 In the second trend, authors are more likely to use and compare against the same set of papers, as shown in Figure 6 by the rise in expected incom9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) th"
Q18-1028,E12-2021,0,0.105368,"Missing"
Q18-1028,W06-1312,0,0.173037,"f citation framing by first developing a state-of-the-art method for automatically classifying citation function and then applying this method to an entire field’s literature to quantify the effects and evolution of framing. Analyzing large-scale changes in citation framing requires an accurate method for classifying the function a citation plays towards furthering an argument. Due to the difficulty of interpreting citation intent, many prior works performed manual analysis (Moravcsik and Murugesan, 1975; Swales, 1990; Harwood, 2009) and only recently have automated approaches been developed (Teufel et al., 2006b; Valenzuela et al., 2015). Here, we unify core aspects of several prior citation annotation schemes (White, 2004; Ding et al., 2014; Hern´andez-Alvarez and Gomez, 2016). Using this scheme, we create 391 Transactions of the Association for Computational Linguistics, vol. 6, pp. 391–406, 2018. Action Editor: Katrin Erk. Submission batch: 8/2017; Revision batch: 12/2017; Published 6/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. one of the largest annotated corpora of citations and use it to train a high-accuracy method for automatically labeling"
Q18-1028,W06-1613,0,0.186058,"f citation framing by first developing a state-of-the-art method for automatically classifying citation function and then applying this method to an entire field’s literature to quantify the effects and evolution of framing. Analyzing large-scale changes in citation framing requires an accurate method for classifying the function a citation plays towards furthering an argument. Due to the difficulty of interpreting citation intent, many prior works performed manual analysis (Moravcsik and Murugesan, 1975; Swales, 1990; Harwood, 2009) and only recently have automated approaches been developed (Teufel et al., 2006b; Valenzuela et al., 2015). Here, we unify core aspects of several prior citation annotation schemes (White, 2004; Ding et al., 2014; Hern´andez-Alvarez and Gomez, 2016). Using this scheme, we create 391 Transactions of the Association for Computational Linguistics, vol. 6, pp. 391–406, 2018. Action Editor: Katrin Erk. Submission batch: 8/2017; Revision batch: 12/2017; Published 6/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. one of the largest annotated corpora of citations and use it to train a high-accuracy method for automatically labeling"
Q18-1028,D11-1055,0,0.0604872,"Missing"
Q18-1028,P84-1044,0,0.0986665,"enable future research. 2 A Corpus for Citation Function Citations play a key role in supporting authors’ contributions throughout a scientific paper.1 Multiple schemes have been proposed on how to classify these different roles, ranging from a handful of classes (Nanba and Okumura, 1999; Pham and Hoffmann, 2003) to twenty or more (Garfield, 1979; Garzone and Mercer, 2000). While suitable for expert manual analysis, many schemes include either fine-grained distinctions that are too rare to reliably identify or subjective classifications that require detailed knowledge of the field or author (Ziman, 1968; Swales, 1990; Harwood, 2009). Motivated by the desire to automatically examine large-scale trends in scholarly behavior, we address these issues by unifying the common aspects of multiple approaches in a single classification. viding background or serving as contrast (Oppenheim and Renn, 1978; Spiegel-R¨using, 1977; Teufel et al., 2006a; Garfield, 1979; Garzone and Mercer, 2000; Abu-Jbara et al., 2013).2 Citation function reflects the specific purpose a citation plays with respect to the current paper’s contributions. We unify the functional roles common in several classifications, e.g., (Sp"
S10-1080,P10-4006,1,0.881392,"Missing"
S10-1080,J07-2002,0,0.0827418,"Missing"
S10-1080,W97-0322,0,0.0710095,"threshold allows for the final number of clusters to be determined by data similarity instead of having to specify the number of clusters. The set of context vectors for a word are clustered using K-Means, which assigns a context to the most similar cluster centroid. If the nearest centroid has a similarity less than the cluster threshold and there are not K clusters, the context forms a new cluster. We define the similarity between contexts vectors as the cosine similarity. Once the corpus has been processed, clusters are repeatedly merged using HAC with the average link criteria, following (Pedersen and Bruce, 1997). Average link clustering defines cluster similarity as the mean cosine similarity of the pairwise similarity of all data points from each cluster. Cluster merging stops when the two most similar clusters have a similarity less than the cluster threshold. Reaching a similarity lower than the cluster threshold signifies that each cluster represents a distinct word sense. 3.2 Parameter Tuning Previous WSI evaluations provided a test corpus, a set of golden sense labels, and a scoring mechanism, which allowed models to do parameter tuning prior to providing a set of sense labels. The SemEval 2010"
S10-1080,D07-1043,0,0.0315813,"with an induced sense. Each test context is labeled with the name of the cluster whose centroid has the highest cosine similarity to the context vector. We represent the test contexts in the same method used for training; index vectors are re-used from training. 3 Evaluation and Results The WSI task evaluated the submitted solutions with two methods of experimentation: an unsupervised method and a supervised method. The unsupervised method is measured according to the VMeasure and the F-Score. The supervised method is measured using recall. 3.1 Scoring The first measure used is the V-Measure (Rosenberg and Hirschberg, 2007), which compares the 360 vors more homogenous clusters. Conversely, this configuration does poorly when measured by FScore, which tends to favor systems that generate fewer senses per word. When configured for the F-Score, HERMITF performs well; this configuration would have ranked third for the F-Score if it had been submitted. However, its performance is also due to the relatively few senses per word it generates, 1.54. The inverse performance of both optimized configurations is reflective of the contrasting nature of the two performance measures. formed a post-hoc analysis to evaluate the e"
S10-1080,S10-1011,0,\N,Missing
S12-1027,S07-1002,0,0.454688,"ber of overlapping vertices for each partition and selecting the partition with the highest overlap as the sense of w. We extend this to graded annotation by selecting all partitions with at least one vertex present and set the applicability equal to the degree of overlap. 4 Evaluation Across Sense Inventories Directly comparing GWS annotations from the induced and gold standard sense inventories requires first creating a mapping from the induced senses to the gold standard inventory. Agirre et al. (2006) propose a sense-mapping procedure, which was used in the previous two SemEval WSI Tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). We consider this procedure and two extensions of it to support learning a mapping from graded sense annotations. The procedure of Agirre et al. (2006) uses three corpora: (1) a base corpus from which the senses are derived, (2) a mapping corpus annotated with both gold standard senses, denoted gs, and induced senses, denoted is, and (3) a test corpus annotated with is senses that will be converted to gs senses. Once the senses are induced from the base corpus, the mapping corpus is annotated with is senses and a matrix M is built where cell i, j initially contains th"
S12-1027,W06-3814,0,0.0546998,"Missing"
S12-1027,D09-1056,0,0.107163,"Missing"
S12-1027,W06-3812,0,0.0261978,"reference corpus in which it is present. A pair (wi , wj ) is retained only if its set of contexts is dissimilar to the sets of contexts of both its member terms, using the Dice coefficient to measure the similarity of the sets. Pairs with a Dice coefficient above P4 with either of its constituent terms are removed. Last, edges are added between nouns and noun pairs according to their conditional probabilities of occurring with each other. Edges with a conditional probability less than P3 are not included. Once the graph has been constructed, the Chinese Whispers graph partitioning algorithm (Biemann, 2006) is used to identify word senses. Each graph partition is assigned a separate sense of w. Next, each partition is mapped to the set of contexts in the reference corpus in which at least one of its vertices occurs. Partitions whose context sets are a strict subset of another are merged with the subsuming partition. Word sense disambiguation occurs by counting the number of overlapping vertices for each partition and selecting the partition with the highest overlap as the sense of w. We extend this to graded annotation by selecting all partitions with at least one vertex present and set the appl"
S12-1027,W02-0805,0,0.0408319,", etc.) 4. succeed, win, come through, bring home the bacon, deliver the goods (attain success or reach a desired goal) 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense In this context, many annotators would agree that the athlete has both won an object (the gold metal itself) and won a competition (signified by the gold medal). Although contexts can be constructed to elicit only one of these senses, in the example above, a graded annotation best matches human perception. Graded word sense (GWS) annotation offers significant advantages for sense annotation with a finegrained sense inventory. However, creating a sufficiently"
S12-1027,J93-1003,0,0.173736,"duced, communities with three or fewer vertices are removed, under the assumption that these communities contain too few features to reliably disambiguate. Senses are disambiguated by finding the community with the largest overlap score, computed as the weighted Jaccard Index. For a context with the set of features Fi and a community with features Fj , the |F ∩F | overlap is measured as |Fj |· |Fii ∪Fjj |. We adapt this algorithm in three ways. First, rather than use co-occurrence frequency to weight edges between terms, we weight edges accord to their statistical association with the G-test (Dunning, 1993). The G-test weighting helps remove edges whose large edge weights are due to high corpus frequency but provide no disambiguating information, and the weighting also allows the τ parameter to be more consistently set across corpora of different sizes. Second, while Jurgens (2011) used only nouns as vertices in the graph, we include both verbs and adjectives due to needing to identify senses for both. Third, for graded senses, we disambiguate a context by reporting all overlapping communities, weighted by their overlap score. UoY Korkontzelos and Manandhar (2010) propose a WSI model that builds"
S12-1027,D09-1046,0,0.692171,"e evaluation measure alone is appropriate for assessing GWS annotation capability. Therefore, we propose three objectives for the evaluating the sense labeling: (1) Detection of which senses are present, (2) Ranking senses according to applicability, and (3) Perception of the graded presence of each sense. We separate the three objectives as a way to evaluate how well different techniques perform on each aspect individually, which may encourage future work in ensemble WSD methods that use combinations of the techniques. Figure 1 illustrates each evaluation on example annotations. We note that Erk and McCarthy (2009) have also proposed an alternate set of evaluation measures for GWS annotations. Where applicable, we describe and compare their measures 190 to ours for the three objectives. i refer to the set In the following definitions, let SG of senses {s1 , . . . , sn } present in context i according to the gold standard, and similarly, let SLi refer to the set of senses for context i as labeled by a WSD system using the same sense inventory. Let peri (sj ) refer to the perceived numeric applicability rating of sense sj in context i. Detection measures the ability to accurately identify which senses are"
S12-1027,P09-1002,0,0.0732112,"the goods (attain success or reach a desired goal) 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense In this context, many annotators would agree that the athlete has both won an object (the gold metal itself) and won a competition (signified by the gold medal). Although contexts can be constructed to elicit only one of these senses, in the example above, a graded annotation best matches human perception. Graded word sense (GWS) annotation offers significant advantages for sense annotation with a finegrained sense inventory. However, creating a sufficiently large annotated corpus for training supervised GWS disambiguation"
S12-1027,P10-4006,1,0.815253,"Missing"
S12-1027,W11-1104,1,0.779532,"ering and the omission of a sense. Indeed, in cases where the set of senses in a test annotation is completely disjoint from the set of gold standard senses, the JSS will be positive due to comparing the two distributions against their average; In contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the Perception objective. JSD(G||L) = 3 WSI Models For evaluation we adapt three recent graph-based WSI methods for the task of graded-sense annotation: Navigli and Crisafulli (2010), referred to as Squares, Jurgens (2011), referred to as Link, and UoY (Korkontzelos and Manandhar, 2010). At an abstract level, these methods operate in two stages. First, a graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices. Second, senses are derived by clustering or partitioning the graph. We selected these methods based on their superior performance on recent benchmarks and also 1 The JSD is a distance measure in [0, 1], which we convert to a similarity JSS = 1 − JSD for easier comparison. for their significant differences in approach. Followi"
S12-1027,S10-1079,0,0.275714,"cases where the set of senses in a test annotation is completely disjoint from the set of gold standard senses, the JSS will be positive due to comparing the two distributions against their average; In contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the Perception objective. JSD(G||L) = 3 WSI Models For evaluation we adapt three recent graph-based WSI methods for the task of graded-sense annotation: Navigli and Crisafulli (2010), referred to as Squares, Jurgens (2011), referred to as Link, and UoY (Korkontzelos and Manandhar, 2010). At an abstract level, these methods operate in two stages. First, a graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices. Second, senses are derived by clustering or partitioning the graph. We selected these methods based on their superior performance on recent benchmarks and also 1 The JSD is a distance measure in [0, 1], which we convert to a similarity JSS = 1 − JSD for easier comparison. for their significant differences in approach. Following, we briefly summarize each method to highlight its key paramet"
S12-1027,E12-1060,0,0.0834163,"ns for 11 terms. 189 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 189–198, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Therefore, we consider the use of Word Sense Induction (WSI) for GWS annotation. WSI removes the need for substantial training data by automatically deriving a word’s senses and associated sense features through examining its contextual uses. Furthermore, the data-driven sense discovery defines senses as they are present in the corpus, which may identify usages not present in traditional sense inventories (Lau et al., 2012). Last, many WSI models represent senses loosely as abstractions over usages, which potentially may transfer well to expressing GWS annotations as a blend of their sense usages. In this paper, we consider the performance of WSI models on a GWS task. The contributions of this paper are as follows. First, in Sec. 2, we motivate three GWS annotation objectives and propose corresponding measures that provide fine-grained analysis of the capabilities of different WSI models. Second, in Sec. 4, we propose two new sense mapping procedures for converting an induced sense inventory to a reference sense"
S12-1027,S10-1011,0,0.327751,"ces for each partition and selecting the partition with the highest overlap as the sense of w. We extend this to graded annotation by selecting all partitions with at least one vertex present and set the applicability equal to the degree of overlap. 4 Evaluation Across Sense Inventories Directly comparing GWS annotations from the induced and gold standard sense inventories requires first creating a mapping from the induced senses to the gold standard inventory. Agirre et al. (2006) propose a sense-mapping procedure, which was used in the previous two SemEval WSI Tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). We consider this procedure and two extensions of it to support learning a mapping from graded sense annotations. The procedure of Agirre et al. (2006) uses three corpora: (1) a base corpus from which the senses are derived, (2) a mapping corpus annotated with both gold standard senses, denoted gs, and induced senses, denoted is, and (3) a test corpus annotated with is senses that will be converted to gs senses. Once the senses are induced from the base corpus, the mapping corpus is annotated with is senses and a matrix M is built where cell i, j initially contains the counts of each time gsj"
S12-1027,W04-0807,0,0.0735032,"a provided by Erk et al. (2009). Here, three annotators rated the applicability of all WordNet 3.0 senses of a word in a single sentence context. Ratings were done using a 5-point ordinal ranking according to the judgements from 1 – this sense is not applicable to 5 – this usage exactly reflects this sense. Annotators used a wide-range of responses, leading to many applicable senses per instance. We selected the subset of the GWS dataset where each term has 50 annotated contexts, which were distributed evenly between SemCor (Miller et al., 1993) and the SENSEVAL-3 lexical substitution corpus (Mihalcea et al., 2004). Table 1 summarizes the target terms in this context. To prepare the data for evaluation, we constructed the gold standard GWS annotations using the mean applicability ratings of all three annotators for each context. Senses that received a mean rating of 1 (not applicable) were not listed in gold standard labeling for that instance. All remaining responses were normalized to sum to 1. 194 Model Configuration For consistency, all three WSI models were trained using the same reference corpus. We used a 2009 snapshot of Wikipedia,2 which was PoS tagged and lemmatized using the TreeTagger (Schmi"
S12-1027,H93-1061,0,0.0587825,"old standard GWS annotations are derived from a subset of the GWS data provided by Erk et al. (2009). Here, three annotators rated the applicability of all WordNet 3.0 senses of a word in a single sentence context. Ratings were done using a 5-point ordinal ranking according to the judgements from 1 – this sense is not applicable to 5 – this usage exactly reflects this sense. Annotators used a wide-range of responses, leading to many applicable senses per instance. We selected the subset of the GWS dataset where each term has 50 annotated contexts, which were distributed evenly between SemCor (Miller et al., 1993) and the SENSEVAL-3 lexical substitution corpus (Mihalcea et al., 2004). Table 1 summarizes the target terms in this context. To prepare the data for evaluation, we constructed the gold standard GWS annotations using the mean applicability ratings of all three annotators for each context. Senses that received a mean rating of 1 (not applicable) were not listed in gold standard labeling for that instance. All remaining responses were normalized to sum to 1. 194 Model Configuration For consistency, all three WSI models were trained using the same reference corpus. We used a 2009 snapshot of Wiki"
S12-1027,D10-1012,0,0.0950178,"1 of 0.593, despite its significant differences in ordering and the omission of a sense. Indeed, in cases where the set of senses in a test annotation is completely disjoint from the set of gold standard senses, the JSS will be positive due to comparing the two distributions against their average; In contrast, the cosine similarity in such cases will be zero, which we argue better matches the expectation that such an annotation does not meet the Perception objective. JSD(G||L) = 3 WSI Models For evaluation we adapt three recent graph-based WSI methods for the task of graded-sense annotation: Navigli and Crisafulli (2010), referred to as Squares, Jurgens (2011), referred to as Link, and UoY (Korkontzelos and Manandhar, 2010). At an abstract level, these methods operate in two stages. First, a graph is built, using either words or word pairs as vertices, and edges are added denoting some form of association between the vertices. Second, senses are derived by clustering or partitioning the graph. We selected these methods based on their superior performance on recent benchmarks and also 1 The JSD is a distance measure in [0, 1], which we convert to a similarity JSS = 1 − JSD for easier comparison. for their sign"
S12-1027,passonneau-etal-2010-word,0,0.0225089,"in, advance, win, pull ahead, make headway, get ahead, gain ground (obtain advantages, such as points, etc.) 4. succeed, win, come through, bring home the bacon, deliver the goods (attain success or reach a desired goal) 1 Introduction Word Sense Disambiguation (WSD) aims to identify the sense of a word in a given context, using a predefined sense inventory containing the word’s different meanings (Navigli, 2009). Traditionally, WSD approaches have assumed that each occurrence of a word is best labeled with a single sense. However, human annotators often disagree about which sense is present (Passonneau et al., 2010), especially in cases where some of the possible senses are closely related (Chugur et al., 2002; McCarthy, 2006; Palmer et al., 2007). Recently, Erk et al. (2009) have shown that in cases of sense ambiguity, a graded notion of sense In this context, many annotators would agree that the athlete has both won an object (the gold metal itself) and won a competition (signified by the gold medal). Although contexts can be constructed to elicit only one of these senses, in the example above, a graded annotation best matches human perception. Graded word sense (GWS) annotation offers significant adva"
S12-1027,D07-1043,0,0.0556735,"First, our study only considered three graphbased WSI models; future work is needed to assess the capabilities other WSI approaches, such as vector-based or Bayesian. We are also interested in comparing the performance of the Link model with other recently developed all-words WSI approaches such as Van de Cruys and Apidianaki (2011). Second, the proposed evaluation relies on a supervised mapping to the gold standard sense inventory, which has potential to lose information and incorrectly map new senses not in the gold standard. While unsupervised clustering evaluations such as the V-measure (Rosenberg and Hirschberg, 2007) and paired Fscore (Artiles et al., 2009) are capable of evaluating without such a mapping, future work is needed to test extrinsic soft clustering evaluations such as BCubed (Amig´o et al., 2009) or develop analogous techniques that take into account graded class membership used in GWS annotations. Last, we note that our setup normalized the GWS ratings into probability distribution, which is standard in the SemEval evaluation setup. However, this normalization incorrectly transforms GWS annotations where no predominant sense was rated at the highest value, e.g., an annotation of only two sen"
S12-1027,P11-1148,0,0.0220177,"Missing"
S12-1027,W06-2503,0,\N,Missing
S12-1047,W09-2416,0,0.0120009,"Missing"
S12-1047,J90-1003,0,0.250057,"d Pedersen V0 V1 V2 WordNet is used to build the set of concepts connected by WordNet relations to the pairs’ words. Prototypicality is estimated using the vector similarity of the concatenated glosses. Same procedure as V0, with one further expansion to related concepts. Same procedure as V0, with two further expansions to related concepts. Table 2: Descriptions of the participating teams and systems. as the most illustrative and the least associated as the least illustrative. Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words. For this baseline, the prototypicality rating given to a word pair is simply the PMI score for the pair. For  x and y,  two terms p(x,y) PMI(x, y) is defined as log2 p(x)p(y) where p(·) denotes the probability of a term or pair of terms. The PMI score was calculated using the method of Turney (2001) on a corpus of approximately 50 billion tokens, indexed by the Wumpus search engine.4 To calculate p(x, y), we recorded all co-occurrences of both terms within a ten-word window. 5 Systems Three teams submitted six systems for evalua"
S12-1047,S07-1003,1,0.712268,"Missing"
S12-1047,S10-1006,0,0.134072,"Missing"
S12-1047,W10-0204,1,0.0803248,". Give four additional word pairs that illustrate the same relation, in the same order (X on the left, Y on the right). Please do not use phrases composed of two or more words in your examples (e.g., “racing car”). Please do not use names of people, places, or things in your examples (e.g., “Europe”, “Kleenex”). (1) (2) (3) (4) : : : : Figure 1: An example of the two questions for Phase 1. annotations needed, we used Amazon Mechanical Turk (MTurk),2 which is a popular choice in computational linguistics for gathering large numbers of human responses to linguistic questions (Snow et al., 2008; Mohammad and Turney, 2010). We refer to the MTurk workers as Turkers. The data set was built in two phases. In the first phase, Turkers were given three paradigmatic examples of a subcategory and asked to create new pairs that instantiate the same relation as the paradigms. In the second phase, people were asked to distinguish the new pairs from the first phase according to the degree to which they are good representatives of the given subcategory. Phase 1 In the first phase, we built upon the paradigmatic examples of Bejar et al. (1991), who provided one to ten examples for each subcategory. From these examples, we ma"
S12-1047,P08-1052,0,0.00730346,"om the paradigmatic examples and identify what relational or featural attributes best characterize that relation, and (2) to identify the relation of the given pair and rate how similar it is to that shared by the paradigmatic examples. 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE - PURPOSE and SPACE - TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE - PURPOSE includes CAUSE:EFFECT and ACTION :GOAL), for a total of 79 distinct subcategories. Although these c"
S12-1047,P06-1015,0,0.00741066,"shared by the paradigmatic examples. 2.2 Relation Categories Researchers in psychology and linguistics have considered many different categorizations of semantic relations. The particular relation categorization is often driven by both the type of data and the intended application. Nastase and Szpakowicz (2003) propose a two-level hierarchy for noun-modifier relations, which has been widely used (Nakov and Hearst, 2008; Nastase et al., 2006; Turney and Littman, 2005; Turney, 2005). Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al., 2001). We adopt the relation classification scheme of Bejar et al. (1991), which includes ten high-level categories (e.g., CAUSE - PURPOSE and SPACE - TIME). Each category has between five and ten more refined subcategories (e.g., CAUSE - PURPOSE includes CAUSE:EFFECT and ACTION :GOAL), for a total of 79 distinct subcategories. Although these categories do not reflect all possible semantic relations, they greatly expand the coverage of relation types from those used in past relation-based SemEval tasks (Girju et al., 2007; Hendrickx et al., 2010),"
S12-1047,D08-1027,0,0.0383199,"Missing"
S12-1047,J06-3003,1,0.643254,"t variability in how characteristic they are of that class. We present a new SemEval task based on identifying the degree of prototypicality for instances within a given class. As a part of the task, we have assembled the first dataset of graded relational similarity ratings across 79 relation categories. Three teams submitted six systems, which were evaluated using two methods. 1 Introduction Relational similarity measures the degree of correspondence between two relations, where instance pairs that have high relational similarity are said to be analogous, i.e., to express the same relation (Turney, 2006). However, a class of analogous relations may still have significant variability in the degree of relational similarity of its members. Consider the four word pairs dog:bark, cat:meow, floor:squeak, and car:honk. We could say that these four X:Y pairs are all instances of the semantic relation EN TITY :SOUND ; that is, X is an entity that characteristically makes the sound Y . Within a class of analogous pairs, certain pairs are more characteristic of the relation. For example, many would agree that dog:bark and cat:meow are better prototypes of the ENTITY :SOUND relation than floor:squeak. Ou"
S12-1047,S10-1007,0,\N,Missing
S13-2040,E09-1005,0,0.281923,"ts parameters from the trial data, while the BN2 and WN1 systems are completely unsupervised and optimize their parameters directly from the structure of the BabelNet and WordNet graphs. UMCC-DLSI UMCC-DLSI submitted three systems based on the ISR-WN resource (Guti´errez et al., 2011), which enriches the WordNet semantic network using edges from multiple lexical resources, such as WordNet Domains and the eXtended WordNet. WSD was then performed using the ISR-WN network in combination with the algorithm of Guti´errez (2012), which is an extension of the Personalized PageRank algorithm for WSD (Agirre and Soroa, 2009) which includes senses frequency. The algorithm requires initializing the PageRank algorithm with a set of seed synsets (vertices) in the network; this initialization represents the key variation among UMCC’s three approaches. The RUN -1 system performs WSD using all noun instances from the sentence context. In contrast, the RUN -2 works at the discourse level and initializes the PageRank using the synsets of all Team System English French German Italian Spanish DAEBAK! GETALP GETALP UMCC-DLSI UMCC-DLSI UMCC-DLSI PD BN-1 BN-2 RUN -1 RUN -2 RUN -3 0.604 0.263 0.266 0.677 0.685 0.680 0.538 0.261"
S13-2040,S01-1001,0,0.86219,"pating systems, and discuss future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010"
S13-2040,D09-1046,0,0.0512076,"Missing"
S13-2040,S13-2049,1,0.473907,"Missing"
S13-2040,S10-1003,0,0.0606125,"focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has en"
S13-2040,W04-0807,0,0.170117,"troduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation"
S13-2040,W09-2412,0,0.0262841,"Missing"
S13-2040,D12-1128,1,0.887659,"Missing"
S13-2040,P12-3012,1,0.854178,"Missing"
S13-2040,S07-1006,1,0.783276,"d limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and"
S13-2040,S07-1016,0,0.745774,"en more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution ("
S13-2040,C12-1146,0,0.0336153,"DAEBAK! submitted one system called PD (Peripheral Diversity) based on BabelNet path indices from the BabelNet synset graph. Using a ±5 sentence window around the target word, a graph is constructed for all senses of co-occurring lemmas following the procedure proposed by Navigli and Lapata (2010). The final sense is selected based on measuring connectivity to the synsets of neighboring lemmas. The MFS is used as a backoff strategy when no appropriate sense can be picked out. GETALP GETALP submitted three systems, two for BabelNet and one for WordNet, all based on the ant-colony algorithm of (Schwab et al., 2012), which uses the sense inventory network structure to identify paths connecting synsets of the target lemma to the synsets of other lemmas in context. The algorithm requires setting several parameters for the weighting of the structure of the contextbased graph, which vary across the three systems. The BN1 system optimizes its parameters from the trial data, while the BN2 and WN1 systems are completely unsupervised and optimize their parameters directly from the structure of the BabelNet and WordNet graphs. UMCC-DLSI UMCC-DLSI submitted three systems based on the ISR-WN resource (Guti´errez et"
S13-2040,W04-0811,0,0.892093,"s future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Wo"
S13-2040,P95-1026,0,0.301495,". Finally, the RUN -3 system initializes using all words in the sentence. UMCC-DLSI Run-2 0.9 5 0.8 Results and Discussion 0.7 227 0.6 WSD F1 All teams submitted at least one system using the BabelNet inventory, shown in Table 3. The UMCCDLSI systems were consistently able to outperform the MFS baseline (a notoriously hard-to-beat heuristic) in all languages except German. Additionally, the DAEBAK! system outperformed the MFS baseline on French and Italian. The UMCC-DLSI RUN 2 system performed the best for all languages. Notably, this system leverages the single-sense per discourse heuristic (Yarowsky, 1995), which uses the same sense label for all occurrences of a lemma in a document. UMCC-DLSI submitted the only three systems to use Wikipedia-based senses. Table 4 shows their performance. Of the three sense inventories, Wikipedia had the most competitive MFS baseline, scoring at least 0.694 on all languages. Notably, the Wikipedia-based system has the lowest recall of all systems. Despite having superior precision to the MFS baseline, the low recall brought the resulting F1 measure below the MFS. Two teams submitted four total systems for WordNet, shown in Table 5. The UMCC-DLSI RUN -2 system w"
S13-2040,W09-2413,0,\N,Missing
S13-2040,S10-1002,0,\N,Missing
S13-2049,S07-1002,0,0.0320264,"procedure. We follow the 80/20 setup of Manandhar et al. (2010), where the corpus is randomly divided into five partitions, four of which are used to learn the sense mapping; the sense labels for the held-out partition are then converted and compared with the gold standard. This process is repeated so that each partition is tested once. For learning the sense mapping function, we use the distribution mapping technique of Jurgens (2012), which takes into account the sense applicability weights in both labelings. We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). The first evaluation uses a traditional WSD task that directly compares WordNet sense labels. For WSI systems, their induced sense labels are converted to WordNet 3.1 labels via a mapping procedure. The second evaluation performs a direct comparison of the two sense inventories using clustering comparisons. Given two sets of sense labels for an instance, X and Y , the Jaccard Index is used to measure the | agreement: |X∩Y |X∪Y |. The Jaccard Index is maximized when X and Y use identical labels, and is minimized when the sets of sense labels are disjoint. 3.1 3.1.3 WS"
S13-2049,E09-1005,0,0.0431169,"valuation measures for all system and selected baselines. Top system performances are marked in bold. like other teams, the Unimelb systems were trained on a Wikipedia corpus instead of the ukWaC corpus. The University of Sussex (UoS) team submitted two WSI systems that use dependency-parsed features from the corpus, which are then clustered into senses using the MaxMax algorithm (Hope and Keller, 2013); the resulting fine-grained clusters are then combined based on their degree of separability. The La Sapienza team submitted two Unsupervised WSD systems based applying Personalized Page Rank (Agirre and Soroa, 2009) over a WordNet-based network to compare the similarity of each sense with the similarity of the context, ranking each sense according to its similarity. 5 Results and Discussion Table 3 shows the main results for all instances. Additionally, we report the number of induced clusters used to label each sense as #Cl and the number of resulting WordNet 3.1 senses for each sense with #S. As in previous WSD tasks, the MFS baseline was quite competitive, outperforming all systems on detecting which senses were applicable, measured using the Jaccard Index. However, most systems were able to outperfor"
S13-2049,S10-1013,0,0.0216417,"Missing"
S13-2049,D09-1056,0,0.114515,"Missing"
S13-2049,J08-4004,0,0.0286358,"Missing"
S13-2049,D09-1046,0,0.0262003,"Missing"
S13-2049,P09-1002,0,0.0271531,"ord Sense Disambiguation (WSD) attempts to identify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense invento"
S13-2049,ide-suderman-2004-american,0,0.0272517,"was labeled with WordNet 3.0 sense ratings from three untrained lexicographers. Due to the unsupervised nature of the task, participants were not provided with sense-labeled training data. However, WSI systems were provided with the ukWaC corpus (Baroni et al., 2009) to use in inducing senses. Previous SemEval WSI tasks had provided participants with corpora specific to the task’s target terms; in contrast, this task opted to use a large corpus to enable WSI methods that require corpuswide statistics, e.g., statistical associations. Test data was drawn from the Open American National Corpus (Ide and Suderman, 2004, OANC) across a variety of genres and from both the spoken and written portions of the corpus, summarized in Table 2. All contexts were manually inspected to ensure that the lemma being disambiguated was of the correct part of speech and had an interpretation that 291 matched at least one WordNet 3.1 sense. This filtering also removed instances that were in a collocation, or had an idiomatic meaning. Ultimately, 4664 contexts were used as test data, with a minimum of 22 and a maximum of 100 contexts per word. 2.2 Sense Annotation Recent work proposes to gather sense annotations using crowdsou"
S13-2049,S12-1027,1,0.924775,"call. 3.1.1 Transforming Induced Sense Labels In the WSD setting, induced sense labels may be transformed into a reference inventory (e.g., WordNet 3.1) using a sense mapping procedure. We follow the 80/20 setup of Manandhar et al. (2010), where the corpus is randomly divided into five partitions, four of which are used to learn the sense mapping; the sense labels for the held-out partition are then converted and compared with the gold standard. This process is repeated so that each partition is tested once. For learning the sense mapping function, we use the distribution mapping technique of Jurgens (2012), which takes into account the sense applicability weights in both labelings. We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). The first evaluation uses a traditional WSD task that directly compares WordNet sense labels. For WSI systems, their induced sense labels are converted to WordNet 3.1 labels via a mapping procedure. The second evaluation performs a direct comparison of the two sense inventories using clustering comparisons. Given two sets of sense labels for an instance, X and Y , the Jaccard Index is us"
S13-2049,N13-1062,1,0.7829,"dentify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient sense-annotated"
S13-2049,E12-1060,0,0.110794,"tories. AI-KU submitted three WSI systems based on a lexical substitution method; a language model is built from the target word’s contexts in the test data and the ukWaC corpus and then Fastsubs (Yuret, 2012) is used to identify lexical substitutes for the target. Together, the contexts of the target and substitutes are used to build a distributional model using the S-CODE algorithm (Maron et al., 2010). The resulting contextual distributions are then clustered using K-means to identify word senses. The University of Melbourne (Unimelb) team submitted two WSI systems based on the approach of Lau et al. (2012). Their systems use a Hierarchical Dirichlet Process (Teh et al., 2006) to automatically infer the number of senses from contextual and positional features. UnWSD F1 Team System AI-KU AI-KU AI-KU Unimelb Unimelb UoS UoS La Sapienza La Sapienza Base add1000 remove5-add1000 5p 50k #WN Senses top-3 system-1 system-2 All-instances, One sense 1c1inst Semcor MFS Semcor Ranked Senses Cluster Comparison Jac. Ind. Kδsim WNDCG Fuzzy NMI Fuzzy B-Cubed #Cl #S 0.197 0.197 0.244 0.218 0.213 0.192 0.232 0.149 0.149 0.620 0.606 0.642 0.614 0.620 0.596 0.625 0.507 0.510 0.387 0.215 0.332 0.365 0.371 0.315 0.37"
S13-2049,S10-1011,1,0.757746,"e Gain, respectively. Each measure is bounded in [0, 1], where 1 indicates complete agreement with the gold standard. We generalize the traditional definition of WSD Recall such that it measures the average score for each measure across all instances, including those not labeled by the system. Systems are ultimately scored using the F1 measure between each objective’s measure and Recall. 3.1.1 Transforming Induced Sense Labels In the WSD setting, induced sense labels may be transformed into a reference inventory (e.g., WordNet 3.1) using a sense mapping procedure. We follow the 80/20 setup of Manandhar et al. (2010), where the corpus is randomly divided into five partitions, four of which are used to learn the sense mapping; the sense labels for the held-out partition are then converted and compared with the gold standard. This process is repeated so that each partition is tested once. For learning the sense mapping function, we use the distribution mapping technique of Jurgens (2012), which takes into account the sense applicability weights in both labelings. We adopt a two-part evaluation setting used in previous SemEval WSI and WSD tasks (Agirre and Soroa, 2007; Manandhar et al., 2010). The first eval"
S13-2049,W04-0807,0,0.0556944,"weighted sense. In this single-sense setting, systems were evaluated using the standard WSD Precision and Recall measures; we report the F1 measure of Precision and Recall. The remaining subset of instances annotated with multiple senses were evaluated separately. Table 4 shows the systems’ performance on single-sense instances, revealing substantially increased performance and improvement over the MFS baseline for WSI systems. Notably, the performance of the best sense-remapped WSI systems surpasses the performance of many supervised WSD systems in previous WSD evaluations (Kilgarriff, 2002; Mihalcea et al., 2004; Pradhan et al., 2007; Agirre et al., 2010). This performance suggests that WSI systems using graded labels provide a way to leverage huge amounts of unannotated corpus data for finding sense-related features in order to train semi-supervised WSD systems. Table 5 shows the performance on the subset of instances that were annotated with multiple senses. We note that in this setting, the mapping procedure transforms the All-Instances One Sense baseline into the average applicability rating for each sense in the test corpus. Notably, the La Sapienza systems sees a significant performance increas"
S13-2049,S13-2040,1,0.369894,"f a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient sense-annotated data to build WSD syste"
S13-2049,passonneau-etal-2006-inter,0,0.0169681,"r interval data, where 1 indicates perfect agreement and -1 indicates systematic disagreement; two random annotations have an expected α of zero. We treat each sense and instance combination as a separate item to rate. The total IAA for the dataset was 0.504, and on individual words, ranged from 0.903 for number.n to 0.00 for win.v. While this IAA is less than the 0.8 recommended by Krippendorff (2004), it is consistent with the IAA distribution for the sense annotations of MASC on other parts of the OANC corpus: Passonneau et al. (2012a) reports an α of 0.88 to -0.02 with the MASI statistic (Passonneau et al., 2006). Table 2 summarizes the annotation statistics for the Task’s data. The annotation process resulted in far fewer senses per instance in the trial data, which we attribute to using trained annotators. An analysis across the corpora genres showed that the multiplesense annotation rates were similar. Due to the variety of contextual sources, all lemmas were observed with at least two distinct senses. 3 Evaluation positionally-weighted Kendall’s τ similarity, and (3) a weighted variant of Normalized Discounted Cumulative Gain, respectively. Each measure is bounded in [0, 1], where 1 indicates comp"
S13-2049,passonneau-etal-2012-masc,0,0.124151,"uation (WSD) attempts to identify which of a word’s meanings applies in a given context. A long-standing task, WSD is fundamental to many NLP applications (Navigli, 2009). Typically, each usage of a word is treated as expressing only a single sense. However, contextual ambiguity as well as the relatedness of certain meanings can potentially elicit multiple sense interpretations. Recent work has shown that annotators find multiple applicable senses in a given target word context when using fine-grained sense inventories such as WordNet (V´eronis, 1998; Murray and Green, 2004; Erk et al., 2009; Passonneau et al., 2012b; Jurgens, 2013; Navigli et al., 2013). Such contexts would be better annotated with multiple sense labels, weighting each sense according to its applicability (Erk et al., 2009; Jurgens, 2013), in effect allowing ambiguity or multiple interpretations to be explicitly modeled. Accordingly, the first goal of this task is to evaluate WSD systems in a setting where instances may be labeled with one or more senses, weighted by their applicability. WSD methods are ultimately defined and potentially restricted by their choice in sense inventory; for example, a sense inventory may have insufficient"
S13-2049,S07-1016,0,0.0116608,"single-sense setting, systems were evaluated using the standard WSD Precision and Recall measures; we report the F1 measure of Precision and Recall. The remaining subset of instances annotated with multiple senses were evaluated separately. Table 4 shows the systems’ performance on single-sense instances, revealing substantially increased performance and improvement over the MFS baseline for WSI systems. Notably, the performance of the best sense-remapped WSI systems surpasses the performance of many supervised WSD systems in previous WSD evaluations (Kilgarriff, 2002; Mihalcea et al., 2004; Pradhan et al., 2007; Agirre et al., 2010). This performance suggests that WSI systems using graded labels provide a way to leverage huge amounts of unannotated corpus data for finding sense-related features in order to train semi-supervised WSD systems. Table 5 shows the performance on the subset of instances that were annotated with multiple senses. We note that in this setting, the mapping procedure transforms the All-Instances One Sense baseline into the average applicability rating for each sense in the test corpus. Notably, the La Sapienza systems sees a significant performance increase in this setting; the"
S13-2049,D07-1043,0,0.0295695,"G. We refer to this final normalized measure as the Weighted Normalized Discounted Cumulative Gain (WNDCG). 3.2 Sense Cluster Comparisons Sense induction can be viewed as an unsupervised clustering task where usages of a word are grouped into clusters, each representing uses of the same meaning. In previous SemEval tasks on sense induction, instances were labeled with a single sense, which yields a partition over the instances into disjoint sets. The proposed partition can then be compared with a gold-standard partition using many existing clustering comparison methods, such as the V-Measure (Rosenberg and Hirschberg, 2007) or paired FScore (Artiles et al., 2009). Such cluster comparison methods measure the degree of similarity between the sense boundaries created by lexicographers and those created by WSI methods. In the present task, instances are potentially labeled both with multiple senses and with weights reflecting the applicability. This type of sense labeling produces a fuzzy clustering: An instance may belong to one or more sense clusters with its cluster membership relative to its weight for that sense. Formally, we refer to (1) a solution where the sets of instances overlap as a cover and (2) a solut"
S13-2049,rumshisky-etal-2012-word,0,0.0439589,"Missing"
S13-2049,S01-1004,0,\N,Missing
S13-2049,W09-2420,0,\N,Missing
S13-2049,L14-1000,0,\N,Missing
S14-2003,S12-1051,0,0.573556,"alone. 1 Introduction Given two linguistic items, semantic similarity measures the degree to which the two items have the same meaning. Semantic similarity is an essential component of many applications in Natural Language Processing (NLP), and similarity measurements between all types of text as well as between word senses lend themselves to a variety of NLP tasks such as information retrieval (Hliaoutakis et al., 2006) or paraphrasing (Glickman and Dagan, 2003). Semantic similarity evaluations have largely focused on comparing similar types of lexical items. Most recently, tasks in SemEval (Agirre et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Nav"
S14-2003,S14-2001,0,0.0683247,"Missing"
S14-2003,J08-4004,0,0.0124731,"e average similarity value of non-OOV items. Baseline scores were made public after the evaluation period ended. Because LCS is a simple procedure, a second baseline based on Greedy String Tiling (GST) (Wise, 1996) was added after the evaluation period concluded. Unlike LCS, GST better handles the transpositions of tokens across the two texts and can still report high similarity when encountering reordered text. The minimum match length for GST was set to 6. inter-annotator correlations of 0.377–0.832. However, we note that Pearson correlation and Krippendorff’s α are not directly comparable (Artstein and Poesio, 2008), as annotators’ scores may be correlated, but completely disagree. Second, the two-phase construction process produced values that were evenly distributed across the rating scale, shown in Figure 1 as the distribution of the values for all data sets. However, we note that this creation procedure was very resource intensive and, therefore, semi-automated or crowdsourcing-based approaches for producing high-quality data will be needed to expand the size of the data in future CLSS-based evaluations. Nevertheless, as a pilot task, the manual effort was essential for ensuring a rigorouslyconstruct"
S14-2003,W13-3806,0,0.0852038,"se 1 were rated for their similarity according to the scale described in Section 2.2. An initial pilot study showed that crowdsourcing was only moderately effective for producing these ratings with high agreement. Furthermore, the texts used in Task 3 came from a variety of genres, such as scientific domains, which some workers had difficulty understanding. While we note that crowdsourcing has been used in prior STS tasks for generating similarity scores (Agirre et al., 2012; Agirre et al., 2013), both tasks’ efforts encountered lower worker score correlations on some portions of the dataset (Diab, 2013), suggesting that crowdsourcing may not be reliable for judging the similarity of certain types of text. See Section 3.5 for additional details. Therefore, to ensure high quality, the first two organizers rated all items independently. Because the sentence-to-phrase and phrase-to-word comparisons contain slang and idiomatic language, a third American English mother tongue annotator was added for those data sets. The third annotator was compensated e250 for their assistance. Annotators were allowed to make finer-grained distinctions in similarity using multiples of 0.25. For all items, when any"
S14-2003,P06-1014,1,0.050394,"Missing"
S14-2003,N13-1092,0,0.0642978,"Missing"
S14-2003,J14-4005,1,0.692332,"re et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Navigli, 2014b), and modeling the semantics of multi-word expressions (Marelli et al., 2014) or polysemous words (Pilehvar and Navigli, 2014a). Task 3 was designed with three main objectives. First, the task should include multiple types of comparison in order to assess each type’s difficulty and whether specialized resources are needed for each. Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types. Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text- and sense"
S14-2003,P14-1044,1,0.162584,"re et al., 2012) and *SEM (Agirre et al., 2013) have introduced benchmarks for measuring Semantic Textual Similarity (STS) between similar-sized sentences and phrases. Other data sets such as that This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 17 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17–26, Dublin, Ireland, August 23-24, 2014. tion (Sp¨arck Jones, 2007), gloss-to-sense mapping (Pilehvar and Navigli, 2014b), and modeling the semantics of multi-word expressions (Marelli et al., 2014) or polysemous words (Pilehvar and Navigli, 2014a). Task 3 was designed with three main objectives. First, the task should include multiple types of comparison in order to assess each type’s difficulty and whether specialized resources are needed for each. Second, the task should incorporate text from multiple domains and writing styles to ensure that system performance is robust across text types. Third, the similarity methods should be able to operate at the sense level, thereby potentially uniting text- and sense"
S14-2003,ide-suderman-2004-american,0,0.00893705,"from specific domains, social media, and text with idiomatic or slang language. Table 3 summarizes the corpora and their distribution across the test and training sets for each comparison type, with a high-level description of the genre of the data. We briefly describe the corpora next. The WikiNews, Reuters 21578, and Microsoft Research (MSR) Paraphrase corpora are all drawn from newswire text, with WikiNews being authored by volunteer writers and the latter two corpora written by professionals. Travel Guides was drawn from the Berlitz travel guides data in the Open American National Corpus (Ide and Suderman, 2004) and includes very verbose sentences 1 Annotation materials along with all training and test data are available on the task website http://alt.qcri. org/semeval2014/task3/. 18 4 – Very Similar The two items have very similar meanings and the most important ideas, concepts, or actions in the larger text are represented in the smaller text. Some less important information may be missing, but the smaller text is a very good summary of the larger text. 3 – Somewhat Similar The two items share many of the same important ideas, concepts, or actions, but include slightly different details. The smalle"
S14-2003,S12-1046,0,0.00660937,"download?” is captured in the phrase “streaming vintage movies for free”, or how similar is “circumscribe” to the phrase “beating around the bush.” Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality. Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications. Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al., 2012), keyphrase identification (Kim et al., 2010), lexical substitution (McCarthy and Navigli, 2009), summarizaThis paper introduces a new SemEval task on Cross-Level Semantic Similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Highquality data sets were constructed for four comparison types using multi-stage annotation procedures with a graded scale of similarity. Nineteen teams submitted 38 systems. Most systems surpassed the baseline performance, with several attaining high perform"
S14-2003,S12-1047,1,0.484906,"l task for evaluating the capabilities of systems at measuring all types of semantic similarity, independently of the size of the text. To accomplish this objective, systems were presented with items from four comparison types: (1) paragraph to sentence, (2) sentence to phrase, (3) phrase to word, and (4) word to sense. Given a pair of items, a system must assess the degree to which the meaning of the larger item is captured in the smaller item. WordNet 3.0 was chosen as the sense inventory (Fellbaum, 1998). 2.2 Task Data Rating Scale 3.1 Following previous SemEval tasks (Agirre et al., 2012; Jurgens et al., 2012), Task 3 recognizes that two items’ similarity may fall within a range of similarity values, rather than having a binary notion of similar or dissimilar. Initially a six-point (0–5) scale similar to that used in the STS tasks was considered (Agirre et al., 2012); however, annotators found difficulty in deciding between the lower-similarity options. After multiple revisions and feedback from a group of initial annotators, we developed a five-point Likert scale for rating a pair’s similarity, shown in Table 1.1 The scale was designed to systematically order a broad range of semantic relations: s"
S14-2003,S01-1004,0,0.0226851,"Missing"
S14-2003,S10-1004,0,0.00761061,"vintage movies for free”, or how similar is “circumscribe” to the phrase “beating around the bush.” Furthermore, by incorporating comparisons of a variety of item sizes, Task 3 unifies in a single task multiple objectives from different areas of NLP such as paraphrasing, summarization, and compositionality. Because CLSS generalizes STS to items of different types, successful CLSS systems can directly be applied to all STS-based applications. Furthermore, CLSS systems can be used in other similarity-based applications such as text simplification (Specia et al., 2012), keyphrase identification (Kim et al., 2010), lexical substitution (McCarthy and Navigli, 2009), summarizaThis paper introduces a new SemEval task on Cross-Level Semantic Similarity (CLSS), which measures the degree to which the meaning of a larger linguistic item, such as a paragraph, is captured by a smaller item, such as a sentence. Highquality data sets were constructed for four comparison types using multi-stage annotation procedures with a graded scale of similarity. Nineteen teams submitted 38 systems. Most systems surpassed the baseline performance, with several attaining high performance for multiple comparison types. Further,"
S14-2003,2005.mtsummit-papers.11,0,0.00244139,"ce on Wikipedia. Food reviews were drawn from the SNAP Amazon Fine Food Reviews data set (McAuley and Leskovec, 2013) and are customer-authored reviews for a variety of food items. Fables were taken from a collection of Aesop’s Fables. The Yahoo! Answers corpus was derived from the Yahoo! Answers data set, which is a collection of questions and answers from the Community Question Answering (CQA) site; the data set is notable for having the highest degree of ungrammaticality in our test set. SMT Europarl is a collection of texts from the English-language proceedings of the European parliament (Koehn, 2005); Europarl data was also used in the PPDB corpus (Ganitkevitch et al., 2013), from which phrases were extracted. Wikipedia was used to generate two phrase data sets from (1) extracting the definitional portion of an article’s initial sentence, e.g., “An [article name] is a [definition],” and (2) captions for an article’s images. Web queries were gathered from online sources of realworld queries. Last, the first and second authors generated slang and idiomatic phrases based on expressions contained in Wiktionary. 3.2 Annotation Process A two-phase process was used to produce the test and traini"
S14-2003,S13-1004,0,\N,Missing
S15-1021,W11-2501,0,0.37889,"tion about lexical and relational similarities for the classifier to generalize and to gain recall. Therefore, as further validation, a second experiment is carried out, where the systems have to classify word pairs from a different domain than the domains in the training set. The objective is to assess the importance of the domain-aware training instances for the classification. The K&H dataset contains only instances from three domains and is imbalanced between the number of instances across domains and relation types. Therefore, our second experiment tests each method on the BLESS dataset (Baroni and Lenci, 2011), which spans 17 topical domains and includes five relation types, the three in K&H and (a) attributes of concepts, a relation holding between nouns and adjectives, and (b) actions performed by/to concepts a relation holding between nouns and verbs. In total, the BLESS dataset contains 14400 positive instances and an equal number of negative instances. This experiment measures the generalizability of each system and tests the capabilities of the systems for lexical-semantic relation types other than taxonomic relations. Domain-aware training instances To show the importance of the domain-aware"
S15-1021,P14-1023,0,0.0665731,"nstances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. The graph encodes the distributional behavior of each word in the pair and consequently, patterns of co-occurrence expressing each target relation are extracted from it as relational information. The second approach uses word embeddings which have been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target seman"
S15-1021,P99-1008,0,0.0152874,"ties among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisitio"
S15-1021,W03-0415,0,0.0316758,"es of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be improved using the distributional similarity of words. In contrast with the previous approaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and"
S15-1021,P06-1038,0,0.0330214,"ttern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined"
S15-1021,J13-3008,1,0.128812,"Missing"
S15-1021,P14-1113,0,0.167986,"jective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semantic relations. Given a set of target semantic relations R = {r1 , . . . , rn }, and a set of word pairs W = {(x, y)1 , . . . , (x, y)n }, the task is to label each word pair (x, y)"
S15-1021,J06-1005,0,0.0169068,"ation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relat"
S15-1021,C92-2082,0,0.691702,"been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techn"
S15-1021,W09-0205,0,0.151917,"e to requiring co-occurrence, other works have classified the relation of a word pair using lexical similarity, i.e., the similarity of the concepts themselves. Given two word pairs, (w1 , w2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even wh"
S15-1021,S12-1047,1,0.748516,"to 3 words] y [0 to 1 words]. Using the initial set of lexical patterns extracted from a corpus, additional patterns are generated by optionally generalizing each word to its part of speech. For N seed pairs, the most frequent kN patterns are retained. We follow Turney (2008b) and set k = 20. The patterns retained are then used as features to train an SVM classifier over the set of possible relation types. DSZhila & DSLevy Word embeddings have previously been shown to accurately measure relational similarity; Zhila et al. (2013) demonstrate state-ofthe-art performance on SemEval-2012 Task 2 (Jurgens et al., 2012) which measures word pair similarity within a particular semantic relation (i.e., which pairs are most prototypical of a semantic relation). This approach can easily be extended to the classification setting: Given a target word pair (x, y), the similarity is computed between (x, y) and each word pair (x, y)i of a target relation r. The average of these similarity measurements was taken 187 as the final score for each relation r.4 Finally, the word pair is classified as an instance of the relation with the highest associated score. Two types of embeddings are used, (a) the word embeddings prod"
S15-1021,D10-1108,0,0.00836648,"64.7 F 8.7 14.2 59.0 21.0 14.2 73.1 74.2 73.5 76.4 Wikipedia P R F 77.0 11.7 20.4 89.4 16.2 27.5 94.0 75.5 83.7 32.8 22.6 26.8 27.7 15.6 20.0 96.8 87.7 92.0 97.6 89.3 93.2 95.4 86.1 90.5 96.7 88.4 92.4 Table 3: Aggregated results obtained for the indomain setup with the K&H dataset. Detailed results are presented in the Appendix A. occur in text. Therefore, in the first experiment, we test whether the recall of classification systems is improved when the word pair representation encodes information about lexical and relational similarity. As an evaluation dataset, we expand on the dataset of Kozareva and Hovy (2010) (K&H), which was collected from hyponym-hypernym instances from WordNet (Miller, 1995) spanning three topical domains: animals, plants and vehicles. Because our systems are capable of classifying instances with more than one relation at once, we enhance this dataset with instances of two more relation types: co-hyponymy and meronymy. Co-hyponyms are extracted directly from the K&H dataset: two words are co-hyponyms if they have the same direct ancestor.5 To avoid including generic nouns, such as “migrator” in the “animal” domain, only leaf nodes are considered. The meronym instances are extra"
S15-1021,P14-2050,0,0.201948,"enses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semant"
S15-1021,W14-1618,0,0.410157,"enses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semant"
S15-1021,P14-5010,0,0.00321295,"Setup Corpora Many pattern-based systems increase the size of the input corpus in an attempt to overcome data sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIR C LASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation"
S15-1021,N13-1090,0,0.549126,"i, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The go"
S15-1021,W12-4104,0,0.0214934,"nd relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b;"
S15-1021,P09-1113,0,0.136745,"lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur 182 David Jurgens McGill University Montreal, Canada jurgens@cs.mcgill.ca Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can classify. As an alternat"
S15-1021,P10-1134,1,0.846022,"roaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov"
S15-1021,E09-1071,0,0.0480814,"Missing"
S15-1021,S13-2056,0,0.0316207,"al basis function kernel (Platt, 1999) is trained using WEKA (Hall et al., 2009) to classify each word pair based on its representation provided by a graph-based representation model (Section 3.1) or a word embeddings representation model (Section 3.2) for N different lexical relations. The SVM classifier generates a distribution over relation labels and the highest weighted label is selected as the relation holding between the members of the word pair. 4 Experiments While several datasets have been created for detecting semantic relations between two words in context (Hendrickx et al., 2010; Segura-Bedmar et al., 2013), in our work we focus on the classification of word pairs as instances of lexical-semantic relations out of context. The performance of the GraCE and WECE systems is tested across two datasets, focusing on their ability to classify instances of specific lexical-semantic relations as well as to provide insights into the systems’ generalization capabilities. 4.1 Experimental Setup Corpora Many pattern-based systems increase the size of the input corpus in an attempt to overcome data sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using two corpora of"
S15-1021,P06-1040,0,0.189331,"2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. Th"
S15-1021,J06-3003,0,0.259422,"2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. Th"
S15-1021,C08-1114,0,0.0743919,"ing the type of lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur 182 David Jurgens McGill University Montreal, Canada jurgens@cs.mcgill.ca Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can cl"
S15-1021,C02-1114,0,0.394996,"preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed"
S15-1021,N13-1120,0,0.0989502,"of the corpus created with dependency relations. The graph encodes the distributional behavior of each word in the pair and consequently, patterns of co-occurrence expressing each target relation are extracted from it as relational information. The second approach uses word embeddings which have been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, dependin"
S15-1021,S10-1006,0,\N,Missing
S16-1169,E09-1005,0,0.04607,"stems was done according to their F1 scores. 4 Systems Five teams submitted 13 systems, where each team’s systems were variations on a common architecture. No system utilized resource-specific features beyond the gloss (e.g., the Wiktionary markup) and so all systems were ultimately submitted in the constrained category. Systems were compared against two baselines. 4.1 Participants The MSejrKU systems build definitional representations based on skip-gram vectors trained on Wikipedia data and incorporates syntactic features. 1098 Words in a candidate gloss are disambiguated using the method of Agirre and Soroa (2009) and then a classifier predicts the goodness of fit for a candidate attachment synset related to those in the gloss. The Duluth systems perform string matching to compare a definition with each of the glosses in WordNet. Given a new definition, systems differ in which words are included from the WordNet synset for comparison: Duluth2 uses only the words in the definition after stopword removal, while Duluth1 extends Duluth2 by including words from the hypernyms of the compared synset. Duluth3 extends Duluth1 with words from the hyponyms but also takes the step of breaking each definition into"
S16-1169,S12-1051,0,0.0334628,"4’s datasets. A system’s task is to identify, for a new word sense, the target synset and the corresponding operation. incorporating polysemy into the task by requiring systems to specify a concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense. Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity (Agirre et al., 2012; Agirre et al., 2013, STS) and Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets (Hovy et al., 2006) and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the"
S16-1169,S13-1004,0,0.0211258,"m’s task is to identify, for a new word sense, the target synset and the corresponding operation. incorporating polysemy into the task by requiring systems to specify a concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense. Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity (Agirre et al., 2012; Agirre et al., 2013, STS) and Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets (Hovy et al., 2006) and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such a"
S16-1169,S15-2151,0,0.0811161,"-markup) for performing the integration and may use additional information from any dictionary, including the one from which the target word sense had been obtained, e.g., Wiktionary. • Constrained: the system might use any resource other than dictionaries. We allowed each team to submit up to three runs per system type to let them explore different configurations, features, or parameter settings in the official rankings. 2.2 Related Tasks Task 14 directly relates to three branches of prior tasks in SemEval. First, two recent tasks have evaluated automatic methods for constructing taxonomies (Bordea et al., 2015; Bordea et al., 2016). In these tasks, participants are presented with word pairs –but no glosses– and tasked with organizing the words into hypernym relationships. Task 14 provides the next step in such evaluations by explicitly Lemma geoscience POS noun mudslide noun euthanize verb changing room noun Apple noun own verb Definition Any of several sciences that deal with the Earth A mixed drink consisting of vodka, Kahlua and Bailey’s. To submit (a person or animal) to euthanasia. A room, especially in a gym, designed for people to change their clothes. An American multinational technology co"
S16-1169,S16-1168,0,0.0965396,"Missing"
S16-1169,J06-1003,0,0.735663,"ntended target synset and the one outputted by the system. However, we recognize that links in the taxonomy do not necessarily represent uniform semantic distances, since siblings that are deep in the hierarchy tend to be more related to one another. Hence, a direct edge-counting approach might not provide a reliable basis for the evaluation of the attachment accuracy. Interestingly, the attachment accuracy evaluation can be cast as a WordNet-based semantic similarity measurement in which the goal is to compute the similarity between two concepts based on the structural properties of WordNet (Budanitsky and Hirst, 2006b), most important of which is the distance between the two. Therefore, we measure accuracy using the Wu and Palmer (1994, Wu&P) semantic similarity measure, defined as: 2 · depthLCS (1) depth1 + depth2 where depth1 and depth2 are the depths of the two concepts in WordNet’s subsumption hierarchy (hypernymy/hyponymy relations) and DepthLCS is the depth of their least common subsumer, i.e., the most specific concept which is an ancestor of both the concepts. For each instance in the test set for which the system made a prediction, we measure the Wu&P similarity of the output attachment and the c"
S16-1169,esuli-sebastiani-2006-sentiwordnet,0,0.146338,"Missing"
S16-1169,P08-1017,0,0.239662,"Missing"
S16-1169,N06-2015,0,0.0476498,"concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense. Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity (Agirre et al., 2012; Agirre et al., 2013, STS) and Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets (Hovy et al., 2006) and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate. Third, prior tasks on Word Sense Induction (WSI) 1094 have evaluated methods that automatically discover"
S16-1169,P15-1010,1,0.838753,"h2 uses only the words in the definition after stopword removal, while Duluth1 extends Duluth2 by including words from the hypernyms of the compared synset. Duluth3 extends Duluth1 with words from the hyponyms but also takes the step of breaking each definition into character tri-grams to capture surface-form regularities. The UMNDuluth team performs a similar approach but weights gloss similarity by favoring specific kinds of terms, such as those that are longer and those that appear in WordNet. The TALN systems project the definition of the novel term into a vector space using S ENS E MBED (Iacobacci et al., 2015). Then this vector is compared with the vectors for senses in WordNet to find the closest match. System variations address issues when words have no associated vectors and how to select between candidate attachments. The JRC system uses a form of second-order similarity by representing each definition as a vector over the synsets that contain its words. New terms are attached by finding the WordNet synset whose definition has maximal cosine similarity. The VCU systems adopt multiple approaches based on textual similarity. Run1 uses a secondorder expansion by representing a definition using fre"
S16-1169,S13-2049,1,0.852284,"n evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate. Third, prior tasks on Word Sense Induction (WSI) 1094 have evaluated methods that automatically discover the different meanings of a word (Manandhar et al., 2010; Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013). However, the new senses discovered by these methods were never integrated into any taxonomy, making them difficult to use and relate to existing concepts. Task 14 provides a natural next step for WSI pairs, should any novel induced senses be matched with a gloss describing it. 3 Task Data Given that WordNet 3.0 offers wide coverage of common concepts, the majority of novel concepts to be integrated are likely to come from topical domains, informal expressions, and neologisms. Therefore, the dataset for Task 14 was constructed to contain concepts from a wide varie"
S16-1169,N15-1169,1,0.89615,"Missing"
S16-1169,S14-2003,1,0.871791,"t and the corresponding operation. incorporating polysemy into the task by requiring systems to specify a concept, rather than a word, as a hypernym. For example, when recognizing the relationships that a dog is a canine, the system would be required to specify that the concept should be attached to the animal sense of canine, not the tooth sense. Second, the task of comparing a gloss associated with a new concept is closely related to the recent tasks on semantic similarity, i.e., Semantic Textual Similarity (Agirre et al., 2012; Agirre et al., 2013, STS) and Cross-Level Semantic Similarity (Jurgens et al., 2014, CLSS). Indeed, prior STS tasks included gloss pairs from OntoNotes in the datasets (Hovy et al., 2006) and CLSS had, among its four different evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate."
S16-1169,E12-1060,0,0.0286342,"e coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy (Poprat et al., 2008; Snow et al., 2006; Toral et al., 2008; Yamada et al., 2011; Jurgens and Pilehvar, 2015). However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words (Lau et al., 2012); however, these senses were not re-integrated into the taxonomy. Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire 1092 Proceedings of SemEval-2016, pages 1092–1102, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics new automated approaches for using the definitions in these resource to expand WordNet with new concepts. Accordingly, the task provides a high-quality dataset of one thousand definitions from a wide range of domains to be added to the WordNet hierarchy, either by adding them as new concept"
S16-1169,S10-1011,0,0.016173,"rent evaluation types, an evaluation for systems measuring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate. Third, prior tasks on Word Sense Induction (WSI) 1094 have evaluated methods that automatically discover the different meanings of a word (Manandhar et al., 2010; Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013). However, the new senses discovered by these methods were never integrated into any taxonomy, making them difficult to use and relate to existing concepts. Task 14 provides a natural next step for WSI pairs, should any novel induced senses be matched with a gloss describing it. 3 Task Data Given that WordNet 3.0 offers wide coverage of common concepts, the majority of novel concepts to be integrated are likely to come from topical domains, informal expressions, and neologisms. Therefore, the dataset for Task 14 was constructed to conta"
S16-1169,P14-5010,0,0.00666983,"Missing"
S16-1169,S13-2035,0,0.0242744,"uring the similarity between word senses and words. However, while textual similarity is likely to be core component of Task 14 systems, the data is often richer than raw text by containing (a) regular linguistic structure where the parent concept is likely to be introduced first and (b) contextual features from where the gloss appears such as hyperlinks or example usages, which may help to disambiguate. Third, prior tasks on Word Sense Induction (WSI) 1094 have evaluated methods that automatically discover the different meanings of a word (Manandhar et al., 2010; Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013). However, the new senses discovered by these methods were never integrated into any taxonomy, making them difficult to use and relate to existing concepts. Task 14 provides a natural next step for WSI pairs, should any novel induced senses be matched with a gloss describing it. 3 Task Data Given that WordNet 3.0 offers wide coverage of common concepts, the majority of novel concepts to be integrated are likely to come from topical domains, informal expressions, and neologisms. Therefore, the dataset for Task 14 was constructed to contain concepts from a wide variety of domains and to include"
S16-1169,P13-1132,1,0.912458,"Missing"
S16-1169,W08-0507,0,0.045568,"troduction Semantic networks and ontologies are key resources in Natural Language Processing. Of these resources, WordNet (Fellbaum, 1998), the de facto standard lexical database of English, has remained in widespread use over the past two decades, with a broad range of applications such as Word Sense Disambiguation (Navigli, 2009), Query expansion Hence, a variety of techniques have tried to tackle the coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy (Poprat et al., 2008; Snow et al., 2006; Toral et al., 2008; Yamada et al., 2011; Jurgens and Pilehvar, 2015). However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words (Lau et al., 2012); however, these senses were not re-integrated into the taxonomy. Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire 1092 Proceedings of SemEval-2"
S16-1169,P06-1101,0,0.281153,"etworks and ontologies are key resources in Natural Language Processing. Of these resources, WordNet (Fellbaum, 1998), the de facto standard lexical database of English, has remained in widespread use over the past two decades, with a broad range of applications such as Word Sense Disambiguation (Navigli, 2009), Query expansion Hence, a variety of techniques have tried to tackle the coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy (Poprat et al., 2008; Snow et al., 2006; Toral et al., 2008; Yamada et al., 2011; Jurgens and Pilehvar, 2015). However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words (Lau et al., 2012); however, these senses were not re-integrated into the taxonomy. Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire 1092 Proceedings of SemEval-2016, pages 1092–110"
S16-1169,toral-etal-2008-named,0,0.058103,"ies are key resources in Natural Language Processing. Of these resources, WordNet (Fellbaum, 1998), the de facto standard lexical database of English, has remained in widespread use over the past two decades, with a broad range of applications such as Word Sense Disambiguation (Navigli, 2009), Query expansion Hence, a variety of techniques have tried to tackle the coverage limitation of WordNet, often by drawing new word senses from other domain-specific or collaboratively-constructed dictionaries and adding the new word senses to the WordNet hierarchy (Poprat et al., 2008; Snow et al., 2006; Toral et al., 2008; Yamada et al., 2011; Jurgens and Pilehvar, 2015). However, these approaches have usually been tested on relatively small datasets, often testing for word-level relationships without precisely measuring integration accuracy at the concept level. Similarly, other techniques have been proposed for automatically discovering novel senses of words (Lau et al., 2012); however, these senses were not re-integrated into the taxonomy. Given the availability of large-scale dictionaries such as Wiktionary, Task 14 is designed to inspire 1092 Proceedings of SemEval-2016, pages 1092–1102, c San Diego, Cali"
S16-1169,tsvetkov-etal-2014-augmenting-english,0,0.0676202,"Missing"
S16-1169,P94-1019,0,\N,Missing
S16-1169,I11-1098,0,\N,Missing
W09-4302,W09-0214,0,0.048145,"semantic spaces has been introduced by Fortuna et al.[5]. Their approach focused on finding key words that existed in multiple spaces, and defining a concrete set of semantics for these landmark words. As semantics from distinct spaces are created, they can be evaluated according to their relation to these landmark terms, and at any point in time, the words most closely associated to the landmark provide terms describing events related to the landmarks. Sagi et al. propose an alternate approach of uses a single corpus and includes temporal semantics after generating an initial set of semantics[17]. This generates semantic vectors for a corpus spanning many time ranges of interest and reducing dimensionality via SVD. Then, to develop temporal semantics for a term, documents from a specific time range are used to generate temporal vectors through a process very similar to Random Indexing; in this process the first set of semantic vectors generated are used in place of index vectors when using equation (1). While these approaches allow for accurate representations of semantic shifts, they face significant challenges when scaling to a large streaming set of documents, due to a reliance on"
W09-4302,P10-4006,1,\N,Missing
W10-2801,J07-2002,0,0.0697663,"Missing"
W10-2801,P10-4006,1,0.878091,"Missing"
W11-1104,D09-1056,0,0.107298,"Missing"
W11-1104,E03-1020,0,0.34377,"ontext. Word Sense Induction (WSI) discovers the different senses of a word, such as “law,” by examining its contextual uses. By deriving the senses of a word directly from a corpus, WSI is able to identify specialized, topical meanings in domains such as medicine or law, which predefined sense inventories may not include. aWe consider graph-based approaches to WSI, which typically construct a graph from word occurrences or collocations. The core problem is how to identify sense-specific information within the graph in order to perform sense induction. Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs. We reinterpret the challenge of identifying sensespecific information in a co-occurrence graph as one of community detection, where a community is defined as a group of connected nodes that are more connected to each other than to the rest of the graph (Fortunato, 2010). Within the co-occurrence graph, we hypothesize that communities identify sensespecific contexts for each of the terms. Community detection identifies groups of contextual cues that constrain each o"
W11-1104,P10-4006,1,0.889693,"Missing"
W11-1104,D10-1073,0,0.270374,"“law,” by examining its contextual uses. By deriving the senses of a word directly from a corpus, WSI is able to identify specialized, topical meanings in domains such as medicine or law, which predefined sense inventories may not include. aWe consider graph-based approaches to WSI, which typically construct a graph from word occurrences or collocations. The core problem is how to identify sense-specific information within the graph in order to perform sense induction. Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs. We reinterpret the challenge of identifying sensespecific information in a co-occurrence graph as one of community detection, where a community is defined as a group of connected nodes that are more connected to each other than to the rest of the graph (Fortunato, 2010). Within the co-occurrence graph, we hypothesize that communities identify sensespecific contexts for each of the terms. Community detection identifies groups of contextual cues that constrain each of the words in a community to a single sense. To test our hypothesis, we require a community"
W11-1104,S10-1011,0,0.260256,"ord’s context to include only a subset of the terms present. Following previous work (V´eronis, 2004), we select only nouns in the context. Early experiments indicated that including infrequent terms in the co-occurrence graph yielded poor performance, which we attribute to having too few connecting edges to identify meaningful community structure. Therefore, we include only those nouns occurring in the most frequent 5000 tokens, which are likely to be representative the largest communities in which a term takes part. Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al., 2010), which are used in our evaluation. The selected context terms are then stemmed using the Porter stemmer. Building the Co-occurrence Graph The graph is iteratively constructed by adding edges between the terms from a context. For each pair-wise combination of terms, an edge is added and its weight is increased by 1. This step effectively embeds a clique if it did not exist before, connecting all of the context’s words within the graph. Once all contexts have been seen, the graph is then pruned to remove all edges with weight below a threshold τ = 25. This step removes edges form infrequent col"
W11-1104,D07-1043,0,0.0284549,"ng corpus consisting of 879,807 multi-sentence contexts for 100 polysemous words, comprised of 50 nouns and 50 verbs. Systems induce sense representations for target words from the training corpus and then use those representations to label the senses of the target words in unseen contexts from a test corpus. We use the entire multi-sentence context for building the co-occurrence graph. The induced sense labeling is scored using two unsupervised and one supervised methods. The unsupervised scores consists of two contrasting measures: the paired FScore (Artiles et al., 2009) and the V-Measure (Rosenberg and Hirschberg, 2007). Briefly, the V-Measure rates the homogeneity and completeness of a clustering solution. Solutions that have word clusters formed from one gold-standard sense are homogeneous; completeness measures the degree to which a gold-standard sense’s instances are assigned to a single cluster. The paired FScore reflects the overlap of the solution and the gold standard in cluster assignments for all pair-wise combiCommunity Size 120 90 60 30 0 1.00 V-Measure Memberships Table 1: Performance results on the SemEval-2010 WSI Task, with rank shown in parentheses. Reference scores of the best submitted sys"
W11-2214,D09-1056,0,0.0522048,"Missing"
W11-2214,E06-1018,0,0.0746066,"d Stevens, 2010a) and cluster the final centroids with Hierarchical Agglomerative Clustering, with the average link criteria as suggested by (Pedersen and Bruce, 1997). 3 Modeling Context For each clustering algorithm, we consider five context models that represent the types of lexical features used by the majority of WSI approaches. Co-Occurrence Contexts formed from word cooccurrence are the most common in WSI algorithms. For each occurrence of a word, those words within a certain range are counted as features. Prior work has used a variety of context sizes, e.g. words in the same sentence (Bordag, 2006), in nearby lexical positions (Gauch and Futrelle, 1993), or within a paragraph-sized context window (Pedersen, 2010). We consider two co-occurrence context models: a 5-word and a 25-word window. We note that in co-occurrence-based word space algorithms, smaller context sizes have shown to better capture paradagmatic similarity, while larger sizes capture semantic associativity (Peirsman et al., 2008; Utsumi, 2010). Dependency-Relations Dependency parsing creates a syntax tree where words are directly linked according to their relation. These links refine cooccurrence based contexts by utilizi"
W11-2214,P10-1046,0,0.143001,"Missing"
W11-2214,W02-0805,0,0.037872,"us words have interrelated senses, with lexicographers often in disagreement for the number of fine-grained senses (Palmer et al., 2007). For example, the most frequent four senses for “law” according to WordNet, shown in Table 1, are similar in several aspects and could be ascribed interchangeably in some contexts. The difficulty of automatically distinguishing two senses is proportional to their similarity because of the increasing likelihood of the two senses sharing similar contexts. While the issue distinguishing between related senses is a recognized issue for Word Sense Disambiguation (Chugur et al., 2002; McCarthy, 2006), which uses supervised training to learn sense distinctions, measuring the impact of sense relatedness on the harder problem of WSI remains unaddressed. The recent SemEval WSI tasks (Agirre and Soroa, 2007; Manandhar and Klapaftis, 2009) have provided a standard framework for evaluating WSI systems, with a controlled training corpus designed to limit sense ambiguity in the example contexts. However, given the potential relatedness of a word’s senses, we view it necessary to consider how WSI methods perform relative to the degree of contextual ambiguity. Our goal is therefore"
W11-2214,J02-2003,0,0.0422364,"ge corpus with manually labeled sense assignments and sense similarity judgements is prohibitively expensive. Therefore, we employ a pseudo-word discrimination task where a base word and a second word, its confounder, are replaced throughout the corpus with a pseudoword. The objective is then to determine which of the words was originally present given the context of an occurrence of the pseudo-word. Due to not requiring manual annotation, this type of task was initially proposed as a substitute for word sense disambiguation (Sch¨utze, 1992; Gale et al., 1992) and for selectional preferences (Clark and Weir, 2002). Following the suggestions of Chambers and Ju116 festival offices play convention tournament concerts 0.13660 0.13751 0.20296 0.29007 0.48348 laws interests politics governments regulations legislation 0.18289 0.20440 0.29125 0.40761 0.56112 Table 2: Example confounders for “festival” and “laws” and their similarities rafsky (2010) on designing pseudo-words, pseudowords were created from words with the same part of speech and equal frequency in the training corpus. We selected nouns occurring more than 5,000 times in a 2009 Wikipedia snapshot and then drew 5,000 contexts for each. The snapsho"
W11-2214,D10-1113,0,0.028136,"rvised score was achieved by a graphbased system (Klapaftis and Manandhar, 2008). 6 Future Work and Conclusion We presented a two evaluation for WSI approaches and examined the performance of a wide range of algorithms. The results raise a potential issue for clustering-based WSI approaches: sense discrimination degrades notably as the sense relatedness increases. We highlight three potential avenues for future research. First, this methodology should be applied to additional WSI models, such as graphbased (Klapaftis and Manandhar, 2008; Navigli and Crisafulli, 2010) and probabilistic models (Dinu and Lapata, 2010; Elshamy et al., 2010). Second, we plan to extend the analysis to different sense distributions, varying number of senses, and for human annotated sense similarity data. Third, this evaluation makes the simplifying assumption of one sense per instance; however, Erk et al. (2009) note that the relations between senses may cause a single word instance to evoke multiple senses within the same context. Therefore, a future experiment should consider how WSI systems might address learning senses given the presence of multiple, similar senses for a single instance. All models, associated data sets,"
W11-2214,E03-1020,0,0.139831,"nsitive to the clustering algorithm and not the lexical features used. 1 Introduction Many words in a language have several distinct meanings. For example, “earth” may refer to the planet Earth, dirt, or solid ground, depending on the context. The goal of Word Sense Induction (WSI) is to automatically discover the different senses by examining how a word is used. This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003). Furthermore, these discovered senses can be used to automatically expand lexical resources such as WordNet or FrameNet (Klapaftis and Manandhar, 2010). Discovering the multiple senses is frequently Keith Stevens2 University of California, Los Angeles Los Angeles, California, USA kstevens@cs.ucla.edu confounded by the relationships between a word’s senses. While homonyms such as “bass” or “bank” have unrelated senses, many polysemous words have interrelated senses, with lexicographers often in disagreement for the number of fine-grained senses (Palmer et al., 2007). For example, the most freq"
W11-2214,S10-1082,0,0.0214485,"ed by a graphbased system (Klapaftis and Manandhar, 2008). 6 Future Work and Conclusion We presented a two evaluation for WSI approaches and examined the performance of a wide range of algorithms. The results raise a potential issue for clustering-based WSI approaches: sense discrimination degrades notably as the sense relatedness increases. We highlight three potential avenues for future research. First, this methodology should be applied to additional WSI models, such as graphbased (Klapaftis and Manandhar, 2008; Navigli and Crisafulli, 2010) and probabilistic models (Dinu and Lapata, 2010; Elshamy et al., 2010). Second, we plan to extend the analysis to different sense distributions, varying number of senses, and for human annotated sense similarity data. Third, this evaluation makes the simplifying assumption of one sense per instance; however, Erk et al. (2009) note that the relations between senses may cause a single word instance to evoke multiple senses within the same context. Therefore, a future experiment should consider how WSI systems might address learning senses given the presence of multiple, similar senses for a single instance. All models, associated data sets, testing framework, and"
W11-2214,P09-1002,0,0.0226984,"roaches: sense discrimination degrades notably as the sense relatedness increases. We highlight three potential avenues for future research. First, this methodology should be applied to additional WSI models, such as graphbased (Klapaftis and Manandhar, 2008; Navigli and Crisafulli, 2010) and probabilistic models (Dinu and Lapata, 2010; Elshamy et al., 2010). Second, we plan to extend the analysis to different sense distributions, varying number of senses, and for human annotated sense similarity data. Third, this evaluation makes the simplifying assumption of one sense per instance; however, Erk et al. (2009) note that the relations between senses may cause a single word instance to evoke multiple senses within the same context. Therefore, a future experiment should consider how WSI systems might address learning senses given the presence of multiple, similar senses for a single instance. All models, associated data sets, testing framework, and scores have been released as a part of the open-source S-Space Package (Jurgens and Stevens, 2010b).5 5 http://code.google.com/p/airhead-research/ References Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discr"
W11-2214,W10-4173,0,0.0250853,"tasets, is larger then the gap for the previous K value. We calculate the gap using 10 artificial data sets sampled from the model. Spectral Clustering Spectral Clustering interprets a dataset’s elements as vertices in graph with edges based on their similarity (Ng et al., 2001). Clusters are found by identifying the graph partition that produces the minimum conductance between every partition. This can be thought of as trying to find small islands that are connected by as few bridges as possible. We refer the reader to (von Luxburg, 2007) for further technical details. To our knowledge, only He et al. (2010) have applied spectral clustering to WSI, which was performed on a Chinese dataset. However, the algorithm used by He et al. requires the number of clusters to be specified. We instead use a hybrid spectral clustering algorithm, first applied to information retrieval (Cheng et al., 2006), that automatically selects the number of clusters. This algorithm recursively partitions a dataset in half by finding the cut that produces the minimum conductance, which builds a tree of partitions. This split is done until either every data point is in its own partition or a maximum number of partitions is"
W11-2214,N06-2015,0,0.021574,"uture WSI evaluations. A potential next step is to vary the proportion of contexts from the confounder. The current method intentionally uses a uniform distribution to avoid potential bias; however, word sense distributions are rarely equal, and a varied distribution would more closely model real world distributions. Similarly, the current method tested only two senses, whereas an n-way disambiguation between multiple confounders should also provide further insight into a WSI approach’s discriminatory abilities. The induced senses are then evaluated against the gold standard labels OntoNotes (Hovy et al., 2006) senses labels for the test corpus. For our evaluation, we use both the two contrasting unsupervised measures, the paired FScore (Artiles et al., 2009) and the V-Measure (Rosenberg and Hirschberg, 2007), and a supervised measure. For each metric, we use the evaluation framework provided by the organizers of SemEval-2 Task 14.1 The V-Measure rates the homogeneity and completeness of a clustering solution. Solutions that have word clusters formed from one gold-standard sense are homogeneous; completeness measures the degree to which a gold-standard sense’s instances are assigned to a single clus"
W11-2214,O97-1002,0,0.137385,"orpus from which the senses are induced; however, such labels are not available in the Task 14 training corpus. For each incorrect sense assignment, we measure the similarity of the confused sense to the correct sense. To our knowledge, no work has been done on calculating sense similarity within the OntoNotes sense hierarchy.2 Therefore, we approximate OntoNotes sense similarity by using sense similarity in the WordNet ontology, on which has many similarity measures have been defined. Following Budanitsky and Hirst (2006), we estimate the WordNet sense similarity using the method proposed by Jiang and Conrath (1997). Each OntoNotes sense si is mapped to a set of WordNet 3.0 senses S i = {wn1 , . . . , wnn } using 2 We suspect that this is in part because a word’s OntoNotes senses have been designed to minimize sense confusion. 119 the sense mapping provided by the CoNLL shared task.3 The sense similarity for two OntoNotes senses is computed using one of two methods: X 1 sim = 1 2 JCN (wni , wnj ), |S ||S |i 1 j 2 wn ∈S ,wn ∈S (1) or sim = argmax JCN (wni , wnj ), (2) wni ∈S 1 ,wnj ∈S 2 where JCN indicates the Jiang-Conrath similarity of two WordNet senses, calculated using WordNet::Similarity (Pedersen e"
W11-2214,P10-4006,1,0.901793,"at are similar to the largest set of contexts, while keeping clusters dissimilar from each other. CBC’s recursion ensures that contexts dissimilar to the large committees are still grouped into their own smaller committees, which enables the discovery of infrequent senses with distinct contexts. We use a hard sense assignment for each context, i.e., a context is labeled with only one sense according to the most similar cluster. Streaming K-Means As WSI moves into inducing senses from Web-scale amounts of data, existing clustering algorithms that keep all contexts in memory become impractical. Jurgens and Stevens (2010a) proposed an on-line hybrid clustering solution using on-line K-Means and Hierarchical Agglomerative Clustering, which automatically decided the number of clusters without retaining all the contexts. To the best of our knowledge, theirs is the only work using an on-line approach. We extend this work by applying a more theoretically sound online K-Means algorithm, called Streaming K-Means (Braverman et al., 2011), to WSI. We use Streaming K-Means to conduct a direct algorithmic comparison with K-Means in the hopes that online approaches can be made just as effective as off-line approaches. St"
W11-2214,S10-1078,0,0.108074,"lations Dependency parsing creates a syntax tree where words are directly linked according to their relation. These links refine cooccurrence based contexts by utilizing syntactic indications of how words are related. Dependency parsed features have proven highly effective for word representations in many NLP applications, e.g., (Pad´o and Lapata, 2007; Baroni et al., 2010). We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature. We note that recently Kern et al. (2010) achieved good WSI performance with only a small, manually-tuned subset of all relations as context. Word Ordering Word ordering can provide a mild form of syntactic information (Jones et al., 2006; Sahlgren et al., 2008). While other syntactic features may provide significantly more information, word ordering is efficient to compute and provides an alternative source of syntactic information for knowledge-lean systems or for languages where NLP tools are not readily available. Because we treat word ordering as a syntactic feature, we limit the context to words occurring in the same sentence."
W11-2214,N10-1010,0,0.0214757,"example, “earth” may refer to the planet Earth, dirt, or solid ground, depending on the context. The goal of Word Sense Induction (WSI) is to automatically discover the different senses by examining how a word is used. This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003). Furthermore, these discovered senses can be used to automatically expand lexical resources such as WordNet or FrameNet (Klapaftis and Manandhar, 2010). Discovering the multiple senses is frequently Keith Stevens2 University of California, Los Angeles Los Angeles, California, USA kstevens@cs.ucla.edu confounded by the relationships between a word’s senses. While homonyms such as “bass” or “bank” have unrelated senses, many polysemous words have interrelated senses, with lexicographers often in disagreement for the number of fine-grained senses (Palmer et al., 2007). For example, the most frequent four senses for “law” according to WordNet, shown in Table 1, are similar in several aspects and could be ascribed interchangeably in some contexts"
W11-2214,W09-2419,0,0.0397144,"Missing"
W11-2214,S10-1011,0,0.0686473,"Missing"
W11-2214,D10-1012,0,0.0333025,"Kern et al., 2010; Pedersen, 2010), whereas the top supervised score was achieved by a graphbased system (Klapaftis and Manandhar, 2008). 6 Future Work and Conclusion We presented a two evaluation for WSI approaches and examined the performance of a wide range of algorithms. The results raise a potential issue for clustering-based WSI approaches: sense discrimination degrades notably as the sense relatedness increases. We highlight three potential avenues for future research. First, this methodology should be applied to additional WSI models, such as graphbased (Klapaftis and Manandhar, 2008; Navigli and Crisafulli, 2010) and probabilistic models (Dinu and Lapata, 2010; Elshamy et al., 2010). Second, we plan to extend the analysis to different sense distributions, varying number of senses, and for human annotated sense similarity data. Third, this evaluation makes the simplifying assumption of one sense per instance; however, Erk et al. (2009) note that the relations between senses may cause a single word instance to evoke multiple senses within the same context. Therefore, a future experiment should consider how WSI systems might address learning senses given the presence of multiple, similar senses for a sin"
W11-2214,nivre-etal-2006-maltparser,0,0.0281726,"0.13751 0.20296 0.29007 0.48348 laws interests politics governments regulations legislation 0.18289 0.20440 0.29125 0.40761 0.56112 Table 2: Example confounders for “festival” and “laws” and their similarities rafsky (2010) on designing pseudo-words, pseudowords were created from words with the same part of speech and equal frequency in the training corpus. We selected nouns occurring more than 5,000 times in a 2009 Wikipedia snapshot and then drew 5,000 contexts for each. The snapshot was tagged with the Stanford Part of Speech Tagger (Toutanova et al., 2003) and parsed with the Malt Parser (Nivre et al., 2006). To evaluate the impact of sense similarity, pseudowords were created from word pairs with a broad range of lexical similarities. We selected lexical similarity as an approximation of sense similarity in order to model the hypothesis that similar senses may appear in similar contexts. Similarity scores were calculated using cosine similarity on contextual distributions built from a sliding ±2 word window over the Wikipedia corpus. Table 2 highlights several example confounders and their similarities with the base term. In total, we generated 5000 termconfounder pairs from 98 base terms, with"
W11-2214,J07-2002,0,0.0383615,"Missing"
W11-2214,W97-0322,0,0.368555,"set to 0. When assigning a data point, it is only assigned to an existing cluster when their similar is above some threshold, otherwise the 115 data point becomes the centroid of a new cluster. Once C reaches a threshold, based on an estimate of the number of data points, or the overall K-Means clustering cost reaches some limit, the centroids are treated as new data points and re-clustered, with the goal of merging some centroids. We follow (Jurgens and Stevens, 2010a) and cluster the final centroids with Hierarchical Agglomerative Clustering, with the average link criteria as suggested by (Pedersen and Bruce, 1997). 3 Modeling Context For each clustering algorithm, we consider five context models that represent the types of lexical features used by the majority of WSI approaches. Co-Occurrence Contexts formed from word cooccurrence are the most common in WSI algorithms. For each occurrence of a word, those words within a certain range are counted as features. Prior work has used a variety of context sizes, e.g. words in the same sentence (Bordag, 2006), in nearby lexical positions (Gauch and Futrelle, 1993), or within a paragraph-sized context window (Pedersen, 2010). We consider two co-occurrence conte"
W11-2214,N06-4007,0,0.172971,"s clusters based on the similarity between two data points. Clusters grow by assigning data points to the cluster with the most similar centroid. After every data point is assigned, each cluster’s centroid is recalculated to be the average of all the data points assigned to the cluster. This process repeats until the centroids converge to a fixed point. We choose initial seeds at random and use the H2 criterion function (Zhao and Karypis, 2001). Although K-Means is efficient and widely used, it requires the number of clusters to be specified a priori. Therefore, we follow the WSI model 114 of Pedersen and Kulkarni (2006) and use the Gap Statistic (Tibshirani et al., 2000) to automatically determine the number of clusters. The Gap Statistic runs K-Means repeatedly with different values of K, ranging from 1 to some sensible maximum. The Gap Statistic first induces a data model from the feature distributions of the initial dataset and then for each K, creates a set of artificial datasets by sampling from the derived model. K is increased until the “gap”, i.e. the distance between the objective function of the original dataset and the average objective function of the artificial datasets, is larger then the gap f"
W11-2214,S10-1081,0,0.0660673,"criteria as suggested by (Pedersen and Bruce, 1997). 3 Modeling Context For each clustering algorithm, we consider five context models that represent the types of lexical features used by the majority of WSI approaches. Co-Occurrence Contexts formed from word cooccurrence are the most common in WSI algorithms. For each occurrence of a word, those words within a certain range are counted as features. Prior work has used a variety of context sizes, e.g. words in the same sentence (Bordag, 2006), in nearby lexical positions (Gauch and Futrelle, 1993), or within a paragraph-sized context window (Pedersen, 2010). We consider two co-occurrence context models: a 5-word and a 25-word window. We note that in co-occurrence-based word space algorithms, smaller context sizes have shown to better capture paradagmatic similarity, while larger sizes capture semantic associativity (Peirsman et al., 2008; Utsumi, 2010). Dependency-Relations Dependency parsing creates a syntax tree where words are directly linked according to their relation. These links refine cooccurrence based contexts by utilizing syntactic indications of how words are related. Dependency parsed features have proven highly effective for word r"
W11-2214,D07-1043,0,0.103494,"however, word sense distributions are rarely equal, and a varied distribution would more closely model real world distributions. Similarly, the current method tested only two senses, whereas an n-way disambiguation between multiple confounders should also provide further insight into a WSI approach’s discriminatory abilities. The induced senses are then evaluated against the gold standard labels OntoNotes (Hovy et al., 2006) senses labels for the test corpus. For our evaluation, we use both the two contrasting unsupervised measures, the paired FScore (Artiles et al., 2009) and the V-Measure (Rosenberg and Hirschberg, 2007), and a supervised measure. For each metric, we use the evaluation framework provided by the organizers of SemEval-2 Task 14.1 The V-Measure rates the homogeneity and completeness of a clustering solution. Solutions that have word clusters formed from one gold-standard sense are homogeneous; completeness measures the degree to which a gold-standard sense’s instances are assigned to a single cluster. The paired FScore measures two types of overlap of a solution and the gold standard in cluster assignments for all in pairwise combination of instances. This score tends to penalize solutions with"
W11-2214,N03-1033,0,0.00395701,"ival offices play convention tournament concerts 0.13660 0.13751 0.20296 0.29007 0.48348 laws interests politics governments regulations legislation 0.18289 0.20440 0.29125 0.40761 0.56112 Table 2: Example confounders for “festival” and “laws” and their similarities rafsky (2010) on designing pseudo-words, pseudowords were created from words with the same part of speech and equal frequency in the training corpus. We selected nouns occurring more than 5,000 times in a 2009 Wikipedia snapshot and then drew 5,000 contexts for each. The snapshot was tagged with the Stanford Part of Speech Tagger (Toutanova et al., 2003) and parsed with the Malt Parser (Nivre et al., 2006). To evaluate the impact of sense similarity, pseudowords were created from word pairs with a broad range of lexical similarities. We selected lexical similarity as an approximation of sense similarity in order to model the hypothesis that similar senses may appear in similar contexts. Similarity scores were calculated using cosine similarity on contextual distributions built from a sliding ±2 word window over the Wikipedia corpus. Table 2 highlights several example confounders and their similarities with the base term. In total, we generate"
W11-2214,utsumi-2010-exploring,0,0.0231718,"gorithms. For each occurrence of a word, those words within a certain range are counted as features. Prior work has used a variety of context sizes, e.g. words in the same sentence (Bordag, 2006), in nearby lexical positions (Gauch and Futrelle, 1993), or within a paragraph-sized context window (Pedersen, 2010). We consider two co-occurrence context models: a 5-word and a 25-word window. We note that in co-occurrence-based word space algorithms, smaller context sizes have shown to better capture paradagmatic similarity, while larger sizes capture semantic associativity (Peirsman et al., 2008; Utsumi, 2010). Dependency-Relations Dependency parsing creates a syntax tree where words are directly linked according to their relation. These links refine cooccurrence based contexts by utilizing syntactic indications of how words are related. Dependency parsed features have proven highly effective for word representations in many NLP applications, e.g., (Pad´o and Lapata, 2007; Baroni et al., 2010). We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature. We note"
W11-2214,W06-2503,0,\N,Missing
W11-2214,S07-1002,0,\N,Missing
W11-2214,J06-1003,0,\N,Missing
W11-2214,N04-3012,0,\N,Missing
W14-3906,W12-2108,0,0.0630235,"Missing"
W14-3906,W10-2914,0,0.0620461,"phenomena in electronic communication, finding similar evidence of code switching (Climent et al., 2003; Lee, 2007; Paolillo, 2011). However, these investigations into code switching have largely examined interpersonal communication or settings where the number of participants is limited. In contrast, social media platforms such as Twitter offer individuals the ability to write a text that is decoupled from direct conversation but may be read widely. Twitter enables users to post messages with special markers known as hashtags, which can serve as a side channel to comment on the post itself (Davidov et al., 2010). As a result, multilingual authors have embraced using hashtags from languages other than the language of their post. Consider the following real examples: When code switching, individuals incorporate elements of multiple languages into the same utterance. While code switching has been studied extensively in formal and spoken contexts, its behavior and prevalence remains unexamined in many newer forms of electronic communication. The present study examines code switching in Twitter, focusing on instances where an author writes a post in one language and then includes a hashtag in a second lan"
W14-3906,P12-3005,0,0.0310063,"Missing"
