2001.mtsummit-teach.6,J90-2002,0,0.429101,"Missing"
2001.mtsummit-teach.6,P91-1022,0,0.243419,"Missing"
2001.mtsummit-teach.6,J93-1004,0,0.288306,"Missing"
2001.mtsummit-teach.6,J93-1006,0,0.156197,"Missing"
2002.eamt-1.6,2001.mtsummit-teach.6,1,0.837771,"component to this section of a course on MT, whether this be to computational linguists, translators or language students. This paper presents a method of assessing the level of understanding of the issues pertaining to complex transfer for nal year undergraduates studying a degree programme in Computational Linguistics. The intention is that this methodology may contribute to a suite of exercises which may be used by other instructors in Machine Translation. 1 Introduction Courses on Machine Translation (MT) need to be tailored to dierent sets of students with differing skills and demands (Kenny & Way, 2001). Any contemporary course on MT ought to equip students with at least a super cial knowledge of the dierences between rule-based and statistical MT direct and indirect approaches and transfer-based and interlingual systems. With regard to this latter distinction, the issue of complex transfer is an integral component to this section of a course on MT, whether this be to computational linguists, translators or language students. Given the diering backgrounds of these sets of students, the course material, methods of teaching and assessment would all dier, but the notion of how MT systems cop"
2002.eamt-1.6,2001.mtsummit-teach.7,0,0.0679795,"Missing"
2003.mtsummit-papers.18,2003.eamt-1.1,0,0.333794,"Missing"
2003.mtsummit-papers.18,bernth-mccord-2000-effect,0,0.133138,"Missing"
2003.mtsummit-papers.18,2003.eamt-1.3,0,0.116079,"Missing"
2003.mtsummit-papers.18,gough-etal-2002-example,1,0.366666,"Missing"
2003.mtsummit-papers.18,habash-dorr-2002-handling,0,0.0656091,"Missing"
2003.mtsummit-papers.18,1995.tmi-1.12,0,0.821804,"Missing"
2003.mtsummit-papers.18,2003.eamt-1.13,0,0.0698594,"Missing"
2003.mtsummit-papers.18,2003.mtsummit-papers.18,1,0.0512624,"Missing"
2003.mtsummit-papers.18,J03-3004,1,0.553567,"Missing"
2003.mtsummit-papers.18,A00-1006,0,\N,Missing
2003.mtsummit-papers.18,2001.mtsummit-papers.27,0,\N,Missing
2003.mtsummit-papers.22,W96-0214,0,\N,Missing
2003.mtsummit-papers.22,C00-2092,0,\N,Missing
2003.mtsummit-papers.22,P98-1022,0,\N,Missing
2003.mtsummit-papers.22,C98-1022,0,\N,Missing
2003.mtsummit-tttt.8,J90-2002,0,0.293967,"Missing"
2003.mtsummit-tttt.8,P91-1022,0,0.0367196,"Missing"
2003.mtsummit-tttt.8,1992.tmi-1.8,0,0.192144,"Missing"
2003.mtsummit-tttt.8,1999.tmi-1.3,0,0.0493922,"Missing"
2003.mtsummit-tttt.8,1992.tmi-1.12,0,0.200499,"Missing"
2003.mtsummit-tttt.8,J93-1004,0,0.0599215,"Missing"
2003.mtsummit-tttt.8,1999.tc-1.8,0,0.0317475,"Missing"
2003.mtsummit-tttt.8,C92-2101,0,0.0624118,"Missing"
2003.mtsummit-tttt.8,J93-1006,0,0.113065,"Missing"
2003.mtsummit-tttt.8,2001.mtsummit-teach.6,1,0.867681,"Missing"
2003.mtsummit-tttt.8,J03-3002,0,0.0205389,"Missing"
2003.mtsummit-tttt.8,soricut-etal-2002-using,0,0.0248569,"Missing"
2003.mtsummit-tttt.8,J03-3004,1,0.876188,"Missing"
2003.mtsummit-tttt.8,P01-1067,0,0.0603978,"Missing"
2004.eamt-1.9,1995.tmi-1.12,0,0.164273,"Missing"
2004.eamt-1.9,P02-1040,0,0.0744702,"Missing"
2004.eamt-1.9,2003.eamt-1.13,0,0.167409,"Missing"
2004.eamt-1.9,2003.mtsummit-papers.18,1,0.632049,"Missing"
2004.eamt-1.9,2003.mtsummit-papers.51,0,0.145538,"Missing"
2004.eamt-1.9,J03-3004,1,0.879103,"Missing"
2004.eamt-1.9,2003.eamt-1.1,0,0.0211987,"Missing"
2004.eamt-1.9,2003.eamt-1.3,0,0.0837056,"Missing"
2004.eamt-1.9,2003.mtsummit-papers.9,0,0.0511002,"Missing"
2004.eamt-1.9,gough-etal-2002-example,1,0.821857,"Missing"
2004.eamt-1.9,2001.mtsummit-papers.27,0,0.0394223,"Missing"
2004.eamt-1.9,A00-1006,0,\N,Missing
2004.tmi-1.11,1992.tmi-1.12,0,0.130597,"Missing"
2004.tmi-1.11,gough-etal-2002-example,1,0.879341,"Missing"
2004.tmi-1.11,2003.mtsummit-papers.18,1,0.908571,"Missing"
2004.tmi-1.11,C92-2101,0,0.130848,"Missing"
2004.tmi-1.11,P98-2141,0,0.023816,"Missing"
2004.tmi-1.11,1999.tmi-1.7,0,0.0305429,"Missing"
2004.tmi-1.11,J03-1002,0,0.019782,"Missing"
2004.tmi-1.11,1993.tmi-1.5,0,0.0989743,"Missing"
2004.tmi-1.11,P91-1024,0,0.144273,"Missing"
2004.tmi-1.11,2003.mtsummit-papers.51,0,0.0907438,"Missing"
2004.tmi-1.11,J03-3004,1,0.108438,"Missing"
2004.tmi-1.11,2004.eamt-1.9,1,0.886556,"Missing"
2004.tmi-1.11,oz-cicekli-1998-ordering,0,\N,Missing
2004.tmi-1.11,C98-2136,0,\N,Missing
2005.eamt-1.26,P04-1041,1,0.881757,"Missing"
2005.eamt-1.26,P03-1046,0,0.115152,"Missing"
2005.eamt-1.26,H94-1020,0,0.395048,"Missing"
2005.eamt-1.26,P02-1040,0,0.102728,"Missing"
2005.eamt-1.26,2001.mtsummit-teach.7,0,0.290509,"Missing"
2005.eamt-1.26,P02-1035,0,0.027175,"Missing"
2005.eamt-1.26,J03-3004,1,0.836852,"Missing"
2005.mtsummit-ebmt.14,1999.tmi-1.3,0,0.0449637,"; 2. Determining the sub-sentential translation links in those retrieved examples; 3. Recombining relevant parts of the target translation links to derive the translation. Figure 1: ELAN user interface Searching for the best matches involves determining a similarity metric based on word occurrences and part-of-speech labels, generalised templates and bilingual dictionaries. The recombination process depends on the nature of the examples used in the first place: from aligning phrase-structure (sub)trees (Hearne & Way, 2003) or dependency trees (Watanabe et al., 2003), to the use of placeables (Brown, 1999) as indicators of chunk boundaries. Another method—and the one used in the EBMT system in our experiments—is to use a set of closedclass words to segment aligned source and target sentences and to derive an additional set of lexical and phrasal resources. (Way & Gough, 2003), (Gough & Way, 2004a) and (Gough & Way, 2004b) base their work on the ‘Marker Hypothesis’ (Green, 1979), a universal psycholinguistic constraint which posits that languages are ‘marked’ for syntactic structure at surface level by a closed set of specific lexemes and morphemes. In a pre-processing stage, (Gough & Way, 2004b"
2005.mtsummit-ebmt.14,2004.eamt-1.9,1,0.808557,"and away@computing.dcu.ie Abstract Users of sign languages are often forced to use a language in which they have reduced competence simply because documentation in their preferred format is not available. While some research exists on translating between natural and sign languages, we present here what we believe to be the first attempt to tackle this problem using an example-based (EBMT) approach. Having obtained a set of English–Dutch Sign Language examples, we employ an approach to EBMT using the ‘Marker Hypothesis’ (Green, 1979), analogous to the successful system of (Way & Gough, 2003), (Gough & Way, 2004a) and (Gough & Way, 2004b). In a set of experiments, we show that encouragingly good translation quality may be obtained using such an approach. Key-words: Example-based machine translation, sign languages, Marker Hypothesis, ECHO corpus. 1 Introduction Just like speakers of a less widely spoken language are often not catered for properly with respect to the provision of documentation in their preferred language, users of sign languages (SLs) observe similar restrictions. Having to read documents in the lingua franca often causes them some hindrance. This is because a system of ‘oralism’ (the"
2005.mtsummit-ebmt.14,2004.tmi-1.11,1,0.835659,"and away@computing.dcu.ie Abstract Users of sign languages are often forced to use a language in which they have reduced competence simply because documentation in their preferred format is not available. While some research exists on translating between natural and sign languages, we present here what we believe to be the first attempt to tackle this problem using an example-based (EBMT) approach. Having obtained a set of English–Dutch Sign Language examples, we employ an approach to EBMT using the ‘Marker Hypothesis’ (Green, 1979), analogous to the successful system of (Way & Gough, 2003), (Gough & Way, 2004a) and (Gough & Way, 2004b). In a set of experiments, we show that encouragingly good translation quality may be obtained using such an approach. Key-words: Example-based machine translation, sign languages, Marker Hypothesis, ECHO corpus. 1 Introduction Just like speakers of a less widely spoken language are often not catered for properly with respect to the provision of documentation in their preferred language, users of sign languages (SLs) observe similar restrictions. Having to read documents in the lingua franca often causes them some hindrance. This is because a system of ‘oralism’ (the"
2005.mtsummit-ebmt.14,2003.mtsummit-papers.22,1,0.825934,"ions: 1. Searching the source side of the bitext for ‘close’ matches and their translations; 2. Determining the sub-sentential translation links in those retrieved examples; 3. Recombining relevant parts of the target translation links to derive the translation. Figure 1: ELAN user interface Searching for the best matches involves determining a similarity metric based on word occurrences and part-of-speech labels, generalised templates and bilingual dictionaries. The recombination process depends on the nature of the examples used in the first place: from aligning phrase-structure (sub)trees (Hearne & Way, 2003) or dependency trees (Watanabe et al., 2003), to the use of placeables (Brown, 1999) as indicators of chunk boundaries. Another method—and the one used in the EBMT system in our experiments—is to use a set of closedclass words to segment aligned source and target sentences and to derive an additional set of lexical and phrasal resources. (Way & Gough, 2003), (Gough & Way, 2004a) and (Gough & Way, 2004b) base their work on the ‘Marker Hypothesis’ (Green, 1979), a universal psycholinguistic constraint which posits that languages are ‘marked’ for syntactic structure at surface level by a closed s"
2005.mtsummit-ebmt.14,2004.tmi-1.7,0,0.110567,"main at approximately that of a ten year old (Holt, 1991). A small body of work has attempted to alleviate the situation for SL users by developing machine translation (MT) systems capable of translating texts written in natural languages into various SLs. This field of SLMT is still in its infancy with research into the area dating back approximately ten years. Many of the systems proposed to date are rule-based systems, based on transfer approaches (Grieve-Smith, 1999), interlingual systems (e.g. the Zardoz system, (Veale et al., 1998)), or hybrid models where these approaches are combined (Huenerfauth, 2004, 2005). On a rather smaller 109 scale, corpus-based approaches have also been proposed (Bauer et al., 1999). Example-Based MT (EBMT) has been around for over 20 years now, from the seminal paper of (Nagao, 1984) to the more recent collection of (Carl & Way, 2003) and beyond. However, as far as we are aware, no previous approaches to SL translation have used such a method. In the medium to long term, our main goal is to develop an EBMT system for the language pair English–Irish Sign Language (ISL), in both directions. However, at this early stage of the project no ISL corpus is available, thou"
2005.mtsummit-ebmt.14,2002.tmi-papers.12,0,0.160427,"Missing"
2005.mtsummit-ebmt.14,zhao-etal-2000-machine,0,0.361218,"l orthography (written annotation of the sign language). Transfer systems have been developed by: • (Grieve-Smith, 1999), who modelled a system for English–ASL using the limited domain of Albuquerque weather reports; • (Marshall & S´ af´ ar, 2002), (S´ af´ ar & Marshall, 2002), whose English–ASL system is semantically driven and uses HPSG semantic feature structures and Discourse Representation Structures to represent the internal structure of linguistic objects; • (Van Zijl & Barker, 2003), who proposed a system for English–South African Sign Language. In terms of Interlingual approaches: • (Zhao et al., 2000) developed an English–ASL system that uses synchronised tree adjoining grammars; • (Veale et al., 1998) developed the Zardoz system for translating English into ISL, ASL and Japanese Sign Language. In addition, (Huenerfauth, 2004, 2005) has proposed a hybrid multi-path system where English is translated into ASL using a combination of an interlingua, transfer methods and direct methods. This work focuses in particular on models for classifier predicates. Systems translating from sign language into written oral-language text have also been developed, one such system being that of (Bauer et al.,"
2005.mtsummit-ebmt.14,P05-2007,0,\N,Missing
2005.mtsummit-papers.38,P04-1041,1,0.867266,"Missing"
2005.mtsummit-papers.38,P03-1046,0,0.147484,"ites In TransBooster, it is essential to be able to distinguish between required elements and optional material, as adjuncts can safely be omitted from the simplified string that we submit to the MT system. This can clearly be seen in (7), where the apposition a long-time rival can freely be omitted. This simple method considerably reduces the complexity of the source strings. More complex cases involve the replacement of constituents by syntactically similar material (see next section). The procedure used for argument/adjunct location is an adapted version of Hockenmaier’s algorithm for CCG (Hockenmaier, 2003). The nodes we label as arguments include all the nodes Hockenmaier labels as arguments together with some of the nodes (e.g. VP children of S where S is headed by a modal verb; quantitative adjectives) which she describes as adjuncts. In ongoing research, we wish to compare this procedure with the annotation of Penn-II nodes with LFG functional information (Cahill et al., 2004). 3.1.4 Skeletons and Substitution Variables The simplified source strings such as (5) are submitted to the MT systems, and these output target strings of the form TL in (8): (8) TL: [ARG01 ] [ADJ01 ]. . . [ARG0L ] [ADJ"
2005.mtsummit-papers.38,2005.eamt-1.26,1,0.640124,"Missing"
2005.mtsummit-papers.38,P02-1040,0,0.100158,"Missing"
2005.mtsummit-papers.38,2001.mtsummit-teach.7,0,0.032325,"Missing"
2005.mtsummit-papers.38,2004.tmi-1.9,0,0.308159,"Missing"
2006.amta-papers.13,J93-2003,0,0.00545117,"into chunks, choose the best chunk translation and recompose the translated chunks in output. 2 3 Related Research (Frederking and Nirenburg, 1994) produced the first MEMT system (Pangloss) by combining the output sentences of three different MT engines, developed in house. In order to calculate a consensus translation, the authors rely on their knowledge of the inner workings of the engines. In (Nomoto, 2004), by contrast, the MT engines are treated as black boxes. He presents a number of statistical confidence models, based on a large array of language models and the IBM1 translation model (Brown et al., 1993) to select the best output string at sentence level. Most of the other recent approaches to MEMT rely on word alignment techniques in the translation hypotheses to infer the units for comparison between the MT systems. (Bangalore et al., 2001) produces alignments between the different hypotheses using edit distance (Levenshtein, 1965). For each aligned unit, a winner is calculated by majority voting and a N-gram language model. Since edit distance only focuses on insertions, deletions and substitutions, the model cannot handle translation hypotheses with a significantly different word order. ("
2006.amta-papers.13,A00-2018,0,0.0922949,"e presented in (Mellebeek et al., 2005). In the following subsections, we will explain the decomposition of the input sentence, the translation of the input chunks, the calculation of the best output chunk and the composition of the output sentence. We will also demonstrate the approach with a worked example. 3.1 Decomposition of Input Our approach presupposes the existence of some sort of syntactic analysis of the input sentence. We report experiments on human parse-annotated sentences (the Penn II Treebank (Marcus et al., 1994)) and on the output of two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2002) in section 4. In a first step, the input sentence is decomposed into a number of syntactically meaningful chunks as in (1). (1) [ARG1 ] [ADJ1 ]. . . [ARGL ] [ADJl ] pivot [ARGL+1 ] [ADJl+1 ]. . . [ARGL+R ] [ADJl+r ] where pivot = the nucleus of the sentence, ARG = 111 2003). The result of this first step on a worked example can be seen in (5). In a next step, we replace the arguments by similar but simpler strings, which we call ‘Substitution Variables’. The purpose of Substitution Variables is: (i) to help to reduce the complexity of the original arguments, which often leads to"
2006.amta-papers.13,A94-1016,0,0.504325,"the Americas give a short overview of the most relevant MEMT techniques. We explain our approach in section 3 and demonstrate it with a worked example. Section 4 contains the description, results and analysis of our experiments. We give avenues for future research in section 5 and summarize our findings in section 6. ously known MEMT proposals operate on MT output for complete input sentences. In the research presented here, we pursue a different approach: we decompose MT input into chunks, choose the best chunk translation and recompose the translated chunks in output. 2 3 Related Research (Frederking and Nirenburg, 1994) produced the first MEMT system (Pangloss) by combining the output sentences of three different MT engines, developed in house. In order to calculate a consensus translation, the authors rely on their knowledge of the inner workings of the engines. In (Nomoto, 2004), by contrast, the MT engines are treated as black boxes. He presents a number of statistical confidence models, based on a large array of language models and the IBM1 translation model (Brown et al., 1993) to select the best output string at sentence level. Most of the other recent approaches to MEMT rely on word alignment techniqu"
2006.amta-papers.13,P03-1046,0,0.0207593,"Missing"
2006.amta-papers.13,P05-3026,0,0.0508826,"to select the best output string at sentence level. Most of the other recent approaches to MEMT rely on word alignment techniques in the translation hypotheses to infer the units for comparison between the MT systems. (Bangalore et al., 2001) produces alignments between the different hypotheses using edit distance (Levenshtein, 1965). For each aligned unit, a winner is calculated by majority voting and a N-gram language model. Since edit distance only focuses on insertions, deletions and substitutions, the model cannot handle translation hypotheses with a significantly different word order. (Jayaraman and Lavie, 2005) try to overcome this problem by introducing a method that can find nonmonotone alignments. They compose a consensus from these alignments by using a language model and confidence score specific to each MT engine. (van Zaanen and Somers, 2005) present ‘Democrat’, a ‘plug-and-play’ MEMT system that relies solely on a simple edit distance-based alignment of the translation hypotheses and does not use additional heuristics to compute the consensus translation. Finally, (Matusov et al., 2006) use well-established techniques from the Statistical MT community to produce alignments of hypotheses base"
2006.amta-papers.13,E06-1005,0,0.0522155,"titutions, the model cannot handle translation hypotheses with a significantly different word order. (Jayaraman and Lavie, 2005) try to overcome this problem by introducing a method that can find nonmonotone alignments. They compose a consensus from these alignments by using a language model and confidence score specific to each MT engine. (van Zaanen and Somers, 2005) present ‘Democrat’, a ‘plug-and-play’ MEMT system that relies solely on a simple edit distance-based alignment of the translation hypotheses and does not use additional heuristics to compute the consensus translation. Finally, (Matusov et al., 2006) use well-established techniques from the Statistical MT community to produce alignments of hypotheses based on pairwise word alignments in an entire corpus instead of at the sentence level. To date, to the best of our knowledge, all previDescription of the Algorithm Given N different MT engines (E1 . . . EN ), the proposed method recursively decomposes an input sentence S into M syntactically meaningful chunks C1 . . . CM . Each chunk Ci (1 &lt; i &lt; M ) is embedded in a minimal necessary context and translated by all MT engines. For each chunk Ci , the translated output candidates Ci1 − CiN are"
2006.amta-papers.13,2005.mtsummit-papers.38,1,0.873363,"Missing"
2006.amta-papers.13,P04-1063,0,0.118926,"ummarize our findings in section 6. ously known MEMT proposals operate on MT output for complete input sentences. In the research presented here, we pursue a different approach: we decompose MT input into chunks, choose the best chunk translation and recompose the translated chunks in output. 2 3 Related Research (Frederking and Nirenburg, 1994) produced the first MEMT system (Pangloss) by combining the output sentences of three different MT engines, developed in house. In order to calculate a consensus translation, the authors rely on their knowledge of the inner workings of the engines. In (Nomoto, 2004), by contrast, the MT engines are treated as black boxes. He presents a number of statistical confidence models, based on a large array of language models and the IBM1 translation model (Brown et al., 1993) to select the best output string at sentence level. Most of the other recent approaches to MEMT rely on word alignment techniques in the translation hypotheses to infer the units for comparison between the MT systems. (Bangalore et al., 2001) produces alignments between the different hypotheses using edit distance (Levenshtein, 1965). For each aligned unit, a winner is calculated by majorit"
2006.amta-papers.13,P02-1040,0,0.0717701,"Missing"
2006.amta-papers.13,P98-2186,0,0.0307195,"(English→Spanish) carried out on an 800sentence test set extracted from the PennII Treebank. 1 Introduction In this paper, we present a novel approach to combine the outputs of multiple MT engines into a consensus translation. Multi-Engine Machine Translation (MEMT) is a term coined by (Frederking and Nirenburg, 1994), who were the first to apply the idea of a multiengine approach in Natural Language Processing to Machine Translation (MT). Researchers in other areas of language technology such as Speech Recognition (Fiscus, 1997), Text Categorization (Larkey and Croft, 1996) and POS Tagging (Roth and Zelenko, 1998) have also experimented with multisystem approaches. Since then, several researchers in the MT community have come up with different techniques to calculate consensus translations from multiple MT engines (cf. section 2). All these previously proposed techniques share one important characteristic: they translate the entire input sentence as is and operate on the resulting target language sentences to calculate a consensus output. Their main difference lies in the method they use to compute word alignments between the multiple output sentences. In contrast to previous MEMT approaches, the techn"
2006.amta-papers.13,2003.mtsummit-papers.51,0,0.061623,"Missing"
2006.amta-papers.13,2005.mtsummit-papers.23,0,0.427265,"Missing"
2006.amta-papers.13,2004.tmi-1.9,0,0.10647,"Missing"
2006.amta-papers.13,P95-1037,0,0.0133401,"d leaving out the adjuncts in (1), we obtain the skeleton in (2) Input Decomposition ... C1 E1 C1_1 C1_N CM ... ... EN CM_1 CM_N Selection C1_best ... CM_best Composition Output Figure 1: A flow chart of the entire MEMT system, with Ci the ith input chunk (1 &lt; i &lt; M ), Ej the j th MT engine (1 &lt; j &lt; N ) and Ci j the translation of Ci by Ej . argument, ADJ = adjunct, {l,r} = number of ADJs to left/right of pivot, and {L,R} = number of ARGs to left/right of pivot. In order to determine the pivot, we compute the head of the local tree by adapting the headlexicalised grammar annotation scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of the head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. The procedure used for argument/adjunct identification is an adapted version of Hockenmaier’s algorithm for CCG (Hockenmaier, (2) [VARG1 ] . . . [VARGL ] pivot [VARGL+1 ] . . . [VARGL+R ] where VARGi is the simpler string substituting ARGi By matching the previously established translations of the Substitution Variables VARGi (1 &lt;= i &lt;= L + R) in the translation of the skeleton in (2), we are able to (i) extract the translation of t"
2006.amta-papers.13,H94-1020,0,0.0107646,"get and the final composition of the output are based on the TransBooster architecture presented in (Mellebeek et al., 2005). In the following subsections, we will explain the decomposition of the input sentence, the translation of the input chunks, the calculation of the best output chunk and the composition of the output sentence. We will also demonstrate the approach with a worked example. 3.1 Decomposition of Input Our approach presupposes the existence of some sort of syntactic analysis of the input sentence. We report experiments on human parse-annotated sentences (the Penn II Treebank (Marcus et al., 1994)) and on the output of two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2002) in section 4. In a first step, the input sentence is decomposed into a number of syntactically meaningful chunks as in (1). (1) [ARG1 ] [ADJ1 ]. . . [ARGL ] [ADJl ] pivot [ARGL+1 ] [ADJl+1 ]. . . [ARGL+R ] [ADJl+r ] where pivot = the nucleus of the sentence, ARG = 111 2003). The result of this first step on a worked example can be seen in (5). In a next step, we replace the arguments by similar but simpler strings, which we call ‘Substitution Variables’. The purpose of Substitution Variables is: (i) t"
2006.amta-papers.13,2005.eamt-1.20,0,\N,Missing
2006.amta-papers.13,C98-2181,0,\N,Missing
2006.amta-papers.17,1999.tmi-1.3,0,0.0308549,"selección. 2. Determining the sub-sentential translation links in those retrieved examples. 3. Recombining relevant parts of the target translation links to derive the translation. In order to determine a similarity metric during the search for relevant matches, word co-occurrence, part-of-speech labels, generalised templates and bilingual dictionaries are often used. The recombination process depends on the nature of the examples used in the first place, which may include aligning phrase-structure trees (Hearne and Way, 2003) or dependency trees (Watanabe et al., 2003), or using placeables (Brown, 1999) as indicators of chunk boundaries. 2.1 Taking into account marker tag information (label, and relative sentence position), and lexical similarity (via mutual information), the marker chunks in (3) are automatically generated from the marker-tagged strings in (2): (3) a. <PRON> You click : <PRON> Usted cliquea b. <PREP> on the red button : <PREP> en el botón rojo c. <PREP> to view : <PREP> para ver d. <DET> the effect : <DET> el efecto e. <PREP> of the selection : <PREP> de la selección Marker-based EBMT One approach in EBMT is to use a set of closedclass words to segment aligned source and ta"
2006.amta-papers.17,2004.tmi-1.11,1,0.848034,"ce of a marker word (e.g. determiners, quantifiers, conjunctions etc.), and together with cognate matches and mutual information scores, aligned marker chunks are derived. In order to describe this resource creation in more detail, consider the English–Spanish example in (1): (1) You click on the red button to view the effect of the selection. -> Usted cliquea en el botón rojo para ver el efecto de la selección. In our experiments our marker set consisted of determiners, prepositions, conjunctions, personal pronouns, possessive pronouns, quantifiers and wh-adverbs, following (Gough, 2005; and Gough and Way, 2004). We also made use of auxiliary verbs, such as has and is in English and their Spanish counterparts ha and es, in addition to punctuation, which acted as chunk-final, rather than chunk-initial markers. 3 TransBooster: Architecture TransBooster uses a chunking algorithm to divide input strings into smaller and simpler constituents, sends those constituents in a minimal necessary context to an MT system and recomposes the MT output chunks to obtain the overall translation of the original input string. 149 Our approach presupposes the existence of some sort of syntactic analysis of the input sent"
2006.amta-papers.17,2003.mtsummit-papers.22,1,0.837819,"cliquea <PREP> en <DET> el botón rojo <PREP> para ver <DET> el efecto <PREP> de <DET> la selección. 2. Determining the sub-sentential translation links in those retrieved examples. 3. Recombining relevant parts of the target translation links to derive the translation. In order to determine a similarity metric during the search for relevant matches, word co-occurrence, part-of-speech labels, generalised templates and bilingual dictionaries are often used. The recombination process depends on the nature of the examples used in the first place, which may include aligning phrase-structure trees (Hearne and Way, 2003) or dependency trees (Watanabe et al., 2003), or using placeables (Brown, 1999) as indicators of chunk boundaries. 2.1 Taking into account marker tag information (label, and relative sentence position), and lexical similarity (via mutual information), the marker chunks in (3) are automatically generated from the marker-tagged strings in (2): (3) a. <PRON> You click : <PRON> Usted cliquea b. <PREP> on the red button : <PREP> en el botón rojo c. <PREP> to view : <PREP> para ver d. <DET> the effect : <DET> el efecto e. <PREP> of the selection : <PREP> de la selección Marker-based EBMT One approac"
2006.amta-papers.17,P03-1046,0,0.0197715,"tation scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of this head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. In the case of the example sentence (4), the pivot is likes. During the decomposition, it is essential to be able to distinguish between arguments (required elements) and adjuncts (optional material), as adjuncts can safely be omitted from the simplified string that we submit to the MT system. The procedure used for argument/adjunct location is an adapted version of Hockenmaier's algorithm for CCG (Hockenmaier, 2003). The result of this first step on the example sentence (4) can be seen in (7): (7) [The chairman, a longtime rival of Bill Gates,]ARG1 [likes]pivot [fast and confidential deals]ARG2 . 3.2 Skeletons and Substitution Variables In the next step, we replace the arguments by similar but simpler strings, which we call ‘Substitution Variables’. The purpose of Substitution Variables is: (i) to help to reduce the complexity of the original arguments, which often leads to an improved translation of the pivot; (ii) to help keep track of the location of the translation of the arguments in target. In choo"
2006.amta-papers.17,koen-2004-pharaoh,0,0.0182006,"to determine subsentential links between sentence pairs. In addition to these chunk alignments, we used statistical techniques to extract a high quality word-level lexicon (which in turn was used during the chunk alignment process). Following the refined alignment method of (Och and Ney, 2003), we used the GIZA++ statistical word alignment tool2 to perform source-target and target-source word alignment. The resulting ‘refined’ word alignment set was then passed along with the chunk database to the system decoder (for the results reported in this paper we used the Pharaoh phrase-based decoder (Koehn, 2004)). For training the EBMT system we made use of a subsection of the English-Spanish section of the Europarl corpus (Koehn, 2005). The corpus was filtered based on sentence length (maximum sentence length set at 40 words for Spanish and English) and relative sentence length ratio (a relative sentence length ratio of 1.5 was used), resulting in 958K English-Spanish sentence pairs. For testing purposes two sets of data were used, each consisting of 800 English sentences. The first set was randomly extracted from section 23 of the WSJ section of the Penn II Treebank3; the second set consists of ran"
2006.amta-papers.17,2005.mtsummit-papers.11,0,0.0159209,"to extract a high quality word-level lexicon (which in turn was used during the chunk alignment process). Following the refined alignment method of (Och and Ney, 2003), we used the GIZA++ statistical word alignment tool2 to perform source-target and target-source word alignment. The resulting ‘refined’ word alignment set was then passed along with the chunk database to the system decoder (for the results reported in this paper we used the Pharaoh phrase-based decoder (Koehn, 2004)). For training the EBMT system we made use of a subsection of the English-Spanish section of the Europarl corpus (Koehn, 2005). The corpus was filtered based on sentence length (maximum sentence length set at 40 words for Spanish and English) and relative sentence length ratio (a relative sentence length ratio of 1.5 was used), resulting in 958K English-Spanish sentence pairs. For testing purposes two sets of data were used, each consisting of 800 English sentences. The first set was randomly extracted from section 23 of the WSJ section of the Penn II Treebank3; the second set consists of randomly extracted sentences from the test section of the Europarl corpus, which had been parsed with (Bikel, 2002). We decided to"
2006.amta-papers.17,N03-1017,0,0.0242043,"Missing"
2006.amta-papers.17,P95-1037,0,0.0770656,"entence is decomposed into a number of syntactically meaningful chunks as in (6): (6) [ARG1] [ADJ1]. . . [ARGL] [ADJl] pivot [ARGL+1] [ADJl+1]. . . [ARGL+R] [ADJl+r] where pivot = the nucleus of the sentence, ARG = argument, ADJ = adjunct, {l,r} = number of ADJs to left/right of pivot, and {L,R} = number of ARGs to left/right of pivot. The pivot is the part of the string that must remain unaltered during decomposition in order to avoid an incorrect translation. In order to determine the pivot, we compute the head of the local tree by adapting the head-lexicalised grammar annotation scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of this head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. In the case of the example sentence (4), the pivot is likes. During the decomposition, it is essential to be able to distinguish between arguments (required elements) and adjuncts (optional material), as adjuncts can safely be omitted from the simplified string that we submit to the MT system. The procedure used for argument/adjunct location is an adapted version of Hockenmaier's algorithm for CCG (Hockenmaier, 2003). The result of"
2006.amta-papers.17,H94-1020,0,0.0544782,"h counterparts ha and es, in addition to punctuation, which acted as chunk-final, rather than chunk-initial markers. 3 TransBooster: Architecture TransBooster uses a chunking algorithm to divide input strings into smaller and simpler constituents, sends those constituents in a minimal necessary context to an MT system and recomposes the MT output chunks to obtain the overall translation of the original input string. 149 Our approach presupposes the existence of some sort of syntactic analysis of the input sentence. We report experiments on human parseannotated sentences (the Penn II Treebank (Marcus et al., 1994)) and on the output of a state-of-the-art statistical parser (Bikel, 2002) in Section 5. Essentially, each TransBooster run from a parsed input string to a translated output string consists of the following 5 steps. 1. Finding the Pivot. 2. Locating Arguments and Adjuncts (‘Satellites’) in the source language. 3. Creating and Translating Skeletons and Substitution Variables. 4. Translating Satellites. 5. Combining the translation of Satellites into the output string. We briefly explain each of these steps by processing the following simple example sentence: (4) The chairman, a long-time rival"
2006.amta-papers.17,2005.mtsummit-papers.38,1,0.895422,"Missing"
2006.amta-papers.17,2005.eamt-1.26,1,0.895742,"Missing"
2006.amta-papers.17,2006.eamt-1.24,1,0.761114,"Missing"
2006.amta-papers.17,J03-1002,0,\N,Missing
2006.amta-papers.26,2005.mtsummit-osmtw.2,1,0.884828,"Missing"
2006.amta-papers.26,1999.tmi-1.3,0,0.0513231,"out in EBMT. As with SMT, EBMT makes use of a corpus of source–target sententially-aligned examples to automatically extract translation resources. During translation an EBMT system: 1. Searches the source side of the corpus for “close” matches and their equivalent target language translations; 2. Identifies useful source–target fragments contained in those retrieved examples; 3. Recombines relevant target language fragments to derive the final translation of the input sentence. The nature of the examples used in the first place may include using placeables as indicators of chunk boundaries (Brown, 1999) or aligning phrasestructure (sub-)trees (Hearne and Way, 2003) or dependency trees (Watanabe et al., 2003). 3.1 The MaTrEx System The M AT R E X (Machine Translation Using Examples) system used in our experiments is a data-driven MT engine, built following an extremely modular design. It consists of a number of extendible and re-implementable modules, the most important of which are: 233 “The dog brought the newspaper in his mouth” =⇒“Txakurrak egunkaria ahoan zekarren” Txakur-rak egunkari-a aho-an zekarren The-dog ergative-3-s Subject the-newspaper absolutive-3-s Object in-his-mouth inessive"
2006.amta-papers.26,H05-1085,0,0.0117684,"them to represent relevant templates for Turkish. The authors believe that their approach is applicable to many pairs of natural languages, assuming sets of morphologically tagged bilingual examples. There is currently no template implementation in our EBMT system, but we plan to integrate related techniques in the near future and to apply them to Basque. (Al-Onaizan et al., 1999) relates that some transformations of Czech input text, performed with the aim of harmonizing words with equivalents in English, provided a small additional increase in translation quality over basic lemmatization. (Goldwater and McClosky, 2005), also working with English and Czech, compare four different ways to use only morphological information to improve translation: lemmas, pseudowords, modified lemmas and morphemes. In the case of morphemes they propose a modification in the translation model itself to take advantage of morphological information, rather than simply transforming the output. Word truncation, which requires no morphological information at all, was effective but did not perform quite as well as lemmatization. They found that certain tags were more useful when they treated them as discrete words, while others provid"
2006.amta-papers.26,2004.tmi-1.11,1,0.84454,"ved chunk and word alignments. In a preprocessing stage, the aligned source-target sentences are passed in turn to the word alignment, chunking and chunk alignment modules to create our chunk and lexical example databases used during translation. In our experiments we investigated a number of different chunking and alignment strategies which we describe in more detail in what follows. is created at each new occurrence of a marker word, with the restriction that each chunk must contain at least one content (or non-marker) word. In addition to the set of marker words used in the experiments of (Gough and Way, 2004; Groves and Way, 2005), punctuation is also used to segment the aligned sentences – with the punctuation occurring in chunk final, rather than initial, position. 3.2.2 As previously mentioned, Basque is an agglutinative language and morphemes can be used as the basic units of analysis. However, this makes it more difficult to apply the marker-based chunker, described in Section 3.2.1, to Basque. Therefore, as a starting point, we have decided to make use of existing tools developed for Basque. The Ixa group at the University of the Basque Country has developed two syntactic analyzers: • PATRI"
2006.amta-papers.26,J04-2003,0,0.0234737,"noted that Basque is more complex morphologically than German or Czech. In German there are four grammar cases represented by means of inflection, seven in Czech and seventeen for Basque. In (Niessen and Ney, 2001), the authors present work on German-English SMT, where they investigate various type of morphosyntactic restructuring of sentences to harmonize word order in both languages. In particular, they merge German verbs with their detached prefixes, and undo question inversion in both German and English. The results reveal a better exploitation of the bilingual training dataset. Later on (Niessen and Ney, 2004), they annotate a handful of frequent ambiguous German multi word expressions with POS tags, combine idiomatic multi-word expressions into single words, decompose German words into a hierarchical representation using lemmas and morphological tags, and use a MaxEnt model to combine the different levels of representation in the translation model. They conclude that the restructuring operations yielded a large improvement in translation quality, but the morphological decomposition provided only a slight additional benefit. (Lee, 2004) presents a system for Arabic-English translation, where the Ar"
2006.amta-papers.26,J03-1002,0,0.0249993,"ngular). Fitxategi zaharra ezin izan da irakurri ⇓ [ fitxategi zahar ++abs++ms] [ ezin izan da irakurri ] Just yesterday ([The old file] [could not be read]) After some adaptation, the chunks obtained in this manner are actually very comparable to the English chunks obtained with the marker-based chunker (see Section 4 for more details). Moreover, for future work, we also plan to consider adapting the markerbased chunker to Basque. 3.3 Alignment Strategies Word alignment For word/morpheme alignment we used the G IZA ++ statistical word alignment toolkit, and following the “refined” method of (Och and Ney, 2003), extracted a set of high-quality word/morpheme alignments from the original unidirectional alignment sets. These along with the extracted chunk alignments were passed to the translation decoder. Chunk alignment In order to align the chunks obtained by the chunking procedures described in Section 3.2, we make use of a dynamic programming “edit-distance style” alignment algorithm. In the following, a denotes an alignment between a target sequence e and a source sequence f , with I = |e |and J = |f |. Given two sequences of chunks, we are looking for the most likely alignment a ˆ: a ˆ = argmax P"
2006.amta-papers.26,W05-0833,1,0.933895,"ignments. In a preprocessing stage, the aligned source-target sentences are passed in turn to the word alignment, chunking and chunk alignment modules to create our chunk and lexical example databases used during translation. In our experiments we investigated a number of different chunking and alignment strategies which we describe in more detail in what follows. is created at each new occurrence of a marker word, with the restriction that each chunk must contain at least one content (or non-marker) word. In addition to the set of marker words used in the experiments of (Gough and Way, 2004; Groves and Way, 2005), punctuation is also used to segment the aligned sentences – with the punctuation occurring in chunk final, rather than initial, position. 3.2.2 As previously mentioned, Basque is an agglutinative language and morphemes can be used as the basic units of analysis. However, this makes it more difficult to apply the marker-based chunker, described in Section 3.2.1, to Basque. Therefore, as a starting point, we have decided to make use of existing tools developed for Basque. The Ixa group at the University of the Basque Country has developed two syntactic analyzers: • PATRIXA is an unification gr"
2006.amta-papers.26,J04-4002,0,0.0313595,"Missing"
2006.amta-papers.26,2003.mtsummit-papers.22,1,0.820734,"of source–target sententially-aligned examples to automatically extract translation resources. During translation an EBMT system: 1. Searches the source side of the corpus for “close” matches and their equivalent target language translations; 2. Identifies useful source–target fragments contained in those retrieved examples; 3. Recombines relevant target language fragments to derive the final translation of the input sentence. The nature of the examples used in the first place may include using placeables as indicators of chunk boundaries (Brown, 1999) or aligning phrasestructure (sub-)trees (Hearne and Way, 2003) or dependency trees (Watanabe et al., 2003). 3.1 The MaTrEx System The M AT R E X (Machine Translation Using Examples) system used in our experiments is a data-driven MT engine, built following an extremely modular design. It consists of a number of extendible and re-implementable modules, the most important of which are: 233 “The dog brought the newspaper in his mouth” =⇒“Txakurrak egunkaria ahoan zekarren” Txakur-rak egunkari-a aho-an zekarren The-dog ergative-3-s Subject the-newspaper absolutive-3-s Object in-his-mouth inessive-3-s Modifier brought egunkari-a zekarren zekarren zekarren egu"
2006.amta-papers.26,N03-1017,0,0.0104338,"The same kind of model applies to cognates. In the case of chunk labels, a simple matching algorithm is used. It is possible to combine several sources of knowledge in a log-linear framework, in the following manner: X logP (ei |fj ) = λk logPk (ei |fj ) − logZ, where Pk (.) represents a given source of knowledge, λk the associated weight parameter and Z a normalization parameter. Integrating SMT data Whilst EBMT has always made use of both lexical and phrasal information (Nagao, 1984), it is only recently that SMT has moved towards the use of phrases in their translation models and decoders (Koehn et al., 2003; Koehn, 2004). It has, therefore, become harder than ever to identify the differences between these two datadriven approaches (Way and Gough, 2005). However, despite the convergence of the two paradigms, recent research (Groves and Way, 2005; Groves and Way, 2006) has shown that by combining elements from EBMT and SMT to create hybrid data-driven systems capable of outperforming the baseline systems from which they are derived. Therefore, SMT phrasal alignments are also added to the aligned chunks extracted by the chunk alignment module, in order to produce higher quality translations. 3.4 De"
2006.amta-papers.26,koen-2004-pharaoh,0,0.342038,"dedicated tools developed at the University of the Basque Country while investigating the application of our marker-based chunker to Basque. The chunks are then aligned thanks to a dynamic programming algorithm which is similar to an edit-distance algorithm while allowing for block movements (Leusch et al., 2006). This aligner also relies on relationships between chunks, which we compute in several ways. We present a set of Basque to English translation experiments, evaluated on a large corpus consisting of software manuals. We show a significant improvement over state-of-the-art SMT systems (Koehn, 2004) according to several common automatic evaluation metrics. In addition, some manual evaluations have been conducted to assess the quality of our extracted aligned resources, and we find that the high quality of our resources con1 See also http://matxin.sourceforge.net and http://www.opentrad.org. 232 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 232-241, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas tributes positively to the overall translation quality. This paper is organized as follows: In section"
2006.amta-papers.26,N04-4015,0,0.0348297,"ation of the bilingual training dataset. Later on (Niessen and Ney, 2004), they annotate a handful of frequent ambiguous German multi word expressions with POS tags, combine idiomatic multi-word expressions into single words, decompose German words into a hierarchical representation using lemmas and morphological tags, and use a MaxEnt model to combine the different levels of representation in the translation model. They conclude that the restructuring operations yielded a large improvement in translation quality, but the morphological decomposition provided only a slight additional benefit. (Lee, 2004) presents a system for Arabic-English translation, where the Arabic sentences have been presegmented into stems and affixes. Using a phrase-based translation model, Lee concludes that morphological analysis helped only for training corpora up to 350, 000 parallel sentences and that it was not so helpful with a larger corpus consisting of 3.3 million sentences. Note the corpus we used for Basque/English MT is smaller than 350, 000 sentences. (Cicekli and G¨uvenir, 2003) learn translation templates from English-Turkish translation examples. They define a template as an example translation pair w"
2006.amta-papers.26,E06-1031,0,0.228169,"ploits both EBMT and SMT techniques to extract a dataset of aligned chunks. For the extraction of the EBMT data resources, we make use of two different chunking methods. In the case of English, we employ a marker-based chunker based on the Marker Hypothesis (Green, 1979). For Basque, we currently use the dedicated tools developed at the University of the Basque Country while investigating the application of our marker-based chunker to Basque. The chunks are then aligned thanks to a dynamic programming algorithm which is similar to an edit-distance algorithm while allowing for block movements (Leusch et al., 2006). This aligner also relies on relationships between chunks, which we compute in several ways. We present a set of Basque to English translation experiments, evaluated on a large corpus consisting of software manuals. We show a significant improvement over state-of-the-art SMT systems (Koehn, 2004) according to several common automatic evaluation metrics. In addition, some manual evaluations have been conducted to assess the quality of our extracted aligned resources, and we find that the high quality of our resources con1 See also http://matxin.sourceforge.net and http://www.opentrad.org. 232"
2006.amta-papers.26,nevado-etal-2004-translation,0,0.0316322,"Missing"
2006.amta-papers.26,2001.mtsummit-papers.45,0,0.0238173,"T methods to create new, shorter bilingual segments to be included in a TM database. They have performed this task for the Basque-Spanish pair and for other languages pairs such as Catalan-Spanish or English-Spanish and found that the task is much more difficult for Basque. Several other studies have been carried out with (somehow) related languages such as German, Czech or Turkish. However, it has to be noted that Basque is more complex morphologically than German or Czech. In German there are four grammar cases represented by means of inflection, seven in Czech and seventeen for Basque. In (Niessen and Ney, 2001), the authors present work on German-English SMT, where they investigate various type of morphosyntactic restructuring of sentences to harmonize word order in both languages. In particular, they merge German verbs with their detached prefixes, and undo question inversion in both German and English. The results reveal a better exploitation of the bilingual training dataset. Later on (Niessen and Ney, 2004), they annotate a handful of frequent ambiguous German multi word expressions with POS tags, combine idiomatic multi-word expressions into single words, decompose German words into a hierarchi"
2006.amta-papers.26,J93-2003,0,\N,Missing
2006.eamt-1.15,2005.mtsummit-papers.29,0,0.0261809,"f the words in the original input sentence and thus often fails to produce a syntactically well-formed output translation in these cases. Following these observations we re-ranked the output of both the baseline and hybrid EBMT systems, using the same language model as was used in the equivalent PBSMT experiments. (Bangalore et al., 2002) make use of a trigram language model to help select the best translation from multiple candidates, but do not explicitly report on the actual contribution of the language model and deal with outputs from multiple MT engines rather than from a single system. (Aramaki, Kurohashi, Kashioka, & Kato, 2005) use a language model, but only to re-order the words in the final translation produced by their system, rather than during the re-ranking of translation candidates. French–English Results The results from these experiments for French–English are shown in Table 3. Comparing these results to those of the EBMT and H-EBMT systems, we can see that using the language model improves the performance of both the baseline and hybrid ‘statistical EBMT’ systems. These results illustrate how the language model guides the reordering of these word-to-word translations to improve overall translation quality"
2006.eamt-1.15,2004.tmi-1.14,0,0.0287168,"Missing"
2006.eamt-1.15,C02-1134,0,0.0415426,"Missing"
2006.eamt-1.15,J90-2002,0,0.262427,"on, it is segmented into all possible ngrams that might be retrieved from the system’s memories. For each n-gram these resources are searched from maximal context (specific source–target sentence-pairs) to minimal context (word-for-word translation). 4 Statistical MT While EBMT models of translation have since their very inception (Nagao, 1984) incorporated both lexical and phrasal information, it is only quite recently that SMT practitioners have obtained higher translation quality via phrase-based models (e.g. (Koehn, Och, & Marcu, 2003; Och, 2003)) compared to the older word-based systems (Brown et al., 1990, 1993). This inclusion of chunks as well as word alignments has been so successful that PBSMT has become, by some distance, the most dominant approach in MT research today. 4.1 Phrasal Alignment Techniques A number of methods are available in order to extract phrase correspondences from a bilingual training corpus. The most common method is to first perform word alignment using EM methods, such as that performed by Giza++. Following the method of (Och & Ney, 2003), word alignment is performed in both source–target and target–source directions. The intersection of these unidirectional alignmen"
2006.eamt-1.15,J93-2003,0,0.019859,"Missing"
2006.eamt-1.15,1999.tmi-1.3,0,0.0379675,"ial translation links in those retrieved examples; 3. Recombining relevant parts of the target translation links to derive the translation. Searching for the best matches involves determining a similarity metric based on word occurrences and part-of-speech labels, generalised templates and bilingual dictionaries. The recombination process depends on the nature of the examples used in 3 http://www.isi.edu/∼och/Giza++.html the first place, which may include aligning phrase-structure (sub-)trees (Hearne & Way, 2003) or dependency trees (Watanabe, Kurohashi, & Aramaki, 2003), or using placeables (Brown, 1999) as indicators of chunk boundaries. 3.1 Marker-Based EBMT An alternative approach used in the EBMT system used in our experiments (Gough, 2005; Way & Gough, 2005) is to use a set of closed-class words to segment aligned source and target sentences and to derive an additional set of lexical and phrasal resources. This series of research papers is based on the ‘Marker Hypothesis’ (Green, 1979), a universal psycholinguistic constraint which posits that languages are ‘marked’ for syntactic structure at surface level by a closed set of specific lexemes and morphemes. In a pre-processing stage, the"
2006.eamt-1.15,1999.mtsummit-1.92,0,0.0777667,"Missing"
2006.eamt-1.15,P01-1030,0,0.0804448,"Missing"
2006.eamt-1.15,W05-0833,1,0.857945,"system incorporating marker chunks and SMT sub-sentential alignments is capable of outperforming both baseline translation models for French–English translation. In this paper, we show that similar gains are to be had from constructing a hybrid ‘statistical EBMT’ system capable of outperforming the baseline system of (Way & Gough, 2005). Using the Europarl (Koehn, 2005) training and test sets we show that this time around, although all ‘hybrid’ variants of the EBMT system fall short of the quality achieved by the baseline PBSMT system, merging elements of the marker-based and SMT data, as in (Groves & Way, 2005), to create a hybrid ‘example-based SMT’ system, outperforms the baseline SMT and EBMT systems from which it is derived. Furthermore, we provide further evidence in favour of hybrid systems by adding an SMT target language model to all EBMT system variants and demonstrate that this too has a positive effect on translation quality. 1 Introduction Almost all research in MT being carried out today is corpus-based. Within this field, by far the most dominant paradigm is PB∗ This work is supported by an IRCSET Ph.D. Fellowship Award. SMT, but much important work continues to be carried out in EBMT."
2006.eamt-1.15,2003.mtsummit-papers.22,1,0.850671,"source side of the bitext for ‘close’ matches and their translations; 2. Determining the sub-sentential translation links in those retrieved examples; 3. Recombining relevant parts of the target translation links to derive the translation. Searching for the best matches involves determining a similarity metric based on word occurrences and part-of-speech labels, generalised templates and bilingual dictionaries. The recombination process depends on the nature of the examples used in 3 http://www.isi.edu/∼och/Giza++.html the first place, which may include aligning phrase-structure (sub-)trees (Hearne & Way, 2003) or dependency trees (Watanabe, Kurohashi, & Aramaki, 2003), or using placeables (Brown, 1999) as indicators of chunk boundaries. 3.1 Marker-Based EBMT An alternative approach used in the EBMT system used in our experiments (Gough, 2005; Way & Gough, 2005) is to use a set of closed-class words to segment aligned source and target sentences and to derive an additional set of lexical and phrasal resources. This series of research papers is based on the ‘Marker Hypothesis’ (Green, 1979), a universal psycholinguistic constraint which posits that languages are ‘marked’ for syntactic structure at su"
2006.eamt-1.15,koen-2004-pharaoh,0,0.22074,"Missing"
2006.eamt-1.15,2005.mtsummit-papers.11,0,0.630389,"rming a word-based SMT system trained on reasonably large data sets. (Groves & Way, 2005) take this a stage further and demonstrate that while the EBMT system also outperforms a phrase-based SMT (PBSMT) system, a hybrid ‘example-based SMT’ system incorporating marker chunks and SMT sub-sentential alignments is capable of outperforming both baseline translation models for French–English translation. In this paper, we show that similar gains are to be had from constructing a hybrid ‘statistical EBMT’ system capable of outperforming the baseline system of (Way & Gough, 2005). Using the Europarl (Koehn, 2005) training and test sets we show that this time around, although all ‘hybrid’ variants of the EBMT system fall short of the quality achieved by the baseline PBSMT system, merging elements of the marker-based and SMT data, as in (Groves & Way, 2005), to create a hybrid ‘example-based SMT’ system, outperforms the baseline SMT and EBMT systems from which it is derived. Furthermore, we provide further evidence in favour of hybrid systems by adding an SMT target language model to all EBMT system variants and demonstrate that this too has a positive effect on translation quality. 1 Introduction Almos"
2006.eamt-1.15,N03-1017,0,0.0194615,": (5) &lt;LEX> autumn : &lt;LEX> automne When a new sentence is submitted for translation, it is segmented into all possible ngrams that might be retrieved from the system’s memories. For each n-gram these resources are searched from maximal context (specific source–target sentence-pairs) to minimal context (word-for-word translation). 4 Statistical MT While EBMT models of translation have since their very inception (Nagao, 1984) incorporated both lexical and phrasal information, it is only quite recently that SMT practitioners have obtained higher translation quality via phrase-based models (e.g. (Koehn, Och, & Marcu, 2003; Och, 2003)) compared to the older word-based systems (Brown et al., 1990, 1993). This inclusion of chunks as well as word alignments has been so successful that PBSMT has become, by some distance, the most dominant approach in MT research today. 4.1 Phrasal Alignment Techniques A number of methods are available in order to extract phrase correspondences from a bilingual training corpus. The most common method is to first perform word alignment using EM methods, such as that performed by Giza++. Following the method of (Och & Ney, 2003), word alignment is performed in both source–target and t"
2006.eamt-1.15,langlais-simard-2002-merging,0,0.0357817,"Missing"
2006.eamt-1.15,P01-1050,0,0.432972,"Missing"
2006.eamt-1.15,P03-1021,0,0.0170631,"automne When a new sentence is submitted for translation, it is segmented into all possible ngrams that might be retrieved from the system’s memories. For each n-gram these resources are searched from maximal context (specific source–target sentence-pairs) to minimal context (word-for-word translation). 4 Statistical MT While EBMT models of translation have since their very inception (Nagao, 1984) incorporated both lexical and phrasal information, it is only quite recently that SMT practitioners have obtained higher translation quality via phrase-based models (e.g. (Koehn, Och, & Marcu, 2003; Och, 2003)) compared to the older word-based systems (Brown et al., 1990, 1993). This inclusion of chunks as well as word alignments has been so successful that PBSMT has become, by some distance, the most dominant approach in MT research today. 4.1 Phrasal Alignment Techniques A number of methods are available in order to extract phrase correspondences from a bilingual training corpus. The most common method is to first perform word alignment using EM methods, such as that performed by Giza++. Following the method of (Och & Ney, 2003), word alignment is performed in both source–target and target–source"
2006.eamt-1.15,J03-1002,0,0.0308153,"sting, are of rather limited value. Accordingly, in (Groves & Way, 2005), we replicated their experiments using the Pharaoh phrase-based SMT Decoder (Koehn, 2004)1 instead of the word-based ISI ReWrite Decoder.2 In general, in (Groves & Way, 2005) we showed that the baseline phrase-based SMT system still fell short of the quality obtained via EBMT for these evaluation metrics for English–French and French–English. However, when Pharaoh was seeded with the data sets automatically induced by both 1 2 http://www.isi.edu/licensed-sw/pharaoh/ http://www.isi.edu/licensed-sw/rewrite-decoder/ Giza++ (Och & Ney, 2003)3 and the EBMT system of (Way & Gough, 2005), better translation quality results are seen for French–English (0.489 BLEU score) than for the EBMT system per se (0.4611). While (Groves & Way, 2005) show that a hybrid example-based SMT system can outperform both an SMT system and an EBMT system from which it is built, our primary goal in this paper is to see whether a new hybrid model of ‘statistical EBMT’ can similarly outperform the baseline systems. Finally, (Aue et al., 2004) observe that their approach of merging dependency treelets with phrase-based SMT may be considered as an instance of"
2006.eamt-1.15,P02-1040,0,0.101271,"ce length ratio of 1.5. The sentences contained in the various training sets were randomly selected and were therefore not necessarily supersets of each other. For testing, we randomly selected 5000 sentences from the Europarl common test set, again limiting sentence length to 40 words. For this test set, the average sentence length was 20.50 words for French and 18.99 words for English. We performed translation for both French–English and English–French, automatically evaluating translation performance over all systems in terms of Word-Error Rate (WER), Sentence-Error Rate (SER), BLEU score (Papineni, Roukos, Ward, & Zhu, 2002), and Precision and Recall (Turian, Shen, & Melamed, 2003). Our experiments together with their results are described in more detail in the following sections. 5.1 EBMT vs. PBSMT In order to evaluate the performance of PBSMT against our Marker-based EBMT system, we built a baseline PBSMT system using the Pharaoh phrase-based SMT decoder along with the SRI language modeling toolkit.4 The translation model used in the system was created using the phrasal extraction technique as described in section 4.1. Our EBMT system used the Markerbased techniques as described in section 3.1 to create chunks"
2006.eamt-1.15,2003.mtsummit-papers.51,0,0.0380106,"raining sets were randomly selected and were therefore not necessarily supersets of each other. For testing, we randomly selected 5000 sentences from the Europarl common test set, again limiting sentence length to 40 words. For this test set, the average sentence length was 20.50 words for French and 18.99 words for English. We performed translation for both French–English and English–French, automatically evaluating translation performance over all systems in terms of Word-Error Rate (WER), Sentence-Error Rate (SER), BLEU score (Papineni, Roukos, Ward, & Zhu, 2002), and Precision and Recall (Turian, Shen, & Melamed, 2003). Our experiments together with their results are described in more detail in the following sections. 5.1 EBMT vs. PBSMT In order to evaluate the performance of PBSMT against our Marker-based EBMT system, we built a baseline PBSMT system using the Pharaoh phrase-based SMT decoder along with the SRI language modeling toolkit.4 The translation model used in the system was created using the phrasal extraction technique as described in section 4.1. Our EBMT system used the Markerbased techniques as described in section 3.1 to create chunks, generalised templates and lexical resources. 5.1.1 Frenc"
2006.eamt-1.15,C00-2172,0,0.227402,"Missing"
2006.eamt-1.24,2003.mtsummit-papers.6,0,0.0521718,"Missing"
2006.eamt-1.24,P05-1033,0,0.217935,"Missing"
2006.eamt-1.24,P03-1046,0,0.0471658,"tion scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of this head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. In the case of the example sentence (1), the pivot is ‘likes’. During the decomposition, it is essential to be able to distinguish between arguments (required elements) and adjuncts (optional material), as adjuncts can safely be omitted from the simplified string that we submit to the MT system. The procedure used for argument/adjunct location is an adapted version of Hockenmaier’s algorithm for CCG (Hockenmaier, 2003). The result of this first step on a the example sentence (1) can be seen in (4). (4) 3.2 [The chairman, a long-time rival of Bill Gates,]ARG1 [likes]pivot [fast and confidential deals]ARG2 . Skeletons Variables and Substitution In a next step, we replace the arguments by similar but simpler strings, which we call ‘Substitution Variables’. The purpose of Substitution Variables is: (i) to help to reduce the complexity of the original arguments, which often leads to an improved translation of the pivot; (ii) to help keep track of the location of the translation of the arguments in target. In cho"
2006.eamt-1.24,koen-2004-pharaoh,0,0.0643615,"ule-based and statistical MT systems tend to perform better at translating shorter sentences than longer ones. TransBooster decomposes source language sentences into syntactically simpler and shorter chunks, sends the chunks to a baseline MT system and recomposes the translated output into target language sentences. It has already proved successful in experiments with rule-based MT systems (Mellebeek, Khasin, Owczarzak, Van Genabith, & Way, 2005). In this paper we apply the TransBooster wrapper technology to a state-of-the-art phrase-based English → Spanish SMT model constructed with Pharaoh (Koehn, 2004) and we report a statistically significant improvement in BLEU and NIST score. The paper is organised as follows. In section 2, we give a short overview of the most relevant methods that incorporate syntactic knowledge in SMT models. We explain our approach in section 3 and demonstrate it with a worked example. Sections 4 and 5 contain the description, results and analysis of our experiments. We summarize our findings in section 6. 2 Related Research One of the major difficulties SMT faces is its inability to model long-distance dependencies and correct word order for many language pairs. In t"
2006.eamt-1.24,2005.mtsummit-papers.11,0,0.0839355,"ess leads to an improvement in translation quality compared to the original output by Systran in (2), as is shown in (10): (10) 4 El presidente, un rival de largo plazo de Bill Gates, tiene gusto de repartos r´apidos y confidenciales. Experimental Setup For our experiments, the phrase-based SMT system (English → Spanish) was constructed using the Pharaoh phrase-based SMT decoder, and the SRI Language Modeling toolkit.1 We used an interpolated trigram language model with Kneser-Ney discounting. The data used to train the system was taken from the English-Spanish section of the Europarl corpus (Koehn, 2005). From this data, 501K sentence pairs were randomly extracted from the designated training section of the corpus and lowercased. Sentence length was limited to a maximum of 40 words for both Spanish and English, 1 http://www.speech.sri.com/projects/srilm/ with sentence pairs having a maximum relative sentence length ratio of 1.5. From this data we used the method of (Och & Ney, 2003) to extract phrase correspondences from GIZA++ word alignments. For testing purposes two sets of data were used, each consisting of 800 English sentences. The first set was randomly extracted from section 23 of the"
2006.eamt-1.24,N03-1017,0,0.0303935,"Missing"
2006.eamt-1.24,P95-1037,0,0.101654,"e is decomposed into a number of syntactically meaningful chunks as in (3). (3) [ARG1 ] [ADJ1 ]. . . [ARGL ] [ADJl ] pivot [ARGL+1 ] [ADJl+1 ]. . . [ARGL+R ] [ADJl+r ] where pivot = the nucleus of the sentence, ARG = argument, ADJ = adjunct, {l,r} = number of ADJs to left/right of pivot, and {L,R} = number of ARGs to left/right of pivot. The pivot is the part of the string that must remain unaltered during decomposition in order to ensure a correct translation. In order to determine the pivot, we compute the head of the local tree by adapting the head-lexicalised grammar annotation scheme of (Magerman, 1995). In certain cases, we derive a ‘complex pivot’ consisting of this head terminal together with some of its neighbours, e.g. phrasal verbs or strings of auxiliaries. In the case of the example sentence (1), the pivot is ‘likes’. During the decomposition, it is essential to be able to distinguish between arguments (required elements) and adjuncts (optional material), as adjuncts can safely be omitted from the simplified string that we submit to the MT system. The procedure used for argument/adjunct location is an adapted version of Hockenmaier’s algorithm for CCG (Hockenmaier, 2003). The result"
2006.eamt-1.24,H94-1020,0,0.118183,"or assumptions. It therefore does not make use of linguistically motivated syntax, in contrast to TransBooster. 3 TransBooster: ture ArchitecTransBooster uses a chunking algorithm to divide input strings into smaller and simpler constituents, sends those constituents in a minimal necessary context to a baseline MT system and recomposes the MT output chunks to obtain the overall translation of the original input string. Our approach presupposes the existence of some sort of syntactic analysis of the input sentence. We report experiments on human parse-annotated sentences (the Penn II Treebank (Marcus et al., 1994)) and on the output of a state-of-the-art statistical parser (Charniak, 2000) in section 5. Essentially, each TransBooster cycle from a parsed input string to a translated output string consists of the following 5 steps: 1. Finding the Pivot. 2. Locating Arguments and Adjuncts (‘Satellites’) in the source language. 3. Creating and Translating Skeletons and Substitution Variables. 4. Translating Satellites. 5. Combining the translation of Satellites into the output string. We briefly explain each of these steps by processing the following simple example sentence. (1) The chairman, a long-time r"
2006.eamt-1.24,2005.eamt-1.26,1,0.899862,"Missing"
2006.eamt-1.24,J03-1002,0,0.00360759,"and the SRI Language Modeling toolkit.1 We used an interpolated trigram language model with Kneser-Ney discounting. The data used to train the system was taken from the English-Spanish section of the Europarl corpus (Koehn, 2005). From this data, 501K sentence pairs were randomly extracted from the designated training section of the corpus and lowercased. Sentence length was limited to a maximum of 40 words for both Spanish and English, 1 http://www.speech.sri.com/projects/srilm/ with sentence pairs having a maximum relative sentence length ratio of 1.5. From this data we used the method of (Och & Ney, 2003) to extract phrase correspondences from GIZA++ word alignments. For testing purposes two sets of data were used, each consisting of 800 English sentences. The first set was randomly extracted from section 23 of the WSJ section of the Penn II Treebank; the second set consists of randomly extracted sentences from the test section of the Europarl corpus, which had been parsed with (Bikel, 2002). We decided to use two different sets of test data instead of one because we are faced with two ‘out-of-domain’ phenomena that have an influence on the scores, one affecting the TransBooster algorithm, the"
2006.eamt-1.24,P02-1040,0,0.0767457,"e scores, one affecting the TransBooster algorithm, the other the phrase-based SMT system. On the one hand, the TransBooster decomposition algorithm performs better on ‘perfectly’ parse-annotated sentences from the Penn Treebank than on the output produced by a statistical parser as (Bikel, 2002), which introduces a certain amount of noise. On the other hand, Pharaoh was trained on data from the Europarl corpus, so it performs much better on translating Europarl data than out-of-domain Wall Street Journal text. 5 Results and Evaluation We present results of an automatic evaluation using BLEU (Papineni, Roukos, Ward, & Zhu, 2002) and NIST (Doddington, 2002) against the 800-sentence test sets mentioned in section 4. In each case, the statistical significance of the results was tested by using the BLEU/NIST resampling toolkit described in (Zhang & Vogel, 2004).2 We also conduct a manual evaluation of the first 200 sentences in the Europarl test set. Finally, we analyse the differences between the output of Pharaoh and TransBooster, and provide a number of example translations. 2 http://projectile.is.cs.cmu.edu/research/public/ tools/bootStrap/tutorial.htm 5.1 Automatic Evaluation 5.1.1 Europarl English→Spanish Pharaoh"
2006.eamt-1.24,P01-1067,0,0.215018,"Missing"
2006.eamt-1.24,P02-1039,0,0.0437931,"Missing"
2006.eamt-1.24,2004.tmi-1.9,0,0.100205,"Missing"
2006.eamt-1.24,A00-2018,0,\N,Missing
2006.eamt-1.24,2005.mtsummit-papers.38,1,\N,Missing
2006.eamt-1.8,C00-1011,0,0.0650277,"uently, the less probable sub-derivation will never be used to build the most probable derivation and can be removed from the parse space. This algorithm is integrated into the second phase of the procedure used to build the translation space described in section 3.1: as the sets of fragments which are relevant to the input string at each position [i][j] in the translation space are computed, only the fragment starting the highest-scoring sub-derivation is retained. Although the search for SDER does not involve actually estimating probabilities, the Viterbi algorithm can nevertheless be used (Bod, 2000). Derivation lengths are computed by assigning each fragment equal probability, meaning that the shortest derivation can be computed as the most probable one using Viterbi: if each fragment has probability p, then the probability of a derivation which uses n fragments is p n and, since 0 < p < 1, the smallest n must have the largest probability. 4 Experiments We present bidirectional DOT experiments on the English-French section of the HomeCentre corpus, which contains 810 parsed, sentence-aligned translation pairs. This corpus comprises a Xerox printer manual, which was was translated by prof"
2006.eamt-1.8,E03-1005,0,0.122376,"Missing"
2006.eamt-1.8,P01-1030,0,0.055691,"Missing"
2006.eamt-1.8,C04-1154,1,0.799716,"pth 2 or less, link depth 3 or less and link depth 4 or less – the corresponding grammar sizes are given in Table 1. In the interests of robustness, we handle input sentences not covered fully by the grammar by assigning to them the best sequence of partial analyses according to the relevant ranking strategy, leaving untranslated words in the output string where necessary. We evaluate using three different automatic translation evaluation metrics: exact match, BLEU and F-score.7 4 An algorithm to accomplish this task automatically, which gives encouraging preliminary results, is described in (Groves, Hearne, & Way, 2004). 5 When computing MPT and MPP, we set the sampling thresholds Perr and θ given in section 3.2 to 0.01 and 2 respectively (determined empirically), and the maximum number of samples to 10,000. 6 For example, the link depths of the representations in Figure 1(a) and (b) and are 5 and 1 respectively. 7 We used version 11a of the BLEU EN-to-FR: FR-to-EN: depth=1 depth≤2 depth≤3 depth≤4 6,140 6,197 29,081 29,355 148,165 150,460 1,956,786 2,012,632 Table 1: Grammar sizes for each training set. 5 5.1 Results and Discussion English-to-French Translation Accuracy Table 2 shows, for each evaluation me"
2006.eamt-1.8,2003.mtsummit-papers.22,1,0.788129,"es at substructural level.4 Finally, our dataset was split randomly into 12 training/test splits such that all test words also appeared in the training set. Each of these splits comprises 80 test sentences and 730 training tree pairs; 6 of the splits have English as the source language and French as the target language and the other 6 splits have French as source and English as target. We translate each test sentence using the four ranking strategies – MPT, MPP, 5 MPD and SDER) – as described in section 3. We prune the fragment base extracted from each training set with respect to link depth (Hearne & Way, 2003), namely the greatest number of steps taken which depart from a linked node to get from the root node to any frontier node.6 This yields fragment bases comprising fragments of link depth 1, link depth 2 or less, link depth 3 or less and link depth 4 or less – the corresponding grammar sizes are given in Table 1. In the interests of robustness, we handle input sentences not covered fully by the grammar by assigning to them the best sequence of partial analyses according to the relevant ranking strategy, leaving untranslated words in the output string where necessary. We evaluate using three dif"
2006.eamt-1.8,J03-1002,0,0.00721433,"Missing"
2006.eamt-1.8,C96-2215,0,0.479459,"Missing"
2006.eamt-1.8,C00-2092,0,\N,Missing
2006.iwslt-evaluation.4,2004.tmi-1.11,1,0.40914,"and outputs a set of word alignments. 1. Introduction • Chunking Module: takes in an aligned corpus and produces source and target chunks. In this paper, we present the Data-Driven MT system developed at DCU, M AT R E X (Machine Translation using Examples). This system is a hybrid system which exploits both EBMT and SMT techniques to extract a dataset of aligned chunks [1]. The EBMT data resources are extracted using a two-step approach. First, the source and the target sentences are chunked using several different methods. In the case of English and Italian, we employ a marker-based chunker [2, 3]. In the case of Arabic, we use the chunker provided with the ASVM toolkit [4]. The chunks are then aligned thanks to a dynamic programming algorithm which is similar to an edit-distance algorithm while allowing for block movements [5, 6]. This aligner relies on the identification of relationships between chunks, which can be defined and computed in several ways. We also extract SMT-style aligned phrases from word alignments, as described in [7]. These two types of resources are then combined and given to the decoding module, currently a wrapper around a phrase-based SMT decoder. We participat"
2006.iwslt-evaluation.4,N04-1033,0,0.153582,"allow for block movements or “jumps”, following the idea introduced in [5] in the context of MT evaluation. This adaptation can be necessary if the order of constituents is significantly different in the source and target languages. In our previous work, we found out that it was useful in the case of Basque-English [6], but not for SpanishEnglish [1]. In our experiments, we thus decided to include this adapted algorithm for Arabic, but not for Italian (cf. Figure 4). k l This model is often used in SMT as a feature of a loglinear model; in this context, it is called a word-based lexicon model [15]. 2.2.5. Cognate identification It is also possible to take into account a feature based on the identification of cognates. This is especially useful for texts with technical terms, for which it is possible to identify a 3 This algorithm is actually a classical edit-distance algorithm in which distances are replaced by opposite-log-conditional probabilities. 33 significant number of cognates. We use the notation: ( 1 if there is a cognate between eil and fjk , C(eil , fjk ) = 0 otherwise. P HRAMER, a phrase-based SMT decoder. This decoder implements Minimum-Error-Rate Training [19] within a lo"
2006.iwslt-evaluation.4,N04-4038,0,0.106419,"n an aligned corpus and produces source and target chunks. In this paper, we present the Data-Driven MT system developed at DCU, M AT R E X (Machine Translation using Examples). This system is a hybrid system which exploits both EBMT and SMT techniques to extract a dataset of aligned chunks [1]. The EBMT data resources are extracted using a two-step approach. First, the source and the target sentences are chunked using several different methods. In the case of English and Italian, we employ a marker-based chunker [2, 3]. In the case of Arabic, we use the chunker provided with the ASVM toolkit [4]. The chunks are then aligned thanks to a dynamic programming algorithm which is similar to an edit-distance algorithm while allowing for block movements [5, 6]. This aligner relies on the identification of relationships between chunks, which can be defined and computed in several ways. We also extract SMT-style aligned phrases from word alignments, as described in [7]. These two types of resources are then combined and given to the decoding module, currently a wrapper around a phrase-based SMT decoder. We participated in the Open Data Track for the following translation directions: Arabic-Eng"
2006.iwslt-evaluation.4,koen-2004-pharaoh,0,0.0508348,"Italian, case information was removed. For Arabic, the tokenization was handled by the ASVM toolkit previously mentioned. The weights of the log-linear model are not optimized; we experimented with different sets of parameters and did not find any significant difference as long as the weights stay in the interval [0.5 − 1.5]. Outside this interval, the quality of the model decreases. 2.2.7. Integrating SMT data Whilst EBMT has always made use of both lexical and phrasal information [16], it is only recently that SMT has moved towards the use of phrases in their translation models and decoders [7, 17]. It has, therefore, become harder than ever to identify the differences between these two datadriven approaches [10]. However, despite the convergence of the two paradigms, recent research [10, 18] has shown that it is possible to combine elements from EBMT and SMT to create hybrid data-driven systems capable of outperforming the baseline systems from which they are derived. Therefore, SMT phrasal alignments are also added to the aligned chunks extracted by the chunk alignment module, in order to produce higher quality translations. The integration is very easy: the various counts associated"
2006.iwslt-evaluation.4,E06-1031,0,0.0701196,"ion using Examples). This system is a hybrid system which exploits both EBMT and SMT techniques to extract a dataset of aligned chunks [1]. The EBMT data resources are extracted using a two-step approach. First, the source and the target sentences are chunked using several different methods. In the case of English and Italian, we employ a marker-based chunker [2, 3]. In the case of Arabic, we use the chunker provided with the ASVM toolkit [4]. The chunks are then aligned thanks to a dynamic programming algorithm which is similar to an edit-distance algorithm while allowing for block movements [5, 6]. This aligner relies on the identification of relationships between chunks, which can be defined and computed in several ways. We also extract SMT-style aligned phrases from word alignments, as described in [7]. These two types of resources are then combined and given to the decoding module, currently a wrapper around a phrase-based SMT decoder. We participated in the Open Data Track for the following translation directions: Arabic-English and Italian-English, for which we translated both the single-best ASR hypotheses and the text input. We report the results of the system for the provided e"
2006.iwslt-evaluation.4,2006.amta-papers.26,1,0.93494,"ion using Examples). This system is a hybrid system which exploits both EBMT and SMT techniques to extract a dataset of aligned chunks [1]. The EBMT data resources are extracted using a two-step approach. First, the source and the target sentences are chunked using several different methods. In the case of English and Italian, we employ a marker-based chunker [2, 3]. In the case of Arabic, we use the chunker provided with the ASVM toolkit [4]. The chunks are then aligned thanks to a dynamic programming algorithm which is similar to an edit-distance algorithm while allowing for block movements [5, 6]. This aligner relies on the identification of relationships between chunks, which can be defined and computed in several ways. We also extract SMT-style aligned phrases from word alignments, as described in [7]. These two types of resources are then combined and given to the decoding module, currently a wrapper around a phrase-based SMT decoder. We participated in the Open Data Track for the following translation directions: Arabic-English and Italian-English, for which we translated both the single-best ASR hypotheses and the text input. We report the results of the system for the provided e"
2006.iwslt-evaluation.4,P03-1021,0,0.278898,"xicon model [15]. 2.2.5. Cognate identification It is also possible to take into account a feature based on the identification of cognates. This is especially useful for texts with technical terms, for which it is possible to identify a 3 This algorithm is actually a classical edit-distance algorithm in which distances are replaced by opposite-log-conditional probabilities. 33 significant number of cognates. We use the notation: ( 1 if there is a cognate between eil and fjk , C(eil , fjk ) = 0 otherwise. P HRAMER, a phrase-based SMT decoder. This decoder implements Minimum-Error-Rate Training [19] within a loglinear framework [20]. The BLEU metric [21] is optimized using the provided development set. We use a loglinear combination of several common feature functions: phrase translation probabilities (in both directions), wordbased translation probabilities (lexicon model, in both directions), a phrase length penalty and a target language model [22]. The phrase translation probabilities are simply estimated thanks to relative frequencies computed on the aligned dataset of chunks obtained as described above.4 Word-based translation probabilities are introduced to smooth the phrase transl"
2006.iwslt-evaluation.4,N03-1017,0,0.14696,"e source and the target sentences are chunked using several different methods. In the case of English and Italian, we employ a marker-based chunker [2, 3]. In the case of Arabic, we use the chunker provided with the ASVM toolkit [4]. The chunks are then aligned thanks to a dynamic programming algorithm which is similar to an edit-distance algorithm while allowing for block movements [5, 6]. This aligner relies on the identification of relationships between chunks, which can be defined and computed in several ways. We also extract SMT-style aligned phrases from word alignments, as described in [7]. These two types of resources are then combined and given to the decoding module, currently a wrapper around a phrase-based SMT decoder. We participated in the Open Data Track for the following translation directions: Arabic-English and Italian-English, for which we translated both the single-best ASR hypotheses and the text input. We report the results of the system for the provided evaluation sets. • Chunk Alignment Module: takes the source and target chunks and aligns them on a sentence-by-sentence level. • Decoder: searches for a translation using the original aligned corpus and derived c"
2006.iwslt-evaluation.4,P02-1038,0,0.0273419,"dentification It is also possible to take into account a feature based on the identification of cognates. This is especially useful for texts with technical terms, for which it is possible to identify a 3 This algorithm is actually a classical edit-distance algorithm in which distances are replaced by opposite-log-conditional probabilities. 33 significant number of cognates. We use the notation: ( 1 if there is a cognate between eil and fjk , C(eil , fjk ) = 0 otherwise. P HRAMER, a phrase-based SMT decoder. This decoder implements Minimum-Error-Rate Training [19] within a loglinear framework [20]. The BLEU metric [21] is optimized using the provided development set. We use a loglinear combination of several common feature functions: phrase translation probabilities (in both directions), wordbased translation probabilities (lexicon model, in both directions), a phrase length penalty and a target language model [22]. The phrase translation probabilities are simply estimated thanks to relative frequencies computed on the aligned dataset of chunks obtained as described above.4 Word-based translation probabilities are introduced to smooth the phrase translation probabilities, that tend to"
2006.iwslt-evaluation.4,P02-1040,0,0.112191,"so possible to take into account a feature based on the identification of cognates. This is especially useful for texts with technical terms, for which it is possible to identify a 3 This algorithm is actually a classical edit-distance algorithm in which distances are replaced by opposite-log-conditional probabilities. 33 significant number of cognates. We use the notation: ( 1 if there is a cognate between eil and fjk , C(eil , fjk ) = 0 otherwise. P HRAMER, a phrase-based SMT decoder. This decoder implements Minimum-Error-Rate Training [19] within a loglinear framework [20]. The BLEU metric [21] is optimized using the provided development set. We use a loglinear combination of several common feature functions: phrase translation probabilities (in both directions), wordbased translation probabilities (lexicon model, in both directions), a phrase length penalty and a target language model [22]. The phrase translation probabilities are simply estimated thanks to relative frequencies computed on the aligned dataset of chunks obtained as described above.4 Word-based translation probabilities are introduced to smooth the phrase translation probabilities, that tend to be over-estimated for"
2006.iwslt-evaluation.4,J03-1002,0,0.0250715,"coder. We participated in the Open Data Track for the following translation directions: Arabic-English and Italian-English, for which we translated both the single-best ASR hypotheses and the text input. We report the results of the system for the provided evaluation sets. • Chunk Alignment Module: takes the source and target chunks and aligns them on a sentence-by-sentence level. • Decoder: searches for a translation using the original aligned corpus and derived chunk and word alignments. The Word Alignment and the Decoder modules are currently wrappers around existing tools, namely G IZA ++ [9] and P HRAMER.1 In our experiments we investigated a number of different chunking and alignment strategies which we describe in more detail in what follows. An overview of the entire translation process is given in Figure 1: the aligned source-target sentences are passed in turn to the word alignment, chunking and chunk alignment modules, in order to create our chunk and lexical example databases. These databases are then given to the decoder to translate new sentences. 1 http://www.utdallas.edu/˜mgo031000/phramer/. 31 Figure 1: Translation Process 2.1. Chunking Strategies to the chunks extrac"
2006.iwslt-evaluation.4,2005.iwslt-1.20,0,0.0260684,"nal probabilities. 33 significant number of cognates. We use the notation: ( 1 if there is a cognate between eil and fjk , C(eil , fjk ) = 0 otherwise. P HRAMER, a phrase-based SMT decoder. This decoder implements Minimum-Error-Rate Training [19] within a loglinear framework [20]. The BLEU metric [21] is optimized using the provided development set. We use a loglinear combination of several common feature functions: phrase translation probabilities (in both directions), wordbased translation probabilities (lexicon model, in both directions), a phrase length penalty and a target language model [22]. The phrase translation probabilities are simply estimated thanks to relative frequencies computed on the aligned dataset of chunks obtained as described above.4 Word-based translation probabilities are introduced to smooth the phrase translation probabilities, that tend to be over-estimated for phrases that appear only once in the training data [15]. The target (English) language model is a simple 3-gram language model trained on the English portion of the training data, using the SRI Language Modeling Toolkit [23], with modified Kneser-Ney smoothing [24]. We then use the following feature:"
2006.iwslt-evaluation.4,W05-0833,1,0.92034,"holinguistic constraint which posits that all languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signify context. Using a set of closed-class (or “marker”) words, such as determiners, conjunctions, prepositions, possessive and personal pronouns, aligned source-target sentences are segmented into chunks [2] during a pre-processing step. A chunk is created at each new occurrence of a marker word, with the restriction that each chunk must contain at least one content (or nonmarker) word. In addition to the set of marker words used in the experiments of [2, 10], punctuation is also used to segment the aligned sentences – with the punctuation occurring in chunk-final, rather than initial, position. An example of such a chunking is given in Figure 2, for English and Italian. 2.1.3. Remarks about Chunking Since each module of the system can be changed independently of the others, it is possible to use a variety of chunkers. The Marker-Based approach has several obvious advantages: it is simple (linear complexity), easily adaptable, and does not need expensive training on Treebanks. Adapting this chunker to a new language simply amounts to providing the"
2006.iwslt-evaluation.4,W04-1602,0,0.0145445,"of the entire translation process is given in Figure 1: the aligned source-target sentences are passed in turn to the word alignment, chunking and chunk alignment modules, in order to create our chunk and lexical example databases. These databases are then given to the decoder to translate new sentences. 1 http://www.utdallas.edu/˜mgo031000/phramer/. 31 Figure 1: Translation Process 2.1. Chunking Strategies to the chunks extracted with the marker-based chunker. The ASVM toolkit is based on Support Vector Machines, a Machine Learning algorithm, and has been trained on the Arabic Penn Treebank [11]. The chunking process is achieved through a pipeline approach: tokenization, lemmatisation, POS tagging, and finally chunking are performed in turn. 2.1.1. Marker-Based Chunking One method for the extraction of chunks, used in the creation of the example database, is based on the Marker Hypothesis [3], a psycholinguistic constraint which posits that all languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signify context. Using a set of closed-class (or “marker”) words, such as determiners, conjunctions, prepositions, possessive and personal pronouns,"
2006.iwslt-evaluation.4,takezawa-etal-2002-toward,0,0.016063,"mber of source words. 2.2.6. Chunks label If a label is assigned to chunks during the chunking process, we can compare the labels in the source and the target and use this information to relate chunks. A chunk label may be N P (for a noun phrase), or DET if we use the marker-based chunker and the first word of the chunk is a determiner. The feature is a simple binary feature: ( 1 if ei and fj share the same label, hl (ei , fj ) = 0 otherwise. 3. Experimental results 3.1. Data The experiments were carried out using the provided datasets, extracted from the Basic Travel Expression Corpus (BTEC) [25]. This multilingual speech corpus contains tourism-related sentences similar to those that are usually found in phrasebooks for tourists going abroad. We participated in the Open Data Track for the following translation directions: Arabic-English and Italian-English, for which we translated both the single-best ASR hypotheses and the text input. For the supplied data track, 20,000 aligned sentences were provided for training, for both Arabic and Italian. We performed some filtering based on the lengths and the relative lengths of the sentences, ending up with 19,378 aligned sentences for Arabi"
2006.iwslt-evaluation.4,W00-0726,0,0.0506143,"lexity), easily adaptable, and does not need expensive training on Treebanks. Adapting this chunker to a new language simply amounts to providing the system with a list of marker words. For example, in the case of Italian, we easily extracted a list of markers from the MorphIt lexicon [12], making it possible to apply the Marker-Based chunker to Italian. However, we do not exclude the possibility to use different types of chunkers that may be available. In particular, in the case of English, several statistical chunkers have been developed, notably in the context of the CoNLL 2000 shared task [13]. A summary of the strategies used for each language is displayed in Figure 3. 2.1.2. Arabic Chunking The language characteristics of Arabic makes the direct application of the Marker-Based chunker described above more difficult. In the case of Arabic, determiners, prepositions, and pronouns do not usually form independent tokens but are usually part of a token which also contains a noun, an adjective, or a verb. Consequently, in order to identify the markers, one would need to perform some disambiguation at different levels, in particular tokenization and POS tagging. We would thus lose one o"
2006.iwslt-evaluation.4,J93-2003,0,0.015686,"gn the chunks obtained by the chunking procedures described in Section 2.1, we make use of a “editdistance style” dynamic programming alignment algorithm. In the following, a denotes an alignment between a target sequence e and a source sequence f , with I = |e |and J = |f |. Given two sequences of chunks, we are looking for the most likely alignment a ˆ: a ˆ = argmax P(a|e, f ) = argmax P(a, e|f ). a Alignment Strategy “Edit-Distance” with Jumps “Edit-Distance” Instead of using an Expectation-Maximization algorithm to estimate these parameters, as commonly done when performing word alignment [9, 14], we directly compute these parameters by relying on the information contained within the chunks. The conditional probability P(etk |fsk ) can be computed in several ways. In our experiments, we have considered three main sources of knowledge: (i) word-to-word translation probabilities, (ii) word-to-word cognates and (iii) chunk labels, which are described in the following sections. (1) a We first consider alignments such as those obtained by an edit-distance algorithm, i.e. 2.2.3. Knowledge Source Combination a = (t1 , s1 )(t2 , s2 ) . . . (tn , sn ), These sources of knowledge can be combine"
2006.iwslt-evaluation.4,E03-1026,0,\N,Missing
2006.iwslt-evaluation.4,C96-2141,0,\N,Missing
2006.iwslt-evaluation.4,W05-0909,0,\N,Missing
2006.iwslt-evaluation.4,P07-1039,1,\N,Missing
2006.iwslt-evaluation.4,J00-2004,0,\N,Missing
2006.iwslt-evaluation.4,2004.iwslt-evaluation.1,0,\N,Missing
2006.iwslt-evaluation.4,J97-3002,0,\N,Missing
2006.iwslt-evaluation.4,2006.iwslt-evaluation.6,0,\N,Missing
2006.iwslt-evaluation.4,W96-0107,0,\N,Missing
2006.iwslt-evaluation.4,2007.tmi-papers.14,1,\N,Missing
2007.iwslt-1.10,W05-0909,0,0.0644104,"Missing"
2007.iwslt-1.10,J93-2003,0,0.00710991,"translation using the original aligned corpus and derived chunk and word alignments. The word alignment module is one of the most important modules. (Ma et al., 2007a) has shown that word packing can improve statistical machine translation. In our experiments, we improve the word alignment module in our MaTrEx system using this word packing technique. In addition, we demonstrate the effects of smoothing the translation tables with out-ofdomain data, and introduce a translation-based method for case and punctuation restoration. 2.1 2.1.1 Word Packing Motivation Most current statistical models (Brown et al., 1993; Vogel et al., 1996) treat the aligned sentences in the corpus as sequences of tokens that are meant to be interpreted as words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words – a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realize the s"
2007.iwslt-1.10,W96-0107,0,0.151585,"ing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. 2.1.2 Bootstrapping Word Alignment Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment (Kitamura & Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003). We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. In this way, word packing can be applied several times: once we have grouped some words together, they become the new basic unit to consider, and we can re-run the same method to obtain additional groupings. However, in practice 1 See (Ma et al., 2007b) for an investigation into different segmentations of source and target languages depending on the language"
2007.iwslt-1.10,N03-1017,0,0.0233226,"Missing"
2007.iwslt-1.10,2006.iwslt-evaluation.6,0,0.212636,"Missing"
2007.iwslt-1.10,P07-1039,1,0.889564,"of extendible and re-implementable modules (Armstrong et al., 2006; Stroppa and Way, 2006), the most significant of which are: • Word Alignment Module: takes as its input an aligned corpus and outputs a set of word alignments. • Chunking Module: takes in an aligned corpus and produces source and target chunks. • Chunk Alignment Module: takes the source and target chunks and aligns them on a sentence-by-sentence level. • Decoder : searches for a translation using the original aligned corpus and derived chunk and word alignments. The word alignment module is one of the most important modules. (Ma et al., 2007a) has shown that word packing can improve statistical machine translation. In our experiments, we improve the word alignment module in our MaTrEx system using this word packing technique. In addition, we demonstrate the effects of smoothing the translation tables with out-ofdomain data, and introduce a translation-based method for case and punctuation restoration. 2.1 2.1.1 Word Packing Motivation Most current statistical models (Brown et al., 1993; Vogel et al., 1996) treat the aligned sentences in the corpus as sequences of tokens that are meant to be interpreted as words; the goal of the a"
2007.iwslt-1.10,2007.tmi-papers.14,1,0.874075,"of extendible and re-implementable modules (Armstrong et al., 2006; Stroppa and Way, 2006), the most significant of which are: • Word Alignment Module: takes as its input an aligned corpus and outputs a set of word alignments. • Chunking Module: takes in an aligned corpus and produces source and target chunks. • Chunk Alignment Module: takes the source and target chunks and aligns them on a sentence-by-sentence level. • Decoder : searches for a translation using the original aligned corpus and derived chunk and word alignments. The word alignment module is one of the most important modules. (Ma et al., 2007a) has shown that word packing can improve statistical machine translation. In our experiments, we improve the word alignment module in our MaTrEx system using this word packing technique. In addition, we demonstrate the effects of smoothing the translation tables with out-ofdomain data, and introduce a translation-based method for case and punctuation restoration. 2.1 2.1.1 Word Packing Motivation Most current statistical models (Brown et al., 1993; Vogel et al., 1996) treat the aligned sentences in the corpus as sequences of tokens that are meant to be interpreted as words; the goal of the a"
2007.iwslt-1.10,J03-1002,0,0.00504169,"ain data is much larger than in-domain-data. This might cause a bias toward the choice of an incorrect translation obtained via out-of-domain data of words and phrases, when these occur in both in-domain and out-of-domain training data sets(Lee, 2006). In the current work we tried to avoid both problems by smoothing the in-domain translation tables with word translation probabilities from the out-of-domain data. In other words, we added phrases of length one from the out-ofdomain data to our in-domain phrase tables. We use the out-of-domain data to obtain the lexical probabilities via Giza++ (Och and Ney, 2003) to obtain word-level alignments in both language directions. For consistency, the bidirectional alignments are used to derive word translation scores (Koehn et al., 2003). The resulting wordbased translation table is combined with the indomain translation table to construct a larger smoothed translation table. We tried to use the out-of-domain translation tables for translating OOV words only; however, we found that using the OOV translation tables helps in translating both in-vocabulary and OOV items. We combine the out-of-domain word-based translation table with the in-domain phrase table s"
2007.iwslt-1.10,P02-1040,0,0.0822365,"Missing"
2007.iwslt-1.10,2006.iwslt-evaluation.4,1,0.450267,"Missing"
2007.iwslt-1.10,takezawa-etal-2002-toward,0,0.0916195,"Missing"
2007.iwslt-1.10,E03-1026,0,0.0300714,"we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. 2.1.2 Bootstrapping Word Alignment Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment (Kitamura & Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003). We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. In this way, word packing can be applied several times: once we have grouped some words together, they become the new basic unit to consider, and we can re-run the same method to obtain additional groupings. However, in practice 1 See (Ma et al., 2007b) for an investigation into different segmentations of source and target languages depending on the language pair at hand. we have not seen m"
2007.iwslt-1.10,C96-2141,0,0.131708,"e original aligned corpus and derived chunk and word alignments. The word alignment module is one of the most important modules. (Ma et al., 2007a) has shown that word packing can improve statistical machine translation. In our experiments, we improve the word alignment module in our MaTrEx system using this word packing technique. In addition, we demonstrate the effects of smoothing the translation tables with out-ofdomain data, and introduce a translation-based method for case and punctuation restoration. 2.1 2.1.1 Word Packing Motivation Most current statistical models (Brown et al., 1993; Vogel et al., 1996) treat the aligned sentences in the corpus as sequences of tokens that are meant to be interpreted as words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words – a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realize the same concept using var"
2007.iwslt-1.10,J97-3002,0,0.00859085,"ences in the corpus as sequences of tokens that are meant to be interpreted as words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words – a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of words (Wu, 1997).1 Although some statistical alignment models allow for 1-to-n word alignments for those reasons, they rarely question the monolingual tokenization and the basic unit of the alignment process remains the word. In our system, we focus on 1-to-n alignments with the goal of simplifying the task of automatic word aligners by packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. 2.1.2 Bootstr"
2007.mtsummit-papers.40,P80-1024,0,0.0669602,"Missing"
2007.mtsummit-papers.40,2005.mtsummit-osmtw.4,0,0.102763,"Missing"
2007.mtsummit-papers.40,carreras-etal-2004-freeling,0,0.0289062,"a shallowtransfer engine suited to machine translation between Matxin is a classical transfer system consisting of three main components: (i) analysis of the source language into a dependency tree structure, (ii) transfer from the source language dependency tree to a target language dependency structure, and (iii) generation of the output translation from the target dependency structure. These three components are described in more detail in what follows. Analysis The analysis of the Spanish source sentences into dependency trees is performed using an adapted version of the FreeLing toolkit (Carreras et al., 2004).2 FreeLing contains a partofspeech tagger and a shallow parser (or chunker) for Spanish. In Freeling, tagging and shallow parsing are performed using the Machine Learning AdaBoost models (Freund & Schapire, 1997). The shallow parses provided by Freeling are then augmented with dependency information, using a set of rules that identify the dependencies in the sentence. First, the relationships between chunks is established, based on their labels. As an example, consider the chunked Spanish sentence in (1): [np] Bagdad (a three-pronged attack rocked Baghdad) Here the dependency parser identif"
2007.mtsummit-papers.40,2005.eamt-1.12,1,0.888458,"Missing"
2007.mtsummit-papers.40,2004.tmi-1.11,1,0.821473,"recognizes syntactic structures by means of features assigned to word units, following the constraint grammar formalism (Karlsson, 1995). An example of chunked sentences is given in (3), for Spanish and Basque: (3) Spanish: Un triple atentado sacude Bagdad: sacude [np] Bagdad Basque: atentatu hirukoitz batek Bagdad astintzen du =&gt; [np] atentatu hirukoitz batek [np] Bagdad [verbchain] astintzen du Note that, since each module of the system can be changed independently of the others, it is possible to use a variety of chunkers, including those of the Markerbased approach, used in other works (Gough & Way, 2004; Stroppa et al., 2006; Stroppa & Way, 2006). The translation process can be decomposed as follows: the aligned sourcetarget sentences are passed in turn to the Figure 1: Translation Process in MaTrEx Alignment Strategies Word alignment Word alignment is performed using the Giza++ statistical word alignment toolkit and we followed the “refined” method of (Koehn et al., 2003) to extract a set of highquality word alignments from the original uni directional alignment sets. These along with the extracted chunk alignments were passed to the translation decoder. Chunk alignment In order to align"
2007.mtsummit-papers.40,N03-1017,0,0.0232144,"verbchain] astintzen du Note that, since each module of the system can be changed independently of the others, it is possible to use a variety of chunkers, including those of the Markerbased approach, used in other works (Gough & Way, 2004; Stroppa et al., 2006; Stroppa & Way, 2006). The translation process can be decomposed as follows: the aligned sourcetarget sentences are passed in turn to the Figure 1: Translation Process in MaTrEx Alignment Strategies Word alignment Word alignment is performed using the Giza++ statistical word alignment toolkit and we followed the “refined” method of (Koehn et al., 2003) to extract a set of highquality word alignments from the original uni directional alignment sets. These along with the extracted chunk alignments were passed to the translation decoder. Chunk alignment In order to align the chunks obtained by the chunking procedures introduced in Section “Chunking”, we make use of an “editdistance style” dynamic programming alignment algorithm, as described in (Stroppa et al., 2006). This algorithm works as follows. First, a “similarity” measure is determined for each pair of sourcetarget chunks. Then, given these similarities, we use a modified version o"
2007.mtsummit-papers.40,E06-1031,0,0.012599,"r. Chunk alignment In order to align the chunks obtained by the chunking procedures introduced in Section “Chunking”, we make use of an “editdistance style” dynamic programming alignment algorithm, as described in (Stroppa et al., 2006). This algorithm works as follows. First, a “similarity” measure is determined for each pair of sourcetarget chunks. Then, given these similarities, we use a modified version of the editdistance alignment algorithm to find the optimal alignment between the source and the target chunks. The modification consists of allowing for jumps in the alignment process (Leusch et al., 2006), which is a desirable property for translating between languages showing significant syntactic differences. This is the case for Spanish and Basque, where the order of the constituents in a sentence can be very different. To compute the “similarity” between pair of chunks, we rely on the information contained within the chunks. More precisely, we relate chunks by using the wordto word probabilities that were extracted from the word alignment module. The relationship between a source chunk and a target chunk is computed thanks to a model similar to IBM model 1 (Stroppa et al., 2006). Integra"
2007.mtsummit-papers.40,P03-1021,0,0.00941508,"to combine elements from EBMT and SMT to create hybrid datadriven systems capable of outperforming the baseline systems from which they are derived, as shown in (Groves and Way, 2005). Therefore, we also make use of SMT phrasal alignments, which are added to the aligned chunks extracted by the chunk alignment module. The SMT phrasal alignment follows the procedure of (Koehn et al., 2003). Decoder The decoding module is capable of retrieving already translated sentences and also provides a wrapper around Moses, a phrasebased decoder. This decoder also implements MinimumErrorRate Training (Och, 2003) within a loglinear framework (Och & Ney, 2002). The BLEU metric (Papineni et al., 2002) is optimized on a development set. We use a loglinear combination of several common feature functions: phrase translation probabilities (in both directions), wordbased translation probabilities (lexicon model, in both directions), a phrase length penalty and a target language model. The decoder also relies on a target language model. The Basque language model is a simple 3gram language model trained on the Basque portion of the training data, using the SRI Language Modeling Toolkit,4 with modified Knes"
2007.mtsummit-papers.40,W06-3112,1,0.877091,"Missing"
2007.mtsummit-papers.40,P02-1040,0,0.0792267,"apable of outperforming the baseline systems from which they are derived, as shown in (Groves and Way, 2005). Therefore, we also make use of SMT phrasal alignments, which are added to the aligned chunks extracted by the chunk alignment module. The SMT phrasal alignment follows the procedure of (Koehn et al., 2003). Decoder The decoding module is capable of retrieving already translated sentences and also provides a wrapper around Moses, a phrasebased decoder. This decoder also implements MinimumErrorRate Training (Och, 2003) within a loglinear framework (Och & Ney, 2002). The BLEU metric (Papineni et al., 2002) is optimized on a development set. We use a loglinear combination of several common feature functions: phrase translation probabilities (in both directions), wordbased translation probabilities (lexicon model, in both directions), a phrase length penalty and a target language model. The decoder also relies on a target language model. The Basque language model is a simple 3gram language model trained on the Basque portion of the training data, using the SRI Language Modeling Toolkit,4 with modified KneserNey smoothing. 4  MorphemeBased Machine Translation Basque is an agglutinative langu"
2007.mtsummit-papers.40,W05-0908,0,0.0240718,"Missing"
2007.mtsummit-papers.40,2006.amta-papers.25,0,0.0503217,"ss to one Basque reference translation per sentence. Evaluation is performed in a caseinsensitive manner. Because of the specific nature of Basque, we perform two types of evaluation: a wordbased evaluation, and a morpheme based evaluation. Since human evaluation is an expensive process, we selected 50 sentences from the ConsumerTest corpus to be human evaluated; this corpus is referred to as ConsumerTestHuman. The same applies to EitbTest, yielding EitbTestHuman. We used the editdistance metric (Przybocki et al., 2006) called HTER or Translation Error Rate with humantargeted references (Snover et al., 2006). Edit distance is defined as the number of modifications a native Basque professional translator has to make so that the resulting edited translation is an easily understandable Basque sentence that contains the complete meaning of the source sentence. We used the software described in (Snover et al., 2006) to compute HTER. The postediting work took 6 hours in total. Automatic Evaluation Results For the ConsumerTest corpus, the results obtained with the MaTrEx system are higher than those obtained with the Matxin system. With respect to the BLEU score, this difference is 1.58 points absolute"
2007.mtsummit-papers.40,2006.amta-papers.26,1,0.731084,"Missing"
2007.mtsummit-papers.40,2006.iwslt-evaluation.4,1,0.921046,"• A definition of Basque paradigms (sets of correspondences between partial surface forms and partial lexical forms). Those paradigms are similar to continuation classes in twolevel morphology (Koskeniemmi, 1983). Lists of surface form to lexical form correspondences for complex lexical units (including multiword units). This dictionary is compiled into a finitestate transducer which is used to perform the morphological generation of Basque words. A more detailed description of this process can be found in (ArmentanoOller et al., 2005). 3  MaTrEx: a DataDriven System The MaTrEx system (Stroppa & Way, 2006) used in our experiments is a modular datadriven MT engine, which consists of a number of extendible and reimplementable modules, the most important of which are: • • • • Word Alignment Module: takes as its input an aligned corpus and outputs a set of word alignments. Chunking Module: takes in an aligned corpus and produces source and target chunks. Chunk Alignment Module: takes the source and target chunks and aligns them on a sentenceby sentence level. Decoder: searches for a translation using the original aligned corpus and derived chunk and word alignments. The Word Alignment and the D"
2007.mtsummit-papers.40,E06-1032,0,\N,Missing
2007.mtsummit-papers.40,przybocki-etal-2006-edit,0,\N,Missing
2007.mtsummit-papers.44,2004.tmi-1.11,1,0.827162,"ure is proposed. • A complete system setup was discussed by Stein (Stein et al., 2006) for German and German sign language on the domain weather reports. Further, they describe how to improve the results with sign language specific pre- and postprocessing methods. 4 Data-Driven MT in DCU and RWTH University Over the last 10 years, the National Centre for Language Technology at DCU has developed a successful track record in research on Data-Driven MT. This is evident from the work of (Veale & Way, 1997) involving a template-driven approach to EBMT, to the Marker-Based segmentation research of (Gough & Way, 2004b) and more recently the work of (Stroppa & Way, 2006) on the development of the MaTrEx MT system (cf. section 4.1) which has performed well in international evaluations such as IWSLT.2 For over a decade, the RWTH University has been focussing research on SMT. The system has achieved very competitive results in all international evaluations in which it has participated (TC-STAR3 ,IWSLT, NIST4 ). In light of these developments, we have chosen to combine the approaches of these two prominent data-driven MT research centres and apply their approaches to the area of SL translation. These systems a"
2007.mtsummit-papers.44,W05-0833,1,0.847614,"algorithm is employed to align the chunks created in the chunking module. Rather than using the Expectation-Maximization algorithm for parameter estimation, instead these are directly computed according to the information within the chunks. This information is obtained from three sources: word-to-word translation probabilities, word-to-word cognates and chunk labels. The resulting aligned chunks are then combined with the SMT phrasal alignments. The two alignment styles Figure 1: MaTrEx Architecture are merged to help produce translations of a higher quality following the recent research of (Groves & Way, 2005; Groves & Way, 2006) 4.1.4 Decoder The MaTrEx decoder is a wrapper around Moses (Koehn et al., 2007), a phrase-based SMT decoder. Minimum-Error-Rate training (Och, 2003) is implemented within a log-linear framework (Och & Ney, 2002) and the BLEU metric (Papineni et al., 2002) is optimized using the development set. 4.2 The RWTH MT System We use a SMT system to automatically transfer the meaning of a source language sentence into a target language sentence. Our baseline system maximizes the translation probability directly using a log-linear model (Och & Ney, 2002) shown in below: P  M J I e"
2007.mtsummit-papers.44,H90-1021,0,0.0235709,"n language (EN–DE, DE–EN) (iv) and the novel translation pairings of SL to SL (DGS–ISL, ISL–DGS). Figure 2: Permutation graph of a source sentence f1 f2 f3 f4 using a window size w = 2 for a) local constraints, b) IBM constraints and c) inverse IBM constraints evident from the work of (Morrissey & Way, 2006), whose SL research made use of data from the ECHO project data. For the most part, that which is available is so small in terms of sentence quantity that it is unusable for data-driven MT. For these reason we chose to create our own corpora. We found a suitable dataset in the ATIS corpus (Hemphill et al., 1990). The ATIS (Air Travel Information System) corpus is a dataset of transcriptions from speech containing information on flights, aircraft, cities and similar related information. This corpus is particularly suited to our MT needs as it is within a closed domain and has a small vocabulary. The domain itself has a potentially practical use for Deaf people. The ATIS corpus consists of 595 English sentences. Although this is a significantly smaller dataset than that used in mainstream data-driven MT, it is sufficient to feed our systems, as demonstrated in section 6. We had this dataset translated"
2007.mtsummit-papers.44,N03-1017,0,0.00343872,"ew language pairs. An overview of the translation process is in Figure 1. The decoder is fed by different example databases to translate new sentences. These chunk and lexical example databases are created using the the Word Alignment, Chunking and Chunk Alignment Modules that are themselves fed by aligned source-target sentences. 4.1.1 Word Alignment Module Word alignment for the system is performed using Giza++ (Och, 2003), a statistical word alignment toolkit. A set of high-quality word alignments are extracted from the original uni-directional alignment sets using the “refined” method of (Koehn et al., 2003). 4.1.2 Chunking Module The primary chunking strategy employed for our language pairs in this system is based on the Marker Hypothesis (Green, 1979). This method is based on the universal psycholinguistic constraint that languages are marked for syntactic structure at their surface level by closed sets of lexemes or morphemes. Lists of closed-class “marker” words ( i.e. prepositions, conjunctions, determiners etc.) are used to segment the sentences and derive a new data source: a set of marker chunks. Each chunk consists of one or more marker words and at least one non-marker word to ensure co"
2007.mtsummit-papers.44,2005.mtsummit-papers.11,0,0.0151008,"Missing"
2007.mtsummit-papers.44,2002.tmi-papers.12,0,0.070933,"Missing"
2007.mtsummit-papers.44,2005.mtsummit-ebmt.14,1,0.834326,"he generation of ASL. There have also been interlingual approaches adopted by (Veale et al., 1998) and (Zhao et al., 2000), the latter employing synchronised tree– adjoining grammars. A second generation hybrid approach has been developed by (Huenerfauth, 2005) where interlingual, transfer and direct approaches are integrated. 1 http://www.let.kun.nl/sign-lang/echo/data.html 3.2 Current Developments More recently, SLMT has followed the more mainstream MT trend away from rule-based approaches toward data-driven methods. The following groups are active in their ‘third generation’ approaches: • (Morrissey & Way, 2005; Morrissey & Way, 2006) investigate corpus-based methods for example-based sign language translation from English to the sign language of the Netherlands. • (Chiu et al., 2007) present a system for the language pair Chinese and Taiwanese sign language. They show that their optimizing method surpasses IBM model 2. • Basic work on Spanish and Spanish sign language was done by (San-Segundo et al., 2006). Here, a speech to gesture architecture is proposed. • A complete system setup was discussed by Stein (Stein et al., 2006) for German and German sign language on the domain weather reports. Furth"
2007.mtsummit-papers.44,P03-1021,0,0.0654225,"(EBMT) and SMT approaches. The system is modular in design consisting of a number of extendible and reimplementable modules. This modular design makes it particularly adaptable to new language pairs. An overview of the translation process is in Figure 1. The decoder is fed by different example databases to translate new sentences. These chunk and lexical example databases are created using the the Word Alignment, Chunking and Chunk Alignment Modules that are themselves fed by aligned source-target sentences. 4.1.1 Word Alignment Module Word alignment for the system is performed using Giza++ (Och, 2003), a statistical word alignment toolkit. A set of high-quality word alignments are extracted from the original uni-directional alignment sets using the “refined” method of (Koehn et al., 2003). 4.1.2 Chunking Module The primary chunking strategy employed for our language pairs in this system is based on the Marker Hypothesis (Green, 1979). This method is based on the universal psycholinguistic constraint that languages are marked for syntactic structure at their surface level by closed sets of lexemes or morphemes. Lists of closed-class “marker” words ( i.e. prepositions, conjunctions, determin"
2007.mtsummit-papers.44,P02-1038,1,0.446131,"hunks. This information is obtained from three sources: word-to-word translation probabilities, word-to-word cognates and chunk labels. The resulting aligned chunks are then combined with the SMT phrasal alignments. The two alignment styles Figure 1: MaTrEx Architecture are merged to help produce translations of a higher quality following the recent research of (Groves & Way, 2005; Groves & Way, 2006) 4.1.4 Decoder The MaTrEx decoder is a wrapper around Moses (Koehn et al., 2007), a phrase-based SMT decoder. Minimum-Error-Rate training (Och, 2003) is implemented within a log-linear framework (Och & Ney, 2002) and the BLEU metric (Papineni et al., 2002) is optimized using the development set. 4.2 The RWTH MT System We use a SMT system to automatically transfer the meaning of a source language sentence into a target language sentence. Our baseline system maximizes the translation probability directly using a log-linear model (Och & Ney, 2002) shown in below: P  M J I exp ) , f λ h (e m=1 m m 1 1 P  p(eI1 |f1J ) = X M exp eI1 , f1J ) m=1 λm hm (˜ I e˜1 with a set of different features hm , scaling factors λm and the denominator a normalization factor that can be ignored in the maximization proces"
2007.mtsummit-papers.44,P02-1040,0,0.0742212,"om three sources: word-to-word translation probabilities, word-to-word cognates and chunk labels. The resulting aligned chunks are then combined with the SMT phrasal alignments. The two alignment styles Figure 1: MaTrEx Architecture are merged to help produce translations of a higher quality following the recent research of (Groves & Way, 2005; Groves & Way, 2006) 4.1.4 Decoder The MaTrEx decoder is a wrapper around Moses (Koehn et al., 2007), a phrase-based SMT decoder. Minimum-Error-Rate training (Och, 2003) is implemented within a log-linear framework (Och & Ney, 2002) and the BLEU metric (Papineni et al., 2002) is optimized using the development set. 4.2 The RWTH MT System We use a SMT system to automatically transfer the meaning of a source language sentence into a target language sentence. Our baseline system maximizes the translation probability directly using a log-linear model (Och & Ney, 2002) shown in below: P  M J I exp ) , f λ h (e m=1 m m 1 1 P  p(eI1 |f1J ) = X M exp eI1 , f1J ) m=1 λm hm (˜ I e˜1 with a set of different features hm , scaling factors λm and the denominator a normalization factor that can be ignored in the maximization process. We choose the λm by optimizing an MT perf"
2007.mtsummit-papers.44,2006.eamt-1.21,1,0.823333,"following groups are active in their ‘third generation’ approaches: • (Morrissey & Way, 2005; Morrissey & Way, 2006) investigate corpus-based methods for example-based sign language translation from English to the sign language of the Netherlands. • (Chiu et al., 2007) present a system for the language pair Chinese and Taiwanese sign language. They show that their optimizing method surpasses IBM model 2. • Basic work on Spanish and Spanish sign language was done by (San-Segundo et al., 2006). Here, a speech to gesture architecture is proposed. • A complete system setup was discussed by Stein (Stein et al., 2006) for German and German sign language on the domain weather reports. Further, they describe how to improve the results with sign language specific pre- and postprocessing methods. 4 Data-Driven MT in DCU and RWTH University Over the last 10 years, the National Centre for Language Technology at DCU has developed a successful track record in research on Data-Driven MT. This is evident from the work of (Veale & Way, 1997) involving a template-driven approach to EBMT, to the Marker-Based segmentation research of (Gough & Way, 2004b) and more recently the work of (Stroppa & Way, 2006) on the develop"
2007.mtsummit-papers.44,2007.tmi-papers.26,1,0.823492,"promising results for the addition of EBMT-style chunks, increasing distortion limits and reordering constraints. This shows some potential for producing improved translations if incorporated together in a data-driven system. Our research has also highlighted the need for MT to be applied to SLs to aid communication with the Deaf and hearing communities and have outlined current developments in this area. With this in mind, we have begun to take our work further by adding an SL recognition tool to the front end of our current system to develop a fully automatic SL-tospoken language MT system (Stein et al., 2007). For the ATIS corpus and its available video recordings in ISL, some preliminary but promising experiments have been carried out to connect the recognition and MT processes. At a later stage, to facilitate a more practical use for the Deaf we hope to reverse the language direction and produce SL translations of spoken language through the medium of an avatar, thereby allowing Deaf people to translate and access information in their natural language. The development of both these language directions leads naturally to the merging of both systems to allow for translation from SL-to-SL, a novel"
2007.mtsummit-papers.44,2006.iwslt-evaluation.4,1,0.827273,"ussed by Stein (Stein et al., 2006) for German and German sign language on the domain weather reports. Further, they describe how to improve the results with sign language specific pre- and postprocessing methods. 4 Data-Driven MT in DCU and RWTH University Over the last 10 years, the National Centre for Language Technology at DCU has developed a successful track record in research on Data-Driven MT. This is evident from the work of (Veale & Way, 1997) involving a template-driven approach to EBMT, to the Marker-Based segmentation research of (Gough & Way, 2004b) and more recently the work of (Stroppa & Way, 2006) on the development of the MaTrEx MT system (cf. section 4.1) which has performed well in international evaluations such as IWSLT.2 For over a decade, the RWTH University has been focussing research on SMT. The system has achieved very competitive results in all international evaluations in which it has participated (TC-STAR3 ,IWSLT, NIST4 ). In light of these developments, we have chosen to combine the approaches of these two prominent data-driven MT research centres and apply their approaches to the area of SL translation. These systems are described in more detail in sections 4.1 and 4.2. 2"
2007.mtsummit-papers.44,zhao-etal-2000-machine,0,0.512048,"d generation’ approaches. Transfer-based approaches have included the work of (Grieve-Smith, 1999) who translated English weather reports into American Sign Language (ASL) by mapping syntactic structures. (Van Zijl & Barker, 2003) also used a syntactic approach in their work on South African Sign Language with most of their focus on avatar production. (Marshall & S´af´ar, 2002; S´af´ar & Marshall, 2002) employ discourse representation structures and use HPSG semantic feature structures for the generation of ASL. There have also been interlingual approaches adopted by (Veale et al., 1998) and (Zhao et al., 2000), the latter employing synchronised tree– adjoining grammars. A second generation hybrid approach has been developed by (Huenerfauth, 2005) where interlingual, transfer and direct approaches are integrated. 1 http://www.let.kun.nl/sign-lang/echo/data.html 3.2 Current Developments More recently, SLMT has followed the more mainstream MT trend away from rule-based approaches toward data-driven methods. The following groups are active in their ‘third generation’ approaches: • (Morrissey & Way, 2005; Morrissey & Way, 2006) investigate corpus-based methods for example-based sign language translation"
2007.mtsummit-papers.44,P05-2007,0,\N,Missing
2007.mtsummit-papers.44,W05-0831,1,\N,Missing
2007.mtsummit-papers.44,2006.iwslt-evaluation.15,1,\N,Missing
2007.mtsummit-papers.62,W05-0909,0,0.0330317,"rated 6 training/test splits for the dataset at random such that (i) all test words also appeared in the training set, (ii) all splits have English as the source language and French as the target language and (iii) each test set contains 80 test sentences each training set contains 730 tree pairs. We then applied the 6 splits to each of the 9 versions of the dataset, trained the MT system on each training set and tested on each corresponding test set. We evaluated the translation output using three automatic evaluation metrics, BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee & Lavie, 2005), averaging the results over the 6 splits in order to gain a single score for each of the 9 variants of the aligned dataset. We also measured coverage for each variant. These scores are presented in Table 2. We note that most of the automatically aligned runs outperform the manual scores for the BLEU and METEOR metric and that the NIST scores for the automatic alignments are very competitive. Furthermore, all the automatically aligned datasets achieve higher coverage than the manually aligned run. Configurations manual skip1_score1 skip1_score2 skip2_score1 skip2_score2 BLEU 0.5222 0.5038 0.52"
2007.mtsummit-papers.62,2003.mtsummit-papers.13,0,0.120595,"motivation is that the structural and translational divergences that exist between source and target structures should be captured during the alignments process rather than smoothed away in order to allow for higher recall, cf. (Hearne et al., 2007). We do not view the parse trees as constraints, but rather as accurate syntactic representations of the text which can help to guide the alignment process. 2.1. Dependency Structures We are particularly interested in aligning phrase-structure trees, but solutions which have been applied to the alignment of dependency structures are also relevant. Ding et al. (2003) present a strategy for inducing word alignments over dependency structures. However, dependency analyses contain only lexically headed phrases, and we want to capture links between non-lexically headed phrases not described in dependency representations. Matsumoto et al. (1993) induce alignments in dependency structures, but with the intention of using the alignments to resolve parse ambiguities. Their algorithm only aligns simple sentences: alignment of complex sentences is done by first segmenting the sentence into smaller chunks and then aligning those chunks, and the original tree structu"
2007.mtsummit-papers.62,P03-2041,0,0.0567608,"nducing word alignments over dependency structures. However, dependency analyses contain only lexically headed phrases, and we want to capture links between non-lexically headed phrases not described in dependency representations. Matsumoto et al. (1993) induce alignments in dependency structures, but with the intention of using the alignments to resolve parse ambiguities. Their algorithm only aligns simple sentences: alignment of complex sentences is done by first segmenting the sentence into smaller chunks and then aligning those chunks, and the original tree structures are not retrievable. Eisner (2003) develops a tree-mapping algorithm for use on dependency structures which he claims is adaptable for use on phrase-structure trees. However, the alignment process is heavily linked to the translation strategy of which it forms part. We prefer alignment to be a separate offline process which can then be applied to numerous different tasks. 2.2. Phrase-structure Trees Groves et al. (2004) present a rule-based aligner which builds upon automatically induced word alignments. While their algorithm is in theory language pairindependent, in later experiments it performed poorly when evaluated on lang"
2007.mtsummit-papers.62,P03-1011,0,0.0671452,"endency structures which he claims is adaptable for use on phrase-structure trees. However, the alignment process is heavily linked to the translation strategy of which it forms part. We prefer alignment to be a separate offline process which can then be applied to numerous different tasks. 2.2. Phrase-structure Trees Groves et al. (2004) present a rule-based aligner which builds upon automatically induced word alignments. While their algorithm is in theory language pairindependent, in later experiments it performed poorly when evaluated on language pairs other than those used in development. Gildea (2003) proposes a method for aligning nonisomorphic phrase-structure trees using a stochastic treesubstitution grammar (STSG). This approach involves the altering of the tree structure in order to impose isomorphism, which impacts on its portability to other domains. Lu et al. (2001) describe a stochastic inversion transduction grammar, based on (Wu, 1995), which uses a monolingual grammar to parse the source sentence and builds a target language parse based on this, while simultaneously inducing alignments. These alignments are then extracted and converted into translation templates. Imposition of"
2007.mtsummit-papers.62,C04-1154,1,0.710578,"algorithm only aligns simple sentences: alignment of complex sentences is done by first segmenting the sentence into smaller chunks and then aligning those chunks, and the original tree structures are not retrievable. Eisner (2003) develops a tree-mapping algorithm for use on dependency structures which he claims is adaptable for use on phrase-structure trees. However, the alignment process is heavily linked to the translation strategy of which it forms part. We prefer alignment to be a separate offline process which can then be applied to numerous different tasks. 2.2. Phrase-structure Trees Groves et al. (2004) present a rule-based aligner which builds upon automatically induced word alignments. While their algorithm is in theory language pairindependent, in later experiments it performed poorly when evaluated on language pairs other than those used in development. Gildea (2003) proposes a method for aligning nonisomorphic phrase-structure trees using a stochastic treesubstitution grammar (STSG). This approach involves the altering of the tree structure in order to impose isomorphism, which impacts on its portability to other domains. Lu et al. (2001) describe a stochastic inversion transduction gra"
2007.mtsummit-papers.62,2003.mtsummit-papers.22,1,0.851985,"s against a manually aligned gold standard, and perform an extrinsic evaluation by using the aligned data to train and test a DOT system. Our results show that translation accuracy is comparable to that of the same translation system trained on manually aligned data, and coverage improves. 1. Introduction The majority of approaches to data-driven Machine Translation (MT) focus on string-to-string models, despite the fact that tree-to-tree models achieve promising results (Hearne & Way, 2006; Nesson et al., 2006). Some tree-totree models, such as Data-Oriented Translation (DOT) (Poutsma, 2003; Hearne & Way, 2003, 2006), require source and target tree pairs that are aligned at subsentential level. In most previous experiments with DOT systems the training tree pairs were aligned manually. However, such a task is time-consuming and error-prone, and requires considerable expertise in both the source and target languages, and so there is an obvious need to induce the alignments automatically. A considerable amount of research has been carried out on the subject of sub-sentential alignment between structured representations of sentence pairs. However, many of the solutions presented share one or both of t"
2007.mtsummit-papers.62,2006.eamt-1.8,1,0.930093,"ndent algorithm which automatically induces alignments between phrase-structure trees. We evaluate the alignments themselves against a manually aligned gold standard, and perform an extrinsic evaluation by using the aligned data to train and test a DOT system. Our results show that translation accuracy is comparable to that of the same translation system trained on manually aligned data, and coverage improves. 1. Introduction The majority of approaches to data-driven Machine Translation (MT) focus on string-to-string models, despite the fact that tree-to-tree models achieve promising results (Hearne & Way, 2006; Nesson et al., 2006). Some tree-totree models, such as Data-Oriented Translation (DOT) (Poutsma, 2003; Hearne & Way, 2003, 2006), require source and target tree pairs that are aligned at subsentential level. In most previous experiments with DOT systems the training tree pairs were aligned manually. However, such a task is time-consuming and error-prone, and requires considerable expertise in both the source and target languages, and so there is an obvious need to induce the alignments automatically. A considerable amount of research has been carried out on the subject of sub-sentential alig"
2007.mtsummit-papers.62,2007.tmi-papers.11,1,0.903621,"actors like non-isomorphism as obstacles, and alter the trees as part of the alignment process. Other related work in the area of alignment in general views the use of tree structures as a negative aspect which may result in the loss of generalisation ability (Wellington et al., 2006). However, we chose to align pre-determined tree structures without editing them; our motivation is that the structural and translational divergences that exist between source and target structures should be captured during the alignments process rather than smoothed away in order to allow for higher recall, cf. (Hearne et al., 2007). We do not view the parse trees as constraints, but rather as accurate syntactic representations of the text which can help to guide the alignment process. 2.1. Dependency Structures We are particularly interested in aligning phrase-structure trees, but solutions which have been applied to the alignment of dependency structures are also relevant. Ding et al. (2003) present a strategy for inducing word alignments over dependency structures. However, dependency analyses contain only lexically headed phrases, and we want to capture links between non-lexically headed phrases not described in depe"
2007.mtsummit-papers.62,2005.mtsummit-papers.11,0,0.0327489,"than phrase-level precision. The explanation for this may lie in how the MT system works: because DOT displays a preference for using larger fragments when building translations wherever possible, the impact of inconsistencies amongst smaller fragments (i.e. word-level alignments) is minimised. Nevertheless, the evaluations we have presented indicate that our algorithm performs well and provides a viable solution to the challenge of inducing sub-tree alignments. 6. Future Work Application of our alignment algorithm to parsed sections of the English–Spanish and English–German EuroParl corpora (Koehn, 2005) is currently underway. We intend to replicate the evaluations presented here for these datasets in order to (i) gain a clearer picture of differences in performance between aligner configurations and (ii) to demonstrate the language-independent nature of the alignment strategy. We are also currently investigating other uses of the automatically aligned data. One such use is the extraction of the aligned phrases for use in a phrasebased SMT system. In addition to our current use of word-translation probability tables, we expect that factoring in phrase-table probabilities when either scoring o"
2007.mtsummit-papers.62,P07-2045,0,0.0645868,"n-blocked non-lexical hypotheses remain then cording to (1). all links non-lexical links while non-blocked lexical hypotheses remain do Score score2 Configurations Precision Recall Precision Recall |x |&quot;|y| 4 link and block the highest-scoring hypothesis ! |t ) α(t |s ) (1) γ(!s, t&quot;) = α(s |t ) α(t |s ) α(s i=1 P (xj |yi ) skip1 score1 0.6096 0.7723 0.8424 0.7394 l l l l l l l l = hypotheses (3) block all α(x|y) contradicting 0.6192 0.7869 0.8107 0.7756 skip1 score2 |y| end while j=1 skip2 score1 0.6162 0.7783 0.8394 are 0.7486 Individual string-correspondence scores α(x|y) comMoses decoder1 (Koehn et al., 2007). To improve the quality of the word-alignments, we induce them from a lowercased version of the parallel corpus, but our algorithm does not change the case of the data. Two alternative scoring functions are given in (2) and (3). They differ in that score2 divides the sum over the probabilities that xi corresponds to each yj by the number of words in y. The intended effect of this is again to reduce any bias in favour of aligning shorter-span constituents over constituents of longer span. (2) Score score1 y x j i α x|y = Π Σ P xi|y j y (3) Score score2 x Σ P xi|y j i y α x|y = Π j Given a tree"
2007.mtsummit-papers.62,O01-2004,0,0.22066,"Missing"
2007.mtsummit-papers.62,P93-1004,0,0.0832656,"se trees as constraints, but rather as accurate syntactic representations of the text which can help to guide the alignment process. 2.1. Dependency Structures We are particularly interested in aligning phrase-structure trees, but solutions which have been applied to the alignment of dependency structures are also relevant. Ding et al. (2003) present a strategy for inducing word alignments over dependency structures. However, dependency analyses contain only lexically headed phrases, and we want to capture links between non-lexically headed phrases not described in dependency representations. Matsumoto et al. (1993) induce alignments in dependency structures, but with the intention of using the alignments to resolve parse ambiguities. Their algorithm only aligns simple sentences: alignment of complex sentences is done by first segmenting the sentence into smaller chunks and then aligning those chunks, and the original tree structures are not retrievable. Eisner (2003) develops a tree-mapping algorithm for use on dependency structures which he claims is adaptable for use on phrase-structure trees. However, the alignment process is heavily linked to the translation strategy of which it forms part. We prefe"
2007.mtsummit-papers.62,2006.amta-papers.15,0,0.0146318,"h automatically induces alignments between phrase-structure trees. We evaluate the alignments themselves against a manually aligned gold standard, and perform an extrinsic evaluation by using the aligned data to train and test a DOT system. Our results show that translation accuracy is comparable to that of the same translation system trained on manually aligned data, and coverage improves. 1. Introduction The majority of approaches to data-driven Machine Translation (MT) focus on string-to-string models, despite the fact that tree-to-tree models achieve promising results (Hearne & Way, 2006; Nesson et al., 2006). Some tree-totree models, such as Data-Oriented Translation (DOT) (Poutsma, 2003; Hearne & Way, 2003, 2006), require source and target tree pairs that are aligned at subsentential level. In most previous experiments with DOT systems the training tree pairs were aligned manually. However, such a task is time-consuming and error-prone, and requires considerable expertise in both the source and target languages, and so there is an obvious need to induce the alignments automatically. A considerable amount of research has been carried out on the subject of sub-sentential alignment between structur"
2007.mtsummit-papers.62,J03-1002,0,0.039017,"Missing"
2007.mtsummit-papers.62,P02-1040,0,0.111335,"ligner configurations specified in Section 3.3. We also generated 6 training/test splits for the dataset at random such that (i) all test words also appeared in the training set, (ii) all splits have English as the source language and French as the target language and (iii) each test set contains 80 test sentences each training set contains 730 tree pairs. We then applied the 6 splits to each of the 9 versions of the dataset, trained the MT system on each training set and tested on each corresponding test set. We evaluated the translation output using three automatic evaluation metrics, BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee & Lavie, 2005), averaging the results over the 6 splits in order to gain a single score for each of the 9 variants of the aligned dataset. We also measured coverage for each variant. These scores are presented in Table 2. We note that most of the automatically aligned runs outperform the manual scores for the BLEU and METEOR metric and that the NIST scores for the automatic alignments are very competitive. Furthermore, all the automatically aligned datasets achieve higher coverage than the manually aligned run. Configurations manual skip1_score1 s"
2007.mtsummit-papers.62,C02-1010,0,0.159409,"er to impose isomorphism, which impacts on its portability to other domains. Lu et al. (2001) describe a stochastic inversion transduction grammar, based on (Wu, 1995), which uses a monolingual grammar to parse the source sentence and builds a target language parse based on this, while simultaneously inducing alignments. These alignments are then extracted and converted into translation templates. Imposition of source language structure onto the target language is not always desirable. Nevertheless, on the evidence presented here, tree-to-string alignment models warrant further investigation. Wang et al. (2002) develop an interesting method for structural alignment which they call “bilingual chunking”. Given a pair of phrase-structure trees, they perform word alignment on the surface forms and then extract chunks from both trees simultaneously. The chunking is guided by the tree structure and constraints which ensure word alignments do not cross chunks. The chunks are then POS-tagged using an HMM tagger. Again, the original tree structures are lost during the alignment process. While the methods outlined above all achieve competitive results, those presented by Lu et al. (2001) and Wang et al. (2002"
2007.mtsummit-papers.62,P06-1123,0,0.0127431,"ential alignment can be loosely grouped according to whether they focus on aligning dependency structures or phrasestructure trees. Many approaches do not view alignment as an independent task, but rather as a means to achieving another goal such as solving parse ambiguities or acquiring translation templates. Some such approaches view factors like non-isomorphism as obstacles, and alter the trees as part of the alignment process. Other related work in the area of alignment in general views the use of tree structures as a negative aspect which may result in the loss of generalisation ability (Wellington et al., 2006). However, we chose to align pre-determined tree structures without editing them; our motivation is that the structural and translational divergences that exist between source and target structures should be captured during the alignments process rather than smoothed away in order to allow for higher recall, cf. (Hearne et al., 2007). We do not view the parse trees as constraints, but rather as accurate syntactic representations of the text which can help to guide the alignment process. 2.1. Dependency Structures We are particularly interested in aligning phrase-structure trees, but solutions"
2007.mtsummit-papers.62,P95-1033,0,0.0403419,"based aligner which builds upon automatically induced word alignments. While their algorithm is in theory language pairindependent, in later experiments it performed poorly when evaluated on language pairs other than those used in development. Gildea (2003) proposes a method for aligning nonisomorphic phrase-structure trees using a stochastic treesubstitution grammar (STSG). This approach involves the altering of the tree structure in order to impose isomorphism, which impacts on its portability to other domains. Lu et al. (2001) describe a stochastic inversion transduction grammar, based on (Wu, 1995), which uses a monolingual grammar to parse the source sentence and builds a target language parse based on this, while simultaneously inducing alignments. These alignments are then extracted and converted into translation templates. Imposition of source language structure onto the target language is not always desirable. Nevertheless, on the evidence presented here, tree-to-string alignment models warrant further investigation. Wang et al. (2002) develop an interesting method for structural alignment which they call “bilingual chunking”. Given a pair of phrase-structure trees, they perform wo"
2007.mtsummit-papers.62,J97-3002,0,0.0993693,"language-specific information beyond the (automatically induced) word-alignment probabilities and does not make use of the node labels in the tree pairs. 3.1. Alignment Well-Formedness Criteria Links are induced between tree pairs such that they meet the following well-formedness criteria: (i) a node can only be linked once; (ii) descendants of a source linked node may only link to descendants of its target linked counterpart; (iii) ancestors of a source linked node may only link to ancestors of its target linked counterpart. These criteria are akin to the “crossing constraints” described in (Wu, 1997) which forbid alignments between constituents that cross each other. Our criteria differ from those of Wu because we impose them on a pair of fully monolingually parsed trees, thus our criteria are more strict. The constraints in (Wu, 1997), on the other hand, are imposed inherently during the bilingual parsing and alignment process. In what follows, a hypothesised alignment is ill-formed with respect to the existing alignments if it violates any of these criteria. 3.2. Algorithm In this section we describe how our algorithm scores and selects links. In some instances, we present alternative m"
2007.tmi-papers.11,A00-2018,0,0.0322371,"ences as accurately as possible, and that MT systems making use of these alignments should employ them in conjunction with broad-coverage models (such as wordand phrase-alignments) in order to preserve robustness. 7 Future Work In order to improve the accuracy of our treealignment algorithm, we plan to investigate alternative word-alignment techniques (e.g. (Tiedemann, 2004; Liang et al., 2006; Ma et al., 2007)) in order to establish which one is most appropriate for our task. With regard to the broader area of parallel treebank construction and the use of statistical parsers such as those of Charniak (2000) and Bikel (2002), we would like to examine the impact of imperfect parse quality on the capture of translational divergences. We plan to extend our aligner so that it works with n-best parse forests on the source and/or target sides, thereby giving the aligner some (limited) influence over the configuration of the aligned parse trees. Finally, we plan to investigate how best to incorporate the translation information encoded in parallel treebanks into existing data-driven MT systems, both indirectly in terms of complementary phrase/chunk extraction methods and directly in terms of inducing sy"
2007.tmi-papers.11,P05-1033,0,0.0338901,"work has argued for the development of parallel treebanks, defined as bitexts for which the sentences are annotated with syntactic trees and are aligned below clause level (Volk and Samuelsson, 2004). Such resources could be useful for many applications, e.g. as training or evaluation corpora for word and phrase alignment, as training material for data-driven MT systems and for the automatic induction of transfer rules, and for translation studies. Their development is particularly pertinent to the recent efforts towards incorporating syntax into data-driven MT systems, e.g. (Melamed, 2004), (Chiang, 2005), (Galley et al., 2006), (Hearne and Way, 2006), (Marcu et al., 2006), (Zollmann and Venugopal, 2006). In this paper, we focus on how translational divergences are captured within a parallel treebank using a fully-automatic statistical tree-totree aligner.1 In doing so, we take a somewhat different perspective on tree-alignment from that of e.g. (Wu, 2000; Wellington et al., 2006). We do not incorporate trees for the express purpose of constraining the word- and phrase-alignment processes, although this is certainly a consequence of using trees. Our purpose in aligning monolingual syntactic re"
2007.tmi-papers.11,J94-4004,0,0.282874,"arguable as to whether these strings really represent precisely the same meaning. However, the relationship between these words is not ignored in the tree-alignment; rather it is captured by the link between the three NP links in combination. In fact, many different pieces of information can be inferred from the tree-alignment given in (3) regarding the relationship between ’s and de, despite the fact that they are not directly linked; examples exhibiting varying degrees of contextual granularity are given in (4). 3 Translational Divergences Work such as that of e.g. (Lindop and Tsujii, 1992; Dorr, 1994; Trujillo, 1999) makes explicit the types of translational divergences which occur in real data. These divergences occur frequently even for language pairs with relatively similar surface word order, and generally prove challenging for MT models (Hutchins and Somers, 1992). 6 An important characteristic of parallel treebanks is that they provide explicit details, through treealignments, about the occurrence and nature of such divergences. In this section, we examine how translational divergences are represented in the HomeCentre English–French parallel treebank. This corpus comprises a Xerox"
2007.tmi-papers.11,P06-1121,0,0.0175924,"for the development of parallel treebanks, defined as bitexts for which the sentences are annotated with syntactic trees and are aligned below clause level (Volk and Samuelsson, 2004). Such resources could be useful for many applications, e.g. as training or evaluation corpora for word and phrase alignment, as training material for data-driven MT systems and for the automatic induction of transfer rules, and for translation studies. Their development is particularly pertinent to the recent efforts towards incorporating syntax into data-driven MT systems, e.g. (Melamed, 2004), (Chiang, 2005), (Galley et al., 2006), (Hearne and Way, 2006), (Marcu et al., 2006), (Zollmann and Venugopal, 2006). In this paper, we focus on how translational divergences are captured within a parallel treebank using a fully-automatic statistical tree-totree aligner.1 In doing so, we take a somewhat different perspective on tree-alignment from that of e.g. (Wu, 2000; Wellington et al., 2006). We do not incorporate trees for the express purpose of constraining the word- and phrase-alignment processes, although this is certainly a consequence of using trees. Our purpose in aligning monolingual syntactic representations is to mak"
2007.tmi-papers.11,2006.eamt-1.8,1,0.782417,"parallel treebanks, defined as bitexts for which the sentences are annotated with syntactic trees and are aligned below clause level (Volk and Samuelsson, 2004). Such resources could be useful for many applications, e.g. as training or evaluation corpora for word and phrase alignment, as training material for data-driven MT systems and for the automatic induction of transfer rules, and for translation studies. Their development is particularly pertinent to the recent efforts towards incorporating syntax into data-driven MT systems, e.g. (Melamed, 2004), (Chiang, 2005), (Galley et al., 2006), (Hearne and Way, 2006), (Marcu et al., 2006), (Zollmann and Venugopal, 2006). In this paper, we focus on how translational divergences are captured within a parallel treebank using a fully-automatic statistical tree-totree aligner.1 In doing so, we take a somewhat different perspective on tree-alignment from that of e.g. (Wu, 2000; Wellington et al., 2006). We do not incorporate trees for the express purpose of constraining the word- and phrase-alignment processes, although this is certainly a consequence of using trees. Our purpose in aligning monolingual syntactic representations is to make explicit the syntactic"
2007.tmi-papers.11,N03-1017,0,0.0111582,"rate alignments at lexical level. The lexical accuracy scores illustrate clearly that there is an imbalance between precision and recall: recall is consistently higher than precision across all variants of the alignment algorithm. The reason for this is based in the word-alignments used to seed our tree-alignment algorithm. We have adopted the widely used alignment tool GIZA++ (Och and Ney, 2003) (and, more recently, Moses (Koehn et al., 2007) which is based directly on GIZA++) which prioritises broad coverage rather than high precision (Tiedemann, 2004) and is appropriate to stringbased SMT (Koehn et al., 2003). However, the work presented here indicates that the preference in terms of expressing translational divergences through tree-alignment is for the opposite – high precision rather than broad coverage – and this mismatch appears to impact on the overall quality of the alignments. We suggest that this has implications not only for tree-alignment itself but also for the broader area of induction of syntax-aware models for SMT. Despite these observations, training our DOT system on automatically-aligned data gives slightly better translation performance than training on the manually-aligned data."
2007.tmi-papers.11,N06-1014,0,0.0640384,"Missing"
2007.tmi-papers.11,P07-1039,1,0.829709,"mar achieves correspondingly lower coverage and, consequently, performance suffers. We conclude that it is appropriate for tree-alignment to prioritise precision in order to capture translational divergences as accurately as possible, and that MT systems making use of these alignments should employ them in conjunction with broad-coverage models (such as wordand phrase-alignments) in order to preserve robustness. 7 Future Work In order to improve the accuracy of our treealignment algorithm, we plan to investigate alternative word-alignment techniques (e.g. (Tiedemann, 2004; Liang et al., 2006; Ma et al., 2007)) in order to establish which one is most appropriate for our task. With regard to the broader area of parallel treebank construction and the use of statistical parsers such as those of Charniak (2000) and Bikel (2002), we would like to examine the impact of imperfect parse quality on the capture of translational divergences. We plan to extend our aligner so that it works with n-best parse forests on the source and/or target sides, thereby giving the aligner some (limited) influence over the configuration of the aligned parse trees. Finally, we plan to investigate how best to incorporate the t"
2007.tmi-papers.11,W06-1606,0,0.0130102,"ned as bitexts for which the sentences are annotated with syntactic trees and are aligned below clause level (Volk and Samuelsson, 2004). Such resources could be useful for many applications, e.g. as training or evaluation corpora for word and phrase alignment, as training material for data-driven MT systems and for the automatic induction of transfer rules, and for translation studies. Their development is particularly pertinent to the recent efforts towards incorporating syntax into data-driven MT systems, e.g. (Melamed, 2004), (Chiang, 2005), (Galley et al., 2006), (Hearne and Way, 2006), (Marcu et al., 2006), (Zollmann and Venugopal, 2006). In this paper, we focus on how translational divergences are captured within a parallel treebank using a fully-automatic statistical tree-totree aligner.1 In doing so, we take a somewhat different perspective on tree-alignment from that of e.g. (Wu, 2000; Wellington et al., 2006). We do not incorporate trees for the express purpose of constraining the word- and phrase-alignment processes, although this is certainly a consequence of using trees. Our purpose in aligning monolingual syntactic representations is to make explicit the syntactic divergences between s"
2007.tmi-papers.11,J03-1002,0,0.00757775,"e scores for phrase-level alignments are much higher than those for lexical ones, and through the manual evaluation where complex translation phenomena are identified correctly at a high level but then negated by inaccurate alignments at lexical level. The lexical accuracy scores illustrate clearly that there is an imbalance between precision and recall: recall is consistently higher than precision across all variants of the alignment algorithm. The reason for this is based in the word-alignments used to seed our tree-alignment algorithm. We have adopted the widely used alignment tool GIZA++ (Och and Ney, 2003) (and, more recently, Moses (Koehn et al., 2007) which is based directly on GIZA++) which prioritises broad coverage rather than high precision (Tiedemann, 2004) and is appropriate to stringbased SMT (Koehn et al., 2003). However, the work presented here indicates that the preference in terms of expressing translational divergences through tree-alignment is for the opposite – high precision rather than broad coverage – and this mismatch appears to impact on the overall quality of the alignments. We suggest that this has implications not only for tree-alignment itself but also for the broader a"
2007.tmi-papers.11,P02-1040,0,0.079673,"two automatic evaluation methodologies. In Section 5.2 we then go on to describe the capture of translational divergences by manually analysing the aligner output. 5.1 Automatic Evaluation We use two automatic evaluation methodologies in order to gain an overview of aligner performance: (i) we compare the links induced by the algorithm to those induced manually and compute precision and recall scores; (ii) we train a Data-Oriented Translation (DOT) system (Hearne and Way, 2006) on both the manually aligned data and the automatically aligned data and assess translation accuracy using the Bleu (Papineni et al., 2002), NIST (Doddington, 2002) and Meteor 8 http://www.statmt.org/moses/ Although our method of scoring is similar to IBM model 1, and Moses runs GIZA++ trained on IBM model 4, we found that using the Moses word-alignment probabilities yielded better results than those output directly by GIZA++. 9 90 Configurations manual skip2 score1 skip2 score2 skip2 score1 span1 skip2 score2 span1 all links Precision Recall – – 0.6162 0.7783 0.6215 0.7876 0.6256 0.8100 0.6245 0.7962 Alignment Evaluation lexical links non-lexical links Precision Recall Precision Recall – – – – 0.5057 0.7441 0.8394 0.7486 0.5131"
2007.tmi-papers.11,W06-2717,0,0.0494349,"string. On the other hand, if a node remains unaligned in a tree pair there is no equivalent implication. Because treealignment is hierarchical, many other nodes can carry indirect information regarding how an unaligned node (or group of unaligned nodes) is represented in the target string. Some consequences of this are as follows. Firstly, the strategy in word-alignment is to leave as few words unlinked as possible “even when non-literal translations make it difficult to find corresponding words” (Melamed, 1998). Contrast this with the more conservative guideline for tree-alignment given in (Samuelsson and Volk, 2006): nodes are linked only when the substrings they dominate “represent the same meaning and ... could serve as translation units outside the current sentence context.” This latter strategy is affordable because alignments at higher levels in the tree pair will account for the translation equivalence. Secondly, word-alignment allows many-to-many alignments at the word level but not phrasal alignments unless every word in the source phrase corresponds to every word in 2 Of course, an alternative parsing scheme which gives internal labelled structure in such phrases might permit further sub-tree li"
2007.tmi-papers.11,C04-1031,0,0.146414,"ied correctly at a high level but then negated by inaccurate alignments at lexical level. The lexical accuracy scores illustrate clearly that there is an imbalance between precision and recall: recall is consistently higher than precision across all variants of the alignment algorithm. The reason for this is based in the word-alignments used to seed our tree-alignment algorithm. We have adopted the widely used alignment tool GIZA++ (Och and Ney, 2003) (and, more recently, Moses (Koehn et al., 2007) which is based directly on GIZA++) which prioritises broad coverage rather than high precision (Tiedemann, 2004) and is appropriate to stringbased SMT (Koehn et al., 2003). However, the work presented here indicates that the preference in terms of expressing translational divergences through tree-alignment is for the opposite – high precision rather than broad coverage – and this mismatch appears to impact on the overall quality of the alignments. We suggest that this has implications not only for tree-alignment itself but also for the broader area of induction of syntax-aware models for SMT. Despite these observations, training our DOT system on automatically-aligned data gives slightly better translat"
2007.tmi-papers.11,2007.mtsummit-papers.62,1,0.904232,"ual annotation per1 Although the definition of a parallel treebank leaves room for a variety of types of tree structure, in this paper we focus on constituent structure trees only. 85 spective, outlining crucial ways in which it differs from the word-alignment process. We show how translational divergences are represented in an aligned parallel treebank in Section 3, giving insights into why such resources would be useful. In Section 4 we outline an automatic method for statistically inducing tree alignments between parsed sentence pairs – full details of the alignment algorithm are given in (Tinsley et al., 2007). In Section 5 we analyse the output to see how well translational divergences are captured. Finally, in Sections 6 and 7 we conclude and describe plans for future work. 2 Manual Tree-to-Tree Alignment The tree-to-tree alignment process assumes a parsed, translationally equivalent sentence pair and involves introducing links between nonterminal nodes in the source and target phrasestructure trees. Inserting a link between a node pair indicates that the substrings dominated by those nodes are translationally equivalent, i.e. that all meaning in the source substring is encapsulated in the target"
2007.tmi-papers.11,W04-1910,0,0.181524,"bias towards coverage rather than precision. This preference for high precision rather than broad coverage in terms of expressing translational divergences through tree-alignment stands in direct opposition to the situation for SMT word-alignment models. We suggest that this has implications not only for tree-alignment itself but also for the broader area of induction of syntaxaware models for SMT. 1 Introduction Previous work has argued for the development of parallel treebanks, defined as bitexts for which the sentences are annotated with syntactic trees and are aligned below clause level (Volk and Samuelsson, 2004). Such resources could be useful for many applications, e.g. as training or evaluation corpora for word and phrase alignment, as training material for data-driven MT systems and for the automatic induction of transfer rules, and for translation studies. Their development is particularly pertinent to the recent efforts towards incorporating syntax into data-driven MT systems, e.g. (Melamed, 2004), (Chiang, 2005), (Galley et al., 2006), (Hearne and Way, 2006), (Marcu et al., 2006), (Zollmann and Venugopal, 2006). In this paper, we focus on how translational divergences are captured within a para"
2007.tmi-papers.11,P06-1123,0,0.0318525,"and for the automatic induction of transfer rules, and for translation studies. Their development is particularly pertinent to the recent efforts towards incorporating syntax into data-driven MT systems, e.g. (Melamed, 2004), (Chiang, 2005), (Galley et al., 2006), (Hearne and Way, 2006), (Marcu et al., 2006), (Zollmann and Venugopal, 2006). In this paper, we focus on how translational divergences are captured within a parallel treebank using a fully-automatic statistical tree-totree aligner.1 In doing so, we take a somewhat different perspective on tree-alignment from that of e.g. (Wu, 2000; Wellington et al., 2006). We do not incorporate trees for the express purpose of constraining the word- and phrase-alignment processes, although this is certainly a consequence of using trees. Our purpose in aligning monolingual syntactic representations is to make explicit the syntactic divergences between sentence pairs rather than homogenising them. We are not seeking to maximise the number of links between a given tree pair, but rather to find the set of links which most precisely expresses the translational equivalences between that tree pair. How best to exploit such information through model induction for synt"
2007.tmi-papers.11,W06-3119,0,0.019123,"ch the sentences are annotated with syntactic trees and are aligned below clause level (Volk and Samuelsson, 2004). Such resources could be useful for many applications, e.g. as training or evaluation corpora for word and phrase alignment, as training material for data-driven MT systems and for the automatic induction of transfer rules, and for translation studies. Their development is particularly pertinent to the recent efforts towards incorporating syntax into data-driven MT systems, e.g. (Melamed, 2004), (Chiang, 2005), (Galley et al., 2006), (Hearne and Way, 2006), (Marcu et al., 2006), (Zollmann and Venugopal, 2006). In this paper, we focus on how translational divergences are captured within a parallel treebank using a fully-automatic statistical tree-totree aligner.1 In doing so, we take a somewhat different perspective on tree-alignment from that of e.g. (Wu, 2000; Wellington et al., 2006). We do not incorporate trees for the express purpose of constraining the word- and phrase-alignment processes, although this is certainly a consequence of using trees. Our purpose in aligning monolingual syntactic representations is to make explicit the syntactic divergences between sentence pairs rather than homoge"
2007.tmi-papers.11,P04-1083,0,0.0187469,"duction Previous work has argued for the development of parallel treebanks, defined as bitexts for which the sentences are annotated with syntactic trees and are aligned below clause level (Volk and Samuelsson, 2004). Such resources could be useful for many applications, e.g. as training or evaluation corpora for word and phrase alignment, as training material for data-driven MT systems and for the automatic induction of transfer rules, and for translation studies. Their development is particularly pertinent to the recent efforts towards incorporating syntax into data-driven MT systems, e.g. (Melamed, 2004), (Chiang, 2005), (Galley et al., 2006), (Hearne and Way, 2006), (Marcu et al., 2006), (Zollmann and Venugopal, 2006). In this paper, we focus on how translational divergences are captured within a parallel treebank using a fully-automatic statistical tree-totree aligner.1 In doing so, we take a somewhat different perspective on tree-alignment from that of e.g. (Wu, 2000; Wellington et al., 2006). We do not incorporate trees for the express purpose of constraining the word- and phrase-alignment processes, although this is certainly a consequence of using trees. Our purpose in aligning monoling"
2007.tmi-papers.11,P07-2045,0,\N,Missing
2007.tmi-papers.11,dorr-etal-2002-duster,0,\N,Missing
2007.tmi-papers.14,J93-2003,0,0.0191398,"pus, where these two corpus now share exactly the same English sentences. In order to test the scalability of our chunking approach, we first use 150k of the sentence pairs for training, which we call the Small Data set. Then we use all the sentence pairs (around 300k sentence pairs) for training. We call this the Large Data set. We tag all the English sentences in the training and test sets using a maximum entropy-based Part-of-Speech taggerMXPOST (Ratnaparkhi, 1996), which was trained on the Penn Treebank (Marcus et al., 1993). We use the GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003)2 and refinement heuristics described in (Koehn et al., 2003) to derive the final word alignment. We used the Maximum Entropy toolkit ‘maxent’,3 and the Memory-based learning toolkit TiMBL4 for parameter estimation. 4.4 Statistics on Training Data To demonstrate the feasibility of adapting our chunking approach to different languages, we obtained some statistics on the chunks of two training sets derived from French–English (FE, 300k-sentence pairs) and German–English 2 More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 5 iterations of Model 3, an"
2007.tmi-papers.14,J96-1002,0,0.00781452,"pair in English. 3.2 Data Representation (Ramshaw and Marcus, 1995) introduced a data representation for baseNP chunking by converting it into a tagging task: words inside a baseNP were marked I, words outside a baseNP receive an O tag, and a special tag B was used for the first word Parameter Estimation In this section, we briefly introduce two wellknown machine learning techniques we used for parameter estimation, namely Maximum Entropy (MaxEnt) and Memory-based learning (MBL). Both of them are widely used in Natural Language Processing (NLP). Maximum Entropy was first introduced in NLP by (Berger et al., 1996). It is also used for chunking (Koeling, 2000). Memorybased learning (e.g. (Daelemans and Van den Bosch, 2005)) is based on the simple twin ideas that: • learning is based on the storage of exemplars, and • processing is based on the retrieval of exemplars, or for similarity-based reasoning, on the basis of exemplars. 116 IB IE all chunk-initial words receive a B tag all chunk-final words receive a E tag all chunk-initial words receive a B tag, all chunk-final words receive a E tag; if there is only one word in the chunk, it receives a B tag all chunk-initial words receive a B tag, all chunk-f"
2007.tmi-papers.14,2004.tmi-1.11,1,0.842722,"t which posits that all languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signify context. Using a set of closed-class (or “marker”) words, such as determiners, conjunctions, prepositions, possessive and personal pronouns, aligned source–target sentences are segmented into chunks. A chunk is created at each new occurrence of a marker word, with the restriction that each chunk must contain at least one content (or non-marker) word. Although marker-based chunking has been used in bilingual tasks such as machine translation between European languages (Gough and Way, 2004; Groves and Way, 2005; Stroppa and Way, 2006), which are relatively similar with regard to marker words and word orders, it is less appropriate for language pairs as different as Chinese and English (Ma, 2006). 2.2 Bilingual Chunking Bilingual chunkers are usually based on parsing technology. (Wu, 1997) proposed Inversion Transduction Grammar (ITG) as suitable for the task of bilingual parsing. The stochastic ITG brings bilingual constraints to many corpus analysis tasks such as segmentation, bracketing, and parsing, which are usually carried out in a monolingual context. However, it is diffi"
2007.tmi-papers.14,W05-0833,1,0.842543,"ll languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signify context. Using a set of closed-class (or “marker”) words, such as determiners, conjunctions, prepositions, possessive and personal pronouns, aligned source–target sentences are segmented into chunks. A chunk is created at each new occurrence of a marker word, with the restriction that each chunk must contain at least one content (or non-marker) word. Although marker-based chunking has been used in bilingual tasks such as machine translation between European languages (Gough and Way, 2004; Groves and Way, 2005; Stroppa and Way, 2006), which are relatively similar with regard to marker words and word orders, it is less appropriate for language pairs as different as Chinese and English (Ma, 2006). 2.2 Bilingual Chunking Bilingual chunkers are usually based on parsing technology. (Wu, 1997) proposed Inversion Transduction Grammar (ITG) as suitable for the task of bilingual parsing. The stochastic ITG brings bilingual constraints to many corpus analysis tasks such as segmentation, bracketing, and parsing, which are usually carried out in a monolingual context. However, it is difficult to write a broad"
2007.tmi-papers.14,2005.mtsummit-papers.11,0,0.0696529,"Missing"
2007.tmi-papers.14,W00-0726,0,0.321319,"Missing"
2007.tmi-papers.14,N03-1017,0,0.0178152,"er to test the scalability of our chunking approach, we first use 150k of the sentence pairs for training, which we call the Small Data set. Then we use all the sentence pairs (around 300k sentence pairs) for training. We call this the Large Data set. We tag all the English sentences in the training and test sets using a maximum entropy-based Part-of-Speech taggerMXPOST (Ratnaparkhi, 1996), which was trained on the Penn Treebank (Marcus et al., 1993). We use the GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003)2 and refinement heuristics described in (Koehn et al., 2003) to derive the final word alignment. We used the Maximum Entropy toolkit ‘maxent’,3 and the Memory-based learning toolkit TiMBL4 for parameter estimation. 4.4 Statistics on Training Data To demonstrate the feasibility of adapting our chunking approach to different languages, we obtained some statistics on the chunks of two training sets derived from French–English (FE, 300k-sentence pairs) and German–English 2 More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 5 iterations of Model 3, and 5 iterations of Model 4. 3 http://homepages.inf.ed.ac.uk/s0450736 /maxent toolk"
2007.tmi-papers.14,E99-1023,0,0.100783,"Missing"
2007.tmi-papers.14,W00-0729,0,0.027302,"d Marcus, 1995) introduced a data representation for baseNP chunking by converting it into a tagging task: words inside a baseNP were marked I, words outside a baseNP receive an O tag, and a special tag B was used for the first word Parameter Estimation In this section, we briefly introduce two wellknown machine learning techniques we used for parameter estimation, namely Maximum Entropy (MaxEnt) and Memory-based learning (MBL). Both of them are widely used in Natural Language Processing (NLP). Maximum Entropy was first introduced in NLP by (Berger et al., 1996). It is also used for chunking (Koeling, 2000). Memorybased learning (e.g. (Daelemans and Van den Bosch, 2005)) is based on the simple twin ideas that: • learning is based on the storage of exemplars, and • processing is based on the retrieval of exemplars, or for similarity-based reasoning, on the basis of exemplars. 116 IB IE all chunk-initial words receive a B tag all chunk-final words receive a E tag all chunk-initial words receive a B tag, all chunk-final words receive a E tag; if there is only one word in the chunk, it receives a B tag all chunk-initial words receive a B tag, all chunk-final words receive a E tag; if there is only o"
2007.tmi-papers.14,N06-1014,0,0.0431116,"Missing"
2007.tmi-papers.14,2006.iwslt-evaluation.4,1,0.855542,"d for surface syntax by a specific closed set of lexemes or morphemes which signify context. Using a set of closed-class (or “marker”) words, such as determiners, conjunctions, prepositions, possessive and personal pronouns, aligned source–target sentences are segmented into chunks. A chunk is created at each new occurrence of a marker word, with the restriction that each chunk must contain at least one content (or non-marker) word. Although marker-based chunking has been used in bilingual tasks such as machine translation between European languages (Gough and Way, 2004; Groves and Way, 2005; Stroppa and Way, 2006), which are relatively similar with regard to marker words and word orders, it is less appropriate for language pairs as different as Chinese and English (Ma, 2006). 2.2 Bilingual Chunking Bilingual chunkers are usually based on parsing technology. (Wu, 1997) proposed Inversion Transduction Grammar (ITG) as suitable for the task of bilingual parsing. The stochastic ITG brings bilingual constraints to many corpus analysis tasks such as segmentation, bracketing, and parsing, which are usually carried out in a monolingual context. However, it is difficult to write a broad bilingual ITG grammar ca"
2007.tmi-papers.14,C04-1031,0,0.0520402,"Missing"
2007.tmi-papers.14,C02-1010,0,0.0149445,"ard to marker words and word orders, it is less appropriate for language pairs as different as Chinese and English (Ma, 2006). 2.2 Bilingual Chunking Bilingual chunkers are usually based on parsing technology. (Wu, 1997) proposed Inversion Transduction Grammar (ITG) as suitable for the task of bilingual parsing. The stochastic ITG brings bilingual constraints to many corpus analysis tasks such as segmentation, bracketing, and parsing, which are usually carried out in a monolingual context. However, it is difficult to write a broad bilingual ITG grammar capable of dealing with long sentences. (Wang et al., 2002) proposed an algorithm integrating chunking and alignment and obtained good precision. However, this method needs quite a lot of syntax information and prior knowledge. (Liu et al., 2004) proposed an integrated probabilistic model for bilingual chunking and alignment independent of syntax information and grammatical rules. 3 3.1 Alignment-Guided Chunking Notation While in this paper, we focus on both French– English and German–English, the method proposed is applicable to any language pair. The notation however assumes the French– English task in what follows. Given a French sentence f1I consi"
2007.tmi-papers.14,J97-3002,0,0.185356,"nto chunks. A chunk is created at each new occurrence of a marker word, with the restriction that each chunk must contain at least one content (or non-marker) word. Although marker-based chunking has been used in bilingual tasks such as machine translation between European languages (Gough and Way, 2004; Groves and Way, 2005; Stroppa and Way, 2006), which are relatively similar with regard to marker words and word orders, it is less appropriate for language pairs as different as Chinese and English (Ma, 2006). 2.2 Bilingual Chunking Bilingual chunkers are usually based on parsing technology. (Wu, 1997) proposed Inversion Transduction Grammar (ITG) as suitable for the task of bilingual parsing. The stochastic ITG brings bilingual constraints to many corpus analysis tasks such as segmentation, bracketing, and parsing, which are usually carried out in a monolingual context. However, it is difficult to write a broad bilingual ITG grammar capable of dealing with long sentences. (Wang et al., 2002) proposed an algorithm integrating chunking and alignment and obtained good precision. However, this method needs quite a lot of syntax information and prior knowledge. (Liu et al., 2004) proposed an in"
2007.tmi-papers.14,P07-1039,1,0.885392,"Missing"
2007.tmi-papers.14,J93-2004,0,0.0305366,"gual chunking. Section 3 describes our chunking method. In Section 4, the experimental setting is described. In Section 5, we evaluate our chunking method on a one-reference ‘gold standard’ testset. Section 6 concludes the paper and gives avenues for future work. 2 2.1 Previous Research Monolingual Chunking Most state-of-the-art monolingual chunking methods are linguistically motivated. The CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) defined chunking as dividing text into syntactically related nonoverlapping groups of words. Chunks are directly converted from the Penn Treebank (Marcus et al., 1993) and each chunk is labelled with a specific grammatical category, such as NP, VP, PP, ADJP etc. This chunking method is sensitive to the grammars of a specific language and performs chunking in a monolingual context. Marker-based chunking is another syntaxaware chunking strategy. This chunking approach is based on the “Marker Hypothesis” (Green, 1979), a psycholinguistic constraint which posits that all languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signify context. Using a set of closed-class (or “marker”) words, such as determiners, conjunction"
2007.tmi-papers.14,J03-1002,0,0.00848913,"corpus now share exactly the same English sentences. In order to test the scalability of our chunking approach, we first use 150k of the sentence pairs for training, which we call the Small Data set. Then we use all the sentence pairs (around 300k sentence pairs) for training. We call this the Large Data set. We tag all the English sentences in the training and test sets using a maximum entropy-based Part-of-Speech taggerMXPOST (Ratnaparkhi, 1996), which was trained on the Penn Treebank (Marcus et al., 1993). We use the GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003)2 and refinement heuristics described in (Koehn et al., 2003) to derive the final word alignment. We used the Maximum Entropy toolkit ‘maxent’,3 and the Memory-based learning toolkit TiMBL4 for parameter estimation. 4.4 Statistics on Training Data To demonstrate the feasibility of adapting our chunking approach to different languages, we obtained some statistics on the chunks of two training sets derived from French–English (FE, 300k-sentence pairs) and German–English 2 More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 5 iterations of Model 3, and 5 iterations of Mo"
2007.tmi-papers.14,W95-0107,0,0.059039,"tation for the English sentence in Figure 1 is shown in (2): (2) Word alignment: 0-0 1-1 2-2 34 4-5 5-7 6-6 7-8 8-9 9-10 10-12 11-11 12-13 The/E city/E bears/E the/I weight/E of/E powerful/I symbols/E for/E all/E three/E monotheistic/I religions/E ./ The AGC chunks derivable via our method are displayed in Figure 1. Again, note the dependence of determiners and adjectives on their following head noun. 3.3 Figure 1: Example of AGC chunks Note that the method is able to capture adjective–noun combinations in each language, as well as the determiner-noun pair in English. 3.2 Data Representation (Ramshaw and Marcus, 1995) introduced a data representation for baseNP chunking by converting it into a tagging task: words inside a baseNP were marked I, words outside a baseNP receive an O tag, and a special tag B was used for the first word Parameter Estimation In this section, we briefly introduce two wellknown machine learning techniques we used for parameter estimation, namely Maximum Entropy (MaxEnt) and Memory-based learning (MBL). Both of them are widely used in Natural Language Processing (NLP). Maximum Entropy was first introduced in NLP by (Berger et al., 1996). It is also used for chunking (Koeling, 2000)."
2007.tmi-papers.14,W96-0213,0,\N,Missing
2007.tmi-papers.26,2006.iwslt-evaluation.15,1,0.871056,"Missing"
2007.tmi-papers.26,2005.mtsummit-ebmt.14,1,0.803034,"Missing"
2007.tmi-papers.26,2006.eamt-1.21,1,0.889231,"Missing"
2007.tmi-papers.28,W05-0909,0,0.0472932,"lar for the optimisation of the weights of the log-linear model), and the final evaluation is conducted using the test set (using the CRR=Correct Recognition Result input condition). For both Chinese and Italian, POS-tagging is performed using the M X P OST tagger (Ratnaparkhi, 1996). Table 1 summarizes the various corpus statistics. The number of training/test examples refers to the examples involved in the classification task. For all experiments, the quality of the translation output is evaluated using the accuracy measures BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), using 7 references and ignoring case information. For BLEU and NIST, we also Train. Sentences Running words Vocabulary size Train. examples Dev. Sentences Running words Vocabulary size Test examples Eval. Sentences Running words Vocabulary size Test examples Chinese–English Italian–English 44,501 323,958 351,303 11,421 10,363 434,442 21,484 156,237 169,476 10,418 7,359 391,626 489 (7 refs.) 5,214 39,183 1,137 1,821 8,004 489 (7 refs.) 4,976 39,368 1,234 1,776 7,993 500 (7 refs.) 5,550 44,089 1,328 2,038 8,301 500 (7 refs.) 5,787 44,271 1,467 1,976 9,103 Table 1: Chinese–English and Italian–E"
2007.tmi-papers.28,J93-2003,0,0.00624254,"ontext-informed features, i.e. the sourcesimilarity exploitation, results in an improvement in translation quality, for Italian-to-English and Chinese-to-English translations tasks. 2 Log-Linear Phrase-Based SMT In statistical machine translation (SMT), translation is modeled as a decision process, in which the translation eI1 = e1 . . . ei . . . eI of a source sentence f1J = f1J = f1 . . . fj . . . fJ is chosen to maximize: argmax P(eI1 |f1J ) = argmax P(f1J |eI1 ).P(eI1 ), I,eI1 I,eI1 (1) where P(f1J |eI1 ) and P(eI1 ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P(eI1 |f1J ) is directly modeled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language 231 model: log P(eI1 |f1J ) = m X λm hm (f1J , eI1 , sK 1 ) m=1 + λLM log P(eI1 ), (2) where sK 1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (e˜1 , . . . , e˜k ) and (f˜1 , . . . , f˜k ) such that (we set i0 := 0): ∀1 ≤ k ≤ K, sk := (ik ; bk , jk ), e˜k := eik−1 +1 . . . eik , f˜k := fbk . . . fjk . K X ˜"
2007.tmi-papers.28,2003.mtsummit-papers.4,0,0.0138252,"the translation of these fragments (transfer) (Nagao, 1984; Somers, 1999; Carl and Way, 2003). A number of matching techniques and notions of similarity have been proposed. Consequently, EBMT crucially relies on the retrieval of source sentences similar to f in the bilingual training corpus; in other words, EBMT is source-similarity based. Let us also mention (Somers et al., 1994), which marks the fragments to translate with their (left and right) contexts. Source and Target Similarity While the use of target-similarity may avoid problems such as boundary-friction usually encountered in EBMT (Brown et al., 2003), the use of source-similarity may limit ambiguity problems (cf. Section 3). By exploiting the two types of similarity, we hope to benefit from the strength of both aspects. 5 5.1 Experimental Results Data, Tasks, and Baseline The experiments were carried out using the Chinese–English and Italian–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. Trai"
2007.tmi-papers.28,P05-1048,0,0.0497272,"t the accuracy and the flexibility of discriminative learning (Cowan et al., 2006; Liang et al., 2006; Tillmann and Zhang, 2006; Wellington et al., 2006). These papers generally require one to redefine one’s training procedures; on the contrary our approach introduces new features while keeping the strength of existing state-of-the-art systems. The exploitation of source-similarity is one of the key components of EBMT (Nagao, 1984; Somers, 1999; Carl and Way, 2003); one could say that our approach is a combination of EBMT and SMT since we exploit both source similarity and target similarity. (Carpuat and Wu, 2005) present an attempt to use word-sense disambiguation techniques to MT in order to enhance lexical selection; in a sense, we are also performing some sort of word-sense disambiguation, even if the handling of lexical selection is performed totally implicitly in our case. 7 Conclusion In this paper, we have introduced new features for log-linear phrase-based SMT, that take into account contextual information about the source phrases to translate. This contextual information can take the form of left and right context words, as well as other source of knowledge such as Part-Of-Speech information."
2007.tmi-papers.28,W02-1001,0,0.0225766,"This does not affect our general line of reasoning. 3.1 Context-Informed Features Context-Based Disambiguation The optimization of the feature weights λm can be performed in a discriminative learning setting (Och and Ney, 2002). However, it is important to note that these weights are meta-parameters. Indeed, the dependencies between the parameters of the standard phrase-based approach consist of: (i) relationships between single phrases (modeled by ˜ (ii) relationships between consecutive target h), words (modeled by the language model), which is generally characteristic of generative models (Collins, 2002; Dietterich, 2002). Notably, dependencies between consecutive source phrases are not directly expressed. Discriminative frameworks usually allow for the introduction of (relatively) unrestricted dependencies that are relevant to the decision process. In particular, disambiguation problems can be solved by taking the direct context of the entity to disambiguate into account (e.g. Dietterich (2002)). In the translation example displayed in Figure 1, the source right context is sufficient to solve the ambiguity: when followed by di baseball, the (Italian) word partita is very likely to correspon"
2007.tmi-papers.28,W06-1628,0,0.070058,"Missing"
2007.tmi-papers.28,W06-1607,0,0.0273569,"f source phrases into account. Word-based features A feature that includes the direct left and right context words (resp. fbk −1 and fjk +1 ) of a given phrase f˜k = fbk . . . fjk takes the following form: 4 Memory-Based Disambiguation 4.1 A Classification Approach The direct estimation of P(e˜k |f˜k , CI(f˜k )), for example using relative frequencies, is problematic. Indeed, it is well known that the estimation of P(e˜k |f˜k ) using relative frequencies results in the overestimation of the probabilities of long phrases K X J I K ˜ ˜ hm (fk , fbk −1 , fjk +1 , e˜k , sk ). (Zens and Ney, 2004; Foster et al., 2006); a frehm (f1 , e1 , s1 ) = quent remedy consists of introducing a smoothing k=1 factor, which takes the form of lexical-based feaIn this case, the contextual information can be tures (Zens and Ney, 2004). Similar issues and seen as a window of size 3 (focus phrase + left a variety of smoothing techniques are discussed context word + right context word), centered on in (Foster et al., 2006). In the case of contextthe source phrase f˜k . Larger contexts may also be informed features, since the context is also taken considered. More generally, we have: into account, this estimation problem can o"
2007.tmi-papers.28,N03-1017,0,0.116106,"Missing"
2007.tmi-papers.28,koen-2004-pharaoh,0,0.292053,"is a feature that applies to a single where h phrase-pair.1 It thus follows: m X m=1 λm K X ˜ m (f˜k , e˜k , sk ) = h K X ˜ f˜k , e˜k , sk ), h( k=1 k=1 ˜= with h m X ˜ m . (4) λm h m=1 In this context, the translation process amounts to: (i) choosing a segmentation of the source sentence, (ii) translating each source phrase, and possibly (iii) re-ordering the target segments obtained. The target language model is used to guide the decision process; in case no particular constraints are assumed, it is common to employ beam search techniques to reduce the number of hypotheses to be considered (Koehn, 2004). Equations (2) and (4) characterize what is referred to as the standard phrase-based approach in the following. 1 Figure 1: Examples of ambiguity for the (Italian) word partita, easily solved when considering its context 3 A remarkable property of this approach is that the usual translational features involved in those models only depend on a pair of source/target phrases, i.e. they do not take into account the contexts of those phrases. This means that each feature hm in equation (2) can be rewritten as: hm (f1J , eI1 , sK 1 )= C’`e una partita di baseball oggi ? (⇔ Is there a baseball game"
2007.tmi-papers.28,P06-1096,0,0.0564009,"Missing"
2007.tmi-papers.28,P02-1038,0,0.762423,"ork that enables the estimation of these features while avoiding sparseness problems. We evaluate the performance of our approach on Italian-to-English and Chineseto-English translation tasks using a state-of-the-art phrase-based SMT system, and report significant improvements for both BLEU and NIST scores when adding the context-informed features. 1 Introduction In log-linear phrase-based SMT, the probability P(eI1 |f1J ) of target phrase eI1 given a source phrase f1J is modeled as a (log-linear) combination of features that usually comprise some translational features, and a language model (Och and Ney, 2002). The usual translational features involved in those models express dependencies between source and target phrases, but not dependencies between source phrases themselves. In particular, the context in which those phrases occur is never taken into account during translation. While the language model can be seen as a way to exploit target similarity (between the translation and Andy Way Dublin City University, Dublin, Ireland away@ computing.dcu.ie other target sentences), one could ask whether it is also possible to exploit source similarity, i.e. to take into account the context in which the"
2007.tmi-papers.28,P03-1021,0,0.0346756,"xt is an intermediary result of the estimation of P(e˜k |f˜k , CI(f˜k )). In ˜ m (f˜k , CI(f˜k ), e˜k , sk ) = addition to the feature h log P(e˜k |f˜k , CI(f˜k )), we consider a simple binary feature based on this intermediary result: ˜ best h   1 if e˜k is (one of) the target phrases = with the most support,   0 otherwise, where “most support” means the highest probability according to P(e˜k |f˜k , CI(f˜k )). The two ˜ m and h ˜ best are integrated in the logfeatures h linear model. As for the standard phrasebased approach, their weights are optimized using minimum-error-rate training (Och, 2003). 4.4 Implementation Issues When predicting a target phrase given a source phrase and its context, the source phrase is intuitively the feature with the highest prediction power; in all our experiments, it is the feature with the highest IG. In the trie constructed by IGT REE, this is thus the feature on which the first branching decision is taken. Consequently, when classifying a source phrase f˜k with its context, there are two possible situations, depending on f˜k being in the training material or not. In the first case, f˜k is matched, and we proceed further down the trie. At this stage, i"
2007.tmi-papers.28,P02-1040,0,0.110535,"pment set (devset 4) was used for tuning purposes (in particular for the optimisation of the weights of the log-linear model), and the final evaluation is conducted using the test set (using the CRR=Correct Recognition Result input condition). For both Chinese and Italian, POS-tagging is performed using the M X P OST tagger (Ratnaparkhi, 1996). Table 1 summarizes the various corpus statistics. The number of training/test examples refers to the examples involved in the classification task. For all experiments, the quality of the translation output is evaluated using the accuracy measures BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), using 7 references and ignoring case information. For BLEU and NIST, we also Train. Sentences Running words Vocabulary size Train. examples Dev. Sentences Running words Vocabulary size Test examples Eval. Sentences Running words Vocabulary size Test examples Chinese–English Italian–English 44,501 323,958 351,303 11,421 10,363 434,442 21,484 156,237 169,476 10,418 7,359 391,626 489 (7 refs.) 5,214 39,183 1,137 1,821 8,004 489 (7 refs.) 4,976 39,368 1,234 1,776 7,993 500 (7 refs.) 5,550 44,089 1,328 2,038 8,301 500 (7 refs.) 5,787"
2007.tmi-papers.28,W96-0213,0,0.361764,"Missing"
2007.tmi-papers.28,W05-0908,0,0.0612328,"e Train. examples Dev. Sentences Running words Vocabulary size Test examples Eval. Sentences Running words Vocabulary size Test examples Chinese–English Italian–English 44,501 323,958 351,303 11,421 10,363 434,442 21,484 156,237 169,476 10,418 7,359 391,626 489 (7 refs.) 5,214 39,183 1,137 1,821 8,004 489 (7 refs.) 4,976 39,368 1,234 1,776 7,993 500 (7 refs.) 5,550 44,089 1,328 2,038 8,301 500 (7 refs.) 5,787 44,271 1,467 1,976 9,103 Table 1: Chinese–English and Italian–English corpus statistics report statistical significance p-values, estimated using approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005).7 To assess the validity of our approach, we use the state-of-the-art phrase-based SMT system M OSES (Koehn et al., 2007).8 The baseline system is composed of the usual features: phrase-based probabilities and lexical weighting in both directions, phrase and word penalties, and re-ordering. Our system additionally includes the memory-based features described in Sections 3 and 4. 5.2 Translation Results The results obtained for the Italian–English and Chinese–English translation tasks using the IWSLT data are summarized in Table 2. The contextual information may include the (context) words, th"
2007.tmi-papers.28,takezawa-etal-2002-toward,0,0.0141726,"xts. Source and Target Similarity While the use of target-similarity may avoid problems such as boundary-friction usually encountered in EBMT (Brown et al., 2003), the use of source-similarity may limit ambiguity problems (cf. Section 3). By exploiting the two types of similarity, we hope to benefit from the strength of both aspects. 5 5.1 Experimental Results Data, Tasks, and Baseline The experiments were carried out using the Chinese–English and Italian–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. Training was performed using the default training set, to which we added the sets devset1, devset2, and devset3. The development set (devset 4) was used for tuning purposes (in particular for the optimisation of the weights of the log-linear model), and the final evaluation is conducted using the test set (using the CRR=Correct Recognition Result input condition). For both Chinese and Italian, POS-tagging is performed using the M X P OST tagger (Ratnaparkhi, 1"
2007.tmi-papers.28,P06-1091,0,0.069543,"will select the correct translation baseball game as the most 232 probable among all the possible combinations of target words: gone of baseball, game of baseball, baseball partita, baseball game, etc., but this solution appears to be more expensive than simply looking at the context. In particular, the context can be used to early prune weak candidates, which allows spending more time on promising candidates. Several discriminative frameworks have been proposed recently in the context of MT to fully exploit the flexibility of discriminative approaches (Cowan et al., 2006; Liang et al., 2006; Tillmann and Zhang, 2006; Wellington et al., 2006). Unfortunately, this flexibility usually comes at the price of training complexity. An alternative in-between approach, pursued in this paper, consists of introducing context-informed features in the original log-linear framework. This enables us to take the context of source phrases into accounts, while benefiting from the existing training and optimization procedures of the standard phrase-based approach. 3.2 Class-based features In addition to the context words themselves, it is possible to exploit several knowledge sources characterizing the context. For example,"
2007.tmi-papers.28,2006.amta-papers.28,0,0.0677454,"anslation baseball game as the most 232 probable among all the possible combinations of target words: gone of baseball, game of baseball, baseball partita, baseball game, etc., but this solution appears to be more expensive than simply looking at the context. In particular, the context can be used to early prune weak candidates, which allows spending more time on promising candidates. Several discriminative frameworks have been proposed recently in the context of MT to fully exploit the flexibility of discriminative approaches (Cowan et al., 2006; Liang et al., 2006; Tillmann and Zhang, 2006; Wellington et al., 2006). Unfortunately, this flexibility usually comes at the price of training complexity. An alternative in-between approach, pursued in this paper, consists of introducing context-informed features in the original log-linear framework. This enables us to take the context of source phrases into accounts, while benefiting from the existing training and optimization procedures of the standard phrase-based approach. 3.2 Class-based features In addition to the context words themselves, it is possible to exploit several knowledge sources characterizing the context. For example, we can consider the Part-"
2007.tmi-papers.28,N04-1033,0,0.0389915,"t take the context of source phrases into account. Word-based features A feature that includes the direct left and right context words (resp. fbk −1 and fjk +1 ) of a given phrase f˜k = fbk . . . fjk takes the following form: 4 Memory-Based Disambiguation 4.1 A Classification Approach The direct estimation of P(e˜k |f˜k , CI(f˜k )), for example using relative frequencies, is problematic. Indeed, it is well known that the estimation of P(e˜k |f˜k ) using relative frequencies results in the overestimation of the probabilities of long phrases K X J I K ˜ ˜ hm (fk , fbk −1 , fjk +1 , e˜k , sk ). (Zens and Ney, 2004; Foster et al., 2006); a frehm (f1 , e1 , s1 ) = quent remedy consists of introducing a smoothing k=1 factor, which takes the form of lexical-based feaIn this case, the contextual information can be tures (Zens and Ney, 2004). Similar issues and seen as a window of size 3 (focus phrase + left a variety of smoothing techniques are discussed context word + right context word), centered on in (Foster et al., 2006). In the case of contextthe source phrase f˜k . Larger contexts may also be informed features, since the context is also taken considered. More generally, we have: into account, this es"
2007.tmi-papers.28,2006.iwslt-evaluation.1,0,\N,Missing
2008.iwslt-evaluation.3,2006.iwslt-evaluation.4,1,0.856625,", we give details about the various novel extensions to M AT R E X as summarised above. In Section 3, the experimental setup is presented and experimental results obtained for various language pairs are reported in Section 4. In Section 5, we conclude, and provide avenues for further research. 2. The M AT R E X System The M AT R E X system is a hybrid system which exploits both EBMT and SMT techniques to extract a dataset of aligned chunks [6]. It is a modular data-driven MT engine, built following established designpatterns, and consists of a number of extensible and re-implementable modules [1, 6], the most significant of which are: • Word Alignment Module: takes as its input an aligned corpus and outputs a set of word alignments. 1. Introduction In this paper, we describe some new extensions to the datadriven MT system developed at DCU, M AT R E X (Machine Translation using examples), subsequent to our participation at IWSLT 2006 [1] and IWSLT 2007 [2]. Firstly, we test our novel word and phrase alignment modules including word packing [3], syntax-enhanced word alignment [4] and parallel treebank-based phrase extraction [5]. Secondly, in addition to smoothing translation tables with o"
2008.iwslt-evaluation.3,2007.iwslt-1.10,1,0.867113,"exploits both EBMT and SMT techniques to extract a dataset of aligned chunks [6]. It is a modular data-driven MT engine, built following established designpatterns, and consists of a number of extensible and re-implementable modules [1, 6], the most significant of which are: • Word Alignment Module: takes as its input an aligned corpus and outputs a set of word alignments. 1. Introduction In this paper, we describe some new extensions to the datadriven MT system developed at DCU, M AT R E X (Machine Translation using examples), subsequent to our participation at IWSLT 2006 [1] and IWSLT 2007 [2]. Firstly, we test our novel word and phrase alignment modules including word packing [3], syntax-enhanced word alignment [4] and parallel treebank-based phrase extraction [5]. Secondly, in addition to smoothing translation tables with out-of-domain data [2], we attempt to improve system - 26 - • Chunking Module: takes in an aligned corpus and produces source and target chunks. • Chunk Alignment Module: takes the source and target chunks and aligns them on a sentence-by-sentence level. • Decoder: searches for a translation using the original aligned corpus and derived chunk and word alignments"
2008.iwslt-evaluation.3,P07-1039,1,0.813558,"a modular data-driven MT engine, built following established designpatterns, and consists of a number of extensible and re-implementable modules [1, 6], the most significant of which are: • Word Alignment Module: takes as its input an aligned corpus and outputs a set of word alignments. 1. Introduction In this paper, we describe some new extensions to the datadriven MT system developed at DCU, M AT R E X (Machine Translation using examples), subsequent to our participation at IWSLT 2006 [1] and IWSLT 2007 [2]. Firstly, we test our novel word and phrase alignment modules including word packing [3], syntax-enhanced word alignment [4] and parallel treebank-based phrase extraction [5]. Secondly, in addition to smoothing translation tables with out-of-domain data [2], we attempt to improve system - 26 - • Chunking Module: takes in an aligned corpus and produces source and target chunks. • Chunk Alignment Module: takes the source and target chunks and aligns them on a sentence-by-sentence level. • Decoder: searches for a translation using the original aligned corpus and derived chunk and word alignments. Proceedings of IWSLT 2008, Hawaii - U.S.A. For this participation, our system has been"
2008.iwslt-evaluation.3,W08-0409,1,0.875211,"lt following established designpatterns, and consists of a number of extensible and re-implementable modules [1, 6], the most significant of which are: • Word Alignment Module: takes as its input an aligned corpus and outputs a set of word alignments. 1. Introduction In this paper, we describe some new extensions to the datadriven MT system developed at DCU, M AT R E X (Machine Translation using examples), subsequent to our participation at IWSLT 2006 [1] and IWSLT 2007 [2]. Firstly, we test our novel word and phrase alignment modules including word packing [3], syntax-enhanced word alignment [4] and parallel treebank-based phrase extraction [5]. Secondly, in addition to smoothing translation tables with out-of-domain data [2], we attempt to improve system - 26 - • Chunking Module: takes in an aligned corpus and produces source and target chunks. • Chunk Alignment Module: takes the source and target chunks and aligns them on a sentence-by-sentence level. • Decoder: searches for a translation using the original aligned corpus and derived chunk and word alignments. Proceedings of IWSLT 2008, Hawaii - U.S.A. For this participation, our system has been enriched with various novel word and"
2008.iwslt-evaluation.3,2007.mtsummit-papers.62,1,0.893921,"sts of a number of extensible and re-implementable modules [1, 6], the most significant of which are: • Word Alignment Module: takes as its input an aligned corpus and outputs a set of word alignments. 1. Introduction In this paper, we describe some new extensions to the datadriven MT system developed at DCU, M AT R E X (Machine Translation using examples), subsequent to our participation at IWSLT 2006 [1] and IWSLT 2007 [2]. Firstly, we test our novel word and phrase alignment modules including word packing [3], syntax-enhanced word alignment [4] and parallel treebank-based phrase extraction [5]. Secondly, in addition to smoothing translation tables with out-of-domain data [2], we attempt to improve system - 26 - • Chunking Module: takes in an aligned corpus and produces source and target chunks. • Chunk Alignment Module: takes the source and target chunks and aligns them on a sentence-by-sentence level. • Decoder: searches for a translation using the original aligned corpus and derived chunk and word alignments. Proceedings of IWSLT 2008, Hawaii - U.S.A. For this participation, our system has been enriched with various novel word and phrasal alignment modules, including word packing"
2008.iwslt-evaluation.3,J93-2003,0,0.0128769,"original aligned corpus and derived chunk and word alignments. Proceedings of IWSLT 2008, Hawaii - U.S.A. For this participation, our system has been enriched with various novel word and phrasal alignment modules, including word packing [3], syntax-enhanced word alignment [4] and parallel treebank-based phrase extraction [5] as described in the following sections. We also developed some effective domain adaptation heuristics to facilitate the use of large-scale out-of-domain data to help improve system performance. 2.1. Improving Word Alignment via Word Packing Most current statistical models [7, 8] treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words, a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realise the same concept using varying numbers of"
2008.iwslt-evaluation.3,C96-2141,0,0.125896,"original aligned corpus and derived chunk and word alignments. Proceedings of IWSLT 2008, Hawaii - U.S.A. For this participation, our system has been enriched with various novel word and phrasal alignment modules, including word packing [3], syntax-enhanced word alignment [4] and parallel treebank-based phrase extraction [5] as described in the following sections. We also developed some effective domain adaptation heuristics to facilitate the use of large-scale out-of-domain data to help improve system performance. 2.1. Improving Word Alignment via Word Packing Most current statistical models [7, 8] treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words, a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realise the same concept using varying numbers of"
2008.iwslt-evaluation.3,J97-3002,0,0.0173211,"the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words, a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realise the same concept using varying numbers of words [9]. Although some statistical alignment models allow for 1to-n word alignments for those reasons, they rarely question the monolingual tokenization, and the basic unit of the alignment process remains the word. In our system, we focus on 1-to-n alignments with the goal of simplifying the task of automatic word aligners by packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our word packi"
2008.iwslt-evaluation.3,J03-1002,0,0.0208796,", they rarely question the monolingual tokenization, and the basic unit of the alignment process remains the word. In our system, we focus on 1-to-n alignments with the goal of simplifying the task of automatic word aligners by packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our word packing approach consists of using the output from an existing statistical word aligner (G IZA ++, [10]) to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment [11, 12, 13]. We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. In this way, word packing can be applied several times; once we have grouped some words together, they become the new basic unit to consider, and we can re-run"
2008.iwslt-evaluation.3,W96-0107,0,0.0442187,"y packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our word packing approach consists of using the output from an existing statistical word aligner (G IZA ++, [10]) to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment [11, 12, 13]. We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. In this way, word packing can be applied several times; once we have grouped some words together, they become the new basic unit to consider, and we can re-run the same method to get additional groupings. However, in practice, we have not seen much benefit from running it more than twice (few new candidates are extracted after two iterations). Word packing has been shown to be an effective"
2008.iwslt-evaluation.3,J00-2004,0,0.039119,"y packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our word packing approach consists of using the output from an existing statistical word aligner (G IZA ++, [10]) to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment [11, 12, 13]. We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. In this way, word packing can be applied several times; once we have grouped some words together, they become the new basic unit to consider, and we can re-run the same method to get additional groupings. However, in practice, we have not seen much benefit from running it more than twice (few new candidates are extracted after two iterations). Word packing has been shown to be an effective"
2008.iwslt-evaluation.3,E03-1026,0,0.0324119,"y packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our word packing approach consists of using the output from an existing statistical word aligner (G IZA ++, [10]) to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment [11, 12, 13]. We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. In this way, word packing can be applied several times; once we have grouped some words together, they become the new basic unit to consider, and we can re-run the same method to get additional groupings. However, in practice, we have not seen much benefit from running it more than twice (few new candidates are extracted after two iterations). Word packing has been shown to be an effective"
2008.iwslt-evaluation.3,H05-1012,0,0.0188177,"nment process. Given such an observation, our model is composed of two related alignment models. The first one is an anchor alignment model which is used to find a set of anchor links; the other one is a syntax-enhanced alignment model aiming to process the words left unaligned after anchoring. Figure 1: Syntactic dependencies can help word alignment eI1 , Formally, given a source sentence f1J and target sentence we seek to find the optimal alignment Aˆ such that: Aˆ = argmaxP (A|f1J , eI1 ) (1) A We use a model (2) that directly models the linkage between source and target words similarly to [14]. We decompose this model into an anchor alignment model (3) and a syntax-enhanced model (4) by distinguishing anchor alignments from non-anchor alignments: p(A|f1J , eI1 ) = J Y p(aj |f1J , eI1 , aj−1 1 ) (2) j=0 = 1 · pǫ (A∆ |f1J , eI1 ) · Z Y p(aj |f1J , eI1 , aj−1 1 , A∆ ) ¯ j∈∆ (3) (4) Proceedings of IWSLT 2008, Hawaii - U.S.A. Here A∆ denotes the set of anchor alignment where a set of word indice ∆ ⊂ {1, . . . , J} are involved. The syntax-enhanced model is used to model the align¯ We ment of the words left unaligned after anchoring ( ∆). directly model the linkage between source and tar"
2008.iwslt-evaluation.3,2008.jeptalnrecital-court.14,1,0.740903,"e results. Compared to word packing, the syntax-enhanced model has the advantage of capturing long distance dependencies between words. However, this approach relies on the quality of the dependency parsing. Furthermore, the acquisition of anchor alignments is essential for the overall performance. 2.3. Parallel Treebank-Based Phrase Extraction Previous experiments have shown that, for a number of European language pairs, augmenting the standard phrasebased translation model with syntactically motivated phrase pairs extracted from a parallel treebank consistently improves translation accuracy [15, 16]. A parallel treebank is a linguistically annotated parallel corpus aligned at subsentential level. The sub-sentential alignments imply translational equivalence between the yields of the linked constituent pair. Figure 2 contrasts phrase-pair extraction from a parallel treebank with standard SMT phrase extraction based on word alignments. In order to use this technique for the purposes of the IWSLT tasks, we needed to build a parallel treebank for each language pair from the original parallel training corpus. That is, one for Chinese–English (also serves English–Chinese), Chinese–Spanish and"
2008.iwslt-evaluation.3,2006.iwslt-evaluation.6,0,0.0162135,"degradation in performance when adapting the phrase-based system with out-of-domain phrase translation is due to two main problems: • First, different domains indicate different phrase styles, i.e. questions versus news style; • Second, phrases from the out-of-domain data usually have a higher score than in-domain phrases due to the fact that there is much more out-of-domain data than in-domain data. This might cause a bias toward the choice of an incorrect translation obtained via out-ofdomain data of words and phrases, when these occur in both in-domain and out-of-domain training data sets [20]. To avoid these problems, [20] combined out-of-domain data with domain-specific data by assigning a higher weight Proceedings of IWSLT 2008, Hawaii - U.S.A. Training Sentence Pair Official journal of the European Community ↔ Journal officiel des Communaut´es europ´eennes NP NP S::NP PP NNP NNP IN Official journal of N NP DT JJ the European AP::A Journal officiel NNS community PP P NP des N AP::A Communaut´es europe´ennes (a) (b) † Official journal † Official journal of ∗ Official journal of the European Communities ∗ of ∗ of the European Communities ∗ the European Communities ∗ European ⋄ Com"
2008.iwslt-evaluation.3,takezawa-etal-2002-toward,0,0.0201078,"ed approach suffers from translation quality. In order to better restore the final punctuation mark, we combine the output of LM and translation-based approaches with a majority voting procedure. With two proposed hypotheses from the LM-based method and one from the translationbased method, we choose the hypothesis using majority voting. If no solution can be found using this approach, we choose the first hypothesis proposed by LM-based method. 3. Experimental Setup 3.1. Data The experiments were carried out using the datasets provided, extracted from the Basic Travel Expression Corpus (BTEC) [21]. This multilingual speech corpus contains tourism-related sentences similar to those that are usually found in phrasebooks for tourists going abroad. We participated in CHALLENGE, BTEC and PIVOT tasks and covered all language pairs and translation directions in this evaluation campaign. We translated both the single-best hypotheses and the correct recognition results. For our primary submissions, training was performed using the default training set, to which we added the data from devset1, devset2, devset3 and devset4 for the Chinese– English task,1 and devset6 was used for development purpo"
2008.iwslt-evaluation.3,N04-4038,0,0.018049,"English Name Entity Lists Version 1.0 were used for Chinese– English and English–Chinese tasks. We also added the HIT corpus as in-domain training data. For translation-based punctuation and case restoration, we used the English side of the training corpus to train the translation system. 3.2. Tools As a preprocessing step, the English sentences were tokenized using the maximum entropy-based tokenizer of the OpenNLP toolkit,2 and case information was removed. The Arabic data was tokenized and segmented using the ASVM toolkit which is based on SVMs, and has been trained on the Arabic Treebank. [22] The AVSM toolkit tokenized the Arabic data and segmented the Arabic words with the same segmentation style as in the Arabic Treebank. 1 More specifically, we use the Chinese side of the bilingual sentence pairs together with the first English reference from the 7 references to construct new sentence pairs. 2 http://opennlp.sourceforge.net/ Proceedings of IWSLT 2008, Hawaii - U.S.A. The Chinese data was segmented using ICTCLAS Olympic version.3 For the numbers identified, we split them into characters. A 5-gram language model4 with Kneser-Ney smoothing was trained with SRILM [23] on the Englis"
2008.iwslt-evaluation.3,P07-2045,0,0.00565686,"Missing"
2008.iwslt-evaluation.3,N07-1051,0,\N,Missing
2009.eamt-1.14,W05-0909,0,0.0114564,"German 348 Dutch 165 NONE 98 Spanish 93 Swedish Finnish 59 40 Portugese 38 Italian 36 Greek 11 Danish 7 Table 4: Test-2000 – repartition according to the original SL ations in both translation directions are carried out based on this testset. For French-to-English, the French part is used as test and the English part as reference. For English-to-French, the latter is used as test and the former as reference. 4.3 Evaluation The results of the translation output are evaluated using three standard automatic evaluation metrics: B LEU (Papineni et al., 2002), N IST (Doddington, 2002) and M ETEOR (Banerjee and Lavie, 2005). 4 Tools 4.1 5 Experiments Alignment and translation All translation experiments are carried out using standard state-of-the-art techniques. Sentence pairs are first word-aligned using GIZA++ implementation of IBM model 4 in both source-to-target As described in the previous sections, we built four different configurations of the same system for two translation directions, French-to-English and English-to-French, and carried out translation 99 experiments. We considered the relative merits to PB-SMT of using data of which the source part actually corresponds to the original SL, meaning that t"
2009.eamt-1.14,2005.mtsummit-papers.11,0,0.0630367,"Missing"
2009.eamt-1.14,N03-1017,0,0.00572017,"(i.e. original English) is used as test and the French part (i.e. French translated from English) as reference. • French translated from ¬English; • English translated from ¬French. Table 3 shows the repartition in terms of number of sentence pairs according to the original SL for the training corpus and the testset associated with 98 and target-to-source translation directions (Brown et al., 1993; Och and Ney, 2003) for each training set. After obtaining the intersection of these directional alignments, alignments from the union are also inserted; this insertion process is heuristicsdriven (Koehn et al., 2003). Once the word alignments are finalised, all word- and phrase-pairs which are consistent with the word alignment and which comprise at most 7 words are extracted. Phrase-pairs are extracted by standard PB-SMT techniques using the Moses system (Koehn et al., 2007). A 5-gram language model is trained with SRILM (Stolcke, 2002) on the English side of the training data for French-to-English translation experiments and on the French side of the training data for English-to-French translation experiments. Finally decoding is carried out with Moses. config-4 The original SL is French, meaning that t"
2009.eamt-1.14,J03-1002,0,0.0104671,"ted from English) as test and the English part (i.e. original English) as reference. English-to-French translation evaluations are based on the same portion of the data; this time, the English part (i.e. original English) is used as test and the French part (i.e. French translated from English) as reference. • French translated from ¬English; • English translated from ¬French. Table 3 shows the repartition in terms of number of sentence pairs according to the original SL for the training corpus and the testset associated with 98 and target-to-source translation directions (Brown et al., 1993; Och and Ney, 2003) for each training set. After obtaining the intersection of these directional alignments, alignments from the union are also inserted; this insertion process is heuristicsdriven (Koehn et al., 2003). Once the word alignments are finalised, all word- and phrase-pairs which are consistent with the word alignment and which comprise at most 7 words are extracted. Phrase-pairs are extracted by standard PB-SMT techniques using the Moses system (Koehn et al., 2007). A 5-gram language model is trained with SRILM (Stolcke, 2002) on the English side of the training data for French-to-English translation"
2009.eamt-1.14,P02-1040,0,0.0767414,"ngoing (cf. footnote 4). test sentences English 558 French 547 German 348 Dutch 165 NONE 98 Spanish 93 Swedish Finnish 59 40 Portugese 38 Italian 36 Greek 11 Danish 7 Table 4: Test-2000 – repartition according to the original SL ations in both translation directions are carried out based on this testset. For French-to-English, the French part is used as test and the English part as reference. For English-to-French, the latter is used as test and the former as reference. 4.3 Evaluation The results of the translation output are evaluated using three standard automatic evaluation metrics: B LEU (Papineni et al., 2002), N IST (Doddington, 2002) and M ETEOR (Banerjee and Lavie, 2005). 4 Tools 4.1 5 Experiments Alignment and translation All translation experiments are carried out using standard state-of-the-art techniques. Sentence pairs are first word-aligned using GIZA++ implementation of IBM model 4 in both source-to-target As described in the previous sections, we built four different configurations of the same system for two translation directions, French-to-English and English-to-French, and carried out translation 99 experiments. We considered the relative merits to PB-SMT of using data of which the so"
2009.eamt-1.14,steinberger-etal-2006-jrc,0,0.0605784,"Missing"
2009.eamt-1.14,P07-1108,0,0.0510011,"Missing"
2009.eamt-1.14,J90-2002,0,\N,Missing
2009.eamt-1.14,C08-1144,0,\N,Missing
2009.eamt-1.14,P07-2045,0,\N,Missing
2009.eamt-1.20,J93-2003,0,0.0190121,"Missing"
2009.eamt-1.20,W09-0416,1,0.819247,"ot the first approach that has used the Marker Hypothesis in MT. While most work has centred on building EBMT systems which relate source and target phrase pairs comprised of words (e.g. (Juola, 1994; Way and Gough, 2003; Gough and Way, 2 2004; Groves and Way, 2005a)), systems have also been successfully constructed where phrases consist of word–morpheme mappings (e.g. (Stroppa et al., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2005), Figure 2): While these results do not appear in the paper, they were included in the presentation accompanying that paper. Thanks to Yanjun Ma for this clarification. . Once 7 sets of closed-class words (determiners,"
2009.eamt-1.20,2005.mtsummit-papers.30,0,0.0384191,"Missing"
2009.eamt-1.20,2004.tmi-1.11,1,0.905113,"“Marker Hypothesis” (Green, 1979) (cf. section 3). Essentially, we use linguistic information in the form of closedclass word lists to filter the set of bilingual phrase pairs, by taking into account the alignment information and the type of the words involved in the alignments. The inspiration for the set of experiments carried out in this paper was that successful ExampleBased MT (EBMT) systems (Nagao, 1984; Carl and Way, 2003) have been built using the Marker Hypothesis to segment source–target aligned sentence pairs into linguistically motivated bilingual chunks (cf. (Way and Gough, 2003; Gough and Way, 2004)). These systems have proven to be particularly useful where good translation performance is required with much smaller translation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT ali"
2009.eamt-1.20,W05-0833,1,0.961125,"nt information and the type of the words involved in the alignments. The inspiration for the set of experiments carried out in this paper was that successful ExampleBased MT (EBMT) systems (Nagao, 1984; Carl and Way, 2003) have been built using the Marker Hypothesis to segment source–target aligned sentence pairs into linguistically motivated bilingual chunks (cf. (Way and Gough, 2003; Gough and Way, 2004)). These systems have proven to be particularly useful where good translation performance is required with much smaller translation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT alignments, a BLEU score of 36.43 was obtained, compared to a score of 37.53 with 1,732,715 phrase pairs built using Giza++ (Och and Ney, 2003). The remainder of this paper is organised as follows. Section 2 review"
2009.eamt-1.20,D07-1103,0,0.0498626,"Missing"
2009.eamt-1.20,koen-2004-pharaoh,0,0.0421452,"d Gough, 2003; Gough and Way, 2004)). These systems have proven to be particularly useful where good translation performance is required with much smaller translation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT alignments, a BLEU score of 36.43 was obtained, compared to a score of 37.53 with 1,732,715 phrase pairs built using Giza++ (Och and Ney, 2003). The remainder of this paper is organised as follows. Section 2 reviews other research work that has also focused on the filtering of the bilingual phrase pairs. Then, in section 3 we describe our approach. Section 4 describes the experiments conducted on four language pairs and the results achieved. The paper ends with our concluding remarks together with avenues for further research. 2 Related Work Previous approaches to filter t"
2009.eamt-1.20,W04-3250,0,0.0556677,"d Gough, 2003; Gough and Way, 2004)). These systems have proven to be particularly useful where good translation performance is required with much smaller translation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT alignments, a BLEU score of 36.43 was obtained, compared to a score of 37.53 with 1,732,715 phrase pairs built using Giza++ (Och and Ney, 2003). The remainder of this paper is organised as follows. Section 2 reviews other research work that has also focused on the filtering of the bilingual phrase pairs. Then, in section 3 we describe our approach. Section 4 describes the experiments conducted on four language pairs and the results achieved. The paper ends with our concluding remarks together with avenues for further research. 2 Related Work Previous approaches to filter t"
2009.eamt-1.20,2005.mtsummit-papers.11,0,0.0700668,"., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2005), Figure 2): While these results do not appear in the paper, they were included in the presentation accompanying that paper. Thanks to Yanjun Ma for this clarification. . Once 7 sets of closed-class words (determiners, quantifiers, conjunctions, prepositions, whadverbs, possessive and personal pronouns, cf. (6) below) have been built for English and French, the marker words in (2) can be tagged, as in (3): &lt;DET&gt; that is almost &lt;DET&gt; a personal record &lt;PREP&gt; for &lt;PRON&gt; me &lt;DET&gt; this autumn! −→&lt;DET&gt; c’ est pratiquement &lt;DET&gt; un record personnel &lt;PREP&gt; pour &lt;PRON&gt; moi , &lt;DET&gt; cet automne! (3) The"
2009.eamt-1.20,N03-1017,0,0.0926979,"e show that our simple yet novel approach can filter the phrase table by up to a third yet still provide competitive results compared to the baseline. Furthermore, it provides a nice balance between the unfiltered approach and pruning using stop words, where the deterioration in translation quality is unacceptably high. 1 (1) ): , tj+m BP(sI1 , tJ1 , A) = {(si+n j i 0 0 0 ∀(i , j ) ∈ A : i ≤ i ≤ i + n ⇔ j ≤ j 0 ≤ j + m}, Introduction The state-of-the-art statistical approach to machine translation (MT) is the phrase-based model. Phrase-based statistical MT (PB-SMT) systems (Zens et al., 2002; Koehn et al., 2003) are based on the log linear model combination of several feature functions (Och and Ney, 2002), one c 2009 European Association for Machine Translation. where A = {(i, j) : i ∈ [1, I] ∧ j ∈ [1, J]} is a set of pairs with the alignment information between the words in the source sentence sI1 and the words in the target sentence tJ1 . According to equation (1), all words within a bilingual phrase pair are consecutive and not aligned with words from outside the bilingual phrase pair. It is worth noting that bilingual phrase pairs may contain words that are not aligned at all, even at the beginni"
2009.eamt-1.20,P07-2045,0,0.0145693,". We experimented with this second criterion because, as a result of the bilingual phrase pairs extraction algorithm, unaligned words may appear at the beginning or the end of a phrase, and we wanted to test whether this introduces any noise in the translation table. Note that, after extracting the set of bilingual phrase pairs from a word-aligned sentence pair, two or more bilingual phrase pairs may only differ in that some of them contain unaligned words at the beginning or the end of the phrases. Experiments All the experiments were performed using the Moses open-source decoder for PB-SMT (Koehn et al., 2007) and the SRILM language modelling toolkit (Stolcke, 2002). Training was carried out as follows: 1. Word alignments were obtained using Giza++ (Och and Ney, 2003) and symmetrized in the usual way (Koehn et al., 2003). 2. Bilingual phrase pairs were extracted from the word-aligned sentence pairs. 3. Extracted phrase pairs were filtered following the approach presented in this paper, and then scored. 4. Weights were optimised using minimum error rate training (MERT) in the usual manner (Och, 2003). With the two filtering criteria explained in Section 3, we tested different lists of words: closed"
2009.eamt-1.20,2007.mtsummit-papers.40,1,0.818689,"79), which states that the syntactic structure of a language is marked at the surface level by a closed set of marker (closed) words. As stated in the introduction, this paper is not the first approach that has used the Marker Hypothesis in MT. While most work has centred on building EBMT systems which relate source and target phrase pairs comprised of words (e.g. (Juola, 1994; Way and Gough, 2003; Gough and Way, 2 2004; Groves and Way, 2005a)), systems have also been successfully constructed where phrases consist of word–morpheme mappings (e.g. (Stroppa et al., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2005), Figure 2): While these results do not ap"
2009.eamt-1.20,D07-1036,0,0.0415314,"Missing"
2009.eamt-1.20,2007.tmi-papers.14,1,0.88022,"Missing"
2009.eamt-1.20,P03-1021,0,0.0230491,"eriments All the experiments were performed using the Moses open-source decoder for PB-SMT (Koehn et al., 2007) and the SRILM language modelling toolkit (Stolcke, 2002). Training was carried out as follows: 1. Word alignments were obtained using Giza++ (Och and Ney, 2003) and symmetrized in the usual way (Koehn et al., 2003). 2. Bilingual phrase pairs were extracted from the word-aligned sentence pairs. 3. Extracted phrase pairs were filtered following the approach presented in this paper, and then scored. 4. Weights were optimised using minimum error rate training (MERT) in the usual manner (Och, 2003). With the two filtering criteria explained in Section 3, we tested different lists of words: closed words: A list of closed words in each language is provided to the filtering algorithm. These lists contain determiners, prepositions, pronouns, coordinate and subordinate conjunctions, relative and possessive pronouns, and punctuation marks. They consist of 193 Spanish words, 174 French words and 185 English words. Examples include those in (6): 3 147 http://www.statmt.org/wmt09/ In all cases the baseline system, i.e. when no filtering of the bilingual phrase pairs is done, performs better than"
2009.eamt-1.20,P02-1038,0,0.0580039,"provide competitive results compared to the baseline. Furthermore, it provides a nice balance between the unfiltered approach and pruning using stop words, where the deterioration in translation quality is unacceptably high. 1 (1) ): , tj+m BP(sI1 , tJ1 , A) = {(si+n j i 0 0 0 ∀(i , j ) ∈ A : i ≤ i ≤ i + n ⇔ j ≤ j 0 ≤ j + m}, Introduction The state-of-the-art statistical approach to machine translation (MT) is the phrase-based model. Phrase-based statistical MT (PB-SMT) systems (Zens et al., 2002; Koehn et al., 2003) are based on the log linear model combination of several feature functions (Och and Ney, 2002), one c 2009 European Association for Machine Translation. where A = {(i, j) : i ∈ [1, I] ∧ j ∈ [1, J]} is a set of pairs with the alignment information between the words in the source sentence sI1 and the words in the target sentence tJ1 . According to equation (1), all words within a bilingual phrase pair are consecutive and not aligned with words from outside the bilingual phrase pair. It is worth noting that bilingual phrase pairs may contain words that are not aligned at all, even at the beginning or the end of the phrase. In order to make the extraction of bilingual phrase pairs computat"
2009.eamt-1.20,J03-1002,0,0.0131914,"anslation tables than are traditionally used in PB-SMT. For example, Groves and Way (2005a) showed that for a range of systems built with different amounts of data, on average the translation table of a PB-SMT system was about five times the size of the equivalent EBMT system. In a related paper, on a training set of 203K English–French aligned sentence pairs, Groves and Way (2005b) showed that seeding a PB-SMT system built using Pharaoh (Koehn, 2004a) with 403,317 EBMT alignments, a BLEU score of 36.43 was obtained, compared to a score of 37.53 with 1,732,715 phrase pairs built using Giza++ (Och and Ney, 2003). The remainder of this paper is organised as follows. Section 2 reviews other research work that has also focused on the filtering of the bilingual phrase pairs. Then, in section 3 we describe our approach. Section 4 describes the experiments conducted on four language pairs and the results achieved. The paper ends with our concluding remarks together with avenues for further research. 2 Related Work Previous approaches to filter the phrase pairs used in PB-SMT can be divided into two classes: • those methods that filter the phrase table according to the text to be translated; • those more ge"
2009.eamt-1.20,P02-1040,0,0.0828818,"Missing"
2009.eamt-1.20,2006.amta-papers.25,0,0.0587861,"Missing"
2009.eamt-1.20,2006.iwslt-evaluation.4,1,0.867377,"ker (closed) words. As stated in the introduction, this paper is not the first approach that has used the Marker Hypothesis in MT. While most work has centred on building EBMT systems which relate source and target phrase pairs comprised of words (e.g. (Juola, 1994; Way and Gough, 2003; Gough and Way, 2 2004; Groves and Way, 2005a)), systems have also been successfully constructed where phrases consist of word–morpheme mappings (e.g. (Stroppa et al., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2005), Figure 2): While these results do not appear in the paper, they were included in the presentation accompanying that paper. Thanks to Yanjun Ma for this cla"
2009.eamt-1.20,2006.amta-papers.26,1,0.849286,"ng is known as the Marker Hypothesis (Green, 1979), which states that the syntactic structure of a language is marked at the surface level by a closed set of marker (closed) words. As stated in the introduction, this paper is not the first approach that has used the Marker Hypothesis in MT. While most work has centred on building EBMT systems which relate source and target phrase pairs comprised of words (e.g. (Juola, 1994; Way and Gough, 2003; Gough and Way, 2 2004; Groves and Way, 2005a)), systems have also been successfully constructed where phrases consist of word–morpheme mappings (e.g. (Stroppa et al., 2006) for English–Basque, and (Labaka et al., 2007) for Spanish–Basque). Marker-based chunking still plays a significant role in the M AT R E X (Stroppa and Way, 2006; Hassan et al., 2007; Tinsley et al., 2008; Du et al., 2009) system, where the sets of marker words needed for bilingual chunking are extracted automatically rather than by assembling these by hand as its predecessors did (e.g. (Way and Gough, 2003; Gough and Way, 2004)). To give the reader some idea of how marker words are used in practice in such systems, we revisit an example from (Groves and Way, 2005a), namely (2) (from (Koehn, 2"
2009.eamt-1.20,W08-0326,1,0.84189,"Missing"
2009.eamt-1.20,2002.tmi-tutorials.2,0,0.403263,"cu.ie Felipe S´anchez-Mart´ınez Dept. Llenguatges i Sistemes Inform`atics Universitat d’Alacant E-03071 Alacant, Spain fsanchez@dlsi.ua.es Abstract of which is the phrase translation probability estimated after extracting bilingual phrase pairs from the parallel corpus.1 Bilingual phrase pairs are automatically extracted after computing the word alignments (Brown et al., 1993; Och and Ney, 2003). The set BP(sJ1 , tI1 , A) of bilingual phrase pairs extracted from the word-aligned sentence pair sI1 = (s1 , . . . , si , . . . , sI ) and tJ1 = (t1 , . . . , tj , . . . , tJ ) is defined as in (1) (Zens et al., 2002): State-of-the-art statistical machine translation systems make use of a large translation table obtained after scoring a set of bilingual phrase pairs automatically extracted from a parallel corpus. The number of bilingual phrase pairs extracted from a pair of aligned sentences grows exponentially as the length of the sentences increases; therefore, the number of entries in the phrase table used to carry out the translation may become unmanageable, especially when online, ‘on demand’ translation is required in real time. We describe the use of closed-class words to filter the set of bilingual"
2009.eamt-1.20,2007.iwslt-1.4,0,\N,Missing
2009.eamt-1.20,W10-1720,1,\N,Missing
2009.eamt-1.32,P07-1005,0,0.0290351,"(Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007) of integrating state-of-the-art WSD methods into SMT to improve the overall translation quality have met with more success. Language models arguably play the most significant role in today’s PB-SMT systems. It is obvious that a straightforward addition of a source language model will make no contribution as this will be cancelled out by the denominator in the noisy-channel model of SMT. However, for some time now the feeling was that some incorporation of source language information into SMT systems had to help. (Stroppa et al., 2007) added source-side contextual f"
2009.eamt-1.32,J07-2003,0,0.0231926,"ch. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT baseline model in terms of automatic evaluation scores, but more improvements come to light in a manual investigation. One final paper in this strand of research is that of (He et al., 2008), who despite not mentioning the obvious link between the two pieces of work, show that the source language features used by (Stroppa et al., 2007) are also of benefit when used with the Hiero (Chiang, 2007) decoder. As regards supertagged models of translation, (Hassan et al., 2006, 2007b, 2008; Hassan, 2009) have demonstrated clearly that adding supertags (essentially, part-of-speech tags of words plus local subcategorisation requirements) in the target language model and on the target side of the translation model improve state-of-the-art PBSMT systems. The system of (Hassan et al., 2007a) was ranked first according to human evaluators on the IWSLT 2007 Arabic–English task, despite the improvements in system design not being shown to their best advantage by the automatic evaluation metrics. Mo"
2009.eamt-1.32,C04-1041,0,0.00701127,"onference of the EAMT, pages 234–241, Barcelona, May 2009 234 that the word projects upwards. Like (Hassan et al., 2006, 2007, 2008; Hassan, 2009), in this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robust and efficient supertagging approach: like standard taggers, supertaggers employ probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore and Joshi, 1999). There are currently two supertagging approaches available: LTAG-based (Bangalore and Joshi, 1999) and CCG-based (Clark and Curran, 2004). Both the LTAG (Chen et al., 2006) and the CCG supertag sets (Hockenmaier, 2003) were acquired from the WSJ section of the PennII Treebank using hand-built extraction rules. Here we test both the LTAG and CCG supertaggers. We extract the supertagged components of context words (±1/±2) along with the source phrase (Koehn et al., 2003) in a standard PBSMT system. We use a memory-based classification approach to obtain the probability for the given additional contexts with the source phrase. In this paper we discuss these and other empirical issues. The remainder of the paper is organized as fol"
2009.eamt-1.32,W06-1628,0,0.0207219,"orating context-dependent phrasal translation probabilities learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier which implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian—English and Chinese— English IWSLT tasks. Unlike other recent proposals to exploit the accuracy and the flexibility of discriminative learning (e.g. Cowan et al., 2006; Liang et al., 2006), the strength of the approach of (Stroppa et al., 2007) is that no redefinition of one’s training procedures is required. Like the work of (Max et al., 2008), the present work is directly motivated by and an extension of the approach of (Stroppa et al., 2007). 235 The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual fea"
2009.eamt-1.32,W06-1607,0,0.0145345,"sted reader to (Stroppa et al., 2007) for more details of how MemoryBased Learning (MBL) is used for classification of source examples for use in the log-linear MT framework. Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating P( eˆk |fˆk , CI( fˆk )) using relative frequencies (say) is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context informed features, since the context is also Experimental Set-Up 6.1 Features Used The distribution of target phrases given a source phrase and its contextual information is normalised to estimate P( eˆk |fˆk ,CI( fˆk )). Therefore our expected feature is derived as in (9): hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (9) In addition to the above feature, we derived two more features hˆmod and hˆbest from the posterior probability P( eˆk |fˆk ) and P( eˆk |fˆk ,CI( fˆk )). The feature hˆmod is defined as in (10): hˆmod = log [α P( eˆk |fˆk ,CI( fˆk )) + (1- α) P( eˆk |fˆk )] (1"
2009.eamt-1.32,P01-1027,0,0.376294,"Missing"
2009.eamt-1.32,J99-2004,0,0.0114076,"rtaggers exist only for English. In this paper, we begin to explore whether such a system could indeed generate improvements across all PB-SMT system components. Our novel approach combines the methods of (Stroppa et al., 2007) and (Hassan et al., 2006, 2007, 2008; Hassan, 2009) in one model. We extend a standard PB-SMT system with syntactic descriptions on the source side. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicondriven approaches to linguistic syntax, namely Lexicalized Tree-Adjoining Grammar (LTAG: Joshi and Schabes, 1992; Bangalore and Joshi, 1999) and Combinatory Categorial Grammar (CCG: Steedman, 2000). In such approaches, the grammar consists of a very rich lexicon and a small set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorisation information and the hierarchy of phrase categories Proceedings of the 13th Annual Conference of the EAMT, pages 234–241, Barcelona, May 2009 234 that the word projects upwards. Like (Hassan et al., 2006, 2007, 2008; Hassan, 2009), i"
2009.eamt-1.32,J96-1002,0,0.0206265,"we discuss these and other empirical issues. The remainder of the paper is organized as follows. In section 2 we discuss related work. Section 3 gives a brief overview of PBSMT. In section 4 we describe the context-informed features contained in our baseline log-linear phrase-based SMT system. In section 5 we describe the memory-based classification approach. Section 6 describes the features used in the experiments, and the pre-processing required. Section 7 includes the results obtained, together with some analysis. Section 8 concludes, and provides avenues for further work. 2 Related Work (Berger et al., 1996) first suggested contextsensitive modelling of word translations in order to integrate local contextual information into their IBM translation models using a Maximum Entropy (MaxEnt) model, but the work is not supported by any significant evaluation results. García Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate"
2009.eamt-1.32,W07-0719,0,0.0333399,"nt lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. (Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007) of integrating state-of-the-art WSD methods into SMT to improve the overall translation quality have met with more success. Language models"
2009.eamt-1.32,P05-1048,0,0.0449464,"result in improved translation quality, as noted by a number of researchers. (Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007) of integrating state-of-the-art WSD methods into SMT to improve the overall translation quality have met with more success. Language models arguably play the most significant role in today’s PB-SMT systems. It is obvious that a straightforward addition of a source language model will make no contribution as this will be cancelled out by the denominator in the noisy-channel model of SMT. However, for some time now the feeling was that some incorporation of source language information into SMT"
2009.eamt-1.32,W08-0302,0,0.263931,"imates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian—English and Chinese— English IWSLT tasks. Unlike other recent proposals to exploit the accuracy and the flexibility of discriminative learning (e.g. Cowan et al., 2006; Liang et al., 2006), the strength of the approach of (Stroppa et al., 2007) is that no redefinition of one’s training procedures is required. Like the work of (Max et al., 2008), the present work is directly motivated by and an extension of the approach of (Stroppa et al., 2007). 235 The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT baseline model in terms of automatic evaluation scores, but more improvements come to light in a manual investigation. One final paper in this strand of research is that of (He et al."
2009.eamt-1.32,D07-1007,0,0.0665287,"umber of researchers. (Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007) of integrating state-of-the-art WSD methods into SMT to improve the overall translation quality have met with more success. Language models arguably play the most significant role in today’s PB-SMT systems. It is obvious that a straightforward addition of a source language model will make no contribution as this will be cancelled out by the denominator in the noisy-channel model of SMT. However, for some time now the feeling was that some incorporation of source language information into SMT systems had to help. (Stroppa et al., 2007) added sourc"
2009.eamt-1.32,N03-1017,0,0.0540342,"probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore and Joshi, 1999). There are currently two supertagging approaches available: LTAG-based (Bangalore and Joshi, 1999) and CCG-based (Clark and Curran, 2004). Both the LTAG (Chen et al., 2006) and the CCG supertag sets (Hockenmaier, 2003) were acquired from the WSJ section of the PennII Treebank using hand-built extraction rules. Here we test both the LTAG and CCG supertaggers. We extract the supertagged components of context words (±1/±2) along with the source phrase (Koehn et al., 2003) in a standard PBSMT system. We use a memory-based classification approach to obtain the probability for the given additional contexts with the source phrase. In this paper we discuss these and other empirical issues. The remainder of the paper is organized as follows. In section 2 we discuss related work. Section 3 gives a brief overview of PBSMT. In section 4 we describe the context-informed features contained in our baseline log-linear phrase-based SMT system. In section 5 we describe the memory-based classification approach. Section 6 describes the features used in the experiments, and the"
2009.eamt-1.32,2007.iwslt-1.10,1,0.852685,"ty when adding supertags as context-informed features. 1 Introduction In log-linear phrase-based SMT, the probability P(eI1|fJ1) of a target phrase eI1 given a source phrase fJ1 is modelled as a log-linear combination of features which normally consist of a finite set © 2009 European Association for Machine Translation. of translational features, and a language model (Och and Ney, 2002). The usual translational features involved in those models express dependencies between the source and target phrases, but not dependencies between the phrases in the source language themselves. Stroppa et al. (2007) were the first to show that incorporating source language context using neighbouring words and part-of-speech tags had the potential to improve translation quality. In a separate strand of research, Hassan et al. (2006, 2007, 2008) showed that incorporating lexical syntactic descriptions in the form of supertags in the target language model and on the target side of the translation model could improve significantly on state-of-the-art approaches to MT. Despite the significance of this work, it is currently not possible to develop a fully supertagged PB-SMT system given that supertaggers exist"
2009.eamt-1.32,P06-1096,0,0.0190838,"ndent phrasal translation probabilities learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier which implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian—English and Chinese— English IWSLT tasks. Unlike other recent proposals to exploit the accuracy and the flexibility of discriminative learning (e.g. Cowan et al., 2006; Liang et al., 2006), the strength of the approach of (Stroppa et al., 2007) is that no redefinition of one’s training procedures is required. Like the work of (Max et al., 2008), the present work is directly motivated by and an extension of the approach of (Stroppa et al., 2007). 235 The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual features as (Stroppa et"
2009.eamt-1.32,P07-1037,1,0.163249,"Missing"
2009.eamt-1.32,2008.eamt-1.17,0,0.179946,"contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier which implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian—English and Chinese— English IWSLT tasks. Unlike other recent proposals to exploit the accuracy and the flexibility of discriminative learning (e.g. Cowan et al., 2006; Liang et al., 2006), the strength of the approach of (Stroppa et al., 2007) is that no redefinition of one’s training procedures is required. Like the work of (Max et al., 2008), the present work is directly motivated by and an extension of the approach of (Stroppa et al., 2007). 235 The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT baseline mo"
2009.eamt-1.32,P03-1021,0,0.00387557,"ise, We performed three different experiments by integrating these three features hˆmbl , hˆmod and hˆbest directly into the log-linear model. In the first experiment E1, the baseline feature log P( eˆk |fˆk ) is directly replaced by hˆmod . In the second experiment (E2), we integrated the hˆmbl feature together with the baseline features, keeping all the features unaffected. In the third experiment (E3), both the features hˆmbl and hˆbest are integrated into the model in the same manner. As for the standard phrase-based approach, their weights are optimized using minimum-error-rate training (Och, 2003) for each of the experiments we carried out. 6.2 Pre-Processing As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrase-table represented as a list of aligned phrases accompanied with several features. Since these features do not express the context in which those phrases occur, no context information is kept in the phrase-table, and there is no way to recover this information from the phrase-table. In order to take into account the contextinformed features for use with such decoders, the devset and test set that need to"
2009.eamt-1.32,C08-1041,0,0.0220602,"th, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT baseline model in terms of automatic evaluation scores, but more improvements come to light in a manual investigation. One final paper in this strand of research is that of (He et al., 2008), who despite not mentioning the obvious link between the two pieces of work, show that the source language features used by (Stroppa et al., 2007) are also of benefit when used with the Hiero (Chiang, 2007) decoder. As regards supertagged models of translation, (Hassan et al., 2006, 2007b, 2008; Hassan, 2009) have demonstrated clearly that adding supertags (essentially, part-of-speech tags of words plus local subcategorisation requirements) in the target language model and on the target side of the translation model improve state-of-the-art PBSMT systems. The system of (Hassan et al., 2007a)"
2009.eamt-1.32,P02-1038,0,0.133102,"f words plus local subcategorisation requirements) in the target language model and on the target side of the translation model improve state-of-the-art PBSMT systems. The system of (Hassan et al., 2007a) was ranked first according to human evaluators on the IWSLT 2007 Arabic–English task, despite the improvements in system design not being shown to their best advantage by the automatic evaluation metrics. More recently, (Hassan, 2009) has demonstrated that improvements can even be gained over the leading NIST07 Arabic–English system of (Ittycheriah and Roukos, 2007). combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): 3 4 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . eI of a source sentence f1 J = f1 . . . fJ is chosen to maximize (1): arg max P (e1I |f1J )  arg max P ( f1 J |e1I ).P (e1I ) I ,e1I (1) where P ( f1 |e ) and P (e ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modelled as a (log-linear) I 1 I 1 m 1  LM log P"
2009.eamt-1.32,P02-1040,0,0.107341,"Missing"
2009.eamt-1.32,N07-1008,0,0.00591983,"hat adding supertags (essentially, part-of-speech tags of words plus local subcategorisation requirements) in the target language model and on the target side of the translation model improve state-of-the-art PBSMT systems. The system of (Hassan et al., 2007a) was ranked first according to human evaluators on the IWSLT 2007 Arabic–English task, despite the improvements in system design not being shown to their best advantage by the automatic evaluation metrics. More recently, (Hassan, 2009) has demonstrated that improvements can even be gained over the leading NIST07 Arabic–English system of (Ittycheriah and Roukos, 2007). combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): 3 4 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . eI of a source sentence f1 J = f1 . . . fJ is chosen to maximize (1): arg max P (e1I |f1J )  arg max P ( f1 J |e1I ).P (e1I ) I ,e1I (1) where P ( f1 |e ) and P (e ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modell"
2009.eamt-1.32,C92-2066,0,0.328103,"MT system given that supertaggers exist only for English. In this paper, we begin to explore whether such a system could indeed generate improvements across all PB-SMT system components. Our novel approach combines the methods of (Stroppa et al., 2007) and (Hassan et al., 2006, 2007, 2008; Hassan, 2009) in one model. We extend a standard PB-SMT system with syntactic descriptions on the source side. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicondriven approaches to linguistic syntax, namely Lexicalized Tree-Adjoining Grammar (LTAG: Joshi and Schabes, 1992; Bangalore and Joshi, 1999) and Combinatory Categorial Grammar (CCG: Steedman, 2000). In such approaches, the grammar consists of a very rich lexicon and a small set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorisation information and the hierarchy of phrase categories Proceedings of the 13th Annual Conference of the EAMT, pages 234–241, Barcelona, May 2009 234 that the word projects upwards. Like (Hassan et al., 2006,"
2009.eamt-1.32,2007.tmi-papers.28,1,0.413276,"Missing"
2009.eamt-1.32,koen-2004-pharaoh,0,0.00921006,"n the first experiment E1, the baseline feature log P( eˆk |fˆk ) is directly replaced by hˆmod . In the second experiment (E2), we integrated the hˆmbl feature together with the baseline features, keeping all the features unaffected. In the third experiment (E3), both the features hˆmbl and hˆbest are integrated into the model in the same manner. As for the standard phrase-based approach, their weights are optimized using minimum-error-rate training (Och, 2003) for each of the experiments we carried out. 6.2 Pre-Processing As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrase-table represented as a list of aligned phrases accompanied with several features. Since these features do not express the context in which those phrases occur, no context information is kept in the phrase-table, and there is no way to recover this information from the phrase-table. In order to take into account the contextinformed features for use with such decoders, the devset and test set that need to be translated is pre-processed. Each word appearing in the test set and devset is assigned a unique id. First we prepare the phrase table using t"
2009.eamt-1.32,H05-1097,0,0.208056,"1) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. (Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Gim"
2009.eamt-1.32,P07-2045,0,0.0527183,"Missing"
2009.eamt-1.32,N04-1033,0,0.0258205,"ifier, we modify the standard phrase-extraction method of (Koehn et al., 2003) to extract the context of the source phrases at the same time as the phrases themselves. Importantly, therefore, the context extraction comes at no extra cost. We refer the interested reader to (Stroppa et al., 2007) for more details of how MemoryBased Learning (MBL) is used for classification of source examples for use in the log-linear MT framework. Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating P( eˆk |fˆk , CI( fˆk )) using relative frequencies (say) is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context informed features, since the context is also Experimental Set-Up 6.1 Features Used The distribution of target phrases given a source phrase and its contextual information is normalised to estimate P( eˆk |fˆk ,CI( fˆk )). Therefore our expected feature is derived as in (9): hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (9) I"
2009.eamt-1.32,2006.iwslt-evaluation.4,1,\N,Missing
2009.eamt-1.34,J93-2003,0,0.0123787,"orithm. The interpolation factor λ can be optimised on development set. When a zeroorder transition model (a uniform transition distribution) is used, we constrain the emission probability by a threshold t, which is set as the minimal reliability score for each link. Again, t can be optimised according to the development set. The decoding is performed separately in two directions (Chinese-to-English and English-toChinese), and we then obtain the refined alignments as the final word alignment. 4.3 4.3.1 Baselines Word Alignment We used the G IZA ++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003) for word alignment, and the heuristics described in (Koehn et al., 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation 4.4 Evaluation We evaluate the intrinsic quality of the predicted alignment A with Precision, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessari"
2009.eamt-1.34,N06-4004,0,0.0300402,"Missing"
2009.eamt-1.34,J93-1003,0,0.0238346,"Y p(aj |cJ1 , eI1 , a1j−1 , A∆ )1−λ · ¯ j∈∆ J Y p(aj |aj−1 , A∆ )λ j=1 We can use factor λ to weight the emission model and transition model probabilities so that the system can be optimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactically enhanced model can be classified into two groups: statistics-based features and syntactic features which are similar to those in (Ma et al., 2008) 3.1 Statistics-based Features The statistics-based features we used include IBM model 1 score, Log-likelihood ratio (Dunning, 1993) and POS translation probability. We choose these features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , accordi"
2009.eamt-1.34,W08-0409,1,0.62805,"es into such models. On the other hand, discriminative models are more flexible to incorporate arbitrary features. In this paper, we introduce a simple yet flexible framework for word alignment. To take the advantage of the strength of generative models, we use these models to obtain a set of anchor alignments. We then incorporate syntactic features induced by the anchor alignments into a discriminative word alignment model. The syntactic features we used are syntactic dependencies. This decision is motivated by the fact that if words tend to be dependent on each other, so does the alignment (Ma et al., 2008). If we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencies relating unaligned words to aligned anchor words to expand the alignment. Figure 1 gives an illustrative example. Note that the link (c2 , e4 ) can be easily identified, but the link involving the fourth Chinese word (a function word denoting ‘time’) (c4 , e4 ) is hard. In such cases, we can make use of the dependency relationship (‘tclause’) between c2 and c4 to help the alignment process. Figure 1: Dependencies for word alignment c 2009 European Association for Machine Translation."
2009.eamt-1.34,J00-2004,0,0.0406643,"ities so that the system can be optimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactically enhanced model can be classified into two groups: statistics-based features and syntactic features which are similar to those in (Ma et al., 2008) 3.1 Statistics-based Features The statistics-based features we used include IBM model 1 score, Log-likelihood ratio (Dunning, 1993) and POS translation probability. We choose these features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj , Rc , cj ′ >. While we are not aiming to justify the fe"
2009.eamt-1.34,J07-3002,0,0.142988,"† Patrik Lambert‡ Andy Way†‡ † National Centre for Language Technology ‡ Centre for Next Generation for Localisation School of Computing, Dublin City University Dublin 9, Ireland {yma,plambert,away@computing.dcu.ie} Abstract However, these models need a certain amount of annotated word alignment data, which is often subject to criticism since the annotation of word alignment is a highly subjective task. Moreover, parameters optimised on manually annotated data are not necessarily optimal for MT tasks. Recent research attempts to combine the merits of both generative and discriminative models (Fraser and Marcu, 2007), or to tune a discriminative model according to MT metrics (Lambert et al., 2007). We introduce a syntactically enhanced word alignment model that is more flexible than state-of-the-art generative word alignment models and can be tuned according to different end tasks. First of all, this model takes the advantages of both unsupervised and supervised word alignment approaches by obtaining anchor alignments from unsupervised generative models and seeding the anchor alignments into a supervised discriminative model. Second, this model offers the flexibility of tuning the alignment according to d"
2009.eamt-1.34,H05-1011,0,0.01725,"ptimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactically enhanced model can be classified into two groups: statistics-based features and syntactic features which are similar to those in (Ma et al., 2008) 3.1 Statistics-based Features The statistics-based features we used include IBM model 1 score, Log-likelihood ratio (Dunning, 1993) and POS translation probability. We choose these features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj , Rc , cj ′ >. While we are not aiming to justify the feasibility of the dependency corr"
2009.eamt-1.34,P02-1050,0,0.0154312,"e features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj , Rc , cj ′ >. While we are not aiming to justify the feasibility of the dependency correspondence assumption by proving to what extent Re = Rc under the condition described above, we do believe that cj and cj ′ are likely to be dependent on each other. Given the anchor alignment A∆ , a candidate link (j, i) and the dependency trees, we can design four classes of feature functions. 3.2.1 Agreement features The agreement features can be further classified into dependency agreement features and dependency label agreement features. Given a candidate link (j, i) and the"
2009.eamt-1.34,H05-1012,0,0.0155429,"ment set. In our experiments we set α = 0.9. 2.2.2 Syntactically Enhanced Word Alignment The syntactically enhanced model is used to model the alignment of the words left unaligned after anchoring. We directly model the linkage between source and target words using a discriminative word alignment framework where various features can be incorporated. Given a source word cj and the target sentence eI1 , we search for the alignment aj such that: aˆj = argmax{pλM (aj |cJ1 , eI1 , a1j−1 , A∆ )} 1 aj A We use a model (1) that directly models the linkage between source and target words similarly to (Ittycheriah and Roukos, 2005). The Chinese-toEnglish word alignment AC→E = {i|aj = i} is modelled as shown in (1). We decompose this model into an emission model and a transition model ( 4). The emission model can be further decomposed into an anchor alignment model (2) and a syntactically enhanced model (3) by distinguishing the anchor alignment from the nonanchor alignment. p(A|cJ1 , eI1 ) = J Y p(aj |cJ1 , eI1 , a1j−1 ) 1 · pǫ (A∆ |cJ1 , eI1 ) · (2) Z Y p(aj |cJ1 , eI1 , a1j−1 , A∆ ) ·(3) ¯ j∈∆ J Y p(aj |aj−1 , A∆ ) (4) j=1 2.2 aj λm hm (cJ1 , eI1 , aj1 , A∆ , Tc , Te )} m=1 In this decision rule, we assume that a set"
2009.eamt-1.34,N03-1017,0,0.0058174,"nsition model (a uniform transition distribution) is used, we constrain the emission probability by a threshold t, which is set as the minimal reliability score for each link. Again, t can be optimised according to the development set. The decoding is performed separately in two directions (Chinese-to-English and English-toChinese), and we then obtain the refined alignments as the final word alignment. 4.3 4.3.1 Baselines Word Alignment We used the G IZA ++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003) for word alignment, and the heuristics described in (Koehn et al., 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation 4.4 Evaluation We evaluate the intrinsic quality of the predicted alignment A with Precision, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et"
2009.eamt-1.34,P07-2045,0,0.0140549,"Missing"
2009.eamt-1.34,N07-2022,1,0.799024,"Next Generation for Localisation School of Computing, Dublin City University Dublin 9, Ireland {yma,plambert,away@computing.dcu.ie} Abstract However, these models need a certain amount of annotated word alignment data, which is often subject to criticism since the annotation of word alignment is a highly subjective task. Moreover, parameters optimised on manually annotated data are not necessarily optimal for MT tasks. Recent research attempts to combine the merits of both generative and discriminative models (Fraser and Marcu, 2007), or to tune a discriminative model according to MT metrics (Lambert et al., 2007). We introduce a syntactically enhanced word alignment model that is more flexible than state-of-the-art generative word alignment models and can be tuned according to different end tasks. First of all, this model takes the advantages of both unsupervised and supervised word alignment approaches by obtaining anchor alignments from unsupervised generative models and seeding the anchor alignments into a supervised discriminative model. Second, this model offers the flexibility of tuning the alignment according to different optimisation criteria. Our experiments show that using our word alignment"
2009.eamt-1.34,N06-1014,0,0.0193518,"heuristics described in (Koehn et al., 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation 4.4 Evaluation We evaluate the intrinsic quality of the predicted alignment A with Precision, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we use a Chinese– English MT task to extrinsically evaluate the quality of our word alignment. The translation output is measured using B LEU (Papineni et al., 2002). 5 Experiments 5.1 Word Alignment Results We performed word alignment bidirectionally using our approach to obtain the refined alignments (Koehn et al., 2003) and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 3. We can see that both the syntactically enhanced model based on HMM intersection anchors and on IBM"
2009.eamt-1.34,P05-1057,0,0.0163677,"he system can be optimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactically enhanced model can be classified into two groups: statistics-based features and syntactic features which are similar to those in (Ma et al., 2008) 3.1 Statistics-based Features The statistics-based features we used include IBM model 1 score, Log-likelihood ratio (Dunning, 1993) and POS translation probability. We choose these features because they are empirically proven to be effective in word alignment tasks (Melamed, 2000; Liu et al., 2005; Moore, 2005). 3.2 Syntactic Features The dependency relation Re (resp. Rc ) between two English (resp. Chinese) words ei and ei′ (resp. cj and cj ′ ) in the dependency tree of the English sentence eI1 (resp. Chinese sentence cJ1 ) can be represented as a triple &lt;ei , Re , ei′ > (resp. &lt;cj , Rc , ej ′ >). Given cJ1 , eI1 and their syntactic dependency trees TcJ , TeI , if ei is aligned to cj and 1 1 ei′ aligned to cj ′ , according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple &lt;cj , Rc , cj ′ >. While we are not aiming to justify the feasibility of the d"
2009.eamt-1.34,J03-1002,0,0.0332275,"mation of pˆ(aj |aj−1 ) is calculated following the homogeneous HMM model (Vogel et al., 1996). Under this model, we assume that the 251 (5) probability depends only on the jump width (i−i′ ), in order to make the alignment parameters independent of absolute word positions. Using a set of non-negative parameters {c(i − i′ )}, the transition probability can be written in the form: p(aj |aj−1 , A∆ ) = PI c(i − i′ ) ′′ i′′ =1 c(i − i′ ) We use the refined model which extends the HMM network with I empty words e2I I+1 and adds parameter p0 to account for the transition probability to empty words (Och and Ney, 2003). If a zero-order dependence is assumed in a transition model, the emission models is the only information to guide the word alignment. 2.4 Model Interpolation We interpolate the general alignment model (1) as follows: p(A|cJ1 , eI1 ) = 1 · pǫ (A∆ |cJ1 , eI1 )1−λ · Z Y p(aj |cJ1 , eI1 , a1j−1 , A∆ )1−λ · ¯ j∈∆ J Y p(aj |aj−1 , A∆ )λ j=1 We can use factor λ to weight the emission model and transition model probabilities so that the system can be optimised according to different objectives. 3 Feature Functions for Syntactically Enhanced Word Alignment The various features used in our syntactical"
2009.eamt-1.34,P03-1021,0,0.0215296,"Missing"
2009.eamt-1.34,P02-1040,0,0.0789029,"ion, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we use a Chinese– English MT task to extrinsically evaluate the quality of our word alignment. The translation output is measured using B LEU (Papineni et al., 2002). 5 Experiments 5.1 Word Alignment Results We performed word alignment bidirectionally using our approach to obtain the refined alignments (Koehn et al., 2003) and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 3. We can see that both the syntactically enhanced model based on HMM intersection anchors and on IBM model 4 anchors achieved higher Fscores than the pure generative word alignment models. It is also can be seen that zero-order syntactic models are better in precision and firstorder models are superior in recall."
2009.eamt-1.34,W96-0213,0,0.0601125,"Missing"
2009.eamt-1.34,2006.iwslt-papers.7,0,0.0135253,"l., 2003) to derive the intersection and refined alignment. 4.3.2 Machine Translation 4.4 Evaluation We evaluate the intrinsic quality of the predicted alignment A with Precision, Recall and the balanced F-score with α = 0.5 (cf. (Fraser and Marcu, 2007)). Recall = F-score(A, S, α) = More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. 1 + α P recision(A,S) 1−α Recall(A,S) Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we use a Chinese– English MT task to extrinsically evaluate the quality of our word alignment. The translation output is measured using B LEU (Papineni et al., 2002). 5 Experiments 5.1 Word Alignment Results We performed word alignment bidirectionally using our approach to obtain the refined alignments (Koehn et al., 2003) and compared our results with two strong baselines based on generative word alignment models. The results are shown in Table 3. We can see that both the syntactically enhanced model based on HMM intersection anchors and on IBM model 4 anchors achieved higher Fsc"
2009.eamt-1.34,C96-2141,0,0.066659,"l pǫ (A∆ ) aims to find a set of high-precision links. Various approaches can be used for this purpose. Given the anchor alignment, the first-order transition probability model ( 4) can be defined as follows: ( 1.0 if ∈ ∆, p(aj |aj−1 , A∆ ) = pˆ(aj |aj−1 ) otherwise. Such a definition implies that an anchor alignment is always believed to be a correct alignment, maximum likelihood estimates obtained on a goldstandard word alignment corpus are used when the current word fj is not involved in an anchor alignment. The estimation of pˆ(aj |aj−1 ) is calculated following the homogeneous HMM model (Vogel et al., 1996). Under this model, we assume that the 251 (5) probability depends only on the jump width (i−i′ ), in order to make the alignment parameters independent of absolute word positions. Using a set of non-negative parameters {c(i − i′ )}, the transition probability can be written in the form: p(aj |aj−1 , A∆ ) = PI c(i − i′ ) ′′ i′′ =1 c(i − i′ ) We use the refined model which extends the HMM network with I empty words e2I I+1 and adds parameter p0 to account for the transition probability to empty words (Och and Ney, 2003). If a zero-order dependence is assumed in a transition model, the emission"
2009.eamt-1.34,2007.iwslt-1.1,0,\N,Missing
2009.eamt-1.7,P07-1038,0,0.230254,"ection 5. The paper ends with our concluding remarks together with avenues for further research. 2008) match n-best head-modifier dependencies extracted from n-best constituency parses. They also consider the probabilities given by the constituency parser. Dependency information is also used in metrics that incorporate different information sources. (Gim´enez and M`arquez, 2008) experimented using different levels of linguistic features and dependency relation-based metrics are among their best metrics at both system and sentence levels. Machine learning metrics such as (Ye et al., 2007) and (Albrecht and Hwa, 2007) also use some head-modifier dependency matches or dependency chains as features. 2 Evaluation Metrics in MT Automatic evaluation metrics enable researchers to validate and optimise translation methods quickly. Simple n-gram-based metrics such as B LEU (Papineni et al., 2002) are fundamental to the development and tuning of MT systems. However, these ngram-based metrics suffer from several shortcomings, such as low correlation with human judgement on the sentence level, exhibiting a bias towards statistical systems (Callison-Burch et al., 2006), and inconsistency in related evaluation scenario"
2009.eamt-1.7,W05-0909,0,0.665079,"anslation (MT). Prior to their introduction, most results were anecdotal, or researchers had to conduct expensive human evaluations in order to validate their work. However, seven years after their introduction, there is widespread recognition in MT that these string-based metrics are not discriminative enough to reflect the translation quality of today’s systems, many of which have gone beyond n-grams (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which are not wholly stringbased. Perhaps the best-known alternative metric is M ETEOR (Banerjee and Lavie, 2005), which c 2009 European Association for Machine Translation. Proceedings of the 13th Annual Conference of the EAMT, pages 44–51, Barcelona, May 2009 44 ent tasks and domains. The remainder of the paper is organised as follows. In section 2, we outline approaches to automatic MT evaluation which are relevant to our work. In particular, in section 3 we describe the LFG labelled dependency approach of (Owczarzak et al., 2007a; Owczarzak et al., 2007b). In section 4, we demonstrate how labelled dependencies can be matched using SVMs, and describe the range of experiments carried out in section 5."
2009.eamt-1.7,W05-0904,0,0.11113,"eses and the reference but ignores any differences in quality among these hypotheses. Both rankingand regression-based methods have been reported to be successful in various MT evaluation tasks. In our experiments we combine them with the dependency-based method of (Owczarzak et al., 2007a; Owczarzak et al., 2007b) and directly compare them in a ranking task. Dependency-based Metrics The shortcomings of n-gram metrics have led a number of researchers to exploit more grammatical information in the hypothesis and reference sentences. Syntactic features were first introduced in MT evaluation in (Liu and Gildea, 2005), who developed several metrics using constituency or dependency structure. (Owczarzak et al., 2007a; Owczarzak et al., 2007b) improved on the dependency matching of (Liu and Gildea, 2005) by using n-best labelled dependency triples produced by an LFG parser, so that parser noise is reduced and partial matchings can be found. (Kahn et al., 3 LFG Labelled Dependencies Our work extends the method of (Owczarzak et al., 2007a; Owczarzak et al., 2007b) who use labelled dependencies in Lexical-Function Grammar (LFG). In LFG, a sentence is represented in both a hierarchical tree structure (C-structur"
2009.eamt-1.7,P04-1041,1,0.890397,"Missing"
2009.eamt-1.7,W07-0714,1,0.886979,"Missing"
2009.eamt-1.7,E06-1032,0,0.470881,"and domains. 1 Introduction There is no doubt that the onset of automatic evaluation metrics such as B LEU (Papineni et al., 2002) has led directly to improvements in quality in machine translation (MT). Prior to their introduction, most results were anecdotal, or researchers had to conduct expensive human evaluations in order to validate their work. However, seven years after their introduction, there is widespread recognition in MT that these string-based metrics are not discriminative enough to reflect the translation quality of today’s systems, many of which have gone beyond n-grams (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which are not wholly stringbased. Perhaps the best-known alternative metric is M ETEOR (Banerjee and Lavie, 2005), which c 2009 European Association for Machine Translation. Proceedings of the 13th Annual Conference of the EAMT, pages 44–51, Barcelona, May 2009 44 ent tasks and domains. The remainder of the paper is organised as follows. In section 2, we outline approaches to automatic MT evaluation which are relevant to our work. In particular, in section 3 we describe the LFG labelled dependency approach of (Owczarzak et"
2009.eamt-1.7,P02-1040,0,0.0804882,"an other existing metrics with human judgements. Other research in this area has presented machine learning methods which learn directly from human judgements. In this paper, we present a novel combination of dependency- and machine learning-based approaches to automatic MT evaluation, and demonstrate greater correlations with human judgement than the existing state-of-the-art methods. In addition, we examine the extent to which our novel method can be generalised across different tasks and domains. 1 Introduction There is no doubt that the onset of automatic evaluation metrics such as B LEU (Papineni et al., 2002) has led directly to improvements in quality in machine translation (MT). Prior to their introduction, most results were anecdotal, or researchers had to conduct expensive human evaluations in order to validate their work. However, seven years after their introduction, there is widespread recognition in MT that these string-based metrics are not discriminative enough to reflect the translation quality of today’s systems, many of which have gone beyond n-grams (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which are not wholly stringbas"
2009.eamt-1.7,W08-0309,0,0.063288,"call (R) or both (PR). We test with SVMs of linear (L), polynomial (P) and RBF (R) kernels (KERNEL) using the SVMLight software. Each configuration is denoted with {NORM}-{DEP}-{KERNEL} in both ranking and regression experimental results. We use the following three metrics as baselines: B LEU (BLEU-4), add-one B LEU (BLEU4s) and the labelled LFG-based metric (LFG-F) as described in (Owczarzak et al., 2007a; Owczarzak et al., 2007b). Note that the result of the LFG-F metric would have among the highest correlations with human judgement in the WMT08 shared evaluation task. 5.3 (12) However, in (Callison-Burch et al., 2008), it is argued that averaging ρ is meaningless, and so pair-wise consistent percentage is used instead to measure correlations in the WMT08 shared evaluation task. The pair-wise consistent percentage is equal to the number of correct pair-wise comparisons made by a metric divided by the total number of pair-wise comparisons to make. We report both consistent percentage and sentence-level Spearman’s correlation in our experiments. The Spearman’s correlation is first computed on each ranking, and then averaged. We explore the choice of different {KERNEL}s (Table 1) with PR-HV data representation"
2009.eamt-1.7,W07-0736,0,0.452368,"Owczarzak et al., 2007b) have extended this line of research with the use of a term-based encoding of LFG labelled dependency graphs into unordered sets of dependency triples, and calculating precision, recall, and f-measure on the sets corresponding to the translation and reference sentences. With the addition of partial matching and n-best parses, (Owczarzak et al., 2007a; Owczarzak et al., 2007b) considerably outperform Liu and Gildea’s (2005) highest correlations with human judgement. Another line of research has led to machine learning methods which learn directly from human judgements (Ye et al., 2007). In this paper, we combine the syntax (dependency)-based and the machine learning-based approaches, and show greater correlations with human judgement than (Owczarzak et al., 2007a; Owczarzak et al., 2007b). We use both Ranking and Regression Support Vector Machines (SVMs) (Burges, 1998) in a range of experiments on different language pairs and data sets. We also examine the extent to which our novel method can be generalised across differRecently novel MT evaluation metrics have been presented which go beyond pure string matching, and which correlate better than other existing metrics with h"
2009.eamt-1.7,D08-1064,0,0.0518389,"so use some head-modifier dependency matches or dependency chains as features. 2 Evaluation Metrics in MT Automatic evaluation metrics enable researchers to validate and optimise translation methods quickly. Simple n-gram-based metrics such as B LEU (Papineni et al., 2002) are fundamental to the development and tuning of MT systems. However, these ngram-based metrics suffer from several shortcomings, such as low correlation with human judgement on the sentence level, exhibiting a bias towards statistical systems (Callison-Burch et al., 2006), and inconsistency in related evaluation scenarios (Chiang et al., 2008). Many approaches have been taken to overcome the insufficiencies of B LEU. Wordbased metrics like M ETEOR (Banerjee and Lavie, 2005) try to improve on the matching scheme; paraphrase-based methods such as ParaEval incorporate paraphrases extracted from an external data source (Zhou et al., 2006); syntactic methods try to use syntax information in hypothesis and reference (cf. section 2.1); and machine learning methods learn directly from human judgements (cf. section 2.2). 2.1 2.2 Machine Learning-based Metrics Three kinds of machine learning-based approaches have been used in MT evaluation:"
2009.eamt-1.7,N06-1057,0,0.0728512,"velopment and tuning of MT systems. However, these ngram-based metrics suffer from several shortcomings, such as low correlation with human judgement on the sentence level, exhibiting a bias towards statistical systems (Callison-Burch et al., 2006), and inconsistency in related evaluation scenarios (Chiang et al., 2008). Many approaches have been taken to overcome the insufficiencies of B LEU. Wordbased metrics like M ETEOR (Banerjee and Lavie, 2005) try to improve on the matching scheme; paraphrase-based methods such as ParaEval incorporate paraphrases extracted from an external data source (Zhou et al., 2006); syntactic methods try to use syntax information in hypothesis and reference (cf. section 2.1); and machine learning methods learn directly from human judgements (cf. section 2.2). 2.1 2.2 Machine Learning-based Metrics Three kinds of machine learning-based approaches have been used in MT evaluation: (i) Classification-based approaches (Corston-Oliver et al., 2001) train a classifier to discriminate between the reference and the hypothesis. The higher the likelihood of a hypothesis being a reference, the better its quality is assumed to be; (ii) Regression-based methods (Albrecht and Hwa, 200"
2009.eamt-1.7,P05-1033,0,0.113763,"Missing"
2009.eamt-1.7,W08-0332,0,\N,Missing
2009.eamt-1.7,P01-1020,0,\N,Missing
2009.iwslt-evaluation.4,2007.iwslt-1.10,1,0.898288,"decoding stage. The second technique is used to select the optimal training data that can be used to build MT systems. In this year’s participation, we use three different prototype SMT systems, and the output from each system are combined using standard system combination method. Our system is the top system for Chinese–English CHALLENGE task in terms of B LEU score. 1. Introduction In this paper, we describe some new extensions to the hybrid data-driven MT system developed at DCU, M AT R E X (Machine Translation using Examples), subsequent to our participation at IWSLT 2006 [1], IWSLT 2007 [2] and IWSLT 2008 [3]. In this year’s participation, optimising the system in a low-resource scenario is our main focus. The first technique deployed in our system is word lattice decoding, where the input of the system is not a string of words, but rather a lattice encoding multiple segmentations for a single sentence. This method has been repeatedly demonstrated to be effective in improving the coverage of the MT systems [4, 5, 6, 7]. Another technique investigated is a novel data selection method, which differentiates high- and low-quality bilingual sentence pairs in the training data, and us"
2009.iwslt-evaluation.4,P09-3009,1,0.693718,"ng data. Given that state-of-theart word alignment models only allows 1-to-n mappings between source and target words, those sentences which include n-to-m mappings between source and target words (for example, paraphrases, non-literal translations, and multiword expressions) are considered to be noise. The noisy sentence pairs can potentially hinder a word aligner in achieving high quality alignments; moreover, the errors in word alignment will be propagated in later stages of MT training including phrase extraction. To remove the noisy sentence pairs, we use a method as shown in Algorithm 1 [10]. vˆ1K = arg max{P (v1K |f1I )} (1) v1K ,K Then in the decoding stage, we seek the translation of the most likely source segmentation, as in (2): eˆJ1 = arg max{P (eJ1 |ˆ v1K )} (2) eJ 1 ,J In such a scenario, some segmentations which are potentially optimal for translation may be lost. This motivates the need 1 We can also use language models to assign probabilities to each edge as in [4]. In this case, however, we have to rely on some segmented data to train the language model. 2 +Pron: pronoun, +Pers: personal, +A1sg: 1st person singular, +Pnon: no possessive +Dat: dative - 30 - eJ 1 ,J ≃ v"
2009.iwslt-evaluation.4,2008.iwslt-evaluation.3,1,0.843396,"e second technique is used to select the optimal training data that can be used to build MT systems. In this year’s participation, we use three different prototype SMT systems, and the output from each system are combined using standard system combination method. Our system is the top system for Chinese–English CHALLENGE task in terms of B LEU score. 1. Introduction In this paper, we describe some new extensions to the hybrid data-driven MT system developed at DCU, M AT R E X (Machine Translation using Examples), subsequent to our participation at IWSLT 2006 [1], IWSLT 2007 [2] and IWSLT 2008 [3]. In this year’s participation, optimising the system in a low-resource scenario is our main focus. The first technique deployed in our system is word lattice decoding, where the input of the system is not a string of words, but rather a lattice encoding multiple segmentations for a single sentence. This method has been repeatedly demonstrated to be effective in improving the coverage of the MT systems [4, 5, 6, 7]. Another technique investigated is a novel data selection method, which differentiates high- and low-quality bilingual sentence pairs in the training data, and use them separately i"
2009.iwslt-evaluation.4,2005.iwslt-1.18,0,0.0782723,"nsions to the hybrid data-driven MT system developed at DCU, M AT R E X (Machine Translation using Examples), subsequent to our participation at IWSLT 2006 [1], IWSLT 2007 [2] and IWSLT 2008 [3]. In this year’s participation, optimising the system in a low-resource scenario is our main focus. The first technique deployed in our system is word lattice decoding, where the input of the system is not a string of words, but rather a lattice encoding multiple segmentations for a single sentence. This method has been repeatedly demonstrated to be effective in improving the coverage of the MT systems [4, 5, 6, 7]. Another technique investigated is a novel data selection method, which differentiates high- and low-quality bilingual sentence pairs in the training data, and use them separately in training MT systems. We participate in the CHALLENGE tasks and the BTEC Chinese–English and Turkish–English tasks. For CHALLENGE tasks, both the single-best ASR hypotheses and the correct recognition results are translated. Three different prototype SMT systems are built for each translation task and a few novel techniques are then applied to different systems. - 29 - The final submission is a combination of the"
2009.iwslt-evaluation.4,P08-1115,0,0.0613429,"nsions to the hybrid data-driven MT system developed at DCU, M AT R E X (Machine Translation using Examples), subsequent to our participation at IWSLT 2006 [1], IWSLT 2007 [2] and IWSLT 2008 [3]. In this year’s participation, optimising the system in a low-resource scenario is our main focus. The first technique deployed in our system is word lattice decoding, where the input of the system is not a string of words, but rather a lattice encoding multiple segmentations for a single sentence. This method has been repeatedly demonstrated to be effective in improving the coverage of the MT systems [4, 5, 6, 7]. Another technique investigated is a novel data selection method, which differentiates high- and low-quality bilingual sentence pairs in the training data, and use them separately in training MT systems. We participate in the CHALLENGE tasks and the BTEC Chinese–English and Turkish–English tasks. For CHALLENGE tasks, both the single-best ASR hypotheses and the correct recognition results are translated. Three different prototype SMT systems are built for each translation task and a few novel techniques are then applied to different systems. - 29 - The final submission is a combination of the"
2009.iwslt-evaluation.4,E09-1063,1,0.851642,"nsions to the hybrid data-driven MT system developed at DCU, M AT R E X (Machine Translation using Examples), subsequent to our participation at IWSLT 2006 [1], IWSLT 2007 [2] and IWSLT 2008 [3]. In this year’s participation, optimising the system in a low-resource scenario is our main focus. The first technique deployed in our system is word lattice decoding, where the input of the system is not a string of words, but rather a lattice encoding multiple segmentations for a single sentence. This method has been repeatedly demonstrated to be effective in improving the coverage of the MT systems [4, 5, 6, 7]. Another technique investigated is a novel data selection method, which differentiates high- and low-quality bilingual sentence pairs in the training data, and use them separately in training MT systems. We participate in the CHALLENGE tasks and the BTEC Chinese–English and Turkish–English tasks. For CHALLENGE tasks, both the single-best ASR hypotheses and the correct recognition results are translated. Three different prototype SMT systems are built for each translation task and a few novel techniques are then applied to different systems. - 29 - The final submission is a combination of the"
2009.iwslt-evaluation.4,W09-0416,1,0.927538,"ental setup is presented and experimental results obtained for various language pairs are reported in Section 4. In Section 5, we conclude, and provide avenues for further research. 2. The M AT R E X System The M AT R E X system is a hybrid data-driven MT system which exploits aspects of different MT paradigms [1]. The system follows a modular design and facilitates the incorporation of different MT engines and novel techniques. In this year’s participation, besides additional MT engines, the system has also been extended with a system combination module which can combine different MT outputs [8]. In the following subsections, we describe the main techniques used in the participation of IWSLT 2009. 2.1. Word Lattice To mitigate the negative effects of the inaccurate word segmentation, word lattice, which encodes a few alternative segmentations for a given sentence, can be used as input [5, 6] of the MT systems. This technique can be applied to languages where the word boundaries are not orthographically marked such as Chinese, or languages with rich morphology such as Turkish. In the decoding stage, the various segmentation alternatives can be encoded into a compact representation of"
2009.iwslt-evaluation.4,P02-1040,0,0.0844907,"? ¡ð  ÷ /  ? what number should i dial for information ?  ( à ? what days of the week does it take place ? Ù / ¥  the keys go here . Ç For multiple system combination, we implement an Minimum Bayes-Risk-Confusion Network (MBR-CN) framework as used in [8]. Due to the varying word order in the MT hypotheses, it is essential to decide the backbone which determines the general word order of the confusion network. Instead of using a single system output as the skeleton, we employ a MBR decoder to select the best single system output from the merged N-best list by minimising the B LEU [11] loss, as in (6): eˆi = arg min N X {1 − BLEU (ej , ei )} (6) i∈{1,··· ,N } j=1 where ei and ej are hypotheses in the N-best list, and N indicates the number of hypotheses in the merged N-best list. BLEU (ej , ei ) calculates sentence-level B LEU score of ei with ej as the reference translation. The confusion network is built using the output of MBR decoder as the backbone which determines the word order of the combination. The other hypotheses are aligned against the backbone based on the TER metric. NULL words are allowed in the alignment. Either votes or some form of confidence measures are"
2009.iwslt-evaluation.4,P06-1055,0,0.0100779,"ng. If no solution can be found using this approach, we choose the first hypothesis proposed by the LM-based method. 3. Experimental Setup In our experiments, we used data provided within the evaluation campaign; no additional data resources are used. The detailed data setting will be explained when we report the experimental results for each task. In addition to the original manual segmentation, we used LDC segmenter to segment Chinese sentences. In order to train Syntax-Based SMT systems, we need to parse the sentences in target language, i.e. Chinese or English in our case. Berkeley parser [13] with default setting is used to parse both Chinese and English sentences. The G IZA ++ implementation [14] of IBM Model 4 [15] is used as the baseline for word alignment, and the “GrowDiag-Final” (GDF) and intersection (INT) heuristics3 described in [16] to derive the refined alignment from bidirectional alignments. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the above-mentioned met"
2009.iwslt-evaluation.4,J03-1002,0,0.00235521,"d method. 3. Experimental Setup In our experiments, we used data provided within the evaluation campaign; no additional data resources are used. The detailed data setting will be explained when we report the experimental results for each task. In addition to the original manual segmentation, we used LDC segmenter to segment Chinese sentences. In order to train Syntax-Based SMT systems, we need to parse the sentences in target language, i.e. Chinese or English in our case. Berkeley parser [13] with default setting is used to parse both Chinese and English sentences. The G IZA ++ implementation [14] of IBM Model 4 [15] is used as the baseline for word alignment, and the “GrowDiag-Final” (GDF) and intersection (INT) heuristics3 described in [16] to derive the refined alignment from bidirectional alignments. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the above-mentioned method, we perform phrase-extraction using heuristics described in [16], MERT [17] optimising the B LEU metric"
2009.iwslt-evaluation.4,N03-1017,0,0.00337109,"detailed data setting will be explained when we report the experimental results for each task. In addition to the original manual segmentation, we used LDC segmenter to segment Chinese sentences. In order to train Syntax-Based SMT systems, we need to parse the sentences in target language, i.e. Chinese or English in our case. Berkeley parser [13] with default setting is used to parse both Chinese and English sentences. The G IZA ++ implementation [14] of IBM Model 4 [15] is used as the baseline for word alignment, and the “GrowDiag-Final” (GDF) and intersection (INT) heuristics3 described in [16] to derive the refined alignment from bidirectional alignments. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the above-mentioned method, we perform phrase-extraction using heuristics described in [16], MERT [17] optimising the B LEU metric, a 5-gram language model with Kneser-Ney smoothing [18] trained with SRILM4 [19] on the English side 3 In our experiments, we only tried these two"
2009.iwslt-evaluation.4,P03-1021,0,0.0104404,"The G IZA ++ implementation [14] of IBM Model 4 [15] is used as the baseline for word alignment, and the “GrowDiag-Final” (GDF) and intersection (INT) heuristics3 described in [16] to derive the refined alignment from bidirectional alignments. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the above-mentioned method, we perform phrase-extraction using heuristics described in [16], MERT [17] optimising the B LEU metric, a 5-gram language model with Kneser-Ney smoothing [18] trained with SRILM4 [19] on the English side 3 In our experiments, we only tried these two heuristics due to limited amount of time; however, other heuristics are also worth exploiting. 4 Specifically, we used SRILM release 1.4.6. - 32 - of the training data, and M OSES for decoding. Three open-source SMT systems, i.e. PB-SMT system Moses [20], Hierarchical Phrase-Based system Joshua [21] and Syntax-Based SMT system SAMT [22] are used in our experiments. 4. Experimental Results In the following subsections, we"
2009.iwslt-evaluation.4,P07-2045,0,0.0205731,"Missing"
2009.iwslt-evaluation.4,W09-0424,0,0.0327074,"Missing"
2009.iwslt-evaluation.4,W06-3119,0,0.0520639,"Missing"
2009.mtsummit-posters.12,P06-1002,0,0.222937,"of the data used. In Section 5, the results are discussed. Finally, some conclusions are provided together with avenues for further research. 2 Related Work In this section, we review some alignment characteristics that have been observed to have some impact in phrase extraction and MT output, for the SMT approaches we consider in this paper. We will thus consider these characteristics (and more) to investigate what kind of alignment helps depending on the system and type or amount of training data. In several papers the impact of higher precision or higher recall alignments has been studied. Ayan and Dorr (2006) and Chen and Federico (2006) observed that higher precision alignments favoured a phrase-based SMT system. In the former case, it was observed with an English Chinese training corpus of 1.1 million running words (for the English side), and with an English Arabic corpus of 3.3 million words of News and treebank data. In the latter case, the BTEC corpus was used (Chinese, Japanese, Arabic and Italian to English, with 180k English words). Fraser and Marcu (2007) compared the performance of translation systems trained on several alignments of varying quality. Their results on large corpora do not"
2009.mtsummit-posters.12,J93-2003,0,0.0156789,"a log-linear combination of feature functions calculated at the sentence pair level. In a first pass, the training corpus was aligned selecting for each sentence pair (s, t) the alignment ˆ which maximises a combination of varhypothesis a ious models, as expressed in (1): (1) (1) (1) ˆ(1) = arg max λa1 ha1 + λa2 ha2 + λlb hlb a a (1) (1) (1) + λ(1) um hum + λcn hcn + λcl hcl + λhp hhp (1) where h stands for the feature functions h(s, t) used, and the λs are their corresponding weights. ha1 and ha2 are word association models based on source-target and target-source IBM model 1 probabilities (Brown et al., 1993). hlb is proportional to the number of links in a. hum is an unlinked word model proportional to the IBM model 1 NULL link probability. hcn and hcl are distortion models, counting respectively the number and amplitude (the difference between target word positions) of crossing links. Finally, hhp is a “hole penalty” model, proportional to the number of embedded positions between two target words linked to the same source words (or vice-versa). We performed a second alignment pass in which the association score model with IBM1 probabilities and the unlinked model were substituted by two improved"
2009.mtsummit-posters.12,J07-3002,0,0.259443,"system and type or amount of training data. In several papers the impact of higher precision or higher recall alignments has been studied. Ayan and Dorr (2006) and Chen and Federico (2006) observed that higher precision alignments favoured a phrase-based SMT system. In the former case, it was observed with an English Chinese training corpus of 1.1 million running words (for the English side), and with an English Arabic corpus of 3.3 million words of News and treebank data. In the latter case, the BTEC corpus was used (Chinese, Japanese, Arabic and Italian to English, with 180k English words). Fraser and Marcu (2007) compared the performance of translation systems trained on several alignments of varying quality. Their results on large corpora do not confirm the hypothesis that higher precision alignments help phrase-based SMT systems more than higher recall alignments. For example, among their 3 systems trained on a 67 million word French English corpus, the highest precision alignment has the best BLEU score when alignment quality is low, and the highest recall alignment is the best when alignment quality is high. Their results suggest that when there is not enough data to produce good quality alignment"
2009.mtsummit-posters.12,N03-1017,0,0.0123354,"evaluation was performed with the BLEU score (Papineni et al., 2002). Translations were computed either by Moses (Koehn et al., 2007) with all default parameters, or by a baseline n-gram-based system with constrained reordered search (Crego and Mari˜no, 2007). 5 Results We present intrinsic and extrinsic evaluation results as well as some statistics for 9 alignment sets. 3 sets are baseline sets, and correspond to combinations of the Giza++ (Och and Ney, 2003) source-target and target-source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003). The other sets were aligned with the optimum weights of the discriminative aligner (Section 3.1) resulting from optimisations according to the F-score, to the phrase-based system BLEU score and to the ngram-based system BLEU score (referred to as F, PB and NB, respectively). Because the optimisation algorithm can get stuck in a poor local maximum, the optimisation with each criterion was performed with three different random seeds. To have an idea of the error introduced by the optimisation process, we kept the weights of the two optimisations which reached the highest values in the developm"
2009.mtsummit-posters.12,P07-2045,0,0.0220958,"o an alignment quality metric (F-score, see Section 4.3). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which are helpful in translation, they should not depend on the aligner used. However, they could depend on the MT system, the language pair, or the corpus size or type. The second contribution of this paper is to study more systematically how the considered characteristics depend on these parameters. We report results for two different SMT systems: a phrase-based system (Moses (Koehn et al., 2007)) and an n-gram-based system (Crego and Mari˜no, 2007). We performed this comparison on two different tasks: translation from Chinese to English, trained with IWSLT data (BTEC corpus, a small corpus in the travelling domain), and translation from Spanish to English, trained on a 2.7 million word corpus of the European Parliament proceedings. First we discuss related work. In Section 3, we describe the alignment optimisation procedures according to F-score and BLEU, and give more details on the alignment system used. Then in Section 4, we provide a summary of the experiments performed on each t"
2009.mtsummit-posters.12,N07-2022,1,0.865518,"ms (a phrase-based and an n-gram-based system) on Chinese to English IWSLT data, and Spanish to English European Parliament data. We give alignment hints to improve BLEU score, depending on the SMT system used and the type of corpus. 1 Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) extract their translation models from word alignment trained in a previous stage. Many papers have shown that alignment quality is poorly correlated with MT quality (for example Vilar et al. (2006)). Then, we can tune the alignment directly according to MT metrics (Lambert et al., 2007). In this paper we rather try to find out which alignment characteristics help or worsen translation. In the related papers (see next section) some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. The contributions of this paper are twofold. Firstly, the problem is considered from the inverse point of view: we start from an initial alignment and tune it directly according to a translation quality metric (BLEU score (Papineni et al., 2002)) and according to an alignment quality metric (F-score, see"
2009.mtsummit-posters.12,J06-4004,1,0.922385,"Missing"
2009.mtsummit-posters.12,J00-2004,0,0.0193689,"1 NULL link probability. hcn and hcl are distortion models, counting respectively the number and amplitude (the difference between target word positions) of crossing links. Finally, hhp is a “hole penalty” model, proportional to the number of embedded positions between two target words linked to the same source words (or vice-versa). We performed a second alignment pass in which the association score model with IBM1 probabilities and the unlinked model were substituted by two improved models benefiting from the first-pass links: an Association score model har with Relative link probabilities (Melamed, 2000), and source and target fertility models (hf s and hf t ) giving the probability for a given word to have one, two, three or four or more links. Second pass models are listed in (2). (2) (2) (2) (2) ˆ(2) = arg max λ(2) a ar har + λlb hlb + λf s hf s + a (2) λf t hf t + λ(2) cn hcn + λcl hcl + λhp hhp (2) To find the best hypothesis, we implemented a beam-search algorithm based on dynamic programming. In a given sentence pair, the best 3 links for each source and for each target word are considered in search. The parameters of the first and second alignment passes were optimised together, to gi"
2009.mtsummit-posters.12,H05-1011,0,0.110043,"ingual phrases involved with embedded words are still extracted. However, those bilingual phrases involving the long-distance oneto-many link itself may be large and thus not easy to reuse. The same problem may happen with long crossing links. Thus we expect that alignments optimised according to BLEU will have shorter links, or shorter crossing links, or fewer embedded words than manual alignments. 3 Alignment Optimisation Procedure Our aim was to obtain alignments optimised according to both an intrinsic and an extrinsic criterion. To achieve this, we used a discriminative alignment system (Moore, 2005) because of its flexibility. For both criteria, the optimisation consisted of maximising a function of the alignment system parameters: F-score (intrinsic criterion) and BLEU score (extrinsic criterion). First we describe the alignment system used, then the optimisation procedure. 3.1 Discriminative Alignment System This aligner implements a log-linear combination of feature functions calculated at the sentence pair level. In a first pass, the training corpus was aligned selecting for each sentence pair (s, t) the alignment ˆ which maximises a combination of varhypothesis a ious models, as exp"
2009.mtsummit-posters.12,J03-1002,0,0.0178041,", F = , |A| |GS | P +R where A, GS and G are respectively the computed link set, the reference sure link set, and the total reference link set. Extrinsic evaluation was performed with the BLEU score (Papineni et al., 2002). Translations were computed either by Moses (Koehn et al., 2007) with all default parameters, or by a baseline n-gram-based system with constrained reordered search (Crego and Mari˜no, 2007). 5 Results We present intrinsic and extrinsic evaluation results as well as some statistics for 9 alignment sets. 3 sets are baseline sets, and correspond to combinations of the Giza++ (Och and Ney, 2003) source-target and target-source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003). The other sets were aligned with the optimum weights of the discriminative aligner (Section 3.1) resulting from optimisations according to the F-score, to the phrase-based system BLEU score and to the ngram-based system BLEU score (referred to as F, PB and NB, respectively). Because the optimisation algorithm can get stuck in a poor local maximum, the optimisation with each criterion was performed with three different random seeds. To have"
2009.mtsummit-posters.12,P03-1021,0,0.00416473,"he F-score (see Section 4). This constitutes the first iteration of the optimisation algorithm, realised with initial parameters. 1 The weights in each pass can be normalised such that one (1) (2) weight is set to 1. This is why λa1 and λar were not free parameters. Then, alignment system parameters were simply adjusted by the optimisation algorithm so as to maximise the F-score. In the case of Function (4), the training corpus was aligned with initial parameters and these alignments were used to build either an n-gram-based or a phrase-based SMT system. The model weights were tuned via MERT (Och, 2003), with the Moses MERT utility (which was adapted to the n-grambased system). Then a translation of a development corpus was obtained and evaluated using BLEU (see Section 4). Thus, at each iteration, the considered parallel corpus was aligned (with the two successive passes), an SMT system was built from the resulting alignments (including bilingual phrase extraction, model(s) estimation and MERT) and the development set was translated to obtain the BLEU score. At the end of this process we obtained the alignment parameters which maximise the BLEU score. Note that we used two developments sets"
2009.mtsummit-posters.12,P02-1040,0,0.0763899,"we can tune the alignment directly according to MT metrics (Lambert et al., 2007). In this paper we rather try to find out which alignment characteristics help or worsen translation. In the related papers (see next section) some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. The contributions of this paper are twofold. Firstly, the problem is considered from the inverse point of view: we start from an initial alignment and tune it directly according to a translation quality metric (BLEU score (Papineni et al., 2002)) and according to an alignment quality metric (F-score, see Section 4.3). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which are helpful in translation, they should not depend on the aligner used. However, they could depend on the MT system, the language pair, or the corpus size or type. The second contribution of this paper is to study more systematically how the considered characteristics depend on these parameters. We report results for two different SMT systems: a phrase-base"
2009.mtsummit-posters.12,takezawa-etal-2002-toward,0,0.0614575,"form: ˆ k+1 = λ ˆ k − ak g ˆk ) ˆ k (λ λ (5) ˆ k ) is the estimate of the gradient g(λ) ≡ ˆk (λ where g ˆ k based on the previous eval∂E/∂λ at the iterate λ uations of the objective function. ak denotes a positive number that usually decreases as k increases. We performed about 80 evaluations of the objective function. Note that in general, SPSA converges to a local maximum. 4 4.1 Experiments Chinese English BTEC Task The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2007 evaluation campaign, extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). Training data consisted of the default training set, to which we added the sets devset1, devset2 and devset3. The resulting corpus contains 41.5k sentence pairs having respectively 9.4 and 8.7 words on average for English and Chinese. English and Chinese vocabulary sizes are respectively 9.8k and 11.4k. Manual annotation of word alignment was carried out on devset3, of which 251 sentence pairs were used as the development set and 251 for testing. For MT evaluation, we used IWSLT 2006 test set (500 sentences, 6.1k words, 7 references) as development set for the internal SMT MERT procedure, an"
2009.mtsummit-posters.12,2006.iwslt-papers.7,0,0.727119,"teristics that are helpful in translation. We report results for two different SMT systems (a phrase-based and an n-gram-based system) on Chinese to English IWSLT data, and Spanish to English European Parliament data. We give alignment hints to improve BLEU score, depending on the SMT system used and the type of corpus. 1 Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) extract their translation models from word alignment trained in a previous stage. Many papers have shown that alignment quality is poorly correlated with MT quality (for example Vilar et al. (2006)). Then, we can tune the alignment directly according to MT metrics (Lambert et al., 2007). In this paper we rather try to find out which alignment characteristics help or worsen translation. In the related papers (see next section) some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. The contributions of this paper are twofold. Firstly, the problem is considered from the inverse point of view: we start from an initial alignment and tune it directly according to a translation quality metric (BLEU"
2009.mtsummit-posters.18,W05-0909,0,0.053511,"Missing"
2009.mtsummit-posters.18,P05-1033,0,0.0372639,"y obtaining phrase structure parses for both the source and target sides using monolingual parsers and then aligning the subtrees using a statistical tree aligner. Hearne et al. (2008) go a step further by building on the work of (Tinsley et al., 2007) and adding phrase pairs induced from dependency parse trees. Note that all these approaches work on string-based translation models, i.e. syntactic knowledge is merely used to extract linguistically motivated phrase pairs. The phrase tables still contain translations of strings, just like in Moses. There also exist a number of other approaches (Chiang, 2005; Quirk et al., 2005; Galley et al., 2006; Hassan et al., 2008) which have developed different models where the incorporation of syntax has shown itself to be beneficial. However such models are not restricted to the string-based translation modeling and are thus somewhat out of the scope of this paper. In this paper, we extend the experiments of Hearne et al. (2008) by adding another syntax-aware phrase extraction methodology, namely percolated dependencies. We also scale up the volume of the training data, and compare and contrast the resultant phrase tables (cf. section 4). 2.2 Head Percola"
2009.mtsummit-posters.18,P97-1003,0,0.131917,"4). 2.2 Head Percolation It is possible to obtain a dependency parse for a sentence from its constituency parse (Gaifman, 1965) by exploiting lexicalized heads, i.e. head words of each phrase or constituent. In the absence of this information, a head percolation table is used to select the head node in each constituent structure. For example, the head of a phrase (NP (DET The) (N box)) is the node (N box). This implies an entry in the head percolation table specifying the node N as a head child of the node NP. Head percolation tables were first introduced in Magerman (1995) and implemented in Collins (1997). Head percolation tables are so called because, to extract headdependent information from a constituency parsed treebank, the lexical items are percolated like features from the heads to their parent projections. A head percolation table consists of hand-coded rules identifying the head-child of each node. We impleSYSTEM S TR (S) C ON (C) D EP (D) P ERC (P) S+C S+D S+P C+D C+P D+P S+C+D S+C+P S+D+P C+D+P S+C+D+P BLEU 31.29 30.64 30.75 29.19 32.87 32.69 32.34 31.24 30.99 31.40 32.70 32.49 32.62 31.46 32.82 (a) JOC DATA NIS MET WER 6.31 63.91 61.09 6.34 63.82 60.72 6.31 64.12 61.34 6.09 62.12 6"
2009.mtsummit-posters.18,P06-1121,0,0.0217943,"s for both the source and target sides using monolingual parsers and then aligning the subtrees using a statistical tree aligner. Hearne et al. (2008) go a step further by building on the work of (Tinsley et al., 2007) and adding phrase pairs induced from dependency parse trees. Note that all these approaches work on string-based translation models, i.e. syntactic knowledge is merely used to extract linguistically motivated phrase pairs. The phrase tables still contain translations of strings, just like in Moses. There also exist a number of other approaches (Chiang, 2005; Quirk et al., 2005; Galley et al., 2006; Hassan et al., 2008) which have developed different models where the incorporation of syntax has shown itself to be beneficial. However such models are not restricted to the string-based translation modeling and are thus somewhat out of the scope of this paper. In this paper, we extend the experiments of Hearne et al. (2008) by adding another syntax-aware phrase extraction methodology, namely percolated dependencies. We also scale up the volume of the training data, and compare and contrast the resultant phrase tables (cf. section 4). 2.2 Head Percolation It is possible to obtain a dependenc"
2009.mtsummit-posters.18,W05-0833,1,0.92655,"re-existing techniques, namely syntaxaware PB-SMT and generation of dependency structures from phrase-stucture parse trees. 2.1 Syntax-aware PB-SMT Incorporation of linguistic knowledge into the phrase extraction process has shown mixed results in recent years. For instance, Koehn et al. (2003), demonstrated that using syntax to constrain their phrase-based system actually harmed its quality. In contrast, all of the following approaches have shown that augmenting the baseline string-based translation model with syntax-aware word and phrase alignments causes translation performance to improve. Groves and Way (2005) extract EBMT phrase pairs by monolingually chunking both the source and target sides using closed-class marker words (Green, 1979) and then aligning the resulting chunks using mutual information techniques. Tinsley et al. (2007) extract phrase pairs by obtaining phrase structure parses for both the source and target sides using monolingual parsers and then aligning the subtrees using a statistical tree aligner. Hearne et al. (2008) go a step further by building on the work of (Tinsley et al., 2007) and adding phrase pairs induced from dependency parse trees. Note that all these approaches wor"
2009.mtsummit-posters.18,2008.jeptalnrecital-court.14,0,0.454445,"parallel data involves using union and intersection heuristics on both source-to-target and target-to-source word alignments, in the Moses system (Koehn et al., 2007). This string-based extraction methodology gives rise to ‘non-linguistic’ chunk pairs, henceforth known as S TR.1 In this paper, we seek to investigate performance of the baseline Moses MT system by changing one step only, namely the phrase extraction process. Specifically, this entails using three sets of syntactically motivated phrase pairs such as those extracted from node-aligned parallel treebanks. Tinsley et al. (2007) and Hearne et al. (2008) extracted phrasepairs from constituency-aligned and dependencyaligned data, giving rise to two types of linguistic chunk pairs: C ON and D EP respectively. Both these data sets were obtained by monolingual parsing of training sentences, subtree-aligning the parsed trees, and extracting word and phrase alignments . A prerequisite for this approach is the existence of constituency and dependency parsers for both the source and target languages. Hearne et al. (2008) demonstrated on a very small set of training data that combining string-based extraction (baseline Moses) with either of the syntax"
2009.mtsummit-posters.18,2005.mtsummit-papers.11,0,0.0214435,"system and data used in our experiments followed by a brief description of the four phrase extraction methodologies. 3.1 Tools and Resources As described in the previous section, we develop four French–English PB-SMT systems for our experiments: S TR, C ON, D EP, and P ERC. We use two different datasets. We obtain results on a small parallel corpora of approximately 7,700 parallel sentences—the JOC English–French parallel corpus (Chiao et al., 2006) [7,723 train + 400 dev + 599 test sentences]—and a larger set of 100,000 parallel sentences extracted from the freely available Europarl corpus (Koehn, 2005) [100,000 train + 1,889 dev + 2,000 test sentences]. Experimenting on the JOC corpus allows us compare our results directly with those of Hearne et al. (2008), while at the same time we successfully scale up their experiments by almost 13 times. We also used an open source tree aligner (Zhechev, 2009) to obtain subtree-alignments for the linguistic chunks C ON, D EP, and P ERC. The tree aligner works by performing a greedy search on all possible alignments between the tree pair nodes and scores using lexical probabilities to select the highest scoring alignment hypothesis. Constituency parse t"
2009.mtsummit-posters.18,P07-2045,0,0.0566193,"lities to select the highest scoring alignment hypothesis. Constituency parse trees were obtained by using the Berkeley parser (Petrov et al., 2006) for both the French and English sides, and dependency parse trees were obtained from the English and French versions of the Syntex parser (Bourigault et al., 2005). The dependency structures were converted into bracketed format to enable using the tree aligner. We used GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) for building a 5gram language model, Minimum Error Rate Training (Och, 2003) for tuning, and the Moses decoder (Koehn et al., 2007) in each of our systems. Thus the only difference between each of the four systems is in the phrase table used in the translation model. 3.2 Phrase Extraction We explore four different types of phrase pairs in this paper. The first type we term ‘non-linguistic’ in that phrase pairs are extracted by carrying out stringbased union and intersection of source-to-target and target-to-source GIZA++ word alignments (Koehn et al., 2003). The resulting phrases are mere sequences of aligned words occurring together and have no a priori syntactic motivation (cf. footnote 1). We label these as S TR. The r"
2009.mtsummit-posters.18,N03-1017,0,0.385978,"arsed trees, and extracting word and phrase alignments . A prerequisite for this approach is the existence of constituency and dependency parsers for both the source and target languages. Hearne et al. (2008) demonstrated on a very small set of training data that combining string-based extraction (baseline Moses) with either of the syntaxinduced phrase extractions resulted in improved translation accuracy with a general trend toward preferring dependency-based over constituency-based phrases. However, there exist more robust and accu1 The phrase-based statistical machine translation (PB-SMT) (Koehn et al., 2003) model is the most In the context of SMT a phrase may be any sequence of consecutive words (n-gram), not necessarily syntactic constituents. rate phrase structure parsers than dependency structure parsers for most languages in NLP applications, which has led to alternate measures of automatically generating dependencies from phrase structure parses, as shown on pages 129–131 of Nivre (2006). In this paper, we heuristically obtain dependency parses by using lexical head information in constituency parse trees. While the head percolation tables themselves are nothing new, the use of phrase pairs"
2009.mtsummit-posters.18,E09-1061,0,0.0267487,"racy. Thus we can supplement SMT phrases with syntax-aware phrases. Most system development today uses one particular approach to generate phrase pairs for us in translation, namely that of Koehn et al. (2003) (or perhaps more accurately, using the word- and phrasealignment scripts in Moses (Koehn et al., 2007)). However, some researchers have pointed out that system performance can be increased when chunks induced by other methods (EBMT (Groves and Way, 2005); constituency parsers (Tinsley et al., 2007); dependency parsers (Hearne et al., 2008)) are added to the SMT phrase table. 2 cf. also (Lopez, 2009), who argues that due the lack of systematicity in MT system development, it is extremely difficult to compare systems purporting to be of different types, and nigh on impossible to pinpoint exactly to which component any gains in performance might accurately be attributed. The point is: adherence to one approach may lead to sub-optimal system performance; if any one phrase pair induced by some other method proves to be useful, then ignoring other approaches will cause translation performance to deteriorate, even when the data size is increased. Accordingly, in this paper we invesigated whethe"
2009.mtsummit-posters.18,P95-1037,0,0.386011,"sultant phrase tables (cf. section 4). 2.2 Head Percolation It is possible to obtain a dependency parse for a sentence from its constituency parse (Gaifman, 1965) by exploiting lexicalized heads, i.e. head words of each phrase or constituent. In the absence of this information, a head percolation table is used to select the head node in each constituent structure. For example, the head of a phrase (NP (DET The) (N box)) is the node (N box). This implies an entry in the head percolation table specifying the node N as a head child of the node NP. Head percolation tables were first introduced in Magerman (1995) and implemented in Collins (1997). Head percolation tables are so called because, to extract headdependent information from a constituency parsed treebank, the lexical items are percolated like features from the heads to their parent projections. A head percolation table consists of hand-coded rules identifying the head-child of each node. We impleSYSTEM S TR (S) C ON (C) D EP (D) P ERC (P) S+C S+D S+P C+D C+P D+P S+C+D S+C+P S+D+P C+D+P S+C+D+P BLEU 31.29 30.64 30.75 29.19 32.87 32.69 32.34 31.24 30.99 31.40 32.70 32.49 32.62 31.46 32.82 (a) JOC DATA NIS MET WER 6.31 63.91 61.09 6.34 63.82 6"
2009.mtsummit-posters.18,P03-1021,0,0.00769344,"ee pair nodes and scores using lexical probabilities to select the highest scoring alignment hypothesis. Constituency parse trees were obtained by using the Berkeley parser (Petrov et al., 2006) for both the French and English sides, and dependency parse trees were obtained from the English and French versions of the Syntex parser (Bourigault et al., 2005). The dependency structures were converted into bracketed format to enable using the tree aligner. We used GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) for building a 5gram language model, Minimum Error Rate Training (Och, 2003) for tuning, and the Moses decoder (Koehn et al., 2007) in each of our systems. Thus the only difference between each of the four systems is in the phrase table used in the translation model. 3.2 Phrase Extraction We explore four different types of phrase pairs in this paper. The first type we term ‘non-linguistic’ in that phrase pairs are extracted by carrying out stringbased union and intersection of source-to-target and target-to-source GIZA++ word alignments (Koehn et al., 2003). The resulting phrases are mere sequences of aligned words occurring together and have no a priori syntactic mot"
2009.mtsummit-posters.18,J03-1002,0,0.00259952,"ic chunks C ON, D EP, and P ERC. The tree aligner works by performing a greedy search on all possible alignments between the tree pair nodes and scores using lexical probabilities to select the highest scoring alignment hypothesis. Constituency parse trees were obtained by using the Berkeley parser (Petrov et al., 2006) for both the French and English sides, and dependency parse trees were obtained from the English and French versions of the Syntex parser (Bourigault et al., 2005). The dependency structures were converted into bracketed format to enable using the tree aligner. We used GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) for building a 5gram language model, Minimum Error Rate Training (Och, 2003) for tuning, and the Moses decoder (Koehn et al., 2007) in each of our systems. Thus the only difference between each of the four systems is in the phrase table used in the translation model. 3.2 Phrase Extraction We explore four different types of phrase pairs in this paper. The first type we term ‘non-linguistic’ in that phrase pairs are extracted by carrying out stringbased union and intersection of source-to-target and target-to-source GIZA++ word alignments (Koehn et al.,"
2009.mtsummit-posters.18,P02-1040,0,0.0758823,"Missing"
2009.mtsummit-posters.18,P06-1055,0,0.0118875,"s]. Experimenting on the JOC corpus allows us compare our results directly with those of Hearne et al. (2008), while at the same time we successfully scale up their experiments by almost 13 times. We also used an open source tree aligner (Zhechev, 2009) to obtain subtree-alignments for the linguistic chunks C ON, D EP, and P ERC. The tree aligner works by performing a greedy search on all possible alignments between the tree pair nodes and scores using lexical probabilities to select the highest scoring alignment hypothesis. Constituency parse trees were obtained by using the Berkeley parser (Petrov et al., 2006) for both the French and English sides, and dependency parse trees were obtained from the English and French versions of the Syntex parser (Bourigault et al., 2005). The dependency structures were converted into bracketed format to enable using the tree aligner. We used GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) for building a 5gram language model, Minimum Error Rate Training (Och, 2003) for tuning, and the Moses decoder (Koehn et al., 2007) in each of our systems. Thus the only difference between each of the four systems is in the phrase table used in the translation"
2009.mtsummit-posters.18,P05-1034,0,0.0294524,"rase structure parses for both the source and target sides using monolingual parsers and then aligning the subtrees using a statistical tree aligner. Hearne et al. (2008) go a step further by building on the work of (Tinsley et al., 2007) and adding phrase pairs induced from dependency parse trees. Note that all these approaches work on string-based translation models, i.e. syntactic knowledge is merely used to extract linguistically motivated phrase pairs. The phrase tables still contain translations of strings, just like in Moses. There also exist a number of other approaches (Chiang, 2005; Quirk et al., 2005; Galley et al., 2006; Hassan et al., 2008) which have developed different models where the incorporation of syntax has shown itself to be beneficial. However such models are not restricted to the string-based translation modeling and are thus somewhat out of the scope of this paper. In this paper, we extend the experiments of Hearne et al. (2008) by adding another syntax-aware phrase extraction methodology, namely percolated dependencies. We also scale up the volume of the training data, and compare and contrast the resultant phrase tables (cf. section 4). 2.2 Head Percolation It is possible"
2009.mtsummit-posters.18,2009.eamt-1.20,1,0.866406,"Missing"
2009.mtsummit-posters.18,H01-1014,0,0.0292702,".22 45.07 45.58 44.77 BLEU 28.50 25.64 25.24 25.87 29.50 29.30 29.45 26.32 26.37 26.57 29.29 29.49 29.39 26.90 29.40 (b) EUROPARL DATA NIS MET WER 7.00 57.83 57.43 6.55 55.26 60.77 6.59 54.65 60.73 6.59 55.63 60.76 7.10 58.55 56.62 7.08 58.43 56.84 7.10 58.54 56.73 6.69 55.56 59.97 6.62 56.05 60.41 6.74 55.83 59.53 7.09 58.48 56.70 7.10 58.50 56.59 7.09 58.49 56.80 6.75 56.14 59.38 7.09 58.49 56.67 PER 44.11 46.82 46.51 46.48 43.40 43.62 43.43 45.90 46.40 45.62 43.41 43.45 43.65 45.53 43.49 Table 1: Summary of the results on (a) JOC and (b) Europarl test data mented the algorithm described in Xia and Palmer (2001) to obtain head-dependent relations between words of a sentence. The head percolation algorithm will output the head or governor for each word in the sentence. In case the word is the head word of the sentence, it will be assigned a default value as its head. We used this method to obtain dependency parse structures from constituency parse structures for both the source and target languages. We distinguish these structures from the dependency structures obtained directly from a dependency parser by labelling the former as percolated dependencies. Theoretically, these percolated dependencies ar"
2009.mtsummit-posters.18,C08-1144,0,0.0151596,"hs for the other automatic evaluation metrics, this tendency Figure 1: Bar graph to show that adding P ERC chunks (black bar) to any system (white bar) generally boosts the BLEU score: Europarl data is confirmed across all evaluation metrics used in our experiments for both corpora. 5 Conclusion and Future Work While producing smaller translation models and believed to contain more useful (syntax-aware) phrases than the standard string-based extraction, the syntaxbased extractions may perform worse than the PBSMT string-based baseline, especially as the amount of training data increases (cf. (Zollmann et al., 2008)).2 However, it has been observed by many researchers that rather than replacing one with the other, combining both types of induced phrases into one translation model significantly improves the translation accuracy. Thus we can supplement SMT phrases with syntax-aware phrases. Most system development today uses one particular approach to generate phrase pairs for us in translation, namely that of Koehn et al. (2003) (or perhaps more accurately, using the word- and phrasealignment scripts in Moses (Koehn et al., 2007)). However, some researchers have pointed out that system performance can be"
2009.mtsummit-posters.18,chiao-etal-2006-evaluation,0,\N,Missing
2009.mtsummit-posters.7,W05-0909,0,0.0262759,"influence the word sequence of the consensus output? To study the first issue, considering that the word order of CN is decided by the backbone, we performed a set of experiments to compare the influence on consensus output of selecting different backbones for our CN. Table 1 shows the comparison results. We use the WMT09 English-to-French system combination shared task as the evaluation data set, including 2525 sentences and 16 1-best systems. TER is used as the default hypothesis alignment metric. The results are reported in TER, casesensitive BLEU, NIST (Doddington, 2002) and Meteor (MTR) (Banerjee and Lavie, 2005). The Worst-CN, Best-CN and MBR-CN are the outputs of the CNs using the worst single, best single and MBR result as the backbone respectively. We can see that 1) MBR is better than the best single system; 2) the MBR-CN obtains the best perBackbone Oracle Worst Single Best Single MBR Worst-CN Best-CN MBR-CN TER 52.58 69.19 59.21 58.05 59.16 57.03 56.84 BLEU 33.84 14.73 25.43 26.54 23.53 26.73 27.56 NIST 8.04 5.57 6.99 7.12 7.04 7.29 7.33 MTR 23.95 12.40 18.97 19.81 17.63 19.84 20.33 Table 1: The influence of backbone on CN formance in terms of the four automatic evaluation metrics. The better t"
2009.mtsummit-posters.7,D07-1029,0,0.0760956,"ce the TER-based (Snover et al., 2006) system combination strategy was introduced to system combination in (Sim et al., 2007) and was shown to outperform the Word Error Rate (WER) alignment metric, many hypothesis alignment metrics have been proposed and successfully applied in system combination, such as ITER (Rosti et al., 2008), ITG (Karakos et al., 2008) and IHMM (He et al., 2008). In all these papers, the proposed alignment method outperformed the TER-based baseline system. In system combination, source—target-related knowledge has been shown to significantly improve translation quality (Huang and Papineni, 2007; Rosti et al., 2007a; He et al., 2008). At present, although mainstream statistical machine translation (SMT) systems are implemented based on various paradigms—phrasal, hierarchical and syntax-based—all of them still use the word alignment between the source and target side as the cornerstone. Such systems have demonstrated that word alignment accuracy plays a crucial role when it comes to translation quality. Intuitively, such bilingual word alignment contextual information could be useful in the post-processing stage, especially for the system combination phase. This paper proposes a sourc"
2009.mtsummit-posters.7,D08-1011,0,0.32152,"d method outperforms the state-of-the-art TERbased alignment model in our experiments on the WMT09 English-to-French and NIST Chinese-to-English data sets respectively. Experimental results demonstrate that our proposed approach scores consistently among the best results across different data and language pair conditions. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion networkbased networks, either single (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Sim et al., 2007; He et al., 2008) or multiple networks (Rosti et al., 2007b; Rosti et al., 2008), have become the state-of-the-art methodology to implement the combination strategy. A CN is essentially a directed acyclic graph which is built by aligning a set of translation hypotheses against a reference or “backbone”. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Generally, like the translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list"
2009.mtsummit-posters.7,P08-2021,0,0.0135473,"a merged set of hypotheses, and then the remaining hypotheses are aligned against the backbone by a specific alignment approach. Currently, most research on system combination has focussed on hypothesis alignment due to its significant role in combination. Since the TER-based (Snover et al., 2006) system combination strategy was introduced to system combination in (Sim et al., 2007) and was shown to outperform the Word Error Rate (WER) alignment metric, many hypothesis alignment metrics have been proposed and successfully applied in system combination, such as ITER (Rosti et al., 2008), ITG (Karakos et al., 2008) and IHMM (He et al., 2008). In all these papers, the proposed alignment method outperformed the TER-based baseline system. In system combination, source—target-related knowledge has been shown to significantly improve translation quality (Huang and Papineni, 2007; Rosti et al., 2007a; He et al., 2008). At present, although mainstream statistical machine translation (SMT) systems are implemented based on various paradigms—phrasal, hierarchical and syntax-based—all of them still use the word alignment between the source and target side as the cornerstone. Such systems have demonstrated that wor"
2009.mtsummit-posters.7,N04-1022,0,0.0137748,"ected acyclic graph which is built by aligning a set of translation hypotheses against a reference or “backbone”. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Generally, like the translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN is constructed on the word level by a state-of-the-art framework. Firstly, a minimum Bayes-risk (MBR) decoder (Kumar and Byrne, 2004) is utilised to choose the backbone from a merged set of hypotheses, and then the remaining hypotheses are aligned against the backbone by a specific alignment approach. Currently, most research on system combination has focussed on hypothesis alignment due to its significant role in combination. Since the TER-based (Snover et al., 2006) system combination strategy was introduced to system combination in (Sim et al., 2007) and was shown to outperform the Word Error Rate (WER) alignment metric, many hypothesis alignment metrics have been proposed and successfully applied in system combination,"
2009.mtsummit-posters.7,D07-1105,0,0.120795,"ark in this position on the hypothesis side which denotes “deletion”; similarly, the “insertion” operation can be performed. Fig. 1 (c) shows the results after performing word reordering and the edit operation. 4 Experimental Settings In this section, we introduce the experimental settings for evaluating our source-side contextinformed hypothesis alignment method. 4.1 Training Data To verify the effectiveness of our method, we performed experiments on Chinese-to-English (C2E) and English-to-French (E2F) data sets. Diversity has a significant influence on the performance of system combination (Macherey and Och, 2007). Although we have different types of MT systems to generate a set of translations, the training data for these systems are basically the same. This would cause a high correlation between the hypotheses and would potentially decrease the system combination performance. In order to increase the diversity, we sample the training data to train a number of translation models. Furthermore, we can adjust parameters such as the distortion limit or use different development sets to reduce any such correlation. Chinese-to-English Task 5 sub-training data sets are randomly sampled from a large-scale dat"
2009.mtsummit-posters.7,E06-1005,0,0.499824,"uild the confusion network (CN). The source-side context-based method outperforms the state-of-the-art TERbased alignment model in our experiments on the WMT09 English-to-French and NIST Chinese-to-English data sets respectively. Experimental results demonstrate that our proposed approach scores consistently among the best results across different data and language pair conditions. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion networkbased networks, either single (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Sim et al., 2007; He et al., 2008) or multiple networks (Rosti et al., 2007b; Rosti et al., 2008), have become the state-of-the-art methodology to implement the combination strategy. A CN is essentially a directed acyclic graph which is built by aligning a set of translation hypotheses against a reference or “backbone”. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Generally, like the translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different"
2009.mtsummit-posters.7,J03-1002,0,0.0112133,"match’ algorithm is incapable of dealing with this issue. In our experiments, three dominant types of hypothesis alignment metrics are used, namely TER, HMM (Matusov et al., 2006) and IHMM. The data set we used is still the WMT09 English-to-French system combination shared task. TER aligns the words based on the exact match principle; HMM uses the same principle as the word alignment model of (Vogel et al., 1996), while IHMM uses two similarity models and one distortion model to perform the alignment. Table 2 shows the results for these three metrics. alignment links derived from the GIZA++ (Och and Ney, 2003) training. 3.2 Description of Algorithm In this experiment, the three CNs are built on the MBR-based backbone, and decoded using the same features and weights. We can see that in this task, the HMM approach outperforms the other two methods. When we manually examine the alignment result, the HMM method has a higher word alignment accuracy and produces a lower non-grammatical error rate. As the source–target word alignment task, the aim of hypothesis alignment is to obtain the best word alignment links between the hypothesis and the backbone. Intuitively, this task has been performed in the pro"
2009.mtsummit-posters.7,P02-1040,0,0.0772195,"istics: 1) it is a word-level graph; 2) a monotone decoding process is selected. Therefore, hypothesis alignment plays a vital role in the CN because the backbone sentence decides the skeleton and word order of the consensus output. Figure 1 shows the main steps of how to align the hypotheses and how to carry out the word reordering as well as construct the CN. In Fig. 1(a), hypotheses from different MT systems are merged to form a new N -best list, from which the backbone is selected using the MBR decoder. The most frequently used loss functions in MBR are TER (Snover et al., 2006) and BLEU (Papineni et al., 2002). Then as illustrated in Fig. 1(b), E1 is assumed as the backbone, with the rest of the hypotheses aligned against it. The symbol @ denotes a null word. Note that there are only three types of word alignment in system combination, namely, 1-to-1, 1to-null and null-to-1 in terms of bidirectional alignment. According to the word alignment, word reordering is carried out and a CN is constructed based on the reordered hypotheses as Fig. 1 (c) shows. Finally, a set of global and local features are integrated into a log-linear model to decode the CN. The most challenging problem for CN decoding is t"
2009.mtsummit-posters.7,N07-1029,0,0.170646,"work (CN). The source-side context-based method outperforms the state-of-the-art TERbased alignment model in our experiments on the WMT09 English-to-French and NIST Chinese-to-English data sets respectively. Experimental results demonstrate that our proposed approach scores consistently among the best results across different data and language pair conditions. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion networkbased networks, either single (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Sim et al., 2007; He et al., 2008) or multiple networks (Rosti et al., 2007b; Rosti et al., 2008), have become the state-of-the-art methodology to implement the combination strategy. A CN is essentially a directed acyclic graph which is built by aligning a set of translation hypotheses against a reference or “backbone”. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Generally, like the translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search"
2009.mtsummit-posters.7,W08-0329,0,0.230986,"model in our experiments on the WMT09 English-to-French and NIST Chinese-to-English data sets respectively. Experimental results demonstrate that our proposed approach scores consistently among the best results across different data and language pair conditions. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion networkbased networks, either single (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Sim et al., 2007; He et al., 2008) or multiple networks (Rosti et al., 2007b; Rosti et al., 2008), have become the state-of-the-art methodology to implement the combination strategy. A CN is essentially a directed acyclic graph which is built by aligning a set of translation hypotheses against a reference or “backbone”. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Generally, like the translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN"
2009.mtsummit-posters.7,P07-1040,0,0.193333,"work (CN). The source-side context-based method outperforms the state-of-the-art TERbased alignment model in our experiments on the WMT09 English-to-French and NIST Chinese-to-English data sets respectively. Experimental results demonstrate that our proposed approach scores consistently among the best results across different data and language pair conditions. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion networkbased networks, either single (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Sim et al., 2007; He et al., 2008) or multiple networks (Rosti et al., 2007b; Rosti et al., 2008), have become the state-of-the-art methodology to implement the combination strategy. A CN is essentially a directed acyclic graph which is built by aligning a set of translation hypotheses against a reference or “backbone”. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Generally, like the translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search"
2009.mtsummit-posters.7,2006.amta-papers.25,0,0.105749,"combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN is constructed on the word level by a state-of-the-art framework. Firstly, a minimum Bayes-risk (MBR) decoder (Kumar and Byrne, 2004) is utilised to choose the backbone from a merged set of hypotheses, and then the remaining hypotheses are aligned against the backbone by a specific alignment approach. Currently, most research on system combination has focussed on hypothesis alignment due to its significant role in combination. Since the TER-based (Snover et al., 2006) system combination strategy was introduced to system combination in (Sim et al., 2007) and was shown to outperform the Word Error Rate (WER) alignment metric, many hypothesis alignment metrics have been proposed and successfully applied in system combination, such as ITER (Rosti et al., 2008), ITG (Karakos et al., 2008) and IHMM (He et al., 2008). In all these papers, the proposed alignment method outperformed the TER-based baseline system. In system combination, source—target-related knowledge has been shown to significantly improve translation quality (Huang and Papineni, 2007; Rosti et al."
2009.mtsummit-posters.7,C96-2141,0,0.13533,"ng synonyms to each other is a challenging issue. For instance, in Fig. 1 (b), “risen” in E1 and “increased” in E2 and E3 express the same meaning with different morphologies. Of course, a simple ‘exact match’ algorithm is incapable of dealing with this issue. In our experiments, three dominant types of hypothesis alignment metrics are used, namely TER, HMM (Matusov et al., 2006) and IHMM. The data set we used is still the WMT09 English-to-French system combination shared task. TER aligns the words based on the exact match principle; HMM uses the same principle as the word alignment model of (Vogel et al., 1996), while IHMM uses two similarity models and one distortion model to perform the alignment. Table 2 shows the results for these three metrics. alignment links derived from the GIZA++ (Och and Ney, 2003) training. 3.2 Description of Algorithm In this experiment, the three CNs are built on the MBR-based backbone, and decoded using the same features and weights. We can see that in this task, the HMM approach outperforms the other two methods. When we manually examine the alignment result, the HMM method has a higher word alignment accuracy and produces a lower non-grammatical error rate. As the so"
2009.mtsummit-posters.7,2004.tmi-1.9,0,0.126907,"Missing"
2009.mtsummit-posters.8,P07-1038,0,0.0331674,"t and tuning of MT systems and are widely applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exploit machine learning techniques (Albrecht and Hwa, 2007; Ye et al., 2007; He and Way, 2009). In this paper, we investigate the impact of using three representative metrics and their combinations as error functions in MERT. We have not tested with metrics that exploit deeper linguistic information because their computation is typically quite slow and thus less appropriate for MERT tuning. 3.1 B LEU B LEU is the most popular evaluation metric in MT development. Although it suffers from several shortcomings, such as low correlation with human judgment on the sentence level, preference to statistical systems (Callison-Burch et al., 2006) and inconsist"
2009.mtsummit-posters.8,W05-0909,0,0.0901601,"the results can be improved by using an entirely different evaluation metric or a combination of different metrics during MERT. 3 Automatic MT Evaluation Metrics Automatic evaluation metrics enable researchers to validate and optimize translation methods quickly. Simple n-gram-based metrics such as B LEU (Papineni et al., 2002) are fundamental to the development and tuning of MT systems and are widely applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exploit machine learning techniques (Albrecht and Hwa, 2007; Ye et al., 2007; He and Way, 2009). In this paper, we investigate the impact of using three representative metrics and their combinations as error functions in MERT. We have not tested with metrics that exploit deeper linguistic informati"
2009.mtsummit-posters.8,W08-0309,0,0.0167253,"e error function in MERT. B LEU performs n-gram matching between the output and the reference and the score is n-gram precision with a brevity penalty, as in (4): B LEU (n) = n Y 1 P RECin · bp (4) i=1 where n is the order of n-gram, P RECi is the igram precision and bp is the brevity penalty, as in (5): len(Ref ) − 1, 0)) (5) len(Out) where len(Ref ) is the length of the reference and len(Out) the length of the output. The n-gram matching scheme in B LEU makes it very sensitive to small changes in the output, especially in the single reference scenario. It has been shown in evaluation tasks (Callison-Burch et al., 2008) that B LEU has a lower correlation with human judgment than newer metrics like M ETEOR and T ER. bp = exp(max( 3.2 M ETEOR M ETEOR tries to solve the problems of B LEU by performing multi-stage unigram matching and adding recall as a consideration. With the use of unigram matching, M ETEOR is less sensitive to variations in word order, and with multi-stage matching, M ETEOR can consider stemming and WordNet semantic information. The M ETEOR score is calculated as in (6): M ETEOR = PR · (1 − cp) αP + (1 − α)R (6) where P is the unigram precision, R is the unigram recall and cp is the chunk pen"
2009.mtsummit-posters.8,E06-1032,0,0.0281307,"of translation errors correctly; for example, when using B LEU with only a single reference as the objective function, the results can be improved by using an entirely different evaluation metric or a combination of different metrics during MERT. 3 Automatic MT Evaluation Metrics Automatic evaluation metrics enable researchers to validate and optimize translation methods quickly. Simple n-gram-based metrics such as B LEU (Papineni et al., 2002) are fundamental to the development and tuning of MT systems and are widely applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exploit machine learning techniques (Albrecht and Hwa, 2007; Ye et al., 2007; He and Way, 2009). In this paper, we investigate the impact of using three representative metrics and the"
2009.mtsummit-posters.8,D08-1064,0,0.0191342,"In this paper, we investigate the impact of using three representative metrics and their combinations as error functions in MERT. We have not tested with metrics that exploit deeper linguistic information because their computation is typically quite slow and thus less appropriate for MERT tuning. 3.1 B LEU B LEU is the most popular evaluation metric in MT development. Although it suffers from several shortcomings, such as low correlation with human judgment on the sentence level, preference to statistical systems (Callison-Burch et al., 2006) and inconsistency in related evaluation scenarios (Chiang et al., 2008), it is still the automatic evaluation metric used in many translation campaigns and is often used as the error function in MERT. B LEU performs n-gram matching between the output and the reference and the score is n-gram precision with a brevity penalty, as in (4): B LEU (n) = n Y 1 P RECin · bp (4) i=1 where n is the order of n-gram, P RECi is the igram precision and bp is the brevity penalty, as in (5): len(Ref ) − 1, 0)) (5) len(Out) where len(Ref ) is the length of the reference and len(Out) the length of the output. The n-gram matching scheme in B LEU makes it very sensitive to small cha"
2009.mtsummit-posters.8,W09-0426,0,0.390997,"rs in log-linear models by searching for the best parameter settings on the N-best output which minimizes translation errors according to automatic evaluation metrics such as B LEU (Papineni et al., 2002), M ETEOR (Banerjee and Lavie, 2005) or T ER (Snover et al., 2006). In many shared translation tasks, it has been common practice to tune parameters with MERT against a mainstream evaluation metric to improve translation quality with respect to both automatic and human judgments. Most of the time the metric used in MERT is B LEU, but efforts have also been made to tune against other criteria (Dyer et al., 2009). In this paper, we investigate the effect of the choice of metric, or the objective function in MERT. We find that in the single reference scenario (WMT08 English–French), tuning on B LEU leads to significantly inferior B LEU scores than when tuning on M ETEOR. We replicate similar results in another direction of the experiment with a slightly modified version of M ETEOR. Based on this investigation, we propose three ways of combining different metrics into one error function to avoid the bias of single metrics: Linear Combination, and two methods of Constrained Search. These approaches recei"
2009.mtsummit-posters.8,2009.eamt-1.7,1,0.762581,"applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exploit machine learning techniques (Albrecht and Hwa, 2007; Ye et al., 2007; He and Way, 2009). In this paper, we investigate the impact of using three representative metrics and their combinations as error functions in MERT. We have not tested with metrics that exploit deeper linguistic information because their computation is typically quite slow and thus less appropriate for MERT tuning. 3.1 B LEU B LEU is the most popular evaluation metric in MT development. Although it suffers from several shortcomings, such as low correlation with human judgment on the sentence level, preference to statistical systems (Callison-Burch et al., 2006) and inconsistency in related evaluation scenarios"
2009.mtsummit-posters.8,2005.mtsummit-papers.11,0,0.00737216,"s status as the de facto automatic evaluation criterion in shared translation campaigns. How(9) As some metrics are biased to longer/shorter outputs, the length ratio helps us see whether a change in score is a real improvement, or rather a bias. 4.1 Experimental Settings We conduct English–French and French–English single reference experiments on WMT 2008 data (WMT08). We use the top-1000 sentences in the original development set as our development set and the remaining 1000 as the test set. We train the translation model and a 4-gram language model on 1,288,074 sentence-pairs from Europarl (Koehn, 2005). The multiple reference experiments run on NIST 2006 (MT06) data. The translation model and a 3-gram language model is trained on data provided by LDC. The single reference results are given in Tables 1 and 2, with the multiple reference results shown in Table 3. Scores in bold are statistically (1,000 bootstraps, 300 sentences each bootstrap) better than the others listed. We report results on both dev and test sets for the WMT08 dataset. All other results are test set results due to limited space. In all experiments, we tune our parameters using a modified version of ZMERT (Zaidan, 2009) on"
2009.mtsummit-posters.8,D07-1091,0,0.0276976,"is organised as follows. Section 2 reviews MERT, and Section 3 describes the characteristics of the different evaluation metrics we use. Sections 4 and 5 describe and analyse our experiments on single metrics as the objective functions in MERT. Sections 6 and 7 introduce our combination of different error functions and present experimental results. We conclude in Section 8, together with some avenues for further research. 2 Minimum Error Rate Training Minimum Error Rate Training (MERT) is rooted in the log-linear models which have been applied successfully in SMT. In (Och and Ney, 2002) and (Koehn and Hoang, 2007), log-linear models are used to incorporate various information sources (features hi ) into the translation model, as in (1): X 1 λi hi (e, f) p(e|f) = exp Z n (1) i=1 MERT tunes the weights λi to minimize the errors on the error surface of the N-best list of the development set, as in (2): argmin Err(e∗ (λ); ref) (2) λ In practice, the function Err is estimated by errors on a specific automatic evaluation metric m (most often B LEU). Then MERT is actually optimising on (3): argmin errm (e∗ (λ); ref) (3) λ Most current research has focused on the algorithm of MERT itself. For example, (Machere"
2009.mtsummit-posters.8,P07-2045,0,0.0193699,"Missing"
2009.mtsummit-posters.8,W05-0904,0,0.0328653,"on methods quickly. Simple n-gram-based metrics such as B LEU (Papineni et al., 2002) are fundamental to the development and tuning of MT systems and are widely applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exploit machine learning techniques (Albrecht and Hwa, 2007; Ye et al., 2007; He and Way, 2009). In this paper, we investigate the impact of using three representative metrics and their combinations as error functions in MERT. We have not tested with metrics that exploit deeper linguistic information because their computation is typically quite slow and thus less appropriate for MERT tuning. 3.1 B LEU B LEU is the most popular evaluation metric in MT development. Although it suffers from several shortcomings, such as low correlation"
2009.mtsummit-posters.8,D08-1076,0,0.0604049,", 2007), log-linear models are used to incorporate various information sources (features hi ) into the translation model, as in (1): X 1 λi hi (e, f) p(e|f) = exp Z n (1) i=1 MERT tunes the weights λi to minimize the errors on the error surface of the N-best list of the development set, as in (2): argmin Err(e∗ (λ); ref) (2) λ In practice, the function Err is estimated by errors on a specific automatic evaluation metric m (most often B LEU). Then MERT is actually optimising on (3): argmin errm (e∗ (λ); ref) (3) λ Most current research has focused on the algorithm of MERT itself. For example, (Macherey et al., 2008) use word lattices instead of an N-best list to estimate the search space, and (Moore and Quirk, 2008) use random restarts to avoid local optima. In this research, however, we try to improve the error surface/objective function on which MERT optimises. We will show that when the error surface cannot estimate the actual number of translation errors correctly; for example, when using B LEU with only a single reference as the objective function, the results can be improved by using an entirely different evaluation metric or a combination of different metrics during MERT. 3 Automatic MT Evaluation"
2009.mtsummit-posters.8,C08-1074,0,0.0375063,"translation model, as in (1): X 1 λi hi (e, f) p(e|f) = exp Z n (1) i=1 MERT tunes the weights λi to minimize the errors on the error surface of the N-best list of the development set, as in (2): argmin Err(e∗ (λ); ref) (2) λ In practice, the function Err is estimated by errors on a specific automatic evaluation metric m (most often B LEU). Then MERT is actually optimising on (3): argmin errm (e∗ (λ); ref) (3) λ Most current research has focused on the algorithm of MERT itself. For example, (Macherey et al., 2008) use word lattices instead of an N-best list to estimate the search space, and (Moore and Quirk, 2008) use random restarts to avoid local optima. In this research, however, we try to improve the error surface/objective function on which MERT optimises. We will show that when the error surface cannot estimate the actual number of translation errors correctly; for example, when using B LEU with only a single reference as the objective function, the results can be improved by using an entirely different evaluation metric or a combination of different metrics during MERT. 3 Automatic MT Evaluation Metrics Automatic evaluation metrics enable researchers to validate and optimize translation methods"
2009.mtsummit-posters.8,P03-1021,0,0.239019,"Missing"
2009.mtsummit-posters.8,P02-1038,0,0.0480355,"o. The rest of the paper is organised as follows. Section 2 reviews MERT, and Section 3 describes the characteristics of the different evaluation metrics we use. Sections 4 and 5 describe and analyse our experiments on single metrics as the objective functions in MERT. Sections 6 and 7 introduce our combination of different error functions and present experimental results. We conclude in Section 8, together with some avenues for further research. 2 Minimum Error Rate Training Minimum Error Rate Training (MERT) is rooted in the log-linear models which have been applied successfully in SMT. In (Och and Ney, 2002) and (Koehn and Hoang, 2007), log-linear models are used to incorporate various information sources (features hi ) into the translation model, as in (1): X 1 λi hi (e, f) p(e|f) = exp Z n (1) i=1 MERT tunes the weights λi to minimize the errors on the error surface of the N-best list of the development set, as in (2): argmin Err(e∗ (λ); ref) (2) λ In practice, the function Err is estimated by errors on a specific automatic evaluation metric m (most often B LEU). Then MERT is actually optimising on (3): argmin errm (e∗ (λ); ref) (3) λ Most current research has focused on the algorithm of MERT i"
2009.mtsummit-posters.8,W07-0714,1,0.911856,"Missing"
2009.mtsummit-posters.8,P02-1040,0,0.082055,"research, however, we try to improve the error surface/objective function on which MERT optimises. We will show that when the error surface cannot estimate the actual number of translation errors correctly; for example, when using B LEU with only a single reference as the objective function, the results can be improved by using an entirely different evaluation metric or a combination of different metrics during MERT. 3 Automatic MT Evaluation Metrics Automatic evaluation metrics enable researchers to validate and optimize translation methods quickly. Simple n-gram-based metrics such as B LEU (Papineni et al., 2002) are fundamental to the development and tuning of MT systems and are widely applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exp"
2009.mtsummit-posters.8,2006.amta-papers.25,0,0.420352,"an entirely different evaluation metric or a combination of different metrics during MERT. 3 Automatic MT Evaluation Metrics Automatic evaluation metrics enable researchers to validate and optimize translation methods quickly. Simple n-gram-based metrics such as B LEU (Papineni et al., 2002) are fundamental to the development and tuning of MT systems and are widely applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exploit machine learning techniques (Albrecht and Hwa, 2007; Ye et al., 2007; He and Way, 2009). In this paper, we investigate the impact of using three representative metrics and their combinations as error functions in MERT. We have not tested with metrics that exploit deeper linguistic information because their computation is"
2009.mtsummit-posters.8,W07-0736,0,0.0236446,"ms and are widely applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exploit machine learning techniques (Albrecht and Hwa, 2007; Ye et al., 2007; He and Way, 2009). In this paper, we investigate the impact of using three representative metrics and their combinations as error functions in MERT. We have not tested with metrics that exploit deeper linguistic information because their computation is typically quite slow and thus less appropriate for MERT tuning. 3.1 B LEU B LEU is the most popular evaluation metric in MT development. Although it suffers from several shortcomings, such as low correlation with human judgment on the sentence level, preference to statistical systems (Callison-Burch et al., 2006) and inconsistency in related e"
2009.mtsummit-posters.8,N06-1057,0,0.0198288,"validate and optimize translation methods quickly. Simple n-gram-based metrics such as B LEU (Papineni et al., 2002) are fundamental to the development and tuning of MT systems and are widely applied in MERT. However, it is well known that B LEU has many limitations (Callison-Burch et al., 2006). Many approaches have been proposed to overcome these insufficiencies, including M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006), both of which try to improve on the matching strategy used in B LEU. There are other types of MT metrics that exploit deeper features such as paraphrases (Zhou et al., 2006), or syntax (Liu and Gildea, 2005; Owczarzak et al., 2007). There are also metrics that try to exploit machine learning techniques (Albrecht and Hwa, 2007; Ye et al., 2007; He and Way, 2009). In this paper, we investigate the impact of using three representative metrics and their combinations as error functions in MERT. We have not tested with metrics that exploit deeper linguistic information because their computation is typically quite slow and thus less appropriate for MERT tuning. 3.1 B LEU B LEU is the most popular evaluation metric in MT development. Although it suffers from several shor"
2010.amta-commercial.14,C10-2043,1,0.838625,"Missing"
2010.amta-commercial.14,2006.iwslt-evaluation.4,1,0.844918,"at this juncture that the framework discussed in section 6 is not yet a fully commercial system, nor are what we describe in sections 3 and 4 the limits of our MT system’s capabilities. There will be significantly more developments and innovations over the lifetime of PLuTO. 2.1 The PLuTO Consortium PLuTO comprises a dynamic industry-academia consortium, each member of which brings significant experience and expertise to some facet of the service. The Centre for Next Generation Localisation (CNGL) 2 at Dublin City University brings to the target platform a state-of-the-art MT engine, MaTrEx (Stroppa and Way, 2006; Tinsley et al., 2008, Penkale et al., 2010) (described further in section 3). ESTeam AB contributes a comprehensive translation software environment, including multilayered, multi-domain translation memory technology. The Information Retrieval Facility (IRF) provides search and retrieval expertise as well as a substantial multilingual patent repository. Finally, CrossLanguage and The Dutch Patent User Information Group (WON) provide evaluation, analysis 2 http://www.cngl.ie 2.2 PLuTO Objectives a) Development of a rapid solution for patent search and translation by drawing on the expertise o"
2010.amta-commercial.14,W08-0326,1,0.899346,"Missing"
2010.amta-papers.11,P06-1067,0,0.0571373,"Missing"
2010.amta-papers.11,J82-2005,0,0.733876,"Missing"
2010.amta-papers.11,W09-2307,0,0.251311,"anguage pairs, phrase-based statistical machine translation (PBSMT) systems (Koehn et al., 2003) incorporate two different of methods: 1) learning phrase pairs with different word orders in the source and target sentences; 2) attempting potential target phrase orders during the decoding phase, and penalizing potential phrase orders using both distance-based and lexical reordering models. However, for some language pairs, this model is not powerful enough to capture the word order differences between the source and target sentences. To tackle this problem, previous studies (Wang et al., 2007a; Chang et al., 2009a) showed that syntactic reorderings can benefit state-of-the-art PBSMT systems by handling systematic differences in word order between language pairs. From their conclusions, for the Chinese-English task, syntactic reorderings can greatly improve the performance by explicitly modeling the structural differences between this language pair. Interestingly, lots of work has been reported on syntactic reorderings and similar conclusions have been drawn from them. These methods can be roughly divided into two main categories (Elming, 2008): the deterministic reordering approach and the non-determi"
2010.amta-papers.11,P05-1066,0,0.190262,"Missing"
2010.amta-papers.11,2010.eamt-1.32,1,0.830881,"Missing"
2010.amta-papers.11,W08-0406,0,0.0598634,"e this problem, previous studies (Wang et al., 2007a; Chang et al., 2009a) showed that syntactic reorderings can benefit state-of-the-art PBSMT systems by handling systematic differences in word order between language pairs. From their conclusions, for the Chinese-English task, syntactic reorderings can greatly improve the performance by explicitly modeling the structural differences between this language pair. Interestingly, lots of work has been reported on syntactic reorderings and similar conclusions have been drawn from them. These methods can be roughly divided into two main categories (Elming, 2008): the deterministic reordering approach and the non-deterministic reordering approach. For the deterministic approach, syntactic reorderings take place outside the PBSMT system, and the corresponding PBSMT systems only deal with the reordered source sentences. In this approach, syntactic reorderings can be performed by manually created rules (Collins et al., 2005; Wang et al., 2007a), or by rules extracted automatically from parse trees (Collins et al., 2005; Habash, 2007). For some typical syntactic structures (e.g. DE construction in Chinese), classifiers (Chang et al., 2009b; Du et al., 201"
2010.amta-papers.11,W09-0809,0,0.0221817,"Missing"
2010.amta-papers.11,2007.mtsummit-papers.29,0,0.311358,"rderings and similar conclusions have been drawn from them. These methods can be roughly divided into two main categories (Elming, 2008): the deterministic reordering approach and the non-deterministic reordering approach. For the deterministic approach, syntactic reorderings take place outside the PBSMT system, and the corresponding PBSMT systems only deal with the reordered source sentences. In this approach, syntactic reorderings can be performed by manually created rules (Collins et al., 2005; Wang et al., 2007a), or by rules extracted automatically from parse trees (Collins et al., 2005; Habash, 2007). For some typical syntactic structures (e.g. DE construction in Chinese), classifiers (Chang et al., 2009b; Du et al., 2010) are built to carry out source reorderings. For the non-deterministic approach, both the original and reordered source sentences are fed into the PBSMT decoders, and the decisions are left to the decoders to choose the most appropriate one. (Crego et al., 2007) used syntactic structures to reorder the input into word lattices for N-gram-based Statistical Machine Translation. (Zhang et al., 2007a; Zhang et al., 2007b) employed chunks and POS tags to extract reordering rul"
2010.amta-papers.11,2010.eamt-1.26,1,0.857466,"et al., 2009b) for a Chinese-English task. However, rewriting the source sentence cannot be undone by the decoders (Al-Onaizan et al., 2006), which makes the deterministic approach less flexible than the non-deterministic one. Nevertheless, for the non-deterministic approach, most of the work relies on the syntactic information (cf. parse tree, chunks, POS tags) but never addresses which kind of rules are favoured by the decoders in SMT systems. Accordingly, the final systems might not benefit from many of the reordering rules. In this paper, we adopt the lattice scoring approach proposed in (Jiang et al., 2010) to discover reorderings contained in phrase alignments that are favoured by a baseline PBSMT system. Given this, the central idea of this work is to feed these reorders back to the baseline PBSMT system with optional reordering information on the source-side, and let the decoder choose better reorderings according to our inputs. To accomplish this, syntactic reordering patterns on the source side are used to represent the potential reorderings from the lattice scoring outputs. However, these patterns are also used to transform the baseline inputs into word lattices to carry potential reorderi"
2010.amta-papers.11,N03-1017,0,0.0200072,"Missing"
2010.amta-papers.11,P07-1091,0,0.0196563,"reorderings. For the non-deterministic approach, both the original and reordered source sentences are fed into the PBSMT decoders, and the decisions are left to the decoders to choose the most appropriate one. (Crego et al., 2007) used syntactic structures to reorder the input into word lattices for N-gram-based Statistical Machine Translation. (Zhang et al., 2007a; Zhang et al., 2007b) employed chunks and POS tags to extract reordering rules, language models and reordering models are also used to weight the generated word lattices. Weighted n-best lists generated from rules are also used in (Li et al., 2007) for input into the decoders, while the rules are created from a syntactic parser. On the other hand, using the syntactic rules to score the output word order is adopted by (Elming, 2008; Elming, 2009), both on English-Danish and English-Arabic tasks, which confirmed the effectiveness of syntactic reorderings for distant language pairs. Another related pieces of work applies syntactic reordering information extracted from phrase orientation classifiers as an extra feature in PBSMT systems (Chang et al., 2009b) for a Chinese-English task. However, rewriting the source sentence cannot be undone"
2010.amta-papers.11,ma-2006-champollion,0,0.0129365,"corresponding with {Es , · · · , Es+r−1 }, then weight for Ej is defined as in (8): w(Ej ) = ws−j+1 (Pi ) (1 − α) ∗ preo (Pi ) ∗ Pr (8) k t=1 wt (Pi ) where s &lt;= j &lt; s + r, and wt (Pi ) is the reordering scheme weight defined in formula (5). Here we suppose equal probabilities for all possible reorderings which start with a same lattice node. 5 Experiments The experiments are conducted on a medium-sized corpus for Chinese-English task. The training data is the FBIS corpus, which is a multilingual paragraph-aligned corpus with LDC resource number LDC2003E14, and we use the Champollion aligner (Ma, 2006) to perform sentence alignment to obtain 256,911 sentence pairs. We randomly selected 2,000 pairs for devset and another 2,000 pairs for test set, which is referred as FBIS set in this paper. The rest of the data is used as the training set. Evaluation results are reported on two different sets: FBIS set and the NIST 2008 test data. For FBIS set, only one reference translation is avaible for both devset and testset. For NIST data, we use the NIST 2005 test set which includes 1,082 sentences as the devset, while the NIST 2008 set is used as the test set with 1,357 sentences. In both devset and"
2010.amta-papers.11,P02-1038,0,0.0705871,"Missing"
2010.amta-papers.11,P06-1055,0,0.0826934,"Missing"
2010.amta-papers.11,D07-1077,0,0.13883,"Missing"
2010.amta-papers.11,D07-1078,0,0.377998,"between different language pairs, phrase-based statistical machine translation (PBSMT) systems (Koehn et al., 2003) incorporate two different of methods: 1) learning phrase pairs with different word orders in the source and target sentences; 2) attempting potential target phrase orders during the decoding phase, and penalizing potential phrase orders using both distance-based and lexical reordering models. However, for some language pairs, this model is not powerful enough to capture the word order differences between the source and target sentences. To tackle this problem, previous studies (Wang et al., 2007a; Chang et al., 2009a) showed that syntactic reorderings can benefit state-of-the-art PBSMT systems by handling systematic differences in word order between language pairs. From their conclusions, for the Chinese-English task, syntactic reorderings can greatly improve the performance by explicitly modeling the structural differences between this language pair. Interestingly, lots of work has been reported on syntactic reorderings and similar conclusions have been drawn from them. These methods can be roughly divided into two main categories (Elming, 2008): the deterministic reordering approac"
2010.amta-papers.11,C04-1073,0,0.114555,"Missing"
2010.amta-papers.11,2002.tmi-tutorials.2,0,0.0802603,"Missing"
2010.amta-papers.11,W07-0401,0,0.25359,"Missing"
2010.amta-papers.11,2007.iwslt-1.3,0,0.0586398,"Missing"
2010.amta-papers.11,P02-1040,0,\N,Missing
2010.amta-papers.11,P07-2045,0,\N,Missing
2010.amta-papers.11,2005.iwslt-1.8,0,\N,Missing
2010.amta-papers.11,W09-0436,0,\N,Missing
2010.amta-papers.11,P03-1021,0,\N,Missing
2010.amta-papers.16,W09-0432,0,0.184409,"Missing"
2010.amta-papers.16,eck-etal-2004-language,0,0.25838,"he classifier used in our experiment. Section 4 presents the experimental setup, the data and provides details of the specific experiments carried out. The results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in term"
2010.amta-papers.16,Y09-2027,1,0.845331,"Missing"
2010.amta-papers.16,2005.eamt-1.17,0,0.0567223,"is (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in terms of perplexity reduction and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include translation models. Using unsupervised clustering techniques on the bilingual training data, automatic clusters were created and each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to"
2010.amta-papers.16,2005.eamt-1.19,0,0.298303,"Missing"
2010.amta-papers.16,P07-2045,0,0.0121647,"Missing"
2010.amta-papers.16,W07-0733,0,0.569288,"n and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include translation models. Using unsupervised clustering techniques on the bilingual training data, automatic clusters were created and each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific traini"
2010.amta-papers.16,W02-1405,0,0.521618,"e combined training model. The remainder of the paper is organized as follows. In Section 2, we present previous research in the field of Domain Adaptation in SMT. Section 3 describes details of the classifier used in our experiment. Section 4 presents the experimental setup, the data and provides details of the specific experiments carried out. The results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and"
2010.amta-papers.16,W08-0320,0,0.404527,"d each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific training of SMT models. This effort resulted in significant improvement in domain-dependent translation when compared to domain-independent translation. Bertoldi and Federico’s (2009) experiments utilizing in-domain monolingual resources to improve d"
2010.amta-papers.16,J03-1002,0,0.00407199,"for too short sentences (Papineni et al., 2002). Usually this score reflects the fluency of the translated sentence. On the other hand NIST scores are quite similar to BLEU 4 http://www.openmatrex.org/ scores, but use an arithmetic average rather than a geometric one (Doddington, 2002). NIST weighs more informative n-grams higher than the others. Hence NIST scores reflect the adequacy of the translated sentences. In the context of our work, we report both BLEU and NIST scores to capture different aspects of the quality of a translation. For training, word alignment was performed using Giza++ (Och and Ney, 2003), followed by the creation of the phrase and the re-ordering tables using Moses training (Koehn et al., 2007). 5gram language models were built on the domainspecific training data using the SRILM toolset (Stolcke, 2002). After training, each of the model components was tuned using Minimum Error-Rate Training (MERT) (Och, 2003) on the BLEU metric. This process helps us tune models to domain-specific development sets. Finally the models were tested on both domainspecific as well as combined domain test sets. Since the primary objective of our experiments was to achieve better translation of a mi"
2010.amta-papers.16,P03-1021,0,0.00737576,"the others. Hence NIST scores reflect the adequacy of the translated sentences. In the context of our work, we report both BLEU and NIST scores to capture different aspects of the quality of a translation. For training, word alignment was performed using Giza++ (Och and Ney, 2003), followed by the creation of the phrase and the re-ordering tables using Moses training (Koehn et al., 2007). 5gram language models were built on the domainspecific training data using the SRILM toolset (Stolcke, 2002). After training, each of the model components was tuned using Minimum Error-Rate Training (MERT) (Och, 2003) on the BLEU metric. This process helps us tune models to domain-specific development sets. Finally the models were tested on both domainspecific as well as combined domain test sets. Since the primary objective of our experiments was to achieve better translation of a mix of sentences coming from multiple domains, we tested all our translation models using a combined test set from both domains. Moreover, we also tested the same models using domain-specific test sets to get a clear understanding of the effect of domain-specific data. 4.3 Domain Adaptation Experiments In this section we describ"
2010.amta-papers.16,2007.mtsummit-papers.68,0,0.656756,"o translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific training of SMT models. This effort resulted in significant improvement in domain-dependent translation when compared to domain-independent translation. Bertoldi and Federico’s (2009) experiments utilizing in-domain monolingual resources to improve domain adaptation also achieved considerable improvements. They used domain-specific baseline systems to translate in-domain monolingua"
2010.amta-papers.16,C04-1059,0,0.0344827,"he results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in terms of perplexity reduction and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include"
2010.amta-papers.16,P02-1040,0,\N,Missing
2010.amta-papers.16,2006.iwslt-evaluation.4,1,\N,Missing
2010.amta-papers.16,D07-1054,0,\N,Missing
2010.amta-papers.16,W07-0717,0,\N,Missing
2010.amta-papers.23,J96-1002,0,0.0209051,"a brief overview of HPB. In Section 4 we describe the context-informed features contained in our baseline HPB model. In Section 5 we describe our memory-based classification approach. Section 6 describes experimental set-ups. Section 7 presents the results obtained, and offers a brief qualitative analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work MT research on incorporating contexts into SMT models can be broadly divided into two categories: source-context modelling such as (Stroppa et al., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) b"
2010.amta-papers.23,2009.mtsummit-papers.12,0,0.0121654,"l., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as (Cowan et al., 2006) gener"
2010.amta-papers.23,D09-1022,0,0.0362701,"such as (Stroppa et al., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as"
2010.amta-papers.23,C92-2066,0,0.242831,"α+1 , ..., wα+i ) of a given source phrase α. In our experiments, we consider a context size of 2 (i.e., i := 2). It also includes boundary words (wntstart j and wntend ) of subphrases covered by nonterminals j in the α. Like (Chiang, 2007), we restrict the number of nonterminals to two (i.e., j := 2). The resultant lexical features form a window of size 2(i+j) features. Thus, lexical contextual information (CIlex ) can be described as in (4): Figure 1: CCG and LTAG supertag sequences. In our experiments two kinds of supertags are employed: those from lexicalized tree-adjoining grammar, LTAG (Joshi and Schabes, 1992), and combinatory categorial grammar, CCG (Steedman, 2000). Both the LTAG and the CCG supertag sets were acquired from the WSJ section of the Penn-II Treebank using hand-built extraction rules. Here we use both the LTAG and CCG supertaggers. In LTAG, a lexical item is associated with an elementary tree, while in CCG the supertag constitutes a CCG lexical category with a set of word-to-word dependencies. The two alternative supertag descriptions can be viewed as closely related functional descriptors of words. Like CIpos , we define the contextual information (CIst ) defining supertags as in (6"
2010.amta-papers.23,W06-1628,0,0.0642608,"Missing"
2010.amta-papers.23,J07-2003,0,0.394007,"ssfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also in"
2010.amta-papers.23,N09-1025,0,0.0267371,"focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source context and a language model created from source-side grammatical dependency structures. While their source-side dependency language model does not produce any improvement, the other features seem to be effective in Arabic-to-English and Chinese-to-English translation. Chiang et al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system. In order to limit the the size of their model, they restrict words to being among the 100 most frequently occurring words from the training data; all other words are replaced with a special token. One final paper in this strand of research is that of (He et al., 2008), who despite not mentioning the link between the two pieces of work, show that the low-level source-language features used by (Stroppa et al., 2007) are also of benefi"
2010.amta-papers.23,H05-1097,0,0.0642552,"as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as (Cowan et al., 2006) generally require a redefinition of the traini"
2010.amta-papers.23,P02-1038,0,0.80659,"based SMT model (Chiang, 2007) uses the bilingual phrase pairs of phrase-based SMT (PBSMT) (Koehn et al., 2003) as a starting point to learn hierarchial rules using probabilistic synchronous context-free grammar (PSCFG). The decoding process in the hierarchical phrase-based SMT (HPB) model is based on bottom-up chart parsing (Chiang, 2007). This chart parsing decoder, also known as Hiero, does not require explicit syntactic representation on either side of the phrases in rules. State-of-the-art SMT models (Koehn et al., 2003; Chiang, 2007) can be viewed as log-linear combinations of features (Och and Ney, 2002) that usually comprise translational features and the language model. The translational features typically involved in these models express dependencies between the source and target phrases, but not dependencies between the phrases in the source language themselves, i.e. they do not take into account the contexts of those phrases. Word sense disambiguation (WSD), a task intricately related to MT, typically employs rich contextsensitive features to determine contextually the most likely sense of a polysemous word. Inspired by these context-rich WSD techniques, researchers have tried to integra"
2010.amta-papers.23,W06-1607,0,0.0192106,"e, when supertags are combined with lexical features, the CI is S formed by the union of these features, i.e., CI= CIst CIlex . 5 Memory-Based Disambiguation As Stroppa et al. (2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, (Zens and Ney, 2004) showed that the estimation of phrase translation probabilities using relative frequencies results in overestimation of the probabilities of long phrases. Accordingly, smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context-informed features, since the context is also taken into account, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers able to estimate P(γ|α, CI(α)) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations, matched to a new source phrase to be translated. In this work we use the approximate memory-based classifier IGTree1 (Daelemans and van den Bosch, 2005). IGTree makes a heuristic approximation of knearest neighbour search by storing"
2010.amta-papers.23,P01-1027,0,0.0761219,"Missing"
2010.amta-papers.23,N09-1013,0,0.0447881,"Missing"
2010.amta-papers.23,W07-0719,0,0.0564061,"Missing"
2010.amta-papers.23,2008.eamt-1.17,0,0.0184333,"nse of a polysemous word. Inspired by these context-rich WSD techniques, researchers have tried to integrate various contextual knowledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighb"
2010.amta-papers.23,D09-1023,0,0.0210909,"009a) and grammatical dependency relations (Haque et al., 2009b) have been modelled as useful source context to improve phrase selection in PBSMT. Alternative SMT architectures: Bangalore et al. (2007) propose an SMT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Sh"
2010.amta-papers.23,D09-1008,0,0.0200964,"9) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source context and a language model created from source-side grammatical dependency structures. While their source-side dependency language model does not produce any improvement, the other features seem to be effective in Arabic-to-English and Chinese-to-English translation. Chiang et al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated i"
2010.amta-papers.23,D07-1007,0,0.1589,"t-rich WSD techniques, researchers have tried to integrate various contextual knowledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prov"
2010.amta-papers.23,2007.tmi-papers.28,1,0.703467,"Missing"
2010.amta-papers.23,W04-3250,0,0.0940844,"Missing"
2010.amta-papers.23,N03-1017,0,0.00659903,"contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also influenced by more distant words in the sentence. S"
2010.amta-papers.23,Y09-1019,1,0.821505,"Missing"
2010.amta-papers.23,2009.eamt-1.32,1,0.68043,"owledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a st"
2010.amta-papers.23,N04-1033,0,0.0277164,"a window of size 2(i + j). We compare the effect of supertag features in contrastive experiments using words and POS tags as context in order to observe the relative effects of different features. In addition, we combine the syntactic features with the lexical features. For instance, when supertags are combined with lexical features, the CI is S formed by the union of these features, i.e., CI= CIst CIlex . 5 Memory-Based Disambiguation As Stroppa et al. (2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, (Zens and Ney, 2004) showed that the estimation of phrase translation probabilities using relative frequencies results in overestimation of the probabilities of long phrases. Accordingly, smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context-informed features, since the context is also taken into account, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers able to estimate P(γ|α, CI(α)) by similarity-based reasoning over memorized nearest-neighb"
2010.amta-papers.23,D08-1039,0,0.0238295,"Missing"
2010.amta-papers.23,P07-1020,0,0.0166708,"training procedure; in contrast, Stroppa et al. (2007) introduce new features while retaining the strength of existing stateof-the-art systems. Other recent approaches to integrate state-of-the-art WSD methods into PBSMT (Gim´enez and M`arquez, 2007; Carpuat and Wu, 2007) have met with success as well. Following the work of (Stroppa et al., 2007), rich and complex syntactic structures such as supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b) have been modelled as useful source context to improve phrase selection in PBSMT. Alternative SMT architectures: Bangalore et al. (2007) propose an SMT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phras"
2010.amta-papers.23,P07-1005,0,0.0391009,"MT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source cont"
2010.amta-papers.23,C08-1041,0,0.211341,"rtags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also influenced by more distant words in the sentence. Syntactic contexts that capture long-distance dependencies between words in a sentence can be a useful means to disambiguate among translations. Accordingly, integration of such syntactic contexts could lead to improved translation quality in PBSMT. For instance, Haque et al. (2009a) showed that supertags are more powerful source contexts than neighbouring words and part-of-speech tags to disambiguate a source phrase in PBSMT. Inspired by"
2010.amta-papers.27,J93-2003,0,0.0108662,"in (3): 3.3.3 System-Independent Features (He et al., 2010) use several features that are independent of the translation system, which are useful when a third-party translation service is used or the MT system is simply treated as a black-box: P r(y = 1|x) ≈ PA,B (f ) ≡ 1 1 + exp (Af + B) • Source-Side Language Model Score and Perplexity • Target-Side Language Model Perplexity • The Pseudo-Source Fuzzy Match Score: they translate the output back to obtain a pseudo source sentence. They compute the fuzzy match score between the original source sentence and this pseudo-source • The IBM Model 1 (Brown et al., 1993) scores in both directions 4 Evaluation Methodology We conduct a human evaluation on TM–MT integration with professional post-editors. In this section we introduce the evaluation data we use, the posteditors, the evaluation environment and the questionnaire which we give to the post-editors after they have completed the evaluation. 4.1 Data Our raw data set is an English–French translation memory which consists of 51K sentence pairs of technical translation from Symantec. We randomly selected 43K to train an SMT system and used this system to translate the English side of the remaining 8K sent"
2010.amta-papers.27,D09-1030,0,0.0229881,"ognized as TM outputs by one post-editor, which shows both the potential and the necessity for TM–MT integration. This work can be extended in several ways. First of all, in this paper we concentrated on proprietary data and professional post-editors, according to the major paradigm in the localization industry. However, at the same time this limits the number of annotators we can hire, as well as the types of evaluations we can perform. We can obtain more comprehensive results by experimenting on open-domain data sets, and applying crowd-sourcing technologies such as Amazon Mechanical Turk2 (Callison-Burch, 2009). Secondly, during the evaluation we were able to collect a number of human judgements for training a new translation recommendation system. We plan to train a new recommendation model and to compare the difference with models trained on automatic metric scores, when we have collected more humanannotated data. Finally, this experiment can also be extended by measuring the actual post-editing time instead of the judgement time, which can lead to a more precise approximation of reduced post-editing effort when using translation recommendation to integrate MT outputs into a TM system. Acknowledge"
2010.amta-papers.27,2009.mtsummit-btm.7,0,0.05994,"help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs. In this respect, it is related to (Guerberof, 2009), which compares the post-editing effort required for MT and TM outputs respectively, as well as (Tatsumi, 2009), which studies the correlation between automatic evaluation scores and postediting effort. Our work differs in that this paper measures how the integration of TM and MT systems can help post-editors, not how post-editors perform using separate TM or MT systems. 3 The Translation Recommendation System In this section we briefly review the translation recommendation system presented by (He et al., 2010). They use an SVM binary classifier to predict the relative quality of the SMT outp"
2010.amta-papers.27,P10-1064,1,0.442255,"Missing"
2010.amta-papers.27,2009.mtsummit-papers.8,0,0.042498,"riendly to the TM environment, such as (Specia et al., 2009a) and (Specia et al., 2009b), both of which focus on improving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs. In this respect, it is related to (Guerberof, 2009), which compares the post-editing effort required for MT and TM outputs respectively, as well a"
2010.amta-papers.27,N03-1017,0,0.0141447,"Missing"
2010.amta-papers.27,P07-2045,0,0.0163488,"ving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs. In this respect, it is related to (Guerberof, 2009), which compares the post-editing effort required for MT and TM outputs respectively, as well as (Tatsumi, 2009), which studies the correlation between automatic evaluation scores and postediting effort. Our w"
2010.amta-papers.27,P02-1038,0,0.0595401,"ompleted the evaluation. 4.1 Data Our raw data set is an English–French translation memory which consists of 51K sentence pairs of technical translation from Symantec. We randomly selected 43K to train an SMT system and used this system to translate the English side of the remaining 8K sentence pairs as recommendation candidates. We train SVM translation recommendation models with 4-fold cross validation on these 8K sentence pairs, and randomly select 300 from the cross validation test sets for human evaluation. More specifically, for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the target side of the training data, and Moses (Koehn et al., 2007) to decode. For the translation recommendation model, we output a confidence level using the method in Section 3.2 and all the features in Section 3.3. 4.2 The Post-editors Five professional post-editors help us to complete this study. Four"
2010.amta-papers.27,P03-1021,0,0.0158949,"d 43K to train an SMT system and used this system to translate the English side of the remaining 8K sentence pairs as recommendation candidates. We train SVM translation recommendation models with 4-fold cross validation on these 8K sentence pairs, and randomly select 300 from the cross validation test sets for human evaluation. More specifically, for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the target side of the training data, and Moses (Koehn et al., 2007) to decode. For the translation recommendation model, we output a confidence level using the method in Section 3.2 and all the features in Section 3.3. 4.2 The Post-editors Five professional post-editors help us to complete this study. Four of them are full-time post-editors, and one is a part-time post-editor. All of the editors are hired through the localization vendors of the IT security company and have experien"
2010.amta-papers.27,2009.mtsummit-papers.14,0,0.301696,"several other models that try to combine the merits of TM and MT systems. The first strand is to design MT confidence estimation measures that are friendly to the TM environment, such as (Specia et al., 2009a) and (Specia et al., 2009b), both of which focus on improving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs."
2010.amta-papers.27,2006.amta-papers.25,0,0.192937,"chines (SVMs: (Cortes and Vapnik, 1995)) max-margin binary classifiers, perform Radial Basis Function (RBF) kernel parameter optimization to find the optimal meta-parameters for the classifier, employ posterior probability-based confidence estimation to support user-based tuning for precision and recall, experiment with feature sets involving MT-, TMand system-independent features, and use automatic MT evaluation metrics to simulate post-editing effort. However, the evaluation in (He et al., 2010) suffers from lack of human-annotated data. Instead they use the TER automatic evaluation metric (Snover et al., 2006) to approximate human judgement. Despite the fact that the correlations between automatic evaluation metrics and human judgements are improving, professional post-editors are the ones that hold the final verdict over the quality of MT/TM integration. In order to draw grounded conclusions on the performance of the (He et al., 2010) recommendation framework, it is essential to conduct user studies to show whether or not systems developed using automatic evaluation metrics are confirmed by human judgements. Our experimental results support validation of the approach to approximate post-editing ef"
2010.amta-papers.27,2009.eamt-1.5,0,0.0399007,"f recommendation performance and user behaviour in Sections 5 and 6, respectively. Section 7 concludes and points out avenues for future research. 2 Related Work The translation recommendation system we experiment with is an implementation of the translation recommendation model proposed in (He et al., 2010), which we review in more detail in Section 3. Besides the translation recommendation model, there are several other models that try to combine the merits of TM and MT systems. The first strand is to design MT confidence estimation measures that are friendly to the TM environment, such as (Specia et al., 2009a) and (Specia et al., 2009b), both of which focus on improving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment"
2010.amta-papers.27,2009.mtsummit-papers.16,0,0.0527645,"f recommendation performance and user behaviour in Sections 5 and 6, respectively. Section 7 concludes and points out avenues for future research. 2 Related Work The translation recommendation system we experiment with is an implementation of the translation recommendation model proposed in (He et al., 2010), which we review in more detail in Section 3. Besides the translation recommendation model, there are several other models that try to combine the merits of TM and MT systems. The first strand is to design MT confidence estimation measures that are friendly to the TM environment, such as (Specia et al., 2009a) and (Specia et al., 2009b), both of which focus on improving confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors. The second strand of research focuses on combining TM information into an SMT system, so that the SMT system can produce better translations when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. Moreover, (Koehn and Haddow, 2009) presents a post-editing environment"
2010.amta-papers.27,2009.mtsummit-posters.20,0,0.0706455,"esents a post-editing environment using information from the phrase-based SMT system Moses (Koehn et al., 2007), instead of the fuzzy match information from TMs. Although all these approaches try to tackle the TM–MT integration task from different perspectives, we concentrate on evaluating the method of (He et al., 2010) in this paper. The research presented in this paper focuses on aspects of a user study of post-editors working with MT and TMs. In this respect, it is related to (Guerberof, 2009), which compares the post-editing effort required for MT and TM outputs respectively, as well as (Tatsumi, 2009), which studies the correlation between automatic evaluation scores and postediting effort. Our work differs in that this paper measures how the integration of TM and MT systems can help post-editors, not how post-editors perform using separate TM or MT systems. 3 The Translation Recommendation System In this section we briefly review the translation recommendation system presented by (He et al., 2010). They use an SVM binary classifier to predict the relative quality of the SMT output to make a recommendation. The SVM classifier uses features from the SMT system, the TM and additional linguis"
2010.amta-papers.28,2007.mtsummit-papers.3,0,0.0248798,"l in training a PB-SMT system is to increase the quality of its translations when confronted with unseen data, in this paper we introduce a scoring method which we use to estimate a new feature, which relates to the expected translation quality of the system, and which we use to extend the model of a PB-SMT system. There has been a range of research on the subject of translation quality-based scoring in MT. (Smith and Eisner, 2006) use minimum-risk training to improve on MERT in choosing the appropriate weights for a given set of features. (Liang et al., 2006), (Tillmann and Zhang, 2006) and (Arun and Koehn, 2007) describe methods to introduce a large number of binary features globally trained to increase B LEU (Papineni et al., 2002), although they do not report significant improvements over state-of-the-art PB-SMT systems trained with standard features. A known limitation of MERT is its difficulty to scale to weighting a larger amount of features than those present in a typical PB-SMT system (Och et al., 2004). In another line of research, (Watanabe et al., 2007) and (Chiang et al., 2008) improve this by using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) to estimate a large amou"
2010.amta-papers.28,W05-0909,0,0.0405035,"than 40 words, which left us with 1,083,773 sentences for training. We use the first 500 sentences of dev2006 as a tuning set for MERT. We use test2006 as a development test set, and test2008 as the final test set (each containing 2,000 sentence pairs). We use the 5,000-best translations returned by our decoder to select oracles and perform the scoring (note that (Galron et al., 2009) report obtaining oracles from the 10,000best parse trees of the input sentence). We report our results using a range of evaluation metrics, namely B LEU (Papineni et al., 2002), NIST (Doddington, 2002), Meteor (Banerjee and Lavie, 2005) and the F-Score (Turian et al., 2003). In our discussion, absolute scores for B LEU, Meteor and the F-score are reported as percentages. 4.1 Dealing with unestimated phrase pairs As mentioned in Section 3.4, each sentence will appear in one held-out set. Even though this ensures that all of the training sentences will be decoded in the estimation process, this does not guarantee that every phrase pair will receive a score according to Acc, as a phrase pair needs to occur in an N -best list translating the same source span as an oracle phrase pair in order to receive a score. In fact, out of t"
2010.amta-papers.28,D08-1024,0,0.0452841,"Missing"
2010.amta-papers.28,D09-1039,1,0.795773,"ber of binary features globally trained to increase B LEU (Papineni et al., 2002), although they do not report significant improvements over state-of-the-art PB-SMT systems trained with standard features. A known limitation of MERT is its difficulty to scale to weighting a larger amount of features than those present in a typical PB-SMT system (Och et al., 2004). In another line of research, (Watanabe et al., 2007) and (Chiang et al., 2008) improve this by using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) to estimate a large amount of syntactic and distortion features. (Galron et al., 2009) showed that accuracy-based scoring is a crucial feature when incorporated into the Data-Oriented Translation paradigm (Poutsma, 2000; Hearne and Way, 2003). They introduce editdistance measures to determine the similarity between candidate translation fragments and oracle translation fragments, and allow the system to benefit from knowledge of which fragments are typically involved in derivations of good translations. In this work we build upon this line of research to investigate the effects of accuracy-based scoring specifically for PB-SMT systems. We use a baseline PBSMT system (Koehn et a"
2010.amta-papers.28,2003.mtsummit-papers.22,1,0.802101,"rt PB-SMT systems trained with standard features. A known limitation of MERT is its difficulty to scale to weighting a larger amount of features than those present in a typical PB-SMT system (Och et al., 2004). In another line of research, (Watanabe et al., 2007) and (Chiang et al., 2008) improve this by using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) to estimate a large amount of syntactic and distortion features. (Galron et al., 2009) showed that accuracy-based scoring is a crucial feature when incorporated into the Data-Oriented Translation paradigm (Poutsma, 2000; Hearne and Way, 2003). They introduce editdistance measures to determine the similarity between candidate translation fragments and oracle translation fragments, and allow the system to benefit from knowledge of which fragments are typically involved in derivations of good translations. In this work we build upon this line of research to investigate the effects of accuracy-based scoring specifically for PB-SMT systems. We use a baseline PBSMT system (Koehn et al., 2007) to obtain N -best lists, and then choose oracle translations according to a range of evaluation metrics. We then compare each phrase pair in the N"
2010.amta-papers.28,N03-1017,0,0.0264304,"en selecting oracles (namely B LEU and the F-Score). In addition, we present methods to rescore each model component and provide an indepth analysis of how and why this scoring process works. The remainder of the paper is organised as follows. Section 2 gives a brief introduction to state-ofthe-art PB-SMT and motivates our scoring method, which Section 3 describes. In section 4, our experimental setup is presented and section 5 shows the experimental results and corresponding analysis. We conclude and point out avenues for future research in section 6. 2 Log-linear Phrase-Based SMT In PB-SMT (Koehn et al., 2003), an input sentence f = w1 . . . wn composed of n words is segmented into I phrases f1I . Each source phrase fi in f1I is translated into a corresponding target phrase ei , resulting in a target sentence e = eI1 and an alignment a. For each target phrase ei , this alignment specifies a pair of integers a(ei ) = (l, m), indicating that the phrase ei is translated from the source sentence span wl . . . wm . Target phrases might be reordered. To select among the many phrase translation options and possible input segmentations, we choose the target sentence e that maximises P (e|f), which is model"
2010.amta-papers.28,2005.iwslt-1.8,0,0.119491,"ere each hi (eI1 , f1I ) is a feature function and each λi the corresponding feature weight. Typical features include an n-gram language model over the target translations, and the product of the conditional phrase translation probabilities p(fi |ei ) and p(ei |fi ). These probabilities are estimated using relative frequency over the multiset of phrases extracted from the parallel corpus, and are smoothed by “lexical weighting” features which measure how often were words in a phrase pair aligned in the parallel corpus. State-of-the-art PB-SMT also incorporates lexicalised reordering features (Koehn et al., 2005), which assign a probability to the orientation between a phrase and the previously translated phrase. The modelled orientations are: monotone (a phrase directly follows the previous phrase), swap (a phrase is swapped with the previous phrase) and discontinuous (neither monotone nor swap). Analogous orientations are computed by considering the next phrase. Although the weights λi in (1) are normally estimated by MERT (Och, 2003) to optimise translation quality metrics such as B LEU (Papineni et al., 2002), the estimation of the features themselves does not have any correlation with these trans"
2010.amta-papers.28,P07-2045,0,0.0141708,"al., 2009) showed that accuracy-based scoring is a crucial feature when incorporated into the Data-Oriented Translation paradigm (Poutsma, 2000; Hearne and Way, 2003). They introduce editdistance measures to determine the similarity between candidate translation fragments and oracle translation fragments, and allow the system to benefit from knowledge of which fragments are typically involved in derivations of good translations. In this work we build upon this line of research to investigate the effects of accuracy-based scoring specifically for PB-SMT systems. We use a baseline PBSMT system (Koehn et al., 2007) to obtain N -best lists, and then choose oracle translations according to a range of evaluation metrics. We then compare each phrase pair in the N -best list against phrases present in the oracle translations, and assign a score to each phrase pair according to how similar they are to those oracle phrase pairs. We use this information to incorporate a new feature, which indicates how likely a phrase pair is to contribute to good translations. Obtaining a score for each phrase pair out of the candidate translations for sentences in the training set is reminiscent of estimating phrase counts us"
2010.amta-papers.28,W04-3250,0,0.163148,"Missing"
2010.amta-papers.28,P06-1096,0,0.0559632,"Missing"
2010.amta-papers.28,P02-1038,0,0.0611571,"composed of n words is segmented into I phrases f1I . Each source phrase fi in f1I is translated into a corresponding target phrase ei , resulting in a target sentence e = eI1 and an alignment a. For each target phrase ei , this alignment specifies a pair of integers a(ei ) = (l, m), indicating that the phrase ei is translated from the source sentence span wl . . . wm . Target phrases might be reordered. To select among the many phrase translation options and possible input segmentations, we choose the target sentence e that maximises P (e|f), which is modelled directly by a log-linear model (Och and Ney, 2002) as in (1): P (eI1 |f1I ) = exp( M X λi hi (eI1 , f1I )) (1) i=1 Here each hi (eI1 , f1I ) is a feature function and each λi the corresponding feature weight. Typical features include an n-gram language model over the target translations, and the product of the conditional phrase translation probabilities p(fi |ei ) and p(ei |fi ). These probabilities are estimated using relative frequency over the multiset of phrases extracted from the parallel corpus, and are smoothed by “lexical weighting” features which measure how often were words in a phrase pair aligned in the parallel corpus. State-of-"
2010.amta-papers.28,P03-1021,0,0.0213287,"which measure how often were words in a phrase pair aligned in the parallel corpus. State-of-the-art PB-SMT also incorporates lexicalised reordering features (Koehn et al., 2005), which assign a probability to the orientation between a phrase and the previously translated phrase. The modelled orientations are: monotone (a phrase directly follows the previous phrase), swap (a phrase is swapped with the previous phrase) and discontinuous (neither monotone nor swap). Analogous orientations are computed by considering the next phrase. Although the weights λi in (1) are normally estimated by MERT (Och, 2003) to optimise translation quality metrics such as B LEU (Papineni et al., 2002), the estimation of the features themselves does not have any correlation with these translation quality metrics, given that these probabilities are directly estimated by relative frequency over the parallel corpus. This motivates our work on accuracy-based scoring, which is aimed at incorporating additional features to the PB-SMT model which relate to the contribution that each phrase pair typically brings to the quality of the output translation. 3 Accuracy-Based Scoring As mentioned above, we attempt to incorporat"
2010.amta-papers.28,P02-1040,0,0.103815,"er we introduce a scoring method which we use to estimate a new feature, which relates to the expected translation quality of the system, and which we use to extend the model of a PB-SMT system. There has been a range of research on the subject of translation quality-based scoring in MT. (Smith and Eisner, 2006) use minimum-risk training to improve on MERT in choosing the appropriate weights for a given set of features. (Liang et al., 2006), (Tillmann and Zhang, 2006) and (Arun and Koehn, 2007) describe methods to introduce a large number of binary features globally trained to increase B LEU (Papineni et al., 2002), although they do not report significant improvements over state-of-the-art PB-SMT systems trained with standard features. A known limitation of MERT is its difficulty to scale to weighting a larger amount of features than those present in a typical PB-SMT system (Och et al., 2004). In another line of research, (Watanabe et al., 2007) and (Chiang et al., 2008) improve this by using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) to estimate a large amount of syntactic and distortion features. (Galron et al., 2009) showed that accuracy-based scoring is a crucial feature when"
2010.amta-papers.28,C00-2092,0,0.0372169,"state-of-the-art PB-SMT systems trained with standard features. A known limitation of MERT is its difficulty to scale to weighting a larger amount of features than those present in a typical PB-SMT system (Och et al., 2004). In another line of research, (Watanabe et al., 2007) and (Chiang et al., 2008) improve this by using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) to estimate a large amount of syntactic and distortion features. (Galron et al., 2009) showed that accuracy-based scoring is a crucial feature when incorporated into the Data-Oriented Translation paradigm (Poutsma, 2000; Hearne and Way, 2003). They introduce editdistance measures to determine the similarity between candidate translation fragments and oracle translation fragments, and allow the system to benefit from knowledge of which fragments are typically involved in derivations of good translations. In this work we build upon this line of research to investigate the effects of accuracy-based scoring specifically for PB-SMT systems. We use a baseline PBSMT system (Koehn et al., 2007) to obtain N -best lists, and then choose oracle translations according to a range of evaluation metrics. We then compare ea"
2010.amta-papers.28,P06-2101,0,0.01594,"the individual components themselves only attempt to increase the likelihood of the training corpus, and none of them necessarily correlate with translation quality. Since the ultimate goal in training a PB-SMT system is to increase the quality of its translations when confronted with unseen data, in this paper we introduce a scoring method which we use to estimate a new feature, which relates to the expected translation quality of the system, and which we use to extend the model of a PB-SMT system. There has been a range of research on the subject of translation quality-based scoring in MT. (Smith and Eisner, 2006) use minimum-risk training to improve on MERT in choosing the appropriate weights for a given set of features. (Liang et al., 2006), (Tillmann and Zhang, 2006) and (Arun and Koehn, 2007) describe methods to introduce a large number of binary features globally trained to increase B LEU (Papineni et al., 2002), although they do not report significant improvements over state-of-the-art PB-SMT systems trained with standard features. A known limitation of MERT is its difficulty to scale to weighting a larger amount of features than those present in a typical PB-SMT system (Och et al., 2004). In ano"
2010.amta-papers.28,2006.amta-papers.25,0,0.0254365,"he phrase alignment a used in each candidate translation, a feature most decoders provide. Using this, a function Ω indicating the mapping (as defined by a) between a source sentence span (l, m) and the corresponding set of target phrases in oracle translations O (oracle phrases) is defined in (5): Ω(l, m) = {˜ eo |∃ ˜e ∈ O : e˜o ∈ ˜e ∧ a(˜ eo ) = (l, m)} (5) We experiment with two translation-quality estimators for E, namely B LEU and the F-Score, which the following two subsections describe. In the future we will consider evaluating the effects of additional evaluation metrics, such as TER (Snover et al., 2006). 3.1.1 B LEU The B LEU score (Papineni et al., 2002) computes a geometric mean of the unigram to N -gram precisions between a candidate sentence and a set of references (typically N = 4). If there is not at least one N -gram match, the B LEU score is 0. Since our aim is to use B LEU not at the document level where this phenomenon would be rare, but at the sentence level in Equation (4), this is problematic because in practice B LEU will be 0 for most sentences. We thus follow (Liang et al., 2006) and approximate B LEU by a smoothed version that combines the scores of B LEU for various N , as"
2010.amta-papers.28,P06-1091,0,0.0133391,"quality. Since the ultimate goal in training a PB-SMT system is to increase the quality of its translations when confronted with unseen data, in this paper we introduce a scoring method which we use to estimate a new feature, which relates to the expected translation quality of the system, and which we use to extend the model of a PB-SMT system. There has been a range of research on the subject of translation quality-based scoring in MT. (Smith and Eisner, 2006) use minimum-risk training to improve on MERT in choosing the appropriate weights for a given set of features. (Liang et al., 2006), (Tillmann and Zhang, 2006) and (Arun and Koehn, 2007) describe methods to introduce a large number of binary features globally trained to increase B LEU (Papineni et al., 2002), although they do not report significant improvements over state-of-the-art PB-SMT systems trained with standard features. A known limitation of MERT is its difficulty to scale to weighting a larger amount of features than those present in a typical PB-SMT system (Och et al., 2004). In another line of research, (Watanabe et al., 2007) and (Chiang et al., 2008) improve this by using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 200"
2010.amta-papers.28,2003.mtsummit-papers.51,0,0.334141,"menon would be rare, but at the sentence level in Equation (4), this is problematic because in practice B LEU will be 0 for most sentences. We thus follow (Liang et al., 2006) and approximate B LEU by a smoothed version that combines the scores of B LEU for various N , as in (6): N X B LEUi S BLEU = 24−i+1 (6) i=1 Note that the direct use of document-level approximations of B LEU such as those used in (Watanabe et al., 2007) would be impractical in our approach, as it would introduce dependencies across sentences which would limit parallelisation. 3.1.2 F-Score The General Text Matcher (GTM) (Turian et al., 2003) computes the F-Score between a candidate translation and a reference using the notions of precision and recall. This computation is parameterised by an exponent, which adjusts the weights of longer n-grams in the score. In this work we use the FScore with an exponent of 1.5, which was estimated by evaluating the quality of the oracles obtained on held-out data. 3.2 Similarity Metrics To estimate the function Acc in (3), we need a notion of similarity between the target phrases present in a candidate translation e˜c and the ones present in an oracle translation e˜o . We relate target phrases i"
2010.amta-papers.28,D07-1080,0,0.297758,"ng to improve on MERT in choosing the appropriate weights for a given set of features. (Liang et al., 2006), (Tillmann and Zhang, 2006) and (Arun and Koehn, 2007) describe methods to introduce a large number of binary features globally trained to increase B LEU (Papineni et al., 2002), although they do not report significant improvements over state-of-the-art PB-SMT systems trained with standard features. A known limitation of MERT is its difficulty to scale to weighting a larger amount of features than those present in a typical PB-SMT system (Och et al., 2004). In another line of research, (Watanabe et al., 2007) and (Chiang et al., 2008) improve this by using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) to estimate a large amount of syntactic and distortion features. (Galron et al., 2009) showed that accuracy-based scoring is a crucial feature when incorporated into the Data-Oriented Translation paradigm (Poutsma, 2000; Hearne and Way, 2003). They introduce editdistance measures to determine the similarity between candidate translation fragments and oracle translation fragments, and allow the system to benefit from knowledge of which fragments are typically involved in derivatio"
2010.amta-papers.28,N04-1021,0,\N,Missing
2010.amta-papers.28,2008.iwslt-evaluation.10,0,\N,Missing
2010.amta-papers.9,C08-1005,0,0.0148625,"gainst the “backbone” under a specific alignment metric, such as T ER (Snover et al., 2006), H MM (Matusov et al., 2006), IH MM (He et al., 2008), T ERp (Snover et al., 2009) etc. Synonym matching is the most challenging issue for the hypothesis alignment metric because it has an important impact on alignment accuracy and the final consensus decoding. As a consequence, many hypothesis alignment metrics integrate rich linguistic features to increase the capability of synonym matching. IH MM uses a similarity function to perform synonym matching and it significantly outperforms the T ER method. Ayan et al. (2008) modified T ER to consider substitutions of synonyms using WordNet (Fellbaum, 1980). Snover et al. (2009) extended T ER to T ERp in a similar idea that incorporates the stems and synonym matching (Banerjee and Lavie, 2005) and paraphrase matching (Kauchak and Barzilay, 2006; Zhou et al., 2006) to increase the alignment accuracy. Regarding the T ERp metric, Rosti et al. (2009) firstly used it to increase the hypothesis alignment in the WMT2009 system combination shared task and achieved the best performance in their experiments. Barrault (2010) developed an open source MT system combination usi"
2010.amta-papers.9,W05-0909,0,0.0632791,"nging issue for the hypothesis alignment metric because it has an important impact on alignment accuracy and the final consensus decoding. As a consequence, many hypothesis alignment metrics integrate rich linguistic features to increase the capability of synonym matching. IH MM uses a similarity function to perform synonym matching and it significantly outperforms the T ER method. Ayan et al. (2008) modified T ER to consider substitutions of synonyms using WordNet (Fellbaum, 1980). Snover et al. (2009) extended T ER to T ERp in a similar idea that incorporates the stems and synonym matching (Banerjee and Lavie, 2005) and paraphrase matching (Kauchak and Barzilay, 2006; Zhou et al., 2006) to increase the alignment accuracy. Regarding the T ERp metric, Rosti et al. (2009) firstly used it to increase the hypothesis alignment in the WMT2009 system combination shared task and achieved the best performance in their experiments. Barrault (2010) developed an open source MT system combination using T ERp which is still based on a word-level CN. In this paper, we make good use of the synonyms and paraphrases recognised by the T ERp metric to upgrade our word-level combination framework to the phrase level. Addition"
2010.amta-papers.9,P05-1033,0,0.0317939,"best fluency of the translation. 5 Experimental Settings The experiments are conducted and reported on the NIST 2008 test data. The NIST 2006 test set includes 1,664 sentences and is used as the devset, while the NIST 2008 is used as the test set which contains 1,357 sentences. Each source sentence has 4 references in the two sets. The training data includes 2.5 million pairs of Chinese and English parallel sentences. There are three SMT systems used in our experiments, namely, 1) baseline: Moses (Koehn et al., 2007); 2) R-HPB: our own re-implemented hierarchical phrase-based (R-HPB) system (Chiang, 2005); 3) Moses-chart: a re-implemented HPB in Moses. In order to increase the diversity of MT systems, we also reorder the Chinese sentences using the DE classifier (Chang et al., 2009). Therefore, in our experiments, there are 6 individual systems in all which are trained on the nonreordered and reordered data. The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrize the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (MERT) (Och, 2003). The results of the 6 SMT systems on the NIST 2008 test set are repor"
2010.amta-papers.9,D08-1011,0,0.077168,"our T ERp-based augmented system combination framework achieves significant improvements in terms of BLEU and T ERp scores compared to the state-ofthe-art word-level system combination framework and a T ER-based combination strategy. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion network-based networks have become the stateof-the-art methodology to implement the combination strategy (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; He et al., 2008). A CN is built by aligning a set of translation hypotheses against a reference or “backbone” which is usually generated by a minimum Bayes-risk decoder (MBR) (Kumar and Byrne, 2004). Generally, as with translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN in system combination for SMT is constructed on the word level constrained by the inherent property of the CN. Basically, there are two critical parts to build a word-l"
2010.amta-papers.9,D09-1115,0,0.633161,"n a word-level CN. In this paper, we make good use of the synonyms and paraphrases recognised by the T ERp metric to upgrade our word-level combination framework to the phrase level. Additionally, we develop a weighted MBR using T ERp as the loss function to train system weights for our proposed framework. As to the structure of the CN, the state-ofthe-art form is a word-level network. A CN is essentially a directed acyclic graph which includes weighted arcs and nodes. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Feng et al. (2009) proposed a lattice-based network which allows several words to connect with other several words, i.e., many-to-many mappings. Phrase pair alignment can reduce the risk of producing ungrammatical phrases because of the coherence between the words in a phrase. In this paper, we propose a T ERp-based augmented system combination network in which firstly, T ERp is used as a loss function in a weighted MBR (wMBR) to select a backbone; secondly, T ERp is employed as the hypothesis alignment to carry out the word alignment between the backbone and the set of hypotheses; and then to build a lattice-b"
2010.amta-papers.9,N06-1058,0,0.0241126,"ause it has an important impact on alignment accuracy and the final consensus decoding. As a consequence, many hypothesis alignment metrics integrate rich linguistic features to increase the capability of synonym matching. IH MM uses a similarity function to perform synonym matching and it significantly outperforms the T ER method. Ayan et al. (2008) modified T ER to consider substitutions of synonyms using WordNet (Fellbaum, 1980). Snover et al. (2009) extended T ER to T ERp in a similar idea that incorporates the stems and synonym matching (Banerjee and Lavie, 2005) and paraphrase matching (Kauchak and Barzilay, 2006; Zhou et al., 2006) to increase the alignment accuracy. Regarding the T ERp metric, Rosti et al. (2009) firstly used it to increase the hypothesis alignment in the WMT2009 system combination shared task and achieved the best performance in their experiments. Barrault (2010) developed an open source MT system combination using T ERp which is still based on a word-level CN. In this paper, we make good use of the synonyms and paraphrases recognised by the T ERp metric to upgrade our word-level combination framework to the phrase level. Additionally, we develop a weighted MBR using T ERp as the l"
2010.amta-papers.9,koen-2004-pharaoh,0,0.0406052,"ights. This step ensure that the synonymous/paraphrasal arcs have a higher confidence for the purpose of selecting a more consistent phrase. The two-pass decoding algorithm is described as: • First pass: traverse all the nodes in the lattice and find a path with the maximum probability as the candidate path; • Second pass: trace back along the candidate path and check whether it has any combined arcs. If so, then restore all the combined arcs to a new lattice and decode all the nodes again to generate the final consensus. This step is similar to the N -best generation process in SMT decoding (Koehn, 2004). The purpose of the first pass is to provide a selection preference of synonymous phrases or paraphrases for the decoder which can guarantee the coherence and consistency of the phrases and the context, while the second pass carries out a fair competition between the different synonyms and paraphrases which can guarantee a best fluency of the translation. 5 Experimental Settings The experiments are conducted and reported on the NIST 2008 test data. The NIST 2006 test set includes 1,664 sentences and is used as the devset, while the NIST 2008 is used as the test set which contains 1,357 senten"
2010.amta-papers.9,P07-2045,0,0.00885523,"ries out a fair competition between the different synonyms and paraphrases which can guarantee a best fluency of the translation. 5 Experimental Settings The experiments are conducted and reported on the NIST 2008 test data. The NIST 2006 test set includes 1,664 sentences and is used as the devset, while the NIST 2008 is used as the test set which contains 1,357 sentences. Each source sentence has 4 references in the two sets. The training data includes 2.5 million pairs of Chinese and English parallel sentences. There are three SMT systems used in our experiments, namely, 1) baseline: Moses (Koehn et al., 2007); 2) R-HPB: our own re-implemented hierarchical phrase-based (R-HPB) system (Chiang, 2005); 3) Moses-chart: a re-implemented HPB in Moses. In order to increase the diversity of MT systems, we also reorder the Chinese sentences using the DE classifier (Chang et al., 2009). Therefore, in our experiments, there are 6 individual systems in all which are trained on the nonreordered and reordered data. The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrize the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Trainin"
2010.amta-papers.9,N04-1022,0,0.0628305,"mbination framework and a T ER-based combination strategy. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion network-based networks have become the stateof-the-art methodology to implement the combination strategy (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; He et al., 2008). A CN is built by aligning a set of translation hypotheses against a reference or “backbone” which is usually generated by a minimum Bayes-risk decoder (MBR) (Kumar and Byrne, 2004). Generally, as with translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN in system combination for SMT is constructed on the word level constrained by the inherent property of the CN. Basically, there are two critical parts to build a word-level CN, namely hypothesis alignment and the structure of the CN. Hypothesis alignment involves aligning a set of hypotheses against the “backbone” under a specific alignment metric,"
2010.amta-papers.9,D07-1105,0,0.0140328,"isk Decoding In state-of-the-art MT system combination, MBR decoding plays an important role to select the backbone for the CN. The backbone decides the word orders of the CN and the consensus output. In our framework, we employ T ERp as the Loss Function in MBR to select the backbone as in (2): Eb = arg min E∈Ei Ns ∑ T E R p(Ej , Ei ) (2) j=1 where Ns is the number of systems. Equation (2) indicates an MBR decoder with uniform posterior probabilities. In fact, the uniform posterior distribution only performs robustly when the individual systems have a similar quality and are less correlated (Macherey and Och, 2007). Generally, there are two ways to leverage the robustness of the MBR decoder. One way is the empirical way that filters out the worse or closely relevant individual systems based on some specific metric scores and keeps the better systems with similar quality (Macherey and Och, 2007); the other way is the discriminative way that trains system weights through the discriminative training algorithm. Sim et al. (2007) and Rosti et al. (2007) employed a T ER-based weighted MBR to achieve better results than the uniform distributed MBR. In our T ERp-based method, we also use the second way – system"
2010.amta-papers.9,E06-1005,0,0.224419,"lt.The experiments conducted on the NIST2008 Chinese-to-English test set show that our T ERp-based augmented system combination framework achieves significant improvements in terms of BLEU and T ERp scores compared to the state-ofthe-art word-level system combination framework and a T ER-based combination strategy. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion network-based networks have become the stateof-the-art methodology to implement the combination strategy (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; He et al., 2008). A CN is built by aligning a set of translation hypotheses against a reference or “backbone” which is usually generated by a minimum Bayes-risk decoder (MBR) (Kumar and Byrne, 2004). Generally, as with translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN in system combination for SMT is constructed on the word level constrained by the inherent"
2010.amta-papers.9,P03-1021,0,0.0350267,": our own re-implemented hierarchical phrase-based (R-HPB) system (Chiang, 2005); 3) Moses-chart: a re-implemented HPB in Moses. In order to increase the diversity of MT systems, we also reorder the Chinese sentences using the DE classifier (Chang et al., 2009). Therefore, in our experiments, there are 6 individual systems in all which are trained on the nonreordered and reordered data. The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrize the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (MERT) (Och, 2003). The results of the 6 SMT systems on the NIST 2008 test set are reported in terms of B LEU (Papineni et al., 2002) and T ERp scores and shown in Table 1. In Table 1, “Baseline”, “R-HPB” and “Moseschart” indicate that the systems are trained and tested on non-reordered training data and test set. “+DE” indicates the SMT systems are built and run on a DE-reordered data set. We can see that SYS Baseline Baseline+DE R-HPB R-HPB+DE Moses-chart Moses-chart+DE B LEU 22.42 23.47 20.53 22.36 24.36 24.75 T ERp 63.10 62.89 64.39 63.15 62.58 62.19 Table 1: Individual system results on the reordered and n"
2010.amta-papers.9,J03-1002,0,0.00237258,".5 million pairs of Chinese and English parallel sentences. There are three SMT systems used in our experiments, namely, 1) baseline: Moses (Koehn et al., 2007); 2) R-HPB: our own re-implemented hierarchical phrase-based (R-HPB) system (Chiang, 2005); 3) Moses-chart: a re-implemented HPB in Moses. In order to increase the diversity of MT systems, we also reorder the Chinese sentences using the DE classifier (Chang et al., 2009). Therefore, in our experiments, there are 6 individual systems in all which are trained on the nonreordered and reordered data. The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrize the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (MERT) (Och, 2003). The results of the 6 SMT systems on the NIST 2008 test set are reported in terms of B LEU (Papineni et al., 2002) and T ERp scores and shown in Table 1. In Table 1, “Baseline”, “R-HPB” and “Moseschart” indicate that the systems are trained and tested on non-reordered training data and test set. “+DE” indicates the SMT systems are built and run on a DE-reordered data set. We can see that SYS Baseline Baseline+DE R-HPB R-HPB+DE Moses-"
2010.amta-papers.9,P02-1040,0,0.0792912,"implemented HPB in Moses. In order to increase the diversity of MT systems, we also reorder the Chinese sentences using the DE classifier (Chang et al., 2009). Therefore, in our experiments, there are 6 individual systems in all which are trained on the nonreordered and reordered data. The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrize the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (MERT) (Och, 2003). The results of the 6 SMT systems on the NIST 2008 test set are reported in terms of B LEU (Papineni et al., 2002) and T ERp scores and shown in Table 1. In Table 1, “Baseline”, “R-HPB” and “Moseschart” indicate that the systems are trained and tested on non-reordered training data and test set. “+DE” indicates the SMT systems are built and run on a DE-reordered data set. We can see that SYS Baseline Baseline+DE R-HPB R-HPB+DE Moses-chart Moses-chart+DE B LEU 22.42 23.47 20.53 22.36 24.36 24.75 T ERp 63.10 62.89 64.39 63.15 62.58 62.19 Table 1: Individual system results on the reordered and non-reordered data. the “Moses-chart+DE” is the best individual system. 6 Experimental Results and Analysis In this"
2010.amta-papers.9,W09-0409,0,0.0157426,"hypothesis alignment metrics integrate rich linguistic features to increase the capability of synonym matching. IH MM uses a similarity function to perform synonym matching and it significantly outperforms the T ER method. Ayan et al. (2008) modified T ER to consider substitutions of synonyms using WordNet (Fellbaum, 1980). Snover et al. (2009) extended T ER to T ERp in a similar idea that incorporates the stems and synonym matching (Banerjee and Lavie, 2005) and paraphrase matching (Kauchak and Barzilay, 2006; Zhou et al., 2006) to increase the alignment accuracy. Regarding the T ERp metric, Rosti et al. (2009) firstly used it to increase the hypothesis alignment in the WMT2009 system combination shared task and achieved the best performance in their experiments. Barrault (2010) developed an open source MT system combination using T ERp which is still based on a word-level CN. In this paper, we make good use of the synonyms and paraphrases recognised by the T ERp metric to upgrade our word-level combination framework to the phrase level. Additionally, we develop a weighted MBR using T ERp as the loss function to train system weights for our proposed framework. As to the structure of the CN, the stat"
2010.amta-papers.9,N07-1029,0,0.299724,"2008 Chinese-to-English test set show that our T ERp-based augmented system combination framework achieves significant improvements in terms of BLEU and T ERp scores compared to the state-ofthe-art word-level system combination framework and a T ER-based combination strategy. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion network-based networks have become the stateof-the-art methodology to implement the combination strategy (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; He et al., 2008). A CN is built by aligning a set of translation hypotheses against a reference or “backbone” which is usually generated by a minimum Bayes-risk decoder (MBR) (Kumar and Byrne, 2004). Generally, as with translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN in system combination for SMT is constructed on the word level constrained by the inherent property of the CN. Basically, there"
2010.amta-papers.9,W08-0329,0,0.098165,"Missing"
2010.amta-papers.9,P07-1040,0,0.577621,"2008 Chinese-to-English test set show that our T ERp-based augmented system combination framework achieves significant improvements in terms of BLEU and T ERp scores compared to the state-ofthe-art word-level system combination framework and a T ER-based combination strategy. 1 Introduction In the past several years, multiple system combination has been shown to be helpful in improving translation quality. Recently, confusion network-based networks have become the stateof-the-art methodology to implement the combination strategy (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; He et al., 2008). A CN is built by aligning a set of translation hypotheses against a reference or “backbone” which is usually generated by a minimum Bayes-risk decoder (MBR) (Kumar and Byrne, 2004). Generally, as with translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN in system combination for SMT is constructed on the word level constrained by the inherent property of the CN. Basically, there"
2010.amta-papers.9,2006.amta-papers.25,0,0.398166,"with translation decoding, the CN decoding process also uses a log-linear model, which combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN in system combination for SMT is constructed on the word level constrained by the inherent property of the CN. Basically, there are two critical parts to build a word-level CN, namely hypothesis alignment and the structure of the CN. Hypothesis alignment involves aligning a set of hypotheses against the “backbone” under a specific alignment metric, such as T ER (Snover et al., 2006), H MM (Matusov et al., 2006), IH MM (He et al., 2008), T ERp (Snover et al., 2009) etc. Synonym matching is the most challenging issue for the hypothesis alignment metric because it has an important impact on alignment accuracy and the final consensus decoding. As a consequence, many hypothesis alignment metrics integrate rich linguistic features to increase the capability of synonym matching. IH MM uses a similarity function to perform synonym matching and it significantly outperforms the T ER method. Ayan et al. (2008) modified T ER to consider substitutions of synonyms using WordNet (Fellb"
2010.amta-papers.9,W09-0441,0,0.120516,"hich combines a set of different features, to search for the best path or an N -best list by dynamic programming algorithms. Typically, the dominant CN in system combination for SMT is constructed on the word level constrained by the inherent property of the CN. Basically, there are two critical parts to build a word-level CN, namely hypothesis alignment and the structure of the CN. Hypothesis alignment involves aligning a set of hypotheses against the “backbone” under a specific alignment metric, such as T ER (Snover et al., 2006), H MM (Matusov et al., 2006), IH MM (He et al., 2008), T ERp (Snover et al., 2009) etc. Synonym matching is the most challenging issue for the hypothesis alignment metric because it has an important impact on alignment accuracy and the final consensus decoding. As a consequence, many hypothesis alignment metrics integrate rich linguistic features to increase the capability of synonym matching. IH MM uses a similarity function to perform synonym matching and it significantly outperforms the T ER method. Ayan et al. (2008) modified T ER to consider substitutions of synonyms using WordNet (Fellbaum, 1980). Snover et al. (2009) extended T ER to T ERp in a similar idea that inco"
2010.amta-papers.9,W06-1610,0,0.0218894,"act on alignment accuracy and the final consensus decoding. As a consequence, many hypothesis alignment metrics integrate rich linguistic features to increase the capability of synonym matching. IH MM uses a similarity function to perform synonym matching and it significantly outperforms the T ER method. Ayan et al. (2008) modified T ER to consider substitutions of synonyms using WordNet (Fellbaum, 1980). Snover et al. (2009) extended T ER to T ERp in a similar idea that incorporates the stems and synonym matching (Banerjee and Lavie, 2005) and paraphrase matching (Kauchak and Barzilay, 2006; Zhou et al., 2006) to increase the alignment accuracy. Regarding the T ERp metric, Rosti et al. (2009) firstly used it to increase the hypothesis alignment in the WMT2009 system combination shared task and achieved the best performance in their experiments. Barrault (2010) developed an open source MT system combination using T ERp which is still based on a word-level CN. In this paper, we make good use of the synonyms and paraphrases recognised by the T ERp metric to upgrade our word-level combination framework to the phrase level. Additionally, we develop a weighted MBR using T ERp as the loss function to trai"
2010.amta-papers.9,W09-0436,0,\N,Missing
2010.eamt-1.23,W05-0909,0,0.100253,"Missing"
2010.eamt-1.23,J93-2003,0,0.0162319,"Missing"
2010.eamt-1.23,E06-1032,0,0.0241711,"he proposed three methods and the customised SYSTRAN system In Figure 4, there are 59, 66, 62 and 69 segments which are at least “Good” respectively from “Method 1”, “Method 2”, “Method 3” and the customised SYSTRAN, and 5, 4, 7 and 7 segments respectively from the four systems are declared to be “Poor”. Based only on these numbers, it is difficult to claim which is the best method. However, the interesting finding here is that the differences between these systems evaluated by humans are small while the distinctions evaluated by automatic metrics are huge, especially in terms of BLEU scores (Callison-Burch et al., 2006). From a user’s perspective, multiple segments coud be presented to users in a post-editing environment so that users can select the best segment based on source characteristics. 6 Findings and Analysis Based on these experimental results, we have some interesting findings as well as many open questions about the results, for example, • Q1: how much is the data processed by Method 2 different from that of Method 3? What would happen if we used a different test set with a different TMX version format? • Q2: why is there a huge drop in terms of performance with Method 1 compared to Method 2 and"
2010.eamt-1.23,P05-1033,0,0.0417555,"s, TM systems are a fundamental tool in automatic translation workflows. We argue that there are three main reasons: 1) a TM is easy to build; 2) it complies with industry standards and so is efficient to store, share and re-use; 3) localisation is generally limited to a specific domain using a TM provides a fast and relatively good translation for specific domains. Even though SMT has been significantly developed and the translation quality has been highly improved in recent years from the academic viewpoint (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003; Och and Ney, 2003; Chiang, 2005; Koehn et al., 2007), a lot of work must be done to produce high-quality translations when it is applied into industry application. As a result, SMT has a long way to go to establish itself as an acceptable component in computer-assisted translation (CAT) environments such as a commercial post-editing environment, at least when compared to TM systems or rule-based MT systems. Much research progress has been achieved by combining TM with rule-based or SMT techniques. Roturier (2009) used a hybrid approach combining a customised SYSTRAN system with TM to refine the MT output in Symantec’s local"
2010.eamt-1.23,2009.mtsummit-plenaries.9,0,0.061366,"07), a lot of work must be done to produce high-quality translations when it is applied into industry application. As a result, SMT has a long way to go to establish itself as an acceptable component in computer-assisted translation (CAT) environments such as a commercial post-editing environment, at least when compared to TM systems or rule-based MT systems. Much research progress has been achieved by combining TM with rule-based or SMT techniques. Roturier (2009) used a hybrid approach combining a customised SYSTRAN system with TM to refine the MT output in Symantec’s localisation workflow. DeCamp (2009) and Simard and Isabelle (2009) proposed a way of integrating a phrase-based SMT (PB-SMT) system with TM within a CAT environment. They designed a PB-SMT system which behaves more like the TM component in CAT systems. However, they did not discuss TMX markup issues. In a real world environment, industrial TM data contains TMX tags when exported into this format. During the translation process, especially in a post-editing environment, translations with corresponding markup must be presented to the user or translator for their reference. Therefore, when we adapt SMT systems to TMX data, the fir"
2010.eamt-1.23,N03-1017,0,0.0167006,"anslation. ° In the localisation process, TM systems are a fundamental tool in automatic translation workflows. We argue that there are three main reasons: 1) a TM is easy to build; 2) it complies with industry standards and so is efficient to store, share and re-use; 3) localisation is generally limited to a specific domain using a TM provides a fast and relatively good translation for specific domains. Even though SMT has been significantly developed and the translation quality has been highly improved in recent years from the academic viewpoint (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003; Och and Ney, 2003; Chiang, 2005; Koehn et al., 2007), a lot of work must be done to produce high-quality translations when it is applied into industry application. As a result, SMT has a long way to go to establish itself as an acceptable component in computer-assisted translation (CAT) environments such as a commercial post-editing environment, at least when compared to TM systems or rule-based MT systems. Much research progress has been achieved by combining TM with rule-based or SMT techniques. Roturier (2009) used a hybrid approach combining a customised SYSTRAN system with TM to refine"
2010.eamt-1.23,P07-2045,0,0.0336218,"are a fundamental tool in automatic translation workflows. We argue that there are three main reasons: 1) a TM is easy to build; 2) it complies with industry standards and so is efficient to store, share and re-use; 3) localisation is generally limited to a specific domain using a TM provides a fast and relatively good translation for specific domains. Even though SMT has been significantly developed and the translation quality has been highly improved in recent years from the academic viewpoint (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003; Och and Ney, 2003; Chiang, 2005; Koehn et al., 2007), a lot of work must be done to produce high-quality translations when it is applied into industry application. As a result, SMT has a long way to go to establish itself as an acceptable component in computer-assisted translation (CAT) environments such as a commercial post-editing environment, at least when compared to TM systems or rule-based MT systems. Much research progress has been achieved by combining TM with rule-based or SMT techniques. Roturier (2009) used a hybrid approach combining a customised SYSTRAN system with TM to refine the MT output in Symantec’s localisation workflow. DeC"
2010.eamt-1.23,P02-1038,0,0.0896382,"Missing"
2010.eamt-1.23,J03-1002,0,0.00607702,"localisation process, TM systems are a fundamental tool in automatic translation workflows. We argue that there are three main reasons: 1) a TM is easy to build; 2) it complies with industry standards and so is efficient to store, share and re-use; 3) localisation is generally limited to a specific domain using a TM provides a fast and relatively good translation for specific domains. Even though SMT has been significantly developed and the translation quality has been highly improved in recent years from the academic viewpoint (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003; Och and Ney, 2003; Chiang, 2005; Koehn et al., 2007), a lot of work must be done to produce high-quality translations when it is applied into industry application. As a result, SMT has a long way to go to establish itself as an acceptable component in computer-assisted translation (CAT) environments such as a commercial post-editing environment, at least when compared to TM systems or rule-based MT systems. Much research progress has been achieved by combining TM with rule-based or SMT techniques. Roturier (2009) used a hybrid approach combining a customised SYSTRAN system with TM to refine the MT output in Sy"
2010.eamt-1.23,P02-1040,0,0.0771482,"Missing"
2010.eamt-1.23,C00-2090,0,0.0213326,"pting SMT to Symantec formatted TM data. Three different methods are proposed to handle the Translation Memory eXchange (TMX) markup and a comparative study is carried out between them. Furthermore, we also compare the TMX-based SMT systems with a customised SYSTRAN system through human evaluation and automatic evaluation metrics. The experimental results conducted on the French and English language pair show that the SMT can perform well using TMX as input format either during training or at runtime. 1 Introduction Translation memory (TM) plays an important role in the localisation industry (Planas and Furuse, 2000; Garcia, 2005). TM is an effective way to enable the translation of segments (sentences, paragraphs, or phrases) of documents by searching for similar segments in a database and retrieving the suggested matches with a fuzzy match score. c 2010 European Association for Machine Translation. ° In the localisation process, TM systems are a fundamental tool in automatic translation workflows. We argue that there are three main reasons: 1) a TM is easy to build; 2) it complies with industry standards and so is efficient to store, share and re-use; 3) localisation is generally limited to a specific"
2010.eamt-1.23,2009.mtsummit-papers.14,0,0.073423,"must be done to produce high-quality translations when it is applied into industry application. As a result, SMT has a long way to go to establish itself as an acceptable component in computer-assisted translation (CAT) environments such as a commercial post-editing environment, at least when compared to TM systems or rule-based MT systems. Much research progress has been achieved by combining TM with rule-based or SMT techniques. Roturier (2009) used a hybrid approach combining a customised SYSTRAN system with TM to refine the MT output in Symantec’s localisation workflow. DeCamp (2009) and Simard and Isabelle (2009) proposed a way of integrating a phrase-based SMT (PB-SMT) system with TM within a CAT environment. They designed a PB-SMT system which behaves more like the TM component in CAT systems. However, they did not discuss TMX markup issues. In a real world environment, industrial TM data contains TMX tags when exported into this format. During the translation process, especially in a post-editing environment, translations with corresponding markup must be presented to the user or translator for their reference. Therefore, when we adapt SMT systems to TMX data, the first problem concerns the handlin"
2010.eamt-1.23,2006.amta-papers.25,0,0.0754678,"Missing"
2010.eamt-1.23,P01-1067,0,0.103062,"ssociation for Machine Translation. ° In the localisation process, TM systems are a fundamental tool in automatic translation workflows. We argue that there are three main reasons: 1) a TM is easy to build; 2) it complies with industry standards and so is efficient to store, share and re-use; 3) localisation is generally limited to a specific domain using a TM provides a fast and relatively good translation for specific domains. Even though SMT has been significantly developed and the translation quality has been highly improved in recent years from the academic viewpoint (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003; Och and Ney, 2003; Chiang, 2005; Koehn et al., 2007), a lot of work must be done to produce high-quality translations when it is applied into industry application. As a result, SMT has a long way to go to establish itself as an acceptable component in computer-assisted translation (CAT) environments such as a commercial post-editing environment, at least when compared to TM systems or rule-based MT systems. Much research progress has been achieved by combining TM with rule-based or SMT techniques. Roturier (2009) used a hybrid approach combining a customised SYSTRAN syste"
2010.eamt-1.26,W05-0909,0,0.126005,"Missing"
2010.eamt-1.26,2009.eamt-1.25,0,0.0262113,"Missing"
2010.eamt-1.26,N09-1047,0,0.0152628,"information retrieval methods are utilized to weight the training data to obtain significant improvements over a baseline SMT system. In (Okita, 2009), both wordbased and phrase-based models are trained to decode all the training data, and then the sentence pairs are filtered by various evaluation scores of the decoding results. On the other hand, a lot of work has been done on the learning capabilities of SMT system: by examining system performance under different conditions, (Turchi et al., 2008) argues that SMT systems may not be improved by adding more data in i.i.d ways. Active learning (Haffari et al., 2009) are also introduced to obtain improved performance compared with a random sentence selection scheme. Another work related to this paper introduces a constraint satisfaction approach (Canisiu et al., 2009), which integrates many different solutions to aspects of the output space. It uses an optimized objective function during the decoding of a wordbased SMT system to obtain better translation results. The starting point of this paper is fairly close to that of (Okita, 2009). However, in there, the pure decoding method suffers from the problem that the decoding of the whole corpus takes a fairl"
2010.eamt-1.26,P02-1038,0,0.0807128,"lattice building, and section 6 presents Viterbi decoding on lattices. Experiments and results are carried out in section 7. The conclusion and future work are discussed in section 8. tref as the reference to evaluate translation t∗ , and use the evaluation scores as the criterion for data cleaning. After the removal of sentence pairs with low evaluation scores (or X-gram scores in (Okita, 2009)), a new model is trained on the cleaned corpus. The decoding phase works as follows: denote t as the target sentence for s, and let σs,t be the segmentation of s and t. Following the log-linear model (Och & Ney, 2002), the decoding result t∗ for s is represented as in (1): Y t∗ = arg max Hf (s, t, σ)λf (1) t,σ f ∈F where the set F is a finite set of features and λf are the weights of the feature functions Hf of the aligned source and target sentence pairs. The set of features F consists of a phrase translation model (phrase translation probability, lexical weighting, phrase penalty), a language model, a distancebased reordering model, the word penalty and a lexicalized reordering model (Koehn et al., 2005). For the cleaning stage, various evaluation methods, such as BLEU (Papineni et al., 2002), METEOR (Ba"
2010.eamt-1.26,D07-1103,0,0.0213804,"able that (a possibly large number of) misaligned sentences will be introduced during the automatic sentence alignment phase, some part of the bilingual corpus will not contribute at all to the SMT systems. Another drawback of this paradigm is that it takes a long time to train models on the full corpus. Furthermore, even if the models are ready for use, sometimes they will become too big to fit into memory, and in addition the decoding speed will suffer. To overcome these problems, two possible methods may be useful for improving conventional SMT methods: (i) phrase table pruning techniques (Johnson et al., 2007; Yang et al., 2009), which are carried out on the model side; and (ii) data cleaning methods, which are carried out ab initio on the training data side. Some work has been reported on data cleaning of SMT. In (L¨u et al., 2007), information retrieval methods are utilized to weight the training data to obtain significant improvements over a baseline SMT system. In (Okita, 2009), both wordbased and phrase-based models are trained to decode all the training data, and then the sentence pairs are filtered by various evaluation scores of the decoding results. On the other hand, a lot of work has be"
2010.eamt-1.26,N03-1017,0,0.0998058,"ranslation model, target-side phrase networks are expanded on the lattices and Viterbi searching is used to find approximated decoding results; finally, BLEU score thresholds are used to filter out the low-score sentence pairs for the data cleaning purpose. Our experiments on the FBIS corpus showed improvements of BLEU score from 23.78 to 24.02 in Chinese-English. 1 Introduction To overcome problems of data sparseness, most statistical machine translation (SMT) methods tend to use the largest possible corpora to train the models. Following the word-based (Brown et al., 1993) and phrase-based (Koehn et al., 2003) methods, incorporating as much parallel corpora as c 2010 European Association for Machine Translation. possible normally obtains a better system performance. Even in some SMT evaluation tasks (Zollmann et al., 2008), a huge amount (nearly 10M) of sentence pairs are provided to train the models. However, as the size of the corpus increases dramatically, sentence alignment can only be performed automatically. Since it is inevitable that (a possibly large number of) misaligned sentences will be introduced during the automatic sentence alignment phase, some part of the bilingual corpus will not"
2010.eamt-1.26,P07-2045,0,0.0121897,"considan implicit edge. The new cost is estimated from ered as a semi-cleaned corpus, which is a hard task the history of τ n as in formula (4): for data cleaning. After the sentence alignment, n n cost(τ ) − lmcost(τ ) we have 256,911 sentence pairs as the whole data cost(τ n+1 ) = cost(τ n ) + set. Then 2,000 pairs of development set and 2,000 n +lmcost(τ n+1 ) + word penlaty pairs of test set are selected randomly from the (4) whole data set. After sentence length filtering, the rest of the data set is used as the training set. where lmcost indicates the target language model We use Moses (Koehn et al., 2007) as the basecost of the hypothesis corresponding with the to- line system. The GIZA++ toolkit is used to perken, and word penalty is the same with that in form word alignment and “grow-diag-final” renormal cost. The basic assumption held in formula finement method is adopted (Koehn et al., 2003). (4) is that tokens with better history will generate Phrase extraction is carried out by the method better tokens. of (Zens et al., 2002), which is also used for Formula (4) penalizes tokens via implicit edges. lattice generation in this paper. Minimum error For example, in Figure 3, from node 1 to 8,"
2010.eamt-1.26,D07-1036,0,0.0860883,"Missing"
2010.eamt-1.26,ma-2006-champollion,0,0.089399,"okens are kept using either a probability threshold of the best token or fixed admissible token numbers. 7 Experiments and evaluation 7.1 Experimental settings The experiments are conducted on Chinese-toEnglish. Our experimental data come from FBIS bers of the parenthesized labels of non-implicit corpus, which is a multilingual paragraph aligned edges), reordering scores are calculated by look- corpus with LDC resource number LDC2003E14. ing up source positions of two consecutive tokens. Sentence alignment is carried out by Champollion (2) Implicit-edge cost (line 12 in algorithm 2): aligner (Ma, 2006), which is designed for noisy Suppose token τ n+1 is passed from token τ n via data. In this case, our training data are considan implicit edge. The new cost is estimated from ered as a semi-cleaned corpus, which is a hard task the history of τ n as in formula (4): for data cleaning. After the sentence alignment, n n cost(τ ) − lmcost(τ ) we have 256,911 sentence pairs as the whole data cost(τ n+1 ) = cost(τ n ) + set. Then 2,000 pairs of development set and 2,000 n +lmcost(τ n+1 ) + word penlaty pairs of test set are selected randomly from the (4) whole data set. After sentence length filteri"
2010.eamt-1.26,P09-3009,0,0.708716,"big to fit into memory, and in addition the decoding speed will suffer. To overcome these problems, two possible methods may be useful for improving conventional SMT methods: (i) phrase table pruning techniques (Johnson et al., 2007; Yang et al., 2009), which are carried out on the model side; and (ii) data cleaning methods, which are carried out ab initio on the training data side. Some work has been reported on data cleaning of SMT. In (L¨u et al., 2007), information retrieval methods are utilized to weight the training data to obtain significant improvements over a baseline SMT system. In (Okita, 2009), both wordbased and phrase-based models are trained to decode all the training data, and then the sentence pairs are filtered by various evaluation scores of the decoding results. On the other hand, a lot of work has been done on the learning capabilities of SMT system: by examining system performance under different conditions, (Turchi et al., 2008) argues that SMT systems may not be improved by adding more data in i.i.d ways. Active learning (Haffari et al., 2009) are also introduced to obtain improved performance compared with a random sentence selection scheme. Another work related to thi"
2010.eamt-1.26,P09-2060,0,0.0289665,"Missing"
2010.eamt-1.26,2002.tmi-tutorials.2,0,0.631997,"tences respectively, decode s by the model M to derive its translation t∗ . Then take During the training phase, most phrase-based SMT systems start with the word alignment information, most often using GIZA++,1 an implementation of the IBM Models. Following the heuristic approach proposed by (Och & Ney, 2003), the bidirectional alignments of the sentence pairs are refined by extending the intersection of the alignment points of the two word alignments by the union of the two 1 http://fjoch.com/GIZA++.html word alignments. Finally, the refined word alignments are used to extract phrase pairs (Zens et al., 2002). In this paper, the word alignments are not only used for extracting phrase pairs, but also to reduce the search space in the decoding process for the data cleaning phase. In formula (1), all possible target sentences and all segmentations of both the source and target sentences are considered during the decoding process. However, since word alignment information has already been extracted for each of the sentence pairs in the training corpus, the search space can be approximated by only allowing the possible target sentences and segmentations that can be inferred from the word alignment info"
2010.eamt-1.26,J93-2003,0,\N,Missing
2010.eamt-1.26,P02-1040,0,\N,Missing
2010.eamt-1.26,C08-1144,0,\N,Missing
2010.eamt-1.26,J03-1002,0,\N,Missing
2010.eamt-1.26,2005.iwslt-1.8,0,\N,Missing
2010.eamt-1.26,W08-0305,0,\N,Missing
2010.eamt-1.26,P03-1021,0,\N,Missing
2010.eamt-1.32,W05-0909,0,0.0323282,"al phrases influence the HPB from the aspects of word alignment, phrase extraction and hierarchical phrase generalisation etc. We employ the Stanford DE classifier to preprocess both the training and test data by explicitly labeling d (DE) constructions, as well as reordering phrases. Then we re-train the word alignment using GIZA++ (Och and Ney, 2003) and build a reordered initial phrase table and a hierarchical phrase table. The experimental results within a re-implemented HPB system show significant improvements on NIST 2008 evaluation data in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The remainder of this paper is organised as follows. In section 2, we introduce the phenomenon of word order errors caused by the DE construction. Section 3 studies the reordering mechanism of hierarchical phrases in HPB. In Section 4, we describe 5 different classes of DE construction in Chinese and the algorithm of the Stanford DE classifier. In Section 5, the experiments conducted on NIST 2008 Chinese-to-English evaluation data are reported. Meanwhile, in Section 6, an in-depth analysis on how the syntactic DE reordering affects HPB is carried out. Sec"
2010.eamt-1.32,W09-0436,0,0.224254,"Missing"
2010.eamt-1.32,P05-1033,0,0.695557,"ion, such that it can could be translated in many different ways. (Chang et al., 2009) extended the work of (Wang et al., 2007) and characterised the DE structures into 5 classes based on their behaviour. We give a detailed description of Chang’s DE classifier in Section 4. 3 HPB-style Reordering The hierarchical phrases not only have a powerful generalisation ability, but also a strong reordering capability. The idea of presenting hierarchical phrases is to learn reordering of phrases in the same way that the phrases are good for learning reordering of words, cf. Figure 2 as an illustration (Chiang, 2005). In Figure 2, there are three hierarchical phrase pairs (lexicalized synchronous grammar rule) which are related to syntactic reordering (Chiang, 2005), namely, • <yu X1 you X2 , have X2 with X1 > X1 and X2 are placeholders for sub-phrases (See Figure 2). This rule shows that the Chinese PPs almost always modify VPs on the left, whereas English PPs usually modify VPs on the right. It can be found that this rule is a phrase-reordering rule which generalises the  Aozhou        shi yu you DE guojia zhiyi Beihan bangjiao shaoshu Australia is with North Korea have diplomatic relations t"
2010.eamt-1.32,P05-1066,0,0.264904,"tter match the word order of English. The annotated and reordered training data and test data are applied to a re-implemented HPB system and the impact of the DE construction is examined. The experiments are conducted on the NIST 2008 evaluation data and experimental results show that the BLEU and METEOR scores are significantly improved by 1.83/8.91 and 1.17/2.73 absolute/relative points respectively. 1 Introduction Syntactic structure-based reordering has been shown to be significantly helpful for handling word order issues in phrase-based machine translation (PB-SMT) (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Wang et al., 2007; Elming, 2008; Chang et al., 2009). Generally, PB-SMT has an independent reordering model because the phrases themselves in PB-SMT do not have an ability to perform word reordering. However, as regards the hierarchical PB-SMT (HPB) (Chiang, c 2010 European Association for Machine Translation. ° 2005), it possesses an inherent word reordering capability because each rule is a hierarchical structure which contains the sub-phrases. The order of a sub-phrase pair in a source–target hierarchial phrase pair is decided by the positions of nonterminals when generat"
2010.eamt-1.32,W08-0406,0,0.31908,"ordered training data and test data are applied to a re-implemented HPB system and the impact of the DE construction is examined. The experiments are conducted on the NIST 2008 evaluation data and experimental results show that the BLEU and METEOR scores are significantly improved by 1.83/8.91 and 1.17/2.73 absolute/relative points respectively. 1 Introduction Syntactic structure-based reordering has been shown to be significantly helpful for handling word order issues in phrase-based machine translation (PB-SMT) (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Wang et al., 2007; Elming, 2008; Chang et al., 2009). Generally, PB-SMT has an independent reordering model because the phrases themselves in PB-SMT do not have an ability to perform word reordering. However, as regards the hierarchical PB-SMT (HPB) (Chiang, c 2010 European Association for Machine Translation. ° 2005), it possesses an inherent word reordering capability because each rule is a hierarchical structure which contains the sub-phrases. The order of a sub-phrase pair in a source–target hierarchial phrase pair is decided by the positions of nonterminals when generating the hierarchical phrase based on the word alig"
2010.eamt-1.32,P07-2045,0,0.0116426,"n Stanford DE Classifier from (Chang et al., 2009) 5 Syntactic Reordering for HPB System In this section, we firstly perform experiments using the DE annotated and reordered data into the HPB system to verify whether it works or not; secondly we give an in-depth analysis as to how the DE construction affects the HPB system. Meanwhile, we also test the DE classified and reordered data into a phrase-based system to verify whether this approach would have a consistent improvement if it were applied to different types of MT systems. 5.1 Experimental Settings For our MT experiments, we used Moses (Koehn et al., 2007) as the phrase-based system and employed a re-implementation of HPB (Chiang, 2005) as our hierarchical phrase-based system1 . The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (MERT) (Och, 2003). The training data contains 2,159,232 sentence pairs, including the HK parallel corpus, ISI parallel data, UN data and other news data. The 5-gram language model is trained on the English part of the parallel training data. The development set (devset)"
2010.eamt-1.32,P03-1056,0,0.06714,"The training data contains 2,159,232 sentence pairs, including the HK parallel corpus, ISI parallel data, UN data and other news data. The 5-gram language model is trained on the English part of the parallel training data. The development set (devset) is the NIST MT2006 test set which contains 1,664 sentences. The test set is the NIST MT2008 “current” test set which has 1,357 sentences from two different domains, namely newswire and webdata translation genres. All the dev and test sets have 4 references per source sentence. To run the DE classifier, we firstly use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data and the devset and test set. 1 Our re-implemented HPB doesn’t work better than Moses. In future, we will use Moses Chart decoder that is a HPB as well to re-do the experiments. 5.2 Statistics of 5-class DE Annotation For the DE-annotated MT experiments, after we parse the training data and the devset and the test set, we use the DE classifier to annotate the DE constructions in NPs in all of the parsed data. The 5 classes in section 4.1 are represented by dAB , dAsB , dBprepA , drelc and dAprepB to replace the original d (DE) character. Once t"
2010.eamt-1.32,P07-1091,0,0.243016,"der of English. The annotated and reordered training data and test data are applied to a re-implemented HPB system and the impact of the DE construction is examined. The experiments are conducted on the NIST 2008 evaluation data and experimental results show that the BLEU and METEOR scores are significantly improved by 1.83/8.91 and 1.17/2.73 absolute/relative points respectively. 1 Introduction Syntactic structure-based reordering has been shown to be significantly helpful for handling word order issues in phrase-based machine translation (PB-SMT) (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Wang et al., 2007; Elming, 2008; Chang et al., 2009). Generally, PB-SMT has an independent reordering model because the phrases themselves in PB-SMT do not have an ability to perform word reordering. However, as regards the hierarchical PB-SMT (HPB) (Chiang, c 2010 European Association for Machine Translation. ° 2005), it possesses an inherent word reordering capability because each rule is a hierarchical structure which contains the sub-phrases. The order of a sub-phrase pair in a source–target hierarchial phrase pair is decided by the positions of nonterminals when generating the hierarchi"
2010.eamt-1.32,P03-1021,0,0.0458397,"classified and reordered data into a phrase-based system to verify whether this approach would have a consistent improvement if it were applied to different types of MT systems. 5.1 Experimental Settings For our MT experiments, we used Moses (Koehn et al., 2007) as the phrase-based system and employed a re-implementation of HPB (Chiang, 2005) as our hierarchical phrase-based system1 . The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (MERT) (Och, 2003). The training data contains 2,159,232 sentence pairs, including the HK parallel corpus, ISI parallel data, UN data and other news data. The 5-gram language model is trained on the English part of the parallel training data. The development set (devset) is the NIST MT2006 test set which contains 1,664 sentences. The test set is the NIST MT2008 “current” test set which has 1,357 sentences from two different domains, namely newswire and webdata translation genres. All the dev and test sets have 4 references per source sentence. To run the DE classifier, we firstly use the Stanford Chinese parser"
2010.eamt-1.32,J03-1002,0,0.0133545,"lity. Consequently, the main contributions of this paper are, • applying the DE syntactic reordering approach into the HPB system; • exploring how hierarchical phrases perform word reordering, together with any deficiencies therein; • examining how the DE structural phrases influence the HPB from the aspects of word alignment, phrase extraction and hierarchical phrase generalisation etc. We employ the Stanford DE classifier to preprocess both the training and test data by explicitly labeling d (DE) constructions, as well as reordering phrases. Then we re-train the word alignment using GIZA++ (Och and Ney, 2003) and build a reordered initial phrase table and a hierarchical phrase table. The experimental results within a re-implemented HPB system show significant improvements on NIST 2008 evaluation data in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The remainder of this paper is organised as follows. In section 2, we introduce the phenomenon of word order errors caused by the DE construction. Section 3 studies the reordering mechanism of hierarchical phrases in HPB. In Section 4, we describe 5 different classes of DE construction in"
2010.eamt-1.32,P02-1040,0,0.0839665,"• examining how the DE structural phrases influence the HPB from the aspects of word alignment, phrase extraction and hierarchical phrase generalisation etc. We employ the Stanford DE classifier to preprocess both the training and test data by explicitly labeling d (DE) constructions, as well as reordering phrases. Then we re-train the word alignment using GIZA++ (Och and Ney, 2003) and build a reordered initial phrase table and a hierarchical phrase table. The experimental results within a re-implemented HPB system show significant improvements on NIST 2008 evaluation data in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The remainder of this paper is organised as follows. In section 2, we introduce the phenomenon of word order errors caused by the DE construction. Section 3 studies the reordering mechanism of hierarchical phrases in HPB. In Section 4, we describe 5 different classes of DE construction in Chinese and the algorithm of the Stanford DE classifier. In Section 5, the experiments conducted on NIST 2008 Chinese-to-English evaluation data are reported. Meanwhile, in Section 6, an in-depth analysis on how the syntactic DE reorder"
2010.eamt-1.32,2006.amta-papers.25,0,0.0160354,"he aspects of word alignment, phrase extraction and hierarchical phrase generalisation etc. We employ the Stanford DE classifier to preprocess both the training and test data by explicitly labeling d (DE) constructions, as well as reordering phrases. Then we re-train the word alignment using GIZA++ (Och and Ney, 2003) and build a reordered initial phrase table and a hierarchical phrase table. The experimental results within a re-implemented HPB system show significant improvements on NIST 2008 evaluation data in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) scores. The remainder of this paper is organised as follows. In section 2, we introduce the phenomenon of word order errors caused by the DE construction. Section 3 studies the reordering mechanism of hierarchical phrases in HPB. In Section 4, we describe 5 different classes of DE construction in Chinese and the algorithm of the Stanford DE classifier. In Section 5, the experiments conducted on NIST 2008 Chinese-to-English evaluation data are reported. Meanwhile, in Section 6, an in-depth analysis on how the syntactic DE reordering affects HPB is carried out. Section 7 concludes and gives ave"
2010.eamt-1.32,D07-1077,0,0.296856,"he annotated and reordered training data and test data are applied to a re-implemented HPB system and the impact of the DE construction is examined. The experiments are conducted on the NIST 2008 evaluation data and experimental results show that the BLEU and METEOR scores are significantly improved by 1.83/8.91 and 1.17/2.73 absolute/relative points respectively. 1 Introduction Syntactic structure-based reordering has been shown to be significantly helpful for handling word order issues in phrase-based machine translation (PB-SMT) (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Wang et al., 2007; Elming, 2008; Chang et al., 2009). Generally, PB-SMT has an independent reordering model because the phrases themselves in PB-SMT do not have an ability to perform word reordering. However, as regards the hierarchical PB-SMT (HPB) (Chiang, c 2010 European Association for Machine Translation. ° 2005), it possesses an inherent word reordering capability because each rule is a hierarchical structure which contains the sub-phrases. The order of a sub-phrase pair in a source–target hierarchial phrase pair is decided by the positions of nonterminals when generating the hierarchical phrase based on"
2010.eamt-1.32,C04-1073,0,0.375284,"e Chinese sentences better match the word order of English. The annotated and reordered training data and test data are applied to a re-implemented HPB system and the impact of the DE construction is examined. The experiments are conducted on the NIST 2008 evaluation data and experimental results show that the BLEU and METEOR scores are significantly improved by 1.83/8.91 and 1.17/2.73 absolute/relative points respectively. 1 Introduction Syntactic structure-based reordering has been shown to be significantly helpful for handling word order issues in phrase-based machine translation (PB-SMT) (Xia and McCord, 2004; Collins et al., 2005; Li et al., 2007; Wang et al., 2007; Elming, 2008; Chang et al., 2009). Generally, PB-SMT has an independent reordering model because the phrases themselves in PB-SMT do not have an ability to perform word reordering. However, as regards the hierarchical PB-SMT (HPB) (Chiang, c 2010 European Association for Machine Translation. ° 2005), it possesses an inherent word reordering capability because each rule is a hierarchical structure which contains the sub-phrases. The order of a sub-phrase pair in a source–target hierarchial phrase pair is decided by the positions of non"
2010.eamt-1.32,C08-1027,0,\N,Missing
2010.eamt-1.7,P06-1002,0,0.0598936,"(Lambert et al., 2007). In this paper we instead try to discover which alignment characteristics improve or worsen translation quality by analysing the word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 200"
2010.eamt-1.7,P07-2045,0,0.0128199,"Missing"
2010.eamt-1.7,2008.eamt-1.15,1,0.814014,"untranslated words (words present in the training corpus but not translated) PB BLEU score 3 Experimental Setup Our aim is to obtain alignments optimised according to both an intrinsic and an extrinsic criterion. For each criterion, the optimisation consists of maximising a function of the alignment system parameters: F-score (intrinsic criterion), and BLEU score (extrinsic criterion). We use a discriminative alignment system (Moore, 2005) because of its flexibility. First we describe this aligner, and then the optimisation procedure. 3.1 Discriminative Alignment System This alignment system (Lambert and Banchs, 2008) implements a log-linear combination of N feature functions which are calculated at the sentence pair level. The alignment is performed in two passes. First pass features include word association models based on IBM model 1 probabilities (Brown et al., 1993), an unlinked word model proportional to the IBM model 1 NULL link probability, a feature counting the number of links in the hypothesis, distortion models, etc. In the second alignment pass, the association score model with IBM1 probabilities and the unlinked model are substituted by two improved models benefiting from the first-pass links"
2010.eamt-1.7,N07-2022,1,0.832994,"alignment characteristics that are correlated with BLEU score, we give alignment hints to improve BLEU score using a phrase-based SMT system and different types of corpus. 1 Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) build their translation models from word alignments trained in a previous stage. Many papers have shown that intrinsic alignment quality is poorly correlated with MT quality (for example (Vilar et al., 2006)). Accordingly, some research has attempted to tune the alignment directly according to specific MT evaluation metrics (Lambert et al., 2007). In this paper we instead try to discover which alignment characteristics improve or worsen translation quality by analysing the word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; C"
2010.eamt-1.7,2009.mtsummit-posters.12,1,0.171962,"uced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 2002)). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which ar"
2010.eamt-1.7,J06-4004,1,0.898484,"Missing"
2010.eamt-1.7,H05-1011,0,0.0612816,"of a cluster, in the source or target side For each system we calculated the value for the following alignment and translation quantities: Translation pb notr Number of untranslated words (words present in the training corpus but not translated) PB BLEU score 3 Experimental Setup Our aim is to obtain alignments optimised according to both an intrinsic and an extrinsic criterion. For each criterion, the optimisation consists of maximising a function of the alignment system parameters: F-score (intrinsic criterion), and BLEU score (extrinsic criterion). We use a discriminative alignment system (Moore, 2005) because of its flexibility. First we describe this aligner, and then the optimisation procedure. 3.1 Discriminative Alignment System This alignment system (Lambert and Banchs, 2008) implements a log-linear combination of N feature functions which are calculated at the sentence pair level. The alignment is performed in two passes. First pass features include word association models based on IBM model 1 probabilities (Brown et al., 1993), an unlinked word model proportional to the IBM model 1 NULL link probability, a feature counting the number of links in the hypothesis, distortion models, etc"
2010.eamt-1.7,J93-2003,0,0.0146222,"ximising a function of the alignment system parameters: F-score (intrinsic criterion), and BLEU score (extrinsic criterion). We use a discriminative alignment system (Moore, 2005) because of its flexibility. First we describe this aligner, and then the optimisation procedure. 3.1 Discriminative Alignment System This alignment system (Lambert and Banchs, 2008) implements a log-linear combination of N feature functions which are calculated at the sentence pair level. The alignment is performed in two passes. First pass features include word association models based on IBM model 1 probabilities (Brown et al., 1993), an unlinked word model proportional to the IBM model 1 NULL link probability, a feature counting the number of links in the hypothesis, distortion models, etc. In the second alignment pass, the association score model with IBM1 probabilities and the unlinked model are substituted by two improved models benefiting from the first-pass links: an association score model with relative link probabilities, and source and target fertility models giving the probability for a given word to have one, two, three or four or more links. The best hypothesis is the one with best score for the weighted sum o"
2010.eamt-1.7,J03-1002,0,0.0081337,"ch (Crego and Mari˜no, 2007). In order to limit the error introduced by MERT, we ran 4 MERT instances, each with a different random seed. We then either consider the average of the 4 values, or take the 4 values into account in the statistical analysis of the results. Table 2: BLEU score using different alignment sets on the Spanish–English test data and Chinese– English test data 4 4.2 P = 4.1 Results and Statistical Analysis Translation Results We produced 10 alignment sets in total obtained using different methods. This includes 3 baseline sets, corresponding to combinations of the Giza++ (Och and Ney, 2003) source–target and target–source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003). 6 sets were produced with the optimum weights of the discriminative aligner (Section 3.2) resulting from optimisations according to F-score, to the phrasebased system BLEU score and to the n-gramTable 2 shows the performance of the phrasebased SMT system using the 10 different alignments described above. The optimisation procedure was effective for this system. The best systems built from discriminative alignments were indeed those optimise"
2010.eamt-1.7,P02-1040,0,0.0784161,"(Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 2002)). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which are helpful in translation, they should not depend on the specific aligner used. However, they could depend on parameters such as the type of MT system, the language pair, or the corpus size or type. In this way we can study more systematically how the considered characteristics depend on these parameters. We report results for the Moses phrasebased SMT system (Koehn et al., 2007). We undertook this comparison on two different"
2010.eamt-1.7,J07-3002,0,0.322306,"ch alignment characteristics improve or worsen translation quality by analysing the word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 2002)). In this way, we can investigate for any alignment characteristic ho"
2010.eamt-1.7,2009.mtsummit-papers.5,0,0.234221,"e word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the investigation of alignment characteristics that benefit MT. These characteristics include alignment precision and recall (Ayan and Dorr, 2006; Chen and Federico, 2006; Mari˜no et al., 2006; Fraser and Marcu, 2007), longdistance links (Vilar et al., 2006), unlinked words (Guzman et al., 2009; Lambert et al., 2009), etc. In most of the related papers some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. In this work, we start from an initial alignment and tune it directly according to an intrinsic alignment quality metric (F-score, see Section 3.4) and according to an extrinsic translation quality metric (BLEU score (Papineni et al., 2002)). In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment c"
2010.eamt-1.7,N03-1017,0,0.0201224,"consider the average of the 4 values, or take the 4 values into account in the statistical analysis of the results. Table 2: BLEU score using different alignment sets on the Spanish–English test data and Chinese– English test data 4 4.2 P = 4.1 Results and Statistical Analysis Translation Results We produced 10 alignment sets in total obtained using different methods. This includes 3 baseline sets, corresponding to combinations of the Giza++ (Och and Ney, 2003) source–target and target–source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003). 6 sets were produced with the optimum weights of the discriminative aligner (Section 3.2) resulting from optimisations according to F-score, to the phrasebased system BLEU score and to the n-gramTable 2 shows the performance of the phrasebased SMT system using the 10 different alignments described above. The optimisation procedure was effective for this system. The best systems built from discriminative alignments were indeed those optimised with the phrase-based BLEU score as the objective function. When the alignment weights were tuned on the corresponding training corpus (all tasks except"
2010.eamt-1.7,takezawa-etal-2002-toward,0,0.0817791,"Missing"
2010.eamt-1.7,2006.iwslt-papers.7,0,0.017326,"phrasebased SMT system on Chinese-to-English IWSLT data, and Spanish-to-English European Parliament data. With a statistical analysis into alignment characteristics that are correlated with BLEU score, we give alignment hints to improve BLEU score using a phrase-based SMT system and different types of corpus. 1 Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) build their translation models from word alignments trained in a previous stage. Many papers have shown that intrinsic alignment quality is poorly correlated with MT quality (for example (Vilar et al., 2006)). Accordingly, some research has attempted to tune the alignment directly according to specific MT evaluation metrics (Lambert et al., 2007). In this paper we instead try to discover which alignment characteristics improve or worsen translation quality by analysing the word alignment produced by c 2010 European Association for Machine Translation. the alignment model with different tuning criteria. The findings can potentially benefit our understanding of existing SMT systems as well as designing novel word alignment models. A considerable amount of research effort has been devoted to the inv"
2010.eamt-1.7,J00-2004,0,\N,Missing
2010.iwslt-papers.1,J99-2004,0,\N,Missing
2010.iwslt-papers.1,P05-1033,0,\N,Missing
2010.iwslt-papers.1,N03-1017,0,\N,Missing
2010.iwslt-papers.1,W06-3119,0,\N,Missing
2010.iwslt-papers.1,W07-0702,0,\N,Missing
2011.eamt-1.11,P07-2045,0,0.00648783,"Missing"
2011.eamt-1.11,N06-1014,0,0.0304209,"ents. It generates only 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the num"
2011.eamt-1.11,W96-0201,0,0.0261497,"flows due to the common interface designed within the project. Table 1 describes the mandatory parameters of the interface shared across all the webservices. In addition, a webservice might accept optional parameters that allow to exploit specific functionality of the aligner wrapped by that webservice. Name source language source corpus source language target corpus 2.1.2 GMA GMA – Geometric Mapping and Alignment (Argyle et al., 2004)5 – is an implementation of the Smooth Injective Map Recognizer (Melamed, 1997) algorithm for mapping bitext correspondence and the Geometric Segment Alignment (Melamed, 1996) post-processor for converting general bitext maps to monotonic segment alignments. The tool employs word correspondences, cognates, as well as information from bilingual dictionaries. This tool accepts a pair of parameters for the source and target corpus. Apart from that, it needs a parameter pointing to a configuration file, which contains several parameters, including language-dependent lists of stop words. The webservice offers two parameters for the source and target languages; these denote a language pair, which is internally assigned a configuration file. GMA provides configuration fil"
2011.eamt-1.11,P97-1063,0,0.0245084,"for a range of widely-used state-of-the-art aligners which can be easily exchanged in user workflows due to the common interface designed within the project. Table 1 describes the mandatory parameters of the interface shared across all the webservices. In addition, a webservice might accept optional parameters that allow to exploit specific functionality of the aligner wrapped by that webservice. Name source language source corpus source language target corpus 2.1.2 GMA GMA – Geometric Mapping and Alignment (Argyle et al., 2004)5 – is an implementation of the Smooth Injective Map Recognizer (Melamed, 1997) algorithm for mapping bitext correspondence and the Geometric Segment Alignment (Melamed, 1996) post-processor for converting general bitext maps to monotonic segment alignments. The tool employs word correspondences, cognates, as well as information from bilingual dictionaries. This tool accepts a pair of parameters for the source and target corpus. Apart from that, it needs a parameter pointing to a configuration file, which contains several parameters, including language-dependent lists of stop words. The webservice offers two parameters for the source and target languages; these denote a"
2011.eamt-1.11,moore-2002-fast,0,0.0299673,"This tool requires three parameters (filenames for the source corpus, target corpus and bilingual dictionary, although the dictionary file can be empty). As a webservice, the first two parameters are mandatory, together with the source and target languages. The bilingual dictionary file is optional, if none is provided the webservice will create and use an empty file. Hunalign also provides a set of optional parameters. Some of them are offered by the webservice (bisent, cautious and text), while two of them are activated internally (realign and 4 2.1.3 BSA BSA – Bilingual Sentence Aligner7 (Moore, 2002) – is a three-step hybrid approach. First, sentence-length based alignment is performed; second, statistical word alignment model is trained on the high probability aligned sentences and third, all sentences are realigned based on the word alignments. It generates only 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://"
2011.eamt-1.11,P07-1003,0,0.0126663,"nly 1:1 alignments. This tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the number of iterations to run"
2011.eamt-1.11,J03-1002,0,0.00596422,"Missing"
2011.eamt-1.11,P91-1023,0,0.187551,"d for English, French, German, Spanish and Italian have been obtained from Universit´e de Neuchˆatel.6 Type 2 character ISO code text 2 character ISO code text Table 1: Shared mandatory parameters. Webservices created for sentential aligners (Hunalign, GMA and BSA) are covered in section 2.1, while section 2.2 deals with sub-sentential aligners (GIZA++, BerkeleyAligner and OpenMaTrEx chunk aligner). 2.1 2.1.1 Sentential alignment Hunalign Hunalign (Varga et al., 2005)4 can work in two modes. If a bilingual dictionary is available, this information is combined with sentence-length information (Gale and Church, 1991) and used to identify sentence alignment. In the absence of a bilingual dictionary, it first identifies the alignment using sentence-length information only, then builds an automatic dictionary based on this alignment and finally realigns the text using this dictionary. This tool requires three parameters (filenames for the source corpus, target corpus and bilingual dictionary, although the dictionary file can be empty). As a webservice, the first two parameters are mandatory, together with the source and target languages. The bilingual dictionary file is optional, if none is provided the webs"
2011.eamt-1.11,P09-1104,0,0.0224796,"tool takes three parameters (filenames for the source corpus, target corpus and an alignment probability threshold). All of them are offered by the webservice developed. The threshold is set to 0.5 by default. The output of this tool was altered by modifying the script filter-final-aligned-sents.pl 5 http://nlp.cs.nyu.edu/GMA/ http://members.unine.ch/jacques.savoy/ clef/index.html 7 http://research.microsoft. com/en-us/downloads/ aafd5dcf-4dcc-49b2-8a22-f7055113e656/ 6 http://mokk.bme.hu/resources/hunalign 64 2.2.2 BerkeleyAligner BerkeleyAligner9 (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) is a word alignment toolkit combining unsupervised as well as supervised approaches to word alignment. It features joint training of conditional alignment models (cross-EM), syntactic distortion model, as well as posterior decoding heuristics. Similarly to GIZA++, this tool provides a plethora of options (see documentation/manual.txt in its distribution for details). Most of them are not considered in the webservice interface. The only parameters that are offered by the webservice are the mandatory ones following the common interface and the number of iterations to run the model (default valu"
2011.eamt-1.23,2009.mtsummit-btm.3,0,0.0577465,"Missing"
2011.eamt-1.23,1985.tmi-1.4,0,0.25968,"Missing"
2011.eamt-1.23,W04-3250,0,0.098242,"Missing"
2011.eamt-1.23,J03-1002,0,0.00534824,"Missing"
2011.eamt-1.23,W09-1906,0,0.0576907,"Missing"
2011.eamt-1.28,J10-4005,0,0.0535847,"ammatical target output. On the other hand, CAT systems segment the input text to be translated and compare each segment against the TUs in the TM (Bowker, 2002). CAT systems produce one or more target equivalences for the source segment and professional translators select and recombine them (perhaps with modification) to produce the desired translation. Both EBMT and CAT systems are developed based on a similar premise but in an EBMT approach, selection and recombination are done automatically to produce the translation without the help of a professional translator. Phrase-based SMT systems (Koehn, 2010), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to a CAT environment (Simard, 2003; Bourdaillet et al., 2009). SMT phrases have also been used to populate the knowledge database of an EBMT system (Groves and Way, 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM, has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when no matching TU is found in the TM. The TransT"
2011.eamt-1.28,P01-1050,0,0.0370713,"slation is required to tackle the issue of scarce resources, but it can still suffer from very low accuracy within the SMT framework, even for homogeneous domains (Dandapat et al., 2010). Although SMT and EBMT are both data-driven approaches to MT, both of them have their own advantages and limitations. Typically, an SMT system works well with significant amounts of training data. In contrast, an EBMT approach can be developed with a limited example-base (Somers, 2003); also, as with any other data-driven system, an EBMT system works well when training and test sets are quite close in nature (Marcu, 2001). This is because EBMT systems reuse the segments of test sentences that can be found in the source side of the example-base at runtime. Keeping these points in mind, it is important to develop an MT system of reasonably good quality based on limited amounts of data. In this direction, we are inspired to examine different EBMT approaches which can handle the problem of data sparseness. It is often the case that EBMT systems produce a good translation where SMT fails and vice versa. In order to harness the advantages of both approaches, we use a careful combination of both EBMT and SMT to impro"
2011.eamt-1.28,P02-1040,0,0.0802406,"Missing"
2011.eamt-1.28,2007.mtsummit-papers.49,0,0.0905695,"same applies to the differing parts between two parallel sentences. Generalization in this approach consists of replacing the similar or differing sequences with variables and producing a set of translation templates (including atomic translation templates containing no variables). These translation templates are later used to translate new input sentences. Prior to the above approach, other research was carried out to learn translation templates based on syntactic generalization, e.g. (Kaji et al., 1992). A recent work has also focused on morphological generalization to learn EBMT templates (Phillips et al., 2007). EBMT is often linked with a related technique, namely TM. A TM essentially stores source- and target-language translation pairs (called translation units, TUs) for effective reuse of the previous translations. TM is often used to store examples for EBMT systems. It is also widely used in computer-aided translation (CAT) systems to assist professional translators. EBMT systems first find the example (or a set of ex3 Related Issues 3.1 Type of Corpora Both EBMT and SMT are data-driven approaches to MT which need machine-readable corpora as a prerequisite. The size and type of corpus is also im"
2011.eamt-1.28,W03-0313,0,0.0751211,"in the TM (Bowker, 2002). CAT systems produce one or more target equivalences for the source segment and professional translators select and recombine them (perhaps with modification) to produce the desired translation. Both EBMT and CAT systems are developed based on a similar premise but in an EBMT approach, selection and recombination are done automatically to produce the translation without the help of a professional translator. Phrase-based SMT systems (Koehn, 2010), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to a CAT environment (Simard, 2003; Bourdaillet et al., 2009). SMT phrases have also been used to populate the knowledge database of an EBMT system (Groves and Way, 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM, has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when no matching TU is found in the TM. The TransType system (Langlais et al., 2002) integrates an SMT system within a text editor to suggest possible continuations of the translations b"
2011.eamt-1.28,2006.eamt-1.15,1,0.90968,"ors select and recombine them (perhaps with modification) to produce the desired translation. Both EBMT and CAT systems are developed based on a similar premise but in an EBMT approach, selection and recombination are done automatically to produce the translation without the help of a professional translator. Phrase-based SMT systems (Koehn, 2010), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to a CAT environment (Simard, 2003; Bourdaillet et al., 2009). SMT phrases have also been used to populate the knowledge database of an EBMT system (Groves and Way, 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM, has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when no matching TU is found in the TM. The TransType system (Langlais et al., 2002) integrates an SMT system within a text editor to suggest possible continuations of the translations being typed by the translator. Our approach attempts to integrate the subsentential TM obtained using SMT techniques within an EBMT syste"
2011.eamt-1.28,C92-2101,0,\N,Missing
2011.eamt-1.29,1999.tmi-1.3,0,0.0226373,"segment receives a score. The best alignment is the one with the lowest score. The alignment score is the weighted sum of the values of eight features, which include: the number of SL words with no correspondences in the TL segment, the number of TL words with no correspondences in the SL fragment, the number of SL words with a correspondence in the TL sentence but not in the relevant TL segment, and the difference in length between the SL and the TL segment. Each translation is passed on to the recombination step as long as its score does not exceed five times the length of the SL fragment. Brown (1999) proposed an extension to CMUEBMT that makes use of semantic and syntactic generalized templates. He referred to the template categories as equivalence classes. Examples of semantic and syntactic equivalence classes are given in Table 2. The table shows that class members can in turn contain classes. This is evident from the last line (shown in bold). The system generalizes both the training and the test set: it recursively replaces words and phrases that are part of an equivalence class with the corresponding class tag. Syntactic classes are applied before semantic classes, and disambiguation"
2011.eamt-1.29,1996.amta-1.35,0,0.195739,"hunk with the name of its category, e. g., of a marathon → <PREP> a marathon. The generalized template extension is not part of the current Marclator system. We reimplemented it for our experiments. 3.2 CMU-EBMT The second EBMT system which we used for our experiments is CMU-EBMT.7 The system forms 4 http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC96L14 5 Note that both word and chunk alignment involve statistical knowledge. 6 This is a common procedure for recombinators that do not incorporate a language model. 7 http://sourceforge.net/projects/ cmu-ebmt/ 211 part of PanLite (Frederking and Brown, 1996), an MT architecture developed at Carnegie-Mellon University (CMU). It can also be invoked on its own. The system requires a parallel corpus and a bilingual dictionary. Brown (1996) used entries from a commercial bilingual dictionary for his experiments in translation from Spanish to English. Unlike Marclator, CMU-EBMT does not require subsentential units to be compiled before the actual translation step. The matching step resembles closely that of a traditional EBMT system: CMU-EBMT extracts every substring of the input sentence with a minimum length of two tokens that appears in the SL half"
2011.eamt-1.29,2003.mtsummit-papers.18,1,0.761469,". Each chunk that is not found in the example base is then split into single words. If several TL correspondences for an SL chunk or word are found in the example base, the one with the highest probability is chosen.6 Thus, for each input sentence, the recombinator outputs a single hypothesis. A problem inherent in the approach described above is that the chunks of an input sentence often cannot be found in the example base. Since translating a chunk as a whole is likely to yield a better translation than translating it word by word, it is desirable to increase the chunk coverage of a system. Gough and Way (2003) extended the precursor to Marclator by including an additional layer of abstraction: they produced generalized chunks from word form chunks by replacing the Marker word at the beginning of a word form chunk with the name of its category, e. g., of a marathon → <PREP> a marathon. The generalized template extension is not part of the current Marclator system. We reimplemented it for our experiments. 3.2 CMU-EBMT The second EBMT system which we used for our experiments is CMU-EBMT.7 The system forms 4 http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC96L14 5 Note that both word and"
2011.eamt-1.29,2004.tmi-1.11,1,0.827846,"tor recombination module: in its original form, the recombination module checks for the presence of matching sentences and word form chunks8 before reverting to word-byword translation. We added an additional matching step to follow the chunk matching: in this step, the system replaces the Marker word at the beginning of a chunk by its corresponding Marker tag and searches for the resulting generalized chunk in the example base. Where this attempt fails, the system reverts to word-by-word translation. The only difference remaining to the approach described in Section 3.1 is that the system of Gough and Way (2004) outputs all possible hypotheses for an input sentence, while the Marclator recombinator only outputs the one-best hypothesis. This means that once our system has 8 We subsequently refer to word form chunks (as opposed to generalized chunks) simply as chunks. established a generalized chunk match with the SL side of the example base and has extracted the corresponding TL generalized chunk, it has to make a decision as to which Marker word to insert for the Marker tag. For this, it identifies the SL Marker word underlying the SL generalized chunk that was matched. It gathers the word alignment"
2011.eamt-1.29,W05-0833,1,0.783819,"Translation (CBMT) paradigm. Hence, both SMT and EBMT rely on a sententially aligned bilingual corpus. EBMT systems make use of the parallel corpus by consulting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during the translation step. EBMT systems have often performed worse than SMT systems in the past (cf., for example, Groves and Way (2005)). The biggest shortcoming of EBMT is that it does not combine translations of phrases well. This problem is known as boundary friction (Way, 2001, p. 2). It is particularly frequent when translating into a morphologically rich language. As an example for translating from English into German, assume that the sentence pairs listed in Example 1 are contained in the example base (Way, 2001). (1) A big dog eats a lot of meat. – Ein großer Hund frisst viel Fleisch. I have two ears. – Ich habe zwei Ohren. An EBMT system might make use of the phrases shown in bold to translate a sentence like I have"
2011.eamt-1.29,C92-2101,0,0.269809,"discussion thereof. In Section 5, we give an overview of the issues which we tackled and offer an outlook on future research questions. 2 Related Work When compiling generalized templates, there is a risk of replacing too many parts of an SL–TL pair with variables. To avoid this risk of overgeneralization, generalized templates are usually restricted to certain categories of words. Common candidates for generalization are content words, as replacing them with other content words does not affect the grammar of the sentence. Semantic generalization was explored by Kitamura and Matsumoto (1995). Kaji et al. (1992) applied semantic constraints to their approach to syntactic generalization. Pure syntactic generalization was performed by G¨uvenir and Tunc (1996). Cicekli and G¨uvenir (2001) generalized over sequences of words. The underlying assumption is that given two SL–TL sentence pairs, if the two SL sentences have certain word form sequences in common, the corresponding TL sentences are expected to exhibit the same similarities among each other. The similar parts of the SL sentences are then assumed to be translations of the similar parts of the TL sentences, and the same applies for the differing p"
2011.eamt-1.29,N03-1017,0,0.0135127,"rsity, Dublin 9, Ireland {away,snaskar}@computing.dcu.ie Abstract In this paper, we report our experiments in combining two EBMT systems that rely on generalized templates, Marclator and CMU-EBMT, on an English–German translation task. Our goal was to see whether a statistically significant improvement could be achieved over the individual performances of these two systems. We observed that this was not the case. However, our system consistently outperformed a lexical EBMT baseline system. 1 Introduction The state-of-the-art approach in MT is phrasebased Statistical Machine Translation (SMT) (Koehn et al., 2003). Together with ExampleBased Machine Translation (EBMT) (Nagao, 1984), SMT belongs to the Corpus-Based Machine Translation (CBMT) paradigm. Hence, both SMT and EBMT rely on a sententially aligned bilingual corpus. EBMT systems make use of the parallel corpus by consulting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during"
2011.eamt-1.29,P07-2045,0,0.00906307,"lator OpenMaTrEx Moses 0.1274 0.1269 0.1277 0.0995 0.2763 0.2709 4.3948 4.3815 4.3937 4.2411 5.7880 5.7472 0.4052 0.4047 0.4051 0.3990 0.4914 0.4854 Table 3: Evaluation scores clusive. There are a number of overlaps, i. e., the CMU classes contain 50 words that are also Marker words for English (e. g., after, and, before), and 19 for German (e. g., aber, allen, er). We prompted the system to generalize over the Marker words first, thereby giving preference to the DCU scheme in case of overlaps. Baselines: We established three baselines: Marclator, OpenMaTrEx (Dandapat et al., 2010) and Moses (Koehn et al., 2007). The Marclator baseline was the purely lexical system described in Section 3.1. For the Moses baseline, we used the default system included in OpenMaTrEx. The system uses a 5-gram language model and modified Kneser-Ney smoothing. Training is performed according to the default options and thus includes tuning via MERT (Och, 2003). In addition, a lexicalized reordering model is learnt. The OpenMaTrEx baseline system makes use of EBMT chunk pairs from Marclator and SMT phrase pairs from Moses. We used the default configuration, which includes a 5-gram language model with modified Kneser-Ney smoo"
2011.eamt-1.29,W04-3250,0,0.0609552,"U, while System 1 performed best according to NIST and METEOR. The three systems outperformed the lexical baseline system Marclator according to all three training data English Marker file German Marker file chunking module chunking module English Marker chunks German Marker chunks word alignment module chunk alignment module aligned sentences aligned chunks generalization module aligned generalized chunks IV. III. I. input aligned words II. output recombination module Figure 1: System 1: training and translation process metrics.10 We measured statistical significance by bootstrap resampling (Koehn, 2004) on BLEU.11 The improvement of System 3 over System 2 is statistically significant, while the improvement of System 3 over System 1 is not. The improvements of Systems 1, 2 and 3 over the baseline Marclator system are all significant, as are the improvements of the baseline OpenMaTrEx and Moses system over Systems 1 to 3. 4.3 Chunk Coverage and Chunk-Internal Boundary Friction The evaluation results in Table 3 show that our generalized EBMT systems achieved higher scores than the lexical EBMT system Marclator. This observation supports earlier findings according to which EBMT systems benefit f"
2011.eamt-1.29,J03-1002,0,0.00330683,"ds were ex2 http://www.openmatrex.org/marclator/ marclator.html 3 http://www.apertium.org/ 210 tracted from the Celex database.4 The lists contain a total of 450 Marker words for English and 550 for German. Table 1 lists a sample Marker word for each category. The examples show that entries are included in their inflected forms. Stroppa and Way (2006) found that treating the punctuation marks ! ? , . : ; as additional Marker elements improved performance in their experiments. Following the chunking of the training data, Marclator performs word and chunk alignment. The system relies on Giza++ (Och and Ney, 2003) for word alignment. The chunk alignment algorithm is an edit-distance style algorithm in which the distances are replaced by opposite-log conditional probabilities (Tinsley et al., 2008).5 The recombinator of Marclator is a left-to-right monotone recombinator. When translating an input sentence, it first looks for a matching sentence in the example base. If none is found, the sentence is chunked. Each chunk that is not found in the example base is then split into single words. If several TL correspondences for an SL chunk or word are found in the example base, the one with the highest probabi"
2011.eamt-1.29,P03-1021,0,0.00534394,"an (e. g., aber, allen, er). We prompted the system to generalize over the Marker words first, thereby giving preference to the DCU scheme in case of overlaps. Baselines: We established three baselines: Marclator, OpenMaTrEx (Dandapat et al., 2010) and Moses (Koehn et al., 2007). The Marclator baseline was the purely lexical system described in Section 3.1. For the Moses baseline, we used the default system included in OpenMaTrEx. The system uses a 5-gram language model and modified Kneser-Ney smoothing. Training is performed according to the default options and thus includes tuning via MERT (Och, 2003). In addition, a lexicalized reordering model is learnt. The OpenMaTrEx baseline system makes use of EBMT chunk pairs from Marclator and SMT phrase pairs from Moses. We used the default configuration, which includes a 5-gram language model with modified Kneser-Ney smoothing and tuning via MERT. We included the optional binary feature that records whether a phrase pair is an EBMT chunk pair or not. To train the language models for Moses and OpenMaTrEx, we used the TL side of the training data. 4.2 Results of the MT Systems Table 3 shows the results of our experiments. The best of our systems (S"
2011.eamt-1.29,2006.iwslt-evaluation.4,1,0.926847,"ctic generalization. Category Example determiner personal pronoun demonstrative pronoun possessive pronoun interrogative pronoun indefinite pronoun relative pronoun preposition coordinative conjunction subordinative conjunction cardinal numeral numeric expression auxiliary/modal verb punctuation den euch jenem seine welch andere denen abseits aber falls eins neunundneunzig darf ! Table 1: German Marker categories and examples 3 3.1 Syntactic and Semantic Generalized Templates EBMT at DCU: Marclator Marclator was developed at Dublin City University (DCU) and is part of the MaTrEx architecture (Stroppa and Way, 2006).2 The system does not apply the greedy matching strategy typical of many EBMT systems. Instead, it segments both the training and the test data into chunks. Chunking is based on the Marker Hypothesis (Green, 1979). This is a psycholinguistic hypothesis stating that every language has a closed set of elements that are used to mark certain syntactic constructions. The set of elements includes function words and bound morphemes, such as -ing as an indicator of English progressive-tense verbs and -ly as an indicator of English adverbs. The Marclator chunking module solely considers function words"
2011.eamt-1.29,2001.mtsummit-ebmt.8,1,0.748398,"ting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during the translation step. EBMT systems have often performed worse than SMT systems in the past (cf., for example, Groves and Way (2005)). The biggest shortcoming of EBMT is that it does not combine translations of phrases well. This problem is known as boundary friction (Way, 2001, p. 2). It is particularly frequent when translating into a morphologically rich language. As an example for translating from English into German, assume that the sentence pairs listed in Example 1 are contained in the example base (Way, 2001). (1) A big dog eats a lot of meat. – Ein großer Hund frisst viel Fleisch. I have two ears. – Ich habe zwei Ohren. An EBMT system might make use of the phrases shown in bold to translate a sentence like I have a big dog. into Ich habe ein großer Hund. In doing so, it would neglect the fact that German uses different inflectional forms to mark grammatical"
2011.eamt-1.29,C96-1030,0,\N,Missing
2011.eamt-1.29,W08-0326,1,\N,Missing
2011.eamt-1.29,W10-1720,1,\N,Missing
2011.eamt-1.38,2010.iwslt-papers.1,1,0.948475,"ext, a syntactic label is assigned to each phrase pair extracted from the sentence pair according to word alignments. This 282 syntactic label corresponds to the constituent in the parse tree cover ing the target-side phrase. After that, hierarchical rules are extracted according to the same basic method presented in (Chiang, 2005), but with syntactic labels attached to nonterminals. Phrases which are not covered by a single constituent in the parse tree will be assigned a composite label that results from combining the constituents spanned by the phrase using a set of combinatory operators. (Almaghout et al., 2010) follow the same approach as SAMT to augment the HPB model with syntactic knowledge by assigning syntactic labels to nonterminals in hierarchical rules using Combinatory Categorial Grammar, which provides richer syntactic labels that accurately reflect the syntactic context and dependents of the phrase in addition to being flexible and efficiently extracted without the need for creating a full parse of the sentence. (Hassan et al., 2007) integrate CCG supertags into the target language model and the target side of the translation model of the PB model. They also integrate a grammaticality metr"
2011.eamt-1.38,J99-2004,0,0.0723955,"tagging: According to CCG, each word in the lexicon has a number of supertags, each of which corresponds to a specific syntactic context in which the word may appear. Assigning every possible supertag to each word in the sentence before parsing will create a huge search space, thus putting a heavy burden on the parser to disambiguate them. A solution to this problem is CCG supertagging, which tries to disambiguate the set of supertags assigned to the words of the sentence before parsing according to the words context. This reduces the derivations search space and results in a faster parsing. (Bangalore and Joshi, 1999) use statistics about supertag co-occurrences collected from a parsed corpus to reduce the number of supertags assigned to the words of the sentence. (Clark and Curran, 2004) build a wide coverage CCG parser which uses a log-linear probabilities to supertag the sentence before parsing, leading to faster, more accurate and robust parsing. After supertagging, the parser has only to use combinatory operators to combine supertags assigned to words in order to parse the sentence. That is why supertagging a sentence is considered as “al283 Figure 1: An Arabic sentence and its aligned English transla"
2011.eamt-1.38,W07-0702,0,0.0381911,"s using Combinatory Categorial Grammar, which provides richer syntactic labels that accurately reflect the syntactic context and dependents of the phrase in addition to being flexible and efficiently extracted without the need for creating a full parse of the sentence. (Hassan et al., 2007) integrate CCG supertags into the target language model and the target side of the translation model of the PB model. They also integrate a grammaticality metric into the ngram language model over supertags. This metric penalizes the number of violations of combinatory operators in a sequence of supertags. (Birch et al., 2007) use CCG supertags as a factor in the factored PB translation model (Koehn and Hoang, 2007) following two approaches. The first approach generates CCG supertags as a target-side factor in the factored translation model, and then apply an ngram language model over them. The second approach uses supertags as a source-side factor to direct the decoding process. 3 Combinatory Categorial Grammar CCG (Steedman, 2000) is a grammar formalism in which most of the grammar of the language is stored in the lexicon. The CCG lexicon contains words paired with rich syntactic categories called “supertags”. CC"
2011.eamt-1.38,W04-3250,0,0.202027,"Missing"
2011.eamt-1.38,D07-1091,0,0.0468659,"rately reflect the syntactic context and dependents of the phrase in addition to being flexible and efficiently extracted without the need for creating a full parse of the sentence. (Hassan et al., 2007) integrate CCG supertags into the target language model and the target side of the translation model of the PB model. They also integrate a grammaticality metric into the ngram language model over supertags. This metric penalizes the number of violations of combinatory operators in a sequence of supertags. (Birch et al., 2007) use CCG supertags as a factor in the factored PB translation model (Koehn and Hoang, 2007) following two approaches. The first approach generates CCG supertags as a target-side factor in the factored translation model, and then apply an ngram language model over them. The second approach uses supertags as a source-side factor to direct the decoding process. 3 Combinatory Categorial Grammar CCG (Steedman, 2000) is a grammar formalism in which most of the grammar of the language is stored in the lexicon. The CCG lexicon contains words paired with rich syntactic categories called “supertags”. CCG uses a small set of simple combinatory rules to combine supertags. CCG categories are div"
2011.eamt-1.38,N03-1017,0,0.103061,"epresent the first level of the CCG derivation tree. 4 CCG-based Contextual Labels in HPB 4.1 Motivation SMT systems derive their strength from phrasepairs extracted from the training corpus according to pure statistical methods, which means that phrase-pairs in an SMT system do not necessarily correspond to syntactic constituents. This is one of the main reasons why ungrammatical translations are produced and why it is difficult to incorporate syntax into SMT systems. Restricting SMT systems to use only phrases which represent syntactic constituents caused translation performance to degrade (Koehn et al., 2003). Thus, devising a method which incorporates syntax in SMT systems while maintaining statistically-extracted phrases would be the optimal solution. Using a constituent-based grammar to annotate statistically extracted phrases with syntactic categories proved to be difficult, because constituent grammar has rigid structures which cause it to fail to annotate many phrases (Almaghout et al., 2010). By contrast, a lexicalized grammar such as CCG allow for flexible structures, which enables a CCG supertag to be assigned for a phrase which does not correspond to a grammatical constituent. In additio"
2011.eamt-1.38,P05-1033,0,0.91705,"dition, our experiments demonstrate that the performance of CCG-augmented systems is affected by several factors, including the size and domain of training data and the source language of the translation pairs. The rest of this paper is organized as follows: in Section 2 we review previous approaches which augment HPB system with syntactic knowledge. Section 3 gives a brief introduction to Combinatory Categorial Grammar (CCG). In Section 4 we introduce our approach. Section 5 presents our experiments. Section 6 concludes, and provides avenues for further work. 2 Related Work An HPB MT system (Chiang, 2005) extracts a synchronous Context-Free Grammar (Lewis and Stearns, 1966) from a parallel corpus without syntactic annotation in the form of hierarchical rules. Hierarchical rules are phrases which contain gaps called nonterminals that can be replaced by other phrases. These rules capture the hierarchical aspects of language by providing the ability to translate discontinuous phrases and perform lexical reordering. Having no syntactic constraints imposed on phrases replacing nonterminals in the HPB grammar causes the production of ungrammatical translations. This led to approaches trying to provi"
2011.eamt-1.38,P03-1021,0,0.00813532,"/trac/candc/ 2006) using MADA.2 Baseline Systems: We built two baseline systems: PB and HPB. We built the PB baseline system using the Moses Phrase-Based Decoder (Koehn et al., 2007) with maximum phrase length=12. The HPB baseline system is built using the Moses Chart-Decoder. For all our hierarchical systems, maximum phrase length is set to 12 and maximum rule span is set to 12. Rules extracted contain up to 2 nonterminals. The GIZA++ toolkit3 is used to perform word and phrase alignment and the “grow-diag-final” refinement method is adopted (Koehn et al., 2003). Minimum error rate training (Och, 2003) is performed to tune all our SMT systems. The 5-gram language model in all experiments was trained on the target side of the parallel corpus using the SRILM toolkit4 with modified Kneser-Ney smoothing (Kneser and Ney, 1995). CCG Systems: We built our CCG-augmented HPB systems using Moses Chart Decoder which has an option to extract syntax-augmented rules from an annotated corpus. 5 5.2 Experiments Results Arabic–English: Tables 3 and 4 show the BLEU, METEOR and TER scores for CCG-based systems and baseline systems on Arabic–English news and IWSLT data, respectively. From Table 3 we can see th"
2011.eamt-1.38,P10-1146,0,0.396887,"which affect the performance of syntax-augmented systems. One problem is the strong syntactic constraints imposed by syntaxaugmented rules which restrict the search space of translation and prevent the system in many cases from finding good translations. Another problem is the sparse syntactic labels used in such systems, which cause the generation of low-probability, less reliable rules. This weakens the ability of the system to generalize. As a solution to these problems comes approaches which try to soften syntactic constraints imposed by labeled synchronous rules (Venugopal et al., 2009; Chiang, 2010) have been advanced. In this paper, we propose a method to label nonterminals in hierarchical rules with target-side syntactic contextual labels extracted using Combinatory Categorial Grammar (CCG) (Steedman, 2000). For each target-side phrase in the training corpus, our method uses CCG supertags assigned to its words to extract the left and right syntactic context, represented in the left and right contextual CCG categories of this phrase. Then we annotate the target side of the training corpus by assigning to each phrase a syntactic label that results from combining its left and right contex"
2011.eamt-1.38,P06-1001,0,0.0421068,"Missing"
2011.eamt-1.38,C04-1041,0,0.0668923,"every possible supertag to each word in the sentence before parsing will create a huge search space, thus putting a heavy burden on the parser to disambiguate them. A solution to this problem is CCG supertagging, which tries to disambiguate the set of supertags assigned to the words of the sentence before parsing according to the words context. This reduces the derivations search space and results in a faster parsing. (Bangalore and Joshi, 1999) use statistics about supertag co-occurrences collected from a parsed corpus to reduce the number of supertags assigned to the words of the sentence. (Clark and Curran, 2004) build a wide coverage CCG parser which uses a log-linear probabilities to supertag the sentence before parsing, leading to faster, more accurate and robust parsing. After supertagging, the parser has only to use combinatory operators to combine supertags assigned to words in order to parse the sentence. That is why supertagging a sentence is considered as “al283 Figure 1: An Arabic sentence and its aligned English translation along with CCG derivation tree assigned to the English sentence. most parsing” (Bangalore and Joshi, 1999). Figure 1 illustrates CCG derivation tree assigned to the Engl"
2011.eamt-1.38,P07-1037,1,0.933427,"Missing"
2011.eamt-1.38,N09-1027,0,0.151453,"there are some problems which affect the performance of syntax-augmented systems. One problem is the strong syntactic constraints imposed by syntaxaugmented rules which restrict the search space of translation and prevent the system in many cases from finding good translations. Another problem is the sparse syntactic labels used in such systems, which cause the generation of low-probability, less reliable rules. This weakens the ability of the system to generalize. As a solution to these problems comes approaches which try to soften syntactic constraints imposed by labeled synchronous rules (Venugopal et al., 2009; Chiang, 2010) have been advanced. In this paper, we propose a method to label nonterminals in hierarchical rules with target-side syntactic contextual labels extracted using Combinatory Categorial Grammar (CCG) (Steedman, 2000). For each target-side phrase in the training corpus, our method uses CCG supertags assigned to its words to extract the left and right syntactic context, represented in the left and right contextual CCG categories of this phrase. Then we annotate the target side of the training corpus by assigning to each phrase a syntactic label that results from combining its left a"
2011.eamt-1.38,W06-3119,0,0.579912,"bility to translate discontinuous phrases and perform lexical reordering. Having no syntactic constraints imposed on phrases replacing nonterminals in the HPB grammar causes the production of ungrammatical translations. This led to approaches trying to provide the basic HPB model with syntactic knowledge in the form of syntactic labels attached to nonterminals in hierarchical rules. These labels restrict nonterminal replacement in hierarchical rules to the phrases which match the syntactic constraints imposed by the labels attached to nonterminals. Syntax Augmented Machine Translation (SAMT) (Zollmann and Venugopal, 2006) attaches constituent grammar-based syntactic labels to nonterminals in hierarchical rules. First, each target-side sentence in the training corpus is assigned a parse tree. Next, a syntactic label is assigned to each phrase pair extracted from the sentence pair according to word alignments. This 282 syntactic label corresponds to the constituent in the parse tree cover ing the target-side phrase. After that, hierarchical rules are extracted according to the same basic method presented in (Chiang, 2005), but with syntactic labels attached to nonterminals. Phrases which are not covered by a sin"
2011.eamt-1.38,P07-2045,0,\N,Missing
2011.eamt-1.38,P05-1010,0,\N,Missing
2011.eamt-1.4,W05-0909,0,0.233131,"Missing"
2011.eamt-1.4,E06-1032,0,0.0374124,"ystems over rule-based ones (CallisonBurch et al., 2006). Using BLEU is fast and intuitive, but while this metric has been shown to produce good correlations with human judgment at the document level (Papineni et al., 2002), especially when a large number of reference translations are available, correlation at sentence level is generally low. The NIST evaluation metric (Doddington, 2002) is also string-based, and gives more weight in the evaluation to less frequent n-grams. While this metric has a strong bias in favour of statistical systems, it provides better adequacy correlation than BLEU (Callison-Burch et al., 2006). The GTM metric (Turian et al., 2003) is based on standard measures adopted in other NLP applications (precision, recall and F-measure), which makes its use rather straightforward for NLP practitioners. It focuses on unigrams and rewards sequences of correct unigrams, applying moderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemm"
2011.eamt-1.4,niessen-etal-2000-evaluation,0,0.060211,"r et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al., 2000). We omitted WER and its extension mWER (Nießen et al., 2000) from the experiments reported here as they seem to have been superceded by more recent metrics. TER-plus (Snover et al., 2009) is an extension of TER using phrasal substitutions relying on automatically generated paraphrases, stemming, synonyms and relaxed shifting constraints. This metric is language-dependent and requires WordNet. It has been shown to have the highest average rank in terms of Pearson and Spearman correlation (Przybocki et al., 2008). The DCU-LFG metric (Owczarzak et al., 2007) exploits LFG dependencies and has onl"
2011.eamt-1.4,W07-0718,0,0.0281462,"verview of some of the most widely used automatic MT evaluation metrics, discussing their advantages as well as drawbacks, laying particular emphasis on the metrics used in the comparative evaluation presented in Section 5. The performance of the CoSyne MT system in the early stages of its development can be measured, and its improvement can be monitored over time, against these standard metrics in a reliable and replicable fashion. To ensure the best possible coverage, we decided to use a wide array of metrics, particularly those judged best by recent meta-evaluation exercises (e.g. Callison-Burch et al., 2007; Callison-Burch et al., 2010), without confining ourselves to prominent n-gram based metrics. Since there is no consensus on a single individual metric which is thought to accurately measure MT performance, we decided to adopt an inclusive approach, considering the results of a variety of measures. This should provide a picture that is as reliable and fine-grained as possible. One of the most widely used automatic MT evaluation metrics is BLEU (Papineni et al., 2002), a string-based metric which has come to represent something of a de facto standard in the last few years. This is not surprisi"
2011.eamt-1.4,W07-0411,1,0.87845,"Missing"
2011.eamt-1.4,P02-1040,0,0.0893689,"erage, we decided to use a wide array of metrics, particularly those judged best by recent meta-evaluation exercises (e.g. Callison-Burch et al., 2007; Callison-Burch et al., 2010), without confining ourselves to prominent n-gram based metrics. Since there is no consensus on a single individual metric which is thought to accurately measure MT performance, we decided to adopt an inclusive approach, considering the results of a variety of measures. This should provide a picture that is as reliable and fine-grained as possible. One of the most widely used automatic MT evaluation metrics is BLEU (Papineni et al., 2002), a string-based metric which has come to represent something of a de facto standard in the last few years. This is not surprising given that today most MT research and development efforts are concentrated on statistical approaches; BLEU’s critics argue that it tends to favour statistical systems over rule-based ones (CallisonBurch et al., 2006). Using BLEU is fast and intuitive, but while this metric has been shown to produce good correlations with human judgment at the document level (Papineni et al., 2002), especially when a large number of reference translations are available, correlation"
2011.eamt-1.4,W10-1751,0,0.018734,"tioners. It focuses on unigrams and rewards sequences of correct unigrams, applying moderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemmer and WordNet,3 and it can currently be applied in full only to English, and partly to French, Spanish and Czech, due to the limited availability of synonymy and paraphrase modules. METEORNEXT (Denkowski and Lavie, 2010) is an updated version of the same metric. The TER metric (Snover et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user."
2011.eamt-1.4,2006.amta-papers.25,0,0.0482843,"oderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemmer and WordNet,3 and it can currently be applied in full only to English, and partly to French, Spanish and Czech, due to the limited availability of synonymy and paraphrase modules. METEORNEXT (Denkowski and Lavie, 2010) is an updated version of the same metric. The TER metric (Snover et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al.,"
2011.eamt-1.4,W09-0441,0,0.0132577,"completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al., 2000). We omitted WER and its extension mWER (Nießen et al., 2000) from the experiments reported here as they seem to have been superceded by more recent metrics. TER-plus (Snover et al., 2009) is an extension of TER using phrasal substitutions relying on automatically generated paraphrases, stemming, synonyms and relaxed shifting constraints. This metric is language-dependent and requires WordNet. It has been shown to have the highest average rank in terms of Pearson and Spearman correlation (Przybocki et al., 2008). The DCU-LFG metric (Owczarzak et al., 2007) exploits LFG dependencies and has only a moderate bias towards statistical systems. It requires a dependency parser. It should be noted that among the above measures, METEOR, METEOR-NEXT, TER-plus and DCU-LFG can only be used"
2011.eamt-1.4,2007.mtsummit-papers.27,1,0.605945,"our free online MT systems were used for the baseline evaluation of the CoSyne MT system developed by the University of Amsterdam (Martzoukos and Monz, 2010): • Google Translate13 • Bing Translator14 • Systran15 • FreeTranslation16 These four online MT services were selected first of all because they all cover the three language pairs addressed in year 1 of the CoSyne project (German—English, Italian—English and Dutch—English in both directions). In addition, these are among the most popular free web-based MT systems and are heavily used by the general public of Internet users (Gaspari, 2006; Gaspari and Hutchins, 2007). A final consideration was that three of these five systems are statistical (CoSyne, Google Translate and Bing Translator), while the other two are rule-based (FreeTranslation and Systran). As a result, this mixture of systems offers a good picture of the MT quality currently offered by state-of-the-art representatives of both approaches. 4 Dutch—English NISV provided three different data sets: België Diplomatie consists of 418 HTML document pairs extracted from the Belgian Foreign Affairs website.9 • Video Active is an XML file containing 1,076 document pairs concerning the description of te"
2011.eamt-1.4,2003.mtsummit-papers.51,0,0.0834814,"Missing"
2011.eamt-1.4,W10-1753,1,0.874768,"Missing"
2011.eamt-1.4,C08-1141,0,0.0128615,"and/or language directions needing improvement. By repeating evaluations based on the well-established metrics presented in Section 2 at regular intervals, the improvement of the CoSyne MT system will be gradually monitored and its overall success measured. This evaluation study has shown that rulebased MT systems are outperformed by statistical MT systems for data from the news domain. Plans currently underway to extend the evaluation of the CoSyne MT system include the development of a methodology for diagnostic MT evaluation based on linguistic checkpoints, similar to the one presented in Zhou et al. (2008), who used an ad-hoc tool called Woodpecker. 0.9000 0.8000 0.7000 0.6000 Google Bing Systran Freetranslation CoSyne M12 0.5000 0.4000 0.3000 0.2000 0.1000 0.0000 BLEU NIST METEOR METEOR-NEXT TERp TER GTM DCU-LFG For the Dutch—English translation task, the three statistical MT systems consistently and clearly outperform Systran and FreeTranslation based on all the automatic evaluation metrics. Google outperforms Bing for only three of the metrics (NIST, TER and GTM), whereas for the others Bing receives the higher score. Interestingly, based on TER, the CoSyne MT system does better than Bing, b"
2011.eamt-1.4,P04-1077,0,0.0745498,"Missing"
2011.eamt-1.4,W10-1703,0,\N,Missing
2011.eamt-1.4,2010.iwslt-evaluation.28,0,\N,Missing
2011.eamt-1.4,W07-0700,0,\N,Missing
2011.eamt-1.40,W05-0909,0,0.038572,"was about 5–10 times faster than translating the sentences from scratch. Detailed statistics of the test and development sets obtained by the procedure described above are given in Table 4. 5 Experiments and results The described approach was evaluated in eight different scenarios involving: two language pairs (English–Greek, English–French), both translation directions (to English and from English), and the two domains (Natural Environment, Labour Legislation), using the following automatic evaluation measures: WER, PER, and BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005). The baseline MT systems (denoted as v0) were evaluated using these test sets and results are shown in Table 5. The BLEU, METEOR, PER, and WER scores are percentages; WER and PER are error rates; OOV (out-of-vocabulary) is a ratio of unknown words, i.e. the occurrence of words which do not appear in the parallel training data and thus cannot be translated. The scores among different systems are not freely comparable but they give us some idea of how difficult translation is for particular languages or domains. languages English → French French → English English → Greek Greek → English dom env"
2011.eamt-1.40,2010.amta-papers.16,1,0.556343,"Missing"
2011.eamt-1.40,baroni-bernardini-2004-bootcat,0,0.0135991,"extracted 209 terms for the env domain and 86 for the lab domain. The weights assigned to the terms were signed integers indicating the relevance of each term to a topic-class. Topic-classes correspond to possible sub-categories of the domain. The other input for the crawler is a list of seed URLs relevant to the domain. The seeds for the env domain were selected from relevant lists in the Open Directory Project,5 a repository maintained by volunteer editors. For the lab domain, similar lists were not so easy to find. We therefore adopted a different method, namely using the BootCat toolkit (Baroni and Bernardini, 2004) to create random tuples (i.e. n-combinations of terms) from the terms included in the topic definition. We then ran a query for each tuple on the Yahoo! search engine,6 kept the first five URLs returned for each query and finally constructed the seed list with these URLs. Normalization, the next step in the workflow, concerned encoding identification based on the content_charset header of each document, and, if needed, conversion to UTF-8. Language identification was performed by a modified version of the n-gram-based Lingua::Identify7 tool, which was 4 http://eurovoc.europa.eu/ http://www.dm"
2011.eamt-1.40,eck-etal-2004-language,0,0.413861,"same domain, of the same genre, and the same style as that it is applied on. For many domains, such training resources (monolingual and parallel data) are not available in large enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p."
2011.eamt-1.40,W08-0334,0,0.0158822,"for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Domain adaptation of SMT can be approached in various ways depending on the availability of domain-specific data and their type. If the data is available, it can be directly used to improve components of the MT system: word alignment and phrase extraction (Wu and Wang, 2004), language models (Koehn and Schroeder, 2007), and translation models (Nakov, 2008), usually by m"
2011.eamt-1.40,2005.eamt-1.19,0,0.704171,"e enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens"
2011.eamt-1.40,P05-1058,0,0.192602,"Missing"
2011.eamt-1.40,W07-0733,0,0.0609231,"utomatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general mode"
2011.eamt-1.40,P07-2045,0,0.00586506,".) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Doma"
2011.eamt-1.40,2005.mtsummit-papers.11,0,0.00681904,"eprocessing, training, tuning, decoding, postprocessing, and evaluation. 3.1 General-domain data As for other data-driven MT systems, MaTrEx requires certain data to be trained on, namely parallel data for translation models, monolingual data for language models, and parallel development data for tuning of system parameters. Parameter tuning is not strictly required but has a big influence on system performance. For the baseline system we decided to exploit the widely used data provided by the organizers of the series of SMT workshops (WPT 2005, WMT 2006–2010)1 : the Europarl parallel corpus (Koehn, 2005) version 5 as training data for translation models and language models, and WPT 2005 test set as the development data for parameter optimization. The Europarl parallel corpus is extracted from the proceedings of the European Parliament. For practical reasons we consider this corpus to contain general-domain texts. Version 5 released in Spring 2010 includes texts in 11 European languages including all languages of our interest (English, French, and Greek; see Table 1). Note that the amount of parallel data for English and Greek is only about one half of what is available for English and French."
2011.eamt-1.40,W02-1405,0,0.604769,"nology but also in grammar. In order to achieve optimal performance, an SMT system must be trained on data from the same domain, of the same genre, and the same style as that it is applied on. For many domains, such training resources (monolingual and parallel data) are not available in large enough amounts to train a system of a sufficient quality. However, even small amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere,"
2011.eamt-1.40,J05-4003,0,0.0388536,"lsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus"
2011.eamt-1.40,W08-0320,0,0.0609529,"gium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl corpus statistics for relevant language pairs. from large comparable (non-parallel) corpora to enlarge the in-domain bilingual corpus. Koehn and Schroeder (2007) integrated in-domain and out-of-domain language models as log-linear features in the Moses (Koehn et al., 2007) PB-SMT system with multiple decoding paths for combining multiple domain translation tables. Nakov (2008) combined in-domain translation and reordering models with out-of-domain models also into Moses. In this work, log-linear features were derived to distinguish between phrases of multiple domains by applying data source indicator features. Finch and Sumita (2008) employed a probabilistic mixture model combining two models for questions and declarative sentences with a general model. They used a probabilistic classifier to determine a vector of probability representing class membership. Domain adaptation of SMT can be approached in various ways depending on the availability of domain-specific da"
2011.eamt-1.40,P03-1021,0,0.0040274,"8.1 Table 2: Web-crawled monolingual data statistics. lowercased) versions of the target sides of the parallel data are kept for training the Moses recaser. The lowercased versions of the target sides are used for training an interpolated 5-gram language model with Kneser-Ney discounting using the SRILM toolkit (Stolcke, 2002). Translation models are trained on the relevant parts of the Europarl corpus, lowercased and filtered on sentence level; we kept all sentence pairs having less than 100 words on each side and with length ratio within the interval h0.11,9.0i. Minimum error rate training (Och, 2003, MERT) is employed to optimize the model parameters on the development set. For decoding, test sentences are tokenized, lowercased, and translated by the trained system. Letter casing is then reconstructed by the recaser and extra blank spaces in the tokenized text are removed in order to produce human-readable text. 4 4.1 Acquisition of in-domain resources Web crawling of monolingual data Our workflow for acquiring in-domain monolingual data consists of the following steps: focused web crawling, text normalization, language identification, document clean-up and near-duplicate detection. For"
2011.eamt-1.40,P02-1040,0,0.114242,"a different domain. The correctors confirmed that the process was about 5–10 times faster than translating the sentences from scratch. Detailed statistics of the test and development sets obtained by the procedure described above are given in Table 4. 5 Experiments and results The described approach was evaluated in eight different scenarios involving: two language pairs (English–Greek, English–French), both translation directions (to English and from English), and the two domains (Natural Environment, Labour Legislation), using the following automatic evaluation measures: WER, PER, and BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005). The baseline MT systems (denoted as v0) were evaluated using these test sets and results are shown in Table 5. The BLEU, METEOR, PER, and WER scores are percentages; WER and PER are error rates; OOV (out-of-vocabulary) is a ratio of unknown words, i.e. the occurrence of words which do not appear in the parallel training data and thus cannot be translated. The scores among different systems are not freely comparable but they give us some idea of how difficult translation is for particular languages or domains. languages English →"
2011.eamt-1.40,wu-wang-2004-improving-domain,0,0.512028,"ll amounts of such data can be used to adapt © 2011 European Association for Machine Translation. {vpapa,prokopis,mgiagkou}@ilsp.gr 2 Domain adaptation in SMT Domain adaptation is an active topic in SMT. It was first introduced by Langlais (2002) who integrated in-domain lexicons into the translation model. His work was followed by many others. Eck et al. (2004) presented a language model adaptation technique applying an information retrieval approach based on selecting similar sentences from available training data. Hildebrand et al. (2005) applied the same approach on the translation model. Wu and Wang (2004) and Wu et al. (2005) proposed an alignment adaptation approach to improve domain-specific word alignment. Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 297304 Leuven, Belgium, May 2011 languages (L1–L2) English–French English–Greek sentence pairs 1,725,096 964,242 L1 tokens / vocabulary 47,956,886 73,645 27,446,726 61,497 L2 tokens / vocabulary 53,262,628 103,436 27,537,853 173,435 Table 1: Europarl c"
2011.eamt-1.40,W10-1720,1,\N,Missing
2011.eamt-1.5,2010.iwslt-evaluation.3,1,0.830026,"developing a number of tools ± Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 2128 Leuven, Belgium, May 2011 state-of-the-art components such as Moses (Koehn et al., 2007) and Giza++ (Och and Ney, 2002). Subsequent novel development of the system has resulted in the MaTrEx system achieving world leading ranking in diverse machine translation shared tasks for language pairs as English± Spanish, English±French (Penkale et al., 2010; Tinsley et al., 2008), as well as for non-EU languages (Almaghout et al., 2010; Okita et al., 2010; Srivastava et al., 2008). The principal implemented components of the MaTrEx system to date include: word alignment through word packing (Ma et al., 2007), markerbased chunking and chunk alignment (Gough and Way, 2004), treebank-based phrase extraction (Tinsley and Way, 2009), super-tagging (Hassan et al., 2007), and decoding. The system also includes language-specific extensions such as taggers, parsers, etc. used in pre- and postprocessing modules. All of these modules can be plugged in or out, depending on the needs of the language pair and translation task at hand. in"
2011.eamt-1.5,2010.amta-papers.16,1,0.908863,"Missing"
2011.eamt-1.5,W10-1751,0,0.0442523,"Missing"
2011.eamt-1.5,P07-1037,1,0.880483,"Missing"
2011.eamt-1.5,P02-1038,0,0.044949,"related to translation. Often, making such a leap constitutes a large risk for these entities. Additionally, local patent agencies ± who typically provide expert patent translation services ± are overburdened with requests for human translations. The PLuTO project aims to support these different users by developing a number of tools ± Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 2128 Leuven, Belgium, May 2011 state-of-the-art components such as Moses (Koehn et al., 2007) and Giza++ (Och and Ney, 2002). Subsequent novel development of the system has resulted in the MaTrEx system achieving world leading ranking in diverse machine translation shared tasks for language pairs as English± Spanish, English±French (Penkale et al., 2010; Tinsley et al., 2008), as well as for non-EU languages (Almaghout et al., 2010; Okita et al., 2010; Srivastava et al., 2008). The principal implemented components of the MaTrEx system to date include: word alignment through word packing (Ma et al., 2007), markerbased chunking and chunk alignment (Gough and Way, 2004), treebank-based phrase extraction (Tinsley and W"
2011.eamt-1.5,P02-1040,0,0.0851232,"Missing"
2011.eamt-1.5,2006.amta-papers.26,1,0.82589,"a number of means, e.g. direct text-based translation through a GUI; as backend to a search result; or by means of a number of bespoke tools. A secure connection is established between the client and server to ensure that the translation services are not exploited by unauthorised users. The MT system is deployed at the Centre for Next Generation Localisation in Dublin City University as a multi-tier application encompassing three levels: Machine translation in PLuTO MT in PLuTO is carried out using the MaTrEx (Machine Translation Using Examples) system developed at DCU (Stroppa and Way 2006; Stroppa et al., 2006; Dandapat et al., 2010). It is a hybrid data-driven system built following established design patterns, with an extensible framework allowing for the interchange of novel or previously developed modules. This flexibility is particularly advantageous when adapting to new language pairs and exploring new processing techniques, as language-specific components can be plugged in at various stages in the translation pipeline. The hybrid architecture has the capacity to combine statistical phrase-based, example-based and hierarchical approaches to translation. MaTrEx also acts as a wrapper around ex"
2011.eamt-1.5,2010.amta-commercial.14,1,0.77044,"59 / 56.21 / A 65.52 64.41 65.45 65.81 55.75 / 56.31 / 54.59 / 55.57 / B 65.54 64.45 65.76 65.90 59.73 / 59.93 / 58.96 / 60.9 / C 68.52 68.58 67.98 69.18 54.97 / 55.18 / 54.58 / 54.74 / G 65.61 64.90 65.32 65.73 55.30 / 55.76 / 54.47 / 55.18 / H 65.50 64.85 65.61 65.83 Table 3 BLEU / METEOR-NEXT scores for Ento-Fr MT systems with different in-domain and general domain configurations Table 2 Domain distribution of the sentence pairs and the number of tokens in the English±French parallel corpus (millions) 3.3 Experiments In our previous work on patent domain adaptations for EnglishPortuguese (Tinsley, et al. 2010), the data was very unevenly distributed across the IPC and thus the results were not very definitive. However, having the patent data distributed among more evenly here, as shown in Table 2, we have the opportunity to better test whether combining multi-domain MT models might improve the overall system accuracy, as has been suggested (Haque et al. 2009; Banerjee et al., 2010). In order to test this, we selected the patent domains containing close to, or more than 2 million sentence pairs: A, B, C, G and H. For each of these domains, we had a test set (and a development set) comprising 1,000 h"
2011.eamt-1.5,Y09-2027,1,\N,Missing
2011.eamt-1.5,2004.tmi-1.11,1,\N,Missing
2011.eamt-1.5,P07-2045,0,\N,Missing
2011.eamt-1.5,P07-1039,1,\N,Missing
2011.eamt-1.5,2006.iwslt-evaluation.4,1,\N,Missing
2011.eamt-1.5,W08-0326,1,\N,Missing
2011.eamt-1.5,W10-1720,1,\N,Missing
2011.freeopmt-1.7,atserias-etal-2006-freeling,0,0.0206673,"Missing"
2011.freeopmt-1.7,attia-etal-2010-automatically,1,0.846162,"Missing"
2011.freeopmt-1.7,W10-1751,0,0.017299,"without NEs (en–es no nes). NE-enriched systems are built with different values for the thresholds minimum of occurrences (25, 50, 100 and 200) and minimum percentage of occurrences capitalised (.75, .8, and .85). These values are chosen empirically. We evaluate the systems on the News Commentary 2007 English–Spanish test set (nc-2007) from WMT08,6 which contains 2,000 sentence pairs. The following metrics are used in our experiments:, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), GTM (Turian et al., 2003), METEOR (Lavie and Denkowski, 2009),7 METEOR-Next (Denkowski and Lavie, 2010)7 and DCU-LFG (He et al., 2010).7 Furthermore, we provide for each system execution the amount of unknown tokens (UNK) in the source side of the test set. Statistical significance tests are carried out for BLEU and NIST (with ARK’s code)8 and for GTM (using FastMtEval).9 P-value is set 5 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.7/ apertium-en-es-0.7.1.tar.gz 6 http://www.statmt.org/wmt08/devsets. tgz 7 These are only applied when the target language is English. 8 http://www.ark.cs.cmu.edu/MT/ 9 http://www.computing.dcu.ie/ 39 to 0.05. 5.2 Experiments Prior to running th"
2011.freeopmt-1.7,W10-1753,1,0.873722,"Missing"
2011.freeopmt-1.7,ruimy-etal-2002-clips,0,0.0120316,"http://hdl.handle.net/10609/5644 the evaluation, and compare the performance of the new system to vanilla Apertium. Finally we outline some conclusions and propose lines of future work. 2 MINELex The Multilingual and Interoperable Named Entity Lexicon (MINELex) (Toral et al., 2008; Attia et al., 2010) is a language resource made up of NEs automatically acquired from Wikipedia for 11 languages2 and connected to semantic units of four computational lexicons (English WordNet (Fellbaum, 1998), Spanish WordNet (Verdejo, 1999), Arabic WordNet (Rodr´ıguez et al., 2008) and the Italian PAROLE-SIMPLE (Ruimy et al., 2002)) and to nodes of two ontologies (SUMO (Niles and Pease, 2001) and SIMPLE (Lenci et al., 2000)). In addition, equivalent NEs in different languages are connected by means of interlingual links. Each NE is associated with confidence scores (the number of occurrences of the NE in a corpus and the percentage of times it occurs capitalised), thus allowing the selection of different subsets of the resource according to the requirements and purpose of the application. Table 1 summarises the number of NEs, variants of these NEs (different written forms) and relations of these NEs for English and Span"
2011.freeopmt-1.7,2006.amta-papers.25,0,0.0213084,"s the Apertium engine without any modification (en–es nes), while the second is the Apertium engine without NEs (en–es no nes). NE-enriched systems are built with different values for the thresholds minimum of occurrences (25, 50, 100 and 200) and minimum percentage of occurrences capitalised (.75, .8, and .85). These values are chosen empirically. We evaluate the systems on the News Commentary 2007 English–Spanish test set (nc-2007) from WMT08,6 which contains 2,000 sentence pairs. The following metrics are used in our experiments:, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), GTM (Turian et al., 2003), METEOR (Lavie and Denkowski, 2009),7 METEOR-Next (Denkowski and Lavie, 2010)7 and DCU-LFG (He et al., 2010).7 Furthermore, we provide for each system execution the amount of unknown tokens (UNK) in the source side of the test set. Statistical significance tests are carried out for BLEU and NIST (with ARK’s code)8 and for GTM (using FastMtEval).9 P-value is set 5 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.7/ apertium-en-es-0.7.1.tar.gz 6 http://www.statmt.org/wmt08/devsets. tgz 7 These are only applied when the target language is English. 8 htt"
2011.freeopmt-1.7,2005.mtsummit-papers.11,0,0.0178986,"Missing"
2011.freeopmt-1.7,toral-etal-2008-named,1,0.898893,"Missing"
2011.freeopmt-1.7,2003.mtsummit-papers.51,0,0.0141898,"ut any modification (en–es nes), while the second is the Apertium engine without NEs (en–es no nes). NE-enriched systems are built with different values for the thresholds minimum of occurrences (25, 50, 100 and 200) and minimum percentage of occurrences capitalised (.75, .8, and .85). These values are chosen empirically. We evaluate the systems on the News Commentary 2007 English–Spanish test set (nc-2007) from WMT08,6 which contains 2,000 sentence pairs. The following metrics are used in our experiments:, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), GTM (Turian et al., 2003), METEOR (Lavie and Denkowski, 2009),7 METEOR-Next (Denkowski and Lavie, 2010)7 and DCU-LFG (He et al., 2010).7 Furthermore, we provide for each system execution the amount of unknown tokens (UNK) in the source side of the test set. Statistical significance tests are carried out for BLEU and NIST (with ARK’s code)8 and for GTM (using FastMtEval).9 P-value is set 5 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.7/ apertium-en-es-0.7.1.tar.gz 6 http://www.statmt.org/wmt08/devsets. tgz 7 These are only applied when the target language is English. 8 http://www.ark.cs.cmu.edu/MT/"
2011.freeopmt-1.7,bel-etal-2000-simple,0,0.0097498,"to vanilla Apertium. Finally we outline some conclusions and propose lines of future work. 2 MINELex The Multilingual and Interoperable Named Entity Lexicon (MINELex) (Toral et al., 2008; Attia et al., 2010) is a language resource made up of NEs automatically acquired from Wikipedia for 11 languages2 and connected to semantic units of four computational lexicons (English WordNet (Fellbaum, 1998), Spanish WordNet (Verdejo, 1999), Arabic WordNet (Rodr´ıguez et al., 2008) and the Italian PAROLE-SIMPLE (Ruimy et al., 2002)) and to nodes of two ontologies (SUMO (Niles and Pease, 2001) and SIMPLE (Lenci et al., 2000)). In addition, equivalent NEs in different languages are connected by means of interlingual links. Each NE is associated with confidence scores (the number of occurrences of the NE in a corpus and the percentage of times it occurs capitalised), thus allowing the selection of different subsets of the resource according to the requirements and purpose of the application. Table 1 summarises the number of NEs, variants of these NEs (different written forms) and relations of these NEs for English and Spanish. NEs Variants Instance relations English 948,410 1,541,993 1,366,899 Spanish 99,330 128,79"
2011.freeopmt-1.7,W02-1111,0,0.0968766,"Missing"
2011.freeopmt-1.7,P02-1040,0,0.0804392,"ng (0.7.1).5 Two baselines are considered. The first is the Apertium engine without any modification (en–es nes), while the second is the Apertium engine without NEs (en–es no nes). NE-enriched systems are built with different values for the thresholds minimum of occurrences (25, 50, 100 and 200) and minimum percentage of occurrences capitalised (.75, .8, and .85). These values are chosen empirically. We evaluate the systems on the News Commentary 2007 English–Spanish test set (nc-2007) from WMT08,6 which contains 2,000 sentence pairs. The following metrics are used in our experiments:, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), GTM (Turian et al., 2003), METEOR (Lavie and Denkowski, 2009),7 METEOR-Next (Denkowski and Lavie, 2010)7 and DCU-LFG (He et al., 2010).7 Furthermore, we provide for each system execution the amount of unknown tokens (UNK) in the source side of the test set. Statistical significance tests are carried out for BLEU and NIST (with ARK’s code)8 and for GTM (using FastMtEval).9 P-value is set 5 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.7/ apertium-en-es-0.7.1.tar.gz 6 http://www.statmt.org/wmt08/devsets. tgz 7 These are onl"
2011.iwslt-evaluation.4,2011.iwslt-evaluation.1,0,0.040356,"Missing"
2011.iwslt-evaluation.4,N03-1017,0,0.236145,"provides a brief description of the different SMT models and adaptation techniques used in our experiments. Section 3 details our experimental setup with descriptions on the specific toolsets and data used. Section 4 provides the results of each set of experiments as well as analyses, followed by conclusion and future work in Section 5. 2. Translation Systems This section focuses on the different translation techniques used in the experiments. 2 http://www.ted.com/talks 1 http://iwslt2011.org 3 http://www.euromatrixplus.eu/downloads/35 41 2.1. Phrase-based SMT Systems Phrase-based SMT systems [2] are the most commonly used technique in statistical machine translation nowadays. In this approach, source and target phrase pairs consistent with the word alignment are extracted from the parallel training data. Phrases in PBSMT are just contiguous chunks of text, and are not linguistically motivated. The extracted source-target phrase pairs along with their translation probabilities (computed from the same training data) are stored in a structure known as the ‘phrase table’. During translation, an input sentence is split up into phrases and their corresponding translations are looked up fro"
2011.iwslt-evaluation.4,P05-1033,0,0.443143,"used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw is given as in ( 2): ∗ ¯ P rmix (w|h) = fmix (w|h) + λmix (h)P rmix (w|h) (2) where w is the current word, h is the corresponding ∗ history, fmix is the mixture model discounted relative fre2.3. Hierarchical Phrase-Based System Hierarchical Phrase-Based (HPB) SMT [3] is a tree-based model which extracts a synchronous Context-Free Grammar (CFG) automatically from the training corpus. HPB SMT is based on phrases extracted according to the PB model [2]. Thus, HPB SMT tries to build upon the strengths of PB SMT and adds to it the ability to translate discontinuous phrases and learn phrase-reordering in hierarchical rules without a separate reordering model. HPB SMT uses hierarchical rules as a translation unit. These rules are rewrite rules with aligned pairs of right-hand sides, taking the following form: X →&lt; α, β, ∼&gt; (7) where X is a non-terminal, α and β"
2011.iwslt-evaluation.4,P07-2045,0,0.0348805,"n the training corpora. This mixture model was used to combine the ‘in-domain’ language model with an ‘out-of-domain’ one, with the mixture weights being estimated on the ‘in-domain’ training data by applying a cross-validation scheme. Further improvements on this mixture models were achieved using parameter tying to the most-recent context words [4]. i=1 where, hi (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λi are the corresponding weights of the components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the siz"
2011.iwslt-evaluation.4,P03-1021,0,0.0301456,"cheme. Further improvements on this mixture models were achieved using parameter tying to the most-recent context words [4]. i=1 where, hi (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λi are the corresponding weights of the components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw"
2011.iwslt-evaluation.4,J03-1002,0,0.00250058,"ammaticality of the output. Our experiments will show the effects of this trade-off between label accuracy and sparsity. 3. Experimental Setups This section details the setup for the different experiments. We also provide a brief account of the different tools and datasets used along with the preprocessing and postprocessing procedures employed. 3.1. Tools and Datasets For our PBSMT-based translation experiments we used OpenMaTrEx [15], an open source SMT system which provides a wrapper around the standard log-linear phrase-based SMT system Moses [6]. Word alignment was performed using Giza++ [16]. The phrase and the reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of ou"
2011.iwslt-evaluation.4,P02-1040,0,0.0815708,"d along with the preprocessing and postprocessing procedures employed. 3.1. Tools and Datasets For our PBSMT-based translation experiments we used OpenMaTrEx [15], an open source SMT system which provides a wrapper around the standard log-linear phrase-based SMT system Moses [6]. Word alignment was performed using Giza++ [16]. The phrase and the reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,6"
2011.iwslt-evaluation.4,2011.mtsummit-papers.32,1,0.751983,"e components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw is given as in ( 2): ∗ ¯ P rmix (w|h) = fmix (w|h) + λmix (h)P rmix (w|h) (2) where w is the current word, h is the corresponding ∗ history, fmix is the mixture model discounted relative fre2.3. Hierarchical Phrase-Based System Hierarchical Phrase-Based (HPB) SMT [3] is a tree-based model which extracts a synchronous Context-Free Gra"
2011.iwslt-evaluation.4,W07-0734,0,0.068226,"t on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,624,637 934 934 1,664 1,664 125,948 5,796,505 The datasets used for the experiments included the specific datasets released by the IWSLT 2011 evaluation campaign. The primary bi-lingual training data comprised of a collection of public speech transcriptions on a variety of topics from TED Talks. The development data released for the task, comprised of both the IWSLT-20104 d"
2011.iwslt-evaluation.4,2006.amta-papers.25,0,0.0278312,"alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,624,637 934 934 1,664 1,664 125,948 5,796,505 The datasets used for the experiments included the specific datasets released by the IWSLT 2011 evaluation campaign. The primary bi-lingual training data comprised of a collection of public speech transcriptions on a variety of topics from TED Talks. The development data released for the task, comprised of both the IWSLT-20104 development an"
2011.iwslt-evaluation.4,W06-3119,0,0.0256821,"sted in the translation table recursively from longer phrases and replacing them with the non-terminal symbol X. Non-terminals in hierarchical rules act as placeholders that are replaced with other phrases during translation in a bottom-up fashion. Hierarchical rules are extracted from the training corpus without using any syntactic information. As the resulting system is syntactically unaware, the HPB SMT system can produce ungrammatical translations. Therefore, several approaches have tried to provide the HPB SMT system with syntactic information. Syntax augmented Machine Translation (SAMT) [11] uses target-side phrase-structure grammar syntactic trees to label non-terminals in hierarchical rules. These non-terminal labels represent syntactic constraints imposed on target phrase replacements during translation aiming to produce more grammatical translations. 2.4. CCG-augmented HPB System Following the SAMT approach, CCG-augmented HPB SMT [12] uses CCG [5] to label non-terminals. CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT. This section gives a brief introduction to CCG followed by a de"
2011.iwslt-evaluation.4,2010.iwslt-papers.1,1,0.902746,"resulting system is syntactically unaware, the HPB SMT system can produce ungrammatical translations. Therefore, several approaches have tried to provide the HPB SMT system with syntactic information. Syntax augmented Machine Translation (SAMT) [11] uses target-side phrase-structure grammar syntactic trees to label non-terminals in hierarchical rules. These non-terminal labels represent syntactic constraints imposed on target phrase replacements during translation aiming to produce more grammatical translations. 2.4. CCG-augmented HPB System Following the SAMT approach, CCG-augmented HPB SMT [12] uses CCG [5] to label non-terminals. CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT. This section gives a brief introduction to CCG followed by a description of the approach of extracting non-terminal labels using the same. 2.4.1. Combinatory Categorial Grammar CCG [5] is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags) and a set of combinatory rules which specify how the categories are combined. A supertag is a rich syntactic description that sp"
2011.iwslt-evaluation.4,J99-2004,0,0.024692,"s in using statistically extracted phrases which do not necessarily correspond to syntactic constituents. Secondly, CCG categories reflect rich information about the syntactic structure to which the word/phrase belongs at the lexical level without the need to build a full parse tree for the sentence. Thirdly, CCG parsing is more efficient in comparison to phrase-structure grammar parsing. Because most of the CCG grammar is contained in the lexicon, the process of supertagging, which is to assign supertags (i.e. complex CCG categories) to the words in a sentence, is considered “almost parsing” [13]. After supertagging, the CCG parser is only required to combine the supertags using CCG simple combinatory operators. For the aforementioned reasons, CCG is considered more suitable to be used in SMT than phrase-structure grammar. Attaching CCG categories to non-terminals in hierarchical rules is done in a way similar to that of SAMT approach: • First, each target-side sentence from the parallel corpus is supertagged by assigning the best sequence of CCG supertags to its words. • Next, phrase pairs are extracted from the parallel corpus according to the PBSMT phrase extraction method [2]. • T"
2011.iwslt-evaluation.4,2011.eamt-1.38,1,0.804863,"thermore, some atomic CCG categories have features expressed between brackets which describe certain syntactic information. For example, the atomic category S might have a feature attached to it which distinguishes types of sentences such as declarative S[dcl] or wh-question S[wq]. All the additional information represented in a single CCG category increases the number of different CCG categories and leads to label sparsity problem. In order to address this problem, we simplify CCG non-terminal labels by reducing the amount of the information represented in them using the following approaches [14]: • Feature-dropped CCG labels: these labels are extracted from CCG categories by dropping the syntactic features attached to atomic categories from the label representation. For example, if a phrase has a CCG category S[dcl]/NP, then its feature-dropped CCG label is S/NP. • CCG Contextual Labels: in a CCG contextual label, only left and right argument categories are used in the label representation whereas the resulting category (i.e. the functor) is dropped from the label representation. The resulting CCG contextual label takes the form L R. If any of the argument categories is missing, an X"
2011.iwslt-evaluation.4,Y09-2027,1,0.83049,"we perform case restoration and detokenization for the English data. Case restoration, or truecasing is treated as a translation task. A simple phrase-based translation model is trained on aligned lower-case and truecase data to successfully achieve the task of true-casing. 3.3. PBSMT based Language Model Adaptation Experiments As shown in Table 1, the size of the ‘in-domain’ TED training data is much smaller than the ‘out-of-domain’ Multi-UN training data. Since adding a significant amount of out-ofdomain data to an in-domain corpus reduces the quality of translation for in-domain sentences [23], we decided to use only a part of the out-of-domain data to enhance the translation quality. In order to achieve this, we constructed a language model on the TED monolingual data and computed sentence-level perplexity score for all the sentences in MultiUN, with respect to the TED language model. After sorting the sentences in the ascending order of the perplexity values, only sentences below a specific threshold were selected. This method provided us with the most ‘TED-like’ sentences from the Multi-UN corpora. In order to decide which specific threshold gives us the best possible translatio"
2011.iwslt-evaluation.4,2009.iwslt-papers.4,0,0.0154921,"ents Figure 1 shows the variation of BLEU scores for different adapted language models pertaining to different thresholds. According to our experiments, the best cut-off thresholds were 43.00 and 53.00 for Zh–En and Ar–En language pairs, respectively. For Ar–En language pair, the best BLEU 45 score is achieved for multiple thresholds, and we select the one with the maximum number of sentences in it. The number of Multi-UN sentences thus selected were 55,841 and 89,310 for Zh–En and Ar–En language pairs, respectively. 3.4. HPB Experiments We built our HPB baseline using the Moses Chart Decoder [24]. Continuous phrases are extracted according to the phrase based system settings explained in Section 3.1. Maximum phrase length and maximum rule span are both set to 12 words. The maximum span for the chart during decoding is set to 20 words, above which only monotone concatenation of phrases is used. Rules extracted contain up to 2 non-terminals. Adjacent non-terminals on the source side are not allowed. 3.5. CCG-augmented HPB Experiments We built our CCG-augmented HPB system using the Moses Chart Decoder, which has an option to extract syntaxaugmented rules from an annotated corpus. We used"
2011.iwslt-evaluation.4,W04-3250,0,0.160582,"Missing"
2011.mtsummit-papers.32,N09-1025,0,0.0602809,"Missing"
2011.mtsummit-papers.32,W07-0722,0,0.0178866,"oved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word alignment, generating domainspeciﬁc Viterbi alignments to feed a state-of-the-art phrase-based SMT system. Our work follows the line of research presented in Foster and Kuhn (2007) using mixture modelling and linear/log-linear combination frameworks, but differs in terms of the test set and development sets used for tuning and evaluation. While Foster and Kuhn (2007) used test and development sets which were essentially a combination of data from different training genres, in our case test data (user forum) are inherently different from"
2011.mtsummit-papers.32,eck-etal-2004-language,0,0.126201,"tensively used for language model adaptation, especially in speech recognition. Iyer and Ostendorf (1996) use this technique to capture topic dependencies of words across sentences within language models. Cache-based language models (Kuhn and De Mori, 1990) and dynamic adaptation of language models (Kneser and Steinbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT s"
2011.mtsummit-papers.32,2010.amta-commercial.5,0,0.224215,"Introduction In recent years, Statistical Machine Translation (SMT) technology has been used in many online applications, concentrating on professionally edited enterprise quality online content. At the same time, very little research has gone into adapting ∗ Work done while at CNGL, School of Computing, DCU 285 SMT technology to the translation of user-generated content on the web. While translation of online chats (Flournoy and Callison-Burch, 2000) has received some attention, there is surprisingly little work on translation of online user forum data, despite growing interest in the area (Flournoy and Rueppel, 2010). In this paper we describe our efforts in building a system to address this particular application area. Our experiments are conducted on data collected from online forums on Symantec Security tools and services.1 For a multinational company like Symantec, the primary motivation behind translation of user forum data is to enable access across language barriers to information in the forums. Forum posts are rich in information about issues and problems with tools and services provided by the company, and often provide solutions to problems even before traditional customer-care help lines are ev"
2011.mtsummit-papers.32,W04-3250,0,0.0274837,"Missing"
2011.mtsummit-papers.32,2005.mtsummit-papers.11,0,0.0155595,"ms with tools and services provided by the company, and often provide solutions to problems even before traditional customer-care help lines are even aware of them. The major challenge in developing MT systems for user forum data concerns the lack of proper parallel training material. Forum data is monolingual and hence cannot be used directly to train SMT systems. We use parallel training data in the form of Symantec Enterprise Translation Memories (TMs) from different product and service domains to train the SMT models. As an auxiliary source, we also used portions of the Europarl dataset2 (Koehn, 2005), selected according to their similarity with the forum data (Section 3.2), to supplement the TMbased training data. Symantec TM data, being a part of enterprise documentation, is professionally 1 2 http://community.norton.com/ http://www.statmt.org/europarl/ edited and by and large conforms to the Symantec controlled language guidelines, and is signiﬁcantly different in nature from the user forum data, which is loosely moderated and does not use controlled language at all. In contrast Europarl data is outof-domain with respect to the forum data. The differences between available training and"
2011.mtsummit-papers.32,P07-2045,0,0.0156353,"Missing"
2011.mtsummit-papers.32,W07-0733,0,0.0321071,"retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word alignment, generating domainspeciﬁc Viterbi alignments to feed a state-of-the-art phrase-based SMT system. Our work follows the line of research presented in Foster and Kuhn (2007) using mixture modelling and linear/log-linear combination frameworks, but differs in terms of the test set and development sets used for tuning and eval"
2011.mtsummit-papers.32,W02-1405,0,0.0297662,"ts, followed by conclusions and future work in Section 6. 2 Related Work Mixture Modelling (Hastie et al., 2001), a wellestablished technique for combining multiple mod286 els, has been extensively used for language model adaptation, especially in speech recognition. Iyer and Ostendorf (1996) use this technique to capture topic dependencies of words across sentences within language models. Cache-based language models (Kuhn and De Mori, 1990) and dynamic adaptation of language models (Kneser and Steinbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain"
2011.mtsummit-papers.32,J03-1002,0,0.00718605,"Missing"
2011.mtsummit-papers.32,P03-1021,0,0.0354487,"Missing"
2011.mtsummit-papers.32,C08-1125,0,0.0125168,"nbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word a"
2011.mtsummit-papers.52,W05-0909,0,0.13897,"Missing"
2011.mtsummit-papers.52,P05-1033,0,0.244672,"Missing"
2011.mtsummit-papers.52,2010.jec-1.4,0,0.114664,"s”). When presented with fuzzy matches, translators can avail of useful complete matching sub-segments in previous translations while composing the translation of a new segment. This improves the consistency of translation, as new translations produced by translators are based on the target side of the fuzzy match they have consulted, and translators will build their translations around terminologies already used in the TM. It is, therefore, natural to resort to TMs for consistent translation, and to incorporate fully matching sub-segments from fuzzy match examples into the SMT pipeline (cf. (Koehn and Senellart, 2010), (Zhechev and van Genabith, 2010), and (Ma et al., 2011)). Although these methods have led to improved translations, they only use very simple features (such as a threshold on the fuzzy match score of the complete TM segment) to determine whether matching sub-segments from the fuzzy match are suitable for use in the SMT pipeline. Here, we propose a rich set of linguistic features to select TM fuzzy matches that contain useful sub-segments that improve translation consistency in an SMT pipeline.1 We assume that many factors are rele1 In Ma et al. (2011), we considered a richer set of features"
2011.mtsummit-papers.52,P11-1124,1,0.678615,"Missing"
2011.mtsummit-papers.52,J03-1002,0,0.00594353,"bed in Ma et al. (2011). If a classifier predicts that the markup will lead to improved translation quality and translation, the consistent phrase translation will be reused directly in the translation process. Below we explain how consistent phrase pairs are defined, and how markup classification is performed. Based on these, we discuss why linguistic features are essential for this task. 3.1 Consistent Phrase Pair Identification We use the method of Ma et al. (2011) to extract consistent phrase pairs: extracted phrase pairs are the intersections of bidirectional GIZA++ posterior alignments (Och and Ney, 2003) between the source and the target side of the TM fuzzy match. We use the intersected word alignment to minimize the noise introduced by word alignment in one direction only, in order to ensure translation consistency. 3.2 Markup Classification Following (Ma et al., 2011), we use Support Vector Machines (SVMs, (Cortes and Vapnik, 1995)) to determine whether constraining translation with our consistent phrase pairs can help improve translation quality. We treat constrained translation as a binary classification problem, and use the SVM classifier to decide whether we should mark up a segment or"
2011.mtsummit-papers.52,J04-4002,0,0.0207764,"ation from Symantec, consisting of 87K segment pairs. The average segment length of the English training set is 13.3 words and the size of the training set is comparable to the larger TMs used in the industry. We obtain training samples using the cross-fold translation technique in Ma et al. (2011), so the word aligner, the translation models, and the classifier are all trained on the same training corpus. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. As for SVM parameters, we set c = 2.0 and γ = 0.125. We conducted experiments using a standard log-linear PB-SMT (Och and Ney, 2004) system Moses,8 which is capable of handling user-specified translations for portions of the input during decoding. The maximum phrase length is set to 7. 5.1 Evaluation The performance of the phrase-based SMT system is measured by B LEU score (Papineni et al., 2002) and T ER (Snover et al., 2006). Significance testing is carried out using approximate randomization (Noreen, 1989). We also measure the quality of the classification using precision and recall. Let A be the set of predicted markup input segments, and B be the set of input segments where the markup version has a lower T ER score th"
2011.mtsummit-papers.52,J05-1004,0,0.00720046,"ad POSi =1 iff the first word of the source segment is marked up and has the POS tag POSi . Dependency Features We use dependency relations (obtained using the Stanford parser) to establish the roles of matched parts in the input sentence in terms of syntactic dependencies. The dependency features include DEP Coverage, DEP Position, and DEP Consistency, all of which follow the definitions in Ma et al. (2011). Semantic Role Features Our semantic role labels are obtained using the Suda SRL labeler,6 with constituent trees produced by the Stanford parser as input. The labels follow the PropBank (Palmer et al., 2005) annotation. Following POS features, for each predicate identified in a segment, we define SEMi (¯ em ) as the number of words in the input segment e having the role SEMi that are marked up 4 http://snowball.tartarus.org/ algorithms/english/stop.txt 5 http://nlp.stanford.edu/software/ lex-parser.shtml 6 http://nlp.suda.edu.cn/˜jhli/ with translations from TM, and define #SEMi (e) as the number of words in e that have the SEM role SEMi . The features include SEM Partial Coverage which calculates the marked-up percentage for each argument label,7 SEM Complete Coverage – a binary feature that fir"
2011.mtsummit-papers.52,P02-1040,0,0.0900314,"lation technique in Ma et al. (2011), so the word aligner, the translation models, and the classifier are all trained on the same training corpus. We train the SVM classifier using the libSVM (Chang and Lin, 2001) toolkit. As for SVM parameters, we set c = 2.0 and γ = 0.125. We conducted experiments using a standard log-linear PB-SMT (Och and Ney, 2004) system Moses,8 which is capable of handling user-specified translations for portions of the input during decoding. The maximum phrase length is set to 7. 5.1 Evaluation The performance of the phrase-based SMT system is measured by B LEU score (Papineni et al., 2002) and T ER (Snover et al., 2006). Significance testing is carried out using approximate randomization (Noreen, 1989). We also measure the quality of the classification using precision and recall. Let A be the set of predicted markup input segments, and B be the set of input segments where the markup version has a lower T ER score than the plain version. We stan7 If more than one predicate is identified, the value of the feature is averaged among argument labels for each predicate. 8 http://www.statmt.org/moses/ 459 dardly define precision P and recall R as in (3): |A B| |A B| P = ,R= |A| |B| (3"
2011.mtsummit-papers.52,2009.mtsummit-papers.14,0,0.275606,"s the line of research proposed by Ma et al. (2011), which improves the consistency of translations in PB-SMT systems by constraining the SMT system with consistent phrase pairs induced from TMs. Whether the consistent phrase pairs should be used is determined through discriminative learning. As the research in this paper builds on this previous work of ours, we review it in detail in Section 3. Prior to Ma et al. (2011), several proposals used translation information derived from TM fuzzy matches, such as (i) adding such translations into a phrase table as in Bic¸ici and Dymetman (2008)2 and Simard and Isabelle (2009), or (ii) marking up the input segment using the relevant sub-segment translations in the fuzzy match, and using an MT system to translate the parts that are not marked up, as in Smith and Clark (2009), Koehn to dependency labels. 2 Note that discontinuous phrase pairs are used in Bic¸ici and Dymetman (2008), whereas we use continuous phrase pairs here. 457 and Senellart (2010), and Zhechev and van Genabith (2010). However, these do not include a classification step that determines whether consistent phrase pairs should be used. 3 Constrained Translation via Markup Classification Ma et al. (20"
2011.mtsummit-papers.52,2006.amta-papers.25,0,0.378349,". We use the intersected word alignment to minimize the noise introduced by word alignment in one direction only, in order to ensure translation consistency. 3.2 Markup Classification Following (Ma et al., 2011), we use Support Vector Machines (SVMs, (Cortes and Vapnik, 1995)) to determine whether constraining translation with our consistent phrase pairs can help improve translation quality. We treat constrained translation as a binary classification problem, and use the SVM classifier to decide whether we should mark up a segment or not. We label training data using the automatic T ER score (Snover et al., 2006), as in (1).  +1 if TER(w. markup) < TER(w/o markup) y= −1 if TER(w/o markup) ≥ TER(w. markup) (1) Each data point is associated with a set of features which are discussed in more detail in Section 4. We perform our experiments with the Radial Basis Function (RBF) kernel, and use Platt’s method (Platt, 1999) (as improved by (Lin et al., 2007)) to fit the SVM output to a sigmoid function, to obtain probabilistic outputs from the SVM. 3.3 Rich Linguistic Features for Markup Classification A close look at the markup classification procedure shows that the accuracy of classification (and ultimate"
2011.mtsummit-papers.52,W10-3806,1,0.899614,"Missing"
2011.mtsummit-papers.60,ruimy-etal-2002-clips,0,0.0411229,"Missing"
2011.mtsummit-papers.60,2011.eamt-1.36,0,0.0501854,"rors based on the use of morpho-syntactic information, which shows that their linguistically-informed evaluation measures provide useful insights to understand the weaknesses of their MT system, while also indicating the best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments Linguistic Checkpoints-based Diagnostic Evaluation In this section, we first give an overview of linguistic checkpoints and then detail the evaluation framework and the key components of the system. 3.1 Linguistic Checkpoints A linguistic checkpoint can be defined as a linguistically-motivated unit, (e.g. an ambiguous word, a verb-obje"
2011.mtsummit-papers.60,W06-3101,0,0.421307,"Missing"
2011.mtsummit-papers.60,W07-0707,0,0.0460502,"word order in translation from Chinese into English. Farrús et al. (2011) carry out a manual error analysis on an MT system for Spanish— Catalan and classify the errors into linguistic levels (orthographic, morphological, lexical, semantic, and syntactic). Popoviü et al. (2006) adopt a framework for the automatic analysis of MT errors based on the use of morpho-syntactic information, which shows that their linguistically-informed evaluation measures provide useful insights to understand the weaknesses of their MT system, while also indicating the best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments"
2011.mtsummit-papers.60,vilar-etal-2006-error,0,0.183659,"Missing"
2011.mtsummit-papers.60,W10-3301,0,0.114671,"ord alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each level of linguistic analysis based on ISO standards (i.e. MAF, SynAF, SemAF) and it is compatible with the Linguistic Annotation Framework (LAF) (Ide and Romary, 2003). (iv) It makes use of Kybots (Vossen et al., 2010), established in the FP7 KYOTO project, to define the evaluation targets (linguistic check3 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/MSR%20noncommercial%20license%20agreement.txt 4 http://www.computing.dcu.ie/~atoral/delic4mt (under the license GPL-v3). 5 http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger/ 6 http://code.google.com/p/giza-pp/ 7 http://panacea-lr.eu/ 8 http://www.xces.org/ 9 http://www.kyoto-project.eu/ points). A Kybot profile can be thought of as a regular expression over elements and attributes in KAF documents. The benefits of"
2011.mtsummit-papers.60,P10-1062,0,0.0464647,"best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments Linguistic Checkpoints-based Diagnostic Evaluation In this section, we first give an overview of linguistic checkpoints and then detail the evaluation framework and the key components of the system. 3.1 Linguistic Checkpoints A linguistic checkpoint can be defined as a linguistically-motivated unit, (e.g. an ambiguous word, a verb-object collocation, a POS-n-gram, a constituent, etc.) which is predefined in a linguistic taxonomy for diagnostic evaluation. Such a taxonomy is an inventory of linguistic phenomena of the source language that can"
2011.mtsummit-papers.60,W03-1901,0,0.0286593,"ablished in the FP7 PANACEA project,7 to represent word alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each level of linguistic analysis based on ISO standards (i.e. MAF, SynAF, SemAF) and it is compatible with the Linguistic Annotation Framework (LAF) (Ide and Romary, 2003). (iv) It makes use of Kybots (Vossen et al., 2010), established in the FP7 KYOTO project, to define the evaluation targets (linguistic check3 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/MSR%20noncommercial%20license%20agreement.txt 4 http://www.computing.dcu.ie/~atoral/delic4mt (under the license GPL-v3). 5 http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger/ 6 http://code.google.com/p/giza-pp/ 7 http://panacea-lr.eu/ 8 http://www.xces.org/ 9 http://www.kyoto-project.eu/ points). A Kybot profile can be thought of as a regular expression over elemen"
2011.mtsummit-papers.60,C08-1141,0,0.178031,"nce between the two languages involved in the translation process. The level of detail and the specific linguistic phenomena included in the taxonomy can vary, depending on what the developers and/or the end-users want to investigate as part of the diagnostic evaluation and on the number of aspects that they are interested in. Linguistic checkpoints form the basis of linguistic test suites which are the means by which the MT output is evaluated. 3.2 Diagnostic Evaluation Framework This approach evaluates a system’s ability to handle various linguistic checkpoints. These were first proposed by Zhou et al. (2008), who developed Woodpecker,2 a tool supporting diagnostic evaluation based on linguistic checkpoints. However, this tool has two important drawbacks. Firstly, language-dependent data for English–Chinese (the language pair considered in their paper) is hardcoded in the software, which means that adapting it 2 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/ 530 to other language pairs is not straightforward. Secondly, its license (MSR-LA)3 is quite restrictive, to the extent that researchers would not be able to publicly release their adaptations of the tool. F"
2011.mtsummit-papers.60,J03-1002,0,0.00348558,"ation of the matching checkpoints. The requirements we stipulated for this new tool include: (i) the code had to be well-organized and fully documented; (ii) creating new evaluation targets for any language pair has to be as easy as possible (no coding involved); and (iii) the tool should support different evaluation metrics. Our novel tool, DELiC4MT (Diagnostic Evaluation using Linguistic Checkpoints For Machine Translation), 4 makes extensive use of already available components and representation standards. (i) It uses state-of-the-art PoS taggers and word aligners. Treetagger5 and GIZA++6 (Och and Ney, 2003), respectively, are used in the current version, although any similar tool could be used. (ii) It exploits the Travelling Object (TO) format, established in the FP7 PANACEA project,7 to represent word alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each l"
2011.mtsummit-papers.7,J04-2004,0,0.0198305,"is, or emotion). Previous studies have focused on the tighter integration of ASR and MT to solve the aforementioned problems. The coupling structure proposed in (Ney, 1999) highlights the importance of information sharing between ASR and MT modules and the following studies (Mathias and Byrne, 2005; Zhou et al., 2007) evaluate using merged graphs to achieve optimal translation by integrating, searching, and combining various ASR scores and translation models. These studies can be classiﬁed as Finite State Transducers (FST)-based approaches along with the GIATI-based speech translation system (Casacuberta and Vidal, 2004; Casacuberta et al., 2004; Matusov et al., 2005a). In the FST-based approach, a tighter integration between the ASR and MT modules is accomplished by using FSTs as the basic structure to share information. The approach uses composite decoding to obtain better translation quality from source-side speech input. In this architecture source-side speech is fed into FST modules to obtain target-side translation outputs, and then the synthesis module is used to produce target-side speech. Since the FST-based approach accomplishes a tighter integration between the ASR and MT modules, it is easier to"
2011.mtsummit-papers.7,J07-2003,0,0.0335385,"untouched. Then, similar to the original PBSMT building process, the phonetic PBSMT model is tuned by MERT (Och, 2003) in terms of the BLEU (Papineni et al., 2002) metric. Note that from the previous G2P examples, since source words are transformed into their phonetic forms, the average input length is much larger than the original PBSMT (e.g. one word “jacket” is transformed into ﬁve phones “JH AE K IH T”), which implies a greater computational complexity since maximum phrase length and distortion limit ought to be increased proportionally for comparable performance. Therefore cube pruning (Chiang, 2007) for PBSMT is utilised for faster decoding. Now we use the term “Phone Translation” (PT) to identify this phonetic MT base system and it can be enhanced with different source-side phonetic information. 4 G2P Conversion The G2P module plays an essential role to converts words from the orthographic form (a sequence of letters) to its pronunciation representation (a sequence of phones). It is utilised to transform word-level recognition outputs into phone sequences, and to enable original PBSMT model works on source phone sequences. Because of the irregular correspondence between spelling and pro"
2011.mtsummit-papers.7,N09-1049,0,0.0345275,"Missing"
2011.mtsummit-papers.7,P07-2045,0,0.00705514,"Missing"
2011.mtsummit-papers.7,P03-1021,0,0.00629772,"in the second table, with a different source entry after G2P conversion. For example, the word “life” and “jacket” is converted into “L AY F” and “JH AE K IH T” respectively, then the corresponding source entries are transformed from “life jacket” into “L AY F JH AE K IH T”. • Tune the phonetic PBSMT model with phone sequences. As described in the last section, 84 original word-level development set is transformed into source-side phone sequences with G2P module, and target side is left untouched. Then, similar to the original PBSMT building process, the phonetic PBSMT model is tuned by MERT (Och, 2003) in terms of the BLEU (Papineni et al., 2002) metric. Note that from the previous G2P examples, since source words are transformed into their phonetic forms, the average input length is much larger than the original PBSMT (e.g. one word “jacket” is transformed into ﬁve phones “JH AE K IH T”), which implies a greater computational complexity since maximum phrase length and distortion limit ought to be increased proportionally for comparable performance. Therefore cube pruning (Chiang, 2007) for PBSMT is utilised for faster decoding. Now we use the term “Phone Translation” (PT) to identify this"
2011.mtsummit-papers.7,2010.eamt-1.16,0,0.0564386,"Missing"
2011.mtsummit-papers.7,W09-0441,0,0.0326296,"Missing"
2011.mtsummit-papers.7,C04-1168,0,0.211979,"Missing"
2012.amta-commercial.3,P03-1021,0,0.00657125,"ntent. Secondly, a user&apos;s TMX files (and glossaries, if available) can be used to train an SMT engine completely automatically. Plain bilingual text is extracted from the TMX files to create a parallel 6 https://www.oasis-open.org/committees/xliff/ corpus, which is then subjected to multiple stages of corpus clean-up, one of the main reasons why SmartMATE manages to considerably outperform the Moses (Koehn et al., 2007) baseline on which it is built. Once the various models (phrase-based translation model, language model, lexicalized reordering model) have been constructed and tuned via MERT (Och, 2003), the engine is ready to be tested. A recent example of how effective SmartMATE can be was where a new user (availing of the 30-day free trial) uploaded a 1 million-segment TM file, had an English-toSpanish engine built in less than 5 hours, and then translated 180,000 words in less than 2 hours at a rate of over 1500 words/minute, with a BLEU score (Papineni et al., 2002) of over 70 on a 1000sentence held-out test set. By any measures, this is impressive, and demonstrates how effective SmartMATE can be for LSPs and individual translators alike. 4 Methodology A mixed-methods approach was taken"
2012.amta-commercial.3,P02-1040,0,0.083321,"es to considerably outperform the Moses (Koehn et al., 2007) baseline on which it is built. Once the various models (phrase-based translation model, language model, lexicalized reordering model) have been constructed and tuned via MERT (Och, 2003), the engine is ready to be tested. A recent example of how effective SmartMATE can be was where a new user (availing of the 30-day free trial) uploaded a 1 million-segment TM file, had an English-toSpanish engine built in less than 5 hours, and then translated 180,000 words in less than 2 hours at a rate of over 1500 words/minute, with a BLEU score (Papineni et al., 2002) of over 70 on a 1000sentence held-out test set. By any measures, this is impressive, and demonstrates how effective SmartMATE can be for LSPs and individual translators alike. 4 Methodology A mixed-methods approach was taken to the research to access rich qualitative data about the subjective experiences of the student translators, and to measure student learning using standard quantitative instruments. As already indicated, data were collected using participant questionnaires (containing items accessing experience of other translation technology tools, computer usage, and self-efficacy (see"
2012.amta-commercial.3,2012.amta-wptp.6,1,0.706874,"range of document formats, including Microsoft Office Suite file formats (e.g. Word, Excel and PowerPoint), as well as other popular formats such as .rtf, .html, .ttx and .txt. The XLIFF file – either originally in .xlf format, or generated from some other format via File Filtering – can then optionally be sent through the TM component in order to leverage any previous translations, and through MT for segments which do not match any TM entry at the required threshold level. At this stage, the document becomes available for postediting. SmartMATE provides an online multi-user Editor Suite (cf. Penkale and Way, 2012). Users can make use of the editor themselves to translate the document, or they might delegate this to a third party. After translation has finished, the translated XLIFF file is sent back to File Filtering in order to recover the original file format. TMX files can be used in two different ways in SmartMATE. Firstly, they can be used as traditional Translation Memories. When a new document is ready to be translated, segments in the document which exactly match any TM entry will appear in the editor suite as pre-translated using the target side of that entry. In addition to exact matches, Sma"
2012.amta-commercial.3,vilar-etal-2006-error,0,0.029348,"Missing"
2012.amta-commercial.3,2007.mtsummit-papers.66,0,0.0508028,"Missing"
2012.amta-commercial.3,P07-2045,0,\N,Missing
2012.amta-commercial.3,2001.mtsummit-teach.6,1,\N,Missing
2012.amta-commercial.8,2011.mtsummit-papers.32,1,0.81275,"Missing"
2012.amta-commercial.8,2012.eamt-1.41,1,0.879758,"Missing"
2012.amta-commercial.8,N09-2055,0,0.0424568,"Missing"
2012.amta-commercial.8,W07-0712,0,0.0189929,"g how we build customized engines. These are SMT systems built offline by experienced ALS LT engineers, and guaranteed to outperform the (very good) SmartMATE baseline, with far greater pre- and post-processing, and the incorporation of a feedback and review phase. 3.1 Customized Engine Builds The ALS MT technology comprises a number of components, including parallel corpus extraction, pre-processing, corpus cleaning, training data preparation, model training, tuning, translation and postprocessing. While many of these processes rely on standard tools such as Giza++ (Och & Ney, 2003), IRSTLM (Federico & Cettolo, 2007), MERT (Och, 2003) and Moses (Koehn et al., 2007) – all areas where we have a proven track record, as demonstrated by our publications on word and phrase alignment, language modelling, tuning and decoding4 – much of the success gained by our offering relies on the large number of pre- and post-processing routines that we have developed. While we will not go into too much detail for obvious reasons, despite competitors’ claims to own the ‘clean data’ space, we are confident that no suppliers pre-process our clients’ data to the extent that we do. 3.1.1 Data Cleaning In order to prepare good-qua"
2012.amta-commercial.8,2010.amta-commercial.5,0,0.481359,"e onstream. 2 Related Work Despite the obvious benefits in translating usergenerated content, there do not appear to have been many published attempts at doing so. Hecht & Gergle (2010) examine how knowledge representation differs in 25 different language editions of Wikipedia. They note that this diversity is greater than has been presumed to date, which will impact on applications that use Wikipedia as a knowledge-source. While they hypothesize how knowledge diversity can be leveraged to create “culturally-aware applications” and “hyperlingual applications”, they do not address MT directly. Flournoy & Rueppel (2010) state that “Adobe is working to develop richer community-derived resources in many markets, including community translations and user community forums. MT has natural integration with both scenarios,” without stating precisely how they aim to go about this. Nonetheless, they do provide a useful table which summarizes the requirements for community-based translation, namely: • Quality: Low-medium, • Purpose: Gisting, • Customization: Varied subject matter, • ROI: Difficult to calculate, • Security: Low, • Language Pairs: XX→XX, Primarily EN→XX; also • Input quality: Varied, uncontrolled. More"
2012.amta-commercial.8,D07-1103,0,0.0472062,"oved greatly by 5 Prior to coming to ALS, similar related work involved the SMT systems we built for the 2010 World Cup – http://www.computing.dcu.ie/news/ cngl-launch-world-cup-twanslation-service – where in just a few days, we built 12 engines from scratch to translate in real time online tweets with the WC2010 hash tag. See Lewis (2010) for a similar time-constrained application. 6 http://opus.lingfil.uu.se mining data from tweets and similar sources. In order to ensure fast runtime performance, we massively pruned the phrase-tables with only a small degradation in translation quality (cf. Johnson et al., 2007). Once we have clean parallel data, we perform seven further stages of pre-processing, which includes ensuring the correct text encoding, handling URLs and other special characters (e.g. pipes, quotation marks, brackets), as well as the usual MT processes of tokenisation and lowercasing. 3.2 Adjustments to Pre-Processing In the introduction, we described three main scenarios where we encounter wrong or ‘non-standard’ user-generated source-language content. Where typos have been made owing to authors entering the text too fast, we can use a spellchecker if what has been typed is ‘not too far aw"
2012.amta-commercial.8,2010.eamt-1.37,0,0.043609,"Missing"
2012.amta-commercial.8,2012.eamt-1.11,0,0.0226735,"sources in many markets, including community translations and user community forums. MT has natural integration with both scenarios,” without stating precisely how they aim to go about this. Nonetheless, they do provide a useful table which summarizes the requirements for community-based translation, namely: • Quality: Low-medium, • Purpose: Gisting, • Customization: Varied subject matter, • ROI: Difficult to calculate, • Security: Low, • Language Pairs: XX→XX, Primarily EN→XX; also • Input quality: Varied, uncontrolled. More related to the topic of this paper, Roturier & Bensadoun (2011) and Mitchell & Roturier (2012) discuss ways in which machine-translated user-generated content can be best evaluated. In the former, four MT systems – Microsoft Translator, Systran, “a third-party commercial SMT system that was customized using Symantec translation memories”, and a system called ‘VICTOR’, “a standard phrase-based SMT system trained using Moses” (Koehn et al., 2007) with some extra pre-processing components – were compared using a range of automatic MT evaluation metrics in order to evaluate their suitability in translating user-generated content. In contrast, Mitchell & Roturier (2012) examine the perceive"
2012.amta-commercial.8,P03-1021,0,0.00777337,". These are SMT systems built offline by experienced ALS LT engineers, and guaranteed to outperform the (very good) SmartMATE baseline, with far greater pre- and post-processing, and the incorporation of a feedback and review phase. 3.1 Customized Engine Builds The ALS MT technology comprises a number of components, including parallel corpus extraction, pre-processing, corpus cleaning, training data preparation, model training, tuning, translation and postprocessing. While many of these processes rely on standard tools such as Giza++ (Och & Ney, 2003), IRSTLM (Federico & Cettolo, 2007), MERT (Och, 2003) and Moses (Koehn et al., 2007) – all areas where we have a proven track record, as demonstrated by our publications on word and phrase alignment, language modelling, tuning and decoding4 – much of the success gained by our offering relies on the large number of pre- and post-processing routines that we have developed. While we will not go into too much detail for obvious reasons, despite competitors’ claims to own the ‘clean data’ space, we are confident that no suppliers pre-process our clients’ data to the extent that we do. 3.1.1 Data Cleaning In order to prepare good-quality training mate"
2012.amta-commercial.8,J03-1002,0,0.00456011,"trate instead on describing how we build customized engines. These are SMT systems built offline by experienced ALS LT engineers, and guaranteed to outperform the (very good) SmartMATE baseline, with far greater pre- and post-processing, and the incorporation of a feedback and review phase. 3.1 Customized Engine Builds The ALS MT technology comprises a number of components, including parallel corpus extraction, pre-processing, corpus cleaning, training data preparation, model training, tuning, translation and postprocessing. While many of these processes rely on standard tools such as Giza++ (Och & Ney, 2003), IRSTLM (Federico & Cettolo, 2007), MERT (Och, 2003) and Moses (Koehn et al., 2007) – all areas where we have a proven track record, as demonstrated by our publications on word and phrase alignment, language modelling, tuning and decoding4 – much of the success gained by our offering relies on the large number of pre- and post-processing routines that we have developed. While we will not go into too much detail for obvious reasons, despite competitors’ claims to own the ‘clean data’ space, we are confident that no suppliers pre-process our clients’ data to the extent that we do. 3.1.1 Data Cl"
2012.amta-commercial.8,P02-1040,0,0.0831062,"nhance our source text correction method (cf. Section 3.2), and part of this dictionary is also used to generate a slang glossary to ensure that colloquialisms are translated correctly. We explain the use of glossaries in Section 4.3, as well as discussing some of the translation edits suggested by the users of the featured social network. 4.2 Automatic Evaluation Results Once we had built excellent quality MT systems for the social network provider (cf. Section 3.1), we deployed these inside our SmartMATE toolkit so that they could be accessed by the client via our API. Table 4 reports BLEU (Papineni et al., 2002) scores for forward (F) and reversed (R) MT systems for each language-pair obtained on the test sets, i.e. for English⇔Russian in the first line in Table 4, English→Russian would be the ‘forward’ system, with Russian→English being the ‘reversed’ engine. Systems BLEU (F) BLEU (R) English⇔Russian 86.49 91.01 English⇔Arabic 71.10 88.39 English⇔Turkish 79.65 80.78 Arabic⇔Russian 78.29 72.30 Arabic⇔Turkish 72.07 68.06 Russian⇔Turkish 90.54 88.72 Table 4: BLEU scores of the forward (F) and reversed (R) MT systems. We can see from Table 4 that the BLEU scores on 1000 held-out sentences from the data"
2012.amta-commercial.8,2012.amta-wptp.6,1,0.831295,"al characters (e.g. pipes, quotation marks, brackets), as well as the usual MT processes of tokenisation and lowercasing. 3.2 Adjustments to Pre-Processing In the introduction, we described three main scenarios where we encounter wrong or ‘non-standard’ user-generated source-language content. Where typos have been made owing to authors entering the text too fast, we can use a spellchecker if what has been typed is ‘not too far away’ from what was intended (as measured by edit-distance: Levenshtein, 1966). For the more general case of poor nonnative competence, in another application scenario (Penkale & Way, 2012), we were provided with a reasonably large collection of original mangled English and the edited versions, and we treated this as an MT task in its own right, in the same way that SPE works, i.e. the ‘bad’ original English was the source language, and the ‘good’ post-edited English was the target, and the MT system learnt how to correct many of the errors in the source automatically. This dramatically cut down on the costs for our client. Of course, as we receive more and Bilingual training corpora language-pairs Sentences Words (S) English⇔Russian 3,409,848 17,954,459 English⇔Arabic 3,290,746"
2012.amta-commercial.8,2011.mtsummit-papers.27,0,0.109564,"elop richer community-derived resources in many markets, including community translations and user community forums. MT has natural integration with both scenarios,” without stating precisely how they aim to go about this. Nonetheless, they do provide a useful table which summarizes the requirements for community-based translation, namely: • Quality: Low-medium, • Purpose: Gisting, • Customization: Varied subject matter, • ROI: Difficult to calculate, • Security: Low, • Language Pairs: XX→XX, Primarily EN→XX; also • Input quality: Varied, uncontrolled. More related to the topic of this paper, Roturier & Bensadoun (2011) and Mitchell & Roturier (2012) discuss ways in which machine-translated user-generated content can be best evaluated. In the former, four MT systems – Microsoft Translator, Systran, “a third-party commercial SMT system that was customized using Symantec translation memories”, and a system called ‘VICTOR’, “a standard phrase-based SMT system trained using Moses” (Koehn et al., 2007) with some extra pre-processing components – were compared using a range of automatic MT evaluation metrics in order to evaluate their suitability in translating user-generated content. In contrast, Mitchell & Rotur"
2012.amta-commercial.8,W07-0728,0,0.0115033,"have been pointless, as we would have suffered from the ‘garbage in garbage out’ problem. There are essentially two use-case scenarios: 1. Much of the content is produced by non-native speakers, so the source-language data can be of very poor quality. In this case, we need to translate this into ‘good’ English prior to translation. This ‘monolingual translation’ is the theme of one of the workshops at AMTA-2012, and the pre-editing task we are confronted with here can perhaps be seen as the inverse of the statistical post-editing (SPE) solutions proposed a few years ago (Dugast et al., 2007; Simard et al., 2007). 2. In contrast, source content is authored by native speakers, where the author: (a) either entered the text too fast and so made typographical errors, or (b) deliberately departed from spelling norms to bring about some linguistic effect. In the remainder of this paper, we provide an overview of related work in this area in Section 2. In Section 3, we give an overview of how the ALS statistical MT engines are built, focusing specifically on how the ‘regular’ pre-processing techniques needed to be extended to cope with the above problems, including dealing with shortforms, acronyms, typos, p"
2012.amta-commercial.8,tiedemann-2012-parallel,0,0.0110882,"ty translation results may be achieved, despite reducing the amount of training data. What is quite clear from our engine development is that, contrary to the often heard mantra that ‘more data is better data’, it is more important to do better with less data. 3.1.2 Data Pre-processing For the social network scenario, initially there was no client-specific parallel data for us to use,5 but they were able to provide us with a fair quantity of monolingual data in a range of languages which we were able to use to help improve our language models (LMs). Initially we used mainly OPUS6 sub-corpora (Tiedemann, 2012) as parallel training data of a ‘similar’ type, since we extracted sentencepairs that were constrained by length. This required a large amount of corpus cleaning to remove badly aligned sentences. Target LMs improved greatly by 5 Prior to coming to ALS, similar related work involved the SMT systems we built for the 2010 World Cup – http://www.computing.dcu.ie/news/ cngl-launch-world-cup-twanslation-service – where in just a few days, we built 12 engines from scratch to translate in real time online tweets with the WC2010 hash tag. See Lewis (2010) for a similar time-constrained application. 6"
2012.amta-monomt.2,2011.mtsummit-papers.32,1,0.848094,"Missing"
2012.amta-monomt.2,W11-2131,0,0.0535794,"Missing"
2012.amta-monomt.2,2011.eamt-1.5,1,0.88212,"Missing"
2012.amta-monomt.2,eck-etal-2004-language,0,0.0150169,"solutions are either to use one single generic engine or 34 domain-specific engines, but these are unlikely to be the best ways forward for eBay. Accordingly, we employ data clustering techniques to identify optimal clusters based on the 34 categories for SMT engine-building. 3.1 Since our aim was to find optimal clusters for MT engine building, we chose the LM perplexity feature instead of the other two, because it is closely related to SMT performance: the lower the LM perplexity, the better the MT engine’s performance with respect to translation quality, as has been widely reported (e.g. (Eck et al., 2004; Foster & Kuhn, 2007)). We also chose hierarchical clustering as we want the clustering results to be stable as opposed to changing over time. Clustering features and algorithms We used three different features to perform clustering on the eBay monolingual data, namely: 21 Specifically, LM perplexity is calculated by dividing the data in each of the categories into independent training and testing sections, with random sampling from corresponding items. Language models are built on the training data, and then both withinand cross-category LM perplexities are calculated on the test data. All t"
2012.amta-monomt.2,W07-0717,0,0.0508971,"her to use one single generic engine or 34 domain-specific engines, but these are unlikely to be the best ways forward for eBay. Accordingly, we employ data clustering techniques to identify optimal clusters based on the 34 categories for SMT engine-building. 3.1 Since our aim was to find optimal clusters for MT engine building, we chose the LM perplexity feature instead of the other two, because it is closely related to SMT performance: the lower the LM perplexity, the better the MT engine’s performance with respect to translation quality, as has been widely reported (e.g. (Eck et al., 2004; Foster & Kuhn, 2007)). We also chose hierarchical clustering as we want the clustering results to be stable as opposed to changing over time. Clustering features and algorithms We used three different features to perform clustering on the eBay monolingual data, namely: 21 Specifically, LM perplexity is calculated by dividing the data in each of the categories into independent training and testing sections, with random sampling from corresponding items. Language models are built on the training data, and then both withinand cross-category LM perplexities are calculated on the test data. All the perplexity scores a"
2012.amta-monomt.2,P07-2045,0,0.00366075,"Missing"
2012.amta-monomt.2,N03-2016,0,0.0532092,"Missing"
2012.amta-monomt.2,2011.eamt-1.40,1,0.894972,"Missing"
2012.amta-monomt.2,D08-1090,0,0.0441392,"Missing"
2012.amta-monomt.2,2010.amta-commercial.14,1,0.83552,"Missing"
2012.amta-monomt.2,E12-1083,0,0.0225782,"Missing"
2012.amta-papers.1,W00-0508,0,0.115751,"Missing"
2012.amta-papers.1,J07-2003,0,0.374403,"n IWLST'10 EnglishChinese translation task shows a significant improvement in translation quality. In this paper, results for HPB-SMT are compared with previously published results of phrase-based statistical machine translation (PB-SMT) system (Baseline). The HPB-SMT system outperforms PB-SMT in this regard. 1 Introduction Hierarchical phrase-based statistical machine translation (HPB-SMT) is a statistical machine translation (SMT) approach which uses syntactic information of a language-pair for translation. The translation hypotheses are generated based on hierarchical rules in CYK parsing (Chiang, 2007). Although, the HPBSMT makes use of syntactic information but it does not require syntactically annotated resources during training as syntactic information is automatically inferred from the training data. This makes the training process fully automatic without any knowledge of language-pair. MT systems utilizing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based"
2012.amta-papers.1,2010.eamt-1.32,1,0.887236,"Missing"
2012.amta-papers.1,P08-1115,0,0.0167207,"ided by G2P is used for rule transformation. The case of multiple pronunciations is easily handled by phone confusion network at run-time. An example of an original and a transformed rule table is shown in table 2. Similar kind of transformation can be performed on target-side entry of rule with target language G2P, if it is intended to get the phonetic form of target words for speech synthesis purpose. 3.1 Confusion Network Translation All of the state-of-the-art syntax-based decoders for machine translation (Dyer et al., 2010; Weese et al., 2011) have a facility to operate on word lattices (Dyer et al., 2008). A confusion network is also a type of lattice that has the peculiarity that each path from the start node to the end node goes through all the other nodes and may contain an additional arc labelled ∗delete∗ to skip unwanted item in an input string. To translate confusion networks, two rules are further introduced in the hierarchical phrase grammar as shown below. X → ⟨X ∗ delete∗, X⟩ X → ⟨∗delete ∗ X, X⟩ 4 Experiment and Evaluation The HPB-SMT phone MT system is evaluated on the IWSLT 2010 English-Chinese corpus1 . The corpus contains spoken dialogues related to the travel domain. The select"
2012.amta-papers.1,P10-4002,0,0.0122939,"have mn corresponding rules. Therefore, only base form of a word pronunciation provided by G2P is used for rule transformation. The case of multiple pronunciations is easily handled by phone confusion network at run-time. An example of an original and a transformed rule table is shown in table 2. Similar kind of transformation can be performed on target-side entry of rule with target language G2P, if it is intended to get the phonetic form of target words for speech synthesis purpose. 3.1 Confusion Network Translation All of the state-of-the-art syntax-based decoders for machine translation (Dyer et al., 2010; Weese et al., 2011) have a facility to operate on word lattices (Dyer et al., 2008). A confusion network is also a type of lattice that has the peculiarity that each path from the start node to the end node goes through all the other nodes and may contain an additional arc labelled ∗delete∗ to skip unwanted item in an input string. To translate confusion networks, two rules are further introduced in the hierarchical phrase grammar as shown below. X → ⟨X ∗ delete∗, X⟩ X → ⟨∗delete ∗ X, X⟩ 4 Experiment and Evaluation The HPB-SMT phone MT system is evaluated on the IWSLT 2010 English-Chinese co"
2012.amta-papers.1,2011.iwslt-evaluation.1,0,0.0122821,"izing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based SMT (PB-SMT) (Koehn et al., 2003; Och and Ney, 2004) is their power of translation between dissimilar language pair e.g. English-Chinese. They are also being applied for the speech translation task as highlighted by the significant number of systems participating in recent IWSLT workshops (Paul et al., 2010; Federico et al., 2011) which are in oneway or another making use of syntactic information during translation. Much of the improvement in these systems is mostly because of an improvement in the text based translation technique while keeping the automatic speech recognizer (ASR) as a black-box; providing 1-best word output, confusion networks or lattices as input to the MT system. The technique proposed in this paper goes one step further. It uses the phonetic knowledge from ASR to help improve the speech translation quality for HPBSMT. The most commonly followed method for developing a speech translation system is"
2012.amta-papers.1,2011.mtsummit-papers.7,1,0.856428,"rated such that speech recognition and translation are done in single step. A finite state transducer (Bangalore and Riccardi, 2000; Casacuberta et al., 2001; Mathias and Byrne, 2006) is widely used for this task. This translation approach is similar to speech recognition except that the system outputs text in the target language. This approach has the same problem as speech recognition for large vocabularies. All of the previous studies in speech translation use the word as a basic unit for translation. Recently, an approach for speech translation from phoneticrepresentation was proposed in (Jiang et al., 2011). In this approach, PB-SMT is used for translation of the source language from a phone sequence directly into the target text. The approach uses a confusion network (CN) to deal with phonetic confusions. It has outperformed the MT system operating on word input as highlighted by the results presented in the paper. 1.1 Motivation for HPB-SMT In this paper, a new paradigm for phonetic representation-based speech translation is presented which uses a HPB-SMT technique. For the systems working on text-based translation, HPB-SMT has been shown to perform better than PB-SMT on dissimilar language pa"
2012.amta-papers.1,N03-1017,0,0.00472952,"e HPBSMT makes use of syntactic information but it does not require syntactically annotated resources during training as syntactic information is automatically inferred from the training data. This makes the training process fully automatic without any knowledge of language-pair. MT systems utilizing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based SMT (PB-SMT) (Koehn et al., 2003; Och and Ney, 2004) is their power of translation between dissimilar language pair e.g. English-Chinese. They are also being applied for the speech translation task as highlighted by the significant number of systems participating in recent IWSLT workshops (Paul et al., 2010; Federico et al., 2011) which are in oneway or another making use of syntactic information during translation. Much of the improvement in these systems is mostly because of an improvement in the text based translation technique while keeping the automatic speech recognizer (ASR) as a black-box; providing 1-best word outpu"
2012.amta-papers.1,P07-2045,0,0.00947621,"Missing"
2012.amta-papers.1,C08-1064,0,0.0295697,"Missing"
2012.amta-papers.1,P04-1083,0,0.01459,"slation hypotheses are generated based on hierarchical rules in CYK parsing (Chiang, 2007). Although, the HPBSMT makes use of syntactic information but it does not require syntactically annotated resources during training as syntactic information is automatically inferred from the training data. This makes the training process fully automatic without any knowledge of language-pair. MT systems utilizing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based SMT (PB-SMT) (Koehn et al., 2003; Och and Ney, 2004) is their power of translation between dissimilar language pair e.g. English-Chinese. They are also being applied for the speech translation task as highlighted by the significant number of systems participating in recent IWSLT workshops (Paul et al., 2010; Federico et al., 2011) which are in oneway or another making use of syntactic information during translation. Much of the improvement in these systems is mostly because of an improvement in the text based translat"
2012.amta-papers.1,J04-4002,0,0.0778903,"f syntactic information but it does not require syntactically annotated resources during training as syntactic information is automatically inferred from the training data. This makes the training process fully automatic without any knowledge of language-pair. MT systems utilizing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based SMT (PB-SMT) (Koehn et al., 2003; Och and Ney, 2004) is their power of translation between dissimilar language pair e.g. English-Chinese. They are also being applied for the speech translation task as highlighted by the significant number of systems participating in recent IWSLT workshops (Paul et al., 2010; Federico et al., 2011) which are in oneway or another making use of syntactic information during translation. Much of the improvement in these systems is mostly because of an improvement in the text based translation technique while keeping the automatic speech recognizer (ASR) as a black-box; providing 1-best word output, confusion network"
2012.amta-papers.1,P03-1021,0,0.00450851,"the one described in (Jiang et al., 2011). For HPB-SMT systems, the following training steps are followed. • standard HPB-SMT training is performed on the parallel corpus at word-level. 2 http://www.speech.cs.cmu.edu/cgi-bin/cmudict • the 5-gram language model is trained on the target language using SRILM (Stolcke, 2002). • the features used for word-level systems are phrase probability P (e|f ), lexical probability lexprob(e|f ), inverse lexical probability lexprob(f |e), word penalty and language model. • the word-level system feature weights are optimized on the development set using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as evaluation criteria. • After optimising the feature weights, rule table phonetic transformation is performed to make the model work on phonetic input. • Now the model works on phone-level input. The feature weights are further optimized for phone MT using one more MERT operation on the development set using a span-limit3 reasonable for phone sequence input. For this purpose, the development set is also transformed into the phonetic form using G2P. To work on the confusion network, each input in the development set needs to be transformed into a phone confu"
2012.amta-papers.1,P02-1040,0,0.0849592,"Jiang et al., 2011). For HPB-SMT systems, the following training steps are followed. • standard HPB-SMT training is performed on the parallel corpus at word-level. 2 http://www.speech.cs.cmu.edu/cgi-bin/cmudict • the 5-gram language model is trained on the target language using SRILM (Stolcke, 2002). • the features used for word-level systems are phrase probability P (e|f ), lexical probability lexprob(e|f ), inverse lexical probability lexprob(f |e), word penalty and language model. • the word-level system feature weights are optimized on the development set using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as evaluation criteria. • After optimising the feature weights, rule table phonetic transformation is performed to make the model work on phonetic input. • Now the model works on phone-level input. The feature weights are further optimized for phone MT using one more MERT operation on the development set using a span-limit3 reasonable for phone sequence input. For this purpose, the development set is also transformed into the phonetic form using G2P. To work on the confusion network, each input in the development set needs to be transformed into a phone confusion network as described in secti"
2012.amta-papers.1,2010.iwslt-evaluation.1,0,0.0221154,"ir. MT systems utilizing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based SMT (PB-SMT) (Koehn et al., 2003; Och and Ney, 2004) is their power of translation between dissimilar language pair e.g. English-Chinese. They are also being applied for the speech translation task as highlighted by the significant number of systems participating in recent IWSLT workshops (Paul et al., 2010; Federico et al., 2011) which are in oneway or another making use of syntactic information during translation. Much of the improvement in these systems is mostly because of an improvement in the text based translation technique while keeping the automatic speech recognizer (ASR) as a black-box; providing 1-best word output, confusion networks or lattices as input to the MT system. The technique proposed in this paper goes one step further. It uses the phonetic knowledge from ASR to help improve the speech translation quality for HPBSMT. The most commonly followed method for developing a speec"
2012.amta-papers.1,W11-2160,0,0.098525,"n hierarchical rules in CYK parsing (Chiang, 2007). Although, the HPBSMT makes use of syntactic information but it does not require syntactically annotated resources during training as syntactic information is automatically inferred from the training data. This makes the training process fully automatic without any knowledge of language-pair. MT systems utilizing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based SMT (PB-SMT) (Koehn et al., 2003; Och and Ney, 2004) is their power of translation between dissimilar language pair e.g. English-Chinese. They are also being applied for the speech translation task as highlighted by the significant number of systems participating in recent IWSLT workshops (Paul et al., 2010; Federico et al., 2011) which are in oneway or another making use of syntactic information during translation. Much of the improvement in these systems is mostly because of an improvement in the text based translation technique while keeping the automatic spee"
2012.amta-papers.1,P02-1039,0,0.0475349,"ses are generated based on hierarchical rules in CYK parsing (Chiang, 2007). Although, the HPBSMT makes use of syntactic information but it does not require syntactically annotated resources during training as syntactic information is automatically inferred from the training data. This makes the training process fully automatic without any knowledge of language-pair. MT systems utilizing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based SMT (PB-SMT) (Koehn et al., 2003; Och and Ney, 2004) is their power of translation between dissimilar language pair e.g. English-Chinese. They are also being applied for the speech translation task as highlighted by the significant number of systems participating in recent IWSLT workshops (Paul et al., 2010; Federico et al., 2011) which are in oneway or another making use of syntactic information during translation. Much of the improvement in these systems is mostly because of an improvement in the text based translation technique while keepi"
2012.amta-papers.1,C04-1168,0,0.0190154,"gnizer (ASR) as a black-box; providing 1-best word output, confusion networks or lattices as input to the MT system. The technique proposed in this paper goes one step further. It uses the phonetic knowledge from ASR to help improve the speech translation quality for HPBSMT. The most commonly followed method for developing a speech translation system is the cascade approach. In this approach, ASR, MT and speech synthesis systems are used as black-boxes for each other. The basic unit of information sharing between these components is word i.e the speech is fed into ASR to obtain 1-best,n-best (Zhang et al., 2004) lists, word lattices (Matusov and Ney, 2011; Matusov et al., 2005) or confusion networks (Bertoldi et al., 2008b) then the recognized output is translated into target language using the MT component. The target speech is then synthesized using a speech synthesis component. Except for input-output, there is no information sharing between these components. This approach is straight forward to implement and improvement can be obtained by individually improving each component. But, there are still some drawbacks for cascade approach. • Most of the linguistic information (Phrases, Syntax etc.) abo"
2012.amta-papers.1,W06-3119,0,0.0189052,"pair for translation. The translation hypotheses are generated based on hierarchical rules in CYK parsing (Chiang, 2007). Although, the HPBSMT makes use of syntactic information but it does not require syntactically annotated resources during training as syntactic information is automatically inferred from the training data. This makes the training process fully automatic without any knowledge of language-pair. MT systems utilizing syntactic knowledge have had significant success in recent year for text based translation tasks. The strength of a system using syntactic knowledge (Chiang, 2007; Zollmann and Venugopal, 2006; Melamed, 2004; Yamada and Knight, 2002; Weese et al., 2011) over the simpler phrase-based SMT (PB-SMT) (Koehn et al., 2003; Och and Ney, 2004) is their power of translation between dissimilar language pair e.g. English-Chinese. They are also being applied for the speech translation task as highlighted by the significant number of systems participating in recent IWSLT workshops (Paul et al., 2010; Federico et al., 2011) which are in oneway or another making use of syntactic information during translation. Much of the improvement in these systems is mostly because of an improvement in the text"
2012.amta-papers.1,C08-1144,0,0.0372466,"Missing"
2012.amta-wptp.6,aziz-etal-2012-pet,0,0.0278301,"Missing"
2012.amta-wptp.6,W07-0712,0,0.0642531,"then presented with the built engine along with automatically obtained BLEU (Papineni et al., 2002) scores, which are calculated over the 1,000 randomly held-out sentence pairs and which give an indication of the level of translation quality that could be expected from this engine when used to translate documents of a nature similar to those used when training the engine. The process of building an engine involves creating phrase-based translation models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2005) as well as a Language Model (LM), for which the IRSTLM toolkit (Federico and Cettolo, 2007) is used. In addition, the model weights are optimized using Minimum Error Rate Training (Och, 2003) so as to maximize the BLEU score over the 500 sentence pairs randomly held out from the original TMs for tuning. All of this complexity, as well as the significant hardware requirements needed to host the engine training, are hidden from the user. It is worth noting that since these engines have been built using the user’s own data, they are specialized engines from which a better translation quality can be expected5 when compared to generalpurpose engines such as those provided by services suc"
2012.amta-wptp.6,D10-1044,0,0.0226769,"Missing"
2012.amta-wptp.6,N03-1017,0,0.00537198,"nal 500 sentence pairs for tuning. The remaining data is used to train SMT models using the Moses (Koehn et al., 2007) toolkit. The user is then presented with the built engine along with automatically obtained BLEU (Papineni et al., 2002) scores, which are calculated over the 1,000 randomly held-out sentence pairs and which give an indication of the level of translation quality that could be expected from this engine when used to translate documents of a nature similar to those used when training the engine. The process of building an engine involves creating phrase-based translation models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2005) as well as a Language Model (LM), for which the IRSTLM toolkit (Federico and Cettolo, 2007) is used. In addition, the model weights are optimized using Minimum Error Rate Training (Och, 2003) so as to maximize the BLEU score over the 500 sentence pairs randomly held out from the original TMs for tuning. All of this complexity, as well as the significant hardware requirements needed to host the engine training, are hidden from the user. It is worth noting that since these engines have been built using the user’s own data, they are speciali"
2012.amta-wptp.6,2005.iwslt-1.8,0,0.0217005,"s used to train SMT models using the Moses (Koehn et al., 2007) toolkit. The user is then presented with the built engine along with automatically obtained BLEU (Papineni et al., 2002) scores, which are calculated over the 1,000 randomly held-out sentence pairs and which give an indication of the level of translation quality that could be expected from this engine when used to translate documents of a nature similar to those used when training the engine. The process of building an engine involves creating phrase-based translation models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2005) as well as a Language Model (LM), for which the IRSTLM toolkit (Federico and Cettolo, 2007) is used. In addition, the model weights are optimized using Minimum Error Rate Training (Och, 2003) so as to maximize the BLEU score over the 500 sentence pairs randomly held out from the original TMs for tuning. All of this complexity, as well as the significant hardware requirements needed to host the engine training, are hidden from the user. It is worth noting that since these engines have been built using the user’s own data, they are specialized engines from which a better translation quality can"
2012.amta-wptp.6,P07-2045,0,0.0178603,"ing the correct character encodings are being used, • removing any formatting tags so that they do not interfere with the training process, • removing duplicate sentence pairs, • removing sentence pairs which exceed certain source:target length ratio, • replacing entities such as URLs and e-mails with placeholders to improve the generalization of the statistical models. After the corpus has been cleaned, 1,000 randomly selected sentence pairs are kept apart for evaluation purposes, and an additional 500 sentence pairs for tuning. The remaining data is used to train SMT models using the Moses (Koehn et al., 2007) toolkit. The user is then presented with the built engine along with automatically obtained BLEU (Papineni et al., 2002) scores, which are calculated over the 1,000 randomly held-out sentence pairs and which give an indication of the level of translation quality that could be expected from this engine when used to translate documents of a nature similar to those used when training the engine. The process of building an engine involves creating phrase-based translation models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2005) as well as a Language Model (LM), for which"
2012.amta-wptp.6,C00-2163,0,0.0619705,"dsheet software, or the standard TBX (TermBase eXchange) (ISO 30042, 2008). These glossaries can be exploited in several ways. Firstly they can be used as a complement of TMX files during MT engine building. This has the effect of improving word alignment (and subsequently 5 This is mainly due to the ambiguity introduced by out-ofdomain data (Sennrich, 2012), and is a known effect in the domain adaptation literature, e.g. (Foster et al., 2010) 6 http://translate.google.com 7 http://www.microsofttranslator.com phrase-alignment), as it provides reference points for the SMT alignment algorithms (Och and Ney, 2000). Secondly, they can be used for glossaryinjection during MT. Once an engine has been trained, glossaries can be used while the engine is processing an input document to ensure that the MT output adheres to the terminology specified by the glossary. When using multiple glossaries which provide conflicting entries for the same source term, all of the possible target translations are provided to the engine, which uses its LM to determine which translation option provides the most fluent target sentence. Finally, the editor suite supports the use of glossaries as well, by highlighting any source"
2012.amta-wptp.6,P03-1021,0,0.104155,"e calculated over the 1,000 randomly held-out sentence pairs and which give an indication of the level of translation quality that could be expected from this engine when used to translate documents of a nature similar to those used when training the engine. The process of building an engine involves creating phrase-based translation models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2005) as well as a Language Model (LM), for which the IRSTLM toolkit (Federico and Cettolo, 2007) is used. In addition, the model weights are optimized using Minimum Error Rate Training (Och, 2003) so as to maximize the BLEU score over the 500 sentence pairs randomly held out from the original TMs for tuning. All of this complexity, as well as the significant hardware requirements needed to host the engine training, are hidden from the user. It is worth noting that since these engines have been built using the user’s own data, they are specialized engines from which a better translation quality can be expected5 when compared to generalpurpose engines such as those provided by services such as Google Translate6 or Microsoft Bing Translator,7 which in addition might not offer the same dat"
2012.amta-wptp.6,P02-1040,0,0.0826833,"the training process, • removing duplicate sentence pairs, • removing sentence pairs which exceed certain source:target length ratio, • replacing entities such as URLs and e-mails with placeholders to improve the generalization of the statistical models. After the corpus has been cleaned, 1,000 randomly selected sentence pairs are kept apart for evaluation purposes, and an additional 500 sentence pairs for tuning. The remaining data is used to train SMT models using the Moses (Koehn et al., 2007) toolkit. The user is then presented with the built engine along with automatically obtained BLEU (Papineni et al., 2002) scores, which are calculated over the 1,000 randomly held-out sentence pairs and which give an indication of the level of translation quality that could be expected from this engine when used to translate documents of a nature similar to those used when training the engine. The process of building an engine involves creating phrase-based translation models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2005) as well as a Language Model (LM), for which the IRSTLM toolkit (Federico and Cettolo, 2007) is used. In addition, the model weights are optimized using Minimum Erro"
2012.amta-wptp.6,E12-1055,0,0.0109188,"offer the same data privacy guarantees as SmartMATE. 3.4 Terminology SmartMATE is able to import multilingual glossaries containing user-specific terminology. The accepted formats are CSV (Comma-Separated Values) files, which are obtainable from any spreadsheet software, or the standard TBX (TermBase eXchange) (ISO 30042, 2008). These glossaries can be exploited in several ways. Firstly they can be used as a complement of TMX files during MT engine building. This has the effect of improving word alignment (and subsequently 5 This is mainly due to the ambiguity introduced by out-ofdomain data (Sennrich, 2012), and is a known effect in the domain adaptation literature, e.g. (Foster et al., 2010) 6 http://translate.google.com 7 http://www.microsofttranslator.com phrase-alignment), as it provides reference points for the SMT alignment algorithms (Och and Ney, 2000). Secondly, they can be used for glossaryinjection during MT. Once an engine has been trained, glossaries can be used while the engine is processing an input document to ensure that the MT output adheres to the terminology specified by the glossary. When using multiple glossaries which provide conflicting entries for the same source term, a"
2012.eamt-1.2,2010.jec-1.7,1,0.894254,"Missing"
2012.eamt-1.41,2011.mtsummit-papers.32,1,0.894345,"Missing"
2012.eamt-1.41,P11-2071,0,0.0421364,"Missing"
2012.eamt-1.41,2005.eamt-1.19,0,0.0387288,"briefly reviews relevant related work. Section 3 provides a detailed discussion on the normalization techniques as well as the acquisition of supplementary training material. Section 4 presents the datasets and the experiments and corresponding results, followed by our conclusions and pointers to future work in Section 5. 2 Related Work The technique of using ‘out-of-domain’ datasets to supplement ‘in-domain’ training data has been widely used in domain adaptation of SMT. Information retrieval techniques were used by Eck et al. (2004) to propose a language model adaptation technique for SMT. Hildebrand et al. (2005) utilized this approach to select similar sentences from available bitext to adapt translation models, which improved translation performance. Habash (2008) used spelling expansion, morphological expansion, dictionary term expansion and proper name 170 transliteration to enhance or reuse existing phrase table entries to handle OOVs in Arabic–English MT. More recently an effort to adapt MT by mining bilingual dictionaries from comparable corpora using untranslated OOV words was carried out by Daume III and Jagarlamudi (2011). Our current line of work is related to the work reported in Daume III"
2012.eamt-1.41,W04-3250,0,0.147683,"Missing"
2012.eamt-1.41,2005.mtsummit-papers.11,0,0.038138,"y the spell checker were replaced with the highest ranking suggestion from the spell checker. As in Section 3.4, the spelling corrections were applied only to the test sets to ensure a reduction in the number of spelling error-based OOVs. 3.6 2. Supplementary Data Selection To take care of the VAL tokens which are valid words but absent in the training data, we explored techniques of mining supplementary data to improve the chances of successfully translating these tokens. We used the following freely available parallel data collections as potential sources of supplementary data: 1. Europarl (Koehn, 2005): Parallel corpus comprising of the proceedings of the European 172 Parliament. News Commentary Corpus: Released as a part of the WMT 2011 Translation Task.3 OpenOffice Corpus: Parallel documentation of the Office package from OpenOffice.org, released as part of the OPUS corpus (Tiedemann, 2009). KDE4 Corpus: A parallel corpus of the KDE4 localization files released as part of OPUS. PHP Corpus: Parallel corpus generated from multilingual PHP manuals also released as part of OPUS. OpenSubtitles2011 Corpus:4 A collection of documents released as part of OPUS. EMEA Corpus: A parallel corpus from"
2012.eamt-1.41,J03-1002,0,0.00436446,"lders, the line number and the actual token replaced. This mapping file is used later in the post-processing step to substitute the actual tokens in the position of the unique placeholders. For target sentences having multiple placeholders of the same type, the corresponding actual tokens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task w"
2012.eamt-1.41,P03-1021,0,0.0207458,"the same type, the corresponding actual tokens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task we used a combination of two off-the-shelf spelling correction toolkits. Using the ‘After the Deadline toolkit’ (AtD)5 as our primary spell checker, we also used a Java wrapper on Google’s spellchecking API6 to supplement the AtD spell"
2012.eamt-1.41,eck-etal-2004-language,0,0.354691,"Missing"
2012.eamt-1.41,P02-1040,0,0.084038,"kens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task we used a combination of two off-the-shelf spelling correction toolkits. Using the ‘After the Deadline toolkit’ (AtD)5 as our primary spell checker, we also used a Java wrapper on Google’s spellchecking API6 to supplement the AtD spell checking results. However, the ‘in-domain’ adaptation"
2012.eamt-1.41,2011.mtsummit-papers.27,1,0.836761,"12 European Association for Machine Translation. 169 multinational company, Symantec hosts its forums in different languages (English, German, French etc), but currently the content is siloed in each language. Clearly, translating the forums to make information available across languages would be beneficial for Symantec as well as its multilingual customer base. This forms the primary motivation of techniques presented here. Despite growing interest in translation of forum data (Flournoy and Rueppel, 2010), to date, surprisingly little research has actually focussed on forum data translation (Roturier and Bensadoun, 2011). Compared to professionally edited text, user-generated forum data is often more noisy, taking some liberty with commonly established grammar, punctuation and spelling norms. For our research, we use translation memory (TM) data from Symantec, which is part of their corporate documentation, professionally edited and generally conforming to the Symantec controlled language guidelines. On the other hand, our target data (forum) is only lightly moderated and does not conform to any publication quality guidelines. Hence despite being from the same IT domain, there is a significant difference in s"
2012.eamt-1.41,2010.amta-commercial.5,0,0.0905654,"provide an easy source of information and a viable alternative to traditional customer service options. Being a c 2012 European Association for Machine Translation. 169 multinational company, Symantec hosts its forums in different languages (English, German, French etc), but currently the content is siloed in each language. Clearly, translating the forums to make information available across languages would be beneficial for Symantec as well as its multilingual customer base. This forms the primary motivation of techniques presented here. Despite growing interest in translation of forum data (Flournoy and Rueppel, 2010), to date, surprisingly little research has actually focussed on forum data translation (Roturier and Bensadoun, 2011). Compared to professionally edited text, user-generated forum data is often more noisy, taking some liberty with commonly established grammar, punctuation and spelling norms. For our research, we use translation memory (TM) data from Symantec, which is part of their corporate documentation, professionally edited and generally conforming to the Symantec controlled language guidelines. On the other hand, our target data (forum) is only lightly moderated and does not conform to a"
2012.eamt-1.41,P08-2015,0,0.31078,"aterial. Section 4 presents the datasets and the experiments and corresponding results, followed by our conclusions and pointers to future work in Section 5. 2 Related Work The technique of using ‘out-of-domain’ datasets to supplement ‘in-domain’ training data has been widely used in domain adaptation of SMT. Information retrieval techniques were used by Eck et al. (2004) to propose a language model adaptation technique for SMT. Hildebrand et al. (2005) utilized this approach to select similar sentences from available bitext to adapt translation models, which improved translation performance. Habash (2008) used spelling expansion, morphological expansion, dictionary term expansion and proper name 170 transliteration to enhance or reuse existing phrase table entries to handle OOVs in Arabic–English MT. More recently an effort to adapt MT by mining bilingual dictionaries from comparable corpora using untranslated OOV words was carried out by Daume III and Jagarlamudi (2011). Our current line of work is related to the work reported in Daume III and Jagarlamudi (2011) and that of Habash (2008). In our case, however, the target domain (web-forum) is different from the training data (Symantec TMs) mo"
2012.eamt-1.41,P07-2045,0,\N,Missing
2012.eamt-1.44,2010.iwslt-papers.1,1,0.916804,"nce of the Chinese–English translation over the baseline systems. 1 Introduction Hierarchical Phrase-Based (HPB) Statistical Machine Translation (SMT) (Chiang, 2005) has been demonstrated to be one of the most successful SMT approaches nowadays. Its main idea is to imitate Context-Free Grammar (CFG) production rules in modelling translation rules while maintaining the strength of statistically extracted phrases. However, HPB SMT only models the hierarchical aspect of the language and does not use any linguistic information in rule extraction. A set of approaches (Zollmann and Venugopal, 2006; Almaghout et al., 2010) have tried to incorporate c 2012 European Association for Machine Translation. 193 Andy Way Applied Language Solutions Delph UK andy.way@appliedlanguage.com syntactic information extracted according to different grammar theories in the HPB SMT model by annotating phrases and nonterminals with syntactic labels. These systems face many challenges in integrating their syntax-based constraints with the syntax-free statistically extracted HPB SMT translation grammar, which limits the coverage of these syntactic constraints and thus minimizes the benefit obtained from applying them. In this paper,"
2012.eamt-1.44,2011.eamt-1.38,1,0.276763,"to target-side phrases and nonterminals. These labels are extracted from context-free phrase structure grammar parse trees of the targetside of the parallel corpus. The function of these syntactic labels is to impose syntactic constraints on phrases replacing nonterminals during decoding, allowing this replacement only when the labels of the nonterminal and the replacing phrase match. CCG-augmented HPB (Almaghout et al., 2010) follows the SAMT approach in labelling nonterminals with syntactic labels. It extracts CCG-based labels from CCG forest trees of the target-side of the parallel corpus. Almaghout et al. (2011) use contextual information presented in CCG categories to extract syntactic labels for nonterminals and phrases in the HPB SMT translation model. Birch et al. (2007) use CCG supertags as a source and target factor in the factored Phrase-Based (PB) SMT translation model (Koehn and Hoang, 2007). Hassan et al. (2009) integrate target-side CCG incremental parsing in the Direct Translation Model (DTM2). They also extract a set of syntactic features based on CCG supertags, combinatory operators and parsing states. This helps to build a fully connected parsing structure during decoding and prune hyp"
2012.eamt-1.44,J99-2004,0,0.486579,"d to reduce the complexity of chart decoding by limiting the application of the hierarchical rules to a certain limit (12 words in our experiments, cf. Section 6) above which only glue grammar rules are applied. Glue grammar rules can also be applied below this limit but their application cannot alternate with hierarchical rules, and they always form a left-balanced binary tree on top of the hierarchical rules in the derivation tree. 4 Combinatory Categorial Grammar CCG (Steedman, 2000) is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags, cf. Bangalore and Joshi (1999)) and a set of combinatory rules which specify how the categories are combined. A supertag is a rich syntactic description that specifies the local syntactic context of the word at the lexical level in the form of a set of arguments. Most of the CCG grammar is contained in the lexicon, which is why CCG has simpler combinatory rules compared to CFG productions. CCG categories are divided into atomic and complex categories. Examples of atomic categories are S (sentence), N (noun), NP (noun phrase), etc. Complex categories such as SNP and (SNP)/NP are functions which specify the type and direct"
2012.eamt-1.44,W07-0702,0,0.0200694,"unction of these syntactic labels is to impose syntactic constraints on phrases replacing nonterminals during decoding, allowing this replacement only when the labels of the nonterminal and the replacing phrase match. CCG-augmented HPB (Almaghout et al., 2010) follows the SAMT approach in labelling nonterminals with syntactic labels. It extracts CCG-based labels from CCG forest trees of the target-side of the parallel corpus. Almaghout et al. (2011) use contextual information presented in CCG categories to extract syntactic labels for nonterminals and phrases in the HPB SMT translation model. Birch et al. (2007) use CCG supertags as a source and target factor in the factored Phrase-Based (PB) SMT translation model (Koehn and Hoang, 2007). Hassan et al. (2009) integrate target-side CCG incremental parsing in the Direct Translation Model (DTM2). They also extract a set of syntactic features based on CCG supertags, combinatory operators and parsing states. This helps to build a fully connected parsing structure during decoding and prune hypotheses which do not constitute a valid parsing state. Recently, applying syntactic constraints in syntax-augmented HPB SMT systems in a soft manner has been demonstr"
2012.eamt-1.44,P05-1033,0,0.675755,"mented HPB SMT systems which limit the coverage of the syntactic constraints applied. We present experiments on Arabic–English and Chinese–English translation. Our experiments show that using extended CCG labels helps to increase nonterminal label coverage and achieve significant improvements over the baseline for Arabic– English translation. In addition, combining extended CCG labels with CCGaugmented glue grammar helps to improve the performance of the Chinese–English translation over the baseline systems. 1 Introduction Hierarchical Phrase-Based (HPB) Statistical Machine Translation (SMT) (Chiang, 2005) has been demonstrated to be one of the most successful SMT approaches nowadays. Its main idea is to imitate Context-Free Grammar (CFG) production rules in modelling translation rules while maintaining the strength of statistically extracted phrases. However, HPB SMT only models the hierarchical aspect of the language and does not use any linguistic information in rule extraction. A set of approaches (Zollmann and Venugopal, 2006; Almaghout et al., 2010) have tried to incorporate c 2012 European Association for Machine Translation. 193 Andy Way Applied Language Solutions Delph UK andy.way@appl"
2012.eamt-1.44,P10-1146,0,0.0401663,"sed (PB) SMT translation model (Koehn and Hoang, 2007). Hassan et al. (2009) integrate target-side CCG incremental parsing in the Direct Translation Model (DTM2). They also extract a set of syntactic features based on CCG supertags, combinatory operators and parsing states. This helps to build a fully connected parsing structure during decoding and prune hypotheses which do not constitute a valid parsing state. Recently, applying syntactic constraints in syntax-augmented HPB SMT systems in a soft manner has been demonstrated to improve the performance of these systems (Venugopal et al., 2009; Chiang, 2010). This means that the derivations which violate the syntactic constraints imposed by the model are not prevented per se, but the system learns to favour more grammatical translations. Strong syntactic constraints impose restrictions on the translation search space and consequently have a negative impact on performance. Venugopal et al. (2009) transform the syntactic constraints in the SAMT translation model to a syntactic feature integrated into the log-linear model. They use an unlabelled translation model during decoding. Another SAMT-based syntactic model, which measures the probability of"
2012.eamt-1.44,D09-1123,1,0.928268,"Missing"
2012.eamt-1.44,D07-1091,0,0.0305327,"ing this replacement only when the labels of the nonterminal and the replacing phrase match. CCG-augmented HPB (Almaghout et al., 2010) follows the SAMT approach in labelling nonterminals with syntactic labels. It extracts CCG-based labels from CCG forest trees of the target-side of the parallel corpus. Almaghout et al. (2011) use contextual information presented in CCG categories to extract syntactic labels for nonterminals and phrases in the HPB SMT translation model. Birch et al. (2007) use CCG supertags as a source and target factor in the factored Phrase-Based (PB) SMT translation model (Koehn and Hoang, 2007). Hassan et al. (2009) integrate target-side CCG incremental parsing in the Direct Translation Model (DTM2). They also extract a set of syntactic features based on CCG supertags, combinatory operators and parsing states. This helps to build a fully connected parsing structure during decoding and prune hypotheses which do not constitute a valid parsing state. Recently, applying syntactic constraints in syntax-augmented HPB SMT systems in a soft manner has been demonstrated to improve the performance of these systems (Venugopal et al., 2009; Chiang, 2010). This means that the derivations which v"
2012.eamt-1.44,N03-1017,0,0.0441852,"into the log-linear model. They use an unlabelled translation model during decoding. Another SAMT-based syntactic model, which measures the probability of different labellings of each hierarchical rule, is used to calculate the value of the syntactic feature at each nonterminal replacement during decoding. 3 Hierarchical Phrase-Based SMT HPB SMT (Chiang, 2005) is a tree-based model which extracts a synchronous CFG automatically from the training corpus. HPB SMT extracts hierarchical rules – the fundamental translation units in the HPB model – from phrases extracted according to the PB model (Koehn et al., 2003). Thus, 194 hierarchical rules have the strengths of statistically extracted continuous phrases plus the ability to translate discontinuous phrases and learn phrasereordering without a separate reordering model. The HPB SMT model has two types of rules: hierarchical rules and glue grammar rules. Hierarchical rules are rewrite rules with aligned pairs of right-hand sides, taking the following form: X →< α, β, ∼> (1) where X is a non-terminal, α and β are both strings of terminals and non-terminals, and ∼ is a one-toone correspondence between non-terminal occurrences in α and β. Hierarchical rul"
2012.eamt-1.44,W04-3250,0,0.106134,"Missing"
2012.eamt-1.44,P03-1021,0,0.00513349,"ne system which uses single-category CCG labels and applies strong syntactic constraints (Almaghout et al., 2010). We built our HPB SMT baseline system using the Moses Chart Decoder.3 The GIZA++ toolkit4 is used to perform word and phrase alignment and the “grow-diag-final” refinement method is adopted (Koehn et al., 2003). Maximum phrase length and maximum rule span are both set to 12 words. The maximum span for the chart during decoding is set to 20 words, above which only glue grammar rules are applied. Hierarchical rules extracted contain up to 2 nonterminals. Minimum error rate training (Och, 2003) is performed to tune all our SMT systems. The 5-gram language model in all experiments was trained on the target side 1 http://iwslt2010.fbk.eu/node/27 http://www1.ccls.columbia.edu/MADA/ 3 http://www.statmt.org/moses/?n=Moses.SyntaxTutorial 4 http://fjoch.com/GIZA++.html 2 of the parallel corpus using the SRILM toolkit5 with modified Kneser-Ney smoothing. Our CCGaugmented HPB system was also built using the Moses Chart Decoder, which has an option to extract syntax-augmented rules from an annotated corpus. We used the same rule extraction and decoding settings as for the HPB baseline system."
2012.eamt-1.44,N09-1027,0,0.615085,". In this paper, we try to extend the scope of target-side syntactic constraints in syntaxaugmented HPB SMT. More specifically, we try to exploit the flexibility of Combinatory Categorial Grammar (CCG) (Steedman, 2000) to increase the coverage of syntactic labels used to label phrases and nonterminals in hierarchical rules. In addition, we augment HPB glue grammar rules with CCG combinatory operators with the aim of directing the decoding process towards building a full parse tree of the translation output. We apply these constraints in a soft manner through a feature in the log-linear model (Venugopal et al., 2009). The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 gives an introduction to HPB SMT. Section 4 introduces CCG. Section 5 describes our approach. Section 6 presents our experiments. Finally, Section 7 concludes and provides avenues for future work. 2 Related Work Syntax Augmented Machine Translation (SAMT) (Zollmann and Venugopal, 2006) tries to improve the grammaticality of the HPB SMT translation output by attaching syntactic labels to target-side phrases and nonterminals. These labels are extracted from context-free phrase structure grammar parse tree"
2012.eamt-1.44,W06-3119,0,0.289598,"helps to improve the performance of the Chinese–English translation over the baseline systems. 1 Introduction Hierarchical Phrase-Based (HPB) Statistical Machine Translation (SMT) (Chiang, 2005) has been demonstrated to be one of the most successful SMT approaches nowadays. Its main idea is to imitate Context-Free Grammar (CFG) production rules in modelling translation rules while maintaining the strength of statistically extracted phrases. However, HPB SMT only models the hierarchical aspect of the language and does not use any linguistic information in rule extraction. A set of approaches (Zollmann and Venugopal, 2006; Almaghout et al., 2010) have tried to incorporate c 2012 European Association for Machine Translation. 193 Andy Way Applied Language Solutions Delph UK andy.way@appliedlanguage.com syntactic information extracted according to different grammar theories in the HPB SMT model by annotating phrases and nonterminals with syntactic labels. These systems face many challenges in integrating their syntax-based constraints with the syntax-free statistically extracted HPB SMT translation grammar, which limits the coverage of these syntactic constraints and thus minimizes the benefit obtained from apply"
2013.tc-1.12,2012.eamt-1.41,1,0.864581,"Missing"
2013.tc-1.12,W05-0909,0,0.174588,"Missing"
2013.tc-1.12,2013.mtsummit-user.3,1,0.444138,"sonable bet would be that most of them will involve handling UGC to a large extent. As we argue in Penkale & Way (2013), each of these established, new and yet-to-emerge use-cases for MT has its own level of quality; clearly, raw MT output will never be as good as human translation, so assuming that a &apos;one size fits all&apos; measure of quality will suffice is simply misguided. As we show in that companion paper, we need both dynamic, configurable quality metrics (which projects such as QTLaunchPad are trying to come up with, cf. Uszkoreit, 2013),14 as well as tools – such as Lingo24&apos;s Coach tool (Bota et al., 2013) – which allow users to set their own quality requirements. Clearly, companies that that are ahead of the posse in this regard can expect to make considerable gains over their less flexible competitors. 4. The Changing Role of the Translator Over the last 30 years or so especially, the role of the translator has changed considerably. As we noted in Bota et al. (2013:313): Given the challenges they face in their day-to-day work, most human translators today would acknowledge the critical role of technology in their workflow. However, it is fair to say that some of this technology is more highly"
2013.tc-1.12,2011.eamt-1.5,1,0.830305,"s where certification is needed, detailed product information etc. We summarize these use-cases in Table 4. About the only industrial sector where full PEMT is not appropriate is for &apos;pure&apos; marketing material, where transcreation is more appropriate. Other than this, a full PEMT service can deliver quality which is just as good as human translation, if not better, given the ability of today&apos;s MT engines to adhere rigidly to a client&apos;s glossary where this is important. A couple of use-cases where MT output can be used &apos;as is&apos; or with a certain level of post-editing are patent translation (e.g. Ceausu et al., 2011) and subtitle translation (e.g. Etchegoyhen et al., 2013). If full publishable quality is required, then only full post-editing – or even expert human translation – will suffice. However, if this is for &apos;information only&apos;, then raw MT may be fit for purpose. Note again here that the use-cases outlined for this scenario relate to content with a longer life-span, or where style is a more important consideration, compared to light post-editing. In our experience, clients are more likely to ask for light post-editing for one-off, large-scale jobs, whereas full post-editing is a more typical requir"
2013.tc-1.12,2009.mtsummit-commercial.5,0,0.0975423,"Missing"
2013.tc-1.12,2012.amta-commercial.8,1,0.647985,"are forced to (wrongly) assume human translations to be perfect when conducting automatic MT evaluation, using methods such as BLEU (Papineni et al., 2003), METEOR (Banerjee & Lavie, 2005) and the like. translate.google.com/ www.bing.com/translator To be a little more objective, there are a myriad of successful use-cases using a range of MT providers for different clients, including: • • • • • • • Adobe & ProMT (Flournoy & Duran, 2009), The Church of Jesus Christ of Latter-day Saints & Microsoft Translator Hub (Richardson, 2012), Dell & Safaba/welocalize (Lavie et al., 2013), DuDu & CapitaTI (Jiang et al., 2012a), Ford & Systran/SAIC (Plesco & Rychtyckyj, 2012), Sajan & Asia Online (Wiggins & Holmes, 2011), text&form & LucySoft (Liebscher & Senf, 2013). Another sign that MT is a mature, useful technology is that at the recent MT Summit in France, for the very first time the number of commercial attendees (both users and developers) exceeded those from academia. This is a trend that is likely to continue, with ever more commercial enterprises wanting to attend such events, including large multinational companies, LSPs and MT developers. Further evidence of the assertion that MT has arrived is provide"
2013.tc-1.12,2012.amta-monomt.2,1,0.820377,"are forced to (wrongly) assume human translations to be perfect when conducting automatic MT evaluation, using methods such as BLEU (Papineni et al., 2003), METEOR (Banerjee & Lavie, 2005) and the like. translate.google.com/ www.bing.com/translator To be a little more objective, there are a myriad of successful use-cases using a range of MT providers for different clients, including: • • • • • • • Adobe & ProMT (Flournoy & Duran, 2009), The Church of Jesus Christ of Latter-day Saints & Microsoft Translator Hub (Richardson, 2012), Dell & Safaba/welocalize (Lavie et al., 2013), DuDu & CapitaTI (Jiang et al., 2012a), Ford & Systran/SAIC (Plesco & Rychtyckyj, 2012), Sajan & Asia Online (Wiggins & Holmes, 2011), text&form & LucySoft (Liebscher & Senf, 2013). Another sign that MT is a mature, useful technology is that at the recent MT Summit in France, for the very first time the number of commercial attendees (both users and developers) exceeded those from academia. This is a trend that is likely to continue, with ever more commercial enterprises wanting to attend such events, including large multinational companies, LSPs and MT developers. Further evidence of the assertion that MT has arrived is provide"
2013.tc-1.12,2008.amta-srw.4,0,0.0419825,"have to make sure we remain relevant. That we are seen as useful and essential. There are segments of the industry that clearly think we are replaceable. What are we doing to show them otherwise? Staying in our comfort zones will not solve the dilemma. We cannot continue hiding in the sand and think that just because we do not want it, it will not happen. Rather, we must face the scary challenge posed by progress and run to catch up for the decades we have been complacent while the rest of the industry became digital, mobile and instant. Other informed opinion from translators can be seen in Lagoudaki (2008), as well as from Bellos himself – who is very pro-MT in his book – from Charlotte Brasler and Jost Zetzsche,21 and from Stephen Doherty.22 Whatever your point of view, some welcome perspective is shed on this debate by Jay Marciano, who states:23 I always find it helpful to remind myself that my job is not to provide machine translation but to provide translation. The application of appropriate workflows and technologies to that end is simply the smart way to go about it. Smart, forward-thinking translators and translation companies will thrive in this changing industry. And because MT is not"
2013.tc-1.12,2013.mtsummit-user.1,0,0.0836299,"Missing"
2013.tc-1.12,2013.mtsummit-user.13,0,0.0862303,"Missing"
2013.tc-1.12,P02-1040,0,0.0915925,"Missing"
2013.tc-1.12,2012.amta-wptp.6,1,0.761765,"exist whereby LSPs with no in-house MT offering build engines using third-party technology and market such a service as their own. Of course, UGC takes many forms, but it is all very disposable content; pretty much as soon as it is published, it becomes obsolete. In Jiang et al. (2012a), we used MT (translating hundreds of millions of words in the process) to enable online chat between correspondents where there was no mutually intelligible language. Other areas where the UGC is somewhat more permanent include forum translation (Banerjee et al., 2012), translation of content in online games (Penkale & Way, 2012), and translation of eBay product listings (Jiang et al., 2012b). One other use of MT by companies is for verification of potential user demand. Websites are translated into other languages, and the hits on these new multilingual versions are counted, with precious human resources then steered in the direction of the most popular versions for content verification and, if required, post-editing. Another is for the translation of course syllabi documentation and other educational information, as tackled in the recent Bologna FP7 project.13 For all these use-cases – at least in the initial stages"
2013.tc-1.12,2013.tc-1.13,0,0.677189,"e dark ages. 3 A big driver behind the adoption and development of translation-oriented solutions – from raw MT to fully managed translation, editing and proofreading – will be the ability to offer a range of services which are flexible enough to meet these different quality requirements. Each of the services facilitated by MT will have its own definitions of quality, dependent on the client&apos;s content and business requirements. Quality will be able to be assessed by end-users or buyers, instead of in-country reviewers. Tools will need to be developed – such as Lingo24’s Coach technology (e.g. Penkale & Way (2013), the companion paper to this one) – to facilitate fully customisable, dynamic levels of quality, which can be delivered by MT and/or Translation Memory (TM) technology as required. The remainder of this paper is organised as follows. In Section 2, despite some protestations to the contrary, we argue that the time for MT is now, but also that significant improvements will only be brought about by MT developers working closely together with translators. In Section 3, we describe various use-cases for MT, especially in light of the fact that more and more use-cases are emerging, including where"
2013.tc-1.12,2012.amta-commercial.13,0,0.0732605,"Missing"
2013.tc-1.12,2012.amta-commercial.14,0,0.0342879,"othing yet! It&apos;s a reasonably safe prediction that if some LSPs and translation tools providers cannot handle UGC, then they will undoubtedly fall behind those who can. 12 Over the next five years, the industry is likely to be confronted with a sea change, where most of the data that LSPs receive for translation will be UGC, rather than the relatively clean data they currently need to process. 12 Note also two other recent phenomena: (i) large multinational companies (such as eBay) building their own in-house MT expertise, and (ii) the rise of &apos;DIY&apos; approaches to MT (e.g. Penkale &Way (2012); Richardson (2012)). In both cases, there is a lesser role for LSPs in this space, although examples do exist whereby LSPs with no in-house MT offering build engines using third-party technology and market such a service as their own. Of course, UGC takes many forms, but it is all very disposable content; pretty much as soon as it is published, it becomes obsolete. In Jiang et al. (2012a), we used MT (translating hundreds of millions of words in the process) to enable online chat between correspondents where there was no mutually intelligible language. Other areas where the UGC is somewhat more permanent includ"
2013.tc-1.12,U05-1019,0,0.0279364,"hat differs markedly from that which you started out with. It doesn&apos;t always work, as you&apos;d expect. I typed in “Machine Translation is a very useful tool.”, and via Japanese (“ 機械翻訳は非常に便利なツールです。 ” ), equilibrium was reached after just two cycles with the output “Machine Translation is a very useful tool.”, clearly not what the developers had hoped for, and demonstrating simply that where MT critics are concerned, some people have too much time on their hands! Leaving aside for one minute that back translation itself has been demonstrated to be an untrustworthy method to use for MT evaluation (Somers, 2005), I could use any tool – that&apos;s all MT is, not some panacea for all translation problems – in the wrong way in which its designers had intended and show that it was useless. As an example, I could pour orange juice into my toaster, and mock its unsuitability as a glass. But that&apos;s not what toasters are intended to do; they&apos;re for making toast! It&apos;s easy to show MT to be useless; it&apos;s just as easy to show it to be useful, but some people don&apos;t want to. Increasingly, that&apos;s their loss. Accordingly, in the next section, we map out the landscape where we believe MT can be of use, and will revisit"
2014.amta-researchers.19,P96-1041,0,0.080713,"Feature name Z 0, Z 1, Z 2, Z 3, Z 4, Z 5, Z 6, Z 7, Z 8, Z 9, Z 10 SCM non, SCM high, SCM low, SCM medium SPL 1, SPL 2, SPL 3, SPL 4, SPL 5, SPL 6, SPL 7 SEP Y, SEP N TCM non, TCM high, TCM low, TCM medium NLN 2 2, NLN 2 1, NLN 2 0, NLN 1 1, NLN 1 0, NLN 0 0 CSS non, CSS single, CSS left, CSS right, CSS both LTC non, LTC original, LTC left, LTC right, LTC both, LTC medium CPM AdjacentSame, CPM AdjacentSubstitute, CPM LinkedInterlived, CPM LinkedReversed, CPM SkipForward, CPM LinkedCorss, CPM SkipReversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed"
2014.amta-researchers.19,N12-1047,0,0.0399914,"medium CPM AdjacentSame, CPM AdjacentSubstitute, CPM LinkedInterlived, CPM LinkedReversed, CPM SkipForward, CPM LinkedCorss, CPM SkipReversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement Wang et al. (2013)’s method in Moses for comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cross-folder translation on two language p"
2014.amta-researchers.19,W13-2212,0,0.0138212,"LTC left, LTC right, LTC both, LTC medium CPM AdjacentSame, CPM AdjacentSubstitute, CPM LinkedInterlived, CPM LinkedReversed, CPM SkipForward, CPM LinkedCorss, CPM SkipReversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement Wang et al. (2013)’s method in Moses for comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cro"
2014.amta-researchers.19,P10-1064,1,0.941617,"Missing"
2014.amta-researchers.19,C10-2043,1,0.912336,"Missing"
2014.amta-researchers.19,W04-3250,0,0.0511954,"unting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement Wang et al. (2013)’s method in Moses for comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cross-folder translation on two language pairs. For the English–Chinese task, we split the training data into 50 parts and build 50 systems with the above settings by taking each part as test data and the rest as training data. Systems are tuned via the"
2014.amta-researchers.19,P07-2045,0,0.0282952,"elation between different features. 3 Our Method In this section, we present a generalized discriminative framework which can integrate TM into SMT at decoding time. Under this framework, we add features from Wang et al. (2013) into the phrase-based model as TM feature functions. In addition, we describe how to use multiple fuzzy matches efficiently to improve translation quality. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 250 3.1 Discriminative Framework Generally, in a state-of-the-art statistical translation framework like Moses (Koehn et al., 2007), the direct translation probability is given by a discriminative framework, as shown in Equation (1): PM exp{ m=1 λm hm (e, f )} (1) P (e |f ) = P PM 0 e0 exp{ m=1 λm hm (e , f )} where hm (e, f ) denotes the mth feature function for target e and source f , λm is the weight of this feature function, and M is the number of feature functions considered. This framework works well on pre-defined features, such as the translation model features and language model features, which are based on target e and source f . However, as is well-known, once these features have been induced, the training data"
2014.amta-researchers.19,N03-1017,0,0.0220482,"able JRC-Acquis corpus.1 Sentences are tokenized with scripts in Moses. We randomly select 3000 sentence pairs as dev data and 3000 as test data. We filter sentence pairs longer than 80 words in the training data and 100 words in the dev and test data. We also keep the length ratio less than or equal to 3 in all data sets. Table 1 also shows a summary of English–French corpus. 4.2 Baseline On both language-pairs, we take the phrase-based model in Moses with default settings as our baseline. Word alignment is performed by GIZA++ (Och and Ney, 2003), with heuristic function grow-diag-final-and (Koehn et al., 2003). We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of the training data with modified Kneser-Ney 1 http://ipsc.jrc.ec.europa.eu/index.php?id=198 Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 255 Feature Set Zi SCM s SPL i SEP ¯ TCM s NLN x y CSS s LTC s CPM s Feature name Z 0, Z 1, Z 2, Z 3, Z 4, Z 5, Z 6, Z 7, Z 8, Z 9, Z 10 SCM non, SCM high, SCM low, SCM medium SPL 1, SPL 2, SPL 3, SPL 4, SPL 5, SPL 6, SPL 7 SEP Y, SEP N TCM non, TCM high, TCM low, TCM medium NLN 2 2, NLN 2 1, NLN 2 0, NLN 1 1, NLN 1 0"
2014.amta-researchers.19,2010.jec-1.4,0,0.517381,"of these models. Given a large amount of data, SMT can generate better results for unseen sentences than TM. However, unless sentence-caching is utilised, it treats a seen sentence (such as a sentence in the training data) as unseen. Clearly, TM and SMT complement one another on matched and unmatched segments, so both are receiving increasing attention from translators and researchers, who would like to combine TM and SMT together to obtain better translation quality with methods such as system recommendation (He et al., 2010a,b) or using fragments from TM in SMT (Bic¸ici and Dymetman, 2008; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013) This paper is focused on integrating TM into SMT to improve translation quality. We present a discriminative framework which directly integrates TM-related feature functions into SMT. In this paper, we change features extracted from TM which are defined in a generative Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 249 model (Wang et al., 2013) to feature functions and add them into the phrase-based translation model. Experiments on English–Chinese and English–French tasks show that our method achiev"
2014.amta-researchers.19,P11-1124,1,0.888937,"Missing"
2014.amta-researchers.19,P03-1021,0,0.0154985,", Z 10 SCM non, SCM high, SCM low, SCM medium SPL 1, SPL 2, SPL 3, SPL 4, SPL 5, SPL 6, SPL 7 SEP Y, SEP N TCM non, TCM high, TCM low, TCM medium NLN 2 2, NLN 2 1, NLN 2 0, NLN 1 1, NLN 1 0, NLN 0 0 CSS non, CSS single, CSS left, CSS right, CSS both LTC non, LTC original, LTC left, LTC right, LTC both, LTC medium CPM AdjacentSame, CPM AdjacentSubstitute, CPM LinkedInterlived, CPM LinkedReversed, CPM SkipForward, CPM LinkedCorss, CPM SkipReversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 it"
2014.amta-researchers.19,J03-1002,0,0.00446471,"wn in Table 1. Our English–French data is from the publicly available JRC-Acquis corpus.1 Sentences are tokenized with scripts in Moses. We randomly select 3000 sentence pairs as dev data and 3000 as test data. We filter sentence pairs longer than 80 words in the training data and 100 words in the dev and test data. We also keep the length ratio less than or equal to 3 in all data sets. Table 1 also shows a summary of English–French corpus. 4.2 Baseline On both language-pairs, we take the phrase-based model in Moses with default settings as our baseline. Word alignment is performed by GIZA++ (Och and Ney, 2003), with heuristic function grow-diag-final-and (Koehn et al., 2003). We use SRILM (Stolcke, 2002) to train a 5-gram language model on the target side of the training data with modified Kneser-Ney 1 http://ipsc.jrc.ec.europa.eu/index.php?id=198 Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 255 Feature Set Zi SCM s SPL i SEP ¯ TCM s NLN x y CSS s LTC s CPM s Feature name Z 0, Z 1, Z 2, Z 3, Z 4, Z 5, Z 6, Z 7, Z 8, Z 9, Z 10 SCM non, SCM high, SCM low, SCM medium SPL 1, SPL 2, SPL 3, SPL 4, SPL 5, SPL 6, SPL 7 SEP Y, SEP N TCM non, TCM hig"
2014.amta-researchers.19,P02-1040,0,0.0899527,"Reversed Table 2: The list of TM features extracted on the best match in our system. discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights.2 However, when TM features are incorporated, the number of features grows to more than 50 (Table 2 show the features used in our system when only best match is considered). As MERT is known to be weak when the number of features grows (Durrani et al., 2013), we use MIRA (Cherry and Foster, 2012) instead to tune weights in this case. We set the maximum iteration of MIRA to be 25. Case-insensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement Wang et al. (2013)’s method in Moses for comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cross-folder translation on two language pairs. For the English–Chinese task, we split the training data into 50 parts and build 50 systems with the above settings by taking"
2014.amta-researchers.19,2008.amta-srw.6,0,0.0188191,"or comparison. This method needs first to train three models3 with the factored language model toolkit (Kirchhoff et al., 2007) over the feature sequence of phrase pairs. To obtain such phrase pairs for training, we do cross-folder translation on two language pairs. For the English–Chinese task, we split the training data into 50 parts and build 50 systems with the above settings by taking each part as test data and the rest as training data. Systems are tuned via the devset for the task. For the English–French task, we do 10-cross folder training. After training the systems, forced decoding (Schwartz, 2008) is used to generate the corresponding phrase segmentation on the test data. Then features are extracted on those phrase correspondences.4 We also implement our method in Moses. In this paper, training data is taken as the TM data, so phrase rules from the TM are already included during translation. After the SMT models are trained, word alignment of the TM is also produced as a by-product. 4.3 Experiment Results Table 3 shows our experiment results on two language pairs. We found that our system with TM features achieves comparable results (+0.24/+0.31 on the dev set and +0.17/-0.01 on the te"
2014.amta-researchers.19,2006.amta-papers.25,0,0.0258777,"k in Section 5. 2 Related Work As shown in experiments (e.g. Koehn and Senellart (2010) and Wang et al. (2013)), TM can give better translation than SMT for highly matched segments; SMT is more reliable than TM for other segments. Because of such complementariness, combining TM and SMT together has been explored by some researchers in recent years. He et al. (2010a) present a recommendation system which uses an SVM (Cortes and Vapnik, 1995) binary classifier to select a translation from the outputs of TM and SMT with the selected translation being more suitable to post-editing. They take TER (Snover et al., 2006) score as the measure of post-editing effort and use it to create training instances for SVM. He et al. (2010b) extend this work by re-ranking the N-best list of SMT and TM. However, these works are focused on sentence-level selection and thus the matched phrases in TM are not used so well. For an input sentence, even though it does not have an exact match in the TM, there are some matched phrases which could provide useful hints for translation. Bic¸ici and Dymetman (2008) present a dynamic TM approach which dynamically adds the longest matched noncontinuous phrase and its translation in the"
2014.amta-researchers.19,P13-1002,0,0.293096,"ta, SMT can generate better results for unseen sentences than TM. However, unless sentence-caching is utilised, it treats a seen sentence (such as a sentence in the training data) as unseen. Clearly, TM and SMT complement one another on matched and unmatched segments, so both are receiving increasing attention from translators and researchers, who would like to combine TM and SMT together to obtain better translation quality with methods such as system recommendation (He et al., 2010a,b) or using fragments from TM in SMT (Bic¸ici and Dymetman, 2008; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013) This paper is focused on integrating TM into SMT to improve translation quality. We present a discriminative framework which directly integrates TM-related feature functions into SMT. In this paper, we change features extracted from TM which are defined in a generative Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 249 model (Wang et al., 2013) to feature functions and add them into the phrase-based translation model. Experiments on English–Chinese and English–French tasks show that our method achieves comparable results with Wang et al"
2014.amta-researchers.8,D11-1033,0,0.255256,"a transductive-learning framework to increase the count of important in-domain training instances, which results in phrase-pair weights being favourable to the development set. Bic¸ici and Yuret (2011) employ a feature decay algorithm which can be used in both active learning and transductive learning settings. The decay algorithm is used to increase the variety of the training set by devaluing features that have already been seen from a training set. In recent studies, a cross-entropy difference method has seen increasing interest for the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011). The training dataset is ranked using cross-entropy difference from some language models trained on in-domain or general-domain sentences. Then a threshold is set to select the pseudo indomain sentences. The intuition is to find sentences as close to the target domain and as far from the average of the general-domain as possible. Later, Mansour et al. (2011) argue that “An LM does not capture the connections between the source and target words, and scores the sentences independently”, and linearly interpolate IBM model 1 (Brown et al., 1993) into the cross-entropy difference framework. The tr"
2014.amta-researchers.8,W11-2131,0,0.0375125,"Missing"
2014.amta-researchers.8,2011.iwslt-evaluation.18,0,0.313433,".dcu.ie liangyouli@computing.dcu.ie away@computing.dcu.ie qliu@computing.dcu.ie The CNGL Centre for Global Intelligent Content,School of Computing,Dublin City University, Ireland Abstract In this paper, we describe an effective translation model combination approach based on the estimation of a probabilistic Support Vector Machine (SVM). We collect domain knowledge from both in-domain and general-domain corpora inspired by a commonly used data selection algorithm, which we then use as features for the SVM training. Drawing on previous work on binary-featured phrase table fill-up (Nakov, 2008; Bisazza et al., 2011), we substitute the binary feature in the original work with our probabilistic domain-likeness feature. Later, we design two experiments to evaluate the proposed probabilistic feature-based approach on the French-to-English language pair using data provided at WMT07, WMT13 and IWLST11 translation tasks. Our experiments demonstrate that translation performance can gain significant improvements of up to +0.36 and +0.82 BLEU scores by using our probabilistic feature-based translation model fill-up approach compared with the binary featured fill-up approach in both experiments. 1 Introduction Like"
2014.amta-researchers.8,J93-2003,0,0.0338094,"lem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011). The training dataset is ranked using cross-entropy difference from some language models trained on in-domain or general-domain sentences. Then a threshold is set to select the pseudo indomain sentences. The intuition is to find sentences as close to the target domain and as far from the average of the general-domain as possible. Later, Mansour et al. (2011) argue that “An LM does not capture the connections between the source and target words, and scores the sentences independently”, and linearly interpolate IBM model 1 (Brown et al., 1993) into the cross-entropy difference framework. The translation performance is improved on both Arabicto-English and English-to-French translation tasks compared with the standalone cross-entropy difference approach. Applying adaptation techniques to the statistical models, especially to the translation model, is another popular approach used in domain adaptation for SMT. Some research follows the path of adding in new features into the phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by com"
2014.amta-researchers.8,P13-1126,0,0.0147256,"connections between the source and target words, and scores the sentences independently”, and linearly interpolate IBM model 1 (Brown et al., 1993) into the cross-entropy difference framework. The translation performance is improved on both Arabicto-English and English-to-French translation tasks compared with the standalone cross-entropy difference approach. Applying adaptation techniques to the statistical models, especially to the translation model, is another popular approach used in domain adaptation for SMT. Some research follows the path of adding in new features into the phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combi"
2014.amta-researchers.8,P12-2023,0,0.0146632,"and English-to-French translation tasks compared with the standalone cross-entropy difference approach. Applying adaptation techniques to the statistical models, especially to the translation model, is another popular approach used in domain adaptation for SMT. Some research follows the path of adding in new features into the phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when buil"
2014.amta-researchers.8,W07-0717,0,0.0252747,"g in new features into the phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when building a phrase table for SMT. One of the conclusions is that while out-of-domain data can improve the translation coverage for rare words, it may be harmful for common in-domain words. This suggests that the translations which contain a lot of in-domain evidence should be kept. 2 Related Work The trans"
2014.amta-researchers.8,W08-0509,0,0.0209798,"© The Authors 101 Experiment prob-fill-up(nc 2007,ep 2007) C 16 γ 0.125 Accuracy 0.8139 prob-fill-up(ted 11,nc v9) 2 0.03125 0.8565 Table 3: SVM-tuned parameters values C and γ, where C is the trade-off parameter in equation (3), and γ adjusts the width of RBF in equation (4). 5.3 Translation System Training All SMT systems in our experiments are trained using the phrase-based SMT with Moses 1.0 framework. The reordering model is not included in our translation system since we are interested only in measuring the system effects coming from translation models. We use the word aligner MGIZA++ (Gao and Vogel, 2008) for word alignment in both translation directions, and then symmetrize the word alignment models using the heuristic of grow-diag-final-and. We use all five default Moses 1.0 translation model features. The translation systems are tuned with minimum error rate training (Och, 2003) using case-insensitive BLEU (Papineni et al., 2002) as the optimization measure. A 5-gram language model is trained with the open source IRSTLM toolkit using all the available target sentences in each of the fill-up experiment scenarios. We use the Moses default language model toolkit KenLM at the tuning and decodin"
2014.amta-researchers.8,W12-3154,0,0.137586,"acted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when building a phrase table for SMT. One of the conclusions is that while out-of-domain data can improve the translation coverage for rare words, it may be harmful for common in-domain words. This suggests that the translations which contain a lot of in-domain evidence should be kept. 2 Related Work The translation model fill-up approach was introduced into SMT by Nakov (2008). In his work, the phrase tables are merged by keeping all the phrase pairs unchanged from the in-domain phrase table, and only adding in the phrase pairs from the general-domai"
2014.amta-researchers.8,W11-2123,0,0.020995,"es the same restrictions as in Moore and Lewis (2010), where a token is treated as an instance of &lt;UNK&gt; unless it appears at least twice at the in-domain training dataset. We keep T number of SVM training sentences to tune the parameters in equations (3) and (4). We test the accuracy of the trained SVM using the corresponding SMT development data. The data used for SVM training, language model training and SVM tuning are summarized in Table 2. The SVM-tuned parameters are presented in Table 3. We use the open source IRSTLM toolkit (Federico et al., 2008) for language model training and KenLM (Heafield, 2011) to compute the sentence perplexity. Experiment prob-fill-up(nc 2007,ep 2007) M 42,884 N 40,000 T 2,884 prob-fill-up(ted 11,nc v9) 50,000 45,000 5,000 Table 2: SVM data statistics, where M,N and T are the data sizes (in sentences) used for training, tuning and testing, respectively. 3 http://www.csie.ntu.edu.tw/ ˜cjlin/libsvm Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 101 Experiment prob-fill-up(nc 2007,ep 2007) C 16 γ 0.125 Accuracy 0.8139 prob-fill-up(ted 11,nc v9) 2 0.03125 0.8565 Table 3: SVM-tuned parameters values C and γ, wher"
2014.amta-researchers.8,W04-3250,0,0.0565569,"toolkit using all the available target sentences in each of the fill-up experiment scenarios. We use the Moses default language model toolkit KenLM at the tuning and decoding time. 5.4 Results We set our baseline systems to be the fill-up system of Bisazza et al. (2011) (fill-up(experiment)), which has been integrated within the Moses 1.0 framework. Tables 4 and 5 report our results using case-insensitive BLEU on the corresponding test sets. We use † to indicate where the probabilistic feature-based fill-up approach systems (prob-fill-up heuristic(experiment)) achieve significant improvement (Koehn, 2004) compared with the baseline systems at the level p = 0.01 level with 1000 iterations. System fill-up(nc 2007,ep 2007) Test (news-test2007) 28.01 prob-fill-up Min(nc 2007,ep 2007) 28.03 prob-fill-up-Arithmetic Mean(nc 2007,ep 2007) 28.21 prob-fill-up-Geometric Mean(nc 2007,ep 2007) 28.37† Table 4: prob-fill-up heuristic(nc 2007,ep 2007) experiment BLEU scores on testing data, the significance testing at the level p = 0.01 level with 1000 iterations. The result of the prob-fill-up heuristic(nc 2007,ep 2007) experiment in Table 4 shows that the probabilistic feature-based fill-up systems using th"
2014.amta-researchers.8,P07-2045,0,0.00464247,"he source-side perplexity difference and the target-side perplexity difference in this feature set. 5 Experiment 5.1 Corpora The experiments in this paper use data from WMT07, WMT13 and IWLST11 translation tasks. We choose our experiments on the French-to-English language pair. We first perform some standard data cleaning steps, including tokenization, punctuation normalization, replacement of special characters, lower casing and long sentence removal ( &lt;0 or &gt;80 ), resulting in the preprocessed data summarized in Table 1. We use scripts provided within Moses 1.0 translation system framework (Koehn et al., 2007)2 for all cleaning steps. Corpus News Commentary (nc 2007) Train 42,884 Tune 1,064 (nc-devtest200) Test 2,007 (news-test2007) Europarl (ep 2007) 1,257,436 n/a n/a TED (ted 11) 106,642 934 (dev2010) 1,664 (tst2010) news-commentary-v9 (nc v9) 181,274 n/a n/a Table 1: SMT training corpus statistics There are two fill-up experiments designed to evaluate our approach, defined as prob-fillup heuristic(in-domain,general-domain), such as prob-fill-up heuristic(nc 2007,ep 2007) and prob-fill-up heuristic(ted 11,nc v9), where heuristic refers to the heuristics stated in Section 3 of this paper. The expe"
2014.amta-researchers.8,W07-0733,0,0.0339872,"phrase table. Chen et al. (2013) add vector similarity into the phrase table and use it as a tuning- and decoding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when building a phrase table for SMT. One of the conclusions is that while out-of-domain data can improve the translation coverage for rare words, it may be harmful for common in-domain words. This suggests that the translations which contain a lot of in-domain evidence should be kept. 2 Related Work The translation model fill-up approach w"
2014.amta-researchers.8,D07-1036,1,0.884701,"Missing"
2014.amta-researchers.8,2011.iwslt-papers.5,0,0.0375411,"Missing"
2014.amta-researchers.8,P10-2041,0,0.556658,"etrieval techniques on a transductive-learning framework to increase the count of important in-domain training instances, which results in phrase-pair weights being favourable to the development set. Bic¸ici and Yuret (2011) employ a feature decay algorithm which can be used in both active learning and transductive learning settings. The decay algorithm is used to increase the variety of the training set by devaluing features that have already been seen from a training set. In recent studies, a cross-entropy difference method has seen increasing interest for the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011). The training dataset is ranked using cross-entropy difference from some language models trained on in-domain or general-domain sentences. Then a threshold is set to select the pseudo indomain sentences. The intuition is to find sentences as close to the target domain and as far from the average of the general-domain as possible. Later, Mansour et al. (2011) argue that “An LM does not capture the connections between the source and target words, and scores the sentences independently”, and linearly interpolate IBM model 1 (Brown et al., 1993) into the cross-entropy diffe"
2014.amta-researchers.8,W08-0320,0,0.285391,"ngj@computing.dcu.ie liangyouli@computing.dcu.ie away@computing.dcu.ie qliu@computing.dcu.ie The CNGL Centre for Global Intelligent Content,School of Computing,Dublin City University, Ireland Abstract In this paper, we describe an effective translation model combination approach based on the estimation of a probabilistic Support Vector Machine (SVM). We collect domain knowledge from both in-domain and general-domain corpora inspired by a commonly used data selection algorithm, which we then use as features for the SVM training. Drawing on previous work on binary-featured phrase table fill-up (Nakov, 2008; Bisazza et al., 2011), we substitute the binary feature in the original work with our probabilistic domain-likeness feature. Later, we design two experiments to evaluate the proposed probabilistic feature-based approach on the French-to-English language pair using data provided at WMT07, WMT13 and IWLST11 translation tasks. Our experiments demonstrate that translation performance can gain significant improvements of up to +0.36 and +0.82 BLEU scores by using our probabilistic feature-based translation model fill-up approach compared with the binary featured fill-up approach in both experimen"
2014.amta-researchers.8,P03-1021,0,0.0249442,"on System Training All SMT systems in our experiments are trained using the phrase-based SMT with Moses 1.0 framework. The reordering model is not included in our translation system since we are interested only in measuring the system effects coming from translation models. We use the word aligner MGIZA++ (Gao and Vogel, 2008) for word alignment in both translation directions, and then symmetrize the word alignment models using the heuristic of grow-diag-final-and. We use all five default Moses 1.0 translation model features. The translation systems are tuned with minimum error rate training (Och, 2003) using case-insensitive BLEU (Papineni et al., 2002) as the optimization measure. A 5-gram language model is trained with the open source IRSTLM toolkit using all the available target sentences in each of the fill-up experiment scenarios. We use the Moses default language model toolkit KenLM at the tuning and decoding time. 5.4 Results We set our baseline systems to be the fill-up system of Bisazza et al. (2011) (fill-up(experiment)), which has been integrated within the Moses 1.0 framework. Tables 4 and 5 report our results using case-insensitive BLEU on the corresponding test sets. We use †"
2014.amta-researchers.8,P02-1038,0,0.192404,"atured fill-up approach in both experiments. 1 Introduction Like many machine-learning problems, Statistical Machine Translation (SMT) is a datadependent learning approach. The prerequisite is large amounts of training data in order to generate statistical models. In general, the training data has to be sentence-aligned and bilingual. Some heuristic approaches are often used when deconstructing the training data into phrase-level representations, and the statistical models are computed based on the phrase probability distributions. The generated models are then combined in a log-linear model (Och and Ney, 2002). A basic SMT system may consist of a translation model and a language model, where the translation model provides a target-language translation e for a source-language sentence f, and the language model ensures the fluency of the target-language translation e. One challenge which rises above others in SMT is that the translation performance decreases when there are dissimilarities between the training and the testing environments. This type of challenge is often defined as “domain adaptation” in previous work. The underlying reasons that caused domain adaptation challenge are many, but the ob"
2014.amta-researchers.8,P02-1040,0,0.0896526,"r experiments are trained using the phrase-based SMT with Moses 1.0 framework. The reordering model is not included in our translation system since we are interested only in measuring the system effects coming from translation models. We use the word aligner MGIZA++ (Gao and Vogel, 2008) for word alignment in both translation directions, and then symmetrize the word alignment models using the heuristic of grow-diag-final-and. We use all five default Moses 1.0 translation model features. The translation systems are tuned with minimum error rate training (Och, 2003) using case-insensitive BLEU (Papineni et al., 2002) as the optimization measure. A 5-gram language model is trained with the open source IRSTLM toolkit using all the available target sentences in each of the fill-up experiment scenarios. We use the Moses default language model toolkit KenLM at the tuning and decoding time. 5.4 Results We set our baseline systems to be the fill-up system of Bisazza et al. (2011) (fill-up(experiment)), which has been integrated within the Moses 1.0 framework. Tables 4 and 5 report our results using case-insensitive BLEU on the corresponding test sets. We use † to indicate where the probabilistic feature-based fi"
2014.amta-researchers.8,E12-1055,0,0.0141224,"ding-time feature. The similarity is computed by comparing the vectorized representation of phrase pairs extracted from the development set and the training set. Eidelman et al. (2012) achieve translation performance improvement by including a lexical weight topic feature into the translation model. The topic model used in their work is built based on the source side of the training sentences. There is also work which focuses on translation model combination. Foster and Kuhn (2007) and Koehn and Schroeder (2007) combine the translation models in a log-linear model at tuning and decoding time. Sennrich (2012) proposes an approach to interpolate the translation models based on perplexity minimization. Haddow and Koehn (2012) focus on the extracting and scoring steps when building a phrase table for SMT. One of the conclusions is that while out-of-domain data can improve the translation coverage for rare words, it may be harmful for common in-domain words. This suggests that the translations which contain a lot of in-domain evidence should be kept. 2 Related Work The translation model fill-up approach was introduced into SMT by Nakov (2008). In his work, the phrase tables are merged by keeping all t"
2014.amta-wptp.5,W05-0909,0,0.0206611,"rent points (as was likely e.g. with the biographies included in our data sets), they might, more or less consciously, end up translating them differently. This variable behaviour applies even more to post-editors: the degree and the type of corrections made by the same as well as by different individuals to the MT output for one language pair are likely to be unpredictably inconsistent. 68 We thus evaluated the raw MT output against both the human translations and the post-edited MT output using three state-of-the-art automatic evaluation metrics, namely BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). The automatic MT evaluation scores are shown in Figure 3. Following the conventions used in Snover et al. (2006), the scores against references translated from scratch are named after the metric (i.e. BLEU, METEOR and TER), while the scores against the post-edited references are named appending the prefix H (i.e. HBLEU, HMETEOR and HTER, respectively). If we compare the human translation scores against the PE scores, we can see that the PE scores are consistently better than the human translation scores for all the translation directions across all the metrics ("
2014.amta-wptp.5,2009.mtsummit-btm.7,0,0.312419,"Missing"
2014.amta-wptp.5,2009.mtsummit-papers.8,0,0.0774514,"involved. Guerberof (2009) studied the effectiveness of using MT output as opposed to translation memory fuzzy matches for the purpose of post-editing in an EnglishĺSpanish translation task. She used Language Weaver’s statistical MT engine and trained it on the same TM, performing both quantitative and qualitative analyses. The main result was that the productivity of the translators as well as the quality of the translation improved when post-editing MT output, compared to when processing fuzzy matches from the translation memory database. 1 http://langtech.autodesk.com/productivity.html. 61 Koehn and Haddow (2009) describe Caitra, a tool that makes suggestions for sentence completion, shows word and phrase translation options, and supports PE of MT output. They report a user study carried out with the tool involving 7 translators for the English–French language pair. Among the different types of assistance offered by Caitra, users prefer the prediction of sentence completion and the options from the translation table over the other types of assistance available for post-editing MT output. To the authors’ surprise, PE received the lowest scores among all the options, both in terms of enjoyment and subje"
2014.amta-wptp.5,W12-3123,0,0.52659,"t with the tool involving 7 translators for the English–French language pair. Among the different types of assistance offered by Caitra, users prefer the prediction of sentence completion and the options from the translation table over the other types of assistance available for post-editing MT output. To the authors’ surprise, PE received the lowest scores among all the options, both in terms of enjoyment and subjective usefulness, although PE was as productive as the other types of assistance. In an effort to extend the initial insights presented in particular by Koehn and Haddow (2009) and Koponen (2012), this paper investigates perceived vs real productivity gains brought about by post-editing MT output compared against manual translation from scratch in the relatively open – and thus particularly challenging – news-oriented domain. 3 Set-up of the Study 3.1 Methodology and Materials Output from the CoSyne statistical MT systems (Martzoukos and Monz, 2010) was used in this experiment, and a facility was in place to track the time required by the users to post-edit MT output and to perform manual translations from scratch on texts of similar length and complexity. The texts chosen for the stu"
2014.amta-wptp.5,2012.amta-wptp.2,0,0.115752,"Missing"
2014.amta-wptp.5,2013.mtsummit-wptp.10,0,0.227975,"Missing"
2014.amta-wptp.5,2010.iwslt-evaluation.28,0,0.0213712,"west scores among all the options, both in terms of enjoyment and subjective usefulness, although PE was as productive as the other types of assistance. In an effort to extend the initial insights presented in particular by Koehn and Haddow (2009) and Koponen (2012), this paper investigates perceived vs real productivity gains brought about by post-editing MT output compared against manual translation from scratch in the relatively open – and thus particularly challenging – news-oriented domain. 3 Set-up of the Study 3.1 Methodology and Materials Output from the CoSyne statistical MT systems (Martzoukos and Monz, 2010) was used in this experiment, and a facility was in place to track the time required by the users to post-edit MT output and to perform manual translations from scratch on texts of similar length and complexity. The texts chosen for the study were extracted from “Today in History/Kalenderblatt” and “Beeld en Geluidwiki”, the public wiki sites of the two media organizations that acted as end-user partners in the CoSyne project, namely Deutsche Welle (DW) and the Netherlands Institute for Sound and Vision (NISV).2 These two bilingual wiki sites cover news, accounts of historical events, biograph"
2014.amta-wptp.5,P02-1040,0,0.102721,"cross identical phrases at different points (as was likely e.g. with the biographies included in our data sets), they might, more or less consciously, end up translating them differently. This variable behaviour applies even more to post-editors: the degree and the type of corrections made by the same as well as by different individuals to the MT output for one language pair are likely to be unpredictably inconsistent. 68 We thus evaluated the raw MT output against both the human translations and the post-edited MT output using three state-of-the-art automatic evaluation metrics, namely BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). The automatic MT evaluation scores are shown in Figure 3. Following the conventions used in Snover et al. (2006), the scores against references translated from scratch are named after the metric (i.e. BLEU, METEOR and TER), while the scores against the post-edited references are named appending the prefix H (i.e. HBLEU, HMETEOR and HTER, respectively). If we compare the human translation scores against the PE scores, we can see that the PE scores are consistently better than the human translation scores for all the translation"
2014.amta-wptp.5,2012.amta-wptp.7,0,0.104374,"Missing"
2014.amta-wptp.5,2006.amta-papers.25,0,0.404541,"age input, revising and improving it as required to obtain a final target text of publishable quality. The purpose of this was to add the final revised translation to the public wiki of their respective media organization; hence, the scenario was that of full PE, aiming for optimal quality of the final revised text (Allen, 2003: 306). In addition, it should be noted that while all participants in the experiment had experience in manual translation, none of them had been specifically trained to carry out PE in a realistic professional task. This is quite different from previous studies such as Snover et al. (2006: 227), where monolingual annotators “were coached on how to minimize the edit rate”. To sum up, our study focused on a scenario in which (i) the translators were not trained specifically on PE, and (ii) the objective was publishable quality, as a means of investigating the role of full PE in industrial settings, especially in terms of the perceived vs actual productivity gains. 4 Questionnaire Results 4.1 Profiles of the Participants At the time of completing the questionnaire, the youngest DW staff member was 38 years of age, and the oldest was 59. Overall, the average age of DW staff who co"
2014.amta-wptp.5,2010.jec-1.6,0,0.561873,"Missing"
2014.amta-wptp.5,2012.amta-wptp.10,0,0.0730721,"d translations (columns PEMT), for which the picture is rather mixed (4 expected correlations, 5 no correlations and 3 unexpected ones). Aggregating the data for all the translation directions, we observe consistent results regardless of the metric (TER, BLEU and METEOR) or the translation method (PEMT, HT): all the correlations are as expected, their values ranging from ±0.23 to ±0.42. 6 Conclusions We have presented a study of real vs perceived PE productivity gains for the German— English and Dutch—English bidirectional language pairs. Previous studies such as Plitt and Masselot (2010) and Zhechev (2012) had looked at PE productivity gains compared to manual translation. However, in a similar vein to Koehn and Haddow (2009) and Koponen (2012), this study has crucially brought into the picture the perceptions of the users in terms of PE effort and speed, comparing them to the actual PE time gains. We have found a bias in favour of translation from scratch across all four translation directions for all the levels of perception considered (speed, effort and favourite working method). While the perception of speed and effort seems to correspond to the actual gains to some extent, the favourite wo"
2014.eamt-1.34,W07-0732,0,0.01509,"nthesis. In Penkale and Way (2012), we addressed the problem of translating a bad version of a language into a ‘less poor’ one. This was in the context of translating in-game text, where incorrect English – usually written by a non-native game developer – needs to be improved prior to localisation per se; translating the poor original English ‘as is’ would produce completely unintelligible output. Using post-edited data as the target-side of the training data, our SMT system was able to learn how to automatically post-edit some of the errors made by the source authors, in much the same way as Dugast et al. (2007) and Simard et al. (2007) have shown previously. While we are unaware of any published work on the subject, it is clear that Microsoft have done something similar, albeit for a different purpose. 144 They describe their ‘Contextual Thesaurus’1 as “an English-to-English machine translation system that employs the same architecture that the Microsoft Translator uses when translating different languages”. They list a number of applications for this “large-scale paraphrasing system”, including document simplification, language learning, plagiarism detection, summarization and question answering, t"
2014.eamt-1.34,2012.amta-monomt.1,0,0.0262956,"Missing"
2014.eamt-1.34,P07-2045,0,0.0272953,"Missing"
2014.eamt-1.34,N03-1017,0,0.00964227,"luding spelling (e.g. British English colour vs. American English colour), lexicon (e.g. British English autumn vs. American English fall), word usage (e.g. British English I’m pissed off vs. American English I’m pissed), grammar (e.g. Irish English You’re after spilling my pint vs. British English You’ve just spilt my pint), etc. Considering that often such translation tasks are carried out by humans, monolingual translation becomes costly and time-consuming, especially when one takes into account how much the two languages have in common. Deploying Statistical Machine Translation (SMT, e.g. Koehn et al. (2003)) would appear to offer a solution to the problem. Given that the two varieties are essentially the same language except for some minor differences, we expect most of the translation variants to be captured by an SMT system. Moreover, we rely on the SMT system to be able to capture those structures that are not only acceptable in a language variety but are also preferable; in a rule-based system (RBMT), these could only be handled by complex hand-written rules. The present study was run as a short-term (3∼4week) innovation project between CNGL and Intel. The main goal of the present paper is t"
2014.eamt-1.34,2012.amta-monomt.4,0,0.0198337,"ething similar, albeit for a different purpose. 144 They describe their ‘Contextual Thesaurus’1 as “an English-to-English machine translation system that employs the same architecture that the Microsoft Translator uses when translating different languages”. They list a number of applications for this “large-scale paraphrasing system”, including document simplification, language learning, plagiarism detection, summarization and question answering, to name but a few. As to non-statistical approaches, only Zhang (1998) appears to have applied RBMT to translate from Mandarin Chinese to Cantonese. Murakami et al. (2012) adopted instead a two-stage translation pipeline where Japanese is first rendered in English through pattern-based translation, which is in turn translated into more correct English. Formiga et al. (2012) focused on improving the output of an English-to-Spanish SMT system, where correct morphology is generated in a posttranslation morphological generalisation stage. As well as the use-cases presented already, the current paper addresses a number of real-world problems, which are as yet unsolved in the translation and localisation industries. Notwithstanding the need to come up with a proper t"
2014.eamt-1.34,P03-1021,0,0.0138516,"RUL1 (all) + RUL1 (freq>5) + RUL1 (freq>10) + LEX SUB & dict from aligned data (constraint) + post-decoding LEX SUB BLEU .589 .588 .589 .577 .260 .524 .529 .578 .588 TER 0.292 0.292 0.292 0.301 0.504 0.327 0.324 0.30 0.292 METEOR 0.704 0.704 0.704 0.697 0.445 0.658 0.661 0.70 0.704 Table 1: System A: automatic evaluation scores for the different approaches. to below as System A) was trained using 63,137 length-ratio filtered sentences (approx. 687,410 tokens). A devset of 1,498 sentences (approx. 20,286 tokens) was used to tune the weights for the features in the log-linear model using MERT (Och (2003)). In comparison, the second system (System B) was trained on a larger set of 75,324 sentences (approx. 828,532 tokens) using a different devset containing 1,499 sentences (approx. 20,174 tokens). For both systems we used a single test set comprising 1,500 sentences. 4 Methodology and Results The main goal of the present paper is to show which approach (or combination of approaches) leads to the biggest improvement in translation quality. In more detail, we explored the following options: 1. Guiding decoding to ensure technical terms are translated correctly via supplied dictionaries, 2. Using"
2014.eamt-1.34,padro-stanilovsky-2012-freeling,0,0.0412733,"Missing"
2014.eamt-1.34,P02-1040,0,0.0921564,"d the following options: 1. Guiding decoding to ensure technical terms are translated correctly via supplied dictionaries, 2. Using lexical substitution to replace Brazilian Portuguese words remaining in the output, 3. Using data-driven spelling rules to correct the translation output, 4. Using company-internal and data-driven bilingual dictionaries to both guide decoding and correct the translation output. The results for System A are shown in Table 1, while those for System B are shown in Table 2. Column 1 shows each of the different system variants built, with columns 2–4 showing the BLEU (Papineni et al. (2002)), Translation Edit Rate (TER: Snover et al. (2006)) and METEOR (Lavie and Denkowski (2009)) scores, respectively. Note that for BLEU and METEOR, the higher the score the better, while for TER, a lower score is indicative of better quality. 145 In the next sections, we describe each experiment conducted with the results achieved. 4.1 Translation of technical terms Intel provided us with a list of technical and product names that the system should not mistranslate or lose during decoding. In order to adhere to their requirements, we wrapped those terms in xml tags (i.e. hDNTi . . . h/DNTi) and"
2014.eamt-1.34,2012.amta-wptp.6,1,0.749535,"te the number of potential applications for same language translation, there are only a few works which address the problem. To the best of our knowledge, there is no published research which has directly applied SMT to translate from one language variety to another. However, SMT has been applied for two related tasks in two of our own papers. In patented work described in Cahill et al. (2009), we built an English-to-English system using our in-house MaTrEx system (Tinsley et al. (2008)) to generate an N-best list of outputs that could be used for improved target-language speech synthesis. In Penkale and Way (2012), we addressed the problem of translating a bad version of a language into a ‘less poor’ one. This was in the context of translating in-game text, where incorrect English – usually written by a non-native game developer – needs to be improved prior to localisation per se; translating the poor original English ‘as is’ would produce completely unintelligible output. Using post-edited data as the target-side of the training data, our SMT system was able to learn how to automatically post-edit some of the errors made by the source authors, in much the same way as Dugast et al. (2007) and Simard et"
2014.eamt-1.34,W07-0728,0,0.0118584,"ay (2012), we addressed the problem of translating a bad version of a language into a ‘less poor’ one. This was in the context of translating in-game text, where incorrect English – usually written by a non-native game developer – needs to be improved prior to localisation per se; translating the poor original English ‘as is’ would produce completely unintelligible output. Using post-edited data as the target-side of the training data, our SMT system was able to learn how to automatically post-edit some of the errors made by the source authors, in much the same way as Dugast et al. (2007) and Simard et al. (2007) have shown previously. While we are unaware of any published work on the subject, it is clear that Microsoft have done something similar, albeit for a different purpose. 144 They describe their ‘Contextual Thesaurus’1 as “an English-to-English machine translation system that employs the same architecture that the Microsoft Translator uses when translating different languages”. They list a number of applications for this “large-scale paraphrasing system”, including document simplification, language learning, plagiarism detection, summarization and question answering, to name but a few. As to n"
2014.eamt-1.34,2006.amta-papers.25,0,0.0173406,"e technical terms are translated correctly via supplied dictionaries, 2. Using lexical substitution to replace Brazilian Portuguese words remaining in the output, 3. Using data-driven spelling rules to correct the translation output, 4. Using company-internal and data-driven bilingual dictionaries to both guide decoding and correct the translation output. The results for System A are shown in Table 1, while those for System B are shown in Table 2. Column 1 shows each of the different system variants built, with columns 2–4 showing the BLEU (Papineni et al. (2002)), Translation Edit Rate (TER: Snover et al. (2006)) and METEOR (Lavie and Denkowski (2009)) scores, respectively. Note that for BLEU and METEOR, the higher the score the better, while for TER, a lower score is indicative of better quality. 145 In the next sections, we describe each experiment conducted with the results achieved. 4.1 Translation of technical terms Intel provided us with a list of technical and product names that the system should not mistranslate or lose during decoding. In order to adhere to their requirements, we wrapped those terms in xml tags (i.e. hDNTi . . . h/DNTi) and used both the exclusive and constraint options impl"
2014.eamt-1.34,W08-0326,1,0.808452,"uss some of the pertinent findings, and conclude in Section 6 with some avenues for future work. 2 Same Language Translation Despite the number of potential applications for same language translation, there are only a few works which address the problem. To the best of our knowledge, there is no published research which has directly applied SMT to translate from one language variety to another. However, SMT has been applied for two related tasks in two of our own papers. In patented work described in Cahill et al. (2009), we built an English-to-English system using our in-house MaTrEx system (Tinsley et al. (2008)) to generate an N-best list of outputs that could be used for improved target-language speech synthesis. In Penkale and Way (2012), we addressed the problem of translating a bad version of a language into a ‘less poor’ one. This was in the context of translating in-game text, where incorrect English – usually written by a non-native game developer – needs to be improved prior to localisation per se; translating the poor original English ‘as is’ would produce completely unintelligible output. Using post-edited data as the target-side of the training data, our SMT system was able to learn how t"
2014.eamt-1.34,P98-2238,0,0.51466,"are unaware of any published work on the subject, it is clear that Microsoft have done something similar, albeit for a different purpose. 144 They describe their ‘Contextual Thesaurus’1 as “an English-to-English machine translation system that employs the same architecture that the Microsoft Translator uses when translating different languages”. They list a number of applications for this “large-scale paraphrasing system”, including document simplification, language learning, plagiarism detection, summarization and question answering, to name but a few. As to non-statistical approaches, only Zhang (1998) appears to have applied RBMT to translate from Mandarin Chinese to Cantonese. Murakami et al. (2012) adopted instead a two-stage translation pipeline where Japanese is first rendered in English through pattern-based translation, which is in turn translated into more correct English. Formiga et al. (2012) focused on improving the output of an English-to-Spanish SMT system, where correct morphology is generated in a posttranslation morphological generalisation stage. As well as the use-cases presented already, the current paper addresses a number of real-world problems, which are as yet unsolve"
2014.eamt-1.45,espla-gomis-etal-2014-comparing,1,0.53776,"Missing"
2014.eamt-1.45,P07-2045,0,0.0196031,"36 0.2945 0.2927 0.3583 0.2456 0.3767 TER 0.5601 0.5295 0.4848 0.5016 0.5755 0.5756 0.4726 0.6582 0.4451 OOV 9.5 7.6 7.2 12.6 12.4 6.3 23.1 4.1 Table 2: SMT results. 4 Table 1: Statistics of the parallel datasets. For each dataset the first line corresponds to statistics for Croatian and the second to English. two additional datasets: union and intersection. These are the union and intersection of datasets 10best and reliable. 3 BLEU 0.4092 0.4382 0.5304 0.5176 0.4064 0.4105 0.5448 0.3224 0.5722 Machine Translation Systems Phrase-based statistical MT (PB-SMT) systems are built with Moses 2.1 (Koehn et al., 2007). Tuning is carried out on the development set with minimum error rate training (Och, 2003). All the MT systems use an English language model (LM) from our system for French→English at the WMT-2014 translation shared task (Rubino et al., 2014).9 We built individual LMs on each dataset provided at WMT-2014 and then interpolated them on a development set of the news domain (news2012). Most systems are built on a single dataset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled data"
2014.eamt-1.45,2010.eamt-1.35,1,0.878655,"Missing"
2014.eamt-1.45,P03-1021,0,0.0380974,"4451 OOV 9.5 7.6 7.2 12.6 12.4 6.3 23.1 4.1 Table 2: SMT results. 4 Table 1: Statistics of the parallel datasets. For each dataset the first line corresponds to statistics for Croatian and the second to English. two additional datasets: union and intersection. These are the union and intersection of datasets 10best and reliable. 3 BLEU 0.4092 0.4382 0.5304 0.5176 0.4064 0.4105 0.5448 0.3224 0.5722 Machine Translation Systems Phrase-based statistical MT (PB-SMT) systems are built with Moses 2.1 (Koehn et al., 2007). Tuning is carried out on the development set with minimum error rate training (Och, 2003). All the MT systems use an English language model (LM) from our system for French→English at the WMT-2014 translation shared task (Rubino et al., 2014).9 We built individual LMs on each dataset provided at WMT-2014 and then interpolated them on a development set of the news domain (news2012). Most systems are built on a single dataset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled datasets (1best, 10best, reliable and all) and two systems built on the union and intersection"
2014.eamt-1.45,W13-2506,0,0.182025,"ur legislation and natural environment for English–French and English– Greek (Pecina et al., 2012) and automotive for German to Italian and French (L¨aubli et al., 2013). The rest of the paper is organised as follows. Section 2 presents the crawled datasets used in this study and details the processing undertaken to prepare them for MT. Section 3 details the different MT systems built. Section 4 shows and comments the results obtained. Finally, Section 5 draws conclusions and outlines future lines of work. 2 Crawled Datasets Datasets were crawled using two crawlers: ILSP Focused Crawler (FC) (Papavassiliou et al., 2013) and Bitextor (Espl`a-Gomis et al., 2010). The detection of parallel documents was carried out with two settings for each crawler: 10best and 1best for Bitextor and reliable and all for FC (see (Espl`aGomis et al., 2014) for further details). It is worth mentioning that reliable and 1best are subsets of all and 10best, respectively. These subsets were obtained with a more strict configuration of each crawler and, therefore, are expected to contain higher quality parallel text. In addition, a set of parallel segments was obtained by aligning only those pairs of documents which were checked manu"
2014.eamt-1.45,P02-1040,0,0.0979486,"Missing"
2014.eamt-1.45,2012.eamt-1.38,1,0.898626,"Missing"
2014.eamt-1.45,E12-1055,0,0.0212094,"taset, hence they have one phrase table and one reordering table. These systems include a baseline built on the general-domain data (gen), four systems built on the crawled datasets (1best, 10best, reliable and all) and two systems built on the union and intersection of the best performing10 dataset of each crawler: 10best and reliable. There is also one system (gen+u) built on two datasets, the general-domain (gen) dataset and a domain-specific dataset (union). Phrase tables from the individual systems gen and union are interpolated so that the perplexity on the development set is minimised (Sennrich, 2012). 9 http://www.statmt.org/wmt14/ translation-task.html 10 According to the BLEU score on the development set. The MT systems are evaluated with a set of stateof-the-art evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). For each system we also report the percentage of out-ofvocabulary (OOV) tokens. Table 2 shows the scores obtained by each MT system. We compare our systems to two baselines: a PB-SMT system built on general-domain data (gen) and an on-line MT system, Google Translate11 (google). Systems built solely on in-domain d"
2014.eamt-1.45,2006.amta-papers.25,0,0.202709,"Missing"
2014.tc-1.23,2013.mtsummit-papers.5,0,0.0284717,"ly worse for more open and unpredictable domains 1 such as news (cf. WMT translation task series ). We suggest to study the applicability of SMT to literary text by comparing the degree of freedom and narrowness of parallel corpora for literature to other domains widely studied in the area of MT (technical documentation and news). Such a corpus study can be carried out by using a set of automatic measures. The degree of freedom of the translation can be approximated by the perplexity of the word alignment. The narrowness of the domain can be assessed by using measures such as repetition rate (Bertoldi et al., 2013) and perplexity with respect to a language model (Ruiz and Federico, 2014). Therefore, in order to assess the translatability of literary text with MT, we put the problem in perspective by comparing it to the translatability of other widely studied types of text. Instead of considering the translatability of literature as a whole, we root the study along two axes: 1. Relatedness of the language pair: from pairs of languages that belong to the same family (e.g. Romance languages), through languages that belong to the same group (e.g. Romance and Germanic languages of the Indo-European group) to"
2014.tc-1.23,D10-1016,0,0.0155805,"esults in improved translation productivity, at least for technical domains (Plitt and Masselot, 2010). Having reached this level of maturity, we explore the viability of current state-of-the-art MT for literature, the last bastion of human translation. To what extent is MT useful for literature? At first glance, these two terms (MT and literature) might seem incompatible, but the truth is – to the best of our knowledge – that the applicability of MT to literature has not been studied rigorously from a empirical point of view. 2. Background The first work on MT for literature we are aware of (Genzel et al., 2010) translates poetry by constraining a SMT system to produce translations that obey to particular length, meter and rhyming constraints. Form is preserved at the price of producing a worse translation. However, this work does not study the viability of MT to assist with the translation of poetry. 174 Translating and The Computer 36 The only other work on MT for literature we are aware of (Besacier, 2014) presents a pilot study where MT followed by post-editing is used to translate a short story from English to French. Post-editing is performed by non-professional translators and the author concl"
2014.tc-1.23,P02-1040,0,0.0940549,"Between related languages, translations should be more literal and complex phenomena (e.g. metaphors) might simply transfer to the target language, while they might have more 1 http://www.statmt.org/wmt14/translation-task.html 175 Translating and The Computer 36 complex translations between unrelated languages. Regarding literary genres, in poetry the preservation of form might be considered relevant while in novels it may not. As a preliminary study, we evaluated the translation of a recent best-selling novel for a related language pair (Spanish to Catalan). The scores obtained – 66.2 BLEU (Papineni et al., 2002) points and 23.2 TER (Snover et al., 2006) points – would be considered, in an industrial setting, as very useful for assisting human translation (e.g. by means of post-editing or interactive MT). We expect these scores to generalise to other related language pairs such as Spanish–Portuguese or Spanish–Italian. 2 4. Conclusion In summary, we have proposed a methodology to assess the applicability of MT to literature which aims to give an indication of how well SMT could be expected to perform on literary texts compared to the performance of this technology on technical documentation and news."
2014.tc-1.23,2014.eamt-1.39,0,0.0117314,"translation task series ). We suggest to study the applicability of SMT to literary text by comparing the degree of freedom and narrowness of parallel corpora for literature to other domains widely studied in the area of MT (technical documentation and news). Such a corpus study can be carried out by using a set of automatic measures. The degree of freedom of the translation can be approximated by the perplexity of the word alignment. The narrowness of the domain can be assessed by using measures such as repetition rate (Bertoldi et al., 2013) and perplexity with respect to a language model (Ruiz and Federico, 2014). Therefore, in order to assess the translatability of literary text with MT, we put the problem in perspective by comparing it to the translatability of other widely studied types of text. Instead of considering the translatability of literature as a whole, we root the study along two axes: 1. Relatedness of the language pair: from pairs of languages that belong to the same family (e.g. Romance languages), through languages that belong to the same group (e.g. Romance and Germanic languages of the Indo-European group) to unrelated languages (e.g. Germanic and Sino-Tibetan languages). 2. Litera"
2014.tc-1.23,2006.amta-papers.25,0,0.0231595,"uld be more literal and complex phenomena (e.g. metaphors) might simply transfer to the target language, while they might have more 1 http://www.statmt.org/wmt14/translation-task.html 175 Translating and The Computer 36 complex translations between unrelated languages. Regarding literary genres, in poetry the preservation of form might be considered relevant while in novels it may not. As a preliminary study, we evaluated the translation of a recent best-selling novel for a related language pair (Spanish to Catalan). The scores obtained – 66.2 BLEU (Papineni et al., 2002) points and 23.2 TER (Snover et al., 2006) points – would be considered, in an industrial setting, as very useful for assisting human translation (e.g. by means of post-editing or interactive MT). We expect these scores to generalise to other related language pairs such as Spanish–Portuguese or Spanish–Italian. 2 4. Conclusion In summary, we have proposed a methodology to assess the applicability of MT to literature which aims to give an indication of how well SMT could be expected to perform on literary texts compared to the performance of this technology on technical documentation and news. While we may be far from having MT that is"
2015.eamt-1.12,2013.iwslt-evaluation.20,0,0.0150948,"commercial rule-based engine for English–Farsi translation. It contains 1.5 million words in its database and includes specific dictionaries for 33 different fields of science. Another English–Farsi MT system was developed by the Iran Supreme Council of Information.4 Postchi5 is a bidirectional system listed among the EuroMatrix6 systems for the Farsi language. These systems are not terribly robust or precise examples of Farsi SMT and are usually the by-products of research or commercial projects. The only system that has officially been reported for the purpose of Farsi SMT is FBK’s system (Bertoldi et al., 2013). It was tested on a publicly available dataset and from this viewpoint is the most important system for our purposes.7 2.2 Parallel Corpora for Farsi SMT The first attempts at generating Farsi–English parallel corpora are documented in the Shiraz project (Zajac et al., 2000). The authors constructed a corpus of 3000 parallel sentences, which were translated manually from monolingual online Farsi documents at New Mexico State University. More recently Qasemizadeh et al. (2007) participated in the Farsi part of MULTEXT-EAST8 project (Erjavec, 2010) and developed about 6000 sentences. There is a"
2015.eamt-1.12,2012.eamt-1.60,0,0.0349081,"Missing"
2015.eamt-1.12,erjavec-2010-multext,0,0.0270435,"purpose of Farsi SMT is FBK’s system (Bertoldi et al., 2013). It was tested on a publicly available dataset and from this viewpoint is the most important system for our purposes.7 2.2 Parallel Corpora for Farsi SMT The first attempts at generating Farsi–English parallel corpora are documented in the Shiraz project (Zajac et al., 2000). The authors constructed a corpus of 3000 parallel sentences, which were translated manually from monolingual online Farsi documents at New Mexico State University. More recently Qasemizadeh et al. (2007) participated in the Farsi part of MULTEXT-EAST8 project (Erjavec, 2010) and developed about 6000 sentences. There is also a corpus available in ELRA9 consisting of about 3,500,000 English and Farsi words aligned at sentence level (about 100,000 sentences). This is a mixed domain dataset including a variety of text types such as art, law, culture, literature, poetry, proverbs, religion etc. PEN (Parallel English–Persian News corpus) is another small corpus (Farajian, 2011) generated semi-automatically. It includes almost 30,000 sentences. Farajian developed a method to find similar sentence pairs and for quality assurance used Google Translate.10 All these corpora"
2015.eamt-1.12,N03-1017,0,0.0361792,"Missing"
2015.eamt-1.12,P07-2045,0,0.0261526,"scuss the problems with Mizan in Section 4.1 and perform error analysis on the output translations, where it is used as the SMT training data. In the second part using TEP and TEP++ we carry out monolingual translation between SF and FF (SF2FF) and discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default configuration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sen"
2015.eamt-1.12,W04-3250,0,0.337904,"Missing"
2015.eamt-1.12,P03-1021,0,0.0260544,"d discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default configuration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sentences. Training set sizes are reported in tables. For all experiments BLEU scores for Google Translate are reported as a baseline. LM3 LM5 Google Translate Training set Corpus EN–FA Before After 8.24 10.47 8.54 10.53 FA–EN Before After 1"
2015.eamt-1.12,P02-1040,0,0.0920216,"e carry out monolingual translation between SF and FF (SF2FF) and discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default configuration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sentences. Training set sizes are reported in tables. For all experiments BLEU scores for Google Translate are reported as a baseline. LM3 LM5 Google Translate Training set Corpus EN–FA Bef"
2015.eamt-1.12,I13-1144,0,0.0402882,"Missing"
2015.eamt-1.45,E06-1032,0,\N,Missing
2015.eamt-1.45,W10-1751,0,\N,Missing
2015.eamt-1.45,W14-3301,0,\N,Missing
2015.eamt-1.45,P02-1040,0,\N,Missing
2015.eamt-1.45,W14-3319,1,\N,Missing
2015.eamt-1.45,P11-1105,0,\N,Missing
2015.eamt-1.45,P10-2041,0,\N,Missing
2015.eamt-1.45,W05-0909,0,\N,Missing
2015.eamt-1.45,P07-2045,0,\N,Missing
2015.eamt-1.45,W07-0718,0,\N,Missing
2015.eamt-1.45,C14-1111,0,\N,Missing
2015.eamt-1.45,P12-3005,0,\N,Missing
2015.eamt-1.45,2012.eamt-1.67,1,\N,Missing
2015.eamt-1.45,2014.eamt-1.4,1,\N,Missing
2015.eamt-1.45,W14-3320,0,\N,Missing
2015.eamt-1.45,2005.mtsummit-papers.11,0,\N,Missing
2015.eamt-1.45,ljubesic-etal-2014-tweetcat,1,\N,Missing
2015.eamt-1.45,W15-3036,1,\N,Missing
2015.eamt-1.45,rubino-etal-2014-quality,1,\N,Missing
2015.eamt-1.45,W15-3022,1,\N,Missing
2015.eamt-1.45,W15-4903,1,\N,Missing
2015.eamt-1.45,2015.eamt-1.4,1,\N,Missing
2015.eamt-1.45,espla-gomis-etal-2014-comparing,1,\N,Missing
2015.eamt-1.45,W15-3001,0,\N,Missing
2015.eamt-1.45,ljubesic-toral-2014-cawac,1,\N,Missing
2015.eamt-1.45,W14-0405,1,\N,Missing
2015.eamt-1.45,2005.iwslt-1.8,0,\N,Missing
2015.eamt-1.45,W16-3421,1,\N,Missing
2015.eamt-1.45,D07-1078,0,\N,Missing
2015.eamt-1.45,W08-0509,0,\N,Missing
2015.eamt-1.45,W11-2123,0,\N,Missing
2015.eamt-1.45,P14-1129,0,\N,Missing
2015.eamt-1.45,W16-2347,0,\N,Missing
2015.eamt-1.45,W16-2375,1,\N,Missing
2015.eamt-1.45,W16-2367,1,\N,Missing
2015.eamt-1.45,W16-3423,1,\N,Missing
2015.eamt-1.7,P11-1103,0,0.0205204,"a large-scale Engish-to-Farsi translation task. The rest of this paper is organised as follows. Section 2 reviews the related work and contextualises our work. Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi. Section 4 presents our reordering model, which is then evaluated in Section 5. Finally, Section 6 concludes the paper and outlines avenues of future work. 2 Related Work Phrase-based systems can perform local (short distance) reordering inside the phrases but they are inherently weak at non-local (medium and long distance) reordering (Birch and Osborne, 2011). Previous work to address reordering in PB-SMT can generally be categorised into two groups. Approaches in the first group perform reordering in a pre-processing step (i.e. before decoding) by applying some reordering rules to the source sentences to make them in order more similar to that of the target language (Xia and McCord, 2004; Collins et al., 2005; Genzel, 2010). Although all these approaches have reported improvements, there is a fundamental problem with separating the reordering task into a pre-processing component as every faulty decision in the pre-processing step will be passed a"
2015.eamt-1.7,W09-0434,0,0.0214,"long- and even medium-distance reorderings, since they try to find suitable reorderings only between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this li"
2015.eamt-1.7,D14-1082,0,0.0384929,"Missing"
2015.eamt-1.7,P05-1033,0,0.129487,"lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to find suitable reorderings only between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our"
2015.eamt-1.7,P05-1066,0,0.177195,"Missing"
2015.eamt-1.7,W14-3348,0,0.0422753,"irs and features examined (cf. Table 6) and two additional systems that model the reordering for both types of constituent pairs (rows all) with (ws) and without (wos) surface forms. We compare our systems to two baselines, a standard HPB-SMT system (HPB) and a HPBSMT system with added swap glue grammar rule (HPB sgg) as in Equation 4. The swap glue rule allows adjacent phrases to be reversed. X → (X1 X2 , X2 X1 ) (4) Table 7 shows the results obtained by each of the MT systems according to four automatic evaluation metrics: BLEU, NIST (Doddington, 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014). For each system and evaluation metric we show its relative improvement over the baseline HPB (columns diff). The scores obtained by systems that implement our novel reordering between pairs of dependents (columns dd) are better than those of the baseline, both with (ws) and without (wos) surface forms, accross all the four evaluation metrics. The same is true for models that implement reordering between both pairs of constituent types (columns all), except for the system all wos according to BLEU. The results for systems that perform reordering between pairs of head and dependent offer a mix"
2015.eamt-1.7,D08-1089,0,0.0302344,"espite the satisfactory performance of lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to find suitable reorderings only between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hann"
2015.eamt-1.7,D11-1079,0,0.0203606,"rs from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the first paper in this line of work to be applied to a language pair o"
2015.eamt-1.7,C10-1043,0,0.0214098,"outlines avenues of future work. 2 Related Work Phrase-based systems can perform local (short distance) reordering inside the phrases but they are inherently weak at non-local (medium and long distance) reordering (Birch and Osborne, 2011). Previous work to address reordering in PB-SMT can generally be categorised into two groups. Approaches in the first group perform reordering in a pre-processing step (i.e. before decoding) by applying some reordering rules to the source sentences to make them in order more similar to that of the target language (Xia and McCord, 2004; Collins et al., 2005; Genzel, 2010). Although all these approaches have reported improvements, there is a fundamental problem with separating the reordering task into a pre-processing component as every faulty decision in the pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering mo"
2015.eamt-1.7,J03-1002,0,0.0100549,"g. Table 4 presents the details about this dataset. We parsed the source side (English) of the corpus using the Stanford dependency parser (Chen 1 tribes wandered http://dadegan.ir/catalog/mizan 47 wife wandered unit English Farsi sentences 1,016,758 1,016,758 Train words 13,919,071 14,043,499 sentences 3,000 3,000 Tune words 40,831 41,670 sentences 1,000 1,000 Test words 13,165 13,444 Table 4: Mizan parallel corpus statistics and Manning, 2014) and used the “collapsed representation” of the parser output to obtain direct dependencies between the words in the source sentences. We used GIZA++ (Och and Ney, 2003) to align the words in the corpus. Then we extracted 6,391,255 head−dependent pairs and 5,247,137 dependent−dependent pairs from train dataset and determined the orientation for each pair based on Equation 1. In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classifier with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classifier separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of"
2015.eamt-1.7,P03-1021,0,0.00715687,"the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classifier with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classifier separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of HPM model with default settings (Hoang et al., 2009). We used a 5-gram target language model trained on the Farsi side of the training data. In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT (Och, 2003). 5.2 Impact of different features Since the proposed reordering model has to classify the head−dependent and dependent−dependent pairs into their correct monotone or swap orientation classes, its task can be seen as a binary classification task. We used the Naive Bayes algorithm to build such an orientation classifier. We then used different feature sets in each classification experiment to determine their impact on the accuracy of the model. The features that were examined in this paper are shown in Table 5. All of these features are entirely based on the source sentence and source dependenc"
2015.eamt-1.7,P05-1034,0,0.112252,"lement pairs presented by Dryer (1992). Dryer has shown that these pairs can be used to distinguish SOV and SVO languages. 4 Dependency-based Reordering Model Our reordering model is based on the source dependency tree, an example of which is shown in Figure 1. The dependency tree of a sentence shows the grammatical relations between the head and dependent words of that sentence. For example in Figure 1, the arrow from “he” to “bought” with label “nsubj”, expresses that the 45 dependent word “he” is the subject of the head word “bought”. Under the assumption that constituents move as a whole (Quirk et al., 2005), our proposed reordering model aims to predict the orientation of each dependent word with respect to its head (head−dependent), and also with respect to the other dependents of that head (dependent−dependent orientation). For example, for the sentence in Figure 1 we try to predict the appropriate orientations between the headdependent and dependent-dependent pairs shown in Tables 2 and 3, respectively. Our motivation for using dependency structure as the basis of our reordering model is based on the assumption that, if it is the case that a reordering pattern is employed for one English–Fars"
2015.eamt-1.7,2006.amta-papers.25,0,0.0362609,"r according to the constituent pairs and features examined (cf. Table 6) and two additional systems that model the reordering for both types of constituent pairs (rows all) with (ws) and without (wos) surface forms. We compare our systems to two baselines, a standard HPB-SMT system (HPB) and a HPBSMT system with added swap glue grammar rule (HPB sgg) as in Equation 4. The swap glue rule allows adjacent phrases to be reversed. X → (X1 X2 , X2 X1 ) (4) Table 7 shows the results obtained by each of the MT systems according to four automatic evaluation metrics: BLEU, NIST (Doddington, 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014). For each system and evaluation metric we show its relative improvement over the baseline HPB (columns diff). The scores obtained by systems that implement our novel reordering between pairs of dependents (columns dd) are better than those of the baseline, both with (ws) and without (wos) surface forms, accross all the four evaluation metrics. The same is true for models that implement reordering between both pairs of constituent types (columns all), except for the system all wos according to BLEU. The results for systems that perform reordering between"
2015.eamt-1.7,N04-4026,0,0.0599434,"re-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering model that can be used in combination with the other models in SMT to find the best translation. These approaches range from distortion models (Koehn et al., 2003) to lexical reordering models (Tillmann, 2004). 44 Distortion models generally prefer monotone translation which, while may work for related languages, is not a realistic assumption for translating between languages with different grammatical structure. On top of this limitation, these models do not take the content into consideration, and thus they do not generalise well. Lexical reordering models take content into account and condition reordering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexical models, they have two important limitations"
2015.eamt-1.7,N13-1029,0,0.0131754,"008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pai"
2015.eamt-1.7,J10-2004,0,0.0151409,"cess to the necessary structural information to perform long-distance reordering. However, due to the complexity of the decoding algorithm, they have very low performance on large-scale translations. In order to overcome some of these deficiencies, we propose a dependency-based reordering model for HPB-SMT. Our model uses the dependency structure of the source sentence to capture the medium- and long-distance reorderings between the dependent parts of the sentence. Unlike the syntax-based models that impose harsh syntactic limits on rule extraction and require serious efforts to be optimised (Wang et al., 2010), we use syntactic information only in the reordering model and augment the HPB model with soft dependency constraints. We report experimental results on a large-scale Engish-to-Farsi translation task. The rest of this paper is organised as follows. Section 2 reviews the related work and contextualises our work. Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi. Section 4 presents our reordering model, which is then evaluated in Section 5. Finally, Section 6 concludes the paper and outlines avenues of future work. 2 Related Work Phrase-based s"
2015.eamt-1.7,2009.iwslt-papers.4,0,0.0290221,"Then we extracted 6,391,255 head−dependent pairs and 5,247,137 dependent−dependent pairs from train dataset and determined the orientation for each pair based on Equation 1. In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classifier with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classifier separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of HPM model with default settings (Hoang et al., 2009). We used a 5-gram target language model trained on the Farsi side of the training data. In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT (Och, 2003). 5.2 Impact of different features Since the proposed reordering model has to classify the head−dependent and dependent−dependent pairs into their correct monotone or swap orientation classes, its task can be seen as a binary classification task. We used the Naive Bayes algorithm to build such an orientation classifier. We then used different feature sets in each classificatio"
2015.eamt-1.7,D13-1053,0,0.0189721,"nslation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the f"
2015.eamt-1.7,N03-1017,0,0.026867,"essing component as every faulty decision in the pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering model that can be used in combination with the other models in SMT to find the best translation. These approaches range from distortion models (Koehn et al., 2003) to lexical reordering models (Tillmann, 2004). 44 Distortion models generally prefer monotone translation which, while may work for related languages, is not a realistic assumption for translating between languages with different grammatical structure. On top of this limitation, these models do not take the content into consideration, and thus they do not generalise well. Lexical reordering models take content into account and condition reordering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexica"
2015.eamt-1.7,W04-3250,0,0.178946,"Missing"
2015.eamt-1.7,P06-1066,0,0.0271572,"ring on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to find suitable reorderings only between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pr"
2015.eamt-1.7,P12-1095,0,0.0201087,"ssing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the first paper in this line of work to be applied to a language pair other than Chinese-to-English. Our language pair, Engli"
2015.eamt-1.7,N09-1028,0,0.0190702,"nly between adjacent phrases. The first limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The first approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source e"
2015.eamt-1.7,C04-1073,0,\N,Missing
2015.eamt-1.7,N13-1060,0,\N,Missing
2015.mtsummit-papers.14,2005.eamt-1.9,0,0.0197804,"re scores. Hardt and Elming (2010) propose a sentence-level retraining scheme in which a local phrase table is created and incrementally updated as a file is translated and post-edited. In their work, a modified revision of GIZA++ (Och and Ney, 2003) is used to approximate word alignments of a newly translated sentence to reduce the incremental training time, and then an additional phrase table is produced from the newly aligned sentences with higher priority. The experiments show the efficiency of the incremental retraining system. In the incrementally retrained PE-SMT system, suffix arrays (Callison-Burch et al., 2005; Zhang and Vogel, 2005) are a very efficient technique for the incremental retraining process. Levenberg et al. (2010) introduce a dynamic suffix array to incorporate new training text to the current training data. Denkowski et al. (2014) propose an online model adaptation for PE-SMT in which three methods are used for incremental model adaptation: adding new data to a suffix array-indexed bitext from which grammars are extracted, updating a Bayesian language model with incremental data, and using an online MIRA (Crammer and Singer, 2003) to update the parameters. The simulated experiments sh"
2015.mtsummit-papers.14,E14-4036,0,0.0579855,"Missing"
2015.mtsummit-papers.14,E14-1042,0,0.0509844,"he previously post-edited segments (Guerberof, 2009; Plitt and Masselot, 2010; Carl et al., 2011; OBrien, 2011; Zhechev, 2012; Guerberof, 2013). Furthermore, the order of input segments has been found to have a significant impact on the overall PE-time, i.e., an optimized sequence of input segments can reduce the overall PE-time compared to the typical chronological sequence (Dara et al., 2014). Regarding the PE-SMT, the incremental retraining can be roughly categorized into two different scenarios, namely the segment-level online incremental retraining (segment mode) (Levenberg et al., 2010; Denkowski et al., 2014) and batch-level incremental retraining (batch mode) (Hardt and Elming, 2010; Henr´ıquez Q. et al., 2011; Mathur et al., 2013; Simard and Foster, 2013; Dara et al., 2014; Bertoldi et al., 2014). The former takes one post-edited segment per retraining cycle to immediately update the models, which requires rapid incremental processing of Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 172 the word alignment, phrase/rule generation, language model and parameters tuning etc., while the latter firstly accumulates a batch of segments, and then performs the i"
2015.mtsummit-papers.14,2014.amta-workshop.3,0,0.0153664,"2010) introduce a dynamic suffix array to incorporate new training text to the current training data. Denkowski et al. (2014) propose an online model adaptation for PE-SMT in which three methods are used for incremental model adaptation: adding new data to a suffix array-indexed bitext from which grammars are extracted, updating a Bayesian language model with incremental data, and using an online MIRA (Crammer and Singer, 2003) to update the parameters. The simulated experiments show that significant improvement in MT quality is achieved when these methods are used individually and in tandem. Germann (2014) proposes a dynamic phrase table strategy for an interactive PE-SMT that computes phrase table entries on demand by sampling a suffix array-indexed bitext. Experiments show that without loss of translation quality, the sampling phrase table achieves good performance in terms of speed. In Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 174 our task, we use this dynamic phrase table for incremental retraining in Moses (Koehn et al., 2007). 3 The Incrementally Retrained PE-SMT Paradigm In the post-editing scenario, humans are involved to continuously edit"
2015.mtsummit-papers.14,E12-1025,0,0.0135057,"Nov 3, 2015 |p. 173 data. Relatively inexpensive human costs are iteratively used to translate information-rich sentences. Experimental results show that generally the translation unit-based selection strategies, namely phrases and n-grams, performed best compared to other methods such as random selection, translation confidence, inverse model etc. However, in their work, the AL framework is used for low-resource SMT rather than the PE-SMT scenario. Furthermore, it is a static retraining process in which the test set is constant per iteration, and the retraining procedure is not incremental. Gonzalez-Rubio et al. (2012) apply AL to the interactive MT in which AL techniques are used to select the most informative sentences to reduce human effort for a given translation quality. Experimental results show that applying AL techniques in an interactive MT setting can prove a better tradeoff between required human effort and final translation quality. To the best of our knowledge, the most relevant previous work is that of Dara et al. (2014), which proposes a Cross Entropy Difference (CED) criterion to prioritize input segments in an AL framework for PE-based incremental MT update applications. The fundamental goa"
2015.mtsummit-papers.14,2009.mtsummit-btm.7,0,0.0227491,"to the sequential PE-based incrementally retrained SMT. 1 Introduction In recent years, SMT systems have been widely deployed into the translator’s workflow in the localization and translation industry to improve productivity, refereed to as post-editing-based SMT. However, in most cases, current SMT systems cannot generate high-quality translations, so human effort is usually required. With the help of incrementally improved SMT systems, the productivity of translators/post-editors can be significantly increased due to the early learning of knowledge from the previously post-edited segments (Guerberof, 2009; Plitt and Masselot, 2010; Carl et al., 2011; OBrien, 2011; Zhechev, 2012; Guerberof, 2013). Furthermore, the order of input segments has been found to have a significant impact on the overall PE-time, i.e., an optimized sequence of input segments can reduce the overall PE-time compared to the typical chronological sequence (Dara et al., 2014). Regarding the PE-SMT, the incremental retraining can be roughly categorized into two different scenarios, namely the segment-level online incremental retraining (segment mode) (Levenberg et al., 2010; Denkowski et al., 2014) and batch-level incremental"
2015.mtsummit-papers.14,N09-1047,0,0.205876,"he purpose of the input segment prioritization is to reduce the overall PE time to improve productivity and to reduce the cost. In this scenario, the involvement of human effort implies that the segment prioritization process can be regarded as AL framework-based PE-SMT. In this framework, the input segments are ranked based on the information or uncertainty contained therein. In this section, we will introduce the related work in terms of two aspects: AL-based framework for PE-SMT, and the incrementally retrained PE-SMT. The practical active learning framework for SMT was firstly proposed in Haffari et al. (2009) where a number of high-quality parallel data are acquired from large-scale monolingual Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 173 data. Relatively inexpensive human costs are iteratively used to translate information-rich sentences. Experimental results show that generally the translation unit-based selection strategies, namely phrases and n-grams, performed best compared to other methods such as random selection, translation confidence, inverse model etc. However, in their work, the AL framework is used for low-resource SMT rather than the P"
2015.mtsummit-papers.14,2010.amta-papers.21,0,0.215103,"; Carl et al., 2011; OBrien, 2011; Zhechev, 2012; Guerberof, 2013). Furthermore, the order of input segments has been found to have a significant impact on the overall PE-time, i.e., an optimized sequence of input segments can reduce the overall PE-time compared to the typical chronological sequence (Dara et al., 2014). Regarding the PE-SMT, the incremental retraining can be roughly categorized into two different scenarios, namely the segment-level online incremental retraining (segment mode) (Levenberg et al., 2010; Denkowski et al., 2014) and batch-level incremental retraining (batch mode) (Hardt and Elming, 2010; Henr´ıquez Q. et al., 2011; Mathur et al., 2013; Simard and Foster, 2013; Dara et al., 2014; Bertoldi et al., 2014). The former takes one post-edited segment per retraining cycle to immediately update the models, which requires rapid incremental processing of Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 172 the word alignment, phrase/rule generation, language model and parameters tuning etc., while the latter firstly accumulates a batch of segments, and then performs the incremental retraining process to update the system. The batch-level mode can"
2015.mtsummit-papers.14,2011.eamt-1.18,0,0.0554856,"Missing"
2015.mtsummit-papers.14,P07-2045,0,0.0051987,"iments show that significant improvement in MT quality is achieved when these methods are used individually and in tandem. Germann (2014) proposes a dynamic phrase table strategy for an interactive PE-SMT that computes phrase table entries on demand by sampling a suffix array-indexed bitext. Experiments show that without loss of translation quality, the sampling phrase table achieves good performance in terms of speed. In Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 174 our task, we use this dynamic phrase table for incremental retraining in Moses (Koehn et al., 2007). 3 The Incrementally Retrained PE-SMT Paradigm In the post-editing scenario, humans are involved to continuously edit MT outputs into highquality translations. As discussed in Dara et al. (2014), the fundamental goal of input segment prioritization for PE-SMT is to reduce the overall PE time taken to complete a translation job. The crucial step is to first select the most uncertain sentences or most informative sentences for post-editing in order to learn as much knowledge as possible from these sentences. The workflow of an AL-based incrementally retrained PE-SMT system is as shown in Figure"
2015.mtsummit-papers.14,N10-1062,0,0.125658,"ning of knowledge from the previously post-edited segments (Guerberof, 2009; Plitt and Masselot, 2010; Carl et al., 2011; OBrien, 2011; Zhechev, 2012; Guerberof, 2013). Furthermore, the order of input segments has been found to have a significant impact on the overall PE-time, i.e., an optimized sequence of input segments can reduce the overall PE-time compared to the typical chronological sequence (Dara et al., 2014). Regarding the PE-SMT, the incremental retraining can be roughly categorized into two different scenarios, namely the segment-level online incremental retraining (segment mode) (Levenberg et al., 2010; Denkowski et al., 2014) and batch-level incremental retraining (batch mode) (Hardt and Elming, 2010; Henr´ıquez Q. et al., 2011; Mathur et al., 2013; Simard and Foster, 2013; Dara et al., 2014; Bertoldi et al., 2014). The former takes one post-edited segment per retraining cycle to immediately update the models, which requires rapid incremental processing of Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 172 the word alignment, phrase/rule generation, language model and parameters tuning etc., while the latter firstly accumulates a batch of segments"
2015.mtsummit-papers.14,W13-2237,0,0.0244012,"Missing"
2015.mtsummit-papers.14,J03-1002,0,0.00664805,"ons to rank the segments. Regarding the incrementally retrained SMT, the most challenging and time-consuming steps are the word alignment and the phrase/rule generation. Ortiz-Martinez et al. (2010) incrementally update the feature values of the phrase table by extracting new phrases from the new sentence pairs based on the pre-stored statistics related to the feature scores. Hardt and Elming (2010) propose a sentence-level retraining scheme in which a local phrase table is created and incrementally updated as a file is translated and post-edited. In their work, a modified revision of GIZA++ (Och and Ney, 2003) is used to approximate word alignments of a newly translated sentence to reduce the incremental training time, and then an additional phrase table is produced from the newly aligned sentences with higher priority. The experiments show the efficiency of the incremental retraining system. In the incrementally retrained PE-SMT system, suffix arrays (Callison-Burch et al., 2005; Zhang and Vogel, 2005) are a very efficient technique for the incremental retraining process. Levenberg et al. (2010) introduce a dynamic suffix array to incorporate new training text to the current training data. Denkows"
2015.mtsummit-papers.14,N10-1079,0,0.0239143,"a in order to keep the costs to a minimum for the commercial PE MT applications. However, in the practical scenario, we can take the information of the target side (e.g. translations) into account in batch mode without a significant increase in extra time and human costs by pre-translating the remaining batches in the background while post-editing the current batch. In doing so, we propose to use the confidence of MT translations to rank the segments. Regarding the incrementally retrained SMT, the most challenging and time-consuming steps are the word alignment and the phrase/rule generation. Ortiz-Martinez et al. (2010) incrementally update the feature values of the phrase table by extracting new phrases from the new sentence pairs based on the pre-stored statistics related to the feature scores. Hardt and Elming (2010) propose a sentence-level retraining scheme in which a local phrase table is created and incrementally updated as a file is translated and post-edited. In their work, a modified revision of GIZA++ (Och and Ney, 2003) is used to approximate word alignments of a newly translated sentence to reduce the incremental training time, and then an additional phrase table is produced from the newly align"
2015.mtsummit-papers.14,P02-1040,0,0.0915444,"Missing"
2015.mtsummit-papers.14,2013.mtsummit-papers.24,0,0.0151896,"ermore, the order of input segments has been found to have a significant impact on the overall PE-time, i.e., an optimized sequence of input segments can reduce the overall PE-time compared to the typical chronological sequence (Dara et al., 2014). Regarding the PE-SMT, the incremental retraining can be roughly categorized into two different scenarios, namely the segment-level online incremental retraining (segment mode) (Levenberg et al., 2010; Denkowski et al., 2014) and batch-level incremental retraining (batch mode) (Hardt and Elming, 2010; Henr´ıquez Q. et al., 2011; Mathur et al., 2013; Simard and Foster, 2013; Dara et al., 2014; Bertoldi et al., 2014). The former takes one post-edited segment per retraining cycle to immediately update the models, which requires rapid incremental processing of Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 172 the word alignment, phrase/rule generation, language model and parameters tuning etc., while the latter firstly accumulates a batch of segments, and then performs the incremental retraining process to update the system. The batch-level mode can perform the incremental retraining process in the background while the tr"
2015.mtsummit-papers.14,2006.amta-papers.25,0,0.0308835,"ate to the effectiveness of the methods. The main contributions of this paper include: • Confidence of translation and perplexity methods are proposed to reorder the input segments in the AL-based dynamically retrained SMT. • A deep comparison and investigation of different segment prioritization methods for PESMT using different data sets and language pairs. • A detailed data and results analysis of the correlation between the reordering score and the factors. • Our experiments show that the unnormalized confidence of translations performs best in all tasks and gains around 1.72 to 4.55 TER (Snover et al., 2006) absolute on average. 2 Related Work The purpose of the input segment prioritization is to reduce the overall PE time to improve productivity and to reduce the cost. In this scenario, the involvement of human effort implies that the segment prioritization process can be regarded as AL framework-based PE-SMT. In this framework, the input segments are ranked based on the information or uncertainty contained therein. In this section, we will introduce the related work in terms of two aspects: AL-based framework for PE-SMT, and the incrementally retrained PE-SMT. The practical active learning fram"
2015.mtsummit-papers.14,2005.eamt-1.39,0,0.0239049,"2010) propose a sentence-level retraining scheme in which a local phrase table is created and incrementally updated as a file is translated and post-edited. In their work, a modified revision of GIZA++ (Och and Ney, 2003) is used to approximate word alignments of a newly translated sentence to reduce the incremental training time, and then an additional phrase table is produced from the newly aligned sentences with higher priority. The experiments show the efficiency of the incremental retraining system. In the incrementally retrained PE-SMT system, suffix arrays (Callison-Burch et al., 2005; Zhang and Vogel, 2005) are a very efficient technique for the incremental retraining process. Levenberg et al. (2010) introduce a dynamic suffix array to incorporate new training text to the current training data. Denkowski et al. (2014) propose an online model adaptation for PE-SMT in which three methods are used for incremental model adaptation: adding new data to a suffix array-indexed bitext from which grammars are extracted, updating a Bayesian language model with incremental data, and using an online MIRA (Crammer and Singer, 2003) to update the parameters. The simulated experiments show that significant impr"
2015.mtsummit-papers.14,2012.amta-wptp.10,0,0.0148935,"cent years, SMT systems have been widely deployed into the translator’s workflow in the localization and translation industry to improve productivity, refereed to as post-editing-based SMT. However, in most cases, current SMT systems cannot generate high-quality translations, so human effort is usually required. With the help of incrementally improved SMT systems, the productivity of translators/post-editors can be significantly increased due to the early learning of knowledge from the previously post-edited segments (Guerberof, 2009; Plitt and Masselot, 2010; Carl et al., 2011; OBrien, 2011; Zhechev, 2012; Guerberof, 2013). Furthermore, the order of input segments has been found to have a significant impact on the overall PE-time, i.e., an optimized sequence of input segments can reduce the overall PE-time compared to the typical chronological sequence (Dara et al., 2014). Regarding the PE-SMT, the incremental retraining can be roughly categorized into two different scenarios, namely the segment-level online incremental retraining (segment mode) (Levenberg et al., 2010; Denkowski et al., 2014) and batch-level incremental retraining (batch mode) (Hardt and Elming, 2010; Henr´ıquez Q. et al., 20"
2015.mtsummit-wptp.5,D11-1033,0,0.0148795,"Lv et al. (2007) use information-retrieval techniques in a transductive learning framework to increase the count of important ID training instances, which results in phrase-pair weights being favourable to the devset. Bicici and Yuret (2011) employ a Feature Decay Algorithm (FDA) to increase the variety of the training set by devaluing features that have already been seen from a training set, and experimental results show significant improvements compared to the baseline. There has been increasing interest in applying the CED method to the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Haque et al., 2014). In this method, given an ID corpus I and a general corpus O, language models are built from both, and each sentence s in O is scored according to the entropy difference, as in Equation (1): score(s) = HI (s) − HO (s) (1) where HI (s) is the entropy of s in ID corpus I, and HO (s) is the entropy of s in OOD corpus O. The score(s) is to reflect how similar the sentence s is to the corpus I, and how different the sentence s is from the corpus O. That is, the lower the score given to a sentence, the more useful it is to train a system for the specific domain I. Some other me"
2015.mtsummit-wptp.5,C12-1010,1,0.851181,"I and a general corpus O, language models are built from both, and each sentence s in O is scored according to the entropy difference, as in Equation (1): score(s) = HI (s) − HO (s) (1) where HI (s) is the entropy of s in ID corpus I, and HO (s) is the entropy of s in OOD corpus O. The score(s) is to reflect how similar the sentence s is to the corpus I, and how different the sentence s is from the corpus O. That is, the lower the score given to a sentence, the more useful it is to train a system for the specific domain I. Some other methods have been proposed to select ID data. For example, Banerjee et al. (2012) propose an approach to perform batch selection with the objective of maximizing SMT performance. Toral (2013) and Toral et al. (2014) use linguistic information such as lemmas, Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4) Miami, November 3, 2015 |p. 58 named entity categories and part-of-speech tags to augment perplexity-based data selection and achieved improved results. The CED method is more promising both in domain adaptation for SMT and data selection for active learning-based PE-SMT (Dara et al., 2014), so we choose it to verify its effectiveness in SL-SMT"
2015.mtsummit-wptp.5,W11-2131,0,0.0155488,"-of-domain (OOD) test data in order to improve translation performance. For social localisation data in Trommons, the currently available in-domain (ID) data for each language pair is far from sufficient to build up a high-quality SMT system (cf. Section 5.2). Therefore, we have to use a data selection algorithm to select ID data from the OOD data in order to augment the SL-SMT. Lv et al. (2007) use information-retrieval techniques in a transductive learning framework to increase the count of important ID training instances, which results in phrase-pair weights being favourable to the devset. Bicici and Yuret (2011) employ a Feature Decay Algorithm (FDA) to increase the variety of the training set by devaluing features that have already been seen from a training set, and experimental results show significant improvements compared to the baseline. There has been increasing interest in applying the CED method to the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Haque et al., 2014). In this method, given an ID corpus I and a general corpus O, language models are built from both, and each sentence s in O is scored according to the entropy difference, as in Equation (1): score(s)"
2015.mtsummit-wptp.5,E14-4036,0,0.0247869,"Missing"
2015.mtsummit-wptp.5,2009.mtsummit-btm.7,0,0.0224546,"ial_localisation 2 http://trommons.org/ 3 http://www.therosettafoundation.org/ Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4) Miami, November 3, 2015 |p. 57 and interests of volunteer translators. The idea behind Trommons is to match translation/posteditors/proofreading tasks published by various NGOs with volunteer translators/users registered in Trommons. It is well-known that SMT systems have been widely deployed into the translator’s workflow in the localisation and translation industry to improve productivity, which is also named as post-editing SMT (PE-SMT) (Guerberof, 2009; Plitt and Masselot, 2010; Carl et al., 2011; O’Brien, 2011; Zhechev, 2012; Guerberof, 2013). In Trommons, many language pairs lack high-level translators, or translation jobs take quite a long time to be claimed and finished, so a PE-SMT system for such language pairs or translation projects are a practical solution in which human effort could be significantly reduced and high-quality translations could be achieved. However, data is a big problem for building high-quality PE-SMT systems in Trommons. In this paper, we use domain adaptation techniques to acquire social localisation domain rela"
2015.mtsummit-wptp.5,W14-4806,1,0.838578,"nformation-retrieval techniques in a transductive learning framework to increase the count of important ID training instances, which results in phrase-pair weights being favourable to the devset. Bicici and Yuret (2011) employ a Feature Decay Algorithm (FDA) to increase the variety of the training set by devaluing features that have already been seen from a training set, and experimental results show significant improvements compared to the baseline. There has been increasing interest in applying the CED method to the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Haque et al., 2014). In this method, given an ID corpus I and a general corpus O, language models are built from both, and each sentence s in O is scored according to the entropy difference, as in Equation (1): score(s) = HI (s) − HO (s) (1) where HI (s) is the entropy of s in ID corpus I, and HO (s) is the entropy of s in OOD corpus O. The score(s) is to reflect how similar the sentence s is to the corpus I, and how different the sentence s is from the corpus O. That is, the lower the score given to a sentence, the more useful it is to train a system for the specific domain I. Some other methods have been propo"
2015.mtsummit-wptp.5,D07-1036,0,0.0188302,"characteristics of the test data are substantially different from those of the model parameters, system performance drops significantly. In this case, the domain adaptation can be used to adapt an SMT system to the out-of-domain (OOD) test data in order to improve translation performance. For social localisation data in Trommons, the currently available in-domain (ID) data for each language pair is far from sufficient to build up a high-quality SMT system (cf. Section 5.2). Therefore, we have to use a data selection algorithm to select ID data from the OOD data in order to augment the SL-SMT. Lv et al. (2007) use information-retrieval techniques in a transductive learning framework to increase the count of important ID training instances, which results in phrase-pair weights being favourable to the devset. Bicici and Yuret (2011) employ a Feature Decay Algorithm (FDA) to increase the variety of the training set by devaluing features that have already been seen from a training set, and experimental results show significant improvements compared to the baseline. There has been increasing interest in applying the CED method to the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al.,"
2015.mtsummit-wptp.5,P10-2041,0,0.0283604,"to augment the SL-SMT. Lv et al. (2007) use information-retrieval techniques in a transductive learning framework to increase the count of important ID training instances, which results in phrase-pair weights being favourable to the devset. Bicici and Yuret (2011) employ a Feature Decay Algorithm (FDA) to increase the variety of the training set by devaluing features that have already been seen from a training set, and experimental results show significant improvements compared to the baseline. There has been increasing interest in applying the CED method to the problem of SMT data selection (Moore and Lewis, 2010; Axelrod et al., 2011; Haque et al., 2014). In this method, given an ID corpus I and a general corpus O, language models are built from both, and each sentence s in O is scored according to the entropy difference, as in Equation (1): score(s) = HI (s) − HO (s) (1) where HI (s) is the entropy of s in ID corpus I, and HO (s) is the entropy of s in OOD corpus O. The score(s) is to reflect how similar the sentence s is to the corpus I, and how different the sentence s is from the corpus O. That is, the lower the score given to a sentence, the more useful it is to train a system for the specific d"
2015.mtsummit-wptp.5,P02-1040,0,0.0963172,"hich human effort could be significantly reduced and high-quality translations could be achieved. However, data is a big problem for building high-quality PE-SMT systems in Trommons. In this paper, we use domain adaptation techniques to acquire social localisation domain related data to augment PE-SMT systems. Specifically, a Cross Entropy Difference (CED) method is used to select different scales of data from the Europarl data sets. Experiments conducted on English-to-Spanish show that the domain adaptation method can improve translation quality by 6.82 absolute (20.98 relative) BLEU points (Papineni et al., 2002) and 5.99 absolute (11.26 relative) TER points (Snover et al., 2006) compared to the baseline. 2 Related Work Domain adaptation is a popular but difficult research question in SMT. It is well-known that SMT performance is heavily dependent on the training data and the development set (devset) as well as the estimated model parameters which can best reflect the characteristics of the training data. Therefore, if the characteristics of the test data are substantially different from those of the model parameters, system performance drops significantly. In this case, the domain adaptation can be u"
2015.mtsummit-wptp.5,2006.amta-papers.25,0,0.0157624,"slations could be achieved. However, data is a big problem for building high-quality PE-SMT systems in Trommons. In this paper, we use domain adaptation techniques to acquire social localisation domain related data to augment PE-SMT systems. Specifically, a Cross Entropy Difference (CED) method is used to select different scales of data from the Europarl data sets. Experiments conducted on English-to-Spanish show that the domain adaptation method can improve translation quality by 6.82 absolute (20.98 relative) BLEU points (Papineni et al., 2002) and 5.99 absolute (11.26 relative) TER points (Snover et al., 2006) compared to the baseline. 2 Related Work Domain adaptation is a popular but difficult research question in SMT. It is well-known that SMT performance is heavily dependent on the training data and the development set (devset) as well as the estimated model parameters which can best reflect the characteristics of the training data. Therefore, if the characteristics of the test data are substantially different from those of the model parameters, system performance drops significantly. In this case, the domain adaptation can be used to adapt an SMT system to the out-of-domain (OOD) test data in o"
2015.mtsummit-wptp.5,W13-2803,0,0.0132126,"opy difference, as in Equation (1): score(s) = HI (s) − HO (s) (1) where HI (s) is the entropy of s in ID corpus I, and HO (s) is the entropy of s in OOD corpus O. The score(s) is to reflect how similar the sentence s is to the corpus I, and how different the sentence s is from the corpus O. That is, the lower the score given to a sentence, the more useful it is to train a system for the specific domain I. Some other methods have been proposed to select ID data. For example, Banerjee et al. (2012) propose an approach to perform batch selection with the objective of maximizing SMT performance. Toral (2013) and Toral et al. (2014) use linguistic information such as lemmas, Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4) Miami, November 3, 2015 |p. 58 named entity categories and part-of-speech tags to augment perplexity-based data selection and achieved improved results. The CED method is more promising both in domain adaptation for SMT and data selection for active learning-based PE-SMT (Dara et al., 2014), so we choose it to verify its effectiveness in SL-SMT application. 3 System Description of Trommons In Trommons, registered organisations/NGOs can create projects."
2015.mtsummit-wptp.5,2012.amta-wptp.10,0,0.0302248,"/ Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4) Miami, November 3, 2015 |p. 57 and interests of volunteer translators. The idea behind Trommons is to match translation/posteditors/proofreading tasks published by various NGOs with volunteer translators/users registered in Trommons. It is well-known that SMT systems have been widely deployed into the translator’s workflow in the localisation and translation industry to improve productivity, which is also named as post-editing SMT (PE-SMT) (Guerberof, 2009; Plitt and Masselot, 2010; Carl et al., 2011; O’Brien, 2011; Zhechev, 2012; Guerberof, 2013). In Trommons, many language pairs lack high-level translators, or translation jobs take quite a long time to be claimed and finished, so a PE-SMT system for such language pairs or translation projects are a practical solution in which human effort could be significantly reduced and high-quality translations could be achieved. However, data is a big problem for building high-quality PE-SMT systems in Trommons. In this paper, we use domain adaptation techniques to acquire social localisation domain related data to augment PE-SMT systems. Specifically, a Cross Entropy Differenc"
2016.amta-users.17,N13-1073,0,0.0265896,"m the cloud. The machine is set-up to comply with KantanMT requirements and used afterwards to execute the speciﬁc job. For a build job, the KantanMT training pipeline is composed of 14 steps. Crucial for the efﬁciency of the KantanMT training pipeline is word alignment. Word alignment is the task of identifying word-level translation relations between a source text and its translation. Naturally, to date the KantanMT pipeline has been using Giza++ (Och and Ney (2003)) – the most common word-alignment tool used by the SMT community – for word alignment. An alternative to Giza++ is fast_align (Dyer et al. (2013)), a simple, fast, yet effective tool to perform word alignment. Dyer et al. (2013) show that fast_align is about 10 times faster than IBM Model 4 (Brown et al. (1993)). Moreover, fast_align leads to translation performance comparable to MT engines trained using Giza++ (Dyer et al. (2013)). Accordingly, with the aim of reducing the training time of KantanMT engines, we introduced fast_align into the KantanMT pipeline in place of Giza++. Improved training times would lead to better quality of service as well as reduced resource allocation, an important issue for any cloud-based system. In this"
2016.amta-users.17,P07-2045,0,0.00581411,"bilingual data is divided into three sets – a training, a tuning and a test set – that are used for training and optimizing the engine. 3. Building. Three models are built during this stage: (i) a language model that captures the linguistic aspects of the target language and aims at improving MT output; (ii) a recaser model to set the correct letter casing in the MT output and (iii) a translation model used for decoding unseen text. Monolingual data (in the target language) is often used to improve the quality of the language model. The KantanMT platform employs the open-source toolkit Moses (Koehn et al. (2007)) to train the language, the translation and the recaser models. 2 https://aws.amazon.com/ 3 To the best of our knowledge, such an extensive empirical evaluation of these two word-alignment approaches with industry data has not been performed to date. 4 That is why we refer to the KantanMT architecture as a pipeline architecture. 2 Proceedings of AMTA 2016, vol. 2: MT Users&apos; Track Austin, Oct 28 - Nov 1, 2016 |p. 223 Prepare FS Prepare Data Built LM Score Pack Install SW Cleanse Build RM Optimize Store Fetch data Partition Build TM Data preprocessing Building Postprocessing Storing Validate da"
2016.amta-users.17,W95-0115,0,0.02752,"to date. 4 That is why we refer to the KantanMT architecture as a pipeline architecture. 2 Proceedings of AMTA 2016, vol. 2: MT Users&apos; Track Austin, Oct 28 - Nov 1, 2016 |p. 223 Prepare FS Prepare Data Built LM Score Pack Install SW Cleanse Build RM Optimize Store Fetch data Partition Build TM Data preprocessing Building Postprocessing Storing Validate data Instance setup Figure 1: KantanMT training pipeline. 4. Engine postprocessing. After the engine is built it is scored by calculating three evaluation metrics: (i) BLEU score (Papineni et al. (2002)); (ii) F-Measure (van Rijsbergen (1979); Melamed (1995)) and (iii) Translation Error Rate (TER) (Snover et al. (2006)). If required, the engine is also optimized by using the tuning subset from the training data. 5. Storing. The models, conﬁguration ﬁles and scores are packed and stored for future use. Word alignment is invoked during building the translation model. To compute the word alignment is one of the computationally most expensive tasks in the building step. Up to date, KantanMT was using Giza++ to perform word alignment during this step. Figure 1 shows the original (i.e., using the Giza++ word-alignment tool) KantanMT pipeline for traini"
2016.amta-users.17,J03-1002,0,0.0423378,"analysis jobs. The architecture of KantanMT is based on the Amazon Web Services (AWS)2 . For any job request the interface allocates a machine from the cloud. The machine is set-up to comply with KantanMT requirements and used afterwards to execute the speciﬁc job. For a build job, the KantanMT training pipeline is composed of 14 steps. Crucial for the efﬁciency of the KantanMT training pipeline is word alignment. Word alignment is the task of identifying word-level translation relations between a source text and its translation. Naturally, to date the KantanMT pipeline has been using Giza++ (Och and Ney (2003)) – the most common word-alignment tool used by the SMT community – for word alignment. An alternative to Giza++ is fast_align (Dyer et al. (2013)), a simple, fast, yet effective tool to perform word alignment. Dyer et al. (2013) show that fast_align is about 10 times faster than IBM Model 4 (Brown et al. (1993)). Moreover, fast_align leads to translation performance comparable to MT engines trained using Giza++ (Dyer et al. (2013)). Accordingly, with the aim of reducing the training time of KantanMT engines, we introduced fast_align into the KantanMT pipeline in place of Giza++. Improved trai"
2016.amta-users.17,2006.amta-papers.25,0,0.0133045,"cture as a pipeline architecture. 2 Proceedings of AMTA 2016, vol. 2: MT Users&apos; Track Austin, Oct 28 - Nov 1, 2016 |p. 223 Prepare FS Prepare Data Built LM Score Pack Install SW Cleanse Build RM Optimize Store Fetch data Partition Build TM Data preprocessing Building Postprocessing Storing Validate data Instance setup Figure 1: KantanMT training pipeline. 4. Engine postprocessing. After the engine is built it is scored by calculating three evaluation metrics: (i) BLEU score (Papineni et al. (2002)); (ii) F-Measure (van Rijsbergen (1979); Melamed (1995)) and (iii) Translation Error Rate (TER) (Snover et al. (2006)). If required, the engine is also optimized by using the tuning subset from the training data. 5. Storing. The models, conﬁguration ﬁles and scores are packed and stored for future use. Word alignment is invoked during building the translation model. To compute the word alignment is one of the computationally most expensive tasks in the building step. Up to date, KantanMT was using Giza++ to perform word alignment during this step. Figure 1 shows the original (i.e., using the Giza++ word-alignment tool) KantanMT pipeline for training an engine. 2.2 Faster word-alignment for faster end-user de"
2016.gwc-1.24,P05-1033,0,0.149341,"SMT (PB-SMT) is arguably the most widely used approach to SMT to date. In this model, the translation operates on phrases, i.e. sequences of words whose length is between 1 and a maximum upper limit. In PB-SMT, reordering is generally captured by distance-based models (Koehn et al., 2003) and lexical phrase-based models (Tillmann, 2004; Koehn et al., 2005), which are able to perform local reordering but they cannot capture non-local (long-distance) reordering. The weakness of PB-SMT systems on handling long-distance reordering led to proposing the Hierarchical Phrase-based SMT (HPB-SMT) model(Chiang, 2005), in which the translation operates on tree structures (either derived from a syntactic parser or unsupervised). Despite the relatively good performance offered by HPB-SMT in medium-range reordering, they are still weak on long-distance reordering (Birch et al., 2009). A great deal of work has been carried out to address the reordering problem by incorporating reordering models (RM) into SMT systems. A RM tries to capture the differences in word order in a probabilistic framework and assigns a probability to each possible order of words in the target sentence. Most of the reordering models can"
2016.gwc-1.24,P11-2031,0,0.0192647,"sets of the head and dependent words Table 4: Features for (dep-dep) constituent pairs mantic features (WordNet synsets) on the quality of the MT system. Three different feature sets were examined in this paper, including information from (i) surface forms (surface), (ii) synsets (synset) and (iii) both surface forms and synsets (both). We build six MT systems, as shown in Table 6, according to the constituent pairs and feature sets examined. We compared our MT systems to the standard HPB-SMT system. Each MT system is tuned three times and we report the average scores obtained with multeval3 (Clark et al., 2011) on the MT outputs. The results obtained by each of the MT systems according to two widely used automatic evaluation metrics (BLEU (Papineni et al., 2002), and TER (Snover et al., 2006)) are shown in Table 7. The relative improvement of each evaluation metric over the baseline HPB is shown in columns dif f . Compared to the use of surface features, our novel semantic features based on WordNet synsets lead to better scores for both (head- dep) and (depdep) constituent pairs according to both evaluation metrics, BLEU and TER (except for the dd system in terms of TER, where there is a slight but"
2016.gwc-1.24,D11-1079,0,0.0891,"e target language, these models can not generalize to unseen words with equivalent meaning in the same context. In order to improve syntactic and semantic generalization of the RM, it is necessary to incorporate syntactic and semantic features into the model. While there has been some encouraging work on integrating syntactic features into the RM, to the best of our knowledge, there has been no previous work on integrating semantic Reordering Model Zens and Ney (2006) Features Types lexical Cherry (2013) lexical Green et al. (2010) lexical syntactic Bisazza and Federico and Goto et al. (2013) Gao et al. (2011) and Kazemi et al. (2015) The proposed method (2013) lexical syntactic lexical syntactic lexical syntactic semantic Features surface forms of the source and target words unsupervised class of the source and target words surface forms of frequent source and target words unsupervised class of rare source and target words surface forms of the source words, POS tags of the source words, relative position of the source words sentence length surface forms and POS tags of the source words surface forms and POS tags of the source context words surface forms of the source words dependency relation surf"
2016.gwc-1.24,N10-1129,0,0.137079,"se, if two words in the source language follow a specific reordering pattern in the target language, these models can not generalize to unseen words with equivalent meaning in the same context. In order to improve syntactic and semantic generalization of the RM, it is necessary to incorporate syntactic and semantic features into the model. While there has been some encouraging work on integrating syntactic features into the RM, to the best of our knowledge, there has been no previous work on integrating semantic Reordering Model Zens and Ney (2006) Features Types lexical Cherry (2013) lexical Green et al. (2010) lexical syntactic Bisazza and Federico and Goto et al. (2013) Gao et al. (2011) and Kazemi et al. (2015) The proposed method (2013) lexical syntactic lexical syntactic lexical syntactic semantic Features surface forms of the source and target words unsupervised class of the source and target words surface forms of frequent source and target words unsupervised class of rare source and target words surface forms of the source words, POS tags of the source words, relative position of the source words sentence length surface forms and POS tags of the source words surface forms and POS tags of the"
2016.gwc-1.24,2009.iwslt-papers.4,0,0.0307022,"ad-dep) and 5,247,526 (dep-dep) pairs from our training data set and determined the orientation for each pair based on Equation 1. We then trained a Maximum Entropy classifier (Manning and Klein, 2003) (henceforth MaxEnt) on the extracted constituent pairs from the training data set and use it to predict the orientation probability of each pair of constituents in the tune and test data sets. As mentioned earlier, we used WordNet in order to determine the synset of the English words in the data set. Our baseline SMT system is the Moses implementation of the HPB-SMT model with default settings (Hoang et al., 2009). We used a 5-gram language model and trained it on the Farsi side of the training data set. All experiments used MIRA for tuning the weights of the features used in the HPB model (Cherry and Foster, 2012). The semantic features (synsets) are extracted from WordNet 3.0. For each word, we take the synset that corresponds to its first sense, i.e. the most common one. An alternative would be to apply a word sense disambiguation algorithm. However, these have been shown to perform worse than the first-sense heuristic when WordNet is the inventory of word senses, e.g. (Pedersen and Kolhatkar, 2009;"
2016.gwc-1.24,W15-4906,1,0.880771,"se models can not generalize to unseen words with equivalent meaning in the same context. In order to improve syntactic and semantic generalization of the RM, it is necessary to incorporate syntactic and semantic features into the model. While there has been some encouraging work on integrating syntactic features into the RM, to the best of our knowledge, there has been no previous work on integrating semantic Reordering Model Zens and Ney (2006) Features Types lexical Cherry (2013) lexical Green et al. (2010) lexical syntactic Bisazza and Federico and Goto et al. (2013) Gao et al. (2011) and Kazemi et al. (2015) The proposed method (2013) lexical syntactic lexical syntactic lexical syntactic semantic Features surface forms of the source and target words unsupervised class of the source and target words surface forms of frequent source and target words unsupervised class of rare source and target words surface forms of the source words, POS tags of the source words, relative position of the source words sentence length surface forms and POS tags of the source words surface forms and POS tags of the source context words surface forms of the source words dependency relation surface forms of the source w"
2016.gwc-1.24,N03-1017,0,0.0672375,"Missing"
2016.gwc-1.24,2005.iwslt-1.8,0,0.0456312,"of the translation, especially between languages with major differences in word order. Although SMT systems deliver state-of-theart performance in machine translation nowadays, they perform relatively weakly at addressing the reordering problem. Phrased-based SMT (PB-SMT) is arguably the most widely used approach to SMT to date. In this model, the translation operates on phrases, i.e. sequences of words whose length is between 1 and a maximum upper limit. In PB-SMT, reordering is generally captured by distance-based models (Koehn et al., 2003) and lexical phrase-based models (Tillmann, 2004; Koehn et al., 2005), which are able to perform local reordering but they cannot capture non-local (long-distance) reordering. The weakness of PB-SMT systems on handling long-distance reordering led to proposing the Hierarchical Phrase-based SMT (HPB-SMT) model(Chiang, 2005), in which the translation operates on tree structures (either derived from a syntactic parser or unsupervised). Despite the relatively good performance offered by HPB-SMT in medium-range reordering, they are still weak on long-distance reordering (Birch et al., 2009). A great deal of work has been carried out to address the reordering problem"
2016.gwc-1.24,C10-1081,0,0.0127693,"use a classifier to predict the probability of the orientation between each pair of constituents to be monotone or swap. This probability is used as one feature in the log-linear framework of the HPB-SMT model. Using a classifier enables us to incorporate fine-grained information in the form of features into our RM. Table 3 and Table 4 show the features that we use to characterize (head-dep) and (dep-dep) pairs respectively. As Table 3 and Table 4 show, we use three types of features: lexical, syntactic and semantic. While semantic structures have been previously used for MT reordering, e.g. (Liu and Gilda, 2010), to the best of our knowledge, this is the first work that includes semantic features jointly with lexical and syntactic features in the framework of a syntax-based RM. Using syntactic features, such as dependency relations, enables the RM to make syntactic generalizations. For instance, the RM can learn that in translating between subject-verbobject (SVO) and subject-object-verb (SOV) languages, the object and the verb should be swapped. On top of this syntactic generalization, the RM should be able to make semantic generalizations. To this end, we use WordNet synsets as an additional featur"
2016.gwc-1.24,N03-5008,0,0.0115234,"ed GIZA++ (Och and Ney, 2003) to align the words in the English and Farsi sentences. We parsed the English sentences of our parallel corpus with the Stanford dependency parser (Chen and Manning, 2014) and used the “collapsed representation” of its output which shows the direct dependencies between the words in the English sentence. Having obtained both dependency trees and the word alignments, we extracted 6,391,956 (head-dep) and 5,247,526 (dep-dep) pairs from our training data set and determined the orientation for each pair based on Equation 1. We then trained a Maximum Entropy classifier (Manning and Klein, 2003) (henceforth MaxEnt) on the extracted constituent pairs from the training data set and use it to predict the orientation probability of each pair of constituents in the tune and test data sets. As mentioned earlier, we used WordNet in order to determine the synset of the English words in the data set. Our baseline SMT system is the Moses implementation of the HPB-SMT model with default settings (Hoang et al., 2009). We used a 5-gram language model and trained it on the Farsi side of the training data set. All experiments used MIRA for tuning the weights of the features used in the HPB model (C"
2016.gwc-1.24,J03-1002,0,0.00595312,"in Figure 1. around one million sentences extracted from English novel books and their translation in Farsi. We randomly held out 3,000 and 1,000 sentence pairs for tuning and testing, respectively, and used the remaining sentence pairs for training. Table 5 shows statistics (number of words and sentences) of the data sets used for training, tuning and testing. Train Tune Test Unit sentences words sentences words sentences words English 1,016,758 13,919,071 3,000 40,831 1,000 13,165 Farsi 1,016,758 14,043,499 3,000 41,670 1,000 13,444 Table 5: Mizan parallel corpus statistics We used GIZA++ (Och and Ney, 2003) to align the words in the English and Farsi sentences. We parsed the English sentences of our parallel corpus with the Stanford dependency parser (Chen and Manning, 2014) and used the “collapsed representation” of its output which shows the direct dependencies between the words in the English sentence. Having obtained both dependency trees and the word alignments, we extracted 6,391,956 (head-dep) and 5,247,526 (dep-dep) pairs from our training data set and determined the orientation for each pair based on Equation 1. We then trained a Maximum Entropy classifier (Manning and Klein, 2003) (hen"
2016.gwc-1.24,P02-1040,0,0.094951,"m. Three different feature sets were examined in this paper, including information from (i) surface forms (surface), (ii) synsets (synset) and (iii) both surface forms and synsets (both). We build six MT systems, as shown in Table 6, according to the constituent pairs and feature sets examined. We compared our MT systems to the standard HPB-SMT system. Each MT system is tuned three times and we report the average scores obtained with multeval3 (Clark et al., 2011) on the MT outputs. The results obtained by each of the MT systems according to two widely used automatic evaluation metrics (BLEU (Papineni et al., 2002), and TER (Snover et al., 2006)) are shown in Table 7. The relative improvement of each evaluation metric over the baseline HPB is shown in columns dif f . Compared to the use of surface features, our novel semantic features based on WordNet synsets lead to better scores for both (head- dep) and (depdep) constituent pairs according to both evaluation metrics, BLEU and TER (except for the dd system in terms of TER, where there is a slight but insignificant increase (79.8 vs. 79.7)). As for future work, we propose to work mainly along the following two directions. First, an investigation of the"
2016.gwc-1.24,N09-5005,0,0.0354819,"settings (Hoang et al., 2009). We used a 5-gram language model and trained it on the Farsi side of the training data set. All experiments used MIRA for tuning the weights of the features used in the HPB model (Cherry and Foster, 2012). The semantic features (synsets) are extracted from WordNet 3.0. For each word, we take the synset that corresponds to its first sense, i.e. the most common one. An alternative would be to apply a word sense disambiguation algorithm. However, these have been shown to perform worse than the first-sense heuristic when WordNet is the inventory of word senses, e.g. (Pedersen and Kolhatkar, 2009; Snyder and Palmer, 2004). 4.2 Evaluation: MT Results We selected different feature sets for (head-dep) and (dep-dep) pairs from Table 3 and Table 4 respectively, then we used them in our MaxEnt classifier to determine the impact of our novel seFeatures lex(head),lex(dep) depRel(dep) syn(head),syn(dep) Type lexical syntactic semantic Description surface forms of the head and dependent word dependency relation of the dependent word synsets of the head and dependent word Table 3: Features for (head-dep) constituent pairs Features lex(head),lex(dep1),lex(dep2) depRel(dep1),depRel(dep2) syn(head)"
2016.gwc-1.24,P05-1034,0,0.0636829,"that follow the same semantic structure. 3 Method Following Kazemi et al. (2015) we implement a syntax-based RM for HPB-SMT based on the dependency tree of the source sentence. The dependency tree of a sentence shows the grammatical relation between pairs of head and dependent words in the sentence. As an example, Figure 1 shows the dependency tree of an English sentence. In this figure, the arrow with label “nsubj” from “fox” to “jumped” indicates that the dependent word “fox” is the subject of the head word “jumped”. Given the assumption that constituents move as a whole during translation (Quirk et al., 2005), we take the dependency tree of the source sentence and try to find the ordering of each dependent word with respect to its head (head-dep) and also with respect to the other dependants of that head (dep-dep). For example, for the English sentence in Figure 1, we try to predict the orientation between (head-dep) and (dep-dep) pairs as shown in Table 2. We consider two orientation types between the constituents: monotone and swap. If the order of two constituents in the source sentence is the same as the order of their translation in the target sentence, the orientation is monotone and otherwi"
2016.gwc-1.24,2006.amta-papers.25,0,0.0360075,"ere examined in this paper, including information from (i) surface forms (surface), (ii) synsets (synset) and (iii) both surface forms and synsets (both). We build six MT systems, as shown in Table 6, according to the constituent pairs and feature sets examined. We compared our MT systems to the standard HPB-SMT system. Each MT system is tuned three times and we report the average scores obtained with multeval3 (Clark et al., 2011) on the MT outputs. The results obtained by each of the MT systems according to two widely used automatic evaluation metrics (BLEU (Papineni et al., 2002), and TER (Snover et al., 2006)) are shown in Table 7. The relative improvement of each evaluation metric over the baseline HPB is shown in columns dif f . Compared to the use of surface features, our novel semantic features based on WordNet synsets lead to better scores for both (head- dep) and (depdep) constituent pairs according to both evaluation metrics, BLEU and TER (except for the dd system in terms of TER, where there is a slight but insignificant increase (79.8 vs. 79.7)). As for future work, we propose to work mainly along the following two directions. First, an investigation of the extent to which using a WordNet"
2016.gwc-1.24,W04-0811,0,0.0164981,"We used a 5-gram language model and trained it on the Farsi side of the training data set. All experiments used MIRA for tuning the weights of the features used in the HPB model (Cherry and Foster, 2012). The semantic features (synsets) are extracted from WordNet 3.0. For each word, we take the synset that corresponds to its first sense, i.e. the most common one. An alternative would be to apply a word sense disambiguation algorithm. However, these have been shown to perform worse than the first-sense heuristic when WordNet is the inventory of word senses, e.g. (Pedersen and Kolhatkar, 2009; Snyder and Palmer, 2004). 4.2 Evaluation: MT Results We selected different feature sets for (head-dep) and (dep-dep) pairs from Table 3 and Table 4 respectively, then we used them in our MaxEnt classifier to determine the impact of our novel seFeatures lex(head),lex(dep) depRel(dep) syn(head),syn(dep) Type lexical syntactic semantic Description surface forms of the head and dependent word dependency relation of the dependent word synsets of the head and dependent word Table 3: Features for (head-dep) constituent pairs Features lex(head),lex(dep1),lex(dep2) depRel(dep1),depRel(dep2) syn(head),syn(dep1),syn(dep2) Type"
2016.gwc-1.24,N04-4026,0,0.0558381,"t on the quality of the translation, especially between languages with major differences in word order. Although SMT systems deliver state-of-theart performance in machine translation nowadays, they perform relatively weakly at addressing the reordering problem. Phrased-based SMT (PB-SMT) is arguably the most widely used approach to SMT to date. In this model, the translation operates on phrases, i.e. sequences of words whose length is between 1 and a maximum upper limit. In PB-SMT, reordering is generally captured by distance-based models (Koehn et al., 2003) and lexical phrase-based models (Tillmann, 2004; Koehn et al., 2005), which are able to perform local reordering but they cannot capture non-local (long-distance) reordering. The weakness of PB-SMT systems on handling long-distance reordering led to proposing the Hierarchical Phrase-based SMT (HPB-SMT) model(Chiang, 2005), in which the translation operates on tree structures (either derived from a syntactic parser or unsupervised). Despite the relatively good performance offered by HPB-SMT in medium-range reordering, they are still weak on long-distance reordering (Birch et al., 2009). A great deal of work has been carried out to address t"
2016.gwc-1.24,W06-3108,0,0.21848,"ta to be able to perform required reordering between them. Likewise, if two words in the source language follow a specific reordering pattern in the target language, these models can not generalize to unseen words with equivalent meaning in the same context. In order to improve syntactic and semantic generalization of the RM, it is necessary to incorporate syntactic and semantic features into the model. While there has been some encouraging work on integrating syntactic features into the RM, to the best of our knowledge, there has been no previous work on integrating semantic Reordering Model Zens and Ney (2006) Features Types lexical Cherry (2013) lexical Green et al. (2010) lexical syntactic Bisazza and Federico and Goto et al. (2013) Gao et al. (2011) and Kazemi et al. (2015) The proposed method (2013) lexical syntactic lexical syntactic lexical syntactic semantic Features surface forms of the source and target words unsupervised class of the source and target words surface forms of frequent source and target words unsupervised class of rare source and target words surface forms of the source words, POS tags of the source words, relative position of the source words sentence length surface forms a"
2020.acl-main.359,P19-1019,0,0.0194186,"g of the Association for Computational Linguistics, pages 3898–3908 c July 5 - 10, 2020. 2020 Association for Computational Linguistics achieves state-of-the-art results for many domains and language pairs. However, NMT requires a lot more data than other paradigms (Koehn and Knowles, 2017), which makes it harder to adapt to low-resource scenarios (Sennrich and Zhang, 2019). Using synthetic parallel data via backtranslation has been helpful in some low-resource usecases (Dowling et al., 2019). For extreme cases with no bilingual parallel corpora, unsupervised MT can obtain reasonable results (Artetxe et al., 2019; Lample and Conneau, 2019). However, its application to real low-resource scenarios is still a matter of study (Marchisio et al., 2020). In this work we are motivated by a real-world lowresource use-case, namely the translation of clinical texts from Basque to Spanish (EU-ES). Basque is a minority language, so most of the Electronic Health Records (EHR) are written in Spanish so that any doctor from the Basque public health service can understand them. The development of a system for translating clinical texts from Basque to Spanish could allow Basque-speaking doctors to write EHRs in Basque,"
2020.acl-main.359,W05-0909,0,0.053977,"ion of backtranslated data on the overall translation performance. Regarding the use of dataselection techniques in conjunction with synthetic data, Poncelas and Way (2019) fine-tune NMT models with sentences selected from a backtranslated set, and Chinea-Rios et al. (2017) select monolingual source-side sentences to generate synthetic target strings to improve the translation model. While the most common approach to assessing the translation capabilities of a MT system is via evaluation scores such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), chrF (Popovi´c, 2015), and METEOR (Banerjee and Lavie, 2005), recently research has begun to address another side of quality of translated text, namely lexical richness and diversity. In a recent paper, Vanmassenhove et al. (2019) study the loss of lexical diversity and richness of the same corpora translated with PBSMT and NMT systems. Vanmassenhove et al. (2019) investigate the problem for seen (during MT training) and unseen text using MT systems trained on the Europarl corpus (Koehn, 2005), with original (human-produced and translated) text as well as in a round-trip-translation setting.1 In this work we calculate the same lexical diversity metrics"
2020.acl-main.359,W19-5403,0,0.0194916,"onic Health Records (EHR) are written in Spanish so that any doctor from the Basque public health service can understand them. The development of a system for translating clinical texts from Basque to Spanish could allow Basque-speaking doctors to write EHRs in Basque, thus contributing to the normalisation of the language in specialised areas. We conduct our analysis in the scope of the EUES translation of EHR use-case, as well as on a language pair and a data set that have been well studied in the literature – German to English (DEEN) data used in the WMT Biomedical Translation Shared Task (Bawden et al., 2019). As the EU-ES medical data cannot be made publicly available due to privacy regulations, using the DE-EN data is a way to allow for the replicability of our work. 2 Related Work In this work we extend these ideas by combining backtranslated data from RBMT, PB-SMT, NMT (LSTM) and NMT (Transformer); in addition, we use FDA to select sentences translated by different systems and analyse the impact of data selection of backtranslated data on the overall translation performance. Regarding the use of dataselection techniques in conjunction with synthetic data, Poncelas and Way (2019) fine-tune NMT"
2020.acl-main.359,W18-6315,0,0.0985456,"e corpora translated with PBSMT and NMT systems. Vanmassenhove et al. (2019) investigate the problem for seen (during MT training) and unseen text using MT systems trained on the Europarl corpus (Koehn, 2005), with original (human-produced and translated) text as well as in a round-trip-translation setting.1 In this work we calculate the same lexical diversity metrics as Vanmassenhove et al. (2019), and further use those metrics to improve the data selection process applied to backtranslated data. 3 One of the first papers comparing the performance of different systems for backtranslation was Burlot and Yvon (2018). The authors compared SMT and NMT systems, obtaining similar results. Closer to our work, Soto et al. (2019) also try RBMT, PBSMT and NMT systems for backtranslating EHRs from Spanish into Basque. However, both papers are limited to comparing the performance of systems trained with backtranslated data originating from a single source, without examining whether a combination might be more effective. More recently Poncelas et al. (2019) combined the outputs of PB-SMT and NMT systems used for backtranslation, showing that the combination of synthetic data originating from different sources was u"
2020.acl-main.359,W17-4714,0,0.0653456,"due to privacy regulations, using the DE-EN data is a way to allow for the replicability of our work. 2 Related Work In this work we extend these ideas by combining backtranslated data from RBMT, PB-SMT, NMT (LSTM) and NMT (Transformer); in addition, we use FDA to select sentences translated by different systems and analyse the impact of data selection of backtranslated data on the overall translation performance. Regarding the use of dataselection techniques in conjunction with synthetic data, Poncelas and Way (2019) fine-tune NMT models with sentences selected from a backtranslated set, and Chinea-Rios et al. (2017) select monolingual source-side sentences to generate synthetic target strings to improve the translation model. While the most common approach to assessing the translation capabilities of a MT system is via evaluation scores such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), chrF (Popovi´c, 2015), and METEOR (Banerjee and Lavie, 2005), recently research has begun to address another side of quality of translated text, namely lexical richness and diversity. In a recent paper, Vanmassenhove et al. (2019) study the loss of lexical diversity and richness of the same corpora translate"
2020.acl-main.359,L16-1560,0,0.0408534,"Missing"
2020.acl-main.359,W11-2123,0,0.0313201,"ng those systems, as well as a RBMT one. RBMT: We use Apertium (Forcada et al., 2011) for the EN-DE language pair, and Matxin (Mayor, 2007) for ES-EU, adapted to the clinical domain by the inclusion of the same dictionaries used to train the other systems. PB-SMT: We use Moses with default parameters, using MGIZA for word alignment (Och and Ney, 4 https://ufal.mff.cuni.cz/ufal_ medical_corpus 5 We used the clean-corpus-n.pl script provided with the Moses toolkit (Koehn et al., 2007). 6 http://www.himl.eu/test-sets 3901 2003), an “msd-bidirectional-fe” lexicalised reordering model and a KenLM (Heafield, 2011) 5gram target language model. We tuned the model using Minimum Error Rate Training (Och, 2003) with an n-best list of length 100. LSTM: We use an RNN of 4 layers, with LSTM units of size 512, dropout of 0.2 and a batch-size of 128. We use Adam (Kingma and Ba, 2015) as the learning optimiser, with a learning rate of 0.0001 and 2,000 warmup steps. Transformer: We train a Transformer model with the hyperparameters recommended by OpenNMT,7 halving the batch-size so that it could fit in 2 GPUs, and accordingly doubling the value for gradient accumulation. We train all NMT systems using OpenNMT (Kle"
2020.acl-main.359,W17-4775,0,0.018064,"the amount of data used, while at the same time maintaining highquality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance. 1 Introduction The use of supplementary backtranslated text has led to improved results in several tasks such as automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016; Hokamp, 2017), machine translation (MT) (Sennrich et al., 2016a; Poncelas et al., 2018b), and quality estimation (Yankovskaya et al., 2019). Backtranslated text is a translation of a monolingual corpus in the target language (L2) into the source language (L1) via an already existing MT system, so that the aligned monolingual corpus and its translation can form an L1–L2 parallel corpus. This corpus of synthetic parallel data can then be used for training, typically alongside authentic human-translated data. For MT, backtranslation has become a standard approach to improving the performance of systems when a"
2020.acl-main.359,W16-2378,0,0.0180928,"selection strategies in order to reduce the amount of data used, while at the same time maintaining highquality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance. 1 Introduction The use of supplementary backtranslated text has led to improved results in several tasks such as automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016; Hokamp, 2017), machine translation (MT) (Sennrich et al., 2016a; Poncelas et al., 2018b), and quality estimation (Yankovskaya et al., 2019). Backtranslated text is a translation of a monolingual corpus in the target language (L2) into the source language (L1) via an already existing MT system, so that the aligned monolingual corpus and its translation can form an L1–L2 parallel corpus. This corpus of synthetic parallel data can then be used for training, typically alongside authentic human-translated data. For MT, backtranslation has become a standard approach to improving the performance of"
2020.acl-main.359,D13-1176,0,0.0117966,"ktranslated data from several MT systems and investigate multiple approaches to data selection for backtranslated data based on the Feature Decay Algorithms (FDA: Bic¸ici and Yuret (2015); Poncelas et al. (2018a)) method. We exploit different ways of ranking the data and extracting parallel sentences; we also interleave quality evaluation and lexical diversity/richness information into the ranking process. While our empirical evaluation shows different results for the tested language pairs, this is the first work in this direction and lays a firm foundation for future research. Nowadays, NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), and in particular Transformer (Vaswani et al., 2017) 3898 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3898–3908 c July 5 - 10, 2020. 2020 Association for Computational Linguistics achieves state-of-the-art results for many domains and language pairs. However, NMT requires a lot more data than other paradigms (Koehn and Knowles, 2017), which makes it harder to adapt to low-resource scenarios (Sennrich and Zhang, 2019). Using synthetic parallel data via backtranslation has been helpful in some lo"
2020.acl-main.359,P17-4012,0,0.041824,"11) 5gram target language model. We tuned the model using Minimum Error Rate Training (Och, 2003) with an n-best list of length 100. LSTM: We use an RNN of 4 layers, with LSTM units of size 512, dropout of 0.2 and a batch-size of 128. We use Adam (Kingma and Ba, 2015) as the learning optimiser, with a learning rate of 0.0001 and 2,000 warmup steps. Transformer: We train a Transformer model with the hyperparameters recommended by OpenNMT,7 halving the batch-size so that it could fit in 2 GPUs, and accordingly doubling the value for gradient accumulation. We train all NMT systems using OpenNMT (Klein et al., 2017) for a maximum of 200,000 steps, and select the model that obtains the highest BLEU score on the devset; note that the final systems trained after applying data selection use early stopping with perplexity not decreasing in 3 consecutive steps as our stopping criterion. Backtranslation is performed with the default hyperparameters, including a beam-width of 5 and a batch-size of 30. We use Moses scripts to tokenise and truecase all the corpora to be used for statistical or neural systems. For the NMT systems, we apply BPE (Sennrich et al., 2016b) on the concatenated bilingual corpora with 90,0"
2020.acl-main.359,2005.mtsummit-papers.11,0,0.176272,"bilities of a MT system is via evaluation scores such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), chrF (Popovi´c, 2015), and METEOR (Banerjee and Lavie, 2005), recently research has begun to address another side of quality of translated text, namely lexical richness and diversity. In a recent paper, Vanmassenhove et al. (2019) study the loss of lexical diversity and richness of the same corpora translated with PBSMT and NMT systems. Vanmassenhove et al. (2019) investigate the problem for seen (during MT training) and unseen text using MT systems trained on the Europarl corpus (Koehn, 2005), with original (human-produced and translated) text as well as in a round-trip-translation setting.1 In this work we calculate the same lexical diversity metrics as Vanmassenhove et al. (2019), and further use those metrics to improve the data selection process applied to backtranslated data. 3 One of the first papers comparing the performance of different systems for backtranslation was Burlot and Yvon (2018). The authors compared SMT and NMT systems, obtaining similar results. Closer to our work, Soto et al. (2019) also try RBMT, PBSMT and NMT systems for backtranslating EHRs from Spanish i"
2020.acl-main.359,P07-2045,0,0.0218351,"Missing"
2020.acl-main.359,W17-3204,0,0.046447,"hows different results for the tested language pairs, this is the first work in this direction and lays a firm foundation for future research. Nowadays, NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), and in particular Transformer (Vaswani et al., 2017) 3898 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3898–3908 c July 5 - 10, 2020. 2020 Association for Computational Linguistics achieves state-of-the-art results for many domains and language pairs. However, NMT requires a lot more data than other paradigms (Koehn and Knowles, 2017), which makes it harder to adapt to low-resource scenarios (Sennrich and Zhang, 2019). Using synthetic parallel data via backtranslation has been helpful in some low-resource usecases (Dowling et al., 2019). For extreme cases with no bilingual parallel corpora, unsupervised MT can obtain reasonable results (Artetxe et al., 2019; Lample and Conneau, 2019). However, its application to real low-resource scenarios is still a matter of study (Marchisio et al., 2020). In this work we are motivated by a real-world lowresource use-case, namely the translation of clinical texts from Basque to Spanish ("
2020.acl-main.359,P03-1021,0,0.019804,"language pair, and Matxin (Mayor, 2007) for ES-EU, adapted to the clinical domain by the inclusion of the same dictionaries used to train the other systems. PB-SMT: We use Moses with default parameters, using MGIZA for word alignment (Och and Ney, 4 https://ufal.mff.cuni.cz/ufal_ medical_corpus 5 We used the clean-corpus-n.pl script provided with the Moses toolkit (Koehn et al., 2007). 6 http://www.himl.eu/test-sets 3901 2003), an “msd-bidirectional-fe” lexicalised reordering model and a KenLM (Heafield, 2011) 5gram target language model. We tuned the model using Minimum Error Rate Training (Och, 2003) with an n-best list of length 100. LSTM: We use an RNN of 4 layers, with LSTM units of size 512, dropout of 0.2 and a batch-size of 128. We use Adam (Kingma and Ba, 2015) as the learning optimiser, with a learning rate of 0.0001 and 2,000 warmup steps. Transformer: We train a Transformer model with the hyperparameters recommended by OpenNMT,7 halving the batch-size so that it could fit in 2 GPUs, and accordingly doubling the value for gradient accumulation. We train all NMT systems using OpenNMT (Klein et al., 2017) for a maximum of 200,000 steps, and select the model that obtains the highest"
2020.acl-main.359,J03-1002,0,0.0545775,"Missing"
2020.acl-main.359,P02-1040,0,0.107026,"select sentences translated by different systems and analyse the impact of data selection of backtranslated data on the overall translation performance. Regarding the use of dataselection techniques in conjunction with synthetic data, Poncelas and Way (2019) fine-tune NMT models with sentences selected from a backtranslated set, and Chinea-Rios et al. (2017) select monolingual source-side sentences to generate synthetic target strings to improve the translation model. While the most common approach to assessing the translation capabilities of a MT system is via evaluation scores such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), chrF (Popovi´c, 2015), and METEOR (Banerjee and Lavie, 2005), recently research has begun to address another side of quality of translated text, namely lexical richness and diversity. In a recent paper, Vanmassenhove et al. (2019) study the loss of lexical diversity and richness of the same corpora translated with PBSMT and NMT systems. Vanmassenhove et al. (2019) investigate the problem for seen (during MT training) and unseen text using MT systems trained on the Europarl corpus (Koehn, 2005), with original (human-produced and translated) text as well as in a roun"
2020.acl-main.359,J82-2005,0,0.575514,"Missing"
2020.acl-main.359,W19-8629,1,0.854396,"tion Shared Task (Bawden et al., 2019). As the EU-ES medical data cannot be made publicly available due to privacy regulations, using the DE-EN data is a way to allow for the replicability of our work. 2 Related Work In this work we extend these ideas by combining backtranslated data from RBMT, PB-SMT, NMT (LSTM) and NMT (Transformer); in addition, we use FDA to select sentences translated by different systems and analyse the impact of data selection of backtranslated data on the overall translation performance. Regarding the use of dataselection techniques in conjunction with synthetic data, Poncelas and Way (2019) fine-tune NMT models with sentences selected from a backtranslated set, and Chinea-Rios et al. (2017) select monolingual source-side sentences to generate synthetic target strings to improve the translation model. While the most common approach to assessing the translation capabilities of a MT system is via evaluation scores such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), chrF (Popovi´c, 2015), and METEOR (Banerjee and Lavie, 2005), recently research has begun to address another side of quality of translated text, namely lexical richness and diversity. In a recent paper, Vanm"
2020.acl-main.359,W15-3049,0,0.0279795,"Missing"
2020.acl-main.359,P16-1009,0,0.174123,"me time maintaining highquality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance. 1 Introduction The use of supplementary backtranslated text has led to improved results in several tasks such as automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016; Hokamp, 2017), machine translation (MT) (Sennrich et al., 2016a; Poncelas et al., 2018b), and quality estimation (Yankovskaya et al., 2019). Backtranslated text is a translation of a monolingual corpus in the target language (L2) into the source language (L1) via an already existing MT system, so that the aligned monolingual corpus and its translation can form an L1–L2 parallel corpus. This corpus of synthetic parallel data can then be used for training, typically alongside authentic human-translated data. For MT, backtranslation has become a standard approach to improving the performance of systems when additional monolingual data in the target language"
2020.acl-main.359,P16-1162,0,0.491526,"me time maintaining highquality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance. 1 Introduction The use of supplementary backtranslated text has led to improved results in several tasks such as automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016; Hokamp, 2017), machine translation (MT) (Sennrich et al., 2016a; Poncelas et al., 2018b), and quality estimation (Yankovskaya et al., 2019). Backtranslated text is a translation of a monolingual corpus in the target language (L2) into the source language (L1) via an already existing MT system, so that the aligned monolingual corpus and its translation can form an L1–L2 parallel corpus. This corpus of synthetic parallel data can then be used for training, typically alongside authentic human-translated data. For MT, backtranslation has become a standard approach to improving the performance of systems when additional monolingual data in the target language"
2020.acl-main.359,P19-1021,0,0.021812,"direction and lays a firm foundation for future research. Nowadays, NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), and in particular Transformer (Vaswani et al., 2017) 3898 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3898–3908 c July 5 - 10, 2020. 2020 Association for Computational Linguistics achieves state-of-the-art results for many domains and language pairs. However, NMT requires a lot more data than other paradigms (Koehn and Knowles, 2017), which makes it harder to adapt to low-resource scenarios (Sennrich and Zhang, 2019). Using synthetic parallel data via backtranslation has been helpful in some low-resource usecases (Dowling et al., 2019). For extreme cases with no bilingual parallel corpora, unsupervised MT can obtain reasonable results (Artetxe et al., 2019; Lample and Conneau, 2019). However, its application to real low-resource scenarios is still a matter of study (Marchisio et al., 2020). In this work we are motivated by a real-world lowresource use-case, namely the translation of clinical texts from Basque to Spanish (EU-ES). Basque is a minority language, so most of the Electronic Health Records (EHR)"
2020.acl-main.359,W18-6323,1,0.775751,"e s and CL (ngr) is the number of occurrences of the n-gram ngr in L. The score is then used to rank sentences, with the one with the highest score being selected and added to L. This process is repeated iteratively. To avoid selecting sentences containing the same n-grams, score(s, Sseed , L) applies a penalty to the n-grams (up to order three in the default configuration) proportional to the occurrences that have been already selected. In (1), the term 0.5CL (ngr) is used as the penalty. In the context of MT, FDA has been shown to obtain better results than other methods for data selection (Silva et al., 2018). Acordingly, in this work we too focus on FDA, although our rescoring idea is more general and can be applied to other selection methods based on n-gram overlap. Related work on quality and lexical diversity and richness of MT demonstrates that (i) regardless of the overall performance of an MT system (as measured by both automatic and human evaluation), in general machine-translated text is error-prone and cannot reach human quality (Toral et al., 2018)); and (ii) machine-translated text lacks the lexical richness and diversity of human-translated (or postedited) text (Vanmassenhove et al.,"
2020.acl-main.359,2006.amta-papers.25,0,0.0339419,"y different systems and analyse the impact of data selection of backtranslated data on the overall translation performance. Regarding the use of dataselection techniques in conjunction with synthetic data, Poncelas and Way (2019) fine-tune NMT models with sentences selected from a backtranslated set, and Chinea-Rios et al. (2017) select monolingual source-side sentences to generate synthetic target strings to improve the translation model. While the most common approach to assessing the translation capabilities of a MT system is via evaluation scores such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), chrF (Popovi´c, 2015), and METEOR (Banerjee and Lavie, 2005), recently research has begun to address another side of quality of translated text, namely lexical richness and diversity. In a recent paper, Vanmassenhove et al. (2019) study the loss of lexical diversity and richness of the same corpora translated with PBSMT and NMT systems. Vanmassenhove et al. (2019) investigate the problem for seen (during MT training) and unseen text using MT systems trained on the Europarl corpus (Koehn, 2005), with original (human-produced and translated) text as well as in a round-trip-translation setting."
2020.acl-main.359,W19-7102,1,0.835743,"Missing"
2020.acl-main.359,W18-6312,1,0.844404,"0.5CL (ngr) is used as the penalty. In the context of MT, FDA has been shown to obtain better results than other methods for data selection (Silva et al., 2018). Acordingly, in this work we too focus on FDA, although our rescoring idea is more general and can be applied to other selection methods based on n-gram overlap. Related work on quality and lexical diversity and richness of MT demonstrates that (i) regardless of the overall performance of an MT system (as measured by both automatic and human evaluation), in general machine-translated text is error-prone and cannot reach human quality (Toral et al., 2018)); and (ii) machine-translated text lacks the lexical richness and diversity of human-translated (or postedited) text (Vanmassenhove et al., 2019). In its operation, FDA compares two types of text – the seed and the candidate sentences – without taking into account the quality or the lexical diversity/richness of the candidate text. Our hypothesis is that when selecting data from different sources, FDA cannot account for the differences in quality and lexical diversity/richness of these texts, with the consequence that the selected set (L) is sub-optimal. We test our hypothesis by assessing th"
2020.acl-main.359,W19-6622,1,0.70301,"019) fine-tune NMT models with sentences selected from a backtranslated set, and Chinea-Rios et al. (2017) select monolingual source-side sentences to generate synthetic target strings to improve the translation model. While the most common approach to assessing the translation capabilities of a MT system is via evaluation scores such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), chrF (Popovi´c, 2015), and METEOR (Banerjee and Lavie, 2005), recently research has begun to address another side of quality of translated text, namely lexical richness and diversity. In a recent paper, Vanmassenhove et al. (2019) study the loss of lexical diversity and richness of the same corpora translated with PBSMT and NMT systems. Vanmassenhove et al. (2019) investigate the problem for seen (during MT training) and unseen text using MT systems trained on the Europarl corpus (Koehn, 2005), with original (human-produced and translated) text as well as in a round-trip-translation setting.1 In this work we calculate the same lexical diversity metrics as Vanmassenhove et al. (2019), and further use those metrics to improve the data selection process applied to backtranslated data. 3 One of the first papers comparing t"
2020.acl-main.359,W19-5410,0,0.0457987,"Missing"
2020.acl-main.359,2020.wmt-1.68,0,\N,Missing
2020.acl-srw.25,P08-2064,0,0.0188223,"g systems using only cosine similarity. 3 Experimental Framework The continuum of morphologically rich languages is quite diverse with the one end of the continuum being agglutinative languages, that primarily rely on discrete particles for inflection, and the other being fusional languages, which tend to use a single inflectional morpheme to denote multiple features. While it may be worthwhile to investigate if the same unsupervised methods work across different language categories, it can be expected that if further processing is needed, different approaches have to be taken. Decompounding (Alfonseca et al., 2008) may be more useful for agglutinative languages to tackle the OOV problem, and for many fusional languages internal change and suppletion call for different approaches. In our study we focus on fusional languages. English is primarily an analytic language and Icelandic a fusional language with moderately rich morphology. We will be using the English-Icelandic language pair as a test case. 184 3.1 Data ParIce, an English-Icelandic parallel corpus, was compiled from data consisting of 4.3 million translation segments. It was aligned with LF Aligner, which uses Hunalign (Varga et al., 2005), and"
2020.acl-srw.25,D16-1250,0,0.0270999,"hipour et al. (2011) do outlier detection and show that their filtered corpus results in improved translation quality, even though sentences have been removed. Sarikaya et al. (2009) use context extrapolation to boost the sentence pair coverage, checking whether the distance of the sentences from an anchor point is the same, and whether the sentences have the highest similarity score compared to other pairs within a window, despite being below a defined threshold. Crosslingual word embeddings have been used to calculate distance between equivalences in different languages (Luong et al., 2015; Artetxe et al., 2016). Defauw et al. (2019) treat filtering as a supervised regression problem and show that Levenshtein distance (Levenshtein, 1966) between the target and MT-translated source, as well as cosine distance between sentence embeddings of the source and target, are important features. While they use InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019) has recently been employed for calculating crosslingual semantic textual similarity to detect misalignment with good results (Lo and Simard, 2019). Zipporah (Xu and Koehn, 2017) uses a logistic regression model trained to classify sentence pairs"
2020.acl-srw.25,P18-1073,0,0.126734,"cted information using basic NLP tools to effectively handle rich morphology. 1 Introduction Machine translation (MT) quality has improved substantially with the advent of neural machine translation systems (NMT). However, while the quality gains over statistical machine translation (SMT) systems can be large, in low-resource and domain mismatch settings they are significantly reduced (Koehn and Knowles, 2017). In recent years, unsupervised NMT trained only on monolingual corpora has attracted considerable attention, and has been proposed for scenarios where there is a lack of bilingual data (Artetxe et al., 2018b; Lample et al., 2018). These methods have been shown to perform well for related language pairs (e.g. Wu et al. (2019)), but as the languages differ more the unsupervised methods become less effective (Leng et al., 2019). Kim et al. (2020) show that supervised and semi-supervised baselines with only a small parallel corpus of 50K bilingual sentences Andy Way School of Computing ADAPT Centre Dublin City University Ireland andy.way @adaptcentre.ie consistently outperform the best unsupervised systems for a range of languages, similar and distant. They also show that unsupervised NMT is very se"
2020.acl-srw.25,D18-1399,0,0.11859,"cted information using basic NLP tools to effectively handle rich morphology. 1 Introduction Machine translation (MT) quality has improved substantially with the advent of neural machine translation systems (NMT). However, while the quality gains over statistical machine translation (SMT) systems can be large, in low-resource and domain mismatch settings they are significantly reduced (Koehn and Knowles, 2017). In recent years, unsupervised NMT trained only on monolingual corpora has attracted considerable attention, and has been proposed for scenarios where there is a lack of bilingual data (Artetxe et al., 2018b; Lample et al., 2018). These methods have been shown to perform well for related language pairs (e.g. Wu et al. (2019)), but as the languages differ more the unsupervised methods become less effective (Leng et al., 2019). Kim et al. (2020) show that supervised and semi-supervised baselines with only a small parallel corpus of 50K bilingual sentences Andy Way School of Computing ADAPT Centre Dublin City University Ireland andy.way @adaptcentre.ie consistently outperform the best unsupervised systems for a range of languages, similar and distant. They also show that unsupervised NMT is very se"
2020.acl-srw.25,P19-1309,0,0.112039,"ted hard rules to detect flawed sentences and then proceeds to use a random forest classifier based on lexical translations and several shallow features such as respective length, matching numbers and punctuation. Finally, it scores sentences based on fluency using 5-gram language models. In 2019, at the fourth Conference on Machine Translation, WMT, the shared task on parallel corpora filtering focused on low-resource conditions. The method central to the best-performing submission was the use of crosslingual sentence embeddings, trained from parallel sentence pairs (Chaudhary et al., 2019). Artetxe and Schwenk (2019a) devised a similar method. Both papers tackle the inconsistencies of cosine similarity by investigating the neighbourhood of a given sentence pair, outperforming systems using only cosine similarity. 3 Experimental Framework The continuum of morphologically rich languages is quite diverse with the one end of the continuum being agglutinative languages, that primarily rely on discrete particles for inflection, and the other being fusional languages, which tend to use a single inflectional morpheme to denote multiple features. While it may be worthwhile to investigate if the same unsupervised"
2020.acl-srw.25,W19-6115,1,0.88767,"Missing"
2020.acl-srw.25,W19-6116,1,0.69868,"gment ParIce corpus, there are approx. 3.7 million good segments and around 600K faulty ones. Many of the faulty segments in the corpus are due to misalignment. We will be working with the raw data that made up the 4.3 million segment ParIce corpus. In order to compile a better corpus, we need improved alignment methods to reduce the number of faulty alignments, and we need a classifier that is able to identify the quality of the segments with high precision and recall in order to build as big a corpus as possible with as few faulty segments as possible. 3.2 an external morphological lexicon (Bjarnadóttir et al., 2019). Lemmatising will be carried out using Nefnir (Ingólfsdóttir et al., 2019). For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy.2 We will focus on the most common word embedding models: word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018). As using bilingual sentence embeddings with BERT has been shown to be effective for filtering (Lo and Simard, 2019), we want to experiment with different contextualized embedding models. The main hindrance with these models is t"
2020.acl-srw.25,Q17-1010,0,0.0074231,"r of faulty alignments, and we need a classifier that is able to identify the quality of the segments with high precision and recall in order to build as big a corpus as possible with as few faulty segments as possible. 3.2 an external morphological lexicon (Bjarnadóttir et al., 2019). Lemmatising will be carried out using Nefnir (Ingólfsdóttir et al., 2019). For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy.2 We will focus on the most common word embedding models: word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018). As using bilingual sentence embeddings with BERT has been shown to be effective for filtering (Lo and Simard, 2019), we want to experiment with different contextualized embedding models. The main hindrance with these models is the massive computational resources needed to train, which may limit our possibilites. For alignment and filtering we experiment with Bleualign, Hunalign and vecalign for sentence alignment, Giza++ (Och and Ney, 2003) for word alignments, and Zipporah, BiCleaner and LASER (Artetxe and Schwenk, 2019b) for filtering, and possibly to help wi"
2020.acl-srw.25,J93-2003,0,0.14737,"Missing"
2020.acl-srw.25,P91-1022,0,0.817017,"ignment the highest-scoring sentences can be used as anchors: elements in the data that can reliably be aligned and thus direct further processing. In the next subsections, we describe alignment and filtering methods used in prior work. 2.1 Alignment The first approaches to automatic sentence alignment were length-based. Gale and Church (1991) found that “the correlation between the length of a paragraph in characters and the length of its translation was extremely high”. Motivated by that, they describe a method for aligning sentences based on a simple statistical model of character lengths. Brown et al. (1991) also describe a length-based method, but use tokens instead of characters. In addition, they use signals in the markup as anchor points to segment the corpus into smaller chunks. Kay and Röscheisen (1993) used bilingual lexicons induced from the corpus being aligned. 183 Haruno and Yamazaki (1996) show that combining an induced lexicon with an external dictionary yields better results. Papageorgiou et al. (1994) use part-of-speech, commonly preserved in translation, by computing the optimum alignment based on the PoS-tags. Tschorn and Lüdeling (2003) use a morphological analyzer to improve a"
2020.acl-srw.25,W19-5435,0,0.0148751,"8) uses a set of handcrafted hard rules to detect flawed sentences and then proceeds to use a random forest classifier based on lexical translations and several shallow features such as respective length, matching numbers and punctuation. Finally, it scores sentences based on fluency using 5-gram language models. In 2019, at the fourth Conference on Machine Translation, WMT, the shared task on parallel corpora filtering focused on low-resource conditions. The method central to the best-performing submission was the use of crosslingual sentence embeddings, trained from parallel sentence pairs (Chaudhary et al., 2019). Artetxe and Schwenk (2019a) devised a similar method. Both papers tackle the inconsistencies of cosine similarity by investigating the neighbourhood of a given sentence pair, outperforming systems using only cosine similarity. 3 Experimental Framework The continuum of morphologically rich languages is quite diverse with the one end of the continuum being agglutinative languages, that primarily rely on discrete particles for inflection, and the other being fusional languages, which tend to use a single inflectional morpheme to denote multiple features. While it may be worthwhile to investigat"
2020.acl-srw.25,D17-1070,0,0.0108561,"and whether the sentences have the highest similarity score compared to other pairs within a window, despite being below a defined threshold. Crosslingual word embeddings have been used to calculate distance between equivalences in different languages (Luong et al., 2015; Artetxe et al., 2016). Defauw et al. (2019) treat filtering as a supervised regression problem and show that Levenshtein distance (Levenshtein, 1966) between the target and MT-translated source, as well as cosine distance between sentence embeddings of the source and target, are important features. While they use InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019) has recently been employed for calculating crosslingual semantic textual similarity to detect misalignment with good results (Lo and Simard, 2019). Zipporah (Xu and Koehn, 2017) uses a logistic regression model trained to classify sentence pairs. Noisy data is synthesized and used as negative samples in training. BiCleaner (Sánchez-Cartagena et al., 2018) uses a set of handcrafted hard rules to detect flawed sentences and then proceeds to use a random forest classifier based on lexical translations and several shallow features such as respective length, matching nu"
2020.acl-srw.25,N19-1423,0,0.00517903,"ve the highest similarity score compared to other pairs within a window, despite being below a defined threshold. Crosslingual word embeddings have been used to calculate distance between equivalences in different languages (Luong et al., 2015; Artetxe et al., 2016). Defauw et al. (2019) treat filtering as a supervised regression problem and show that Levenshtein distance (Levenshtein, 1966) between the target and MT-translated source, as well as cosine distance between sentence embeddings of the source and target, are important features. While they use InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019) has recently been employed for calculating crosslingual semantic textual similarity to detect misalignment with good results (Lo and Simard, 2019). Zipporah (Xu and Koehn, 2017) uses a logistic regression model trained to classify sentence pairs. Noisy data is synthesized and used as negative samples in training. BiCleaner (Sánchez-Cartagena et al., 2018) uses a set of handcrafted hard rules to detect flawed sentences and then proceeds to use a random forest classifier based on lexical translations and several shallow features such as respective length, matching numbers and punctuation. Final"
2020.acl-srw.25,P91-1023,0,0.765747,"ments with a corresponding meaning to that of source segments in multilingual texts. While these may seem to be different tasks, the same methods may apply partly to both problems. Filtering is often done by scoring sentences and removing the lowest-scoring ones, whereas in alignment the highest-scoring sentences can be used as anchors: elements in the data that can reliably be aligned and thus direct further processing. In the next subsections, we describe alignment and filtering methods used in prior work. 2.1 Alignment The first approaches to automatic sentence alignment were length-based. Gale and Church (1991) found that “the correlation between the length of a paragraph in characters and the length of its translation was extremely high”. Motivated by that, they describe a method for aligning sentences based on a simple statistical model of character lengths. Brown et al. (1991) also describe a length-based method, but use tokens instead of characters. In addition, they use signals in the markup as anchor points to segment the corpus into smaller chunks. Kay and Röscheisen (1993) used bilingual lexicons induced from the corpus being aligned. 183 Haruno and Yamazaki (1996) show that combining an ind"
2020.acl-srw.25,P96-1018,0,0.406613,"e alignment were length-based. Gale and Church (1991) found that “the correlation between the length of a paragraph in characters and the length of its translation was extremely high”. Motivated by that, they describe a method for aligning sentences based on a simple statistical model of character lengths. Brown et al. (1991) also describe a length-based method, but use tokens instead of characters. In addition, they use signals in the markup as anchor points to segment the corpus into smaller chunks. Kay and Röscheisen (1993) used bilingual lexicons induced from the corpus being aligned. 183 Haruno and Yamazaki (1996) show that combining an induced lexicon with an external dictionary yields better results. Papageorgiou et al. (1994) use part-of-speech, commonly preserved in translation, by computing the optimum alignment based on the PoS-tags. Tschorn and Lüdeling (2003) use a morphological analyzer to improve a dictionary-based distance measure, and Ma (2006) increases the robustness of a lexicon-based aligner by assigning greater weights to less frequent translated words. Sennrich and Volk (2010) use machine translations and BLEU (Papineni et al., 2002) as a similarity score to find reliable alignments t"
2020.acl-srw.25,W18-2709,0,0.041907,"ingual sentences Andy Way School of Computing ADAPT Centre Dublin City University Ireland andy.way @adaptcentre.ie consistently outperform the best unsupervised systems for a range of languages, similar and distant. They also show that unsupervised NMT is very sensitive to domain mismatch, which poses a problem to low-resource language pairs where it can be difficult to match the data domain on both sides. Thus, it is evident that to achieve high quality MT, sentence aligned-texts in two or more languages are required. NMT systems have been shown to be sensitive to noise in the training data (Khayrallah and Koehn, 2018), where noise is defined as segments that decrease output quality of systems trained on the data. It is, therefore, important to be able to accurately align multilingual texts and precisely filter out misalignments and bad translations that adversely affect performance. In the study, conducted on the impact of various types of noise on MT quality, untranslated and misaligned segments had the most detrimental effect. Misaligned segments were by far the most prevalent type of noise in the ParaCrawl1 parallel corpus they used, twice as common as accepted segments. However, misalignments vary; a s"
2020.acl-srw.25,J03-1002,0,0.0522467,"n the most common word embedding models: word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018). As using bilingual sentence embeddings with BERT has been shown to be effective for filtering (Lo and Simard, 2019), we want to experiment with different contextualized embedding models. The main hindrance with these models is the massive computational resources needed to train, which may limit our possibilites. For alignment and filtering we experiment with Bleualign, Hunalign and vecalign for sentence alignment, Giza++ (Och and Ney, 2003) for word alignments, and Zipporah, BiCleaner and LASER (Artetxe and Schwenk, 2019b) for filtering, and possibly to help with anchoring the parallel texts for more effective alignment. Moses (Koehn et al., 2007) will be employed for phrase-based SMT and our NMT system uses the reference implementation of Vaswani et al. (2017) of the transformer-base architecture that is part of the Tensor2Tensor package (Vaswani et al., 2018). Evaluation 4 We are building three evaluation sets, for alignment, filtering, and MT, all sub-sampled and extracted from the ParIce corpus. The MT evaluation set will co"
2020.acl-srw.25,P07-2045,0,0.0116759,"gs with BERT has been shown to be effective for filtering (Lo and Simard, 2019), we want to experiment with different contextualized embedding models. The main hindrance with these models is the massive computational resources needed to train, which may limit our possibilites. For alignment and filtering we experiment with Bleualign, Hunalign and vecalign for sentence alignment, Giza++ (Och and Ney, 2003) for word alignments, and Zipporah, BiCleaner and LASER (Artetxe and Schwenk, 2019b) for filtering, and possibly to help with anchoring the parallel texts for more effective alignment. Moses (Koehn et al., 2007) will be employed for phrase-based SMT and our NMT system uses the reference implementation of Vaswani et al. (2017) of the transformer-base architecture that is part of the Tensor2Tensor package (Vaswani et al., 2018). Evaluation 4 We are building three evaluation sets, for alignment, filtering, and MT, all sub-sampled and extracted from the ParIce corpus. The MT evaluation set will contain 3000 manually aligned and error-free segments. The alignment evaluation set will have 2000 manually aligned sentences and the filtering set 2000 automatically aligned segments, each assigned one of four cl"
2020.acl-srw.25,W17-3204,0,0.011579,"exts in low-resource settings. We propose an effective unsupervised alignment method to tackle the alignment problem. Moreover, we propose a strategy to supplement state-of-theart models with automatically extracted information using basic NLP tools to effectively handle rich morphology. 1 Introduction Machine translation (MT) quality has improved substantially with the advent of neural machine translation systems (NMT). However, while the quality gains over statistical machine translation (SMT) systems can be large, in low-resource and domain mismatch settings they are significantly reduced (Koehn and Knowles, 2017). In recent years, unsupervised NMT trained only on monolingual corpora has attracted considerable attention, and has been proposed for scenarios where there is a lack of bilingual data (Artetxe et al., 2018b; Lample et al., 2018). These methods have been shown to perform well for related language pairs (e.g. Wu et al. (2019)), but as the languages differ more the unsupervised methods become less effective (Leng et al., 2019). Kim et al. (2020) show that supervised and semi-supervised baselines with only a small parallel corpus of 50K bilingual sentences Andy Way School of Computing ADAPT Cent"
2020.acl-srw.25,J82-2005,0,0.721556,"Missing"
2020.acl-srw.25,P19-1017,0,0.0211599,"hile the quality gains over statistical machine translation (SMT) systems can be large, in low-resource and domain mismatch settings they are significantly reduced (Koehn and Knowles, 2017). In recent years, unsupervised NMT trained only on monolingual corpora has attracted considerable attention, and has been proposed for scenarios where there is a lack of bilingual data (Artetxe et al., 2018b; Lample et al., 2018). These methods have been shown to perform well for related language pairs (e.g. Wu et al. (2019)), but as the languages differ more the unsupervised methods become less effective (Leng et al., 2019). Kim et al. (2020) show that supervised and semi-supervised baselines with only a small parallel corpus of 50K bilingual sentences Andy Way School of Computing ADAPT Centre Dublin City University Ireland andy.way @adaptcentre.ie consistently outperform the best unsupervised systems for a range of languages, similar and distant. They also show that unsupervised NMT is very sensitive to domain mismatch, which poses a problem to low-resource language pairs where it can be difficult to match the data domain on both sides. Thus, it is evident that to achieve high quality MT, sentence aligned-texts"
2020.acl-srw.25,K19-1020,0,0.046349,"been used to calculate distance between equivalences in different languages (Luong et al., 2015; Artetxe et al., 2016). Defauw et al. (2019) treat filtering as a supervised regression problem and show that Levenshtein distance (Levenshtein, 1966) between the target and MT-translated source, as well as cosine distance between sentence embeddings of the source and target, are important features. While they use InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019) has recently been employed for calculating crosslingual semantic textual similarity to detect misalignment with good results (Lo and Simard, 2019). Zipporah (Xu and Koehn, 2017) uses a logistic regression model trained to classify sentence pairs. Noisy data is synthesized and used as negative samples in training. BiCleaner (Sánchez-Cartagena et al., 2018) uses a set of handcrafted hard rules to detect flawed sentences and then proceeds to use a random forest classifier based on lexical translations and several shallow features such as respective length, matching numbers and punctuation. Finally, it scores sentences based on fluency using 5-gram language models. In 2019, at the fourth Conference on Machine Translation, WMT, the shared ta"
2020.acl-srw.25,W15-1521,0,0.0254669,"Missing"
2020.acl-srw.25,ma-2006-champollion,0,0.0536035,"ut use tokens instead of characters. In addition, they use signals in the markup as anchor points to segment the corpus into smaller chunks. Kay and Röscheisen (1993) used bilingual lexicons induced from the corpus being aligned. 183 Haruno and Yamazaki (1996) show that combining an induced lexicon with an external dictionary yields better results. Papageorgiou et al. (1994) use part-of-speech, commonly preserved in translation, by computing the optimum alignment based on the PoS-tags. Tschorn and Lüdeling (2003) use a morphological analyzer to improve a dictionary-based distance measure, and Ma (2006) increases the robustness of a lexicon-based aligner by assigning greater weights to less frequent translated words. Sennrich and Volk (2010) use machine translations and BLEU (Papineni et al., 2002) as a similarity score to find reliable alignments to use as anchor points. The gaps between the anchor points are filled using BLEU-based and length-based heuristics. Thompson and Koehn (2019) describe a method based on bilingual sentence embeddings, using the similarity between the embeddings as the scoring function for alignment. 2.2 Filtering Recently, neural networks have been used to find anc"
2020.acl-srw.25,P94-1051,0,0.667002,"n characters and the length of its translation was extremely high”. Motivated by that, they describe a method for aligning sentences based on a simple statistical model of character lengths. Brown et al. (1991) also describe a length-based method, but use tokens instead of characters. In addition, they use signals in the markup as anchor points to segment the corpus into smaller chunks. Kay and Röscheisen (1993) used bilingual lexicons induced from the corpus being aligned. 183 Haruno and Yamazaki (1996) show that combining an induced lexicon with an external dictionary yields better results. Papageorgiou et al. (1994) use part-of-speech, commonly preserved in translation, by computing the optimum alignment based on the PoS-tags. Tschorn and Lüdeling (2003) use a morphological analyzer to improve a dictionary-based distance measure, and Ma (2006) increases the robustness of a lexicon-based aligner by assigning greater weights to less frequent translated words. Sennrich and Volk (2010) use machine translations and BLEU (Papineni et al., 2002) as a similarity score to find reliable alignments to use as anchor points. The gaps between the anchor points are filled using BLEU-based and length-based heuristics. T"
2020.acl-srw.25,P02-1040,0,0.11446,"cons induced from the corpus being aligned. 183 Haruno and Yamazaki (1996) show that combining an induced lexicon with an external dictionary yields better results. Papageorgiou et al. (1994) use part-of-speech, commonly preserved in translation, by computing the optimum alignment based on the PoS-tags. Tschorn and Lüdeling (2003) use a morphological analyzer to improve a dictionary-based distance measure, and Ma (2006) increases the robustness of a lexicon-based aligner by assigning greater weights to less frequent translated words. Sennrich and Volk (2010) use machine translations and BLEU (Papineni et al., 2002) as a similarity score to find reliable alignments to use as anchor points. The gaps between the anchor points are filled using BLEU-based and length-based heuristics. Thompson and Koehn (2019) describe a method based on bilingual sentence embeddings, using the similarity between the embeddings as the scoring function for alignment. 2.2 Filtering Recently, neural networks have been used to find anchor points and detect misalignments. Many of these methods have been devised to extract parallel sentences from comparable corpora, by training classifiers to determine if source and target sentences"
2020.acl-srw.25,D14-1162,0,0.0875094,"lignment methods to reduce the number of faulty alignments, and we need a classifier that is able to identify the quality of the segments with high precision and recall in order to build as big a corpus as possible with as few faulty segments as possible. 3.2 an external morphological lexicon (Bjarnadóttir et al., 2019). Lemmatising will be carried out using Nefnir (Ingólfsdóttir et al., 2019). For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy.2 We will focus on the most common word embedding models: word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018). As using bilingual sentence embeddings with BERT has been shown to be effective for filtering (Lo and Simard, 2019), we want to experiment with different contextualized embedding models. The main hindrance with these models is the massive computational resources needed to train, which may limit our possibilites. For alignment and filtering we experiment with Bleualign, Hunalign and vecalign for sentence alignment, Giza++ (Och and Ney, 2003) for word alignments, and Zipporah, BiCleaner and LASER (Artetxe and Schwenk, 2019b) fo"
2020.acl-srw.25,N18-1202,0,0.00831629,"a classifier that is able to identify the quality of the segments with high precision and recall in order to build as big a corpus as possible with as few faulty segments as possible. 3.2 an external morphological lexicon (Bjarnadóttir et al., 2019). Lemmatising will be carried out using Nefnir (Ingólfsdóttir et al., 2019). For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy.2 We will focus on the most common word embedding models: word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018). As using bilingual sentence embeddings with BERT has been shown to be effective for filtering (Lo and Simard, 2019), we want to experiment with different contextualized embedding models. The main hindrance with these models is the massive computational resources needed to train, which may limit our possibilites. For alignment and filtering we experiment with Bleualign, Hunalign and vecalign for sentence alignment, Giza++ (Och and Ney, 2003) for word alignments, and Zipporah, BiCleaner and LASER (Artetxe and Schwenk, 2019b) for filtering, and possibly to help with anchoring the parallel texts"
2020.acl-srw.25,2010.amta-papers.14,0,0.0234046,"maller chunks. Kay and Röscheisen (1993) used bilingual lexicons induced from the corpus being aligned. 183 Haruno and Yamazaki (1996) show that combining an induced lexicon with an external dictionary yields better results. Papageorgiou et al. (1994) use part-of-speech, commonly preserved in translation, by computing the optimum alignment based on the PoS-tags. Tschorn and Lüdeling (2003) use a morphological analyzer to improve a dictionary-based distance measure, and Ma (2006) increases the robustness of a lexicon-based aligner by assigning greater weights to less frequent translated words. Sennrich and Volk (2010) use machine translations and BLEU (Papineni et al., 2002) as a similarity score to find reliable alignments to use as anchor points. The gaps between the anchor points are filled using BLEU-based and length-based heuristics. Thompson and Koehn (2019) describe a method based on bilingual sentence embeddings, using the similarity between the embeddings as the scoring function for alignment. 2.2 Filtering Recently, neural networks have been used to find anchor points and detect misalignments. Many of these methods have been devised to extract parallel sentences from comparable corpora, by traini"
2020.acl-srw.25,W11-4624,0,0.0606622,"Missing"
2020.acl-srw.25,R19-1133,1,0.747738,"d sentences and the filtering set 2000 automatically aligned segments, each assigned one of four classes: correct, partially misaligned, partially incorrect translation, incorrect. To evaluate the usefulness of our methods for MT, we will use our aligned and filtered corpora to train SMT and NMT systems and compare the results to a baseline where the raw ParIce corpus is used for training. 3.3 Tools and Models In Section 4, we will discuss some of the methods we will be experimenting with. These include applying a variety of available tools and models as well as developing our own. ABLTagger (Steingrímsson et al., 2019) will be used for PoS-tagging Icelandic texts. The tagger employs biLSTMs and 185 Research Plan Our first goal is to set up an unsupervised pipeline for aligning parallel texts. While this is the first step in tackling RQ2, it is also necessary to devise a method to answer RQ1. We will outline how we seek to answer these questions, as well as RQ3. A secondary goal is to investigate methods to improve upon the unsupervised pipeline by exploring how basic NLP tools can help us deal with the data sparsity problem inherent to many morphologically rich languages. In the following subsections we des"
2020.acl-srw.25,2011.mtsummit-papers.47,0,0.0561077,"tween the embeddings as the scoring function for alignment. 2.2 Filtering Recently, neural networks have been used to find anchor points and detect misalignments. Many of these methods have been devised to extract parallel sentences from comparable corpora, by training classifiers to determine if source and target sentences are parallel. Earlier work includes employing the IBM models (Brown et al., 1993) for word alignment. Khadivi and Ney (2005) filter out the noisy part of a corpus based on IBM models 1 and 4 and lengthbased models, and score the alignments on a linear combination of these. Taghipour et al. (2011) do outlier detection and show that their filtered corpus results in improved translation quality, even though sentences have been removed. Sarikaya et al. (2009) use context extrapolation to boost the sentence pair coverage, checking whether the distance of the sentences from an anchor point is the same, and whether the sentences have the highest similarity score compared to other pairs within a window, despite being below a defined threshold. Crosslingual word embeddings have been used to calculate distance between equivalences in different languages (Luong et al., 2015; Artetxe et al., 2016"
2020.acl-srw.25,D19-1136,0,0.0117002,") use part-of-speech, commonly preserved in translation, by computing the optimum alignment based on the PoS-tags. Tschorn and Lüdeling (2003) use a morphological analyzer to improve a dictionary-based distance measure, and Ma (2006) increases the robustness of a lexicon-based aligner by assigning greater weights to less frequent translated words. Sennrich and Volk (2010) use machine translations and BLEU (Papineni et al., 2002) as a similarity score to find reliable alignments to use as anchor points. The gaps between the anchor points are filled using BLEU-based and length-based heuristics. Thompson and Koehn (2019) describe a method based on bilingual sentence embeddings, using the similarity between the embeddings as the scoring function for alignment. 2.2 Filtering Recently, neural networks have been used to find anchor points and detect misalignments. Many of these methods have been devised to extract parallel sentences from comparable corpora, by training classifiers to determine if source and target sentences are parallel. Earlier work includes employing the IBM models (Brown et al., 1993) for word alignment. Khadivi and Ney (2005) filter out the noisy part of a corpus based on IBM models 1 and 4 a"
2020.acl-srw.25,W18-1819,0,0.0195808,"resources needed to train, which may limit our possibilites. For alignment and filtering we experiment with Bleualign, Hunalign and vecalign for sentence alignment, Giza++ (Och and Ney, 2003) for word alignments, and Zipporah, BiCleaner and LASER (Artetxe and Schwenk, 2019b) for filtering, and possibly to help with anchoring the parallel texts for more effective alignment. Moses (Koehn et al., 2007) will be employed for phrase-based SMT and our NMT system uses the reference implementation of Vaswani et al. (2017) of the transformer-base architecture that is part of the Tensor2Tensor package (Vaswani et al., 2018). Evaluation 4 We are building three evaluation sets, for alignment, filtering, and MT, all sub-sampled and extracted from the ParIce corpus. The MT evaluation set will contain 3000 manually aligned and error-free segments. The alignment evaluation set will have 2000 manually aligned sentences and the filtering set 2000 automatically aligned segments, each assigned one of four classes: correct, partially misaligned, partially incorrect translation, incorrect. To evaluate the usefulness of our methods for MT, we will use our aligned and filtered corpora to train SMT and NMT systems and compare"
2020.acl-srw.25,N19-1120,0,0.0136752,"y has improved substantially with the advent of neural machine translation systems (NMT). However, while the quality gains over statistical machine translation (SMT) systems can be large, in low-resource and domain mismatch settings they are significantly reduced (Koehn and Knowles, 2017). In recent years, unsupervised NMT trained only on monolingual corpora has attracted considerable attention, and has been proposed for scenarios where there is a lack of bilingual data (Artetxe et al., 2018b; Lample et al., 2018). These methods have been shown to perform well for related language pairs (e.g. Wu et al. (2019)), but as the languages differ more the unsupervised methods become less effective (Leng et al., 2019). Kim et al. (2020) show that supervised and semi-supervised baselines with only a small parallel corpus of 50K bilingual sentences Andy Way School of Computing ADAPT Centre Dublin City University Ireland andy.way @adaptcentre.ie consistently outperform the best unsupervised systems for a range of languages, similar and distant. They also show that unsupervised NMT is very sensitive to domain mismatch, which poses a problem to low-resource language pairs where it can be difficult to match the"
2020.acl-srw.25,D17-1319,0,0.0224873,"between equivalences in different languages (Luong et al., 2015; Artetxe et al., 2016). Defauw et al. (2019) treat filtering as a supervised regression problem and show that Levenshtein distance (Levenshtein, 1966) between the target and MT-translated source, as well as cosine distance between sentence embeddings of the source and target, are important features. While they use InferSent (Conneau et al., 2017), BERT (Devlin et al., 2019) has recently been employed for calculating crosslingual semantic textual similarity to detect misalignment with good results (Lo and Simard, 2019). Zipporah (Xu and Koehn, 2017) uses a logistic regression model trained to classify sentence pairs. Noisy data is synthesized and used as negative samples in training. BiCleaner (Sánchez-Cartagena et al., 2018) uses a set of handcrafted hard rules to detect flawed sentences and then proceeds to use a random forest classifier based on lexical translations and several shallow features such as respective length, matching numbers and punctuation. Finally, it scores sentences based on fluency using 5-gram language models. In 2019, at the fourth Conference on Machine Translation, WMT, the shared task on parallel corpora filterin"
2020.amta-research.4,D17-1098,0,0.0266776,"Missing"
2020.amta-research.4,W17-4716,0,0.0166403,"d the hypothesis at the current time step t. The source-side alignment information helps place constraints in more reasonable positions. In this way, a number of hypotheses in GBS are pruned, and the search space decreases significantly. 4 4.1 Experiments Translation Tasks In our view, the most interesting finding in Hokamp and Liu (2017) is that GBS-based constrained decoding has a significant role to play in domain adaptation via terminology, which is a very important issue in application scenarios in which the translation process has to comply with specific terminology and/or style guides (Chatterjee et al., 2017). Therefore, in order to compare the proposed HGBS method with GBS, we focus in our experiments on the domain adaptation task for constrained decoding via terminology. We use WMT English–German (EN-DE) and Chinese–English (ZH-EN) translation tasks to perform the comparison experiments. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 41 4.2 Data We use the same data settings for the domain adaptation experiment as in Hokamp and Liu (2017) in terms of the EN-DE task: • the training corpus consists"
2020.amta-research.4,2016.amta-researchers.10,0,0.0169616,"ty of word alignment is better, the translation quality is better. From this observation and analysis, regarding the proposed HGBS method, the hypothesis will be that if we can improve the quality of word alignment of multi-head attention mechanism, we would further improve translation quality and better guide the placement of constraints during the decoding to further improve translation quality. 5 Refining the model with alignmental guiding training In order to verify the effect of alignment (or attention) on our HGBS method, we refined the Transformer model using Guided Alignment Training (Chen et al., 2016). Currently the general Transformer model normally uses 6 layers and 8 heads in each layer. We average all the attention of the 6 layers and 8 heads as a whole attention value, as the Ati in Equation (1). Similar to Chen et al. (2016), we combine decoder cost and alignment cost to build the new loss function Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 46 H(y, x, A, α) in Equation (2): H(y, x, A, α) = HD (y, x) + ωEmse (A, α) (2) Here HD (y, x) is the normal decoder cost of the Transformer mo"
2020.amta-research.4,W14-3348,0,0.072413,"Missing"
2020.amta-research.4,N13-1073,0,0.0416185,"ded by the multi-head attention information. Therefore, we infer that the quality of word alignment between the target and source is closely correlated with translation quality, i.e. a better quality word alignment will produce a higher quality of translations. In this section, we look into this issue by evaluating the word alignment of multi-head attention mechanism and measuring their correlations. Figure 4: Correlations of the alignment error and BLEU score To evaluate the quality of the word alignment from the multi-head attention, we use the word alignment links generated from FastAlign (Dyer et al., 2013) as the “Ground Truth”. Since the word alignment from the multi-head attention is a probability distribution of the time step t in the decoder against all source words, we use Mean Square Error (MSE) as the metric to evaluate the alignment quality as in Equation (1): Emse (A, α) = It T 1 XX (Ati − αti )2 T t=1 i=1 Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track (1) Page 45 where A is the alignment from FastAlign, α is the alignment from the multi-head attention. T is the total time steps for the targ"
2020.amta-research.4,P17-1012,0,0.0410195,"Missing"
2020.amta-research.4,P17-1141,0,0.0635162,"tried early attempts to improve translation consistency for NMT models with discourse-level context. However, these methods do not strictly enforce a constraint, so constraints are not guaranteed to appear in the output. Anderson et al. (2017) extended beam search with a finite state acceptor (FSA) whose states mark the completed subsets of the set of constraints. However, their algorithm has an exponential complexity of O(N k2C ), where n is the sentence length, k is beam size, and C is the constraint count. This results in a very slow decoding speed when the number of constraints increased. Hokamp and Liu (2017) proposed a novel grid beam search (GBS) method that can enforce any constraints to appear in the translation results. In order to ensure the constraints are placed in the right positions in the translation results, GBS assumes that all constraints may appear at each decoding step and extends a beam vertically to grid beams. This means there are several beams at each step rather than a single beam. As a result, the number of hypotheses increases linearly according to the number of constraints. GBS can adapt a general NMT model to a domain translation task and improve the translation quality (H"
2020.amta-research.4,2005.mtsummit-papers.11,0,0.144457,"posed HGBS method with GBS, we focus in our experiments on the domain adaptation task for constrained decoding via terminology. We use WMT English–German (EN-DE) and Chinese–English (ZH-EN) translation tasks to perform the comparison experiments. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 41 4.2 Data We use the same data settings for the domain adaptation experiment as in Hokamp and Liu (2017) in terms of the EN-DE task: • the training corpus consists of 4.4 Million segments from Europarl (Koehn, 2005) and CommonCrawl (Smith et al., 2013); • for the target domain data, the Autodesk Post-Editing corpus (Zhechev, 2012) from the domain of software localisation is used, which is quite different from the WMT data. The corpus is divided into 100,000 training sentences and 1,000 test sentences. Constraints are extracted automatically using PMI between source and target n-grams. The maximum length of a constraint or terminology is set to 5-gram as in Hokamp and Liu (2017). For the ZH-EN translation task, in terms of the training data and testing data, • we use LDC corpora to train the general domai"
2020.amta-research.4,P07-2045,0,0.00951543,"aining data and testing data, • we use LDC corpora to train the general domain Transformer, which consists of 1.25 Million segments;∗ Most sentences in this corpus come from the News domain. • for the target domain data, we also use the Autodesk Post-Editing corpus. 159,816 sentences are extracted as the training set for PMI and constraint extraction. An additional 1,000 sentences are extracted as the test set for our constrained Transformer experiment. The maximum length for PMI constraint extraction is set to 5-grams. All English and German sentences are preprocessed using tools from Moses (Koehn et al., 2007). Chinese sentences are segmented into words using Jieba,† a popular Python toolkit for Chinese word segmentation. Finally, the parallel pre-processed data are segmented to subwords by applying Byte Pair Encoding (Sennrich et al., 2016b), which is capable of encoding open vocabularies with a compact symbol vocabulary of variable-length subword units. 4.3 Systems We use the Transformer model in the open source toolkit THUMT as our baseline system (Zhang et al., 2017).‡ For the constrained Transformer model, we first reimplement the GBS method under Transformer model, which we call GBS-T, and we"
2020.amta-research.4,2015.iwslt-evaluation.11,0,0.0437735,"Missing"
2020.amta-research.4,D15-1166,0,0.147913,"Missing"
2020.amta-research.4,P02-1040,0,0.10598,"Missing"
2020.amta-research.4,P16-1009,0,0.0193602,"the Autodesk Post-Editing corpus. 159,816 sentences are extracted as the training set for PMI and constraint extraction. An additional 1,000 sentences are extracted as the test set for our constrained Transformer experiment. The maximum length for PMI constraint extraction is set to 5-grams. All English and German sentences are preprocessed using tools from Moses (Koehn et al., 2007). Chinese sentences are segmented into words using Jieba,† a popular Python toolkit for Chinese word segmentation. Finally, the parallel pre-processed data are segmented to subwords by applying Byte Pair Encoding (Sennrich et al., 2016b), which is capable of encoding open vocabularies with a compact symbol vocabulary of variable-length subword units. 4.3 Systems We use the Transformer model in the open source toolkit THUMT as our baseline system (Zhang et al., 2017).‡ For the constrained Transformer model, we first reimplement the GBS method under Transformer model, which we call GBS-T, and we then apply our HGBS algorithm to GBS-T to improve its decoding speed, which we call HGBS-T. The evaluation metrics are case-insensitive BLEU and METEOR. In all our experiments, we employ the base Transformer configuration with embeddi"
2020.amta-research.4,P16-1162,0,0.0635048,"the Autodesk Post-Editing corpus. 159,816 sentences are extracted as the training set for PMI and constraint extraction. An additional 1,000 sentences are extracted as the test set for our constrained Transformer experiment. The maximum length for PMI constraint extraction is set to 5-grams. All English and German sentences are preprocessed using tools from Moses (Koehn et al., 2007). Chinese sentences are segmented into words using Jieba,† a popular Python toolkit for Chinese word segmentation. Finally, the parallel pre-processed data are segmented to subwords by applying Byte Pair Encoding (Sennrich et al., 2016b), which is capable of encoding open vocabularies with a compact symbol vocabulary of variable-length subword units. 4.3 Systems We use the Transformer model in the open source toolkit THUMT as our baseline system (Zhang et al., 2017).‡ For the constrained Transformer model, we first reimplement the GBS method under Transformer model, which we call GBS-T, and we then apply our HGBS algorithm to GBS-T to improve its decoding speed, which we call HGBS-T. The evaluation metrics are case-insensitive BLEU and METEOR. In all our experiments, we employ the base Transformer configuration with embeddi"
2020.amta-research.4,P13-1135,0,0.0270515,"focus in our experiments on the domain adaptation task for constrained decoding via terminology. We use WMT English–German (EN-DE) and Chinese–English (ZH-EN) translation tasks to perform the comparison experiments. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 41 4.2 Data We use the same data settings for the domain adaptation experiment as in Hokamp and Liu (2017) in terms of the EN-DE task: • the training corpus consists of 4.4 Million segments from Europarl (Koehn, 2005) and CommonCrawl (Smith et al., 2013); • for the target domain data, the Autodesk Post-Editing corpus (Zhechev, 2012) from the domain of software localisation is used, which is quite different from the WMT data. The corpus is divided into 100,000 training sentences and 1,000 test sentences. Constraints are extracted automatically using PMI between source and target n-grams. The maximum length of a constraint or terminology is set to 5-gram as in Hokamp and Liu (2017). For the ZH-EN translation task, in terms of the training data and testing data, • we use LDC corpora to train the general domain Transformer, which consists of 1.25"
2020.amta-research.4,D17-1301,1,0.902723,"Missing"
2020.amta-research.4,P17-4012,0,0.0592647,"length for PMI constraint extraction is set to 5-grams. All English and German sentences are preprocessed using tools from Moses (Koehn et al., 2007). Chinese sentences are segmented into words using Jieba,† a popular Python toolkit for Chinese word segmentation. Finally, the parallel pre-processed data are segmented to subwords by applying Byte Pair Encoding (Sennrich et al., 2016b), which is capable of encoding open vocabularies with a compact symbol vocabulary of variable-length subword units. 4.3 Systems We use the Transformer model in the open source toolkit THUMT as our baseline system (Zhang et al., 2017).‡ For the constrained Transformer model, we first reimplement the GBS method under Transformer model, which we call GBS-T, and we then apply our HGBS algorithm to GBS-T to improve its decoding speed, which we call HGBS-T. The evaluation metrics are case-insensitive BLEU and METEOR. In all our experiments, we employ the base Transformer configuration with embedding size and hidden size both 512, 6 encoder and decoder layers, 8 attention heads, the standard ReLu activation function and sinusoidal positional embedding, maximum sentence length 80, batch size 4096 tokens, beam size 10. The vocabul"
2020.amta-research.4,2012.amta-wptp.10,0,0.0312008,"terminology. We use WMT English–German (EN-DE) and Chinese–English (ZH-EN) translation tasks to perform the comparison experiments. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 41 4.2 Data We use the same data settings for the domain adaptation experiment as in Hokamp and Liu (2017) in terms of the EN-DE task: • the training corpus consists of 4.4 Million segments from Europarl (Koehn, 2005) and CommonCrawl (Smith et al., 2013); • for the target domain data, the Autodesk Post-Editing corpus (Zhechev, 2012) from the domain of software localisation is used, which is quite different from the WMT data. The corpus is divided into 100,000 training sentences and 1,000 test sentences. Constraints are extracted automatically using PMI between source and target n-grams. The maximum length of a constraint or terminology is set to 5-gram as in Hokamp and Liu (2017). For the ZH-EN translation task, in terms of the training data and testing data, • we use LDC corpora to train the general domain Transformer, which consists of 1.25 Million segments;∗ Most sentences in this corpus come from the News domain. • f"
2020.amta-research.7,W12-3709,0,0.475042,"specific language into English can eliminate the necessity of developing specific sentiment analysis resources for that language (Shalunts et al., 2016). One of the most recent approaches using MT for sentiment classification is described in Tebbifakhr et al. (2019). Their proposed approach for the sentiment classification of Twitter data in German and Italian shows that feeding an English classifier with machine-oriented translations improves its performance. For low-resource languages, MT-based approaches are considered efficient for analysing the sentiment of texts (Kanayama et al., 2004; Balahur and Turchi, 2012). Additionally, several approaches aim to influence the MT to favour a sentiment when generating a translation. Lohar et al. (2017) propose training different SMT systems on sentences that have been tagged with a particular sentiment. Similarly, Si et al. (2019) propose methods for generating translations of both positive and negative sentiments from the same sentence in the source language. In our work, we not only investigate the sentiment classification on direct translation but also on indirect translation. Despite several existing studies on MT translation using a pivot language, both in"
2020.amta-research.7,W14-4012,0,0.0360392,"Missing"
2020.amta-research.7,C04-1071,0,0.714857,"translate a text from a specific language into English can eliminate the necessity of developing specific sentiment analysis resources for that language (Shalunts et al., 2016). One of the most recent approaches using MT for sentiment classification is described in Tebbifakhr et al. (2019). Their proposed approach for the sentiment classification of Twitter data in German and Italian shows that feeding an English classifier with machine-oriented translations improves its performance. For low-resource languages, MT-based approaches are considered efficient for analysing the sentiment of texts (Kanayama et al., 2004; Balahur and Turchi, 2012). Additionally, several approaches aim to influence the MT to favour a sentiment when generating a translation. Lohar et al. (2017) propose training different SMT systems on sentences that have been tagged with a particular sentiment. Similarly, Si et al. (2019) propose methods for generating translations of both positive and negative sentiments from the same sentence in the source language. In our work, we not only investigate the sentiment classification on direct translation but also on indirect translation. Despite several existing studies on MT translation using"
2020.amta-research.7,P17-4012,0,0.0140435,"anguage. For instance, Irish is often translated via English, Basque and Catalan via Spanish, and Breton via French. The translation quality of a document that is indirectly translated is expected to be lower than a direct translation because the final translation accumulates the errors produced by two MT models. This may also have a negative impact on the classifier. We want to analyze the performance of the classifier when classifying indirectly-translated sentences. 4 Experiments 4.1 MT settings We build an NMT system following the transformer approach (Vaswani et al., 2017) using OpenNMT (Klein et al., 2017). The model is trained for a maximum of 400K steps using the recommended parameters,1 selecting the model that obtains the lowest perplexity on the development set. A total of six translation models are built for translating French, Spanish and Japanese from/into English (two models for each pair).We use Paracrawl2 for English-French (51M parallel sentences) and English-Spanish (39M parallel sentences) language pairs, and JParaCrawl (Morishita et al., 2019) dataset (8.7M parallel sentences) for English-Japanese. All the datsets are tokenized, truecased and then Byte Pair Encoding (BPE) (Sennri"
2020.amta-research.7,C10-1072,0,0.0399065,"ised in the following manner. We discuss the related work done in this field in Section 2. In Section 3, we formulate some research questions to be addressed in this area. The experiments are detailed in Section 4. We highlight our results in Section 5. Finally, we conclude our present work in Section 6, followed by some possible future directions in Section 7. 2 Related Work Several studies have addressed the issue of sentiment classification. The work in Pang et al. (2002) examines the effectiveness of applying machine learning techniques to the sentiment classification of movie reviews. In Li et al. (2010) polarity shifting information is incorporated into a document-level sentiment classification system. First, polarity shifting is detected and then classifier combination methods are applied to perform polarity classification. However, in recent studies, deep learning-based approaches are gaining popularity for sentiment classification (Zhang et al., 2019a,b). MT plays a significant role in crosslingual sentiment analysis. An approach that is similar to ours is the work of Araujo et al. (2016). Their experiments show that the performance of the English sentiment analysis tools on texts transla"
2020.amta-research.7,I17-4004,1,0.795889,"www.paracrawl.eu Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 80 Language En → Fr En → Es En → Ja Fr → En Es → En Ja → En BLEU 31.77 40.13 9.21 32.09 40.48 12.85 Table 1: MT performance Spanish are grammatically and lexically closer to English than Japanese, and in addition, the number of training sentences used to build the MT system is four times smaller. 4.2 Sentiment classifier In order to build the sentiment analyser, we use the data from the IJCNLP-2017 Customer Feedback Analysis Task (Liu et al., 2017b). This dataset consists of a collection of English, French, Spanish and Japanese feedback containing short sentences extracted from reviews of products or services (in the hotel, restaurant or software domain). Each sentence is tagged with one or more categories based on the five-class system proposed by Liu et al. (2017a): comment, complaint, request, bug and meaningless. This is a fine-grained classification where only positive feedback is classified as comments whereas the other classes can be assumed to be different variants of negative feedback: “complaint” is defined as a negative comm"
2020.amta-research.7,W19-6101,0,0.0186982,"in terms of sentiment preservation (rather than other common criteria such as adequacy and fluency). We performed translation of customer feedback and categorized it as positive or negative using an automatic classifier. There are several conclusions that we can draw from the experiments carried out. Sentiment classifiers do not classify translated data as well as original sentences. As expected, the outcome of our experiments shows that it is preferable to use the original feedback rather than a translation for classification. The MT-generated feedback introduces errors (Lohar et al., 2019; Nunez et al., 2019) that causes the classifier to show worse performance. Translation quality is not completely correlated to the performance of the classifier. Although the automatic sentiment classifier does not perform well on sentences with low-quality translation, after a certain translation-quality threshold the performance of the classifier is not correlated with the translation quality. There are potential benefits to using MT-translated sentences Although MT models produce errors, they also tend to generate sentences with a lower amount of lexical translation, which facilitates the classification. Moreo"
2020.amta-research.7,W02-1011,0,0.0295727,"cuss the benefits and disadvantages of using machinetranslated sentences for automatic classification. The remainder of this paper is organised in the following manner. We discuss the related work done in this field in Section 2. In Section 3, we formulate some research questions to be addressed in this area. The experiments are detailed in Section 4. We highlight our results in Section 5. Finally, we conclude our present work in Section 6, followed by some possible future directions in Section 7. 2 Related Work Several studies have addressed the issue of sentiment classification. The work in Pang et al. (2002) examines the effectiveness of applying machine learning techniques to the sentiment classification of movie reviews. In Li et al. (2010) polarity shifting information is incorporated into a document-level sentiment classification system. First, polarity shifting is detected and then classifier combination methods are applied to perform polarity classification. However, in recent studies, deep learning-based approaches are gaining popularity for sentiment classification (Zhang et al., 2019a,b). MT plays a significant role in crosslingual sentiment analysis. An approach that is similar to ours"
2020.amta-research.7,P02-1040,0,0.108221,"ls for each pair).We use Paracrawl2 for English-French (51M parallel sentences) and English-Spanish (39M parallel sentences) language pairs, and JParaCrawl (Morishita et al., 2019) dataset (8.7M parallel sentences) for English-Japanese. All the datsets are tokenized, truecased and then Byte Pair Encoding (BPE) (Sennrich et al., 2016c) is applied with 89, 500 merge operations. In order to estimate the performance of the MT models, in Table 1 we present the translation quality when translating a sample of 500 lines from the news-commentary dataset. The translations are evaluated using the BLEU (Papineni et al., 2002) metric. For English-French and English-Spanish language pairs the models achieve decent translation quality, but for the English-Japanese pair, BLEU scores are much lower. The reasons for this is that French and 1 https://opennmt.net/OpenNMT-py/FAQ.html 2 https://www.paracrawl.eu Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 80 Language En → Fr En → Es En → Ja Fr → En Es → En Ja → En BLEU 31.77 40.13 9.21 32.09 40.48 12.85 Table 1: MT performance Spanish are grammatically and lexically closer"
2020.amta-research.7,I17-4024,0,0.0118383,"es such as back-translation (Sennrich et al., 2016b; Poncelas et al., 2018b), in which synthetic data is created by translating sentences from another language, has proven to be useful for improving MT models. We want to explore whether using machine-generated sentences as training data for the classifier also has an impact on the performance. In the future, we want to explore other experimental configurations. For example, in this paper we explored a classifier trained with a single language. We want to investigate whether the performance would be similar when using multilingual classifiers (Plank, 2017). Moreover, in these experiments the MT models were trained on large amounts of data (9M to 51M sentences). Although smaller models are expected to produce lower quality translations, these may be enough for the sentiment classifier to achieve acceptable results. A future extension to this work would involve investigating what is the minimum amount of data necessary for building the MT system to create translations that are good enough for the classifier to perform well. Alternatively, small MT models can be built by selecting a subset of the available data (Silva et al., 2018) that is closer"
2020.amta-research.7,N16-1005,0,0.328991,"2017). The model is trained for a maximum of 400K steps using the recommended parameters,1 selecting the model that obtains the lowest perplexity on the development set. A total of six translation models are built for translating French, Spanish and Japanese from/into English (two models for each pair).We use Paracrawl2 for English-French (51M parallel sentences) and English-Spanish (39M parallel sentences) language pairs, and JParaCrawl (Morishita et al., 2019) dataset (8.7M parallel sentences) for English-Japanese. All the datsets are tokenized, truecased and then Byte Pair Encoding (BPE) (Sennrich et al., 2016c) is applied with 89, 500 merge operations. In order to estimate the performance of the MT models, in Table 1 we present the translation quality when translating a sample of 500 lines from the news-commentary dataset. The translations are evaluated using the BLEU (Papineni et al., 2002) metric. For English-French and English-Spanish language pairs the models achieve decent translation quality, but for the English-Japanese pair, BLEU scores are much lower. The reasons for this is that French and 1 https://opennmt.net/OpenNMT-py/FAQ.html 2 https://www.paracrawl.eu Proceedings of the 14th Confer"
2020.amta-research.7,P16-1009,0,0.115334,"2017). The model is trained for a maximum of 400K steps using the recommended parameters,1 selecting the model that obtains the lowest perplexity on the development set. A total of six translation models are built for translating French, Spanish and Japanese from/into English (two models for each pair).We use Paracrawl2 for English-French (51M parallel sentences) and English-Spanish (39M parallel sentences) language pairs, and JParaCrawl (Morishita et al., 2019) dataset (8.7M parallel sentences) for English-Japanese. All the datsets are tokenized, truecased and then Byte Pair Encoding (BPE) (Sennrich et al., 2016c) is applied with 89, 500 merge operations. In order to estimate the performance of the MT models, in Table 1 we present the translation quality when translating a sample of 500 lines from the news-commentary dataset. The translations are evaluated using the BLEU (Papineni et al., 2002) metric. For English-French and English-Spanish language pairs the models achieve decent translation quality, but for the English-Japanese pair, BLEU scores are much lower. The reasons for this is that French and 1 https://opennmt.net/OpenNMT-py/FAQ.html 2 https://www.paracrawl.eu Proceedings of the 14th Confer"
2020.amta-research.7,P16-1162,0,0.158136,"2017). The model is trained for a maximum of 400K steps using the recommended parameters,1 selecting the model that obtains the lowest perplexity on the development set. A total of six translation models are built for translating French, Spanish and Japanese from/into English (two models for each pair).We use Paracrawl2 for English-French (51M parallel sentences) and English-Spanish (39M parallel sentences) language pairs, and JParaCrawl (Morishita et al., 2019) dataset (8.7M parallel sentences) for English-Japanese. All the datsets are tokenized, truecased and then Byte Pair Encoding (BPE) (Sennrich et al., 2016c) is applied with 89, 500 merge operations. In order to estimate the performance of the MT models, in Table 1 we present the translation quality when translating a sample of 500 lines from the news-commentary dataset. The translations are evaluated using the BLEU (Papineni et al., 2002) metric. For English-French and English-Spanish language pairs the models achieve decent translation quality, but for the English-Japanese pair, BLEU scores are much lower. The reasons for this is that French and 1 https://opennmt.net/OpenNMT-py/FAQ.html 2 https://www.paracrawl.eu Proceedings of the 14th Confer"
2020.amta-research.7,W18-6323,1,0.83257,"tilingual classifiers (Plank, 2017). Moreover, in these experiments the MT models were trained on large amounts of data (9M to 51M sentences). Although smaller models are expected to produce lower quality translations, these may be enough for the sentiment classifier to achieve acceptable results. A future extension to this work would involve investigating what is the minimum amount of data necessary for building the MT system to create translations that are good enough for the classifier to perform well. Alternatively, small MT models can be built by selecting a subset of the available data (Silva et al., 2018) that is closer to the user-generated content. Another configuration would involve adapting the MT models to different categories. Following the approach of (Lohar et al., 2017) we could build different MT models for differProceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 85 ent classes. Alternatively, models could be adapted to translate feedback of a particular sentiment in a similar way to domain-adaptation. This can be done by fine-tuning with in-domain sets (van der Wees et al., 2017; Poncela"
2020.amta-research.7,D19-1140,0,0.216331,"s more difficult (Lohar et al., 2019). Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 78 MT models are built to generate translations that carry the same meaning as the original sentence and are also fluent in the target language. However, in some scenarios, metrics for measuring fluency are less relevant. This is the case in the sentiment analysis of translated sentences, where maintaining the sentiment is the priority, even if the translation is not accurate in terms of adequacy and fluency (Tebbifakhr et al., 2019). Despite the MT system generating understandable translations, it may not manifest the same sentiment as the original sentence. On top of that, in some cases, it is not possible to perform a direct translation, and so translation is required to be done via a pivot language. This may influence the classifier even more, as the errors produced by MT are propagated. In this work, we analyze the difference in the performance of a sentiment classifier when using sentences in the original language, and sentences that have been translated (directly and indirectly) using an MT system. We discuss the b"
2020.amta-research.7,W19-6627,0,0.0267827,"nslations. In the second subtable, we show why in some cases using a translation could be beneficial. The original sentence in French et le quartier pas tr`es sympa. is not a grammatically-correct sentence as the word ne (the negation of a verb in French follows the structure “ne”+VB+“pas”) and the verb est (to be) are omitted which is common in spoken French. The translation into English is accurate in meaning and when the sentence has been translated back to French, the structure of the sentence is correct. Another advantage is that MT-generated texts tend to have a lower lexical diversity (Toral, 2019; Vanmassenhove et al., 2019) which makes the classification easier. This can be seen with the French word sympa which is not as frequent as agr´eable. For example, there are 6, 065 occurrences of the word sympa in the Paracrawl dataset Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 84 whereas agr´eable occurs 78, 689 times. 6 Conclusions In this work we investigated the impact of both direct and indirect translation when evaluated in terms of sentiment preservation (rather than other common cr"
2020.amta-research.7,N07-1061,0,0.246395,"itionally, several approaches aim to influence the MT to favour a sentiment when generating a translation. Lohar et al. (2017) propose training different SMT systems on sentences that have been tagged with a particular sentiment. Similarly, Si et al. (2019) propose methods for generating translations of both positive and negative sentiments from the same sentence in the source language. In our work, we not only investigate the sentiment classification on direct translation but also on indirect translation. Despite several existing studies on MT translation using a pivot language, both in SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007) and NMT (Cheng Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 79 et al., 2017; Liu et al., 2018), to the best of our knowledge, this is the first study where indirect translation is explored for automatic sentiment classification. 3 Research Questions In our experiments, we aim to explore the change in performance of a sentiment classifier when executed on MT-translated sentences. Furthermore, we want to compare the performance when using direct and indirect translation. Th"
2020.amta-research.7,D17-1147,0,0.0256831,"Missing"
2020.amta-research.7,W19-6622,1,0.844326,"the second subtable, we show why in some cases using a translation could be beneficial. The original sentence in French et le quartier pas tr`es sympa. is not a grammatically-correct sentence as the word ne (the negation of a verb in French follows the structure “ne”+VB+“pas”) and the verb est (to be) are omitted which is common in spoken French. The translation into English is accurate in meaning and when the sentence has been translated back to French, the structure of the sentence is correct. Another advantage is that MT-generated texts tend to have a lower lexical diversity (Toral, 2019; Vanmassenhove et al., 2019) which makes the classification easier. This can be seen with the French word sympa which is not as frequent as agr´eable. For example, there are 6, 065 occurrences of the word sympa in the Paracrawl dataset Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 84 whereas agr´eable occurs 78, 689 times. 6 Conclusions In this work we investigated the impact of both direct and indirect translation when evaluated in terms of sentiment preservation (rather than other common criteria such as adequacy and f"
2020.amta-research.7,P07-1108,0,0.0527664,"es aim to influence the MT to favour a sentiment when generating a translation. Lohar et al. (2017) propose training different SMT systems on sentences that have been tagged with a particular sentiment. Similarly, Si et al. (2019) propose methods for generating translations of both positive and negative sentiments from the same sentence in the source language. In our work, we not only investigate the sentiment classification on direct translation but also on indirect translation. Despite several existing studies on MT translation using a pivot language, both in SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007) and NMT (Cheng Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 79 et al., 2017; Liu et al., 2018), to the best of our knowledge, this is the first study where indirect translation is explored for automatic sentiment classification. 3 Research Questions In our experiments, we aim to explore the change in performance of a sentiment classifier when executed on MT-translated sentences. Furthermore, we want to compare the performance when using direct and indirect translation. The research questions"
2020.amta-research.7,D19-1464,0,0.0145478,"ction 7. 2 Related Work Several studies have addressed the issue of sentiment classification. The work in Pang et al. (2002) examines the effectiveness of applying machine learning techniques to the sentiment classification of movie reviews. In Li et al. (2010) polarity shifting information is incorporated into a document-level sentiment classification system. First, polarity shifting is detected and then classifier combination methods are applied to perform polarity classification. However, in recent studies, deep learning-based approaches are gaining popularity for sentiment classification (Zhang et al., 2019a,b). MT plays a significant role in crosslingual sentiment analysis. An approach that is similar to ours is the work of Araujo et al. (2016). Their experiments show that the performance of the English sentiment analysis tools on texts translated into English can be as good as using language-specific tools. Therefore, it may be worth deploying a system following the first approach, assuming some cost on the prediction performance. The work of Barhoumi et al. (2018) shows that the sentiment analysis of Arabic texts translated into English reaches a competitive performance with respect to standa"
2020.eamt-1.21,P19-1122,0,0.0617446,"Missing"
2020.eamt-1.21,J99-2004,0,0.450548,"oehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore and Joshi, 1999; Steedman, 2000) as a conditional context in an interactivepredictive protocol on Transformer (Vaswani et al., 2017), the current state-of-the-art NMT model. In a reference-simulated setting, we found that our target-language syntax-informed interactive setup can significantly reduce human effort in a Frenchto-English translation task. We also extract syntactic features from constituency-based parse trees of the source French sentences following Akoury et al. (2019), and use them as the conditional context in the interactive-predictive Transformer framework. Experiments show that this context"
2020.eamt-1.21,J09-1002,0,0.0879581,"Missing"
2020.eamt-1.21,1997.mtsummit-papers.1,0,0.723576,"nd model supertags and constituency parse tree-based features collectively as the conditional context for interactive prediction in NMT. Our experimental results indicate that these syntactic feature types are complementary. As a result, this collaborative strategy turns out to be the bestperforming in the French-to-English task while significantly outperforming those setups that include either feature type on WPA and WSR. To the best of our knowledge, this is the very first study that investigates the possibility of integrating syntactic knowledge into an interactive MT model. 2 Related Work Foster et al. (1997) were the first to introduce the idea of interactive-predictive MT as an alternative to pure post-editing MT. There have been a number of papers that explored this strategy in order to minimise human effort in translation and cover many use-cases involving SMT: e.g. applying online (Ortiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignme"
2020.eamt-1.21,E12-1025,0,0.0473397,"Missing"
2020.eamt-1.21,P07-1037,1,0.783792,"Missing"
2020.eamt-1.21,W18-1820,0,0.0624757,"Missing"
2020.eamt-1.21,2016.amta-researchers.9,0,0.295217,"oyens, de prendre les choses en main.’ to English. The reference translation is ‘we decide therefore, citizens, to take control of things’ which is used here to simulate the user. The user corrects the first wrong word (things) from the hypothesis. The validated prefix (magenta phrase) and the last modified word (control) are fed back to the NMT system which generates a correct suffix (of things). As of today, NMT (Bahdanau et al., 2015; Vaswani et al., 2017) represents the state-of-theart in MT research. This has led researchers to test interactive-predictive protocol on NMT too, and papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore"
2020.eamt-1.21,W04-3250,0,0.0148959,"Missing"
2020.eamt-1.21,N03-1017,0,0.0815021,"h is used here to simulate the user. The user corrects the first wrong word (things) from the hypothesis. The validated prefix (magenta phrase) and the last modified word (control) are fed back to the NMT system which generates a correct suffix (of things). As of today, NMT (Bahdanau et al., 2015; Vaswani et al., 2017) represents the state-of-theart in MT research. This has led researchers to test interactive-predictive protocol on NMT too, and papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore and Joshi, 1999; Steedman, 2000) as a conditional context in an interactivepredictive protocol on Transformer (Vaswani et al., 2017), the c"
2020.eamt-1.21,P14-2094,0,0.0219439,"integrating syntactic knowledge into an interactive MT model. 2 Related Work Foster et al. (1997) were the first to introduce the idea of interactive-predictive MT as an alternative to pure post-editing MT. There have been a number of papers that explored this strategy in order to minimise human effort in translation and cover many use-cases involving SMT: e.g. applying online (Ortiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic kn"
2020.eamt-1.21,W19-6610,0,0.0153564,"al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT This section presents our fully syntactified inter"
2020.eamt-1.21,D14-1107,0,0.0125701,"ts contain 12,238,995 and 1,500 sentences, respectively. We use 1,500 sentences from the WMT15 news test set newstest2015 as our test set. In order to build our MT systems, we use the Sockeye3 (Hieber et al., 2018) toolkit. Our training setups are as follows. The tokens of the training, evaluation and validation sets are segmented into sub-word units using BPE. We performed 30,000 join operations. We use 6 layers in the encoder and decoder sides, an 8-head attention, hidden layer of size 512, embedding vector of size 512, learning rate 0.0002, and minimum batch size of 1,800 tokens. EasyCCG4 (Lewis and Steedman, 2014), a CCG supertagger, is used for generating the CCG sequence for the English sentences. Transformer (Baseline) Source Syntactified (SS) Target Syntactified (TS) Fully Syntactified (FS) 26.90 26.96 27.10 27.36 (p-value: 0.059) Table 3: The BLEU scores of baseline and syntactified NMT systems. Table 3 shows the performance of our baseline and syntax-sensitive NMT systems in terms of BLEU. The second and third rows represent the NMT models that incorporate source- and targetlanguage syntactic contexts, respectively, which we call source- (SS) and target-syntactified (TS) NMT systems, respectively"
2020.eamt-1.21,W17-4707,0,0.0327765,"Missing"
2020.eamt-1.21,J16-1004,0,0.0277097,"Missing"
2020.eamt-1.21,P02-1040,0,0.107611,"he sub-word units of a word inherit the CCG category of the word. As an example, we show an English sentence with supertags in Table 1. We see from row E of Table 1 that CCG ‘N’ of a word ‘Oberth¨ur’ is distributed over its sub-words (i.e. Ober@@ th@@ u¨ @@ and r). Our first experimental setup is referred to as PredCCG. Akoury et al. (2019) showed that integrating target-side ground-truth syntactic information into Transformer at decoding time significantly improved translation quality, and their syntaxbased model outperformed the baseline Transformer model by a large margin in terms of BLEU (Papineni et al., 2002). In reality, there is no way of obtaining the target-side ground-truth syntactic information at decoding time. However, in interactive-predictive mode, we found a way to obtain a slightly better CCG sequence for the partial translation (i.e. validated prefix) and inject them into the model at run-time, which we believe can positively impact the model’s subsequent predictions. In other words, in our second setup, we integrate a CCG supertagger into our INMT framework, and apply that on the validated prefix and unchecked suffix on the fly. The tagger is invoked when the user makes a correction."
2020.eamt-1.21,K18-1015,0,0.0135389,"(Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT This section presents our fully"
2020.eamt-1.21,2014.eamt-1.5,0,0.116043,"Missing"
2020.eamt-1.21,C16-2004,0,0.0626817,"Missing"
2020.eamt-1.21,P16-1007,0,0.0162171,"tiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT Th"
2020.eamt-1.21,C00-2137,0,0.0138574,"Missing"
2020.eamt-1.21,L16-1561,0,0.0119616,"ble 2, a new CCG tag sequence is generated for the hypothesis, and we see that CCG (N) of the newly added token ‘,’ is correct. Finally, INMT predicts another suggestion (row 9 of Table 2) where we see the remaining predictions are correct in the context. We call this experimental setup OnflyCCG. Note that the model is trained at sub-word level and generates sub-words at output; however, word level tokens are presented to the user. Naturally, On the fly CCG supertagger is applied to a hypothesis of word level. 5.2 MT systems We carry out experiments with French-to-English with the UN corpus2 (Ziemski et al., 2016). The training and development sets contain 12,238,995 and 1,500 sentences, respectively. We use 1,500 sentences from the WMT15 news test set newstest2015 as our test set. In order to build our MT systems, we use the Sockeye3 (Hieber et al., 2018) toolkit. Our training setups are as follows. The tokens of the training, evaluation and validation sets are segmented into sub-word units using BPE. We performed 30,000 join operations. We use 6 layers in the encoder and decoder sides, an 8-head attention, hidden layer of size 512, embedding vector of size 512, learning rate 0.0002, and minimum batch"
2020.eamt-1.26,2007.mtsummit-papers.27,0,0.0853144,"pta and Dhawan, 2019; Chinnery, 2008). The rapid development of c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Andy Way ADAPT Centre Dublin City University School of Computing andy.way@ adaptcentre.ie MT technologies is the result of massive research investment in the field over the past decades focusing on language resources, new methods and techniques with the aim of improving the quality of the MT output. Consequently, the progress made in MT technology has changed the way people are engaging with these systems (Gaspari and Hutchins, 2007). In the past, MT systems were used mainly for gisting purposes, but nowadays they are also being used as a tool supporting writing skills, grammar skills and language production in a second language (L2) (Ni˜no, 2006; Garcia and Pena, 2011). However, research in the MT field focusing on end-users is limited so that, currently, little we know about what the users’ interaction with an MT system could bring to the mental processing of a second language. In this paper, we aim at investigating the role MT systems play, especially, the role that the popular MT system Google Translate plays on the p"
2020.eamt-1.26,N09-2048,0,0.109301,"Missing"
2020.eamt-1.46,L16-1090,0,0.037009,"Missing"
2020.eamt-1.46,aziz-etal-2012-pet,1,0.738909,"er word/segment. It is also faster than error annotation and requires less training, particularly if the translators already have experience of post-editing MT output. It is also the method which is closest to the situation in which MT is intended to be used, and as a result translator opinions of the post-editing tasks can also be elicited. For these reasons, we see post-editing as the HE method that best suits the needs and intended outputs of this study. This section describes the set-up and methodology of the PE task and related survey. 4.1 PET tool and guidelines Post-editing tool (PET) (Aziz et al., 2012) was chosen as the software with which to collect data for this study as it is freely available online and specifically designed for use in HE studies of MT. We configure PET with the default parameters and compose guidelines and instructions for the participants. For example, participants were permitted to use dictionaries while editing the output, but were not permitted to use another MT tool. The guidelines were written in Irish, the target language of this study. 4.2 Pilot study Prior to the main study, we conducted a pilot study to ensure that the tool was set up correctly and to test the"
2020.eamt-1.46,W19-6705,0,0.0401371,"Missing"
2020.eamt-1.46,W19-6806,0,0.0635475,"Missing"
2020.eamt-1.46,W18-2202,1,0.845368,"MT. This study aims to provide the first EN→GA MT HE study, investigating the measurable usefulness of EN→GA in a professional translation capacity. In an attempt to closely match the context in which EN→GA MT is intended to be used, professional translators will undertake post-editing (PE) tasks using MT output. Another aim of this study is to provide a humanderived comparison of EN→GA statistical machine translation (SMT) and neural machine translation (NMT). In previous work, a preliminary comparison of EN→GA SMT and NMT showed that SMT fared better than NMT in terms of automatic metrics (Dowling et al., 2018). More recent publications (Defauw et al., 2019; Dowling et al., 2019) show a more positive picture for EN→GA NMT, but without a direct comparison to SMT. The SMT/NMT comparison presented in this paper will take into account both the quantitative metadata gathered during the study (time per seg3 A recent study by Moorkens (2020) reported that “...few participants appear to use MT at present...” ment, number of keystrokes, etc.) as well as the qualitative opinions and recommendations of the participants. This paper is presented as follows: Section 2 describes related work in the areas of EN→GA"
2020.eamt-1.46,W19-6721,0,0.0296911,"Missing"
2020.eamt-1.46,W11-2123,0,0.0317553,"ean Language Resource Coordination,8 an initiative led by the European Commission to gather language resources for all EU official languages. ELRI9 is an initiative which focuses on the building and sharing of language resources within France, Ireland, Portugal and Spain (Etchegoyhen et al., 2018) (European Language Resource Infrastructure). 3.2 SMT parameters When training the SMT system, we follow parameters identified in previous work. Moses (Koehn et al., 2007), the standard tool for building SMT systems, along with the data described in Section 3.1, is used to train our SMT model. KenLM (Heafield, 2011) is used to train a 6-gram language model using the GA portion of the parallel data, as well as the monolingual GA data. This wider-context language model (3-gram is the default) along with hierarchical reordering tables are used in an attempt to address the divergent word orders of EN and GA (EN having subject-verb-object and GA having verb-subject object word order.) 3.3 NMT parameters As in other research on EN-GA NMT (Defauw et al., 2019; Dowling et al., 2018), we use OpenNMT (Klein et al., 2017) as the basis for training our NMT system. We implement a transformerbased approach (Vaswani et"
2020.eamt-1.46,W19-6613,0,0.101547,"oorkens’ (2020) extensive survey reports varying attitudes between translators based on terms of employment, with freelance translators appearing to be poorly disposed towards MT. Koehn and Knowles (2017) include low– resource languages as one of the main challenges still present in MT research. Unable to exploit cutting-edge techniques that require huge resources, MT researchers must look to creative solutions to improve low–resource MT. Such approaches include the creation of artificial parallel data, e.g. through back-translation (Poncelas et al., 2018), exploiting out-of-domain data (c.f. Imankulova et al., (2019)) and using a betterresourced language as a pivot (Wu and Wang, 2007; Liu et al., 2018; Cheng, 2019). HE is a vital component of MT research (Castilho et al., 2018), with many of the major MT conferences including a translator track to encourage such publications. They are especially valuable in low-resource or minority contexts (e.g. Spanish-Galician MT (Bay´on and S´anchez-Gij´on, 2019), Russian-Japanese MT (Imankulova et al., 2019)) where the languages may be overlooked by global MT companies. There have been comparisons of SMT and NMT since NMT first emerged in the field. The conference on"
2020.eamt-1.46,P17-4012,0,0.0272803,"SMT systems, along with the data described in Section 3.1, is used to train our SMT model. KenLM (Heafield, 2011) is used to train a 6-gram language model using the GA portion of the parallel data, as well as the monolingual GA data. This wider-context language model (3-gram is the default) along with hierarchical reordering tables are used in an attempt to address the divergent word orders of EN and GA (EN having subject-verb-object and GA having verb-subject object word order.) 3.3 NMT parameters As in other research on EN-GA NMT (Defauw et al., 2019; Dowling et al., 2018), we use OpenNMT (Klein et al., 2017) as the basis for training our NMT system. We implement a transformerbased approach (Vaswani et al., 2017), which has shown promising results for low-resource NMT with other language pairs (Lakew et al., 2017; Murray et al., 2019). We use parameters recommended by Vaswani et al., (2017). 3.4 Test data 1,500 sentences of gold-standard data,10 with an average sentence length of 20 words per sentence, were held out from training data in order to perform automatic evaluation. This data contains extracts from DCHG sources such as official correspondence, public announcements, etc. 3.5 Automatic eva"
2020.eamt-1.46,W17-3204,0,0.0260143,"st that SMT seems to outperform NMT (Dowling et al., 2018), although we show a number of examples where the score may be misleading and recommend that a HE study may be necessary to fully understand the quality of each system type. More recent studies (Defauw et al., 2019; Dowling et al., 2019) show the effects of adding artificiallycreated training data to EN-GA NMT. In terms of Irish translators’ attitudes to MT, Moorkens’ (2020) extensive survey reports varying attitudes between translators based on terms of employment, with freelance translators appearing to be poorly disposed towards MT. Koehn and Knowles (2017) include low– resource languages as one of the main challenges still present in MT research. Unable to exploit cutting-edge techniques that require huge resources, MT researchers must look to creative solutions to improve low–resource MT. Such approaches include the creation of artificial parallel data, e.g. through back-translation (Poncelas et al., 2018), exploiting out-of-domain data (c.f. Imankulova et al., (2019)) and using a betterresourced language as a pivot (Wu and Wang, 2007; Liu et al., 2018; Cheng, 2019). HE is a vital component of MT research (Castilho et al., 2018), with many of"
2020.eamt-1.46,P07-2045,0,0.00664096,"webcrawled data (Espl`aGomis et al., 2019). Further to this, we add two new corpora, referred to in Table 1 as ELRC and ELRI. ELRC refers to the European Language Resource Coordination,8 an initiative led by the European Commission to gather language resources for all EU official languages. ELRI9 is an initiative which focuses on the building and sharing of language resources within France, Ireland, Portugal and Spain (Etchegoyhen et al., 2018) (European Language Resource Infrastructure). 3.2 SMT parameters When training the SMT system, we follow parameters identified in previous work. Moses (Koehn et al., 2007), the standard tool for building SMT systems, along with the data described in Section 3.1, is used to train our SMT model. KenLM (Heafield, 2011) is used to train a 6-gram language model using the GA portion of the parallel data, as well as the monolingual GA data. This wider-context language model (3-gram is the default) along with hierarchical reordering tables are used in an attempt to address the divergent word orders of EN and GA (EN having subject-verb-object and GA having verb-subject object word order.) 3.3 NMT parameters As in other research on EN-GA NMT (Defauw et al., 2019; Dowling"
2020.eamt-1.46,D19-5625,0,0.0115364,"data. This wider-context language model (3-gram is the default) along with hierarchical reordering tables are used in an attempt to address the divergent word orders of EN and GA (EN having subject-verb-object and GA having verb-subject object word order.) 3.3 NMT parameters As in other research on EN-GA NMT (Defauw et al., 2019; Dowling et al., 2018), we use OpenNMT (Klein et al., 2017) as the basis for training our NMT system. We implement a transformerbased approach (Vaswani et al., 2017), which has shown promising results for low-resource NMT with other language pairs (Lakew et al., 2017; Murray et al., 2019). We use parameters recommended by Vaswani et al., (2017). 3.4 Test data 1,500 sentences of gold-standard data,10 with an average sentence length of 20 words per sentence, were held out from training data in order to perform automatic evaluation. This data contains extracts from DCHG sources such as official correspondence, public announcements, etc. 3.5 Automatic evaluation Automatic evaluation metrics, while best used to track developmental changes in one particular MT system over time, can also be used to gauge differences in quality between two different MT systems. In this study we genera"
2020.eamt-1.46,N19-4007,0,0.0145024,"verage (avg.) number of seconds (time) per segment (seg.), average number of keystrokes (keys.) per segments, average unchanged segments and HTER of each system for each participant. HTER score. Overall, these results suggest HTER to be a more valuable indication than other metrics gathered. 5.3 PE output With both the survey responses and figures generated using results from PET varying substantially from translator to translator we chose to take a closer look at the differences in PE output provided by the four participants. To identify potentially interesting sentences, we used compare-mt (Neubig et al., 2019), a tool designed to analyse MT output and provide the user with sentences which differ greatly. Although human-generated translations are not the intended input for compare-mt, it was still useful in identifying cases where the participants gave different translations. Input: SMT: P1: P3: NMT: P2: P4: If you have been allocated as a decision-maker.. M´a t´a t´u mar a d´eant´oir cinnt´ı..* If you are a decision manufacturer.. M´as cinnteoir th´u air.. If you are a decision-maker for it.. M´a ainmn´ıodh th´u mar chinnteoir.. If you are named as a decision-maker.. M´a roghna´ıodh mar chinnteoir"
2020.eamt-1.46,P02-1040,0,0.110223,"parameters recommended by Vaswani et al., (2017). 3.4 Test data 1,500 sentences of gold-standard data,10 with an average sentence length of 20 words per sentence, were held out from training data in order to perform automatic evaluation. This data contains extracts from DCHG sources such as official correspondence, public announcements, etc. 3.5 Automatic evaluation Automatic evaluation metrics, while best used to track developmental changes in one particular MT system over time, can also be used to gauge differences in quality between two different MT systems. In this study we generate BLEU (Papineni et al., 2002), TER (Snover et al., 2009), CharacTER (Wang et al., 2016) and ChrF scores (Popovi´c, 2015). SMT NMT BLEU↑ 45.13 46.58 TER↓ 43.51 40.85 ChrF↑ 66.26 67.21 CharacTER↓ 0.29 0.28 4 Parallel texts from two EU bodies: the Digital Corpus of the European Parliament (DCEP) and Directorate General for Translation, Translation Memories (DGT-TM) 5 Crawled from various sources including Citizens Information, an Irish government website that provides information on public services 6 Conradh na Gaeilge is a public organisation tasked with the promotion of the Irish language 7 The state agency providing resea"
2020.eamt-1.46,W15-3049,0,0.139839,"Missing"
2020.eamt-1.46,2006.amta-papers.25,0,0.0838942,", line up with the automatic metrics gathered during this study (BLEU, TER, ChrF and CharacTER scores suggested that the NMT output was of greater quality than that of SMT – see section 3 for more details). The results gathered from PET provided us not only with the post-edited output, but also with the number of keystrokes, annotations, and seconds spent on each segment. We used this data to calculate the average seconds per segment, average keystrokes per segment, and the average unchanged segments per system per participant. These figures, as well as the human-targetered TER (HTER) scores (Snover et al., 2006), are displayed in Table 6. Where MT for dissemination is concerned, temporal effort, or time spent PE, is arguably the most important metric as payment is usually based on words translated. Two of the four participants in this study (P1 and P4) were more productive when working with NMT output. The difference for P4 was sizeable (an average of 48.53 seconds per segment for NMT compared to 193.06 for SMT), although it should be noted that P4 was required to repeat the PE task for the NMT job due to a technical error. It is likely that this led to a faster PE time for this job, and that other v"
2020.eamt-1.46,W18-6312,1,0.780865,"study. 4.2 Pilot study Prior to the main study, we conducted a pilot study to ensure that the tool was set up correctly and to test the robustness of the guidelines. Two Irish linguists each post-edited 10 machine–translated sentences. We then updated the guidelines as per the feedback of both pilot study participants. 4.3 Data Two subsets were extracted from the test data described in Section 3.1, each containing 100 EN sentences, and then translated with the SMT and NMT systems described in 3.2 and 3.3 respectively. With the merits of document-level translation raised in recent MT research (Toral et al., 2018; Werlen et al., 2018) and the importance of context in work using MT for dissemination, we choose to keep the sequence of sentences, rather than extract each of the 200 sentences individually at random. Recent studies have shown that MT can have a negative impact on the linguistic richness of MT output (Vanmassenhove et al., 2019) and postedited translations (Toral et al., 2018). To demonstrate the differences in linguistic richness between SMT and NMT, we calculate standardised typetoken ratio (STTR) with the outputs.11 Table 3 shows that, although a small difference can be seen between jobs"
2020.eamt-1.46,W19-6622,1,0.84122,"Data Two subsets were extracted from the test data described in Section 3.1, each containing 100 EN sentences, and then translated with the SMT and NMT systems described in 3.2 and 3.3 respectively. With the merits of document-level translation raised in recent MT research (Toral et al., 2018; Werlen et al., 2018) and the importance of context in work using MT for dissemination, we choose to keep the sequence of sentences, rather than extract each of the 200 sentences individually at random. Recent studies have shown that MT can have a negative impact on the linguistic richness of MT output (Vanmassenhove et al., 2019) and postedited translations (Toral et al., 2018). To demonstrate the differences in linguistic richness between SMT and NMT, we calculate standardised typetoken ratio (STTR) with the outputs.11 Table 3 shows that, although a small difference can be seen between jobs for both systems, in average both MT systems have a very similar STTR. System SMT NMT Job 1 41.71 43.84 Job 2 42.69 41.33 Average 42.20 42.59 Table 3: Comparison of STTR between SMT and NMT outputs normalised per 1000 words 4.4 Participants With EN-GA MT more likely to be used as a tool to help publish translated content in an off"
2020.eamt-1.46,W16-2342,0,0.013647,"a 1,500 sentences of gold-standard data,10 with an average sentence length of 20 words per sentence, were held out from training data in order to perform automatic evaluation. This data contains extracts from DCHG sources such as official correspondence, public announcements, etc. 3.5 Automatic evaluation Automatic evaluation metrics, while best used to track developmental changes in one particular MT system over time, can also be used to gauge differences in quality between two different MT systems. In this study we generate BLEU (Papineni et al., 2002), TER (Snover et al., 2009), CharacTER (Wang et al., 2016) and ChrF scores (Popovi´c, 2015). SMT NMT BLEU↑ 45.13 46.58 TER↓ 43.51 40.85 ChrF↑ 66.26 67.21 CharacTER↓ 0.29 0.28 4 Parallel texts from two EU bodies: the Digital Corpus of the European Parliament (DCEP) and Directorate General for Translation, Translation Memories (DGT-TM) 5 Crawled from various sources including Citizens Information, an Irish government website that provides information on public services 6 Conradh na Gaeilge is a public organisation tasked with the promotion of the Irish language 7 The state agency providing research, advisory and education in agriculture, horticulture,"
2020.eamt-1.46,D18-1325,0,0.0193383,"dy Prior to the main study, we conducted a pilot study to ensure that the tool was set up correctly and to test the robustness of the guidelines. Two Irish linguists each post-edited 10 machine–translated sentences. We then updated the guidelines as per the feedback of both pilot study participants. 4.3 Data Two subsets were extracted from the test data described in Section 3.1, each containing 100 EN sentences, and then translated with the SMT and NMT systems described in 3.2 and 3.3 respectively. With the merits of document-level translation raised in recent MT research (Toral et al., 2018; Werlen et al., 2018) and the importance of context in work using MT for dissemination, we choose to keep the sequence of sentences, rather than extract each of the 200 sentences individually at random. Recent studies have shown that MT can have a negative impact on the linguistic richness of MT output (Vanmassenhove et al., 2019) and postedited translations (Toral et al., 2018). To demonstrate the differences in linguistic richness between SMT and NMT, we calculate standardised typetoken ratio (STTR) with the outputs.11 Table 3 shows that, although a small difference can be seen between jobs for both systems, in"
2020.eamt-1.46,P07-1108,0,0.392952,"rs based on terms of employment, with freelance translators appearing to be poorly disposed towards MT. Koehn and Knowles (2017) include low– resource languages as one of the main challenges still present in MT research. Unable to exploit cutting-edge techniques that require huge resources, MT researchers must look to creative solutions to improve low–resource MT. Such approaches include the creation of artificial parallel data, e.g. through back-translation (Poncelas et al., 2018), exploiting out-of-domain data (c.f. Imankulova et al., (2019)) and using a betterresourced language as a pivot (Wu and Wang, 2007; Liu et al., 2018; Cheng, 2019). HE is a vital component of MT research (Castilho et al., 2018), with many of the major MT conferences including a translator track to encourage such publications. They are especially valuable in low-resource or minority contexts (e.g. Spanish-Galician MT (Bay´on and S´anchez-Gij´on, 2019), Russian-Japanese MT (Imankulova et al., 2019)) where the languages may be overlooked by global MT companies. There have been comparisons of SMT and NMT since NMT first emerged in the field. The conference on machine translation (WMT) regularly feature both systems, with HE a"
2020.eamt-1.54,W19-6718,1,0.602846,"so far by the lack of extensive highquality LRs that are required to build effective systems, especially parallel corpora. PRINCIPLE aims to improve LR collection efforts in the respective languages, prioritising the two strategic Digital Service Infrastructures (DSIs)1 of eJustice and eProcurement. The LRs assembled and curated in PRINCIPLE will be validated to demonstrate improved MT quality, and will be uploaded via ELRC-SHARE to enhance MT systems provided by eTranslation, that are available to public administrations in Europe, thus promoting language equality for low-resource languages. Way and Gaspari (2019) introduced the PRINCIPLE project at its start, giving a highlevel overview of its main objectives, along with the planned activities and the overall approach to data collection and validation. They also explained its position within the wider eco-system of related, recently finished CEF projects such as iADAATPA (Castilho et al., 2019) and ELRI. 2 This paper provides an update on the progress of PRINCIPLE, focusing on its initial achievements and describing ongoing activities, especially in terms of engaging with stakeholders and MT users, and concludes with future plans to promote the contin"
2020.eamt-1.69,2007.mtsummit-papers.27,0,0.14687,"Missing"
2020.icon-adapmt.4,W19-5206,0,0.0696205,"d test set in Table 1. 2 Our Approaches 2.1 Training Data Augmentation The use of unlabeled monolingual data in addition to limited bitexts for NMT training (Sennrich et al., 1 https://ssmt.iiit.ac.in/ machinetranslation.html 2 https://www.iitp.ac.in/˜ai-nlp-ml/ icon2020/main_prog.html 3 https://github.com/marian-nmt/marian 17 Proceedings of the 17th International Conference on Natural Language Processing: Adap-MT 2020 Shared Task, pages 17–23 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019) is nowadays a common practice in MT development (Barrault et al., 2020). This has even more impact when applied to the specialised domains and many language pairs, for which obtaining parallel data is a challenge. In this task, in order to improve our baseline English-to-Hindi Transformer model, we augmented our training data with target-original synthetic data. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the source sentences of the synthetic data with the extra tokens. Iterative generation and training on synthetic data can yield"
2020.icon-adapmt.4,D19-5213,0,0.0180772,"e impact when applied to the specialised domains and many language pairs, for which obtaining parallel data is a challenge. In this task, in order to improve our baseline English-to-Hindi Transformer model, we augmented our training data with target-original synthetic data. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the source sentences of the synthetic data with the extra tokens. Iterative generation and training on synthetic data can yield increasingly better NMT systems, especially in lowresource scenarios (Hoang et al., 2018; Chen et al., 2019). Since our baseline target-to-source (Hindito-English) MT system is already good in quality, it was used to translate the Hindi monolingual data. 2.2 of lexical ambiguity, ‘cold’ has several possible meanings in the Unified Medical Language System Metathesaurus (Humphreys et al., 1998) including ‘common cold’, ‘cold sensation’ and ‘cold temperature’ (Stevenson and Guo, 2010). Moreover, a polysemous term (e.g. ‘cold’) could have many translation equivalents in a target language. With this in mind, we mined those training examples (i.e. sentence pairs) from the large out-of-domain domain parall"
2020.icon-adapmt.4,D11-1033,0,0.0508733,"r or rarely occur in the training data and are more indicative of the indomain AI corpus. Given the lists of source and target terms, we mine sentences independently from the source and target sides of the out-of-domain bilingual corpus. As pointed out above, we select those sentence pairs from the out-of-domain bilingual corpus whose source or target sides contain at least one domain term. In Nayak et al. (2020b), we empirically showed that such “pseudo” in-domain sentences are more effective than those mined using bilingual cross-entropy difference according to the in-domain language model (Axelrod et al., 2011) for NMT model adaptation. Mixed Fine-Tuning As for adapting our baseline MT model to the AI domain, we implemented mixed fine-tuning of model parameters, where fine-tuning is conducted on the training data that consists of both in-domain and out-of-domain data as described in Chu et al. (2017). The shared task organisers released parallel training data of the AI domain with a limited number of in-domain examples (only 4,872 sentence pairs). The in-domain data was augmented by oversampling the AI training set several times, and an almost similar sized out-of-domain data set is mined from the p"
2020.icon-adapmt.4,R19-1052,1,0.830557,"Moreover, a polysemous term (e.g. ‘cold’) could have many translation equivalents in a target language. With this in mind, we mined those training examples (i.e. sentence pairs) from the large out-of-domain domain parallel corpus whose source or target sentences contain at least one domain term. As pointed out earlier, an extracted out-of-domain sentence that contain a domain term may not represent the desired domain; however, the training examples that include such sentences may play a crucial role in minimising lexical selection errors as far as terminology translation in NMT is concerned (Haque et al., 2019, 2020a). To this end, we exploit the approaches of Rayson and Garside (2000) and Haque et al. (2014, 2018) in order to automatically identify terms in the indomain texts. The idea is to identify those words which are most indicative (or characteristic) of the in-domain corpus compared to a reference corpus. Haque et al. (2014, 2018) used a large corpus which is generic in nature as a reference corpus. We adopted their approach and used a large generic corpus in order to identify terms in the in-domain source (English) and target (Hindi) corpora. In our setup, we also used the source and targe"
2020.icon-adapmt.4,W18-6315,0,0.0175453,"k, we show a couple of sentences from the blind test set in Table 1. 2 Our Approaches 2.1 Training Data Augmentation The use of unlabeled monolingual data in addition to limited bitexts for NMT training (Sennrich et al., 1 https://ssmt.iiit.ac.in/ machinetranslation.html 2 https://www.iitp.ac.in/˜ai-nlp-ml/ icon2020/main_prog.html 3 https://github.com/marian-nmt/marian 17 Proceedings of the 17th International Conference on Natural Language Processing: Adap-MT 2020 Shared Task, pages 17–23 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019) is nowadays a common practice in MT development (Barrault et al., 2020). This has even more impact when applied to the specialised domains and many language pairs, for which obtaining parallel data is a challenge. In this task, in order to improve our baseline English-to-Hindi Transformer model, we augmented our training data with target-original synthetic data. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the source sentences of the synthetic data with the extra tokens. Iterative gener"
2020.icon-adapmt.4,W18-2703,0,0.0224471,"). This has even more impact when applied to the specialised domains and many language pairs, for which obtaining parallel data is a challenge. In this task, in order to improve our baseline English-to-Hindi Transformer model, we augmented our training data with target-original synthetic data. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the source sentences of the synthetic data with the extra tokens. Iterative generation and training on synthetic data can yield increasingly better NMT systems, especially in lowresource scenarios (Hoang et al., 2018; Chen et al., 2019). Since our baseline target-to-source (Hindito-English) MT system is already good in quality, it was used to translate the Hindi monolingual data. 2.2 of lexical ambiguity, ‘cold’ has several possible meanings in the Unified Medical Language System Metathesaurus (Humphreys et al., 1998) including ‘common cold’, ‘cold sensation’ and ‘cold temperature’ (Stevenson and Guo, 2010). Moreover, a polysemous term (e.g. ‘cold’) could have many translation equivalents in a target language. With this in mind, we mined those training examples (i.e. sentence pairs) from the large out-of-"
2020.icon-adapmt.4,P19-1581,0,0.0164121,"stances. The selected sentences are then automatically translated by an NMT system built on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. The synthetic training data whose source-side sentences are original could be more effective for domain adaptation, and the learning method that uses such training data is called ‘self-training’ (Ueffing et al., 2007). In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017; Wuebker et al., 2018; Huck et al., 2019). We followed Chinea-R´ıos et al. (2017) in order to mine those sentences from large monolingual datasets that could be beneficial for fine-tuning the original NMT model. As in Jooste et al. (2020); Nayak et al. (2020b); Parthasarathy et al. (2020), we first identified terms in the AI test set to be translated, and given the list of extracted terms, English sentences which were mined from large monolingual data are similar in style to the AI test set sentences. To put it another way, we followed the method described in Section 2.2 in order to extract sentences form large monolingual corpus. Th"
2020.icon-adapmt.4,2020.wmt-1.91,1,0.871683,"bitexts on which our baseline NMT system was trained as the reference corpora. The intuition is again the same, i.e. to extract those (terminological) expressions from the in-domain data that do not occur or rarely occur in the training data and are more indicative of the indomain AI corpus. Given the lists of source and target terms, we mine sentences independently from the source and target sides of the out-of-domain bilingual corpus. As pointed out above, we select those sentence pairs from the out-of-domain bilingual corpus whose source or target sides contain at least one domain term. In Nayak et al. (2020b), we empirically showed that such “pseudo” in-domain sentences are more effective than those mined using bilingual cross-entropy difference according to the in-domain language model (Axelrod et al., 2011) for NMT model adaptation. Mixed Fine-Tuning As for adapting our baseline MT model to the AI domain, we implemented mixed fine-tuning of model parameters, where fine-tuning is conducted on the training data that consists of both in-domain and out-of-domain data as described in Chu et al. (2017). The shared task organisers released parallel training data of the AI domain with a limited number"
2020.icon-adapmt.4,P02-1040,0,0.106365,"sentences. Our terminology extraction model identified 1,599 AI terms in the blind test set. We mined 98,009 English sentences from the large monolingual data given the list of terms. We followed the approach described above for fine-tuning our best two models (Base2 + Mixed FT and Base3 + Mixed FT) in order to translate the blind test set sentences. The BLEU scores of our MT systems on the blind test set, which the task organisers published, are shown in Table 4. Experiments and Results This section presents the performance of our MT systems in terms of the automatic evaluation metric BLEU (Papineni et al., 2002). Additionally, we performed statistical significance tests using bootstrap resampling methods (Koehn, 2004). We obtained the BLEU scores of our MT systems to evaluate them on the test set, and the scores are reported in Table 3. The first row of Table 3 repBLEU Base 28.97 Base2 (Base + 1M Syn) 30.80 Base3 (Base + 8M Syn) 29.97 Base2 + Mixed FT 42.02 Base3 + Mixed FT 43.03 Base2 + Mixed FT + ST 43.00 Base3 + Mixed FT + ST 43.51 Table 3: The BLEU scores of the English-to-Hindi NMT systems. resents our baseline English-to-Hindi MT system. The Hindi-to-English MT system which has been used to tra"
2020.icon-adapmt.4,2020.wmt-1.27,1,0.852253,"nces are original could be more effective for domain adaptation, and the learning method that uses such training data is called ‘self-training’ (Ueffing et al., 2007). In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017; Wuebker et al., 2018; Huck et al., 2019). We followed Chinea-R´ıos et al. (2017) in order to mine those sentences from large monolingual datasets that could be beneficial for fine-tuning the original NMT model. As in Jooste et al. (2020); Nayak et al. (2020b); Parthasarathy et al. (2020), we first identified terms in the AI test set to be translated, and given the list of extracted terms, English sentences which were mined from large monolingual data are similar in style to the AI test set sentences. To put it another way, we followed the method described in Section 2.2 in order to extract sentences form large monolingual corpus. The monolingual corpus that we used for this purpose contains 95,918,840 sentences which were sampled from CommonCrawl5 and Wikipedia Dumps.6 The English source sentences that have been mined were translated into Hindi using the best MT system (cf. t"
2020.icon-adapmt.4,kobus-etal-2017-domain,0,0.0231466,"ated business scene dialogue (Jooste et al., 2020) in the WAT 20204 (Nakazawa et al., 2020) document-level translation task. However, the adaptation method presented in this paper slightly differs from the conventional mixed finetuning (Chu et al., 2017; Jooste et al., 2020), and is described below. Terms are usually indicators of the nature of a domain and play a critical role in domain-specific MT (Haque et al., 2019, 2020a). Sentences that contain in-domain terms are likely to be in-domain sentences. However, an ambiguous term could have more than one potential meaning. As an example As in Kobus et al. (2017), in order to inform the NMT model about the domain during training and decoding, we add a (domain) tag at the begin4 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2020/index.html 18 ning of the source sentences of the in-domain data, which allows us to control the output domain of the trained system. The NMT system is finally finetuned on the mixture of the in-domain and mined out-of-domain corpora. 2.3 create synthetic data (i.e. source-side original synthetic corpus (SOSC)) to be used for fine-tuning the same NMT model. 3 Data Used and Training Setups For building our baseline models (forward and"
2020.icon-adapmt.4,W04-3250,0,0.248334,"sentences from the large monolingual data given the list of terms. We followed the approach described above for fine-tuning our best two models (Base2 + Mixed FT and Base3 + Mixed FT) in order to translate the blind test set sentences. The BLEU scores of our MT systems on the blind test set, which the task organisers published, are shown in Table 4. Experiments and Results This section presents the performance of our MT systems in terms of the automatic evaluation metric BLEU (Papineni et al., 2002). Additionally, we performed statistical significance tests using bootstrap resampling methods (Koehn, 2004). We obtained the BLEU scores of our MT systems to evaluate them on the test set, and the scores are reported in Table 3. The first row of Table 3 repBLEU Base 28.97 Base2 (Base + 1M Syn) 30.80 Base3 (Base + 8M Syn) 29.97 Base2 + Mixed FT 42.02 Base3 + Mixed FT 43.03 Base2 + Mixed FT + ST 43.00 Base3 + Mixed FT + ST 43.51 Table 3: The BLEU scores of the English-to-Hindi NMT systems. resents our baseline English-to-Hindi MT system. The Hindi-to-English MT system which has been used to translate the Hindi monolingual sentences to English is of good quality (i.e. it produces 28.76 BLEU points on"
2020.icon-adapmt.4,W00-0901,0,0.0310404,"equivalents in a target language. With this in mind, we mined those training examples (i.e. sentence pairs) from the large out-of-domain domain parallel corpus whose source or target sentences contain at least one domain term. As pointed out earlier, an extracted out-of-domain sentence that contain a domain term may not represent the desired domain; however, the training examples that include such sentences may play a crucial role in minimising lexical selection errors as far as terminology translation in NMT is concerned (Haque et al., 2019, 2020a). To this end, we exploit the approaches of Rayson and Garside (2000) and Haque et al. (2014, 2018) in order to automatically identify terms in the indomain texts. The idea is to identify those words which are most indicative (or characteristic) of the in-domain corpus compared to a reference corpus. Haque et al. (2014, 2018) used a large corpus which is generic in nature as a reference corpus. We adopted their approach and used a large generic corpus in order to identify terms in the in-domain source (English) and target (Hindi) corpora. In our setup, we also used the source and target sides of the out-of-domain training bitexts on which our baseline NMT syste"
2020.icon-adapmt.4,P16-1009,0,0.278685,"process was facilitated through automatic extraction of terminology from the in-domain data. This paper outlines the experiments we carried out for this task and reports the performance of our NMT systems on the evaluation test set. 1 Table 1: Sentences from the AI blind test set. Our MT systems are Transformer models (Vaswani et al., 2017) which were trained using the Marian-NMT toolkit.3 In this work, we applied different data augmentation and domain adaptation techniques to train our models, such as using synthetic data from target-side monolingual data through the use of back-translation (Sennrich et al., 2016a; Poncelas et al., 2018), mixed fine-tuning (Chu et al., 2017) and on-the-fly model adaption (Chinea-R´ıos et al., 2017). As for the latter two approaches, we mined sentences and sentence pairs from large out-of-domain monolingual and parallel corpora, respectively, based on domain terms appearing in the in-domain data. Note that the terms were extracted automatically from the in-domain data. This remainder of the paper is organized as follows. Section 2 presents our approaches. We describe the resources we utilized for training in Section 3. Section 4 presents the results obtained, and Secti"
2020.icon-adapmt.4,tiedemann-2012-parallel,0,0.0464624,"MT system is finally finetuned on the mixture of the in-domain and mined out-of-domain corpora. 2.3 create synthetic data (i.e. source-side original synthetic corpus (SOSC)) to be used for fine-tuning the same NMT model. 3 Data Used and Training Setups For building our baseline models (forward and backward), we used only the bilingual data provided by the task organisers. As for Hindi monolingual sentences for back-translation, we sampled them from AI4Bharat-IndicNLP Corpus (Kunchukuttan et al., 2020). The out-of-domain parallel data is compiled from a variety of existing sources, e.g. OPUS7 (Tiedemann, 2012), and after applying standard cleaning procedures including applying a language identifier8 we are left with just over 1.1 million parallel sentence pairs. Table 2 presents the corpus statistics. The development set Mining Sentences for Fine-tuning Chinea-R´ıos et al. (2017) demonstrated that in the case of specialised domains where parallel corpora are scarce, sentences of a large monolingual data that are more related to the test set sentences to be translated could be effective for fine-tuning the original general domain NMT model. They select those instances from a large monolingual corpus"
2020.icon-adapmt.4,P07-1004,0,0.0522544,"ld be effective for fine-tuning the original general domain NMT model. They select those instances from a large monolingual corpus whose vector-space representation is similar to the representation of the test set instances. The selected sentences are then automatically translated by an NMT system built on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. The synthetic training data whose source-side sentences are original could be more effective for domain adaptation, and the learning method that uses such training data is called ‘self-training’ (Ueffing et al., 2007). In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017; Wuebker et al., 2018; Huck et al., 2019). We followed Chinea-R´ıos et al. (2017) in order to mine those sentences from large monolingual datasets that could be beneficial for fine-tuning the original NMT model. As in Jooste et al. (2020); Nayak et al. (2020b); Parthasarathy et al. (2020), we first identified terms in the AI test set to be translated, and given the list of extracted terms, English sentences which were mined"
2020.icon-adapmt.4,D18-1104,0,0.0187285,"ion of the test set instances. The selected sentences are then automatically translated by an NMT system built on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. The synthetic training data whose source-side sentences are original could be more effective for domain adaptation, and the learning method that uses such training data is called ‘self-training’ (Ueffing et al., 2007). In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017; Wuebker et al., 2018; Huck et al., 2019). We followed Chinea-R´ıos et al. (2017) in order to mine those sentences from large monolingual datasets that could be beneficial for fine-tuning the original NMT model. As in Jooste et al. (2020); Nayak et al. (2020b); Parthasarathy et al. (2020), we first identified terms in the AI test set to be translated, and given the list of extracted terms, English sentences which were mined from large monolingual data are similar in style to the AI test set sentences. To put it another way, we followed the method described in Section 2.2 in order to extract sentences form large mo"
2020.icon-adapmt.4,D16-1160,0,0.0230608,"challenges of this task, we show a couple of sentences from the blind test set in Table 1. 2 Our Approaches 2.1 Training Data Augmentation The use of unlabeled monolingual data in addition to limited bitexts for NMT training (Sennrich et al., 1 https://ssmt.iiit.ac.in/ machinetranslation.html 2 https://www.iitp.ac.in/˜ai-nlp-ml/ icon2020/main_prog.html 3 https://github.com/marian-nmt/marian 17 Proceedings of the 17th International Conference on Natural Language Processing: Adap-MT 2020 Shared Task, pages 17–23 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019) is nowadays a common practice in MT development (Barrault et al., 2020). This has even more impact when applied to the specialised domains and many language pairs, for which obtaining parallel data is a challenge. In this task, in order to improve our baseline English-to-Hindi Transformer model, we augmented our training data with target-original synthetic data. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the source sentences of the synthetic data with the extra"
2020.icon-main.14,2020.wmt-1.1,0,0.196361,"Missing"
2020.icon-main.14,W19-5206,0,0.0539609,"methods (Koehn, 2004). We obtained the BLEU scores of our MT systems on the test set, and the scores are reported in Table 5. The first row of Table 5 represents our baseline Hindi-to-English MT system. The English-to-Hindi MT system which has been used to translate the English monolingual sentences (reviews) into Hindi produced 20.52 BLEU points on the development set. The BLEU scores of the MT systems (Base+BT and Base+BT+FT) trained on training data that consists of both authentic and (target- or/and source-original) synthetic parallel data are shown in the next two rows of Table 5. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the source sentences of the synthetic data with the extra 11 http://www.cfilt.iitb.ac.in/iitb_parallel/ http://opus.lingfil.uu.se/ 13 https://pypi.org/project/pycld2/ tokens. BLEU devset test set Base 25.92 23.03 Base+BT 30.84 26.51 Base+BT+FT 30.89 26.85 Base+BT+FT+DA 31.52 27.49 Table 5: The BLEU scores of the English-to-Hindi NMT systems. We observed that the review texts generally contain terms or product names, and terminology translation is a challenging task in MT (Haque et al., 2019a, 2020a). In order to ad"
2020.icon-main.14,R19-1052,1,0.848162,"s of thousands. E-commerce companies and online retailers want to identify complaints given the reviews of a product for their own benefit. Likewise, customers who want to buy a product or service may need such information while avoiding having to consult thousands of reviews about the product. In this context, Gupta et al. (2014) identified the relationship between users’ purchase intent from their social media forums such as Quora1 and Yahoo! Answers,2 and Wang et al. (2015) investigated the problem of identifying purchase intent with using a list of seed intent-indicators (e.g. ‘want to’). Haque et al. (2019b) extend the work of Wang et al. (2015) while increasing the coverage of the purchase intent indicators with the distributed vector representation of words using the continuous skip-gram model (Mikolov et al., 2013). Recently, Preotiuc-Pietro et al. (2019) automatically identified complaints from tweets posted by social media users and potential customers. In Singh et al. (2020), we conducted a similar study in an attempt to identify complaints from opinionated texts (reviews) about products posted in a low-resource language, Hindi, from the the websites of the retail giant Amazon India3 and"
2020.icon-main.14,2020.ngt-1.17,1,0.650723,") our MT systems and (ii) human translators. Note that as a part of our investigation, we created a labeled training dataset of English reviews about products posted in Amazon, and detail the data creation process and statistics in Section 2.2. Unlike Tebbifakhr et al. (2019) who focus on improving a downstream task (i.e. sentiment classification) by controlling translations of an MT system but at the expense of translation quality, we customise our neural MT systems using the standard and commonly-used data augmentation and terminology-aware domain adaptation techniques (Jooste et al., 2020; Haque et al., 2020c; Nayak et al., 2020b; Parthasarathy et al., 2020) so that the translations produced by the MT systems can retain source-side stylistics property and semantics as much as possible. In other words, in this study, we aim to observe the performance of the English classifiers (complaint identifiers) on the translations of the Hindi reviews by the baseline, adapted/customised neural MT systems, and human translators. The remainder of the paper is organised as follows. In Section 2, we detail how we created training data for our experiments. Section 3 describes our MT system building and setups. In"
2020.icon-main.14,2020.icon-adapmt.4,1,0.319561,") our MT systems and (ii) human translators. Note that as a part of our investigation, we created a labeled training dataset of English reviews about products posted in Amazon, and detail the data creation process and statistics in Section 2.2. Unlike Tebbifakhr et al. (2019) who focus on improving a downstream task (i.e. sentiment classification) by controlling translations of an MT system but at the expense of translation quality, we customise our neural MT systems using the standard and commonly-used data augmentation and terminology-aware domain adaptation techniques (Jooste et al., 2020; Haque et al., 2020c; Nayak et al., 2020b; Parthasarathy et al., 2020) so that the translations produced by the MT systems can retain source-side stylistics property and semantics as much as possible. In other words, in this study, we aim to observe the performance of the English classifiers (complaint identifiers) on the translations of the Hindi reviews by the baseline, adapted/customised neural MT systems, and human translators. The remainder of the paper is organised as follows. In Section 2, we detail how we created training data for our experiments. Section 3 describes our MT system building and setups. In"
2020.icon-main.14,W14-4806,1,0.532175,"Missing"
2020.icon-main.14,2020.wat-1.17,1,0.31832,"e Hindi reviews by (i) our MT systems and (ii) human translators. Note that as a part of our investigation, we created a labeled training dataset of English reviews about products posted in Amazon, and detail the data creation process and statistics in Section 2.2. Unlike Tebbifakhr et al. (2019) who focus on improving a downstream task (i.e. sentiment classification) by controlling translations of an MT system but at the expense of translation quality, we customise our neural MT systems using the standard and commonly-used data augmentation and terminology-aware domain adaptation techniques (Jooste et al., 2020; Haque et al., 2020c; Nayak et al., 2020b; Parthasarathy et al., 2020) so that the translations produced by the MT systems can retain source-side stylistics property and semantics as much as possible. In other words, in this study, we aim to observe the performance of the English classifiers (complaint identifiers) on the translations of the Hindi reviews by the baseline, adapted/customised neural MT systems, and human translators. The remainder of the paper is organised as follows. In Section 2, we detail how we created training data for our experiments. Section 3 describes our MT system bui"
2020.icon-main.14,E17-2068,0,0.136169,"Missing"
2020.icon-main.14,W04-3250,0,0.079444,"4 presents the corpus statistics. As above (cf. Section 2.1), for our development set we used 385 reviews from Amazon India and YouTube, which were then manually translated into English (cf. last row of Table 4). sentences words (EN) words (HI) Train 1,102,511 22.4M 23.4M Monolingual English 6.86M 121.3M – Hindi 7.82M – 142.9M Dev. set 385 6,952 7,209 Table 4: The Corpus statistics. We present the performance of our MT systems in terms of the automatic evaluation metric BLEU (Papineni et al., 2002). Additionally, we performed statistical significance tests using bootstrap resampling methods (Koehn, 2004). We obtained the BLEU scores of our MT systems on the test set, and the scores are reported in Table 5. The first row of Table 5 represents our baseline Hindi-to-English MT system. The English-to-Hindi MT system which has been used to translate the English monolingual sentences (reviews) into Hindi produced 20.52 BLEU points on the development set. The BLEU scores of the MT systems (Base+BT and Base+BT+FT) trained on training data that consists of both authentic and (target- or/and source-original) synthetic parallel data are shown in the next two rows of Table 5. As in Caswell et al. (2019),"
2020.icon-main.14,2020.ngt-1.28,0,0.308876,"elated) product categories. As for 9 3 The Hindi-to-English MT Systems Our MT systems are Transformer models (Vaswani et al., 2017) which were trained using the Marian-NMT toolkit.10 The tokens of the training, evaluation and validation sets are segmented into sub-word units using BytePair Encoding (BPE) (Sennrich et al., 2016), and BPE is applied individually on the source and target languages. From our experiences (Jooste et al., 2020; Haque et al., 2020b,c; Nayak et al., 2020b,a; Parthasarathy et al., 2020) in the participation in the recent shared translation tasks (Barrault et al., 2020; Mayhew et al., 2020; Nakazawa et al., 2020) involving low-resource language pairs and domains, we found that the following configuration usually leads to the best results in the low-resource translation settings: (i) the BPE vocabulary size: 6,000, (ii) the sizes of the encoder and decoder layers: 4 and 6, respectively, and (iii) learning-rate: 0.0003. As for the remaining hyperparameters, we followed the recommended best setup from Vaswani et al. (2017). The early stopping criterion is based on cross-entropy; however, the final NMT system is selected as per the highest BLEU score on the validation set. The beam"
2020.icon-main.14,2020.wat-1.12,1,0.332463,"(ii) human translators. Note that as a part of our investigation, we created a labeled training dataset of English reviews about products posted in Amazon, and detail the data creation process and statistics in Section 2.2. Unlike Tebbifakhr et al. (2019) who focus on improving a downstream task (i.e. sentiment classification) by controlling translations of an MT system but at the expense of translation quality, we customise our neural MT systems using the standard and commonly-used data augmentation and terminology-aware domain adaptation techniques (Jooste et al., 2020; Haque et al., 2020c; Nayak et al., 2020b; Parthasarathy et al., 2020) so that the translations produced by the MT systems can retain source-side stylistics property and semantics as much as possible. In other words, in this study, we aim to observe the performance of the English classifiers (complaint identifiers) on the translations of the Hindi reviews by the baseline, adapted/customised neural MT systems, and human translators. The remainder of the paper is organised as follows. In Section 2, we detail how we created training data for our experiments. Section 3 describes our MT system building and setups. In Section 4, we presen"
2020.icon-main.14,2020.wmt-1.91,1,0.808165,"(ii) human translators. Note that as a part of our investigation, we created a labeled training dataset of English reviews about products posted in Amazon, and detail the data creation process and statistics in Section 2.2. Unlike Tebbifakhr et al. (2019) who focus on improving a downstream task (i.e. sentiment classification) by controlling translations of an MT system but at the expense of translation quality, we customise our neural MT systems using the standard and commonly-used data augmentation and terminology-aware domain adaptation techniques (Jooste et al., 2020; Haque et al., 2020c; Nayak et al., 2020b; Parthasarathy et al., 2020) so that the translations produced by the MT systems can retain source-side stylistics property and semantics as much as possible. In other words, in this study, we aim to observe the performance of the English classifiers (complaint identifiers) on the translations of the Hindi reviews by the baseline, adapted/customised neural MT systems, and human translators. The remainder of the paper is organised as follows. In Section 2, we detail how we created training data for our experiments. Section 3 describes our MT system building and setups. In Section 4, we presen"
2020.icon-main.14,P02-1040,0,0.106443,"he AI4Bharat-IndicNLP Corpus (Kunchukuttan et al., 2020) and Amazon review dump (cf. Section 2.2), respectively. Table 4 presents the corpus statistics. As above (cf. Section 2.1), for our development set we used 385 reviews from Amazon India and YouTube, which were then manually translated into English (cf. last row of Table 4). sentences words (EN) words (HI) Train 1,102,511 22.4M 23.4M Monolingual English 6.86M 121.3M – Hindi 7.82M – 142.9M Dev. set 385 6,952 7,209 Table 4: The Corpus statistics. We present the performance of our MT systems in terms of the automatic evaluation metric BLEU (Papineni et al., 2002). Additionally, we performed statistical significance tests using bootstrap resampling methods (Koehn, 2004). We obtained the BLEU scores of our MT systems on the test set, and the scores are reported in Table 5. The first row of Table 5 represents our baseline Hindi-to-English MT system. The English-to-Hindi MT system which has been used to translate the English monolingual sentences (reviews) into Hindi produced 20.52 BLEU points on the development set. The BLEU scores of the MT systems (Base+BT and Base+BT+FT) trained on training data that consists of both authentic and (target- or/and sour"
2020.icon-main.14,2020.wmt-1.27,1,0.680299,"s. Note that as a part of our investigation, we created a labeled training dataset of English reviews about products posted in Amazon, and detail the data creation process and statistics in Section 2.2. Unlike Tebbifakhr et al. (2019) who focus on improving a downstream task (i.e. sentiment classification) by controlling translations of an MT system but at the expense of translation quality, we customise our neural MT systems using the standard and commonly-used data augmentation and terminology-aware domain adaptation techniques (Jooste et al., 2020; Haque et al., 2020c; Nayak et al., 2020b; Parthasarathy et al., 2020) so that the translations produced by the MT systems can retain source-side stylistics property and semantics as much as possible. In other words, in this study, we aim to observe the performance of the English classifiers (complaint identifiers) on the translations of the Hindi reviews by the baseline, adapted/customised neural MT systems, and human translators. The remainder of the paper is organised as follows. In Section 2, we detail how we created training data for our experiments. Section 3 describes our MT system building and setups. In Section 4, we present our experimental methodology"
2020.icon-main.14,P19-1495,0,0.0879887,"o consult thousands of reviews about the product. In this context, Gupta et al. (2014) identified the relationship between users’ purchase intent from their social media forums such as Quora1 and Yahoo! Answers,2 and Wang et al. (2015) investigated the problem of identifying purchase intent with using a list of seed intent-indicators (e.g. ‘want to’). Haque et al. (2019b) extend the work of Wang et al. (2015) while increasing the coverage of the purchase intent indicators with the distributed vector representation of words using the continuous skip-gram model (Mikolov et al., 2013). Recently, Preotiuc-Pietro et al. (2019) automatically identified complaints from tweets posted by social media users and potential customers. In Singh et al. (2020), we conducted a similar study in an attempt to identify complaints from opinionated texts (reviews) about products posted in a low-resource language, Hindi, from the the websites of the retail giant Amazon India3 and the popular social media platform YouTube.4 For investigating this problem in Hindi (Singh et al., 2020), as in Gupta et al. (2014); Wang et al. (2015); Haque et al. (2019b); Preotiuc-Pietro et al. (2019), we had to manually create labeled training data5 1"
2020.icon-main.14,P16-1162,0,0.0295371,"different categories, namely Books, Cell_Phones_and_Accessories, Electronics, and Movies_and_TV. The Hindi reviews which we collected from the websites of Amazon India and YouTube were mainly on books and electronic goods. This is the reason why we considered English reviews on those four (related) product categories. As for 9 3 The Hindi-to-English MT Systems Our MT systems are Transformer models (Vaswani et al., 2017) which were trained using the Marian-NMT toolkit.10 The tokens of the training, evaluation and validation sets are segmented into sub-word units using BytePair Encoding (BPE) (Sennrich et al., 2016), and BPE is applied individually on the source and target languages. From our experiences (Jooste et al., 2020; Haque et al., 2020b,c; Nayak et al., 2020b,a; Parthasarathy et al., 2020) in the participation in the recent shared translation tasks (Barrault et al., 2020; Mayhew et al., 2020; Nakazawa et al., 2020) involving low-resource language pairs and domains, we found that the following configuration usually leads to the best results in the low-resource translation settings: (i) the BPE vocabulary size: 6,000, (ii) the sizes of the encoder and decoder layers: 4 and 6, respectively, and (ii"
2020.icon-main.14,2020.icon-main.14,1,0.0530913,"Missing"
2020.icon-main.14,D19-1140,0,0.121251,". (2015); Haque et al. (2019b); Preotiuc-Pietro et al. (2019), we had to manually create labeled training data5 1 www.quora.com www.answers.yahoo.com 3 https://www.amazon.in/ 4 https://www.YouTube.com/ 5 https://github.com/MrRaghav/ 2 108 Proceedings of the 17th International Conference on Natural Language Processing, pages 108–116 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) by employing a number of human annotators. The process of creating such a data set is not easy; it is not only time-consuming and laborious but also a very expensive task. In this context, Tebbifakhr et al. (2019) investigated possibility of exploiting MT in a specific NLP task in a language for which dedicated tools are not available due to the scarcity of task-specific training data. As in Tebbifakhr et al. (2019), in this work, we considered Hindi, an under-resourced Indic language, and investigate whether MT can play a role in complaint identification and eliminate the requirement for complaint identification tools for Hindi, which require labeled data for training, which is expensive to create. Accordingly, we study the following two scenarios while considering reviews about a variety of products"
2020.icon-main.14,tiedemann-2012-parallel,0,0.0779515,"rom Vaswani et al. (2017). The early stopping criterion is based on cross-entropy; however, the final NMT system is selected as per the highest BLEU score on the validation set. The beam size for search is set to 6. We make our final NMT model with ensembles of 8 models that are sampled from the training run. For building our baseline models (forward 10 https://jmcauley.ucsd.edu/data/amazon/ 110 https://github.com/marian-nmt/marian and backward), we used the IIT Bombay English-Hindi parallel corpus11 (Kunchukuttan et al., 2017) that is compiled from a variety of existing sources, e.g. OPUS12 (Tiedemann, 2012). After applying standard cleaning procedures including applying a language identifier13 we are left with just over 1.1 million parallel sentence pairs. As for Hindi and English monolingual sentences for forward-translation and back-translation, respectively, we sampled them from the AI4Bharat-IndicNLP Corpus (Kunchukuttan et al., 2020) and Amazon review dump (cf. Section 2.2), respectively. Table 4 presents the corpus statistics. As above (cf. Section 2.1), for our development set we used 385 reviews from Amazon India and YouTube, which were then manually translated into English (cf. last row"
2020.icon-main.14,D16-1058,0,0.199563,"Missing"
2020.icon-main.14,C16-1329,0,0.06918,"ssed to the softmax layer after linearising it into a vector whose length is equal to the number of class labels. In our work, the set of class labels includes complaint and non-complaint categories. 4 4.2 Classical Supervised Classification Models The Complaint Identification Models 4.1 LSTM Network Nowadays, recurrent neural networks (RNN), in particular long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) hidden units, have proven to be an effective model for many classification tasks in NLP, e.g. sentiment analysis (Wang et al., 2016), text classification (Joulin et al., 2016; Zhou et al., 2016). RNN is an extension of the feed-forward neural network (NN), which has the gradient vanishing or exploding problems. LSTM deals with the gradient vanishing and exploding problems of RNN. An RNN composed of LSTM hidden units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. More formally, each cell in LSTM can be computed as follows: [ ] ht−1 X= (1) xt ft = σ(Wf · X + bf ) (2) it = σ(Wi · X + bi ) (3) ot = σ(Wo · X + bo ) (4) ct = ft ⊙ ct−1 + it ⊙ tanh (Wc · X + bc ) (5) ht = ot ⊙ tanh (ct ) (6) where Wi , Wf , Wo ∈ Rd×"
2020.iwltp-1.6,W16-3412,1,0.809538,"The contents of the input files in different formats are first extracted, followed by automated language identification which allows the different text files to be grouped by language.14 Within each file, the text is then split into separate sentences, to allow further processes to apply. Each sentence is then pre-processed, which mainly includes tokenisation and truecasing; these operations are performed with scripts that are part of the Moses toolkit15 (Koehn et al., 2007). All document pairs with content in different languages are then automatically aligned with the DOCAL document aligner (Etchegoyhen and Azpeitia, 2016). For all document pairs whose alignment score indicates that the documents are a translation of each other, sentence alignment is then performed on the content, retrieving translations at the sentence level.16 From the aligned sentences a translation memory in TMX format 1.4b is then gener• NationalOrganisations: This group includes all registered users of the NRS from a specific country and resources shared with this group are accessible to all registered users of the NRS based in that Member State. • NationalOrganisations+EuropeanCommission: This group includes all registered users of the N"
2020.iwltp-1.6,P07-2045,0,0.00526572,"le documents containing translations in two or more languages. This is the most complex scenario and its main steps are summarised below.13 The contents of the input files in different formats are first extracted, followed by automated language identification which allows the different text files to be grouped by language.14 Within each file, the text is then split into separate sentences, to allow further processes to apply. Each sentence is then pre-processed, which mainly includes tokenisation and truecasing; these operations are performed with scripts that are part of the Moses toolkit15 (Koehn et al., 2007). All document pairs with content in different languages are then automatically aligned with the DOCAL document aligner (Etchegoyhen and Azpeitia, 2016). For all document pairs whose alignment score indicates that the documents are a translation of each other, sentence alignment is then performed on the content, retrieving translations at the sentence level.16 From the aligned sentences a translation memory in TMX format 1.4b is then gener• NationalOrganisations: This group includes all registered users of the NRS from a specific country and resources shared with this group are accessible to a"
2020.iwltp-1.6,L18-1213,1,0.837898,"Missing"
2020.loresmt-1.14,W17-4714,0,0.0141529,"esearch is inspired by techniques for augmenting the training set artificially. One of these techniques is back-translation (Sennrich et al., 2016a), which involves creating artificial source-side sen108 Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, pages 108–117 c Decmber 04, 2020. 2020 Association for Computational Linguistics tences by translating a monolingual set in the target language. Similar techniques include the use of several models to generate sentences (Poncelas et al., 2019b; Soto et al., 2020), or the use of synthetic data on the target side (Chinea-Rios et al., 2017; Li et al., 2020). A technique that involves multiple segmentation is subword regularization (Kudo, 2018), in which candidate sentences with different splits are sampled, either probabilistically or using a language model for training. In the work of Poncelas et al. (2020b), different splits are used to build an English-Thai MT model. As the Thai language does not use whitespace separation between words, different splits can be applied, to address the fact that all the words and sub-words are joined together in the final output. More recently, Provilkov et al. (2020) introduced BPE-dropout, a"
2020.loresmt-1.14,2012.amta-papers.7,0,0.020856,"ough the quality of these sentence pairs is not as high as that of human-translated sentences (the source side contains mistakes produced by the MT system), the pairs are still useful when used as training data, because they do often improve the models (Poncelas et al., 2019a). Nonetheless, for some languages, the available data are in such short supply that MT models used for generating back-translated sentences may produce a high proportion of noisy sentences. The use of noisy sentences for building MT models could ultimately have a negative impact on the quality of the MT system’s outputs (Goutte et al., 2012), and therefore they are often removed (Khadivi and Ney, 2005; Taghipour et al., 2010; Popovi´c and Poncelas, 2020). We propose employing another technique to augment datasets: using the same set of sentences multiple times, but in slightly altered form each time. Specifically, we modify the sentences by using different Byte Pair Encoding (BPE) (Sennrich et al., 2016b) merge operations. We perform a fine-grained analysis, exploring the use of different splitting options on the source side, on the target side, and on both sides. Building Machine Translation (MT) systems for low-resource languag"
2020.loresmt-1.14,P17-4012,0,0.0124375,"of dataset with different merge operations on the target side (Section 7.1). • Combination of dataset with different merge operations on the source side (Section 7.2). • Combination of dataset with different merge operations on both the source and target side (Section 7.3). In Section 8, we compare translation examples from the different models and analyze the different outcomes. Finally, in Section 9 we conclude and propose how these experiments could be expanded in future work. 5 Experimental Settings The NMT systems we build are Transformer (Vaswani et al., 2017) models, based on OpenNMT (Klein et al., 2017). Models are trained for a maximum of 30K steps using the recommended parameters.1 We have selected the model with the lowest perplexity on the development set. 5.1 Dataset For training the models we use the Tatoeba, GlobalVoices and bible-uedin (Christodouloupoulos and Steedman, 2015) datasets from OPUS project.2 Our dataset thus contains material from the Bible, from news sources, and from less domain-specific multilingual translation examples. The sentences are randomly shuffled, after which 302,768 sentences are used as a training set and the other 1,000 as our dev set. All the sentences a"
2020.loresmt-1.14,W04-3250,0,0.21971,"ained with a different concatenation of datasets. The first column specifies the datasets used in the training. For example, the row TRG89500 & TRG50000 indicates that the training set used for building the MT model consists of sentences split using 89,500 and 50,000 merge operations, respectively We mark in bold those scores that exceed 6.89 BLEU points, i.e. the maximum score achieved by the baseline models presented in Table 1. The scores receive an asterisk when the improvements are statistically significant at p=0.01. Statistical significance has been computed using Bootstrap Resampling (Koehn, 2004). In the table, we find that scores tend to be higher 111 keep duplicates remove duplicates Traindata TRG89500 & TRG50000 TRG89500 & TRG20000 TRG89500 & TRG10000 TRG50000 & TRG20000 TRG50000 & TRG10000 TRG20000 & TRG10000 TRG89500 & TRG50000 & TRG20000 TRG89500 & TRG50000 & TRG10000 TRG89500 & TRG20000 & TRG10000 TRG50000 & TRG20000 & TRG10000 TRG89500 & TRG5000 & TRG20000 & TRG10000 TRG89500 & TRG50000 TRG89500 & TRG20000 TRG89500 & TRG10000 TRG50000 & TRG20000 TRG50000 & TRG10000 TRG20000 & TRG10000 TRG89500 & TRG50000 & TRG20000 TRG89500 & TRG50000 & TRG10000 TRG89500 & TRG20000 & TRG10000"
2020.loresmt-1.14,P18-1007,0,0.0118331,"tion (Sennrich et al., 2016a), which involves creating artificial source-side sen108 Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, pages 108–117 c Decmber 04, 2020. 2020 Association for Computational Linguistics tences by translating a monolingual set in the target language. Similar techniques include the use of several models to generate sentences (Poncelas et al., 2019b; Soto et al., 2020), or the use of synthetic data on the target side (Chinea-Rios et al., 2017; Li et al., 2020). A technique that involves multiple segmentation is subword regularization (Kudo, 2018), in which candidate sentences with different splits are sampled, either probabilistically or using a language model for training. In the work of Poncelas et al. (2020b), different splits are used to build an English-Thai MT model. As the Thai language does not use whitespace separation between words, different splits can be applied, to address the fact that all the words and sub-words are joined together in the final output. More recently, Provilkov et al. (2020) introduced BPE-dropout, an improvement on standard BPE consisting of randomly dropping merges when training the model, such that a"
2020.loresmt-1.14,P02-1040,0,0.110959,",500 operations as a starting point and explore other splits that produce smaller subword units (by using a lower number of merge operations). In our experiments we work with 50,000, 20,000 and 10,000 operations. We also concatenate the dev set using the same configuration of BPE. Test Set In order to evaluate the quality of the models, two test sets are translated. The test sets are the same for all models. In addition to tokenization and truecase, we also use BPE with 89,500 merge operations. We do not use (or combine) other BPE configurations. The translations are evaluated using the BLEU (Papineni et al., 2002) metric. The first test set is taken from the OPUS (Books) dataset (Tiedemann, 2012) (1562 sentences). Specifically, the test set consists of material from two texts available in English and in Esperanto translation, namely Carroll’s Alice’s Adventures in Wonderland (Carroll and Kearney, 1865 (1910) and Poe’s The Fall of the House of Usher (Poe and Grobe, 1839 (2000).3 The second test set (which contains 1256 sentences) consists of an English and an Esperanto version of Oscar Wilde’s Salom´e (Wilde et al., 1891 (1894, 1910), 4 a play originally written in French. As an additional contribution"
2020.loresmt-1.14,2020.amta-research.7,1,0.722992,"improvements in the automatic translation of material from low-resource languages. Languages are considered low-resource when there is little textual material available in the form of electronically stored corpora. They pose significant challenges in the field of Machine Translation (MT), since it is difficult to build models that perform adequately using small amounts of data. Multiple techniques have been developed to improve MT in conditions of data scarcity. A popular approach is to translate indirectly via a pivot language (Utiyama and Isahara, 2007; Firat et al., 2017; Liu et al., 2018; Poncelas et al., 2020a). Moreover, indirect translation can be used 2 Previous work This research is inspired by techniques for augmenting the training set artificially. One of these techniques is back-translation (Sennrich et al., 2016a), which involves creating artificial source-side sen108 Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, pages 108–117 c Decmber 04, 2020. 2020 Association for Computational Linguistics tences by translating a monolingual set in the target language. Similar techniques include the use of several models to generate sentences (Poncelas et al., 2019b;"
2020.loresmt-1.14,2020.sltu-1.33,1,0.728526,"improvements in the automatic translation of material from low-resource languages. Languages are considered low-resource when there is little textual material available in the form of electronically stored corpora. They pose significant challenges in the field of Machine Translation (MT), since it is difficult to build models that perform adequately using small amounts of data. Multiple techniques have been developed to improve MT in conditions of data scarcity. A popular approach is to translate indirectly via a pivot language (Utiyama and Isahara, 2007; Firat et al., 2017; Liu et al., 2018; Poncelas et al., 2020a). Moreover, indirect translation can be used 2 Previous work This research is inspired by techniques for augmenting the training set artificially. One of these techniques is back-translation (Sennrich et al., 2016a), which involves creating artificial source-side sen108 Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, pages 108–117 c Decmber 04, 2020. 2020 Association for Computational Linguistics tences by translating a monolingual set in the target language. Similar techniques include the use of several models to generate sentences (Poncelas et al., 2019b;"
2020.loresmt-1.14,R19-1107,1,0.911398,"urther useful technique for expanding the dataset is backtranslation (Sennrich et al., 2016a). This procedure consists of automatically translating a monolingual text from the target language into the selected source language, and then using the resulting parallel set as training data so the model benefits from this additional information. Although the quality of these sentence pairs is not as high as that of human-translated sentences (the source side contains mistakes produced by the MT system), the pairs are still useful when used as training data, because they do often improve the models (Poncelas et al., 2019a). Nonetheless, for some languages, the available data are in such short supply that MT models used for generating back-translated sentences may produce a high proportion of noisy sentences. The use of noisy sentences for building MT models could ultimately have a negative impact on the quality of the MT system’s outputs (Goutte et al., 2012), and therefore they are often removed (Khadivi and Ney, 2005; Taghipour et al., 2010; Popovi´c and Poncelas, 2020). We propose employing another technique to augment datasets: using the same set of sentences multiple times, but in slightly altered form e"
2020.loresmt-1.14,2020.acl-main.170,0,0.0127637,"data on the target side (Chinea-Rios et al., 2017; Li et al., 2020). A technique that involves multiple segmentation is subword regularization (Kudo, 2018), in which candidate sentences with different splits are sampled, either probabilistically or using a language model for training. In the work of Poncelas et al. (2020b), different splits are used to build an English-Thai MT model. As the Thai language does not use whitespace separation between words, different splits can be applied, to address the fact that all the words and sub-words are joined together in the final output. More recently, Provilkov et al. (2020) introduced BPE-dropout, an improvement on standard BPE consisting of randomly dropping merges when training the model, such that a single word can have several segmentations. 3 The Esperanto language This article is concerned with improving MT models for Esperanto, the most successful constructed international language (Blanke, 2009). It was created in the late nineteenth century, and is said to be currently spoken by over 2 million people, spread across more than 100 countries (Eberhard et al., 2020). During its first century of development, Esperanto was principally maintained by means of m"
2020.loresmt-1.14,P16-1009,0,0.511149,"ls used for generating back-translated sentences may produce a high proportion of noisy sentences. The use of noisy sentences for building MT models could ultimately have a negative impact on the quality of the MT system’s outputs (Goutte et al., 2012), and therefore they are often removed (Khadivi and Ney, 2005; Taghipour et al., 2010; Popovi´c and Poncelas, 2020). We propose employing another technique to augment datasets: using the same set of sentences multiple times, but in slightly altered form each time. Specifically, we modify the sentences by using different Byte Pair Encoding (BPE) (Sennrich et al., 2016b) merge operations. We perform a fine-grained analysis, exploring the use of different splitting options on the source side, on the target side, and on both sides. Building Machine Translation (MT) systems for low-resource languages remains challenging. For many language pairs, parallel data are not widely available, and in such cases MT models do not achieve results comparable to those seen with high-resource languages. When data are scarce, it is of paramount importance to make optimal use of the limited material available. To that end, in this paper we propose employing the same parallel s"
2020.loresmt-1.14,P16-1162,0,0.640487,"ls used for generating back-translated sentences may produce a high proportion of noisy sentences. The use of noisy sentences for building MT models could ultimately have a negative impact on the quality of the MT system’s outputs (Goutte et al., 2012), and therefore they are often removed (Khadivi and Ney, 2005; Taghipour et al., 2010; Popovi´c and Poncelas, 2020). We propose employing another technique to augment datasets: using the same set of sentences multiple times, but in slightly altered form each time. Specifically, we modify the sentences by using different Byte Pair Encoding (BPE) (Sennrich et al., 2016b) merge operations. We perform a fine-grained analysis, exploring the use of different splitting options on the source side, on the target side, and on both sides. Building Machine Translation (MT) systems for low-resource languages remains challenging. For many language pairs, parallel data are not widely available, and in such cases MT models do not achieve results comparable to those seen with high-resource languages. When data are scarce, it is of paramount importance to make optimal use of the limited material available. To that end, in this paper we propose employing the same parallel s"
2020.loresmt-1.14,2020.acl-main.359,1,0.833494,"a). Moreover, indirect translation can be used 2 Previous work This research is inspired by techniques for augmenting the training set artificially. One of these techniques is back-translation (Sennrich et al., 2016a), which involves creating artificial source-side sen108 Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, pages 108–117 c Decmber 04, 2020. 2020 Association for Computational Linguistics tences by translating a monolingual set in the target language. Similar techniques include the use of several models to generate sentences (Poncelas et al., 2019b; Soto et al., 2020), or the use of synthetic data on the target side (Chinea-Rios et al., 2017; Li et al., 2020). A technique that involves multiple segmentation is subword regularization (Kudo, 2018), in which candidate sentences with different splits are sampled, either probabilistically or using a language model for training. In the work of Poncelas et al. (2020b), different splits are used to build an English-Thai MT model. As the Thai language does not use whitespace separation between words, different splits can be applied, to address the fact that all the words and sub-words are joined together in the fin"
2020.loresmt-1.14,tiedemann-2012-parallel,0,0.0452204,"units (by using a lower number of merge operations). In our experiments we work with 50,000, 20,000 and 10,000 operations. We also concatenate the dev set using the same configuration of BPE. Test Set In order to evaluate the quality of the models, two test sets are translated. The test sets are the same for all models. In addition to tokenization and truecase, we also use BPE with 89,500 merge operations. We do not use (or combine) other BPE configurations. The translations are evaluated using the BLEU (Papineni et al., 2002) metric. The first test set is taken from the OPUS (Books) dataset (Tiedemann, 2012) (1562 sentences). Specifically, the test set consists of material from two texts available in English and in Esperanto translation, namely Carroll’s Alice’s Adventures in Wonderland (Carroll and Kearney, 1865 (1910) and Poe’s The Fall of the House of Usher (Poe and Grobe, 1839 (2000).3 The second test set (which contains 1256 sentences) consists of an English and an Esperanto version of Oscar Wilde’s Salom´e (Wilde et al., 1891 (1894, 1910), 4 a play originally written in French. As an additional contribution to this paper, we have made a set of aligned sentences from the texts available via"
2020.loresmt-1.14,N07-1061,0,0.0426624,"e use the constructed language Esperanto to illustrate potential improvements in the automatic translation of material from low-resource languages. Languages are considered low-resource when there is little textual material available in the form of electronically stored corpora. They pose significant challenges in the field of Machine Translation (MT), since it is difficult to build models that perform adequately using small amounts of data. Multiple techniques have been developed to improve MT in conditions of data scarcity. A popular approach is to translate indirectly via a pivot language (Utiyama and Isahara, 2007; Firat et al., 2017; Liu et al., 2018; Poncelas et al., 2020a). Moreover, indirect translation can be used 2 Previous work This research is inspired by techniques for augmenting the training set artificially. One of these techniques is back-translation (Sennrich et al., 2016a), which involves creating artificial source-side sen108 Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages, pages 108–117 c Decmber 04, 2020. 2020 Association for Computational Linguistics tences by translating a monolingual set in the target language. Similar techniques include the use of s"
2020.loresmt-1.15,N12-1047,0,0.078534,"Moses toolkit (Koehn et al., 2007). We used a 5 gram language model trained with modified Kneser Ney smoothing (Kneser and Ney, 1995) using the KenLM toolkit (Heafield et al., 2013). Our PB SMT loglinear features include: (a) 4 translational features (forward and backward phrase and lexi cal probabilities), (b) 8 lexicalised reordering proba bilities (wbemslrbidirectionalfeallff ), (c) 5gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) wordcount and distortion penalties. The weights of the parameters are optimized using the margininfused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decod ing, the cubepruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the Open NMT toolkit (Klein et al., 2017). The NMT systems are Transformer models (Vaswani et al., 2017). The tokens of the training, evaluation and validation sets are segmented into subword units using BytePair Encoding (BPE) (Sennrich et al., 2016b). Recently, Sennrich and Zhang (2019) demonstrated that com monly used hyperparameter configurations do not provide the best results in lowresource settings. Ac cordingly,"
2020.loresmt-1.15,W17-4715,0,0.0620458,"l., 2018), exploit ing training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multitask learning (Niehues and Cho, 2017), selection of hyperparam eters (Sennrich and Zhang, 2019), and pretrained language model finetuning (Liu et al., 2020). De spite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned. For examples, the backtranslation strategy of Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søgaard et al., 2018) and the same is ap plicable in the case of transfer learning too (Mon toya et al., 2019). Hence, this line of research needs more attention from the MT research commu nity. In this context, we refer interested readers to some of the papers (Bentivogli et al., 2016; Castilho et al., 2017) that compared PBSMT and NMT on a variety of usecases. As for lowresource scenar ios, as mention"
2020.loresmt-1.15,W18-2202,1,0.849884,"due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søgaard et al., 2018) and the same is ap plicable in the case of transfer learning too (Mon toya et al., 2019). Hence, this line of research needs more attention from the MT research commu nity. In this context, we refer interested readers to some of the papers (Bentivogli et al., 2016; Castilho et al., 2017) that compared PBSMT and NMT on a variety of usecases. As for lowresource scenar ios, as mentioned above, many studies (e.g. Koehn and Knowles (2017); Östling and Tiedemann (2017); Dowling et al. (2018)) found that PBSMT can pro vide better translations than NMT, and many found the opposite results (Casas et al., 2019; Sen et al., 2019; Sennrich and Zhang, 2019). Hence, the find ings of this line of MT research have indeed yielded a mixed bag of results, leaving the way ahead unclear. In Ramesh et al. (2020), we investigated the performance of PBSMT and NMT systems on two rarelytested underresourced languagepairs, EnglishtoTamil and HinditoTamil, taking a spe cialised data domain (software localisation) into ac count. In particular, in Ramesh et al. (2020), we carried out a comp"
2020.loresmt-1.15,P11-1105,0,0.195806,"al Setups The MT systems This section provides an overview of the PBSMT and NMT systems used for experimentation.1 To build our PBSMT systems we used the Moses toolkit (Koehn et al., 2007). We used a 5 gram language model trained with modified Kneser Ney smoothing (Kneser and Ney, 1995) using the KenLM toolkit (Heafield et al., 2013). Our PB SMT loglinear features include: (a) 4 translational features (forward and backward phrase and lexi cal probabilities), (b) 8 lexicalised reordering proba bilities (wbemslrbidirectionalfeallff ), (c) 5gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) wordcount and distortion penalties. The weights of the parameters are optimized using the margininfused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decod ing, the cubepruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the Open NMT toolkit (Klein et al., 2017). The NMT systems are Transformer models (Vaswani et al., 2017). The tokens of the training, evaluation and validation sets are segmented into subword units using BytePair Encoding (BPE) (Sennrich et al., 2016b). Recently, Senn"
2020.loresmt-1.15,D18-1399,0,0.137454,"Missing"
2020.loresmt-1.15,D16-1025,0,0.0385995,"Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søgaard et al., 2018) and the same is ap plicable in the case of transfer learning too (Mon toya et al., 2019). Hence, this line of research needs more attention from the MT research commu nity. In this context, we refer interested readers to some of the papers (Bentivogli et al., 2016; Castilho et al., 2017) that compared PBSMT and NMT on a variety of usecases. As for lowresource scenar ios, as mentioned above, many studies (e.g. Koehn and Knowles (2017); Östling and Tiedemann (2017); Dowling et al. (2018)) found that PBSMT can pro vide better translations than NMT, and many found the opposite results (Casas et al., 2019; Sen et al., 2019; Sennrich and Zhang, 2019). Hence, the find ings of this line of MT research have indeed yielded a mixed bag of results, leaving the way ahead unclear. In Ramesh et al. (2020), we investigated the performance of PBSMT and NMT syst"
2020.loresmt-1.15,P13-2121,0,0.0214119,"sentropy, perplexity and BLEU (Papineni et al., 2002). The early stop ping criteria is based on crossentropy; however, the final NMT system is selected as per highest BLEU score on the validation set. The beam size for search is set to 12. 2.2 Choice of Languages 2 2.1 Experimental Setups The MT systems This section provides an overview of the PBSMT and NMT systems used for experimentation.1 To build our PBSMT systems we used the Moses toolkit (Koehn et al., 2007). We used a 5 gram language model trained with modified Kneser Ney smoothing (Kneser and Ney, 1995) using the KenLM toolkit (Heafield et al., 2013). Our PB SMT loglinear features include: (a) 4 translational features (forward and backward phrase and lexi cal probabilities), (b) 8 lexicalised reordering proba bilities (wbemslrbidirectionalfeallff ), (c) 5gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) wordcount and distortion penalties. The weights of the parameters are optimized using the margininfused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decod ing, the cubepruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT system"
2020.loresmt-1.15,P07-1019,0,0.129288,"eser Ney smoothing (Kneser and Ney, 1995) using the KenLM toolkit (Heafield et al., 2013). Our PB SMT loglinear features include: (a) 4 translational features (forward and backward phrase and lexi cal probabilities), (b) 8 lexicalised reordering proba bilities (wbemslrbidirectionalfeallff ), (c) 5gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) wordcount and distortion penalties. The weights of the parameters are optimized using the margininfused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decod ing, the cubepruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the Open NMT toolkit (Klein et al., 2017). The NMT systems are Transformer models (Vaswani et al., 2017). The tokens of the training, evaluation and validation sets are segmented into subword units using BytePair Encoding (BPE) (Sennrich et al., 2016b). Recently, Sennrich and Zhang (2019) demonstrated that com monly used hyperparameter configurations do not provide the best results in lowresource settings. Ac cordingly, we carried out a series of experiments in order to find the best hyperparameter configurati"
2020.loresmt-1.15,P17-4012,0,0.141295,"include: (a) 4 translational features (forward and backward phrase and lexi cal probabilities), (b) 8 lexicalised reordering proba bilities (wbemslrbidirectionalfeallff ), (c) 5gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) wordcount and distortion penalties. The weights of the parameters are optimized using the margininfused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decod ing, the cubepruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the Open NMT toolkit (Klein et al., 2017). The NMT systems are Transformer models (Vaswani et al., 2017). The tokens of the training, evaluation and validation sets are segmented into subword units using BytePair Encoding (BPE) (Sennrich et al., 2016b). Recently, Sennrich and Zhang (2019) demonstrated that com monly used hyperparameter configurations do not provide the best results in lowresource settings. Ac cordingly, we carried out a series of experiments in order to find the best hyperparameter configuration for Transformer in our lowresource settings. In par ticular, we found that the following configuration lead to the"
2020.loresmt-1.15,P07-2045,0,0.0130359,"Missing"
2020.loresmt-1.15,W17-3204,0,0.0393397,"rk well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søgaard et al., 2018) and the same is ap plicable in the case of transfer learning too (Mon toya et al., 2019). Hence, this line of research needs more attention from the MT research commu nity. In this context, we refer interested readers to some of the papers (Bentivogli et al., 2016; Castilho et al., 2017) that compared PBSMT and NMT on a variety of usecases. As for lowresource scenar ios, as mentioned above, many studies (e.g. Koehn and Knowles (2017); Östling and Tiedemann (2017); Dowling et al. (2018)) found that PBSMT can pro vide better translations than NMT, and many found the opposite results (Casas et al., 2019; Sen et al., 2019; Sennrich and Zhang, 2019). Hence, the find ings of this line of MT research have indeed yielded a mixed bag of results, leaving the way ahead unclear. In Ramesh et al. (2020), we investigated the performance of PBSMT and NMT systems on two rarelytested underresourced languagepairs, EnglishtoTamil and HinditoTamil, taking a spe cialised data domain (software localisation) into ac count. In parti"
2020.loresmt-1.15,D18-1549,0,0.0828708,"n. In particular, we pro duce rankings of our MT systems via a social media platformbased human evaluation scheme, and demonstrate our findings in the lowresource domainspecific text translation task. 1 Introduction In recent years, MT researchers have proposed ap proaches to counter the data sparsity problem and to improve the performance of NMT systems in low resource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sen nrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploit ing training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multitask learning (Niehues and Cho, 2017), selection of hyperparam eters (Sennrich and Zhang, 2019), and pretrained language model finetuning (Liu et al., 2020). De spite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned. For examples, the backtranslation strategy of Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et"
2020.loresmt-1.15,2020.tacl-1.47,0,0.053469,"p proaches to counter the data sparsity problem and to improve the performance of NMT systems in low resource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sen nrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploit ing training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multitask learning (Niehues and Cho, 2017), selection of hyperparam eters (Sennrich and Zhang, 2019), and pretrained language model finetuning (Liu et al., 2020). De spite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned. For examples, the backtranslation strategy of Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søgaard et al., 2018) and the same is ap plicable in the case of"
2020.loresmt-1.15,L18-1141,0,0.0330365,"Missing"
2020.loresmt-1.15,W17-4708,0,0.0968245,"dings in the lowresource domainspecific text translation task. 1 Introduction In recent years, MT researchers have proposed ap proaches to counter the data sparsity problem and to improve the performance of NMT systems in low resource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sen nrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploit ing training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multitask learning (Niehues and Cho, 2017), selection of hyperparam eters (Sennrich and Zhang, 2019), and pretrained language model finetuning (Liu et al., 2020). De spite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned. For examples, the backtranslation strategy of Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vi"
2020.loresmt-1.15,P02-1040,0,0.115082,"esource Languages, pages 118–125 c Decmber 04, 2020. 2020 Association for Computational Linguistics translated using our proposed social media platform based human evaluation scheme. The remainder of the paper is organized as follows. Section 2 explains the experimental setup including the descriptions on our MT systems and details of the data sets used. Section 3 presents the results with discussions and analysis, while Section 4 concludes our work with avenues for future work. The validation on the development set is performed using three cost functions: crossentropy, perplexity and BLEU (Papineni et al., 2002). The early stop ping criteria is based on crossentropy; however, the final NMT system is selected as per highest BLEU score on the validation set. The beam size for search is set to 12. 2.2 Choice of Languages 2 2.1 Experimental Setups The MT systems This section provides an overview of the PBSMT and NMT systems used for experimentation.1 To build our PBSMT systems we used the Moses toolkit (Koehn et al., 2007). We used a 5 gram language model trained with modified Kneser Ney smoothing (Kneser and Ney, 1995) using the KenLM toolkit (Heafield et al., 2013). Our PB SMT loglinear feature"
2020.loresmt-1.15,W15-3049,0,0.0823302,"the sentences of other lengths (number of words > 3) in the IT setup. Overall, the human evaluators ranked GT as the first choice, PBSMT as the second choice and NMT as the third choice MT systems in the MIXED setup. As for the IT setup, PBSMT was the first choice, NMT was the second choice and GT was the third choice MT systems. We believe that the findings of this work pro vide significant contributions to this line of MT re search. In future, we intend to consider more lan guages from different language families. We also plan to include stringbased MT evaluation metrics such as chrF (Popović, 2015) in our investigation, which have been shown to better reflect the actual performance improvement of NMT. Acknowledgments The ADAPT Centre for Digital Content Technol ogy is funded under the Science Foundation Ire land (SFI) Research Centres Programme (Grant No. 13/RC/2106) and is cofunded under the European Regional Development Fund. This project has par tially received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie SkłodowskaCurie grant agreement No. 713567, and the publication has emanated from research supported in part by a research g"
2020.loresmt-1.15,W19-5346,0,0.0303356,"plicable in the case of transfer learning too (Mon toya et al., 2019). Hence, this line of research needs more attention from the MT research commu nity. In this context, we refer interested readers to some of the papers (Bentivogli et al., 2016; Castilho et al., 2017) that compared PBSMT and NMT on a variety of usecases. As for lowresource scenar ios, as mentioned above, many studies (e.g. Koehn and Knowles (2017); Östling and Tiedemann (2017); Dowling et al. (2018)) found that PBSMT can pro vide better translations than NMT, and many found the opposite results (Casas et al., 2019; Sen et al., 2019; Sennrich and Zhang, 2019). Hence, the find ings of this line of MT research have indeed yielded a mixed bag of results, leaving the way ahead unclear. In Ramesh et al. (2020), we investigated the performance of PBSMT and NMT systems on two rarelytested underresourced languagepairs, EnglishtoTamil and HinditoTamil, taking a spe cialised data domain (software localisation) into ac count. In particular, in Ramesh et al. (2020), we carried out a comprehensive manual error analysis on the translations produced by our PBSMT and NMT systems. This current work extends the work of Ramesh"
2020.loresmt-1.15,P16-1009,0,0.325039,"2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploit ing training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multitask learning (Niehues and Cho, 2017), selection of hyperparam eters (Sennrich and Zhang, 2019), and pretrained language model finetuning (Liu et al., 2020). De spite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned. For examples, the backtranslation strategy of Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søgaard et al., 2018) and the same is ap plicable in the case of transfer learning too (Mon toya et al., 2019). Hence, this line of research needs more attention from the MT research commu nity. In this context, we refer interested readers to some of the papers (Bentivogli et al., 201"
2020.loresmt-1.15,P16-1162,0,0.492955,"2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploit ing training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multitask learning (Niehues and Cho, 2017), selection of hyperparam eters (Sennrich and Zhang, 2019), and pretrained language model finetuning (Liu et al., 2020). De spite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned. For examples, the backtranslation strategy of Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søgaard et al., 2018) and the same is ap plicable in the case of transfer learning too (Mon toya et al., 2019). Hence, this line of research needs more attention from the MT research commu nity. In this context, we refer interested readers to some of the papers (Bentivogli et al., 201"
2020.loresmt-1.15,P19-1021,0,0.112395,"on task. 1 Introduction In recent years, MT researchers have proposed ap proaches to counter the data sparsity problem and to improve the performance of NMT systems in low resource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sen nrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploit ing training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multitask learning (Niehues and Cho, 2017), selection of hyperparam eters (Sennrich and Zhang, 2019), and pretrained language model finetuning (Liu et al., 2020). De spite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned. For examples, the backtranslation strategy of Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søg"
2020.loresmt-1.15,P18-1072,0,0.0521031,"19), and pretrained language model finetuning (Liu et al., 2020). De spite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned. For examples, the backtranslation strategy of Sennrich et al. (2016a) is less effective in lowresource set tings where it is hard to train a good backtranslation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fu jita, 2018) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages (Søgaard et al., 2018) and the same is ap plicable in the case of transfer learning too (Mon toya et al., 2019). Hence, this line of research needs more attention from the MT research commu nity. In this context, we refer interested readers to some of the papers (Bentivogli et al., 2016; Castilho et al., 2017) that compared PBSMT and NMT on a variety of usecases. As for lowresource scenar ios, as mentioned above, many studies (e.g. Koehn and Knowles (2017); Östling and Tiedemann (2017); Dowling et al. (2018)) found that PBSMT can pro vide better translations than NMT, and many found the opposite results (C"
2020.loresmt-1.15,tiedemann-2012-parallel,0,0.0402059,"or investigation are from different language fami lies and morphologically divergent to each other. En glish is a less inflected language, whereas Tamil is a morphologically rich and highly inflected language. Our investigation is from a less inflected language to a highly inflected language. With this, we compare translation in PBSMT and NMT with a translation pair involving two morphologically divergent lan guages. 2.3 Data Used This section presents the datasets used for MT sys tem building (Ramesh et al., 2020). For experi mentation we used data from three different sources: OPUS2 (Tiedemann, 2012), WikiMatrix3 (Schwenk et al., 2019) and PMIndia4 (Haddow and Kirefu, 2020). Corpus statistics are shown in Table 1. We carried out experiments using two different setups: (i) in the first setup, the MT systems were built on a training set compiled from all data domains listed above; we call this setup MIXED, and (ii) in the second setup, the MT systems were built on a train ing set compiled only from different software local isation data from OPUS, viz. GNOME, KDE4 and Ubuntu; we call this setup IT. The development and test set sentences were randomly drawn from these localisation corpora."
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.lrec-1.461,N18-1118,0,0.0219273,"scribes in detail our methodology, followed by section 5. where we present the results and discuss our findings. In section 6., we present our general observations and general guidelines for MT evaluation context, with suggestions for future research. 2. Related Work Although the term “document level” has been used freely to refer to MT systems handling context beyond the sentence level, the definition of what exactly constitutes a documentlevel is not yet well defined. Work on document-level MT show that those systems mostly use a context span of sentence pairs (Tiedemann and Scherrer, 2017; Bawden et al., 2018; M¨uller et al., 2018) and very few have attempted to go beyond that span (Voita et al., 2019). For some of the developed context-aware MT models, test suites have been designed to better evaluate translation of the addressed discourse-level phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019). As for overall MT evaluation, a few attempts have been made to perform human evaluation with document-level set-ups. To reassess claims of “human parity” in MT, Toral et al. (2018) used consecutive single sentences (opposed to randomised single sentences in Hassan et al. (2018)) to"
2020.lrec-1.461,D18-1512,0,0.304858,"Missing"
2020.lrec-1.461,W18-6307,0,0.050152,"Missing"
2020.lrec-1.461,W17-4811,0,0.0269221,"en in section 3. Section 4. describes in detail our methodology, followed by section 5. where we present the results and discuss our findings. In section 6., we present our general observations and general guidelines for MT evaluation context, with suggestions for future research. 2. Related Work Although the term “document level” has been used freely to refer to MT systems handling context beyond the sentence level, the definition of what exactly constitutes a documentlevel is not yet well defined. Work on document-level MT show that those systems mostly use a context span of sentence pairs (Tiedemann and Scherrer, 2017; Bawden et al., 2018; M¨uller et al., 2018) and very few have attempted to go beyond that span (Voita et al., 2019). For some of the developed context-aware MT models, test suites have been designed to better evaluate translation of the addressed discourse-level phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019). As for overall MT evaluation, a few attempts have been made to perform human evaluation with document-level set-ups. To reassess claims of “human parity” in MT, Toral et al. (2018) used consecutive single sentences (opposed to randomised single sentences in Has"
2020.lrec-1.461,tiedemann-2012-parallel,0,0.151583,"Missing"
2020.lrec-1.461,W18-6312,1,0.850075,"aluation, human evaluation 1. Introduction One of the biggest challenges for MT is the ability to handle discourse dependencies and the wider context of a document. Although currently an active community is working on developing discourse-level MT systems, the improvements reported for those systems are still limited. One of the main reasons for this is that the evaluation of documentlevel systems (both automatic and human) has primarily been performed at the sentence level and is, therefore, unable to recognise the real improvements of document-level systems. Furthermore, two recent studies (Toral et al., 2018; L¨aubli et al., 2018) independently reassessed the bold claims of MT “achieving human parity” (Hassan et al., 2018) and found that the lack of extra-sentential context has a great effect on quality assessment. After these two papers were published, WMT19 considered their criticisms and attempted, for the first time, a document-level human evaluation for some language pairs. Despite the increased in the field, both for extending MT systems to operate on the document level, as well as for improving the evaluation methodology by expanding it to the document level, the definition of what exactly"
2020.lrec-1.461,P19-1116,0,0.0253011,"uss our findings. In section 6., we present our general observations and general guidelines for MT evaluation context, with suggestions for future research. 2. Related Work Although the term “document level” has been used freely to refer to MT systems handling context beyond the sentence level, the definition of what exactly constitutes a documentlevel is not yet well defined. Work on document-level MT show that those systems mostly use a context span of sentence pairs (Tiedemann and Scherrer, 2017; Bawden et al., 2018; M¨uller et al., 2018) and very few have attempted to go beyond that span (Voita et al., 2019). For some of the developed context-aware MT models, test suites have been designed to better evaluate translation of the addressed discourse-level phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019). As for overall MT evaluation, a few attempts have been made to perform human evaluation with document-level set-ups. To reassess claims of “human parity” in MT, Toral et al. (2018) used consecutive single sentences (opposed to randomised single sentences in Hassan et al. (2018)) to rank translations by two MT systems (Microsoft and Google) and a human reference from the WMT"
2020.lt4hala-1.7,L16-1153,1,0.851154,"lexical error correction, and consists of spell-checking the OCR output using dictionaries, online spell-checking (Bassil and Alwani, 2012), and using rulebased systems for correcting noise (Thompson et al., 2015). The second group of strategies for correcting OCR output is context-based error correction, in which the goal is to evaluate the likelihood that a sentence has been produced by a native speaker by using an n-gram LM to evaluate the texts produced by the OCR (Zhuang et al., 2004), and to use a noisy-channel model (Brill and Moore, 2000), or a Statistical Machine Translation engine (Afli et al., 2016) to correct the output of the OCR. A final approach proposes using several OCR tools and retrieving the text that is most accurate (Volk et al., 2010; Sch¨afer and Weitz, 2012). Historical documents are conventionally preserved in physical libraries, and increasingly made available through digital databases. This transition, however, usually involves storing the information concerned as images. In order to correctly process the data contained in these images, they need to be converted into machine-readable characters. This process is known as optical character recognition (OCR). Converting a b"
2020.lt4hala-1.7,P13-1021,0,0.0306136,"parent and subject to human intervention. Keywords: OCR Correction, Historical Text, NLP Tools 1. Introduction The approaches involving image-processing perform modifications on the scanned book that make the OCR perform better. Examples of these approaches include adding noise, as through rotation, for augmenting the training set (Bieniecki et al., 2007), reconstructing the image of documents in poor condition (Maekawa et al., 2019), clustering similar words so they are processed together (Kluzner et al., 2009) or jointly modeling the text of the document and the process of rendering glyphs (Berg-Kirkpatrick et al., 2013). Techniques for increasing accuracy by performing postOCR corrections can be divided into three sub-groups. The first group involves lexical error correction, and consists of spell-checking the OCR output using dictionaries, online spell-checking (Bassil and Alwani, 2012), and using rulebased systems for correcting noise (Thompson et al., 2015). The second group of strategies for correcting OCR output is context-based error correction, in which the goal is to evaluate the likelihood that a sentence has been produced by a native speaker by using an n-gram LM to evaluate the texts produced by t"
2020.lt4hala-1.7,P00-1037,0,0.481034,"ctions can be divided into three sub-groups. The first group involves lexical error correction, and consists of spell-checking the OCR output using dictionaries, online spell-checking (Bassil and Alwani, 2012), and using rulebased systems for correcting noise (Thompson et al., 2015). The second group of strategies for correcting OCR output is context-based error correction, in which the goal is to evaluate the likelihood that a sentence has been produced by a native speaker by using an n-gram LM to evaluate the texts produced by the OCR (Zhuang et al., 2004), and to use a noisy-channel model (Brill and Moore, 2000), or a Statistical Machine Translation engine (Afli et al., 2016) to correct the output of the OCR. A final approach proposes using several OCR tools and retrieving the text that is most accurate (Volk et al., 2010; Sch¨afer and Weitz, 2012). Historical documents are conventionally preserved in physical libraries, and increasingly made available through digital databases. This transition, however, usually involves storing the information concerned as images. In order to correctly process the data contained in these images, they need to be converted into machine-readable characters. This proces"
2020.lt4hala-1.7,W11-2123,0,0.0433782,"Missing"
2020.lt4hala-1.7,2005.mtsummit-papers.11,0,0.0211689,"Missing"
2020.lt4hala-1.7,W12-3212,0,\N,Missing
2020.ngt-1.17,D11-1033,0,0.499189,"rnal data, we made use of parallel corpora from a variety of existing sources, e.g. OPUS3 (Tiedemann, 2012). First, we found out which corpora are similar to Duolingo’s training dataset. For this, we measured perplexity of the source and target texts of the external datasets on the in-domain language models (LMs) (i.e. LMs were built on the Duolingo’s data). We selected those corpora whose sentences are found to be more similar to those of Duolingo’s language learning data. 2.2 Selecting ‘Pseudo In-domain’ Parallel Sentences from External Data The state-of-the-art sentence selection method of Axelrod et al. (2011) is used to extract ‘pseudo in-domain’ data from large corpora using bilingual cross-entropy difference. The extracted data is usually used to train domain-specific MT systems or to fine-tune generic MT systems. We conhttps://sharedtask.duolingo.com/ https://acl2020.org/ 3 http://opus.lingfil.uu.se/ 144 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 144–152 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d sider Duolingo’s language learning data released in this shared task as the real in-domain d"
2020.ngt-1.17,N12-1047,0,0.0285516,"calised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. In our experiments, word alignment models are trained using the GIZA++ toolkit4 (Och and Ney, 2003), phrases are extracted following the grow-diag-final-and algorithm of Koehn et al. (2003), Kneser-Ney smoothing is applied at phrase scoring, and a smoothing constant (0.8u) is used for training lexicalised reordering models. The weights of the parameters are optimised using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the MarianNMT (Junczys-Dowmunt et al., 2018) toolkit. The NMT systems are Transformer models (Vaswani et al., 2017). In our experiments, we followed the recommended best set-up from Vaswani et al. (2017). The tokens of the training, evaluation and validation sets are segmented into sub-word units using the Byte-Pair Encoding (BPE) technique (Sennrich et al., 2016). We performed 32,000 join operations. Our training set-up is as f"
2020.ngt-1.17,P11-1105,0,0.0307972,"sults The MT system setups As pointed out earlier, we chose the classical PBSMT and emerging NMT paradigms for building our MT systems. To build our PB-SMT systems, we used the Moses toolkit (Koehn et al., 2007). We used a 5-gram LM trained with modified KneserNey smoothing (Kneser and Ney, 1995) using the KenLM toolkit (Heafield et al., 2013). Our PBSMT log-linear features include: (a) 4 translational features (forward and backward phrase and lexical probabilities), (b) 8 lexicalised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. In our experiments, word alignment models are trained using the GIZA++ toolkit4 (Och and Ney, 2003), phrases are extracted following the grow-diag-final-and algorithm of Koehn et al. (2003), Kneser-Ney smoothing is applied at phrase scoring, and a smoothing constant (0.8u) is used for training lexicalised reordering models. The weights of the parameters are optimised using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a disto"
2020.ngt-1.17,ganitkevitch-callison-burch-2014-multilingual,0,0.028057,"a series of experiments by selecting different sizes of sentences from this dataset following the approach described in Section 2.2. We found that this dataset does have much impact on our system building (cf. row (f) of Table 4). In fact, we see from the last row of Table 4 that inclusion of a part of the OpenSubtitles corpus to the training set of the best setup deteriorates the system’s performance. We call this MT system (row (g)) NEURAL-OpenSub. 3.6 tions for a Portuguese translation used for forming the training set. Additionally, we used Portuguese paraphrases from the PPDB database17 (Ganitkevitch and Callison-Burch, 2014) as the part of the training data for this task. We also used the targetside (Portuguese) sentences of the shared task training set (cf. Table 1), and add them (i.e. identical copy) to the both sides of the training set. The first 1,000 sentences of the development set (cf. Table 1) serves as our development set and the remaining sentences of the development set serves as our test set. PB-SMT NMT Paraphrases Variations 30 (max.) 75 (max.) Table 5: Number of training examples used for the monolingual MT training. PB-SMT NMT BLEU 72.30 40.73 Table 6: The BLEU scores of the monolingual MT systems"
2020.ngt-1.17,P13-2121,0,0.0610125,"Missing"
2020.ngt-1.17,P07-1019,0,0.0402358,"ities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. In our experiments, word alignment models are trained using the GIZA++ toolkit4 (Och and Ney, 2003), phrases are extracted following the grow-diag-final-and algorithm of Koehn et al. (2003), Kneser-Ney smoothing is applied at phrase scoring, and a smoothing constant (0.8u) is used for training lexicalised reordering models. The weights of the parameters are optimised using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the MarianNMT (Junczys-Dowmunt et al., 2018) toolkit. The NMT systems are Transformer models (Vaswani et al., 2017). In our experiments, we followed the recommended best set-up from Vaswani et al. (2017). The tokens of the training, evaluation and validation sets are segmented into sub-word units using the Byte-Pair Encoding (BPE) technique (Sennrich et al., 2016). We performed 32,000 join operations. Our training set-up is as follows. We consider the size of the encoder and decoder layers to be 6. As in Vaswani et a"
2020.ngt-1.17,P18-4020,0,0.0392481,"Missing"
2020.ngt-1.17,W17-3204,0,0.0178537,"2. Note that we used translations by Amazon Translate provided by STAPLE as the reference translations, which are excellent in quality. Thus, the BLEU scores on the test set can provide indications how good or bad our MT systems are. Additionally, we have reported the MT systems’ BLEU scores on the development set. As can be seen from Table 2, the BLEU scores of the MT systems are very low. These scores were expected given the (small) number of sentences used for training. Interestingly, PB-SMT outperforms NMT by a large margin in terms of BLEU, and this can happen in low-resource scenarios (Koehn and Knowles, 2017). PB-SMT Transformer BLEU dev set test set 22.69 19.92 9.57 9.23 Table 2: The BLEU scores of baseline MT systems. 3.4 The External Datasets Used Since we (participants) are allowed to use external data, we decided to use freely available bilingual corpora whose sentences are similar to those of the Duolingo’s English–Portuguese dataset. We took all bilingual corpora available in the OPUS repository, and measured perplexity of source and target texts on in-domain LMs (built on the Duolingo data only). We found that the most similar corpora to the English-side of the Duolingo’s training corpus a"
2020.ngt-1.17,N03-1017,0,0.127383,"high-coverage set of Portuguese translations of the English prompts. We generated a set of source–target pairs (Portuguese-to-Portuguese) from each of the high-coverage sets of alternative Portuguese translations. This served as our training data for Portuguese-to-Portuguese MT system building. Additionally, we used an existing paraphrasing resource (Ganitkevitch and CallisonBurch, 2014) for Portuguese and appended that to the training data. 2.4 Combining multiple n-best Translations In this work, we built a number of MT systems using the state-of-the-art phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) and neural MT (NMT) (Vaswani et al., 2017) approaches. The n-best translations produced by the different MT systems (i.e. a PB-SMT and several NMT systems) are combined adopting a variety of approaches (e.g. majority voting) to produce the final sets of translations of the English prompts. 3 3.1 Experiments and Results The MT system setups As pointed out earlier, we chose the classical PBSMT and emerging NMT paradigms for building our MT systems. To build our PB-SMT systems, we used the Moses toolkit (Koehn et al., 2007). We used a 5-gram LM trained with modified KneserNey smoothing (Kneser a"
2020.ngt-1.17,2020.ngt-1.28,0,0.0616254,"the participants were asked to produce high-coverage sets of plausible translations given English prompts (input source sentences). We present our English-to-Portuguese machine translation (MT) models that were built applying various strategies, e.g. data and sentence selection, monolingual MT for generating alternative translations, and combining multiple nbest translations. Our experiments show that adding the aforementioned techniques to the baseline yields an excellent performance in the English-to-Portuguese translation task. 1 2 2.1 Introduction The ADAPT Centre participated in STAPLE1 (Mayhew et al., 2020), a shared task of the 4th WNGT which will be held at ACL 2020,2 in the English-to-Portuguese language direction. The task focuses on a specific use case of MT, i.e. generating many possible translations for a given input text. Such situations are usually seen on language-learning platforms (e.g. Duolingo) where the learning process includes translation-based exercises, and evaluation is done by comparing learners’ responses with a large set of human-curated acceptable translations. The shared task organisers (Duolingo) have released real language-learner data of Duolingo as training examples."
2020.ngt-1.17,J03-1002,0,0.0116893,"PB-SMT systems, we used the Moses toolkit (Koehn et al., 2007). We used a 5-gram LM trained with modified KneserNey smoothing (Kneser and Ney, 1995) using the KenLM toolkit (Heafield et al., 2013). Our PBSMT log-linear features include: (a) 4 translational features (forward and backward phrase and lexical probabilities), (b) 8 lexicalised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. In our experiments, word alignment models are trained using the GIZA++ toolkit4 (Och and Ney, 2003), phrases are extracted following the grow-diag-final-and algorithm of Koehn et al. (2003), Kneser-Ney smoothing is applied at phrase scoring, and a smoothing constant (0.8u) is used for training lexicalised reordering models. The weights of the parameters are optimised using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the MarianNMT (Junczys-Dowmunt et al., 2018) toolkit. The NMT systems are Transformer models"
2020.ngt-1.17,P02-1040,0,0.107565,"Missing"
2020.ngt-1.17,N18-2051,0,0.0189167,"are similar to the sentences of the Duolingo’s data in terms of domain and style. We appended the extracted ‘pseudo in-domain’ data to the STAPLE’s (Duolingo’s) training data for building different MT systems which are described latter in the paper (cf. Section 3.5). 2.3 Same-language MT Same-language MT has been successfully used in many NLP applications, e.g. text-to-speech synthesis for creating alternative target sequences (Cahill et al., 2009), translation between varieties of the same language (Brazilian Portuguese to European Portuguese) (Fancellu et al., 2014), paraphrase generation (Plachouras et al., 2018), and producing many alternative sequences of a given input question in question answering (Bhattacharjee et al., 2020). In our case, we developed Portuguese-toPortuguese MT systems that were able to generate n-best (same-language) alternative sentences of an input Portuguese sentence. Using this monolingual MT systems, we could obtain a set of alternative sequences of a given Portuguese translation. As mentioned earlier, the Duolingo training data includes a high-coverage set of Portuguese translations of the English prompts. We generated a set of source–target pairs (Portuguese-to-Portuguese"
2020.ngt-1.17,P16-1162,0,0.0603442,"eters are optimised using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the MarianNMT (Junczys-Dowmunt et al., 2018) toolkit. The NMT systems are Transformer models (Vaswani et al., 2017). In our experiments, we followed the recommended best set-up from Vaswani et al. (2017). The tokens of the training, evaluation and validation sets are segmented into sub-word units using the Byte-Pair Encoding (BPE) technique (Sennrich et al., 2016). We performed 32,000 join operations. Our training set-up is as follows. We consider the size of the encoder and decoder layers to be 6. As in Vaswani et al. (2017), we employ residual connection around layers (He et al., 2015), followed by layer normalisation (Ba et al., 2016). The weight matrix between the embedding layers is shared, similar to Press and Wolf (2016). Dropout (Gal and Ghahramani, 2016) between layers is set to 0.10. We use mini-batches of 145 4 http://www.statmt.org/moses/giza/ GIZA++.html size 64 for updating. The models are trained with the Adam optimizer (Kingma and Ba, 2"
2020.ngt-1.17,tiedemann-2012-parallel,0,0.0482062,"ons. The shared task organisers (Duolingo) have released real language-learner data of Duolingo as training examples. We applied a number of strategies to our MT system building process, e.g. monolingual MT, extracting parallel sentences that are similar to the Duolingo’s 1 2 Methodology Selecting External Datasets Since the shared task organisers released training data with a limited number of prompts (only 4,000 English prompts for English-to-Portuguese translation) and allowed participants to use external data, we made use of parallel corpora from a variety of existing sources, e.g. OPUS3 (Tiedemann, 2012). First, we found out which corpora are similar to Duolingo’s training dataset. For this, we measured perplexity of the source and target texts of the external datasets on the in-domain language models (LMs) (i.e. LMs were built on the Duolingo’s data). We selected those corpora whose sentences are found to be more similar to those of Duolingo’s language learning data. 2.2 Selecting ‘Pseudo In-domain’ Parallel Sentences from External Data The state-of-the-art sentence selection method of Axelrod et al. (2011) is used to extract ‘pseudo in-domain’ data from large corpora using bilingual cross-e"
2020.nlptea-1.2,W07-0212,0,0.13949,"Missing"
2020.nlptea-1.2,P18-1007,0,0.013986,"ndidates are found in the n-best candidate list B, no output is offered. our solution is simple, we believe its novelty lies in adding more context to the regular many-to-one language modelling process, which is also reflected in our results (cf. Section 3). 2.3 2.3.1 Out-of-Vocabulary Tokens There is a known limitation of neural networks, i.e. they typically operate with a fixed vocabulary. As for a more complex task such as neural machine translation (Vaswani et al., 2017), sub-word segmentation techniques such as Byte Pair Encoding (Sennrich et al., 2016) or using a unigram language model (Kudo, 2018) are usually utilised in order to solve this problem. Since we calculate the edit distance measure on tokens, it is difficult to apply sub-word segmentation or similar techniques to this problem. As far as the spelling checking is concerned, the presence of out-of-vocabulary tokens in the input sentence may cause overcorrection at decoding because they will not come as suggestions in the n-best list (B). In order to solve this problem, we adopted two strategies: Inference In the decoding process, the many-to-one LSTM network takes each item from the list of n-gram sequences generated from the"
2020.nlptea-1.2,P16-1162,0,0.303328,"er 15 3. if neither the current word cw nor any similar candidates are found in the n-best candidate list B, no output is offered. our solution is simple, we believe its novelty lies in adding more context to the regular many-to-one language modelling process, which is also reflected in our results (cf. Section 3). 2.3 2.3.1 Out-of-Vocabulary Tokens There is a known limitation of neural networks, i.e. they typically operate with a fixed vocabulary. As for a more complex task such as neural machine translation (Vaswani et al., 2017), sub-word segmentation techniques such as Byte Pair Encoding (Sennrich et al., 2016) or using a unigram language model (Kudo, 2018) are usually utilised in order to solve this problem. Since we calculate the edit distance measure on tokens, it is difficult to apply sub-word segmentation or similar techniques to this problem. As far as the spelling checking is concerned, the presence of out-of-vocabulary tokens in the input sentence may cause overcorrection at decoding because they will not come as suggestions in the n-best list (B). In order to solve this problem, we adopted two strategies: Inference In the decoding process, the many-to-one LSTM network takes each item from t"
2020.nlptea-1.2,tiedemann-2012-parallel,0,0.00982334,"ay correction options to users, i.e. via a GUI. 12 Input Sentence &lt;s&gt; students met their principle supervisor at the university Initial Sequence Current Word &lt;s&gt; students &lt;s&gt; students met &lt;s&gt; students met their &lt;s&gt; students met their principle &lt;s&gt; students met their principle supervisor &lt;s&gt; students met their principle supervisor at &lt;s&gt; students met their principle supervisor at the &lt;s&gt; students met their principle supervisor at the university sentences in our training set are linguistically correct and do not have many spelling mistakes. We selected the News Commentary Corpus v114 from OPUS (Tiedemann, 2012) as it is a reasonably clean corpus. We applied the standard filtering and pre-processing steps to the corpus. We are left with 213,036 Arabic sentences after cleaning and pre-processing. We also added a portion of the MultiUN5 corpus from OPUS to the News Commentary corpus. Our final training data contains 554,622 Arabic sentences. The MultiUN corpus is of a better linguistic quality and the News Commentary corpus is more generic in nature. Therefore, we believe that adding the MultiUN corpus to the News Commentary corpus enriches our training data vocabulary. In order to pre-process the trai"
2020.sltu-1.33,W10-3601,0,0.0349068,"ssible segmentations and retrieving those containing the smallest amount of words. Haruechaiyasak and Kongyoung (2009) used conditional random fields to classify each character as either wordbeginning or intra-word. Nararatwong et al. (2018) proposed an improvement to this approach by adding information from POS tags. The use of several segmentations has also been proposed by Kudo (2018), in which he tries to integrate candidates from different segmentations. This technique has applications in a number of topics such as word-alignment (Xi et al., 2011) or language modeling (Seng et al., 2009; Abate et al., 2010). The use of multiple instances in the training data, where only one side is modified, has been used by Poncelas et al. (2019), who showed that using multiple instances of the 242 Dataset Source Sentence Reference Output the train is here. รถไฟมาแลว Deepcut รถไฟนีเปนสายพันธุ This train is a breed. BPE 20000 รถไฟอยูทีนีอยูทีนีดวย The train is located here. Deepcut + BPE 20000 Source Sentence Reference Deepcut BPE 20000 รถไฟนีอยูนี I have a new bicycle. ฉันมีจักรยานใหม ผมไดทำการทดลองใหม ผมมีรถบรรทุกคนใหมๆ This train is located here. Deepcut + BPE 20000 Source Sentence ผมมีรถจัก"
2020.sltu-1.33,P17-4012,0,0.0609692,"get side whereas we keep the same number of merge operations on the source side. The main reason for this is that using several splits on the source side would also mean that NMT models would be evaluated with different versions (different splits) of the test set, which would obviously affect the results. 2. dataset Deepcut character BPE 1000 BPE 5000 BPE 10000 BPE 20000 Table 1: Performance of the NMT model trained with data using different methods for word segmentation on the target-side. 3. Data and Model Configuration The models are built in the English to Thai direction using OpenNMT-py (Klein et al., 2017). We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units and a maximum vocabulary size of 50000 words for each language. We use the Asian Language Treebank (ALT) Parallel Corpus (Riza et al., 2016) for training and evaluation. We split the corpus so we use 20K sentences for training and 106 sentences for development. We use Tatoeba (Tiedemann, 2012) for evaluating the models (173 sentences). In order to evaluate the models we translate the test set and measure the quality of the translation using an automatic evaluation metric, which provides an estimation of how good t"
2020.sltu-1.33,P18-1007,0,0.0451665,"owarawan, 1986), consisting of identifying the longest sequence of characters that match a word in the dictionary. Another approach is maximal-matching method (Sornlertlamvanich, 1993), which consists of generating all possible segmentations and retrieving those containing the smallest amount of words. Haruechaiyasak and Kongyoung (2009) used conditional random fields to classify each character as either wordbeginning or intra-word. Nararatwong et al. (2018) proposed an improvement to this approach by adding information from POS tags. The use of several segmentations has also been proposed by Kudo (2018), in which he tries to integrate candidates from different segmentations. This technique has applications in a number of topics such as word-alignment (Xi et al., 2011) or language modeling (Seng et al., 2009; Abate et al., 2010). The use of multiple instances in the training data, where only one side is modified, has been used by Poncelas et al. (2019), who showed that using multiple instances of the 242 Dataset Source Sentence Reference Output the train is here. รถไฟมาแลว Deepcut รถไฟนีเปนสายพันธุ This train is a breed. BPE 20000 รถไฟอยูทีนีอยูทีนีดวย The train is located here. De"
2020.sltu-1.33,W18-6316,0,0.0236059,"of Byte Pair Encoding, different segmentations of Thai sentences can be obtained. The experiments show that combining these datasets, performance is improved for NMT models trained with a dataset that has been split using a supervised splitting tool. Keywords: Machine Translation, Word Segmentation, Thai Language In Machine Translation (MT), low-resource languages are especially challenging as the amount of parallel data available to train models may not be enough to achieve high translation quality. One approach to mitigate this problem is to augment the training data with similar languages (Lakew et al., 2018) or artificially-generated text (Sennrich et al., 2016a; Poncelas et al., 2018b). State-of-the-art Neural Machine Translation (NMT) models are based on the sequence-to-sequence framework (i.e. models are built using pairs of sequences as training data). Therefore, sentences are modeled as a sequence of tokens. As Thai is a scriptio continua language (where whitespaces are not used to indicate the boundaries between words) sentences need to be segmented in the preprocessing step in order to be converted into sequences of tokens. The segmentation of sentences is a well-known problem in Natural L"
2020.sltu-1.33,P02-1040,0,0.106555,"for training and evaluation. We split the corpus so we use 20K sentences for training and 106 sentences for development. We use Tatoeba (Tiedemann, 2012) for evaluating the models (173 sentences). In order to evaluate the models we translate the test set and measure the quality of the translation using an automatic evaluation metric, which provides an estimation of how good the translation is by comparing it to a human-translated reference. As the evaluation is made in a Thai text (which does not contain n-grams, we use CHRF3 (Popovic, 2015) which is a character-level metric, instead of BLEU (Papineni et al., 2002), which is based on overlap of n-grams. The English side is tokenized and truecased (applying the proper case of the words) and we apply BPE with 89500 operations (the default explored in Sennrich et al. (2016b)). For the Thai side we explore combinations of different approaches of sentence segmentation: Experimental Results First, we evaluate the performance of the models when different merge operations are used on the Thai side. The performance of the model, evaluated using CHRF3 are presented in Table 1. In the table, each row shows the results (the evaluation of the translation of the test"
2020.sltu-1.33,R19-1107,1,0.769589,"conditional random fields to classify each character as either wordbeginning or intra-word. Nararatwong et al. (2018) proposed an improvement to this approach by adding information from POS tags. The use of several segmentations has also been proposed by Kudo (2018), in which he tries to integrate candidates from different segmentations. This technique has applications in a number of topics such as word-alignment (Xi et al., 2011) or language modeling (Seng et al., 2009; Abate et al., 2010). The use of multiple instances in the training data, where only one side is modified, has been used by Poncelas et al. (2019), who showed that using multiple instances of the 242 Dataset Source Sentence Reference Output the train is here. รถไฟมาแลว Deepcut รถไฟนีเปนสายพันธุ This train is a breed. BPE 20000 รถไฟอยูทีนีอยูทีนีดวย The train is located here. Deepcut + BPE 20000 Source Sentence Reference Deepcut BPE 20000 รถไฟนีอยูนี I have a new bicycle. ฉันมีจักรยานใหม ผมไดทำการทดลองใหม ผมมีรถบรรทุกคนใหมๆ This train is located here. Deepcut + BPE 20000 Source Sentence ผมมีรถจักรยานชุดใหมขึนมาแลว I have a new set of bicycles. she does not have many friends in Kyoto. Reference Deepcut เธอไมคอยมีเพื"
2020.sltu-1.33,W15-3049,0,0.0126699,"Asian Language Treebank (ALT) Parallel Corpus (Riza et al., 2016) for training and evaluation. We split the corpus so we use 20K sentences for training and 106 sentences for development. We use Tatoeba (Tiedemann, 2012) for evaluating the models (173 sentences). In order to evaluate the models we translate the test set and measure the quality of the translation using an automatic evaluation metric, which provides an estimation of how good the translation is by comparing it to a human-translated reference. As the evaluation is made in a Thai text (which does not contain n-grams, we use CHRF3 (Popovic, 2015) which is a character-level metric, instead of BLEU (Papineni et al., 2002), which is based on overlap of n-grams. The English side is tokenized and truecased (applying the proper case of the words) and we apply BPE with 89500 operations (the default explored in Sennrich et al. (2016b)). For the Thai side we explore combinations of different approaches of sentence segmentation: Experimental Results First, we evaluate the performance of the models when different merge operations are used on the Thai side. The performance of the model, evaluated using CHRF3 are presented in Table 1. In the table"
2020.sltu-1.33,P16-1009,0,0.657596,"ai sentences can be obtained. The experiments show that combining these datasets, performance is improved for NMT models trained with a dataset that has been split using a supervised splitting tool. Keywords: Machine Translation, Word Segmentation, Thai Language In Machine Translation (MT), low-resource languages are especially challenging as the amount of parallel data available to train models may not be enough to achieve high translation quality. One approach to mitigate this problem is to augment the training data with similar languages (Lakew et al., 2018) or artificially-generated text (Sennrich et al., 2016a; Poncelas et al., 2018b). State-of-the-art Neural Machine Translation (NMT) models are based on the sequence-to-sequence framework (i.e. models are built using pairs of sequences as training data). Therefore, sentences are modeled as a sequence of tokens. As Thai is a scriptio continua language (where whitespaces are not used to indicate the boundaries between words) sentences need to be segmented in the preprocessing step in order to be converted into sequences of tokens. The segmentation of sentences is a well-known problem in Natural Language Processing (NLP), Thai is no exception. How Th"
2020.sltu-1.33,P16-1162,0,0.781431,"ai sentences can be obtained. The experiments show that combining these datasets, performance is improved for NMT models trained with a dataset that has been split using a supervised splitting tool. Keywords: Machine Translation, Word Segmentation, Thai Language In Machine Translation (MT), low-resource languages are especially challenging as the amount of parallel data available to train models may not be enough to achieve high translation quality. One approach to mitigate this problem is to augment the training data with similar languages (Lakew et al., 2018) or artificially-generated text (Sennrich et al., 2016a; Poncelas et al., 2018b). State-of-the-art Neural Machine Translation (NMT) models are based on the sequence-to-sequence framework (i.e. models are built using pairs of sequences as training data). Therefore, sentences are modeled as a sequence of tokens. As Thai is a scriptio continua language (where whitespaces are not used to indicate the boundaries between words) sentences need to be segmented in the preprocessing step in order to be converted into sequences of tokens. The segmentation of sentences is a well-known problem in Natural Language Processing (NLP), Thai is no exception. How Th"
2020.sltu-1.33,H93-1035,0,0.485984,"ned with the Deepcut dataset produced a translation that is either inaccurate or makes no sense. However, the models trained on BPE 20000 or Deepcut BPE 20000 produce a translation closer to the input after some postediting (i.e. removing the characters in gray). 4. Related Work There are several studies aiming to address the problem of splitting words in Thai. One of the first approaches to segmenting is the longest-matching method (Poowarawan, 1986), consisting of identifying the longest sequence of characters that match a word in the dictionary. Another approach is maximal-matching method (Sornlertlamvanich, 1993), which consists of generating all possible segmentations and retrieving those containing the smallest amount of words. Haruechaiyasak and Kongyoung (2009) used conditional random fields to classify each character as either wordbeginning or intra-word. Nararatwong et al. (2018) proposed an improvement to this approach by adding information from POS tags. The use of several segmentations has also been proposed by Kudo (2018), in which he tries to integrate candidates from different segmentations. This technique has applications in a number of topics such as word-alignment (Xi et al., 2011) or l"
2020.sltu-1.33,tiedemann-2012-parallel,0,0.0897019,"e 1: Performance of the NMT model trained with data using different methods for word segmentation on the target-side. 3. Data and Model Configuration The models are built in the English to Thai direction using OpenNMT-py (Klein et al., 2017). We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units and a maximum vocabulary size of 50000 words for each language. We use the Asian Language Treebank (ALT) Parallel Corpus (Riza et al., 2016) for training and evaluation. We split the corpus so we use 20K sentences for training and 106 sentences for development. We use Tatoeba (Tiedemann, 2012) for evaluating the models (173 sentences). In order to evaluate the models we translate the test set and measure the quality of the translation using an automatic evaluation metric, which provides an estimation of how good the translation is by comparing it to a human-translated reference. As the evaluation is made in a Thai text (which does not contain n-grams, we use CHRF3 (Popovic, 2015) which is a character-level metric, instead of BLEU (Papineni et al., 2002), which is based on overlap of n-grams. The English side is tokenized and truecased (applying the proper case of the words) and we"
2020.sltu-1.33,P11-3001,0,0.0248778,"nlertlamvanich, 1993), which consists of generating all possible segmentations and retrieving those containing the smallest amount of words. Haruechaiyasak and Kongyoung (2009) used conditional random fields to classify each character as either wordbeginning or intra-word. Nararatwong et al. (2018) proposed an improvement to this approach by adding information from POS tags. The use of several segmentations has also been proposed by Kudo (2018), in which he tries to integrate candidates from different segmentations. This technique has applications in a number of topics such as word-alignment (Xi et al., 2011) or language modeling (Seng et al., 2009; Abate et al., 2010). The use of multiple instances in the training data, where only one side is modified, has been used by Poncelas et al. (2019), who showed that using multiple instances of the 242 Dataset Source Sentence Reference Output the train is here. รถไฟมาแลว Deepcut รถไฟนีเปนสายพันธุ This train is a breed. BPE 20000 รถไฟอยูทีนีอยูทีนีดวย The train is located here. Deepcut + BPE 20000 Source Sentence Reference Deepcut BPE 20000 รถไฟนีอยูนี I have a new bicycle. ฉันมีจักรยานใหม ผมไดทำการทดลองใหม ผมมีรถบรรทุกคนใหมๆ This train i"
2020.vardial-1.10,N19-1388,0,0.024448,"al., 2018). The systems operate on sub-word units generated by byte-pair encoding (BPE) (Sennrich et al., 2016b). We set the number of BPE merge operations at 32000 both for the source and for the target language texts. We do not use shared vocabularies between the source (English, German) and the target (Serbian, Croatian) languages because they are distinct. For multilingual systems, on the other hand, we build a joint vocabulary for the two target languages (Serbian and Croatian) because they are very similar. These systems are built using the same technique as (Johnson et al., 2017) and (Aharoni et al., 2019), namely adding a target language label “SR” or “HR” to each source sentence. All the systems have Transformer architecture with 6 layers for both the encoder and decoder, model size of 512, feed forward size of 2048, and 8 attention heads. For training, we use Adam optimiser (Kingma and Ba, 2015), initial learning rate of 0.0002, and batch size of 4096 (sub)words. Validation perplexity is calculated after every 4000 batches (at so-called “checkpoints”), and if this perplexity does not improve after 20 checkpoints, the training stops. For set-ups with less than two million segments,8 following"
2020.vardial-1.10,W11-2131,0,0.0855674,"Missing"
2020.vardial-1.10,W17-4755,0,0.0134676,"ecall. The chrF score is based on character n-gram matching (n in range from 1 to 6) instead of word n-gram matching. It is F-score which weights recall two times more than precision. The characTER score is based on edit distance which takes into account not only substitutions, insertions and deletions, but also word sequence reorderings and character sequences in unmatched words. We use the BLEU score because of the long tradition of using it for MT evaluation despite well-known faults, and the two character level scores because they are shown to correlate much better with human assessments (Bojar et al., 2017; Ma et al., 2018). Recently, the chrF score is recommended as a replacement for BLEU (Mathur et al., 2020). In addition to the automatic MT evaluation scores, for each of the systems we report the size of the training corpus, the percentage of particular target language data in this corpus, as well as the training time in terms of days. 4.1 Translation from English The results for translation from English are presented in Table 3. As expected, multilingual systems are better than bilingual for all set-ups, even for the unbalanced clean low-resourced corpus. However, the improvements are small"
2020.vardial-1.10,W18-6315,0,0.0183131,"e two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating English user reviews, we also explore different types of synthetic parallel in-domain data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018a), which is a widely used practice in NMT. We explored two types of synthetic data: back-translated (BT) and forward-translated (FT). BT data consist of in-domain target language data and their machine translations into English, whereas FT data consist of English data and their machine translations into Serbian and Croatian. All our experiments were carried out on publicly available data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 102 Proceedings of the 7th VarD"
2020.vardial-1.10,W18-1820,0,0.0270192,"Croatian by an NMT system thus providing FT synthetic parallel corpora. In order to obtain a balanced corpus in terms of the two target languages, we translated half of the IMDb (about 300k segments) and half of the Amazon (about 500k segments) corpora into Croatian, and the other two halves into Serbian. More details about the NMT systems used for BT and FT can be found in the next section. Detailed statistics for all movie reviews can be seen in Table 2. 3 NMT systems All our systems are based on the Transformer architecture (Vaswani et al., 2017) and built using the Sockeye implementation (Hieber et al., 2018). The systems operate on sub-word units generated by byte-pair encoding (BPE) (Sennrich et al., 2016b). We set the number of BPE merge operations at 32000 both for the source and for the target language texts. We do not use shared vocabularies between the source (English, German) and the target (Serbian, Croatian) languages because they are distinct. For multilingual systems, on the other hand, we build a joint vocabulary for the two target languages (Serbian and Croatian) because they are very similar. These systems are built using the same technique as (Johnson et al., 2017) and (Aharoni et"
2020.vardial-1.10,2009.mtsummit-papers.9,0,0.0931716,"Missing"
2020.vardial-1.10,W18-6316,0,0.338426,"South Slavic languages are generally less supported and investigated in natural language processing, they have been explored in the field of machine translation (MT). Nevertheless, a large part of the work deals with the previous state-of-the-art approach, namely phrase-based statistical machine translation (PBSMT) (Popovi´c and Ljubeˇsi´c, 2014; Toral et al., 2014; Popovi´c and Arˇcan, 2015; Arˇcan et al., 2016; Popovi´c et al., 2016; S´anchez-Cartagena et al., 2016; Mauˇcec and Brest, 2017), while much less work can be found about the new state-of-the-art, neural machine translation (NMT) (Lakew et al., 2018; Lohar et al., 2019). In this work, we focus on NMT into Croatian and Serbian, two very closely related South Slavic languages. We explore two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating"
2020.vardial-1.10,W14-0405,0,0.0403373,"Missing"
2020.vardial-1.10,W19-3715,1,0.80412,"Missing"
2020.vardial-1.10,W18-6450,0,0.0120761,"e is based on character n-gram matching (n in range from 1 to 6) instead of word n-gram matching. It is F-score which weights recall two times more than precision. The characTER score is based on edit distance which takes into account not only substitutions, insertions and deletions, but also word sequence reorderings and character sequences in unmatched words. We use the BLEU score because of the long tradition of using it for MT evaluation despite well-known faults, and the two character level scores because they are shown to correlate much better with human assessments (Bojar et al., 2017; Ma et al., 2018). Recently, the chrF score is recommended as a replacement for BLEU (Mathur et al., 2020). In addition to the automatic MT evaluation scores, for each of the systems we report the size of the training corpus, the percentage of particular target language data in this corpus, as well as the training time in terms of days. 4.1 Translation from English The results for translation from English are presented in Table 3. As expected, multilingual systems are better than bilingual for all set-ups, even for the unbalanced clean low-resourced corpus. However, the improvements are smaller for Croatian, t"
2020.vardial-1.10,2020.acl-main.448,0,0.075307,"Missing"
2020.vardial-1.10,J82-2005,0,0.475792,"Missing"
2020.vardial-1.10,W15-4913,1,0.899953,"Missing"
2020.vardial-1.10,W14-4210,1,0.868872,"Missing"
2020.vardial-1.10,W16-4813,1,0.874385,"Missing"
2020.vardial-1.10,W15-3049,1,0.871723,"Missing"
2020.vardial-1.10,W18-6319,0,0.0146645,"forward-translated IMDb data. Forward translation was performed by the system “+ SELECTED -BT”. +A MAZON -FT: on the data used for the system “+IMD B -FT” enriched with forward-translated Amazon data. Forward translation was performed by the system “+IMD B -FT”. In addition to these mid-resourced partially in-domain systems, we also translate the movie reviews test set by the three systems trained on full out-of-domain OPUS data (EN→HR FULL , EN→SR FULL , EN → HR + SR FULL CLEAN / ED ). 4 Results We evaluate our systems using the following three automatic overall evaluation scores: sacreBLEU (Post, 2018), chrF (Popovi´c, 2015) and characTER (Wang et al., 2016). The BLEU score is based on word ngram (n in range from 1 to 4) precision and brevity penalty which should replace recall. The chrF score is based on character n-gram matching (n in range from 1 to 6) instead of word n-gram matching. It is F-score which weights recall two times more than precision. The characTER score is based on edit distance which takes into account not only substitutions, insertions and deletions, but also word sequence reorderings and character sequences in unmatched words. We use the BLEU score because of the long"
2020.vardial-1.10,W16-3421,0,0.0274555,"Missing"
2020.vardial-1.10,P19-1021,0,0.0192737,"” to each source sentence. All the systems have Transformer architecture with 6 layers for both the encoder and decoder, model size of 512, feed forward size of 2048, and 8 attention heads. For training, we use Adam optimiser (Kingma and Ba, 2015), initial learning rate of 0.0002, and batch size of 4096 (sub)words. Validation perplexity is calculated after every 4000 batches (at so-called “checkpoints”), and if this perplexity does not improve after 20 checkpoints, the training stops. For set-ups with less than two million segments,8 following the recommendations for low-resource settings in (Sennrich and Zhang, 2019), we changed the following parameters: training stops after 10 checkpoints instead of 20, initial learning rate is 0.0001 instead of 0.0002, and checkpoint interval is 100 batches instead of 4000. 3.1 Systems for translating from English The following set-ups were investigated for translating from English into Croatian and Serbian: Clean data only The bilingual (EN→HR CLEAN , EN → SR CLEAN ) and multilingual (EN→HR + SR CLEAN) low-resourced systems are trained only on News and Other data from Table 1. The segments in these texts are mainly properly aligned, so we refer to it as “clean”. It is"
2020.vardial-1.10,P16-1009,0,0.317856,"sely related South Slavic languages. We explore two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating English user reviews, we also explore different types of synthetic parallel in-domain data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018a), which is a widely used practice in NMT. We explored two types of synthetic data: back-translated (BT) and forward-translated (FT). BT data consist of in-domain target language data and their machine translations into English, whereas FT data consist of English data and their machine translations into Serbian and Croatian. All our experiments were carried out on publicly available data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/lic"
2020.vardial-1.10,P16-1162,0,0.373808,"sely related South Slavic languages. We explore two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating English user reviews, we also explore different types of synthetic parallel in-domain data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018a), which is a widely used practice in NMT. We explored two types of synthetic data: back-translated (BT) and forward-translated (FT). BT data consist of in-domain target language data and their machine translations into English, whereas FT data consist of English data and their machine translations into Serbian and Croatian. All our experiments were carried out on publicly available data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/lic"
2020.vardial-1.10,W18-6305,0,0.0610788,"Missing"
2020.vardial-1.10,tiedemann-2012-parallel,0,0.0765211,"ranslating English user reviews into Serbian has been addressed in (Lohar et al., 2019). They compared PBSMT and NMT systems trained on out-of-domain data, and then further investigated the NMT system with additional synthetic data. However, they explored only Serbian as a target language, and their baseline system was built on a very small News corpus of 200k segments. In this work, we also include Croatian, and we build the systems on more data, both out-of-domain as well as in-domain. 2 Data sets 2.1 OPUS parallel data For all our systems, we use the publicly available OPUS4 parallel data (Tiedemann, 2012). The vast majority of these resources for the desired language combinations consists of OpenSubtitles. For English and both target languages, we also used SETIMES News, Bible, Tilde, EU-bookshop, QED, and Tatoeba corpora. In addition, we used GlobalVoices for Serbian, and hrenWac, TED and Wikimedia for Croatian. For German, we only used OpenSubtitles because other corpora are rather sparse. However, including these corpora might be interesting for future work. The original parallel data were filtered in order to eliminate noisy parts: too long segments (more than 100 words), segment pairs wit"
2020.vardial-1.10,2014.eamt-1.45,1,0.816457,"Missing"
2020.vardial-1.10,N18-1136,0,0.0599846,"Missing"
2020.vardial-1.10,W16-2342,0,0.013707,"was performed by the system “+ SELECTED -BT”. +A MAZON -FT: on the data used for the system “+IMD B -FT” enriched with forward-translated Amazon data. Forward translation was performed by the system “+IMD B -FT”. In addition to these mid-resourced partially in-domain systems, we also translate the movie reviews test set by the three systems trained on full out-of-domain OPUS data (EN→HR FULL , EN→SR FULL , EN → HR + SR FULL CLEAN / ED ). 4 Results We evaluate our systems using the following three automatic overall evaluation scores: sacreBLEU (Post, 2018), chrF (Popovi´c, 2015) and characTER (Wang et al., 2016). The BLEU score is based on word ngram (n in range from 1 to 4) precision and brevity penalty which should replace recall. The chrF score is based on character n-gram matching (n in range from 1 to 6) instead of word n-gram matching. It is F-score which weights recall two times more than precision. The characTER score is based on edit distance which takes into account not only substitutions, insertions and deletions, but also word sequence reorderings and character sequences in unmatched words. We use the BLEU score because of the long tradition of using it for MT evaluation despite well-know"
2020.vardial-1.10,W19-5208,0,0.0482939,"Missing"
2020.vardial-1.10,D16-1160,0,0.0182344,"c languages. We explore two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating English user reviews, we also explore different types of synthetic parallel in-domain data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018a), which is a widely used practice in NMT. We explored two types of synthetic data: back-translated (BT) and forward-translated (FT). BT data consist of in-domain target language data and their machine translations into English, whereas FT data consist of English data and their machine translations into Serbian and Croatian. All our experiments were carried out on publicly available data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 102 Proc"
2020.wat-1.12,D11-1033,0,0.0485137,"a. We made use of monolingual data in training in order to improve our baseline models. The use of synthetic data to improve NMT systems is a well-accepted and popular method, especially in low-resource scenarios (Sennrich et al., 2016a). We did not blindly use all sentences of the monolingual data; instead, we select those sentences that are similar in terms of style and domain to the sentences of the parallel data. In order to select the sentences which are similar to those of the parallel data, we use perplexity scores of the monolingual sentences according to the in-domain language model (Axelrod et al., 2011; Toral, 2013; Haque et al., 2020; Nayak et al., 2020; Parthasarathy et al., 2020). The selected monolingual sentences are then back-translated to form synthetic training data. 2.2 Hyperparameters Search We conducted a series of experiments to find the best hyperparameters for Transformer as far as lowresource translation is concerned. For our experiments we primarily used those hyperparameters that are commonly used for low-resource scenarios (Sennrich and Zhang, 2019). Additionally, we varied a handful of parameters to see how the MT systems would perform, e.g. encoder and decoder layer size"
2020.wat-1.12,2020.ngt-1.17,1,0.433733,"in training in order to improve our baseline models. The use of synthetic data to improve NMT systems is a well-accepted and popular method, especially in low-resource scenarios (Sennrich et al., 2016a). We did not blindly use all sentences of the monolingual data; instead, we select those sentences that are similar in terms of style and domain to the sentences of the parallel data. In order to select the sentences which are similar to those of the parallel data, we use perplexity scores of the monolingual sentences according to the in-domain language model (Axelrod et al., 2011; Toral, 2013; Haque et al., 2020; Nayak et al., 2020; Parthasarathy et al., 2020). The selected monolingual sentences are then back-translated to form synthetic training data. 2.2 Hyperparameters Search We conducted a series of experiments to find the best hyperparameters for Transformer as far as lowresource translation is concerned. For our experiments we primarily used those hyperparameters that are commonly used for low-resource scenarios (Sennrich and Zhang, 2019). Additionally, we varied a handful of parameters to see how the MT systems would perform, e.g. encoder and decoder layer sizes. We applied Byte-Pair Encoding"
2020.wat-1.12,P17-4012,0,0.0173978,"1: Statistics of the training, development and test sets. Monolingual-Corpus OpusNlp OSCAR AI4Bharat-IndicNLP Sentences 30k 284K 3.5M Words 1,003,211 14,938,567 53,694,876 Base Base + 1M Table 2: Statistics of the monolingual corpora. Table 3: The BLEU scores of the Odia-to-English MT systems. number of BPE merge operations: 32,000 (ii) the sizes of the encoder and decoder layers: 4 and 6, respectively, and (iii) the learning-rate: 0.02. 3 Results and Discussion We used the state-of-the-art Transformer model in order to prepare our MT systems. For system building, we used the OpenNMT toolkit (Klein et al., 2017). In order to evaluate our MT systems, we used the widely-used evaluation metric, BLEU (Papineni et al., 2002). 4.1 Base Base + 1M Data Used We made use of both the parallel and monolingual data that were provided by the WAT 2020 task organisers.2 Additionally, we used external monolingual data for system building. The statistics of the parallel and monolingual corpora (OpusNlp,3 OSCAR4 and AI4Bharat-IndicNLP)5 are shown in Tables 1 and 2, respectively. In order to remove noisy sentences from the corpus, we used a language identifier CLD26 with a confidence of 95. 4 The Baseline MT System We m"
2020.wat-1.12,2020.wmt-1.91,1,0.546687,"to improve our baseline models. The use of synthetic data to improve NMT systems is a well-accepted and popular method, especially in low-resource scenarios (Sennrich et al., 2016a). We did not blindly use all sentences of the monolingual data; instead, we select those sentences that are similar in terms of style and domain to the sentences of the parallel data. In order to select the sentences which are similar to those of the parallel data, we use perplexity scores of the monolingual sentences according to the in-domain language model (Axelrod et al., 2011; Toral, 2013; Haque et al., 2020; Nayak et al., 2020; Parthasarathy et al., 2020). The selected monolingual sentences are then back-translated to form synthetic training data. 2.2 Hyperparameters Search We conducted a series of experiments to find the best hyperparameters for Transformer as far as lowresource translation is concerned. For our experiments we primarily used those hyperparameters that are commonly used for low-resource scenarios (Sennrich and Zhang, 2019). Additionally, we varied a handful of parameters to see how the MT systems would perform, e.g. encoder and decoder layer sizes. We applied Byte-Pair Encoding (BPE) word segmentat"
2020.wat-1.12,P02-1040,0,0.10852,"LP Sentences 30k 284K 3.5M Words 1,003,211 14,938,567 53,694,876 Base Base + 1M Table 2: Statistics of the monolingual corpora. Table 3: The BLEU scores of the Odia-to-English MT systems. number of BPE merge operations: 32,000 (ii) the sizes of the encoder and decoder layers: 4 and 6, respectively, and (iii) the learning-rate: 0.02. 3 Results and Discussion We used the state-of-the-art Transformer model in order to prepare our MT systems. For system building, we used the OpenNMT toolkit (Klein et al., 2017). In order to evaluate our MT systems, we used the widely-used evaluation metric, BLEU (Papineni et al., 2002). 4.1 Base Base + 1M Data Used We made use of both the parallel and monolingual data that were provided by the WAT 2020 task organisers.2 Additionally, we used external monolingual data for system building. The statistics of the parallel and monolingual corpora (OpusNlp,3 OSCAR4 and AI4Bharat-IndicNLP)5 are shown in Tables 1 and 2, respectively. In order to remove noisy sentences from the corpus, we used a language identifier CLD26 with a confidence of 95. 4 The Baseline MT System We made use of the parallel corpus in order to build our baseline NMT system. The original parallel data includes"
2020.wat-1.12,2020.wmt-1.27,1,0.504005,"line models. The use of synthetic data to improve NMT systems is a well-accepted and popular method, especially in low-resource scenarios (Sennrich et al., 2016a). We did not blindly use all sentences of the monolingual data; instead, we select those sentences that are similar in terms of style and domain to the sentences of the parallel data. In order to select the sentences which are similar to those of the parallel data, we use perplexity scores of the monolingual sentences according to the in-domain language model (Axelrod et al., 2011; Toral, 2013; Haque et al., 2020; Nayak et al., 2020; Parthasarathy et al., 2020). The selected monolingual sentences are then back-translated to form synthetic training data. 2.2 Hyperparameters Search We conducted a series of experiments to find the best hyperparameters for Transformer as far as lowresource translation is concerned. For our experiments we primarily used those hyperparameters that are commonly used for low-resource scenarios (Sennrich and Zhang, 2019). Additionally, we varied a handful of parameters to see how the MT systems would perform, e.g. encoder and decoder layer sizes. We applied Byte-Pair Encoding (BPE) word segmentation (Sennrich et al., 2016b)"
2020.wat-1.12,P16-1009,0,0.028436,"a available. Building NMT systems for under-resourced languages 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2020/index.html still poses a challenge despite recent successes (Nakazawa et al., 2019). As for the task in which we are participating (English-to-Odia), the parallel data that the task organisers provided is relatively small. The organisers also provided us with monolingual data. We made use of monolingual data in training in order to improve our baseline models. The use of synthetic data to improve NMT systems is a well-accepted and popular method, especially in low-resource scenarios (Sennrich et al., 2016a). We did not blindly use all sentences of the monolingual data; instead, we select those sentences that are similar in terms of style and domain to the sentences of the parallel data. In order to select the sentences which are similar to those of the parallel data, we use perplexity scores of the monolingual sentences according to the in-domain language model (Axelrod et al., 2011; Toral, 2013; Haque et al., 2020; Nayak et al., 2020; Parthasarathy et al., 2020). The selected monolingual sentences are then back-translated to form synthetic training data. 2.2 Hyperparameters Search We conducte"
2020.wat-1.12,P16-1162,0,0.0439075,"a available. Building NMT systems for under-resourced languages 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2020/index.html still poses a challenge despite recent successes (Nakazawa et al., 2019). As for the task in which we are participating (English-to-Odia), the parallel data that the task organisers provided is relatively small. The organisers also provided us with monolingual data. We made use of monolingual data in training in order to improve our baseline models. The use of synthetic data to improve NMT systems is a well-accepted and popular method, especially in low-resource scenarios (Sennrich et al., 2016a). We did not blindly use all sentences of the monolingual data; instead, we select those sentences that are similar in terms of style and domain to the sentences of the parallel data. In order to select the sentences which are similar to those of the parallel data, we use perplexity scores of the monolingual sentences according to the in-domain language model (Axelrod et al., 2011; Toral, 2013; Haque et al., 2020; Nayak et al., 2020; Parthasarathy et al., 2020). The selected monolingual sentences are then back-translated to form synthetic training data. 2.2 Hyperparameters Search We conducte"
2020.wat-1.12,P19-1021,0,0.0187376,"ar to those of the parallel data, we use perplexity scores of the monolingual sentences according to the in-domain language model (Axelrod et al., 2011; Toral, 2013; Haque et al., 2020; Nayak et al., 2020; Parthasarathy et al., 2020). The selected monolingual sentences are then back-translated to form synthetic training data. 2.2 Hyperparameters Search We conducted a series of experiments to find the best hyperparameters for Transformer as far as lowresource translation is concerned. For our experiments we primarily used those hyperparameters that are commonly used for low-resource scenarios (Sennrich and Zhang, 2019). Additionally, we varied a handful of parameters to see how the MT systems would perform, e.g. encoder and decoder layer sizes. We applied Byte-Pair Encoding (BPE) word segmentation (Sennrich et al., 2016b) both individually and jointly to the source and target language corpora. Since BPE when applied individually worked better for us, we stick to this setup for our system building. We found that the following hyperparameters provided us with the best result in this low-resource scenario: (i) the 114 Proceedings of the 7th Workshop on Asian Translation, pages 114–117 c December 4, 2020. 2020"
2020.wat-1.12,W13-2803,0,0.0224492,"lingual data in training in order to improve our baseline models. The use of synthetic data to improve NMT systems is a well-accepted and popular method, especially in low-resource scenarios (Sennrich et al., 2016a). We did not blindly use all sentences of the monolingual data; instead, we select those sentences that are similar in terms of style and domain to the sentences of the parallel data. In order to select the sentences which are similar to those of the parallel data, we use perplexity scores of the monolingual sentences according to the in-domain language model (Axelrod et al., 2011; Toral, 2013; Haque et al., 2020; Nayak et al., 2020; Parthasarathy et al., 2020). The selected monolingual sentences are then back-translated to form synthetic training data. 2.2 Hyperparameters Search We conducted a series of experiments to find the best hyperparameters for Transformer as far as lowresource translation is concerned. For our experiments we primarily used those hyperparameters that are commonly used for low-resource scenarios (Sennrich and Zhang, 2019). Additionally, we varied a handful of parameters to see how the MT systems would perform, e.g. encoder and decoder layer sizes. We applied"
2020.wat-1.17,P19-1310,0,0.0281085,"Missing"
2020.wat-1.17,W17-4714,0,0.0362169,"Missing"
2020.wat-1.17,P17-2061,0,0.0180965,"corpus added to the training set of B1 (cf. Table 1). The setup for B1 and B2 was changed by setting the mini-batch size for validation to 32 and the learning rate to 0.0005. Corpus basic basic basic+JW300 Scenario 1: The first scenario is the most basic, where we simply performed conventional finetuning of the model parameters on in-domain data only, namely the BSD training set. Scenario 2: In the second scenario we implemented mixed fine-tuning of model parameters, where fine-tuning is conducted on the training data that consists of both in-domain data and out-ofdomain data as described in Chu et al. (2017). The in-domain data was augmented by oversampling the BSD training set 50 times and the out-ofdomain data is a mixture of JESC and OpenSubtitles. Scenario 3: As for the third scenario, source-side monolingual sentences were mined that are similar in styles to the BSD test set sentences. We followed Nayak et al. (2020) and Parthasarathy et al. (2020) in order to mine those sentences from large monolingual data that could be beneficial for finetuning the original NMT models. We identified terms in the test set to be translated. For this, we followed the monolingual terminology extraction method"
2020.wat-1.17,W14-4806,1,0.530287,"data was augmented by oversampling the BSD training set 50 times and the out-ofdomain data is a mixture of JESC and OpenSubtitles. Scenario 3: As for the third scenario, source-side monolingual sentences were mined that are similar in styles to the BSD test set sentences. We followed Nayak et al. (2020) and Parthasarathy et al. (2020) in order to mine those sentences from large monolingual data that could be beneficial for finetuning the original NMT models. We identified terms in the test set to be translated. For this, we followed the monolingual terminology extraction methods described in Haque et al. (2014, 2018), which used a large corpus that is generic in nature as a reference corpus. In our setup, we used the source-side of the authentic training bitexts on which our NMT system (B2) was trained as the reference corpus. The intuition is to extract those terminological expressions from the test set that do not occur or rarely occur in the training data and are more indicative of the test corpus. Given the list of extracted terms, we mined sentences from the 143 Scenarios 1 2 3 4 Corpus BSD JESC+OpenSubtitles+BSD∗50 SOSC SOSC+BSD BLEU 14.52 18.70 18.53 18.59 RIBES 70.13 73.04 73.16 73.22 Human"
2020.wat-1.17,L16-1147,0,0.0220934,"in-domain data only, mixed fine-tuning and lastly document-level fine-tuning. Corpora Used To train the baseline models a mixture of three corpora was used, where one corpus contains indomain sentences and the other two corpora contain out-of-domain sentences. The in-domain BSD corpus (Rikters et al., 2019) consists of a training set of 20,000 sentences, a development set of 2,051 sentences and a test set of 2,120 sentences. The out-of-domain data that was added to the BSD training set includes the JESC3 (Pryzant et al., 2018) corpus consisting of 2.8 million sentences and the OpenSubtitles4 (Lison and Tiedemann, 2016) corpus consisting of 2.2 million sentences. Furthermore, monolingual data from the target-side (En) of the JW3005 corpus (Agi´c and Vuli´c, 2019) was used to create synthetic data with the use of backtranslation. Finally, source-language monolingual data with n-grams similar to that of the documents in the test set was mined from the Common Crawl Corpus6 to be used as a source-side original synthetic corpus (SOSC) for fine-tuning the NMT model parameters. 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2020/index.html 2 https://github.com/marian-nmt/marian 3 https://nlp.stanford.edu/projects/ jesc/"
2020.wat-1.17,2020.wmt-1.91,1,0.71146,"l parameters on in-domain data only, namely the BSD training set. Scenario 2: In the second scenario we implemented mixed fine-tuning of model parameters, where fine-tuning is conducted on the training data that consists of both in-domain data and out-ofdomain data as described in Chu et al. (2017). The in-domain data was augmented by oversampling the BSD training set 50 times and the out-ofdomain data is a mixture of JESC and OpenSubtitles. Scenario 3: As for the third scenario, source-side monolingual sentences were mined that are similar in styles to the BSD test set sentences. We followed Nayak et al. (2020) and Parthasarathy et al. (2020) in order to mine those sentences from large monolingual data that could be beneficial for finetuning the original NMT models. We identified terms in the test set to be translated. For this, we followed the monolingual terminology extraction methods described in Haque et al. (2014, 2018), which used a large corpus that is generic in nature as a reference corpus. In our setup, we used the source-side of the authentic training bitexts on which our NMT system (B2) was trained as the reference corpus. The intuition is to extract those terminological expressions from"
2020.wat-1.17,P02-1040,0,0.109197,"rained on the BSD, JESC and OpenSubtitles corpora combined. As mentioned earlier, we used the MarianNMT toolkit to train our Transformer models. The setup described in Sennrich et al. (2017) was used as is for B1, more specifically the mini-batch size for validation was 64 and the learning rate 0.0003. The changes to this setup for the other two baseline setups (B2 and B3) are discussed below. The setup differs since BPE with a vocabulary of 32,000 was applied to the training set of B1 and a vocabulary size of 6,000 was used for the other two baseline models (B2 and B3). We obtained the BLEU (Papineni et al., 2002) scores to evaluate these baseline MT systems on the evaluation test set and the scores are presented in Table 1. The second baseline model (B2) was trained on the same training set as B1. The third baseline model’s (B3) training set consisted of the targetside original synthetic corpus added to the training set of B1 (cf. Table 1). The setup for B1 and B2 was changed by setting the mini-batch size for validation to 32 and the learning rate to 0.0005. Corpus basic basic basic+JW300 Scenario 1: The first scenario is the most basic, where we simply performed conventional finetuning of the model"
2020.wat-1.17,2020.wmt-1.27,1,0.569247,"n data only, namely the BSD training set. Scenario 2: In the second scenario we implemented mixed fine-tuning of model parameters, where fine-tuning is conducted on the training data that consists of both in-domain data and out-ofdomain data as described in Chu et al. (2017). The in-domain data was augmented by oversampling the BSD training set 50 times and the out-ofdomain data is a mixture of JESC and OpenSubtitles. Scenario 3: As for the third scenario, source-side monolingual sentences were mined that are similar in styles to the BSD test set sentences. We followed Nayak et al. (2020) and Parthasarathy et al. (2020) in order to mine those sentences from large monolingual data that could be beneficial for finetuning the original NMT models. We identified terms in the test set to be translated. For this, we followed the monolingual terminology extraction methods described in Haque et al. (2014, 2018), which used a large corpus that is generic in nature as a reference corpus. In our setup, we used the source-side of the authentic training bitexts on which our NMT system (B2) was trained as the reference corpus. The intuition is to extract those terminological expressions from the test set that do not occur"
2020.wat-1.17,D10-1092,0,0.0293317,"Missing"
2020.wat-1.17,W04-3250,0,0.555652,"Missing"
2020.wat-1.17,P07-2045,0,0.0230925,"parameters. 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2020/index.html 2 https://github.com/marian-nmt/marian 3 https://nlp.stanford.edu/projects/ jesc/ 4 http://www.opensubtitles.org/ 5 http://opus.nlpl.eu/JW300.php 6 https://commoncrawl.org/the-data/ 142 Proceedings of the 7th Workshop on Asian Translation, pages 142–146 c December 4, 2020. 2020 Association for Computational Linguistics 2.2 Model B1 B2 B3 Preprocessing The source-side sentences (Ja) were segmented using MeCab7 and the target-side sentences (En) were tokenized and lower-cased using the standard scripts from the Moses toolkit (Koehn et al., 2007). For both languages Subword-NMT8 was used to apply byte pair encoding (BPE). We experimented with two different vocabulary sizes: 6,000 and 32,000. Details around these vocabulary sizes will be discussed in more detail in Section 3. 3 4 Improving the Baseline MT Systems The BLEU score of each of the baseline models described in Section 3 is shown in Table 1 and it is clear that B2 is the best-performing MT system out of the three baseline models. Therefore, we decided 7 https://github.com/SamuraiT/ mecab-python3 8 https://github.com/rsennrich/ subword-nmt BPE size 32,000 6,000 6,000 BLEU 16.7"
2020.wat-1.17,W17-4739,0,0.0166476,"al training has ended, with a newly selected corpus. We implemented four different scenarios for fine-tuning the parameters. The Baseline MT Systems We started off by training three baseline models (B1, B2 and B3) and the best baseline model was used to experiment with different data augmentation methods when fine-tuning model parameters. These methods will be described in detail in Section 4. The first baseline model (B1) was trained on the BSD, JESC and OpenSubtitles corpora combined. As mentioned earlier, we used the MarianNMT toolkit to train our Transformer models. The setup described in Sennrich et al. (2017) was used as is for B1, more specifically the mini-batch size for validation was 64 and the learning rate 0.0003. The changes to this setup for the other two baseline setups (B2 and B3) are discussed below. The setup differs since BPE with a vocabulary of 32,000 was applied to the training set of B1 and a vocabulary size of 6,000 was used for the other two baseline models (B2 and B3). We obtained the BLEU (Papineni et al., 2002) scores to evaluate these baseline MT systems on the evaluation test set and the scores are presented in Table 1. The second baseline model (B2) was trained on the same"
2020.wat-1.17,P16-1009,0,0.0350844,"ing This section outlines the corpora used and the steps that were taken to preprocess the data for training. 2.1 Introduction We participated in the WAT 20201 (Nakazawa et al., 2020) document-level BSD translation task and only submitted systems that translate from Japanese-to-English (Ja-to-En). Our MT systems are Transformer models (Vaswani et al., 2017) which were trained using the Marian-NMT toolkit.2 In this work, we applied different domain adaptation techniques, such as using synthetic data from source- and target-side monolingual data through the use of forward- and back-translation (Sennrich et al., 2016; Chinea-R´ıos et al., 2017; Poncelas et al., 2018) and out-of-domain parallel data to train our models. As far as fine-tuning the model parameters is concerned, we experimented with conventional fine-tuning which consists of fine-tuning on in-domain data only, mixed fine-tuning and lastly document-level fine-tuning. Corpora Used To train the baseline models a mixture of three corpora was used, where one corpus contains indomain sentences and the other two corpora contain out-of-domain sentences. The in-domain BSD corpus (Rikters et al., 2019) consists of a training set of 20,000 sentences, a"
2020.wat-1.22,D18-1399,0,0.0942937,"ecialised data domain (software localisation) into consideration. This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation. 1 Introduction In recent years, MT researchers have proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation mo"
2020.wat-1.22,W05-0909,0,0.0550641,"Missing"
2020.wat-1.22,W18-1207,0,0.0217389,"hographic properties (e.g. different phonology, no schwa deletion in Tamil). The grammatical structures of Hindi and Tamil are different too, and they are morphologically divergent and from different language families. We saw that the BPE-based segmentation can completely change the underlying semantic agreements of the source and target sentences, which, in turn, may provide the learner with wrong (reasoning) knowledge about the sentence-pairs. This could be one of the reasons why the BPE-based NMT model is found to be underperforming in this translation task. This finding is corroborated by Banerjee and Bhattacharyya (2018) who in their work found that the Morfessorbased segmentation can yield better translation quality than the BPE-based segmentation for linguistically distant language-pairs, and other way round for the close language-pairs. 5 Conclusion In this paper, we investigated NMT and PB-SMT in resource-poor scenarios, choosing a specialised data domain (software localisation) for translation and two rarely-tested morphologically divergent language-pairs, Hindi-to-Tamil and English-toTamil. We studied translations on two setups, i.e. training data compiled from (i) freely available variety of data domai"
2020.wat-1.22,D16-1025,0,0.0128855,"-cases. Although our primary objective 1 https://translate.google.com/ 178 Proceedings of the 7th Workshop on Asian Translation, pages 178–188 c December 4, 2020. 2020 Association for Computational Linguistics of this work is to study translations of the MT systems (PB-SMT and NMT) in under-resourced conditions, we provide a brief overview on some of the papers that compared PB-SMT and NMT on highresource settings too. Junczys-Dowmunt et al. (2016) compare PB-SMT and NMT on a range of translation-pairs and show that for all translation directions NMT is either on par with or surpasses PB-SMT. Bentivogli et al. (2016) analyse the output of MT systems in an Englishto-German translation task by considering different linguistic categories. Toral and Sánchez-Cartagena (2017) conduct an evaluation to compare NMT and PB-SMT outputs across broader aspects (e.g. fluency, reordering) for 9 language directions. Castilho et al. (2017) conduct an extensive qualitative and quantitative comparative evaluation of PB-SMT and NMT using automatic metrics and professional translators. Popović (2017) carries out an extensive comparison between NMT and PB-SMT languagerelated issues for the German–English language pair in both"
2020.wat-1.22,W18-6315,0,0.0364717,"Missing"
2020.wat-1.22,D19-5213,0,0.0183793,"lytested low-resource language-pairs, English-toTamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into consideration. This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation. 1 Introduction In recent years, MT researchers have proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less"
2020.wat-1.22,N12-1047,0,0.041349,"The MT systems To build our PB-SMT systems we used the Moses toolkit (Koehn et al., 2007). We used a 5-gram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995). Our PB-SMT log-linear features include: (a) 4 translational features (forward and backward phrase and lexical probabilities), (b) 8 lexicalised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. The weights of the parameters are optimized using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the OpenNMT toolkit (Klein et al., 2017). The NMT systems are Transformer models (Vaswani et al., 2017). The tokens of the training, evaluation and validation sets are segmented into sub-word units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016b). Recently, Sennrich and Zhang (2019) demonstrated that commonly used hyper-parameters configuration do not provide the best results in low-resource settings. Accordingly, we carr"
2020.wat-1.22,W17-4715,0,0.0170562,"ple et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fujita, 2018) due to the difficulty of training unsupervised cross-lingual word embeddings for such languages (Søgaard et al., 2018) and the same is applicable in the case of transfer learning too (Montoya et al., 2019). To this end, we investigate the performance of PB-SMT and NMT systems on two rarely-tested under-resourced language-pairs, English-to-Tamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into account. In this context, in Ramesh et al. (2020), we investigated the performance of P"
2020.wat-1.22,P19-1294,0,0.10752,"s of the English-to-Tamil and Hindi-to-Tamil NMT and PBSMT systems built on the in-domain training data. For this, we randomly sampled 100 sentences from the respective test sets (English-to-Tamil and Hindi181 (1) (2) src: hyp: ref: src: hyp: ref: छ व आयात कर ப இற ம ெச ப உ வா कोई गलती नह எ த தவ இ ைல ைழ இ ைல Table 4: Translations that are good in quality were unfairly penalised by the BLEU metric. to-Tamil). The outcome of this analysis is presented in the following sections. 4.3.1 Terminology Translation Terminology translation is arguably viewed as one of the most challenging problems in MT (Dinu et al., 2019; Haque et al., 2019; Exel et al., 2020). Since this work focuses on studying translation of data from a specialised domain, we looked at this area of translation with a special focus. We first looked at the translations of OOV terms in order to see how they are translated into the target. We found that both the NMT systems (English-to-Tamil and Hindi-toTamil) either incorrectly translate the software terms or drop them during translation. This happened for almost all the OOV terms. Nonetheless, the NMT systems are able to correctly translate a handful of OOV terms; this phenomenon is also cor"
2020.wat-1.22,W18-2202,1,0.84487,"nslation of patent documents (Long et al., 2016; Kinoshita et al., 2017), less-explored language pairs (Klubička et al., 2017, 2018), highly investigated “easy” translation pairs (Isabelle et al., 2017), and translation of catalogues of technical tools (Beyer et al., 2017). An opposite picture is also seen in the case of translation of the domain text; Nunez et al. (2019) showed PB-SMT outperforms NMT when translating user-generated content. The MT researchers have tested and compared PB-SMT and NMT in the resource-poor settings too. Koehn and Knowles (2017), Östling and Tiedemann (2017), and Dowling et al. (2018) found that PBSMT can provide better translations than NMT in low-resource scenarios. In contrast to these findings, however, many studies have demonstrated that NMT is better than PB-SMT in low-resource situations (Casas et al., 2019; Sennrich and Zhang, 2019). Hence, the findings of this line of MT research have yielded indeed a mixed bag of results, where way ahead unclear. This work investigates translations of a software localisation text with two low-resource translation-pairs, Hindi-to-Tamil and English-to-Tamil, taking two MT paradigms, PBSMT and NMT, into account. 3 Experimental Setup"
2020.wat-1.22,P11-1105,0,0.0244239,"th two low-resource translation-pairs, Hindi-to-Tamil and English-to-Tamil, taking two MT paradigms, PBSMT and NMT, into account. 3 Experimental Setups 3.1 The MT systems To build our PB-SMT systems we used the Moses toolkit (Koehn et al., 2007). We used a 5-gram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995). Our PB-SMT log-linear features include: (a) 4 translational features (forward and backward phrase and lexical probabilities), (b) 8 lexicalised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. The weights of the parameters are optimized using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the OpenNMT toolkit (Klein et al., 2017). The NMT systems are Transformer models (Vaswani et al., 2017). The tokens of the training, evaluation and validation sets are segmented into sub-word units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016b). Recently, Sennrich"
2020.wat-1.22,2020.eamt-1.29,0,0.141166,"Tamil NMT and PBSMT systems built on the in-domain training data. For this, we randomly sampled 100 sentences from the respective test sets (English-to-Tamil and Hindi181 (1) (2) src: hyp: ref: src: hyp: ref: छ व आयात कर ப இற ம ெச ப உ வா कोई गलती नह எ த தவ இ ைல ைழ இ ைல Table 4: Translations that are good in quality were unfairly penalised by the BLEU metric. to-Tamil). The outcome of this analysis is presented in the following sections. 4.3.1 Terminology Translation Terminology translation is arguably viewed as one of the most challenging problems in MT (Dinu et al., 2019; Haque et al., 2019; Exel et al., 2020). Since this work focuses on studying translation of data from a specialised domain, we looked at this area of translation with a special focus. We first looked at the translations of OOV terms in order to see how they are translated into the target. We found that both the NMT systems (English-to-Tamil and Hindi-toTamil) either incorrectly translate the software terms or drop them during translation. This happened for almost all the OOV terms. Nonetheless, the NMT systems are able to correctly translate a handful of OOV terms; this phenomenon is also corroborated by Haque et al. (2019) while i"
2020.wat-1.22,E17-2045,0,0.0172218,"no relation to the meaning of the source sentence. The top two example translations belong to this category. The translation of the first sentence by the SMT system is partially correct. As for the second example, the SMT system translates it as ‘report’ which is incorrect too. We also see that the translations occasionally contain repetitions of other translated words. This repetition of words is seen only for the NMT system. The bottom two translation examples of Table 9 belong to this category. These findings are corroborated by some of the studies that pursued this line of research (e.g. Farajian et al. (2017)). Unsurprisingly, such erroneous translations are seen more with the Hindito-Tamil translation direction. As for SMT, the MT system translates the third and fourth sentences incorrectly and correctly, respectively. In both cases, unlike NMT, the translations do not contain any repetition of other translated words. We sometimes found the appearance of one or more unexpected words in the translation, which completely changes the meaning of the translation, as shown in Table 10. However, the SMT system correctly translates the first two source sentences shown in Table 10. In the case of the thir"
2020.wat-1.22,R19-1052,1,0.865056,"-Tamil and Hindi-to-Tamil NMT and PBSMT systems built on the in-domain training data. For this, we randomly sampled 100 sentences from the respective test sets (English-to-Tamil and Hindi181 (1) (2) src: hyp: ref: src: hyp: ref: छ व आयात कर ப இற ம ெச ப உ வா कोई गलती नह எ த தவ இ ைல ைழ இ ைல Table 4: Translations that are good in quality were unfairly penalised by the BLEU metric. to-Tamil). The outcome of this analysis is presented in the following sections. 4.3.1 Terminology Translation Terminology translation is arguably viewed as one of the most challenging problems in MT (Dinu et al., 2019; Haque et al., 2019; Exel et al., 2020). Since this work focuses on studying translation of data from a specialised domain, we looked at this area of translation with a special focus. We first looked at the translations of OOV terms in order to see how they are translated into the target. We found that both the NMT systems (English-to-Tamil and Hindi-toTamil) either incorrectly translate the software terms or drop them during translation. This happened for almost all the OOV terms. Nonetheless, the NMT systems are able to correctly translate a handful of OOV terms; this phenomenon is also corroborated by Haque e"
2020.wat-1.22,P07-1019,0,0.0577099,"We used a 5-gram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995). Our PB-SMT log-linear features include: (a) 4 translational features (forward and backward phrase and lexical probabilities), (b) 8 lexicalised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. The weights of the parameters are optimized using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the OpenNMT toolkit (Klein et al., 2017). The NMT systems are Transformer models (Vaswani et al., 2017). The tokens of the training, evaluation and validation sets are segmented into sub-word units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016b). Recently, Sennrich and Zhang (2019) demonstrated that commonly used hyper-parameters configuration do not provide the best results in low-resource settings. Accordingly, we carried out a series of experiments in order to find the best hyperparameter configurations fo"
2020.wat-1.22,D17-1263,0,0.0356175,"Missing"
2020.wat-1.22,W17-5713,0,0.0205258,"ries out an extensive comparison between NMT and PB-SMT languagerelated issues for the German–English language pair in both translation directions. These works (Bentivogli et al., 2016; Castilho et al., 2017; Popović, 2017; Toral and Sánchez-Cartagena, 2017) show that NMT provides better translation quality than the previous state-of-the-art PB-SMT. This trend continues in other studies and use-cases: translation of literary text (Toral and Way, 2018), MT post-editing setups (Specia et al., 2017), industrial setups (Shterionov et al., 2017), translation of patent documents (Long et al., 2016; Kinoshita et al., 2017), less-explored language pairs (Klubička et al., 2017, 2018), highly investigated “easy” translation pairs (Isabelle et al., 2017), and translation of catalogues of technical tools (Beyer et al., 2017). An opposite picture is also seen in the case of translation of the domain text; Nunez et al. (2019) showed PB-SMT outperforms NMT when translating user-generated content. The MT researchers have tested and compared PB-SMT and NMT in the resource-poor settings too. Koehn and Knowles (2017), Östling and Tiedemann (2017), and Dowling et al. (2018) found that PBSMT can provide better translations t"
2020.wat-1.22,P17-4012,0,0.0309615,"eatures include: (a) 4 translational features (forward and backward phrase and lexical probabilities), (b) 8 lexicalised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. The weights of the parameters are optimized using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm (Huang and Chiang, 2007) is applied, with a distortion limit of 12. To build our NMT systems, we used the OpenNMT toolkit (Klein et al., 2017). The NMT systems are Transformer models (Vaswani et al., 2017). The tokens of the training, evaluation and validation sets are segmented into sub-word units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016b). Recently, Sennrich and Zhang (2019) demonstrated that commonly used hyper-parameters configuration do not provide the best results in low-resource settings. Accordingly, we carried out a series of experiments in order to find the best hyperparameter configurations for Transformer in our low-resource settings. In particular, we played with some of the hyperparameters, and found that"
2020.wat-1.22,P07-2045,0,0.00921102,"cenarios. In contrast to these findings, however, many studies have demonstrated that NMT is better than PB-SMT in low-resource situations (Casas et al., 2019; Sennrich and Zhang, 2019). Hence, the findings of this line of MT research have yielded indeed a mixed bag of results, where way ahead unclear. This work investigates translations of a software localisation text with two low-resource translation-pairs, Hindi-to-Tamil and English-to-Tamil, taking two MT paradigms, PBSMT and NMT, into account. 3 Experimental Setups 3.1 The MT systems To build our PB-SMT systems we used the Moses toolkit (Koehn et al., 2007). We used a 5-gram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995). Our PB-SMT log-linear features include: (a) 4 translational features (forward and backward phrase and lexical probabilities), (b) 8 lexicalised reordering probabilities (wbe-mslr-bidirectional-fe-allff ), (c) 5-gram LM probabilities, (d) 5 OSM features (Durrani et al., 2011), and (e) word-count and distortion penalties. The weights of the parameters are optimized using the margin-infused relaxed algorithm (Cherry and Foster, 2012) on the development set. For decoding, the cube-pruning algorithm"
2020.wat-1.22,W17-3204,0,0.0198405,"al., 2017), industrial setups (Shterionov et al., 2017), translation of patent documents (Long et al., 2016; Kinoshita et al., 2017), less-explored language pairs (Klubička et al., 2017, 2018), highly investigated “easy” translation pairs (Isabelle et al., 2017), and translation of catalogues of technical tools (Beyer et al., 2017). An opposite picture is also seen in the case of translation of the domain text; Nunez et al. (2019) showed PB-SMT outperforms NMT when translating user-generated content. The MT researchers have tested and compared PB-SMT and NMT in the resource-poor settings too. Koehn and Knowles (2017), Östling and Tiedemann (2017), and Dowling et al. (2018) found that PBSMT can provide better translations than NMT in low-resource scenarios. In contrast to these findings, however, many studies have demonstrated that NMT is better than PB-SMT in low-resource situations (Casas et al., 2019; Sennrich and Zhang, 2019). Hence, the findings of this line of MT research have yielded indeed a mixed bag of results, where way ahead unclear. This work investigates translations of a software localisation text with two low-resource translation-pairs, Hindi-to-Tamil and English-to-Tamil, taking two MT par"
2020.wat-1.22,D18-1549,0,0.0196732,"(software localisation) into consideration. This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation. 1 Introduction In recent years, MT researchers have proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 20"
2020.wat-1.22,2020.tacl-1.47,0,0.0214339,"proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fujita, 2018) due to the difficulty of training unsupervised cross-lingual word embeddings for such languages (Søgaard et al., 2018) and the same is applicable in the case of transfer lea"
2020.wat-1.22,W16-4602,0,0.0170166,"Popović (2017) carries out an extensive comparison between NMT and PB-SMT languagerelated issues for the German–English language pair in both translation directions. These works (Bentivogli et al., 2016; Castilho et al., 2017; Popović, 2017; Toral and Sánchez-Cartagena, 2017) show that NMT provides better translation quality than the previous state-of-the-art PB-SMT. This trend continues in other studies and use-cases: translation of literary text (Toral and Way, 2018), MT post-editing setups (Specia et al., 2017), industrial setups (Shterionov et al., 2017), translation of patent documents (Long et al., 2016; Kinoshita et al., 2017), less-explored language pairs (Klubička et al., 2017, 2018), highly investigated “easy” translation pairs (Isabelle et al., 2017), and translation of catalogues of technical tools (Beyer et al., 2017). An opposite picture is also seen in the case of translation of the domain text; Nunez et al. (2019) showed PB-SMT outperforms NMT when translating user-generated content. The MT researchers have tested and compared PB-SMT and NMT in the resource-poor settings too. Koehn and Knowles (2017), Östling and Tiedemann (2017), and Dowling et al. (2018) found that PBSMT can prov"
2020.wat-1.22,W19-5302,0,0.0140209,"significant contributions to this line of MT research. In future, we intend to consider more languages from different language families. We also plan to judge errors in translations using the multidimensional quality metrics error annotation framework (Lommel et al., 2014) which is a widely-used standard translation quality assessment toolkit in the translation industry and in MT research. The MT evaluation metrics such as chrF (Popović, 2015) which operates at the character level and COMET (Rei et al., 2020) which achieved new state-of-the-art performance on the WMT 2019 Metrics Shared Task (Ma et al., 2019) obtained high levels of correlation with human judgements. We intend to consider these metrics (chrF and COMET) in our future investigation. As in Exel et al. (2020) who examined terminology translation in NMT in an industrial setup while using the terminology integration approaches presented in Dinu et al. (2019), we intend to investigate terminology translation in NMT using the MT models of Dinu et al. (2019) on English-to-Tamil and Hindi-toTamil. 13/RC/2106) and is co-funded under the European Regional Development Fund. This project has partially received funding from the European Union’s"
2020.wat-1.22,W19-6804,0,0.0123894,"ome success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fujita, 2018) due to the difficulty of training unsupervised cross-lingual word embeddings for such languages (Søgaard et al., 2018) and the same is applicable in the case of transfer learning too (Montoya et al., 2019). To this end, we investigate the performance of PB-SMT and NMT systems on two rarely-tested under-resourced language-pairs, English-to-Tamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into account. In this context, in Ramesh et al. (2020), we investigated the performance of PB-SMT, NMT and a commercial MT system (Google Translate (GT))1 on English-to-Tamil taking the software localisation data into account, i.e. the same data as the one used in this work. In particular, in Ramesh et al. (2020), we produced rankings of the MT systems (PB-SMT, NMT and GT) via a"
2020.wat-1.22,W17-4708,0,0.0224887,"rent neural approaches to low-resource domain-specific text translation. 1 Introduction In recent years, MT researchers have proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fujita, 2018) due to the difficulty of training unsupervised cross-"
2020.wat-1.22,W19-6101,0,0.0175804,"ty than the previous state-of-the-art PB-SMT. This trend continues in other studies and use-cases: translation of literary text (Toral and Way, 2018), MT post-editing setups (Specia et al., 2017), industrial setups (Shterionov et al., 2017), translation of patent documents (Long et al., 2016; Kinoshita et al., 2017), less-explored language pairs (Klubička et al., 2017, 2018), highly investigated “easy” translation pairs (Isabelle et al., 2017), and translation of catalogues of technical tools (Beyer et al., 2017). An opposite picture is also seen in the case of translation of the domain text; Nunez et al. (2019) showed PB-SMT outperforms NMT when translating user-generated content. The MT researchers have tested and compared PB-SMT and NMT in the resource-poor settings too. Koehn and Knowles (2017), Östling and Tiedemann (2017), and Dowling et al. (2018) found that PBSMT can provide better translations than NMT in low-resource scenarios. In contrast to these findings, however, many studies have demonstrated that NMT is better than PB-SMT in low-resource situations (Casas et al., 2019; Sennrich and Zhang, 2019). Hence, the findings of this line of MT research have yielded indeed a mixed bag of results"
2020.wat-1.22,P02-1040,0,0.107223,"rce settings. In particular, we played with some of the hyperparameters, and found that the following configuration lead to the best results in our low-resource translation settings: (i) the BPE vocabulary size: 8,000, (ii) the sizes of encoder and decoder layers: 4 and 6, respectively, (iii) learning-rate: 0.0005, (iv) batch size (token): 4,000, and (v) Transformer head size: 4. As for the remaining hyperparameters, we followed the recommended best set-up from Vaswani et al. (2017). The validation on development set is performed using three cost functions: cross-entropy, perplexity and BLEU (Papineni et al., 2002). The early stopping criteria is based on cross-entropy; however, the final NMT system is selected as per highest BLEU score on the validation set. The beam size for search is set to 12. 179 3.2 Table 1: Data Statistics Choice of Languages In order to test MT on low-resource scenarios, we chose English and two Indian languages: Hindi, and Tamil. English, Hindi, and Tamil are Germanic, Indo-Aryan and Dravidian languages, respectively, so the languages we selected for investigation are from different language families and morphologically divergent to each other. English is a less inflected langu"
2020.wat-1.22,W15-3049,0,0.0483252,"s trained on the sub-word-level training data in comparison to one that was trained on the word-level training data. We believe that the findings of this work provide significant contributions to this line of MT research. In future, we intend to consider more languages from different language families. We also plan to judge errors in translations using the multidimensional quality metrics error annotation framework (Lommel et al., 2014) which is a widely-used standard translation quality assessment toolkit in the translation industry and in MT research. The MT evaluation metrics such as chrF (Popović, 2015) which operates at the character level and COMET (Rei et al., 2020) which achieved new state-of-the-art performance on the WMT 2019 Metrics Shared Task (Ma et al., 2019) obtained high levels of correlation with human judgements. We intend to consider these metrics (chrF and COMET) in our future investigation. As in Exel et al. (2020) who examined terminology translation in NMT in an industrial setup while using the terminology integration approaches presented in Dinu et al. (2019), we intend to investigate terminology translation in NMT using the MT models of Dinu et al. (2019) on English-to-T"
2020.wat-1.22,2020.loresmt-1.15,1,0.557165,"train a good back-translation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fujita, 2018) due to the difficulty of training unsupervised cross-lingual word embeddings for such languages (Søgaard et al., 2018) and the same is applicable in the case of transfer learning too (Montoya et al., 2019). To this end, we investigate the performance of PB-SMT and NMT systems on two rarely-tested under-resourced language-pairs, English-to-Tamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into account. In this context, in Ramesh et al. (2020), we investigated the performance of PB-SMT, NMT and a commercial MT system (Google Translate (GT))1 on English-to-Tamil taking the software localisation data into account, i.e. the same data as the one used in this work. In particular, in Ramesh et al. (2020), we produced rankings of the MT systems (PB-SMT, NMT and GT) via a social media platform-based human evaluation scheme, and demonstrate our findings in this lowresource domain-specific text translation task. The next section talks about some of the papers that compared PB-SMT and NMT on a variety of use-cases. The remainder of the paper"
2020.wat-1.22,2020.emnlp-main.213,0,0.0356814,"ne that was trained on the word-level training data. We believe that the findings of this work provide significant contributions to this line of MT research. In future, we intend to consider more languages from different language families. We also plan to judge errors in translations using the multidimensional quality metrics error annotation framework (Lommel et al., 2014) which is a widely-used standard translation quality assessment toolkit in the translation industry and in MT research. The MT evaluation metrics such as chrF (Popović, 2015) which operates at the character level and COMET (Rei et al., 2020) which achieved new state-of-the-art performance on the WMT 2019 Metrics Shared Task (Ma et al., 2019) obtained high levels of correlation with human judgements. We intend to consider these metrics (chrF and COMET) in our future investigation. As in Exel et al. (2020) who examined terminology translation in NMT in an industrial setup while using the terminology integration approaches presented in Dinu et al. (2019), we intend to investigate terminology translation in NMT using the MT models of Dinu et al. (2019) on English-to-Tamil and Hindi-toTamil. 13/RC/2106) and is co-funded under the Euro"
2020.wat-1.22,P16-1009,0,0.104395,"SMT) and NMT on two rarelytested low-resource language-pairs, English-toTamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into consideration. This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation. 1 Introduction In recent years, MT researchers have proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et"
2020.wat-1.22,P16-1162,0,0.529203,"SMT) and NMT on two rarelytested low-resource language-pairs, English-toTamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into consideration. This paper demonstrates our findings including the identification of several issues of the current neural approaches to low-resource domain-specific text translation. 1 Introduction In recent years, MT researchers have proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et"
2020.wat-1.22,P19-1021,0,0.415931,"text translation. 1 Introduction In recent years, MT researchers have proposed approaches to counter the data sparsity problem and to improve the performance of NMT systems in lowresource scenarios, e.g. augmenting training data from source and/or target monolingual corpora (Sennrich et al., 2016a; Chen et al., 2019), unsupervised learning strategies in the absence of labeled data (Artetxe et al., 2018; Lample et al., 2018), exploiting training data involving other languages (Firat et al., 2017; Johnson et al., 2017), multi-task learning (Niehues and Cho, 2017), selection of hyperparameters (Sennrich and Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fujita, 2018) due to the difficulty of training unsupervised cross-lingual word embeddings for such languages (Søgaard et al"
2020.wat-1.22,2006.amta-papers.25,0,0.291563,"Missing"
2020.wat-1.22,P18-1072,0,0.0156858,"Zhang, 2019), and pre-trained language model fine-tuning (Liu et al., 2020). Despite some success, none of the existing benchmarks can be viewed as an overall solution as far as MT for low-resource language-pairs is concerned. For examples, the back-translation strategy of Sennrich et al. (2016a) is less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017); unsupervised MT does not work well for distant languages (Marie and Fujita, 2018) due to the difficulty of training unsupervised cross-lingual word embeddings for such languages (Søgaard et al., 2018) and the same is applicable in the case of transfer learning too (Montoya et al., 2019). To this end, we investigate the performance of PB-SMT and NMT systems on two rarely-tested under-resourced language-pairs, English-to-Tamil and Hindi-to-Tamil, taking a specialised data domain (software localisation) into account. In this context, in Ramesh et al. (2020), we investigated the performance of PB-SMT, NMT and a commercial MT system (Google Translate (GT))1 on English-to-Tamil taking the software localisation data into account, i.e. the same data as the one used in this work. In particular, in"
2020.wat-1.22,tiedemann-2012-parallel,0,0.0498297,"//github.com/CLD2Owners/cld2 3 train sets words [Ta] MIXED 1,00,047 vocab avg. sent 1,705,034 104,564 17 1,196,008 284,921 14 IT vocab avg. sent 48,461 3,54,426 31,258 8 2,76,514 67,069 7 1,500 10,903 7,879 devset testset 1,500 9,362 English-to-Tamil sents. words [En] words [Ta] 5,355,103 424,701 25 4,066,449 423,599 19 68,352 448,966 31,216 7 407,832 77,323 6 devset 1,500 17,903 13,879 testset 1,500 16,020 12,925 train sets MIXED 222,367 vocab avg. sent 6,748 IT vocab avg. sent Data Used This section presents our datasets. For experimentation we used data from three different sources: OPUS2 (Tiedemann, 2012), WikiMatrix3 (Schwenk et al., 2019) and PMIndia4 (Haddow and Kirefu, 2020). As mentioned above, we carried out experiments on two translation-pairs, English-to-Tamil and Hindi-to-Tamil, and study translation of a specialised domain data, i.e. software localisation. Corpus statistics are shown in Table 1. We carried out experiments using two different setups: (i) in the first setup, the MT systems were built on a training set compiled from all data domains listed above; we call this setup MIXED, and (ii) in the second setup, the MT systems were built on a training set compiled only from differ"
2020.wat-1.22,E17-1100,0,0.0210634,"ecember 4, 2020. 2020 Association for Computational Linguistics of this work is to study translations of the MT systems (PB-SMT and NMT) in under-resourced conditions, we provide a brief overview on some of the papers that compared PB-SMT and NMT on highresource settings too. Junczys-Dowmunt et al. (2016) compare PB-SMT and NMT on a range of translation-pairs and show that for all translation directions NMT is either on par with or surpasses PB-SMT. Bentivogli et al. (2016) analyse the output of MT systems in an Englishto-German translation task by considering different linguistic categories. Toral and Sánchez-Cartagena (2017) conduct an evaluation to compare NMT and PB-SMT outputs across broader aspects (e.g. fluency, reordering) for 9 language directions. Castilho et al. (2017) conduct an extensive qualitative and quantitative comparative evaluation of PB-SMT and NMT using automatic metrics and professional translators. Popović (2017) carries out an extensive comparison between NMT and PB-SMT languagerelated issues for the German–English language pair in both translation directions. These works (Bentivogli et al., 2016; Castilho et al., 2017; Popović, 2017; Toral and Sánchez-Cartagena, 2017) show that NMT provide"
2020.wmt-1.27,D11-1033,0,0.131485,"monolingual data with the updated MT system and appending the resultant synthetic data to the original training data in each iteration. 2.2 Selecting pseudo In-Domain Sentences In an attempt to improve the quality of our NMT engines, we extracted monolingual sentences from large monolingual data that are similar to the styles of the in-domain data. Sentences of a large monolingual corpus similar to the in-domain sentences when selected based on the perplexity according to an in-domain language model were found to be effective in MT (Gao et al., 2002; Yasuda et al., 2008; Foster et al., 2010; Axelrod et al., 2011; Toral, 2013). As for NMT training, we believe that synthetic parallel data created using pseudo in-domain sentences can be better alternatives than those selected randomly. Accordingly, we select “pseudo” in-domain sentences from a large monolingual corpus based on the perplexity scores according to the in-domain language models. The extracted sentences are then back-translated with a target-to-source MT system to form synthetic training data. 2.3 Mining Monolingual Sentences for the Adaptation of the NMT models Chinea-R´ıos et al. (2017) demonstrated that in case of specialised domains or l"
2020.wmt-1.27,W19-5301,0,0.0588229,"Missing"
2020.wmt-1.27,W18-6315,0,0.233909,"olingual sentences for the creation of synthetic bitexts, mining monolingual source and target sentences for the adaptation of neural MT (NMT) systems, finding the optimal set of hyperparameters for Transformer as far as low-resource translation is concerned. The remainder of the paper is organized as follows. In Section 2, we present our approaches for the MT system building. Section 3 first presents details of the data sets used and then presents the evaluation results with some discussions, while Data Augmentation The data augmentation methods (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Bogoychev and Sennrich, 2019; Caswell et al., 2019; Chen et al., 2019), which usually employ the unlabeled monolingual data in addition to limited bitexts, can positively impact the MT system’s performance and are very popular among the MT developers and researchers (Barrault et al., 2019). In other words, use of augmented bitexts that include synthetic data to improve a NMT system is nowadays a common practice, especially in the under-resource scenarios. The synthetic training data whose target-side sentences are original is more effective for domain text translation"
2020.wmt-1.27,W19-5206,0,0.422198,"the baseline yields an excellent performance in the English-to-Tamil and Tamil-to-English translation tasks. 1 2 Our Strategies to improve MT Systems 2.1 Introduction The ADAPT Centre participated in the News Translation Shared Task of the Fifth Conference of Machine Translation (WMT20) in the English-toTamil and Tamil-to-English language directions. To build our neural MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task include applying the state-of-the-art data augmentation approaches (e.g. (Sennrich et al., 2016a; Caswell et al., 2019)), selecting “pseudo” indomain monolingual sentences for the creation of synthetic bitexts, mining monolingual source and target sentences for the adaptation of neural MT (NMT) systems, finding the optimal set of hyperparameters for Transformer as far as low-resource translation is concerned. The remainder of the paper is organized as follows. In Section 2, we present our approaches for the MT system building. Section 3 first presents details of the data sets used and then presents the evaluation results with some discussions, while Data Augmentation The data augmentation methods (Sennrich et"
2020.wmt-1.27,D19-5213,0,0.204602,"tences for the adaptation of neural MT (NMT) systems, finding the optimal set of hyperparameters for Transformer as far as low-resource translation is concerned. The remainder of the paper is organized as follows. In Section 2, we present our approaches for the MT system building. Section 3 first presents details of the data sets used and then presents the evaluation results with some discussions, while Data Augmentation The data augmentation methods (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Bogoychev and Sennrich, 2019; Caswell et al., 2019; Chen et al., 2019), which usually employ the unlabeled monolingual data in addition to limited bitexts, can positively impact the MT system’s performance and are very popular among the MT developers and researchers (Barrault et al., 2019). In other words, use of augmented bitexts that include synthetic data to improve a NMT system is nowadays a common practice, especially in the under-resource scenarios. The synthetic training data whose target-side sentences are original is more effective for domain text translation and generation of fluent translations. In this task, in order to improve our baseline Transform"
2020.wmt-1.27,W17-4714,0,0.0764206,"Missing"
2020.wmt-1.27,W17-4713,0,0.0203426,"test set sentences to be translated could be effective for fine-tuning the original general domain NMT model. They select those instances from large monolingual corpus whose vector-space representation is similar to the representation of the test set instances. The selected sentences are then automatically translated by an NMT system that is trained on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017, 2018; Wuebker et al., 2018; Huck et al., 2019). In our case, since English–Tamil is a lowresource language-pair and have a little amount of bitexts pertaining to the targeted domain (News), we followed Chinea-R´ıos et al. (2017) and mined those sentences from large monolingual data that can be beneficial for fine-tuning the original NMT models. In addition to mining source-side sentences (Chinea-R´ıos et al., 2017), we also mined target language sentences from large monolingual corpus (Huck et al., 2019) when English is the source language. However, our selection methods are different to tho"
2020.wmt-1.27,P05-1045,0,0.105534,"intuition is to extract those terms or sequence of words from the test set that do not occur or rarely occur in the training set and convey representativeness of the test set. We merged the two sets of terms extracted following the two setups above. Given the resultant list of terms, we mine sentences from monolingual corpus. We observed that the WMT20 News development text contains many named entities (NEs) and many of them are out-of-vocabulary items. We also found that our initial MT systems miserably failed to translate many NEs. Therefore, we used Stanford named entity recogniser (NER)2 (Finkel et al., 2005) in order to identify NEs in the English test set. As above, we used the extracted NEs in order to mine sentences from a large monolingual corpus. 263 2 https://nlp.stanford.edu/software/CRF-NER.html We build an English-to-Tamil transliteration system and the extracted English NEs were transliterated into Tamil. Note that we took 5-best Tamil translations for an English NE as in Huck et al. (2019). These Tamil NEs were then used to mine Tamil sentences from a large target monolingual corpus. In order to build the English-to-Tamil transliteration system, we used the 2016 Named Entity Transliter"
2020.wmt-1.27,W14-4806,1,0.627001,"guage. However, our selection methods are different to those of the other papers (ChineaR´ıos et al., 2017; Farajian et al., 2017, 2018; Wuebker et al., 2018; Huck et al., 2019) and are described below. Terms are usually indicators of the nature of a domain and play a critical role in domain-specific MT (Haque et al., 2020). The target translation could lose its meaning if the terminology translation is not dealt with care. Therefore, we focused on mining those sentences from a large monolingual corpus that contain domain terms. For this, we made use the approach of Rayson and Garside (2000); Haque et al. (2014, 2018) for identifying terms in the test set which is to be translated. This term extraction method performs well even on a small amount of sentences (Haque et al., 2014, 2018). The goal is to identify those words which are most indicative (or characteristic) of the test corpus compared to a reference corpus. Haque et al. (2014, 2018) used a large corpus which is generic in nature as a reference corpus. We adopted their approach and used a large generic corpus in order to identify terms in the test set. Additionally, in our second setup, we used the training set on which the NMT systems were"
2020.wmt-1.27,W18-2703,0,0.0256453,"s not bring any improvements in the Tamil-toEnglish translation task, and deteriorates the performance of the MT systems in the English-to-Tamil translation task. Iterative generation and training on synthetic data can yield increasingly better NMT systems, 1 Synthetic data for training is created by the MT system itself (i.e. source-side is original) (Zhang and Zong, 2016; Burlot and Yvon, 2018). 262 Proceedings of the 5th Conference on Machine Translation (WMT), pages 262–268 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics especially in low-resource scenarios (Hoang et al., 2018; Chen et al., 2019). Similarly, in order to produce our final English-to-Tamil and Tamil-toEnglish MT systems, we performed iterative training by back-translating new monolingual data with the updated MT system and appending the resultant synthetic data to the original training data in each iteration. 2.2 Selecting pseudo In-Domain Sentences In an attempt to improve the quality of our NMT engines, we extracted monolingual sentences from large monolingual data that are similar to the styles of the in-domain data. Sentences of a large monolingual corpus similar to the in-domain sentences when s"
2020.wmt-1.27,P19-1581,0,0.328266,"tive for fine-tuning the original general domain NMT model. They select those instances from large monolingual corpus whose vector-space representation is similar to the representation of the test set instances. The selected sentences are then automatically translated by an NMT system that is trained on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017, 2018; Wuebker et al., 2018; Huck et al., 2019). In our case, since English–Tamil is a lowresource language-pair and have a little amount of bitexts pertaining to the targeted domain (News), we followed Chinea-R´ıos et al. (2017) and mined those sentences from large monolingual data that can be beneficial for fine-tuning the original NMT models. In addition to mining source-side sentences (Chinea-R´ıos et al., 2017), we also mined target language sentences from large monolingual corpus (Huck et al., 2019) when English is the source language. However, our selection methods are different to those of the other papers (ChineaR´ıos et al., 2017"
2020.wmt-1.27,D10-1044,0,0.0928099,"Missing"
2020.wmt-1.27,P18-4020,0,0.0485733,"Missing"
2020.wmt-1.27,W09-3523,1,0.715334,"ntences from a large monolingual corpus. 263 2 https://nlp.stanford.edu/software/CRF-NER.html We build an English-to-Tamil transliteration system and the extracted English NEs were transliterated into Tamil. Note that we took 5-best Tamil translations for an English NE as in Huck et al. (2019). These Tamil NEs were then used to mine Tamil sentences from a large target monolingual corpus. In order to build the English-to-Tamil transliteration system, we used the 2016 Named Entity Transliteration Shared Task (NEWS) dataset3 (Duan et al., 2016). We used our in-house machine transliteration tool (Haque et al., 2009) in order to prepare the English-to-Tamil transliteration system. We could not apply this strategy in the Tamilto-English translation task since there is no publicly available NER for Tamil. The source and target sentences that have been mined are translated with the final source-to-target and target-to-source NMT systems, respectively. This results in a set of synthetic sentence-pairs. Source sentences whose target-side is original are tagged with a special token (Caswell et al., 2019) (cf. Section 2.1). As in Chinea-R´ıos et al. (2017), the original MT system is finally fine-tuned on these s"
2020.wmt-1.27,W18-6319,0,0.0154004,"ing data. ing process (cf. Section 2.1) when there were no significant improvements in terms of the test set BLEU scores. This training process provides us with the improved MT systems. As can be seen from Table 3, the final MT systems surpass the respective baseline MT systems with large margins. We translate the blind test sets (newstest2020) for the English-to-Tamil and Tamil-to-English translation tasks released by WMT20 by the best MT systems (cf. Table 3). The blind test sets for the English-to-Tamil and Tamil-to-English tasks contain 6,988 and 997 segments, respectively. The sacreBLEU (Post, 2018) scores of the best NMT systems on newstest2020 are shown in the last column of Table 3.12 3.4 We applied the pseudo in-domain sentence selection strategy described in Section 2.2 to the monolingual corpora (cf. Table 1), and considered the top-scored sentences for back-translation. Note that the in-domain language models for sentence 8 English-to-Tamil Base+ 800K Base+ 1.5M Base+ 2.7M Tamil-to-English Base + 800K Base + 1.5M Base + 2.7M Base + 3.3M The Baseline MT Systems The BLUE scores of the NMT systems trained on the authentic parallel corpus (cf. Table 1) are reported in Table 2. These B"
2020.wmt-1.27,W00-0901,0,0.0541365,"n English is the source language. However, our selection methods are different to those of the other papers (ChineaR´ıos et al., 2017; Farajian et al., 2017, 2018; Wuebker et al., 2018; Huck et al., 2019) and are described below. Terms are usually indicators of the nature of a domain and play a critical role in domain-specific MT (Haque et al., 2020). The target translation could lose its meaning if the terminology translation is not dealt with care. Therefore, we focused on mining those sentences from a large monolingual corpus that contain domain terms. For this, we made use the approach of Rayson and Garside (2000); Haque et al. (2014, 2018) for identifying terms in the test set which is to be translated. This term extraction method performs well even on a small amount of sentences (Haque et al., 2014, 2018). The goal is to identify those words which are most indicative (or characteristic) of the test corpus compared to a reference corpus. Haque et al. (2014, 2018) used a large corpus which is generic in nature as a reference corpus. We adopted their approach and used a large generic corpus in order to identify terms in the test set. Additionally, in our second setup, we used the training set on which t"
2020.wmt-1.27,P16-1009,0,0.377595,"mentioned techniques to the baseline yields an excellent performance in the English-to-Tamil and Tamil-to-English translation tasks. 1 2 Our Strategies to improve MT Systems 2.1 Introduction The ADAPT Centre participated in the News Translation Shared Task of the Fifth Conference of Machine Translation (WMT20) in the English-toTamil and Tamil-to-English language directions. To build our neural MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task include applying the state-of-the-art data augmentation approaches (e.g. (Sennrich et al., 2016a; Caswell et al., 2019)), selecting “pseudo” indomain monolingual sentences for the creation of synthetic bitexts, mining monolingual source and target sentences for the adaptation of neural MT (NMT) systems, finding the optimal set of hyperparameters for Transformer as far as low-resource translation is concerned. The remainder of the paper is organized as follows. In Section 2, we present our approaches for the MT system building. Section 3 first presents details of the data sets used and then presents the evaluation results with some discussions, while Data Augmentation The data augmentati"
2020.wmt-1.27,P16-1162,0,0.452614,"mentioned techniques to the baseline yields an excellent performance in the English-to-Tamil and Tamil-to-English translation tasks. 1 2 Our Strategies to improve MT Systems 2.1 Introduction The ADAPT Centre participated in the News Translation Shared Task of the Fifth Conference of Machine Translation (WMT20) in the English-toTamil and Tamil-to-English language directions. To build our neural MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task include applying the state-of-the-art data augmentation approaches (e.g. (Sennrich et al., 2016a; Caswell et al., 2019)), selecting “pseudo” indomain monolingual sentences for the creation of synthetic bitexts, mining monolingual source and target sentences for the adaptation of neural MT (NMT) systems, finding the optimal set of hyperparameters for Transformer as far as low-resource translation is concerned. The remainder of the paper is organized as follows. In Section 2, we present our approaches for the MT system building. Section 3 first presents details of the data sets used and then presents the evaluation results with some discussions, while Data Augmentation The data augmentati"
2020.wmt-1.27,P19-1021,0,0.0264929,"3,047 test 1,000 23,259 17,966 dev. 989 23,415 17,901 Monolingual Data English 17M Tamil 31M Tuning Hyperparameters for Transformer The NMT systems are Transformer models (Vaswani et al., 2017). To build our NMT systems, we used the MarianNMT (Junczys-Dowmunt et al., 2018) toolkit. The tokens of the training, evaluation and validation sets are segmented into sub-word units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016b). Since English and Tamil are written in their own scripts and have no overlapping characters, BPE is applied individually on the source and target languages. Recently, Sennrich and Zhang (2019) demonstrated that commonly used hyperparameter configuration do not lead to the best results in low-resource settings. Accordingly, we carried out a series of experiments in order to find the best hyperparameter configuration for Trans3 former in our low-resource setting.4 In particular, we played with some of the hyperparameters, and found that the following configuration lead to the best results in our low-resource translation settings: (i) the BPE vocabulary size: 8,000, (ii) the sizes of the encoder and decoder layers: 4 and 6, respectively, (iii) learning-rate: 0.0005, (iv) dropout (Gal"
2020.wmt-1.27,tiedemann-2012-parallel,0,0.128916,"Missing"
2020.wmt-1.27,W13-2803,0,0.192302,"the updated MT system and appending the resultant synthetic data to the original training data in each iteration. 2.2 Selecting pseudo In-Domain Sentences In an attempt to improve the quality of our NMT engines, we extracted monolingual sentences from large monolingual data that are similar to the styles of the in-domain data. Sentences of a large monolingual corpus similar to the in-domain sentences when selected based on the perplexity according to an in-domain language model were found to be effective in MT (Gao et al., 2002; Yasuda et al., 2008; Foster et al., 2010; Axelrod et al., 2011; Toral, 2013). As for NMT training, we believe that synthetic parallel data created using pseudo in-domain sentences can be better alternatives than those selected randomly. Accordingly, we select “pseudo” in-domain sentences from a large monolingual corpus based on the perplexity scores according to the in-domain language models. The extracted sentences are then back-translated with a target-to-source MT system to form synthetic training data. 2.3 Mining Monolingual Sentences for the Adaptation of the NMT models Chinea-R´ıos et al. (2017) demonstrated that in case of specialised domains or low-resource sc"
2020.wmt-1.27,P07-1004,0,0.211005,"wadays a common practice, especially in the under-resource scenarios. The synthetic training data whose target-side sentences are original is more effective for domain text translation and generation of fluent translations. In this task, in order to improve our baseline Transformer models, we augmented our training data with the target-original synthetic data. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the back-translated source sentences with an extra token. Note that we also tried applying the so-called self-training1 strategy (Ueffing et al., 2007) to improve our NMT systems. However, this method does not bring any improvements in the Tamil-toEnglish translation task, and deteriorates the performance of the MT systems in the English-to-Tamil translation task. Iterative generation and training on synthetic data can yield increasingly better NMT systems, 1 Synthetic data for training is created by the MT system itself (i.e. source-side is original) (Zhang and Zong, 2016; Burlot and Yvon, 2018). 262 Proceedings of the 5th Conference on Machine Translation (WMT), pages 262–268 c Online, November 19–20, 2020. 2020 Association for Computation"
2020.wmt-1.27,D18-1104,0,0.0605242,"nslated could be effective for fine-tuning the original general domain NMT model. They select those instances from large monolingual corpus whose vector-space representation is similar to the representation of the test set instances. The selected sentences are then automatically translated by an NMT system that is trained on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017, 2018; Wuebker et al., 2018; Huck et al., 2019). In our case, since English–Tamil is a lowresource language-pair and have a little amount of bitexts pertaining to the targeted domain (News), we followed Chinea-R´ıos et al. (2017) and mined those sentences from large monolingual data that can be beneficial for fine-tuning the original NMT models. In addition to mining source-side sentences (Chinea-R´ıos et al., 2017), we also mined target language sentences from large monolingual corpus (Huck et al., 2019) when English is the source language. However, our selection methods are different to those of the other papers (Chin"
2020.wmt-1.27,I08-2088,0,0.0283711,"iterative training by back-translating new monolingual data with the updated MT system and appending the resultant synthetic data to the original training data in each iteration. 2.2 Selecting pseudo In-Domain Sentences In an attempt to improve the quality of our NMT engines, we extracted monolingual sentences from large monolingual data that are similar to the styles of the in-domain data. Sentences of a large monolingual corpus similar to the in-domain sentences when selected based on the perplexity according to an in-domain language model were found to be effective in MT (Gao et al., 2002; Yasuda et al., 2008; Foster et al., 2010; Axelrod et al., 2011; Toral, 2013). As for NMT training, we believe that synthetic parallel data created using pseudo in-domain sentences can be better alternatives than those selected randomly. Accordingly, we select “pseudo” in-domain sentences from a large monolingual corpus based on the perplexity scores according to the in-domain language models. The extracted sentences are then back-translated with a target-to-source MT system to form synthetic training data. 2.3 Mining Monolingual Sentences for the Adaptation of the NMT models Chinea-R´ıos et al. (2017) demonstrat"
2020.wmt-1.27,D16-1160,0,0.191637,"“pseudo” indomain monolingual sentences for the creation of synthetic bitexts, mining monolingual source and target sentences for the adaptation of neural MT (NMT) systems, finding the optimal set of hyperparameters for Transformer as far as low-resource translation is concerned. The remainder of the paper is organized as follows. In Section 2, we present our approaches for the MT system building. Section 3 first presents details of the data sets used and then presents the evaluation results with some discussions, while Data Augmentation The data augmentation methods (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Bogoychev and Sennrich, 2019; Caswell et al., 2019; Chen et al., 2019), which usually employ the unlabeled monolingual data in addition to limited bitexts, can positively impact the MT system’s performance and are very popular among the MT developers and researchers (Barrault et al., 2019). In other words, use of augmented bitexts that include synthetic data to improve a NMT system is nowadays a common practice, especially in the under-resource scenarios. The synthetic training data whose target-side sentences are original is more effective for d"
2020.wmt-1.91,D11-1033,0,0.608844,"organisers released parallel training data with a limited number of in-domain examples (only 24,247). The organisers also provided the participants with moderate-sized three out-of-domain corpora (totalling to approximately 770K bitexts). In an attempt to improve the quality of our baseline MT systems, we extracted those sentence-pairs from the out-of-domain corpora that are similar to the styles and domain of the texts we aim to translate, and were used in system building. 2.1.1 Selection using Bilingual Cross-Entropy Difference We followed the state-of-the-art sentence selection approach of Axelrod et al. (2011) that extracts pseudo in-domain sentences from out-of-domain 841 Proceedings of the 5th Conference on Machine Translation (WMT), pages 841–848 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics corpora using bilingual cross-entropy difference over each side of the corpus (source and target). The bilingual cross-entropy difference is computed by querying in- and out-of-domain (source and target) language models. 2.1.2 Selection using Terminology Terms are usually indicators of the nature of a domain and plays a critical role in domain-specific MT (Haque et al., 2020)"
2020.wmt-1.91,W19-5301,0,0.0566893,"Missing"
2020.wmt-1.91,W18-6315,0,0.209925,"we participated in the English-to-Basque translation task. To make the readers familiar with the biomedical translation task and to understand the challenges of this task, we show a couple of examples from the blind test set and two terminological expressions from terminology test set in Table 1. For building our MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task roughly include (i) pseudo in-domain parallel and monolingual data selection, (ii) augmenting training data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019; Chen et al., 2019), (iii) mining 2 Our Approaches 2.1 Selecting pseudo In-domain Parallel Sentences The shared task organisers released parallel training data with a limited number of in-domain examples (only 24,247). The organisers also provided the participants with moderate-sized three out-of-domain corpora (totalling to approximately 770K bitexts). In an attempt to improve the quality of our baseline MT systems, we extracted those sentence-pairs from the out-of-domain corpora that are similar to the styles and domain of the texts we aim to tra"
2020.wmt-1.91,W19-5206,0,0.234823,"slation task. To make the readers familiar with the biomedical translation task and to understand the challenges of this task, we show a couple of examples from the blind test set and two terminological expressions from terminology test set in Table 1. For building our MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task roughly include (i) pseudo in-domain parallel and monolingual data selection, (ii) augmenting training data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019; Chen et al., 2019), (iii) mining 2 Our Approaches 2.1 Selecting pseudo In-domain Parallel Sentences The shared task organisers released parallel training data with a limited number of in-domain examples (only 24,247). The organisers also provided the participants with moderate-sized three out-of-domain corpora (totalling to approximately 770K bitexts). In an attempt to improve the quality of our baseline MT systems, we extracted those sentence-pairs from the out-of-domain corpora that are similar to the styles and domain of the texts we aim to translate, and were used in system building. 2.1"
2020.wmt-1.91,D19-5213,0,0.479031,"the readers familiar with the biomedical translation task and to understand the challenges of this task, we show a couple of examples from the blind test set and two terminological expressions from terminology test set in Table 1. For building our MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task roughly include (i) pseudo in-domain parallel and monolingual data selection, (ii) augmenting training data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019; Chen et al., 2019), (iii) mining 2 Our Approaches 2.1 Selecting pseudo In-domain Parallel Sentences The shared task organisers released parallel training data with a limited number of in-domain examples (only 24,247). The organisers also provided the participants with moderate-sized three out-of-domain corpora (totalling to approximately 770K bitexts). In an attempt to improve the quality of our baseline MT systems, we extracted those sentence-pairs from the out-of-domain corpora that are similar to the styles and domain of the texts we aim to translate, and were used in system building. 2.1.1 Selection using B"
2020.wmt-1.91,W17-4714,0,0.123168,"Missing"
2020.wmt-1.91,W17-4713,0,0.127594,"ted to the test set sentences to be translated could be effective for fine-tuning the original general domain NMT model. They select those instances from large monolingual corpus whose vector-space representation is similar to the representation of the test set instances. The selected sentences are then automatically translated by an NMT system built on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017, 2018; Wuebker et al., 2018; Huck et al., 2019). 2.3.1 Mining Source Language Monolingual Sentences Since English–Basque is a low-resource languagepair and have a little amount of bitexts pertaining to the targeted domain (biomedical), we followed Chinea-R´ıos et al. (2017) in order to mine those sentences from large monolingual data that could be beneficial for fine-tuning the original NMT models. In other words, we followed the method described in Section 2.1.2 in order to extract sentences form large monolingual corpus. As above, we identify terms in the test set (i.e. scientific abstracts"
2020.wmt-1.91,D10-1044,0,0.10819,"Missing"
2020.wmt-1.91,W14-4806,1,0.830991,"m (e.g. ‘cold’) could have many translation equivalents in a target language. In our second sentence selection approach, we mine those sentences from large out-of-domain or general domain corpus that contain domain terms. As pointed out above, an extracted sentence that contain a domain term may not represent the desired domain; however, the training examples that include such extracted sentences may play crucial role in minimising lexical selection errors as far as terminology translation is concerned (Haque et al., 2020). To this end, we exploit the approach of Rayson and Garside (2000) and Haque et al. (2014, 2018) in order to automatically identify terms in the indomain texts. The idea is to identify those words which are most indicative (or characteristic) of the in-domain corpus compared to a reference corpus. Haque et al. (2014, 2018) used a large corpus which is generic in nature as a reference corpus. We adopted their approach and used a large generic corpus in order to identify terms in the in-domain source (English) and target (Basque) corpora. Given the lists of source and target terms, we mine sentences independently from the sourceand target-sides of the out-of-domain bilingual corpus."
2020.wmt-1.91,W18-2703,0,0.0279544,"Chen et al. (2019); Bogoychev and Sennrich (2019)) have shown that self-training and back-translation can be complementary to each other. In this task, in order to improve our baseline Transformer models, we augmented our training data with both the target- and source-original synthetic data. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the source sentences of the synthetic data with the extra tokens. Iterative generation and training on synthetic data can yield increasingly better NMT systems, especially in low-resource scenarios (Hoang et al., 2018; Chen et al., 2019). Since our baseline sourceto-target and target-to-source MT systems are already excellent in quality, those were used to translate the monolingual data. As in Section 2.1, we extract those sentences from large monolingual data that are similar to the styles of texts we aim to translate. We used the extracted pseudo in-domain monolingual sentences to produce the source- and target-original synthetic bitexts. As for the NMT training, we believe that synthetic parallel data created from pseudo in-domain sentences could be the better alternatives than those selected randomly."
2020.wmt-1.91,P19-1581,0,0.0695156,"d be effective for fine-tuning the original general domain NMT model. They select those instances from large monolingual corpus whose vector-space representation is similar to the representation of the test set instances. The selected sentences are then automatically translated by an NMT system built on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017, 2018; Wuebker et al., 2018; Huck et al., 2019). 2.3.1 Mining Source Language Monolingual Sentences Since English–Basque is a low-resource languagepair and have a little amount of bitexts pertaining to the targeted domain (biomedical), we followed Chinea-R´ıos et al. (2017) in order to mine those sentences from large monolingual data that could be beneficial for fine-tuning the original NMT models. In other words, we followed the method described in Section 2.1.2 in order to extract sentences form large monolingual corpus. As above, we identify terms in the test set (i.e. scientific abstracts of Medline) to be translated. As for the sub-ta"
2020.wmt-1.91,P18-4020,0,0.0370192,"Missing"
2020.wmt-1.91,W04-3250,0,0.501053,"Missing"
2020.wmt-1.91,C10-1096,0,0.0964596,"Missing"
2020.wmt-1.91,P02-1040,0,0.106786,"un. 3 Since the size of English in-domain monolingual corpus is reasonably big, we did not use any English out-of-domain data for system building. In order to perform tokenisation for English and Basque texts, we used the standard tool of the Moses toolkit. The development data released by the task organisers contains 2,000 sentences (cf. fourth row of Table 2), out of which 1,000 sentences are used as the test set. The remaining sentences of the development set are used for validation. 4 This section presents the performance of our MT systems in terms of the automatic evaluation metric BLEU (Papineni et al., 2002). Additionally, we performed statistical significance tests using bootstrap resampling methods (Koehn, 2004). 4.1 The Baseline MT System First, we build an English-to-Basque NMT system on the in-domain parallel corpus (cf. Table 2) only, and we refer the MT system as Base. Note that size of the original test set is 1,000 and its sentences were randomly sampled from development set released by the organisers (cf. Section 3). We evaluate Base on the original test set and report its BLEU score in Table 3. As far as the BLEU score on original test set is concerned, it is excessively high. When we"
2020.wmt-1.91,W00-0901,0,0.072829,"rs. Moreover, a polysemous term (e.g. ‘cold’) could have many translation equivalents in a target language. In our second sentence selection approach, we mine those sentences from large out-of-domain or general domain corpus that contain domain terms. As pointed out above, an extracted sentence that contain a domain term may not represent the desired domain; however, the training examples that include such extracted sentences may play crucial role in minimising lexical selection errors as far as terminology translation is concerned (Haque et al., 2020). To this end, we exploit the approach of Rayson and Garside (2000) and Haque et al. (2014, 2018) in order to automatically identify terms in the indomain texts. The idea is to identify those words which are most indicative (or characteristic) of the in-domain corpus compared to a reference corpus. Haque et al. (2014, 2018) used a large corpus which is generic in nature as a reference corpus. We adopted their approach and used a large generic corpus in order to identify terms in the in-domain source (English) and target (Basque) corpora. Given the lists of source and target terms, we mine sentences independently from the sourceand target-sides of the out-of-d"
2020.wmt-1.91,P16-1009,0,0.650519,"task addresses a number of language pairs, and we participated in the English-to-Basque translation task. To make the readers familiar with the biomedical translation task and to understand the challenges of this task, we show a couple of examples from the blind test set and two terminological expressions from terminology test set in Table 1. For building our MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task roughly include (i) pseudo in-domain parallel and monolingual data selection, (ii) augmenting training data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019; Chen et al., 2019), (iii) mining 2 Our Approaches 2.1 Selecting pseudo In-domain Parallel Sentences The shared task organisers released parallel training data with a limited number of in-domain examples (only 24,247). The organisers also provided the participants with moderate-sized three out-of-domain corpora (totalling to approximately 770K bitexts). In an attempt to improve the quality of our baseline MT systems, we extracted those sentence-pairs from the out-of-domain corpora that are similar to th"
2020.wmt-1.91,P16-1162,0,0.833789,"task addresses a number of language pairs, and we participated in the English-to-Basque translation task. To make the readers familiar with the biomedical translation task and to understand the challenges of this task, we show a couple of examples from the blind test set and two terminological expressions from terminology test set in Table 1. For building our MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task roughly include (i) pseudo in-domain parallel and monolingual data selection, (ii) augmenting training data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019; Chen et al., 2019), (iii) mining 2 Our Approaches 2.1 Selecting pseudo In-domain Parallel Sentences The shared task organisers released parallel training data with a limited number of in-domain examples (only 24,247). The organisers also provided the participants with moderate-sized three out-of-domain corpora (totalling to approximately 770K bitexts). In an attempt to improve the quality of our baseline MT systems, we extracted those sentence-pairs from the out-of-domain corpora that are similar to th"
2020.wmt-1.91,P19-1021,0,0.025913,"Tuning Hyperparameters for Transformer The NMT systems are Transformer models (Vaswani et al., 2017). To build our NMT systems, we used the MarianNMT (Junczys-Dowmunt et al., 2018) toolkit. The tokens of the training, evaluation and validation sets are segmented into sub-word units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016b). We found that performance of the Transformer model more-or-less similar whether BPE is applied individually or jointly on the source and target languages. We kept the former setup, i.e. BPE is applied individually on the source and target languages. Recently, Sennrich and Zhang (2019) demonstrated that commonly used hyperparameter configuration do not lead to the best results in 843 low-resource settings. Accordingly, we carried out a series of experiments in order to find the best hyperparameter configuration for Transformer in our low-resource setting. In particular, we played with some of the hyperparameters, and found that the following configuration lead to the best results in our low-resource translation settings: (i) the BPE vocabulary size: 6,000, (ii) the sizes of the encoder and decoder layers: 4 and 6, respectively, and (iii) learning-rate: 0.0003. The models ar"
2020.wmt-1.91,tiedemann-2012-parallel,0,0.0753759,"Missing"
2020.wmt-1.91,W13-2803,0,0.194674,"allel data created from pseudo in-domain sentences could be the better alternatives than those selected randomly. Training Data Augmentation The data augmentation methods in NMT (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Bogoychev and Sennrich, 2019; 842 2.2.1 Selection using Language Model Perplexity Sentences of a large monolingual corpus similar to the in-domain sentences when selected based on the perplexity according to an in-domain language model were found to be effective in MT (Gao et al., 2002; Yasuda et al., 2008; Foster et al., 2010; Axelrod et al., 2011; Toral, 2013). Accordingly, we select “pseudo” in-domain sentences from a large monolingual data based on their perplexity scores accoring to the in-domain language model, which are then translated to form synthetic training data. 2.2.2 Selection using Terminology We mine “pseudo” in-domain sentences from large monolingual corpora following the method described in Section 2.1.2. We select those sentences from the monolingual corpus that contain at least one domain term. For mining monolingual sentences we create an efficient Trie structure given the large monolingual data. The idea is to store indices of t"
2020.wmt-1.91,P07-1004,0,0.214732,"ast one domain term. 2.2 Caswell et al., 2019; Chen et al., 2019), which usually employ the unlabeled monolingual data in addition to limited bitexts, can positively impact translation quality and are very popular among the MT developers and researchers (Barrault et al., 2019). In other words, use of synthetic data to improve a NMT system is nowadays a common practice, especially in the under-resource scenarios. The synthetic training data whose source-side sentences are original is more effective for domain adaptation. The learning method that uses such training data is called self-training (Ueffing et al., 2007). The synthetic training data whose targetside is original is more effective for domain text translation and generation of fluent translations (Sennrich et al., 2016a). Many studies (e.g. Chen et al. (2019); Bogoychev and Sennrich (2019)) have shown that self-training and back-translation can be complementary to each other. In this task, in order to improve our baseline Transformer models, we augmented our training data with both the target- and source-original synthetic data. As in Caswell et al. (2019), in order to let the NMT model know that the given source is synthetic, we tag the source"
2020.wmt-1.91,D18-1104,0,0.0585942,"to be translated could be effective for fine-tuning the original general domain NMT model. They select those instances from large monolingual corpus whose vector-space representation is similar to the representation of the test set instances. The selected sentences are then automatically translated by an NMT system built on a general domain data. Finally, the NMT system is fine-tuned with the resultant synthetic data. In a similar line of research, it has also been shown that an NMT system built on general domain data can be fine-tuned using just a few sentences (Farajian et al., 2017, 2018; Wuebker et al., 2018; Huck et al., 2019). 2.3.1 Mining Source Language Monolingual Sentences Since English–Basque is a low-resource languagepair and have a little amount of bitexts pertaining to the targeted domain (biomedical), we followed Chinea-R´ıos et al. (2017) in order to mine those sentences from large monolingual data that could be beneficial for fine-tuning the original NMT models. In other words, we followed the method described in Section 2.1.2 in order to extract sentences form large monolingual corpus. As above, we identify terms in the test set (i.e. scientific abstracts of Medline) to be translate"
2020.wmt-1.91,I08-2088,0,0.0287856,"bitexts. As for the NMT training, we believe that synthetic parallel data created from pseudo in-domain sentences could be the better alternatives than those selected randomly. Training Data Augmentation The data augmentation methods in NMT (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Bogoychev and Sennrich, 2019; 842 2.2.1 Selection using Language Model Perplexity Sentences of a large monolingual corpus similar to the in-domain sentences when selected based on the perplexity according to an in-domain language model were found to be effective in MT (Gao et al., 2002; Yasuda et al., 2008; Foster et al., 2010; Axelrod et al., 2011; Toral, 2013). Accordingly, we select “pseudo” in-domain sentences from a large monolingual data based on their perplexity scores accoring to the in-domain language model, which are then translated to form synthetic training data. 2.2.2 Selection using Terminology We mine “pseudo” in-domain sentences from large monolingual corpora following the method described in Section 2.1.2. We select those sentences from the monolingual corpus that contain at least one domain term. For mining monolingual sentences we create an efficient Trie structure given the"
2020.wmt-1.91,D16-1160,0,0.254587,"of language pairs, and we participated in the English-to-Basque translation task. To make the readers familiar with the biomedical translation task and to understand the challenges of this task, we show a couple of examples from the blind test set and two terminological expressions from terminology test set in Table 1. For building our MT systems we used the Transformer model (Vaswani et al., 2017). Our strategies to build the competitive MT systems for the task roughly include (i) pseudo in-domain parallel and monolingual data selection, (ii) augmenting training data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018; Caswell et al., 2019; Chen et al., 2019), (iii) mining 2 Our Approaches 2.1 Selecting pseudo In-domain Parallel Sentences The shared task organisers released parallel training data with a limited number of in-domain examples (only 24,247). The organisers also provided the participants with moderate-sized three out-of-domain corpora (totalling to approximately 770K bitexts). In an attempt to improve the quality of our baseline MT systems, we extracted those sentence-pairs from the out-of-domain corpora that are similar to the styles and domain of"
2021.mtsummit-loresmt.15,2020.coling-main.304,0,0.0272306,"ontext of this study, we have demonstrated that extending an 8k in-domain baseline dataset by just 5k lines improved the BLEU score by 27 points. 1 Introduction Neural Machine Translation (NMT) has routinely outperformed Statistical Machine Translation (SMT) when large parallel datasets are available (Crego et al., 2016; Wu et al., 2016). Furthermore, Transformer based approaches have demonstrated impressive results in moderate low-resource scenarios (Lankford et al., 2021). NMT involving Transformer model development will improve the performance in specific domains of low-resource languages (Araabi and Monz, 2020). However, the benefits of NMT are less clear when using very low-resource Machine Translation (MT) on in-domain datasets of less than 10k lines. The Irish language is a primary example of a low-resource language that will benefit from such research. This paper reports the results for the MT system developed for the English–Irish shared task at LoResMT 2021 (Ojha et al., 2021). Relevant work is presented in the background section followed by an overview of the proposed approach. The empirical findings are outlined in the results section. Finally, the key findings are presented and discussed. 2"
2021.mtsummit-loresmt.15,P17-2061,0,0.028415,"2.2 Domain adaptation Domain adaptation is a proven approach in addressing the paucity of data in low-resource settings. Fine-tuning an out-of-domain model by further training with in-domain data is effective in improving the performance of translation models (Freitag and Al-Onaizan, 2016; Sennrich et al., 2016). With this approach an NMT model is initially trained using a large out-of-domain corpus. Once fully converged, the out-of-domain model is further trained by fine-tuning its parameters with a low resource in-domain corpus. A modification to this approach is known as mixed fine-tuning (Chu et al., 2017). With this technique, an NMT model is trained on out-of-domain data until fully converged. This serves as a base model which is further trained using the combined in-domain and out-of-domain datasets. 3 Proposed Approach Figure 1: Proposed Approach. Optimal hyperparameters are applied to Transformer models which are trained using one of several possible approaches. The training dataset composition is determined by the chosen approach. Models are subsequently evaluated using a suite of metrics. Hyperparameter optimization of Recurrent Neural Network (RNN) models in low-resource settings has pr"
2021.mtsummit-loresmt.15,P16-5005,0,0.0167681,"with models trained on an extended in-domain dataset. As part of this study, an English-Irish dataset of Covid related data, from the Health and Education domains, was developed. The highestperforming model used a Transformer architecture trained with an extended in-domain Covid dataset. In the context of this study, we have demonstrated that extending an 8k in-domain baseline dataset by just 5k lines improved the BLEU score by 27 points. 1 Introduction Neural Machine Translation (NMT) has routinely outperformed Statistical Machine Translation (SMT) when large parallel datasets are available (Crego et al., 2016; Wu et al., 2016). Furthermore, Transformer based approaches have demonstrated impressive results in moderate low-resource scenarios (Lankford et al., 2021). NMT involving Transformer model development will improve the performance in specific domains of low-resource languages (Araabi and Monz, 2020). However, the benefits of NMT are less clear when using very low-resource Machine Translation (MT) on in-domain datasets of less than 10k lines. The Irish language is a primary example of a low-resource language that will benefit from such research. This paper reports the results for the MT system"
2021.mtsummit-loresmt.15,P17-4012,0,0.0178974,"w Resource Languages Page 146 (a) BLEU (b) TER Figure 2: Translation performance of all approaches using Transformers with 2 heads 4.1.2 Infrastructure Models were developed using a lab of machines each of which has an AMD Ryzen 7 2700X processor, 16 GB memory, a 256 SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16 GB graphic cards and up to 27GB of memory when available (Bisong, 2019). Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT (Klein et al., 2017). 4.1.3 Metrics Automated metrics were used to determine the translation quality. All models were trained and evaluated using the BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and ChrF (Popovi´c, 2015) evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded. 4.2 Results Experimental results achieved using a Transformer architecture, with either 2 or 8 attention heads, are summarized in Table 3 and in Table 4. Clearl"
2021.mtsummit-loresmt.15,D18-2012,0,0.0117846,"er of attention heads, the number of layers and experimenting with regularization techniques such as dropout and label smoothing. Most importantly, the choice of subword model type and the vocabulary size were evaluated. In order to test the effectiveness of our approach, models were trained using three EnglishIrish parallel datasets: a general corpus of 52k lines from the Directorate General for Translation (DGT) and two in-domain corpora of Covid data (8k and 5k lines). All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece (Kudo and Richardson, 2018) subword model. The impact of using separate source and target subword models was not explored. Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 145 Approach Source Lines Covid baseline Covid extended Out-of-domain Fine-tuned Mixed fine-tuned Combined domains Baseline Baseline + Covid DGT Baseline + Covid Baseline + Covid Baseline + Covid DCU DCU + DGT DCU + DGT DCU + DGT 8k 13k 52k 65k 65k 65k Table 1: Datasets used in proposed approach Hyperparameter Learning rate Batch size Atten"
2021.mtsummit-loresmt.15,2021.mtsummit-research.5,1,0.819174,"n domains, was developed. The highestperforming model used a Transformer architecture trained with an extended in-domain Covid dataset. In the context of this study, we have demonstrated that extending an 8k in-domain baseline dataset by just 5k lines improved the BLEU score by 27 points. 1 Introduction Neural Machine Translation (NMT) has routinely outperformed Statistical Machine Translation (SMT) when large parallel datasets are available (Crego et al., 2016; Wu et al., 2016). Furthermore, Transformer based approaches have demonstrated impressive results in moderate low-resource scenarios (Lankford et al., 2021). NMT involving Transformer model development will improve the performance in specific domains of low-resource languages (Araabi and Monz, 2020). However, the benefits of NMT are less clear when using very low-resource Machine Translation (MT) on in-domain datasets of less than 10k lines. The Irish language is a primary example of a low-resource language that will benefit from such research. This paper reports the results for the MT system developed for the English–Irish shared task at LoResMT 2021 (Ojha et al., 2021). Relevant work is presented in the background section followed by an overvie"
2021.mtsummit-loresmt.15,P02-1040,0,0.109415,"els were developed using a lab of machines each of which has an AMD Ryzen 7 2700X processor, 16 GB memory, a 256 SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16 GB graphic cards and up to 27GB of memory when available (Bisong, 2019). Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT (Klein et al., 2017). 4.1.3 Metrics Automated metrics were used to determine the translation quality. All models were trained and evaluated using the BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and ChrF (Popovi´c, 2015) evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded. 4.2 Results Experimental results achieved using a Transformer architecture, with either 2 or 8 attention heads, are summarized in Table 3 and in Table 4. Clearly in the context of our low-resource experiments, it can be seen there is little performance difference using Transformer architectures with a differing numbe"
2021.mtsummit-loresmt.15,W15-3049,0,0.184943,"Missing"
2021.mtsummit-loresmt.15,P19-1021,0,0.0188497,"in data until fully converged. This serves as a base model which is further trained using the combined in-domain and out-of-domain datasets. 3 Proposed Approach Figure 1: Proposed Approach. Optimal hyperparameters are applied to Transformer models which are trained using one of several possible approaches. The training dataset composition is determined by the chosen approach. Models are subsequently evaluated using a suite of metrics. Hyperparameter optimization of Recurrent Neural Network (RNN) models in low-resource settings has previously demonstrated considerable performance improvements (Sennrich and Zhang, 2019). The extent to which such optimization techniques may be applied to Transformer models in similar low-resource scenarios was evaluated in a previous study (Lankford et al., 2021). Evaluations included modifying the number of attention heads, the number of layers and experimenting with regularization techniques such as dropout and label smoothing. Most importantly, the choice of subword model type and the vocabulary size were evaluated. In order to test the effectiveness of our approach, models were trained using three EnglishIrish parallel datasets: a general corpus of 52k lines from the Dire"
2021.mtsummit-loresmt.15,2006.amta-papers.25,0,0.00936496,"b of machines each of which has an AMD Ryzen 7 2700X processor, 16 GB memory, a 256 SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16 GB graphic cards and up to 27GB of memory when available (Bisong, 2019). Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT (Klein et al., 2017). 4.1.3 Metrics Automated metrics were used to determine the translation quality. All models were trained and evaluated using the BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and ChrF (Popovi´c, 2015) evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded. 4.2 Results Experimental results achieved using a Transformer architecture, with either 2 or 8 attention heads, are summarized in Table 3 and in Table 4. Clearly in the context of our low-resource experiments, it can be seen there is little performance difference using Transformer architectures with a differing number of attention heads. The l"
2021.mtsummit-research.5,W19-6620,0,0.0282745,"ning for digital engagement of low-resource languages. It has been shown that an out-of-the-box NMT system, trained on English-Irish data, achieves a lower translation quality compared with using a tailored SMT system (Dowling et Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 48 al, 2018). It is in this context that further research is required in the development of NMT for low-resource languages and the Irish language in particular. Most research on choosing subword models has focused on high resource languages (Ding et al., 2019; Gowda and May, 2020). In the context of developing models for English to Irish translation, there are no clear recommendations on the choice of subword model types. One of the objectives in this study is to identify which type of subword model performs best in this low resource scenario. 2 Background Native speakers of low-resource languages are often excluded from useful content since, more often than not, online content is not available to them in their language of choice. Such a digital divide and the resulting social exclusion experienced by second language speakers, such as refugees liv"
2021.mtsummit-research.5,W18-2202,1,0.852419,"ministration dataset. This includes staff notices, annual reports, website content, press releases and official correspondence. Parallel texts from the Digital Corpus of the European Parliament (DCEP) and the DGT are included in the training data. Crawled data, from sites of a similar domain are included. Furthermore a parallel corpus collected from Conradh na Gaeilge (CnaG), an Irish language organisation that promotes the Irish language, was included. The dataset was compiled as part of a previous study which carried out a preliminary comparison of SMT and NMT models for the Irish language (Dowling et al., 2018). 4.1.2 Infrastructure Models were developed using a lab of machines each of which has an AMD Ryzen 7 2700X processor, 16 GB memory, a 256 SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 52 Architecture BLEU ↑ TER ↓ ChrF3 ↑ Steps dgt-rnn-base dgt-rnn-bpe8k dgt-rnn-bpe16k dgt-rnn-bpe32k dgt-rnn-unigram 52.7 54.6 55.6 55.3 55.6 0.42 0.40 0.39 0.39 0.39 0.71 0.73 0.74 0.74 0.74 75k 85k 100k 95k 105k Runtime (hours) 4.47 5.07 5.58 4.67 5.07 kgCO2 0 0 0 0 0 Table 2:"
2021.mtsummit-research.5,2020.findings-emnlp.352,0,0.0331111,"gagement of low-resource languages. It has been shown that an out-of-the-box NMT system, trained on English-Irish data, achieves a lower translation quality compared with using a tailored SMT system (Dowling et Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 48 al, 2018). It is in this context that further research is required in the development of NMT for low-resource languages and the Irish language in particular. Most research on choosing subword models has focused on high resource languages (Ding et al., 2019; Gowda and May, 2020). In the context of developing models for English to Irish translation, there are no clear recommendations on the choice of subword model types. One of the objectives in this study is to identify which type of subword model performs best in this low resource scenario. 2 Background Native speakers of low-resource languages are often excluded from useful content since, more often than not, online content is not available to them in their language of choice. Such a digital divide and the resulting social exclusion experienced by second language speakers, such as refugees living in developed count"
2021.mtsummit-research.5,P17-4012,0,0.0611425,"k lines Architecture BLEU ↑ TER ↓ ChrF3 ↑ Steps pa-rnn-base pa-rnn-bpe8k pa-rnn-bpe16k pa-rnn-bpe32k pa-rnn-unigram 40.4 41.5 41.5 41.9 41.9 0.47 0.46 0.46 0.47 0.46 0.63 0.64 0.64 0.64 0.64 60k 110k 105k 100k 95k Runtime (hours) 2.13 4.16 3.78 2.88 2.75 kgCO2 0 0 0 0 0 Table 3: RNN performance on PA dataset of 88k lines development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16 GB graphic cards and up to 27GB of memory when available (Bisong, 2019). Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT (Klein et al., 2017). 4.1.3 Metrics As part of this study, several automated metrics were used to determine the translation quality. All models were trained and evaluated on both the DGT and PA datasets using the BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and ChrF (Popovi´c, 2015) evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded. 4.2 Results 4.2.1 Performance of subword models The impact on translation accuracy when choosing"
2021.mtsummit-research.5,P18-1007,0,0.0186699,"n of such attention layers in the translation architecture. Experiments in MT tasks show such models are better in quality due to greater parallelization while requiring significantly less time to train. 2.3 Subword Models Translation, by its nature, requires an open vocabulary and the use of subword models aims to address the fixed vocabulary problem associated with NMT. Rare and unknown words are encoded as sequences of subword units. By adapting the original Byte Pair Encoding (BPE) algorithm (Gage, 1994), the use of BPE submodels can improve translation performance (Sennrich et al., 2015; Kudo, 2018). Designed for NMT, SentencePiece, is a language-independent subword tokenizer that provides an open-source C++ and a Python implementation for subword units. An attractive feature of the tokenizer is that SentencePiece trains subword models directly from raw sentences (Kudo and Richardson, 2018). 2.3.1 Byte Pair Encoding compared with Unigram BPE and unigram language models are similar in that both encode text using fewer bits but each uses a different data compression principle (dictionary vs. entropy). In principle, we would expect the same benefits with the unigram language model as with B"
2021.mtsummit-research.5,D18-2012,0,0.0359262,"the use of subword models aims to address the fixed vocabulary problem associated with NMT. Rare and unknown words are encoded as sequences of subword units. By adapting the original Byte Pair Encoding (BPE) algorithm (Gage, 1994), the use of BPE submodels can improve translation performance (Sennrich et al., 2015; Kudo, 2018). Designed for NMT, SentencePiece, is a language-independent subword tokenizer that provides an open-source C++ and a Python implementation for subword units. An attractive feature of the tokenizer is that SentencePiece trains subword models directly from raw sentences (Kudo and Richardson, 2018). 2.3.1 Byte Pair Encoding compared with Unigram BPE and unigram language models are similar in that both encode text using fewer bits but each uses a different data compression principle (dictionary vs. entropy). In principle, we would expect the same benefits with the unigram language model as with BPE. However, unigram models are often more flexible since they are probabilistic models that output multiple segmentations with their probabilities. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 50 Figure 1: Propose"
2021.mtsummit-research.5,P02-1040,0,0.109613,"ime (hours) 2.13 4.16 3.78 2.88 2.75 kgCO2 0 0 0 0 0 Table 3: RNN performance on PA dataset of 88k lines development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16 GB graphic cards and up to 27GB of memory when available (Bisong, 2019). Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT (Klein et al., 2017). 4.1.3 Metrics As part of this study, several automated metrics were used to determine the translation quality. All models were trained and evaluated on both the DGT and PA datasets using the BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and ChrF (Popovi´c, 2015) evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded. 4.2 Results 4.2.1 Performance of subword models The impact on translation accuracy when choosing a subword model is highlighted in Tables 2 - 5. In training both RNN and Transformer architectures, incorporating any submodel type led to improvements in model accuracy. This finding is evident when training either the s"
2021.mtsummit-research.5,W15-3049,0,0.218534,"Missing"
2021.mtsummit-research.5,W15-3031,0,0.0220325,"Missing"
2021.nodalida-main.7,W18-6318,0,0.0193765,"can be made. This holds for settings ranging from very low-resource to high-resource. Furthermore, we introduce a new gold alignment test set for Icelandic and a new easy-to-use tool for creating manual word alignments. 1 Introduction Word alignment, the task of finding corresponding words in a bilingual sentence pair (see Figure 1), was a key component of statistical machine translation (SMT) systems. While word alignments are not necessary for neural machine translation (NMT), various MT methods incorporating word alignment have been found to achieve significant improvements in performance. Alkhouli et al. (2018) and Liu et al. (2016) use alignments as a Figure 1: A simple example of English-Icelandic word alignments. Corresponding words are connected by edges. Andy Way ADAPT Centre School of Computing Dublin City University Ireland andy.way @adaptcentre.ie prior; Arthur et al. (2016) augment NMT systems with discrete translation lexicons that encode lowfrequency words; Press and Smith (2018) infer a correspondence between words in sentence pairs before encoding/decoding and, as demonstrated by Poncelas et al. (2019), back-translated data created using SMT systems, requiring word alignments, can be va"
2021.nodalida-main.7,P19-1494,0,0.0214026,"pairs before encoding/decoding and, as demonstrated by Poncelas et al. (2019), back-translated data created using SMT systems, requiring word alignments, can be valuable to augment NMT systems. Word alignments have also been utilized to improve automatic post-editing (Pal et al., 2017) as well as to preserve markup in machine-translated texts (Müller, 2017). Various other subfields of NLP make use of word alignments. Shi et al. (2021) show that by simply pipelining word alignment with unsupervised bitext mining, bilingual lexicon induction (BLI) quality can be improved significantly. For BLI, Artetxe et al. (2019) use an unsupervised MT pipeline, also employing word alignments. Kurfalı and Östling (2019) use word alignments to filter noisy parallel corpora, and Paetzold et al. (2017) include word alignment as a part of their pipeline to align monolingual comparable documents. There is a variety of word aligners available. Giza++ (Och and Ney, 2003) and fast_align (Dyer et al., 2013) are easy to use implementations of the IBM models (Brown et al., 1993). Other statistical aligners, such as eflomal (Östling and Tiedemann, 2016), have also been shown to be fast and give competitive results. SimAlign (Maso"
2021.nodalida-main.7,D16-1162,0,0.0186152,"nding words in a bilingual sentence pair (see Figure 1), was a key component of statistical machine translation (SMT) systems. While word alignments are not necessary for neural machine translation (NMT), various MT methods incorporating word alignment have been found to achieve significant improvements in performance. Alkhouli et al. (2018) and Liu et al. (2016) use alignments as a Figure 1: A simple example of English-Icelandic word alignments. Corresponding words are connected by edges. Andy Way ADAPT Centre School of Computing Dublin City University Ireland andy.way @adaptcentre.ie prior; Arthur et al. (2016) augment NMT systems with discrete translation lexicons that encode lowfrequency words; Press and Smith (2018) infer a correspondence between words in sentence pairs before encoding/decoding and, as demonstrated by Poncelas et al. (2019), back-translated data created using SMT systems, requiring word alignments, can be valuable to augment NMT systems. Word alignments have also been utilized to improve automatic post-editing (Pal et al., 2017) as well as to preserve markup in machine-translated texts (Müller, 2017). Various other subfields of NLP make use of word alignments. Shi et al. (2021) s"
2021.nodalida-main.7,W19-6115,1,0.836432,"moving the cursor to select corresponding words and then saving the alignment. It supports up to two users and can export a union or intersection of their alignments in two different formats. to test our approach and other alignment methods on Icelandic, we thus compiled development and test sets. For that purpose, we created a simple graphical tool for performing manual word alignment, AlignMan, which is available under an Apache2 licence. A screen shot from AlignMan can be seen in Figure 2. Two annotators manually aligned 604 sentences, a random sample from the ParIce en-is parallel corpus (Barkarson and Steingrímsson, 2019). They then reviewed the other annotator’s work in order to eliminate mistakes. The two annotations were then combined. All 1-to-1 alignments that 3 https://www-i6.informatik. rwth-aachen.de/goldAlignment/ Experimental Setup By default, Giza++ runs IBM models 1, 3 and 4 as well as an HMM model, while fast_align is based on IBM model 2. We use default settings for these two aligners as well as for eflomal and compared their results after processing their output with different heuristics. These aligners are not trained on other word alignments, but rather on sentencealigned parallel texts. They"
2021.nodalida-main.7,U18-1012,0,0.0643256,"Missing"
2021.nodalida-main.7,J93-2003,0,0.257837,"Missing"
2021.nodalida-main.7,2020.acl-main.747,0,0.0275271,"rparameters for the different aligners. Shown in bold are the ones giving the highest F1 -score. Giza++ only outputs one set of alignments after each run, but for fast_align and eflomal we output alignments for both directions, source→target language and target→source, and then generate alignments from these using different alignment heuristics: intersection and union, as well as grow-diag (gd), grow-diag-final (gdf) and grow-diag-finaland (gdfa). With SimAlign, we induce alignments from two different contextualized embedding models, multilingual BERT (mBert) (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), and run experiments both for whole words and byte-pair encodings (BPE) (Sennrich et al., 2016). The alignments are obtained from similarity matrices using three different methods: Match, a graph-based method that identifies matches in a bipartite graph; Argmax, which aligns two words if the target word is the most similar to the source word, or vice versa; and Itermax, which applies Argmax iteratively and is thus better able to find alignment edges when one word aligns with two or more words in the other language. We did a grid search on the en-is development set, calculating the best scores"
2021.nodalida-main.7,N19-1423,0,0.00507989,"7, 0.98, 0.99, 1.0] Table 2: Hyperparameters for the different aligners. Shown in bold are the ones giving the highest F1 -score. Giza++ only outputs one set of alignments after each run, but for fast_align and eflomal we output alignments for both directions, source→target language and target→source, and then generate alignments from these using different alignment heuristics: intersection and union, as well as grow-diag (gd), grow-diag-final (gdf) and grow-diag-finaland (gdfa). With SimAlign, we induce alignments from two different contextualized embedding models, multilingual BERT (mBert) (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), and run experiments both for whole words and byte-pair encodings (BPE) (Sennrich et al., 2016). The alignments are obtained from similarity matrices using three different methods: Match, a graph-based method that identifies matches in a bipartite graph; Argmax, which aligns two words if the target word is the most similar to the source word, or vice versa; and Itermax, which applies Argmax iteratively and is thus better able to find alignment edges when one word aligns with two or more words in the other language. We did a grid search on the en-is developmen"
2021.nodalida-main.7,N13-1073,0,0.135147,"ields of NLP make use of word alignments. Shi et al. (2021) show that by simply pipelining word alignment with unsupervised bitext mining, bilingual lexicon induction (BLI) quality can be improved significantly. For BLI, Artetxe et al. (2019) use an unsupervised MT pipeline, also employing word alignments. Kurfalı and Östling (2019) use word alignments to filter noisy parallel corpora, and Paetzold et al. (2017) include word alignment as a part of their pipeline to align monolingual comparable documents. There is a variety of word aligners available. Giza++ (Och and Ney, 2003) and fast_align (Dyer et al., 2013) are easy to use implementations of the IBM models (Brown et al., 1993). Other statistical aligners, such as eflomal (Östling and Tiedemann, 2016), have also been shown to be fast and give competitive results. SimAlign (Masoud et al., 2020) takes advantage of the rising availability of contextualized embeddings and leverages them by extracting alignments from similarity matrices. In this work, we present CombAlign, an ensemble of these four tools (Giza++, fast_align, eflomal, and SimAlign). As they are based on different approaches, and all able to attain a fairly high F1 -score, it is reasona"
2021.nodalida-main.7,D19-1453,0,0.027384,"Missing"
2021.nodalida-main.7,I17-1004,0,0.0352811,"Missing"
2021.nodalida-main.7,2005.mtsummit-papers.11,0,0.264787,"Missing"
2021.nodalida-main.7,W19-5438,0,0.028011,"nslated data created using SMT systems, requiring word alignments, can be valuable to augment NMT systems. Word alignments have also been utilized to improve automatic post-editing (Pal et al., 2017) as well as to preserve markup in machine-translated texts (Müller, 2017). Various other subfields of NLP make use of word alignments. Shi et al. (2021) show that by simply pipelining word alignment with unsupervised bitext mining, bilingual lexicon induction (BLI) quality can be improved significantly. For BLI, Artetxe et al. (2019) use an unsupervised MT pipeline, also employing word alignments. Kurfalı and Östling (2019) use word alignments to filter noisy parallel corpora, and Paetzold et al. (2017) include word alignment as a part of their pipeline to align monolingual comparable documents. There is a variety of word aligners available. Giza++ (Och and Ney, 2003) and fast_align (Dyer et al., 2013) are easy to use implementations of the IBM models (Brown et al., 1993). Other statistical aligners, such as eflomal (Östling and Tiedemann, 2016), have also been shown to be fast and give competitive results. SimAlign (Masoud et al., 2020) takes advantage of the rising availability of contextualized embeddings and"
2021.nodalida-main.7,C16-1291,0,0.0185223,"settings ranging from very low-resource to high-resource. Furthermore, we introduce a new gold alignment test set for Icelandic and a new easy-to-use tool for creating manual word alignments. 1 Introduction Word alignment, the task of finding corresponding words in a bilingual sentence pair (see Figure 1), was a key component of statistical machine translation (SMT) systems. While word alignments are not necessary for neural machine translation (NMT), various MT methods incorporating word alignment have been found to achieve significant improvements in performance. Alkhouli et al. (2018) and Liu et al. (2016) use alignments as a Figure 1: A simple example of English-Icelandic word alignments. Corresponding words are connected by edges. Andy Way ADAPT Centre School of Computing Dublin City University Ireland andy.way @adaptcentre.ie prior; Arthur et al. (2016) augment NMT systems with discrete translation lexicons that encode lowfrequency words; Press and Smith (2018) infer a correspondence between words in sentence pairs before encoding/decoding and, as demonstrated by Poncelas et al. (2019), back-translated data created using SMT systems, requiring word alignments, can be valuable to augment NMT"
2021.nodalida-main.7,2020.findings-emnlp.147,0,0.332399,"019) use an unsupervised MT pipeline, also employing word alignments. Kurfalı and Östling (2019) use word alignments to filter noisy parallel corpora, and Paetzold et al. (2017) include word alignment as a part of their pipeline to align monolingual comparable documents. There is a variety of word aligners available. Giza++ (Och and Ney, 2003) and fast_align (Dyer et al., 2013) are easy to use implementations of the IBM models (Brown et al., 1993). Other statistical aligners, such as eflomal (Östling and Tiedemann, 2016), have also been shown to be fast and give competitive results. SimAlign (Masoud et al., 2020) takes advantage of the rising availability of contextualized embeddings and leverages them by extracting alignments from similarity matrices. In this work, we present CombAlign, an ensemble of these four tools (Giza++, fast_align, eflomal, and SimAlign). As they are based on different approaches, and all able to attain a fairly high F1 -score, it is reasonable to expect that combining their results in a sensible way could give better results than using any one of the individual systems. Recently, the first reported results in SMT and NMT for Icelandic were published (Jónsson et al., 2020) wit"
2021.nodalida-main.7,W17-4804,0,0.0202761,"Computing Dublin City University Ireland andy.way @adaptcentre.ie prior; Arthur et al. (2016) augment NMT systems with discrete translation lexicons that encode lowfrequency words; Press and Smith (2018) infer a correspondence between words in sentence pairs before encoding/decoding and, as demonstrated by Poncelas et al. (2019), back-translated data created using SMT systems, requiring word alignments, can be valuable to augment NMT systems. Word alignments have also been utilized to improve automatic post-editing (Pal et al., 2017) as well as to preserve markup in machine-translated texts (Müller, 2017). Various other subfields of NLP make use of word alignments. Shi et al. (2021) show that by simply pipelining word alignment with unsupervised bitext mining, bilingual lexicon induction (BLI) quality can be improved significantly. For BLI, Artetxe et al. (2019) use an unsupervised MT pipeline, also employing word alignments. Kurfalı and Östling (2019) use word alignments to filter noisy parallel corpora, and Paetzold et al. (2017) include word alignment as a part of their pipeline to align monolingual comparable documents. There is a variety of word aligners available. Giza++ (Och and Ney, 20"
2021.nodalida-main.7,P00-1056,0,0.686248,"Missing"
2021.nodalida-main.7,J03-1002,0,0.178525,"(Müller, 2017). Various other subfields of NLP make use of word alignments. Shi et al. (2021) show that by simply pipelining word alignment with unsupervised bitext mining, bilingual lexicon induction (BLI) quality can be improved significantly. For BLI, Artetxe et al. (2019) use an unsupervised MT pipeline, also employing word alignments. Kurfalı and Östling (2019) use word alignments to filter noisy parallel corpora, and Paetzold et al. (2017) include word alignment as a part of their pipeline to align monolingual comparable documents. There is a variety of word aligners available. Giza++ (Och and Ney, 2003) and fast_align (Dyer et al., 2013) are easy to use implementations of the IBM models (Brown et al., 1993). Other statistical aligners, such as eflomal (Östling and Tiedemann, 2016), have also been shown to be fast and give competitive results. SimAlign (Masoud et al., 2020) takes advantage of the rising availability of contextualized embeddings and leverages them by extracting alignments from similarity matrices. In this work, we present CombAlign, an ensemble of these four tools (Giza++, fast_align, eflomal, and SimAlign). As they are based on different approaches, and all able to attain a f"
2021.nodalida-main.7,I17-3001,0,0.0202423,"augment NMT systems. Word alignments have also been utilized to improve automatic post-editing (Pal et al., 2017) as well as to preserve markup in machine-translated texts (Müller, 2017). Various other subfields of NLP make use of word alignments. Shi et al. (2021) show that by simply pipelining word alignment with unsupervised bitext mining, bilingual lexicon induction (BLI) quality can be improved significantly. For BLI, Artetxe et al. (2019) use an unsupervised MT pipeline, also employing word alignments. Kurfalı and Östling (2019) use word alignments to filter noisy parallel corpora, and Paetzold et al. (2017) include word alignment as a part of their pipeline to align monolingual comparable documents. There is a variety of word aligners available. Giza++ (Och and Ney, 2003) and fast_align (Dyer et al., 2013) are easy to use implementations of the IBM models (Brown et al., 1993). Other statistical aligners, such as eflomal (Östling and Tiedemann, 2016), have also been shown to be fast and give competitive results. SimAlign (Masoud et al., 2020) takes advantage of the rising availability of contextualized embeddings and leverages them by extracting alignments from similarity matrices. In this work,"
2021.nodalida-main.7,E17-2056,0,0.0460025,"Missing"
2021.nodalida-main.7,R19-1107,1,0.895581,"Missing"
2021.nodalida-main.7,P16-1162,0,0.0499752,". Giza++ only outputs one set of alignments after each run, but for fast_align and eflomal we output alignments for both directions, source→target language and target→source, and then generate alignments from these using different alignment heuristics: intersection and union, as well as grow-diag (gd), grow-diag-final (gdf) and grow-diag-finaland (gdfa). With SimAlign, we induce alignments from two different contextualized embedding models, multilingual BERT (mBert) (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), and run experiments both for whole words and byte-pair encodings (BPE) (Sennrich et al., 2016). The alignments are obtained from similarity matrices using three different methods: Match, a graph-based method that identifies matches in a bipartite graph; Argmax, which aligns two words if the target word is the most similar to the source word, or vice versa; and Itermax, which applies Argmax iteratively and is thus better able to find alignment edges when one word aligns with two or more words in the other language. We did a grid search on the en-is development set, calculating the best scores using these methods and two other hyperparameters: distortion correction and null extensions, w"
2021.nodalida-main.7,2021.acl-long.67,0,0.0257739,"rthur et al. (2016) augment NMT systems with discrete translation lexicons that encode lowfrequency words; Press and Smith (2018) infer a correspondence between words in sentence pairs before encoding/decoding and, as demonstrated by Poncelas et al. (2019), back-translated data created using SMT systems, requiring word alignments, can be valuable to augment NMT systems. Word alignments have also been utilized to improve automatic post-editing (Pal et al., 2017) as well as to preserve markup in machine-translated texts (Müller, 2017). Various other subfields of NLP make use of word alignments. Shi et al. (2021) show that by simply pipelining word alignment with unsupervised bitext mining, bilingual lexicon induction (BLI) quality can be improved significantly. For BLI, Artetxe et al. (2019) use an unsupervised MT pipeline, also employing word alignments. Kurfalı and Östling (2019) use word alignments to filter noisy parallel corpora, and Paetzold et al. (2017) include word alignment as a part of their pipeline to align monolingual comparable documents. There is a variety of word aligners available. Giza++ (Och and Ney, 2003) and fast_align (Dyer et al., 2013) are easy to use implementations of the I"
2021.nodalida-main.7,E06-1020,0,0.119576,"Missing"
bungeroth-etal-2008-atis,H90-1021,0,\N,Missing
bungeroth-etal-2008-atis,bungeroth-etal-2006-german,1,\N,Missing
C04-1154,2003.mtsummit-papers.13,0,0.169815,"r compound sentences. Other researchers (Imamura, 2001) also use phrasealignment in parsing but in DOT the translation fragments are already in the form of parse-trees. Andy Way School of Computing Dublin City University Dublin 9, Ireland away@computing.dcu.ie (Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. (Gildea, 2003) performs tree-totree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. The method of (Ding et al., 2003) can cope with a limited amount of non-isomorphism, but the algorithm is only suitable for use with dependency trees. We develop a novel algorithm which automatically aligns translationally equivalent tree fragments in a fast and consistent fashion, and which requires little or no knowledge of the language pair. Our approach is similar to that of (Menezes and Richardson, 2003), who use a best-first approach to align dependency-type tree structures. We conduct a number of experiments on the English-French section of the Xerox HomeCentre corpus. Using the manual alignment of (Hearne and Way, 200"
C04-1154,P03-2041,0,0.185777,"lignment e.g. (Kay and R¨oscheisen, 1993; Gale & Church, 1993), no methods exist for aligning non-isomorphic phrasestructure (PS) tree fragments at sub-sentential level for use in MT. (Matsumoto et al., 1993) align hsource,targeti dependency trees, with a view to resolve parsing ambiguities, but their approach cannot deal with complex or compound sentences. Other researchers (Imamura, 2001) also use phrasealignment in parsing but in DOT the translation fragments are already in the form of parse-trees. Andy Way School of Computing Dublin City University Dublin 9, Ireland away@computing.dcu.ie (Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. (Gildea, 2003) performs tree-totree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. The method of (Ding et al., 2003) can cope with a limited amount of non-isomorphism, but the algorithm is only suitable for use with dependency trees. We develop a novel algorithm which automatically aligns translationally equivalent tree fragments in a fast and consistent fashion, and whi"
C04-1154,C94-2178,0,0.015501,"bility of bilingual treebanks aligned at sentential and sub-sentential level. Our novel algorithm attempts to fully automate sub-sentential alignment using an approach inspired by that of (Menezes and Richardson, 2003). The algorithm takes as input a pair of hsource,targeti PS trees and outputs a mapping between the nodes of the tree pair. As with the majority of previous approaches, the algorithm starts by finding lexical correspondences between the source and target trees. Our lexicon is built automatically using a previously developed word aligner based on the k-vec aligner as outlined by (Fung & Church, 1994). This lexical aligner uses a combination of automatically extracted cognate information, mutual information and probabilistic measures to obtain one-to-one lexical correspondences between the source and target strings. During lexical alignment, function words are excluded because, as they are the most common words in a language, they tend to co-occur frequently with the content words they precede. This can lead to the incorrect alignment of content words with function words. The algorithm then proceeds from the aligned lexical terminal nodes in a bottom-up fashion, using a mixture of node lab"
C04-1154,J93-1004,0,0.100848,"d Parsing (DOP: (Bod, 1998; Bod et al., 2003)) require hsource,targeti tree fragments aligned at sentential and sub-sentential levels. In previous approaches to Data-Oriented Translation (DOT: (Poutsma, 2000; Hearne and Way, 2003)), such fragments were produced manually. This is time-consuming, error-prone, and requires considerable expertise of both source and target languages as well as how they are related. The obvious solution, therefore, is to automate the process of subsentential alignment. However, while there are many approaches to sentential alignment e.g. (Kay and R¨oscheisen, 1993; Gale & Church, 1993), no methods exist for aligning non-isomorphic phrasestructure (PS) tree fragments at sub-sentential level for use in MT. (Matsumoto et al., 1993) align hsource,targeti dependency trees, with a view to resolve parsing ambiguities, but their approach cannot deal with complex or compound sentences. Other researchers (Imamura, 2001) also use phrasealignment in parsing but in DOT the translation fragments are already in the form of parse-trees. Andy Way School of Computing Dublin City University Dublin 9, Ireland away@computing.dcu.ie (Eisner, 2003) outlines a computationally expensive structural"
C04-1154,P03-1011,0,0.176286,"MT. (Matsumoto et al., 1993) align hsource,targeti dependency trees, with a view to resolve parsing ambiguities, but their approach cannot deal with complex or compound sentences. Other researchers (Imamura, 2001) also use phrasealignment in parsing but in DOT the translation fragments are already in the form of parse-trees. Andy Way School of Computing Dublin City University Dublin 9, Ireland away@computing.dcu.ie (Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. (Gildea, 2003) performs tree-totree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. The method of (Ding et al., 2003) can cope with a limited amount of non-isomorphism, but the algorithm is only suitable for use with dependency trees. We develop a novel algorithm which automatically aligns translationally equivalent tree fragments in a fast and consistent fashion, and which requires little or no knowledge of the language pair. Our approach is similar to that of (Menezes and Richardson, 2003), who use a best-first approach to align dependency-type"
C04-1154,2003.mtsummit-papers.22,1,0.690899,"m which automatically induces sub-structural alignments between context-free phrase structure trees in a fast and consistent fashion requiring little or no knowledge of the language pair. We present results from a number of experiments which indicate that our method provides a serious alternative to manual alignment. 1 Introduction Approaches to Machine Translation (MT) using Data-Oriented Parsing (DOP: (Bod, 1998; Bod et al., 2003)) require hsource,targeti tree fragments aligned at sentential and sub-sentential levels. In previous approaches to Data-Oriented Translation (DOT: (Poutsma, 2000; Hearne and Way, 2003)), such fragments were produced manually. This is time-consuming, error-prone, and requires considerable expertise of both source and target languages as well as how they are related. The obvious solution, therefore, is to automate the process of subsentential alignment. However, while there are many approaches to sentential alignment e.g. (Kay and R¨oscheisen, 1993; Gale & Church, 1993), no methods exist for aligning non-isomorphic phrasestructure (PS) tree fragments at sub-sentential level for use in MT. (Matsumoto et al., 1993) align hsource,targeti dependency trees, with a view to resolve"
C04-1154,J93-1006,0,0.146173,"Missing"
C04-1154,C00-2092,0,0.368594,"novel algorithm which automatically induces sub-structural alignments between context-free phrase structure trees in a fast and consistent fashion requiring little or no knowledge of the language pair. We present results from a number of experiments which indicate that our method provides a serious alternative to manual alignment. 1 Introduction Approaches to Machine Translation (MT) using Data-Oriented Parsing (DOP: (Bod, 1998; Bod et al., 2003)) require hsource,targeti tree fragments aligned at sentential and sub-sentential levels. In previous approaches to Data-Oriented Translation (DOT: (Poutsma, 2000; Hearne and Way, 2003)), such fragments were produced manually. This is time-consuming, error-prone, and requires considerable expertise of both source and target languages as well as how they are related. The obvious solution, therefore, is to automate the process of subsentential alignment. However, while there are many approaches to sentential alignment e.g. (Kay and R¨oscheisen, 1993; Gale & Church, 1993), no methods exist for aligning non-isomorphic phrasestructure (PS) tree fragments at sub-sentential level for use in MT. (Matsumoto et al., 1993) align hsource,targeti dependency trees,"
C04-1154,W01-1406,0,\N,Missing
C04-1154,2001.mtsummit-ebmt.4,0,\N,Missing
C04-1154,P93-1004,0,\N,Missing
C08-1139,W07-2441,0,0.0550706,"an be used as a parallel treebank, it is not such per se. The authors do not use phrase-structure trees. Instead, tectogrammatical dependency structures are used (Hajiová, 2000). Either a word alignment tool like GIZA++ or a probabilistic electronic dictionary (supplied with the treebank) can be used to automatically align the dependency structures. The presented version contains over 21000 sentence pairs that can be aligned. Because of its nature, this treebank can only be used by MT systems that employ tectogrammatical dependency structures. We are also aware of the existence of the LinES (Ahrenberg, 2007), CroCo (Hansen-Schirra et al., 6 An 2006) and FuSe (Cyrus, 2006) parallel corpora. Although it seems possible to use them as parallel treebanks, they have been designed to serve as resources for the study of translational phenomena and it does not appear that they can be used effectively for other natural language processing tasks. An attempt to develop an automatic tree-totree aligner is described in (Groves et al., 2004). The authors present a promising rule-based system. Further testing, however, has shown that the rules are only applicable to a particular treebank and language pair. This"
C08-1139,W05-0909,0,0.0472982,"obabilities yielded better results than those output directly by GIZA++. Throughout the paper we use boldface to highlight the best results and italics for the worst. 1108 the same tests, such that the only difference across runs are the alignments. For testing, we used the six English–French training / test splits for the HomeCentre used in (Hearne and Way, 2006). Each test set contains 80 test sentences and each training set contains 730 tree pairs. We evaluated the translation output using three automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005). We averaged the results over the six splits. We also measured test-data coverage of the translation system, i.e. the percentage of test sentences for which full trees were generated during translation. We performed this evaluation using both the tree-to-tree algorithm and the string-to-string algorithm, employing greedy-search selection. For the latter case we extracted POS-tagged sentences from the HomeCentre and used them as input for the aligner. The results for the tree-totree case are presented in Table 2 and for the string-to-string case in Table 3. Configurations manual skip1 skip2 sk"
C08-1139,cmejrek-etal-2004-prague,0,0.0491967,"Missing"
C08-1139,cyrus-2006-building,0,0.0148185,"do not use phrase-structure trees. Instead, tectogrammatical dependency structures are used (Hajiová, 2000). Either a word alignment tool like GIZA++ or a probabilistic electronic dictionary (supplied with the treebank) can be used to automatically align the dependency structures. The presented version contains over 21000 sentence pairs that can be aligned. Because of its nature, this treebank can only be used by MT systems that employ tectogrammatical dependency structures. We are also aware of the existence of the LinES (Ahrenberg, 2007), CroCo (Hansen-Schirra et al., 6 An 2006) and FuSe (Cyrus, 2006) parallel corpora. Although it seems possible to use them as parallel treebanks, they have been designed to serve as resources for the study of translational phenomena and it does not appear that they can be used effectively for other natural language processing tasks. An attempt to develop an automatic tree-totree aligner is described in (Groves et al., 2004). The authors present a promising rule-based system. Further testing, however, has shown that the rules are only applicable to a particular treebank and language pair. This means that the set of rules has to be adjusted for each particula"
C08-1139,C04-1154,1,0.682653,"ligned. Because of its nature, this treebank can only be used by MT systems that employ tectogrammatical dependency structures. We are also aware of the existence of the LinES (Ahrenberg, 2007), CroCo (Hansen-Schirra et al., 6 An 2006) and FuSe (Cyrus, 2006) parallel corpora. Although it seems possible to use them as parallel treebanks, they have been designed to serve as resources for the study of translational phenomena and it does not appear that they can be used effectively for other natural language processing tasks. An attempt to develop an automatic tree-totree aligner is described in (Groves et al., 2004). The authors present a promising rule-based system. Further testing, however, has shown that the rules are only applicable to a particular treebank and language pair. This means that the set of rules has to be adjusted for each particular case. Thus, the methods presented in this paper are the only available ones that can be used to produce a sufficiently large parallel treebank appropriate for use by state-of-the-art statistical MT applications (eg. DOT (Hearne and Way, 2006)).6 6 Conclusions We have presented a novel platform for the fast and robust automatic generation of parallel treebank"
C08-1139,han-etal-2002-development,0,0.0293784,"ents and one-to-many nonlexical alignments. The authors also allow unary productions in the trees, which, as stated in section 2.1, does not provide any additional useful information. Another difference is that they deepen the original German and Swedish trees before alignment, rather than preserve their original form. A further attempt to align phrase-structure trees is presented in (Uibo et al., 2005). The authors develop a rule-based method for aligning Estonian and German sentences. The parallel treebank consist of over 500 sentences, but in the version presented only NPs are aligned. In (Han et al., 2002) the authors claim to have built a Korean–English parallel treebank with over 5000 phrase-structure tree pairs, but at the time of writing we were unable to find details about this treebank. Although the Prague Czech–English Dependency Treebank (PCEDT (mejrek et al., 2004)) can be used as a parallel treebank, it is not such per se. The authors do not use phrase-structure trees. Instead, tectogrammatical dependency structures are used (Hajiová, 2000). Either a word alignment tool like GIZA++ or a probabilistic electronic dictionary (supplied with the treebank) can be used to automatically ali"
C08-1139,W06-2705,0,0.0945927,"Missing"
C08-1139,2006.eamt-1.8,1,0.884552,"re we have developed based on this platform has been shown to handle large data sets. We also present evaluation results demonstrating the quality of the derived treebanks and discuss some possible modifications and improvements that can lead to even better results. We expect the presented platform to help boost research in the field of dataoriented machine translation and lead to advancements in other fields where parallel treebanks can be employed. 1 2 Introduction In recent years much effort has been made to make use of syntactic information in statistical machine translation (MT) systems (Hearne and Way, 2006, Nesson et al., 2006). This has led to increased interest in the development of parallel treebanks as the source for such syntactic data. They consist of a parallel corpus, both sides of which have been parsed and aligned at the sub-tree level. So far parallel treebanks have been created manually or semi-automatically. This has proven to be a laborious and time-consuming task that is Automatic Generation of Parallel Treebanks In this section we introduce a method for the automatic generation of parallel treebanks from parallel corpora. The only tool that is required besides the software prese"
C08-1139,2007.tmi-papers.11,1,0.865544,"uarantee the usability of the algorithm for any language pair in many different contexts. Additionally, there are a few wellformedness criteria that have to be followed to enforce feasible alignments: • A node in a tree may only be linked once. • Descendants / ancestors of a source linked node may only be linked to descendants / ancestors of its target linked counterpart. Links produced according to these criteria encode enough information to allow the inference of complex translational patterns from a parallel treebank, including some idiosyncratic translational divergences, as discussed in (Hearne et al., 2007). In what follows, a hypothesised alignment is regarded as incompatible with the existing alignments if it violates any of these criteria. The sub-tree aligner operates on a per sentencepair basis and each sentence-pair is processed in two stages. First, for each possible hypothetical link between two nodes, a translational equivalence score is calculated. Only the links for which a nonzero score is calculated are stored for further processing. Unary productions from the original trees, if available, are collapsed to single nodes, preserving all labels. Thus the aligner will consider a single"
C08-1139,2006.amta-papers.15,0,0.0124729,"ased on this platform has been shown to handle large data sets. We also present evaluation results demonstrating the quality of the derived treebanks and discuss some possible modifications and improvements that can lead to even better results. We expect the presented platform to help boost research in the field of dataoriented machine translation and lead to advancements in other fields where parallel treebanks can be employed. 1 2 Introduction In recent years much effort has been made to make use of syntactic information in statistical machine translation (MT) systems (Hearne and Way, 2006, Nesson et al., 2006). This has led to increased interest in the development of parallel treebanks as the source for such syntactic data. They consist of a parallel corpus, both sides of which have been parsed and aligned at the sub-tree level. So far parallel treebanks have been created manually or semi-automatically. This has proven to be a laborious and time-consuming task that is Automatic Generation of Parallel Treebanks In this section we introduce a method for the automatic generation of parallel treebanks from parallel corpora. The only tool that is required besides the software presented in this paper is"
C08-1139,J03-1002,0,0.00350542,"e source for such syntactic data. They consist of a parallel corpus, both sides of which have been parsed and aligned at the sub-tree level. So far parallel treebanks have been created manually or semi-automatically. This has proven to be a laborious and time-consuming task that is Automatic Generation of Parallel Treebanks In this section we introduce a method for the automatic generation of parallel treebanks from parallel corpora. The only tool that is required besides the software presented in this paper is a word alignment tool. Such tools exist and some are freely available (eg. GIZA++ (Och and Ney, 2003)). If monolingual phrase-structure parsers1 or at least POS taggers exist for both languages, their use for pre-processing the data is highly recommended. In all cases, a word alignment tool is used to first obtain word-alignment probabilities for the © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Henceforth, we will use ‘parser’ to mean ‘monolingual phrase-structure parser’, unless stated otherwise. 1105 Proceedings of the 22nd International Conference on Comput"
C08-1139,P02-1040,0,0.111759,"and repeated We found that using the Moses word-alignment probabilities yielded better results than those output directly by GIZA++. Throughout the paper we use boldface to highlight the best results and italics for the worst. 1108 the same tests, such that the only difference across runs are the alignments. For testing, we used the six English–French training / test splits for the HomeCentre used in (Hearne and Way, 2006). Each test set contains 80 test sentences and each training set contains 730 tree pairs. We evaluated the translation output using three automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005). We averaged the results over the six splits. We also measured test-data coverage of the translation system, i.e. the percentage of test sentences for which full trees were generated during translation. We performed this evaluation using both the tree-to-tree algorithm and the string-to-string algorithm, employing greedy-search selection. For the latter case we extracted POS-tagged sentences from the HomeCentre and used them as input for the aligner. The results for the tree-totree case are presented in Table 2 and for the string-"
C08-1139,W06-2717,0,0.0266152,"erformed on a very small set of data. (John Tinsley (p.c.) reports successfully deriving a parallel treebank with over 700 000 sentence-pairs using our software.) Further experiments on larger data sets — from different languages, as well as from different domains — should help better understand the real qualities of the methods presented here. 5 Existing Parallel Treebanks In this section we look at several attempts at the creation of parallel treebanks besides the HomeCentre treebank presented earlier. Closest to the material presented in this paper comes the parallel treebank presented in (Samuelsson and Volk, 2006). This manually created treebank aligns three languages — German, English and Swedish — consisting of over 1000 sentences from each language. The main difference compared to our method is that they allow manyto-many lexical alignments and one-to-many nonlexical alignments. The authors also allow unary productions in the trees, which, as stated in section 2.1, does not provide any additional useful information. Another difference is that they deepen the original German and Swedish trees before alignment, rather than preserve their original form. A further attempt to align phrase-structure trees"
C08-1139,P07-2045,0,\N,Missing
C08-1139,W08-0411,0,\N,Missing
C10-1033,P07-1091,0,0.0299349,"Missing"
C10-1033,P03-1021,0,0.00843814,"“5-A” and “2-A” represent the accuracy of the 5-class and 2-class respectively. The 2-class is 6.1 Experimental Setting For our SMT experiments, we used two systems, namely Moses (Koehn et al., 2007) and Moseschart. The former is the state-of-the-art PB-SMT system while the latter is a new extended system of the Moses toolkit re-implementing the hierarchical PB-SMT (HPB) model (Chiang, 2005). The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (Och, 2003). The training data contains 2,159,232 sentence pairs.The 5-gram language model is trained on the English part of the parallel training data. The development set (devset) is the NIST MT2006 test set and the test set is the NIST MT2008 “current” test set. All the results are reported in terms of BLEU (Papineni et al., 2002) and METEOR (MTR) (Banerjee and Lavie, 2005) scores. To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the 291 devset and test set. 6.2 Statistics of 5-class DE Annotation For the DE-annot"
C10-1033,J03-1002,0,0.00232114,"5.2 73.6 86.5 76.4 87.9 76.8 88.3 - Table 1: Comparison between the two classifiers on 5-class and 2-class accuracy Table 1 shows the comparison of accuracy, where “5-A” and “2-A” represent the accuracy of the 5-class and 2-class respectively. The 2-class is 6.1 Experimental Setting For our SMT experiments, we used two systems, namely Moses (Koehn et al., 2007) and Moseschart. The former is the state-of-the-art PB-SMT system while the latter is a new extended system of the Moses toolkit re-implementing the hierarchical PB-SMT (HPB) model (Chiang, 2005). The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (Och, 2003). The training data contains 2,159,232 sentence pairs.The 5-gram language model is trained on the English part of the parallel training data. The development set (devset) is the NIST MT2006 test set and the test set is the NIST MT2008 “current” test set. All the results are reported in terms of BLEU (Papineni et al., 2002) and METEOR (MTR) (Banerjee and Lavie, 2005) scores. To run the DE classifiers, we use the Stanford Chinese parser (Levy"
C10-1033,W05-0909,0,0.0159701,"rchical PB-SMT (HPB) model (Chiang, 2005). The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (Och, 2003). The training data contains 2,159,232 sentence pairs.The 5-gram language model is trained on the English part of the parallel training data. The development set (devset) is the NIST MT2006 test set and the test set is the NIST MT2008 “current” test set. All the results are reported in terms of BLEU (Papineni et al., 2002) and METEOR (MTR) (Banerjee and Lavie, 2005) scores. To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the 291 devset and test set. 6.2 Statistics of 5-class DE Annotation For the DE-annotated MT experiments, after we parse the training data, the devset and the test set, we separately use the two DE classifiers to annotate the DE constructions in NPs in all of the parsed data. Once the DE data are labeled, we pre-process the Chinese data by reordering the sentences only with dBprepA and drelc annotations. Table 2 lists the statistics of the DE classe"
C10-1033,W09-0436,0,0.0602546,"Missing"
C10-1033,P05-1033,0,0.169203,"ity of the DE construction, as it can be translated in many different ways. 287 Original:  Aozhou  [   ] shi yu you Beihan A bangjiao [ ]B  DE zhiyi shaoshu Australia is with North Korea have diplomatic relations that few Reference: Australia guojia . countries one of . is [one of the few countries] that [have diplomatic relations with North Korea] .   [ ] [   ] B A Reordered: Literal Australia is [one of the few countries] [have diplomatic relations with North Korea] . Translation: Figure 2: An example of DE construction reordering (extended from the original figure in (Chiang, 2005)) Chang et al. (2009) extended the work of (Wang et al., 2007) and characterised the DE structures into 5 finer-grained classes based on their syntactic behaviour. They argued that one possible reason why the d(DE) construction remains problematic is that previous work has paid insufficient attention to the many ways that the d (DE) construction can be translated, as well as the rich structural cues which exist for these translations. For a Chinese noun phrase [A d B], it can be categorized into one of the following five classes (cf. (Chang et al., 2009) for some real examples of each class):"
C10-1033,P05-1066,0,0.0607481,"Missing"
C10-1033,2010.eamt-1.32,1,0.122593,"erforms better than the baseline system, which is consistent when different DE classifiers are applied; (2) given the reordered test set system, the reordered set produces a better result than the baseline, which is also consistent when different DE classifiers are applied; and (3) the results from the DPLVM-based reordered data are better than those from the LLbased reordered data. From the comparison, one might say that the reordered system was learned 4 The phrases in HPB systems are different from those in PB-SMT because they are variable-based, so we evaluate the hierarchical phrases in (Du and Way, 2010) a better phrase table and the reordered test set addresses the problem of word order. To sum up, from the SMT results and the evaluation results on the word alignment and the phrase table, we can conclude that the DE reordering methods contribute significantly to the improvements in translation quality, and it also implies that using DE reordered data can achieve better word alignment and phrase tables. 8 Conclusions and Future Work In this paper, we presented a new classifier: a DPLVM model to classify the Chinese d(DE) constructions in NPs into 5 classes. We also proposed a new and effectiv"
C10-1033,W08-0406,0,0.0298814,"Missing"
C10-1033,P07-2045,0,0.00666989,"): number of correctly labeled DEs number of all DEs Phrase Type DEPOS +A-pattern +Tree-pattern +POS-gram +Lexical +SemClass +Topicality Log-linear 5-A 2-A 54.8 71.0 67.9 83.7 72.1 84.9 74.9 86.5 75.1 86.7 75.4 86.9 × 100 (9) DPLVM 5-A 2-A 56.2 72.3 69.6 85.2 73.6 86.5 76.4 87.9 76.8 88.3 - Table 1: Comparison between the two classifiers on 5-class and 2-class accuracy Table 1 shows the comparison of accuracy, where “5-A” and “2-A” represent the accuracy of the 5-class and 2-class respectively. The 2-class is 6.1 Experimental Setting For our SMT experiments, we used two systems, namely Moses (Koehn et al., 2007) and Moseschart. The former is the state-of-the-art PB-SMT system while the latter is a new extended system of the Moses toolkit re-implementing the hierarchical PB-SMT (HPB) model (Chiang, 2005). The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (Och, 2003). The training data contains 2,159,232 sentence pairs.The 5-gram language model is trained on the English part of the parallel training data. The development set (devset) is the NIST MT2006"
C10-1033,P02-1040,0,0.0828158,"chieved significant improvements in translation quality. Chang et al. (2009) extended this work by classifying DE into 5 finer-grained categories using a log-linear classifier with rich features in order to achieve higher accuracy both in reordering and in lexical choice. Their experiments showed that a higher 286 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 286–294, Beijing, August 2010 accuracy of the DE classification improved the accuracy of reordering component, and further indirectly improved the translation quality in terms of BLEU (Papineni et al., 2002) scores. We regard the DE classification as a labeling task, and hence propose a new model to label the DE construction using a discriminative latent variable algorithm (DPLVM) (Morency et al., 2007; Sun and Tsujii, 2009), which uses latent variables to carry additional information that may not be expressed by those original labels and capture more complicated dependencies between DE and its corresponding features. We also propose a new feature defined as “tree-pattern” which can automatically learn the reordering rules rather than using manually generated ones. The remainder of this paper is"
C10-1033,E09-1088,0,0.297328,"accuracy both in reordering and in lexical choice. Their experiments showed that a higher 286 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 286–294, Beijing, August 2010 accuracy of the DE classification improved the accuracy of reordering component, and further indirectly improved the translation quality in terms of BLEU (Papineni et al., 2002) scores. We regard the DE classification as a labeling task, and hence propose a new model to label the DE construction using a discriminative latent variable algorithm (DPLVM) (Morency et al., 2007; Sun and Tsujii, 2009), which uses latent variables to carry additional information that may not be expressed by those original labels and capture more complicated dependencies between DE and its corresponding features. We also propose a new feature defined as “tree-pattern” which can automatically learn the reordering rules rather than using manually generated ones. The remainder of this paper is organised as follows. In section 2, we introduce the types of word order errors caused by the DE construction. Section 3 describes the closely related work on DE construction. In section 4, we detail our proposed DPLVM al"
C10-1033,D07-1077,0,0.0175548,"A DE B] and correctly perform the reordering, we can achieve a closer word order with English and hence a good English translation even it is literal. Although the Hiero system has a strong reordering capability in its generalised phrases, it still cannot process some complicated and flexible cases of DE construction like those in Figure 1. Therefore, a lot of work has gone into word reordering before decoding so that the Chinese sentences have a closer word order with corresponding English sentences. 3 Related Work on DE Construction To address the word order problems of the DE construction, Wang et al. (2007) proposed a syntactic reordering approach to deal with structural differences and to reorder source language sentences to be much closer to the order of target language sentences. They presented a set of manually generated syntactic rules to determine whether a d(DE) construction should be reordered or not before translation, such as “For DNPs consisting of ‘XP+DEG’, reorder if XP is PP or LCP” etc. (cf. (Wang et al., 2007)). The deficiency of their algorithm is that they did not fully consider the flexibility of the DE construction, as it can be translated in many different ways. 287 Original"
C10-1033,C04-1073,0,0.0791701,"Missing"
C10-1033,2004.tmi-1.9,0,0.105426,"Missing"
C10-1033,W06-3119,0,0.0320401,"Missing"
C10-1033,P03-1056,0,0.0180608,"003) and then we symmetrized the word alignment using the grow-diag-final heuristic. Parameter tuning is performed using Minimum Error Rate Training (Och, 2003). The training data contains 2,159,232 sentence pairs.The 5-gram language model is trained on the English part of the parallel training data. The development set (devset) is the NIST MT2006 test set and the test set is the NIST MT2008 “current” test set. All the results are reported in terms of BLEU (Papineni et al., 2002) and METEOR (MTR) (Banerjee and Lavie, 2005) scores. To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the 291 devset and test set. 6.2 Statistics of 5-class DE Annotation For the DE-annotated MT experiments, after we parse the training data, the devset and the test set, we separately use the two DE classifiers to annotate the DE constructions in NPs in all of the parsed data. Once the DE data are labeled, we pre-process the Chinese data by reordering the sentences only with dBprepA and drelc annotations. Table 2 lists the statistics of the DE classes in the MT training data, devset and test set using our DPLVM classifier. “dnon ” denotes the"
C10-1033,C08-1027,0,\N,Missing
C10-2043,N03-1017,0,0.00602433,"at we are able to create a gold standard by ranking the TER scores of the MT and TM outputs. Duplicated sentences are removed from the data set, as those will lead to an exact match in the TM system and will not be translated by translators. The average sentence length of the training set is 13.5 words and the size of the training set is comparable to the (larger) translation memories used in the industry. 5.1.2 SMT and TM systems We use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudo-source sentences. We merge distinct 5-best lists from MT and TM systems to produce a new ranking. To create the distinct list for the SMT system, we search over a 100-best list and keep the top-5 distinct outputs. Our data set consists of mainly short sentences, leading to many duplications in the N-b"
C10-2043,2009.eamt-1.5,0,0.0329971,"e formulation of the problem and experiments with the ranking models are presented in Sections 4 and 5. We analyze the post-editing effort approximated by the TER metric in Section 6. Section 7 concludes and points out avenues for future research. 2 Related Work There has been some work to help TM users to apply MT outputs more smoothly. One strand is to improve the MT confidence measures to better predict post-editing effort in order to obtain a quality estimation that has the potential to replace the fuzzy match score in the TM. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a), which uses regression on both the automatic scores and scores assigned by posteditors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of features to model post-editors’ judgment of the translation quality between ‘good’ and ‘bad’, or among three levels of post-editing effort. Another strand is to integrate high confidence MT outputs into the TM, so that the ‘good’ TM entries will remain untouched. In our forthcoming paper, we recommend SMT outputs to a TM user when a binary classifier predicts that SMT outputs are more suitabl"
C10-2043,J82-2005,0,0.747793,"Missing"
C10-2043,2009.mtsummit-papers.16,0,0.0374791,"e formulation of the problem and experiments with the ranking models are presented in Sections 4 and 5. We analyze the post-editing effort approximated by the TER metric in Section 6. Section 7 concludes and points out avenues for future research. 2 Related Work There has been some work to help TM users to apply MT outputs more smoothly. One strand is to improve the MT confidence measures to better predict post-editing effort in order to obtain a quality estimation that has the potential to replace the fuzzy match score in the TM. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a), which uses regression on both the automatic scores and scores assigned by posteditors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of features to model post-editors’ judgment of the translation quality between ‘good’ and ‘bad’, or among three levels of post-editing effort. Another strand is to integrate high confidence MT outputs into the TM, so that the ‘good’ TM entries will remain untouched. In our forthcoming paper, we recommend SMT outputs to a TM user when a binary classifier predicts that SMT outputs are more suitabl"
C10-2043,W07-0736,0,0.035337,"e source and TM entry, normalized by the length of the source as in Eq. (4), as most of the current implementations are based on edit distance while allowing some additional flexible matching. F uzzyM atch(t) = min e EditDistance(s, e) Len(s) (4) where s is the source side of the TM hit t, and e is the source side of an entry in the TM. 4.2 Problem Formulation Ranking lists is a well-researched problem in the information retrieval community, and Ranking SVMs (Joachims, 2002), which optimizes on the ranking correlation τ have already been applied successfully in machine translation evaluation (Ye et al., 2007). We apply the same method here to rerank a merged list of MT and TM outputs. Formally given an MT-produced N-best list M = {m1 , m2 , ..., mn }, a TM-produced m-best list T = {t1 , t2 , ..., tm } for a input sentence s, we define the gold standard using the TER ∪ metric (Snover et al., 2006): for each d ∈ M T, (di , dj ) ∈ r(s) iff T ER(di ) &lt; T ER(dj ). We train and test a Ranking SVM using cross validation on a data set created according to this criterion. Ideally the gold standard would be created by human annotators. We choose to use TER 376 as large-scale annotation is not yet available"
C10-2043,P02-1038,0,0.0608918,"ning 8K sentence pairs, which is used to run cross validation. Note that the 8K sentence pairs are from the same TM, so that we are able to create a gold standard by ranking the TER scores of the MT and TM outputs. Duplicated sentences are removed from the data set, as those will lead to an exact match in the TM system and will not be translated by translators. The average sentence length of the training set is 13.5 words and the size of the training set is comparable to the (larger) translation memories used in the industry. 5.1.2 SMT and TM systems We use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudo-source sentences. We merge distinct 5-best lists from MT and TM systems to produce a new ranking. To create the distinct list for the SMT system, we search over a 100-best list an"
C10-2043,P03-1021,0,0.0274724,"g the TER scores of the MT and TM outputs. Duplicated sentences are removed from the data set, as those will lead to an exact match in the TM system and will not be translated by translators. The average sentence length of the training set is 13.5 words and the size of the training set is comparable to the (larger) translation memories used in the industry. 5.1.2 SMT and TM systems We use a standard log-linear PB-SMT model (Och and Ney, 2002): G IZA ++ implementation of IBM word alignment model 4, the phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. We train a system in the opposite direction using the same data to produce the pseudo-source sentences. We merge distinct 5-best lists from MT and TM systems to produce a new ranking. To create the distinct list for the SMT system, we search over a 100-best list and keep the top-5 distinct outputs. Our data set consists of mainly short sentences, leading to many duplications in the N-best output of the SMT decoder. In such ca"
C10-2043,N04-1023,0,0.0439495,"in a ranking model. The ranking scheme also enables us to show all TM hits to the user, and thus further protects the TM assets. There has also been work to improve SMT using the knowledge from the TM. In (Simard and Isabelle, 2009), the SMT system can produce a better translation when there is an exact or close match in the corresponding TM. They use regression Support Vector Machines to model the quality of the TM segments. This is also related to our work in spirit, but our work is in the opposite direction, i.e. using SMT to enrich TM. Moreover, our ranking model is related to reranking (Shen et al., 2004) in SMT as well. However, our method does not focus on producing better 1-best translation output for an SMT system, but on improving the overall quality of the k-best list that TM systems present to post-editors. Some features in our work are also different in nature to those used in MT reranking. For instance we cannot use N-best posterior scores as they do not make sense for the TM outputs. 3 3.1 The Support Vector Machines The SVM Classifier Classical SVMs (Cortes and Vapnik, 1995) are binary classifiers that classify an input instance based on decision rules which minimize the regularized"
C10-2043,2009.mtsummit-papers.14,0,0.468633,"entries will remain untouched. In our forthcoming paper, we recommend SMT outputs to a TM user when a binary classifier predicts that SMT outputs are more suitable for post-editing for a particular sentence. The research presented here continues the line of research in the second strand. The difference is that we do not limit ourselves to the 1-best output but try to produce a k-best output in a ranking model. The ranking scheme also enables us to show all TM hits to the user, and thus further protects the TM assets. There has also been work to improve SMT using the knowledge from the TM. In (Simard and Isabelle, 2009), the SMT system can produce a better translation when there is an exact or close match in the corresponding TM. They use regression Support Vector Machines to model the quality of the TM segments. This is also related to our work in spirit, but our work is in the opposite direction, i.e. using SMT to enrich TM. Moreover, our ranking model is related to reranking (Shen et al., 2004) in SMT as well. However, our method does not focus on producing better 1-best translation output for an SMT system, but on improving the overall quality of the k-best list that TM systems present to post-editors. S"
C10-2043,2006.amta-papers.25,0,0.384507,"erved and the cost estimation is still valid as an upper bound. More specifically, we recast SMT-TM integration as a ranking problem, where we apply the Ranking SVM technique to produce a ranked list of translations combining the k-best lists of both the MT and the TM systems. We use features independent of the MT and TM systems for ranking, so that outputs from MT and TM can have the same set of features. Ideally the translations should be ranked by their associated postediting efforts, but given the very limited amounts of human annotated data, we use an automatic MT evaluation metric, TER (Snover et al., 2006), which is specifically designed to simulate postediting effort to train and test our ranking model. The rest of the paper is organized as follows: we first briefly introduce related research in Section 2, and review Ranking SVMs in Section 3. The formulation of the problem and experiments with the ranking models are presented in Sections 4 and 5. We analyze the post-editing effort approximated by the TER metric in Section 6. Section 7 concludes and points out avenues for future research. 2 Related Work There has been some work to help TM users to apply MT outputs more smoothly. One strand is"
C10-2043,P07-2045,0,\N,Missing
C12-1010,D11-1033,0,0.214475,"Missing"
C12-1010,2011.mtsummit-papers.32,1,0.769403,"Missing"
C12-1010,2012.eamt-1.41,1,0.717213,"Missing"
C12-1010,2005.eamt-1.9,0,0.0486176,"Missing"
C12-1010,N09-1025,0,0.0684642,"Missing"
C12-1010,P11-2071,0,0.0542134,"Missing"
C12-1010,eck-etal-2004-language,0,0.0767698,"Missing"
C12-1010,D10-1044,0,0.208686,"Missing"
C12-1010,W07-0717,0,0.261809,"Missing"
C12-1010,2011.mtsummit-papers.10,0,0.207403,"Missing"
C12-1010,W11-2123,0,0.0575828,"Missing"
C12-1010,2005.eamt-1.19,0,0.198143,"Missing"
C12-1010,W04-3250,0,0.18146,"Missing"
C12-1010,2005.mtsummit-papers.11,0,0.0133593,"Missing"
C12-1010,P07-2045,0,0.0155806,"Missing"
C12-1010,W07-0733,0,0.288199,"Missing"
C12-1010,N10-1062,0,0.0748053,"Missing"
C12-1010,P03-1021,0,0.151809,"Missing"
C12-1010,J03-1002,0,0.0126689,"Missing"
C12-1010,P02-1040,0,0.0843489,"Missing"
C12-1010,2011.mtsummit-papers.27,1,0.837951,"Missing"
C12-1010,2006.amta-papers.25,0,0.0693185,"Missing"
C16-1131,D11-1033,0,0.0297315,"edding = 50 setting. For the baseline systems with embedding 7 https://catalog.ldc.upenn.edu/LDC2011T07 1394 size 50 and 100, we observe a sudden explosion in the evaluation perplexities with the decay factor setting described in Section 4.1. We experimentally set the learning rate to a decay factor of 0.5 after 8 iterations. In Table 6, the DAGRU approach can produce better perplexity results in all settings. 5 Related Work Domain adaptation for n-gram LMs is a well-studied research field. In general, there are approaches to select data which are similar to ID from GD (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Toral, 2013). There are also model mixture approaches (Bellegarda, 2004; Hsu and Glass, 2006; Allauzen and Riley, 2011), which try to find a weight to combine the ID LM and GD LM. In neural LM work, one approach to perform domain adaptation is to use an additional adaptation layer to combine the GD neural LM into the ID neural LM (Park et al., 2010; Ter-Sarkisov et al., 2014). However, an LM trained on all GD data is required. Curriculum learning (Bengio et al., 2009), which rearranges the training data in a particular order to improve generalization, is also applied on dom"
C16-1131,J81-4005,0,0.754477,"Missing"
C16-1131,P13-2119,0,0.019503,"or the baseline systems with embedding 7 https://catalog.ldc.upenn.edu/LDC2011T07 1394 size 50 and 100, we observe a sudden explosion in the evaluation perplexities with the decay factor setting described in Section 4.1. We experimentally set the learning rate to a decay factor of 0.5 after 8 iterations. In Table 6, the DAGRU approach can produce better perplexity results in all settings. 5 Related Work Domain adaptation for n-gram LMs is a well-studied research field. In general, there are approaches to select data which are similar to ID from GD (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Toral, 2013). There are also model mixture approaches (Bellegarda, 2004; Hsu and Glass, 2006; Allauzen and Riley, 2011), which try to find a weight to combine the ID LM and GD LM. In neural LM work, one approach to perform domain adaptation is to use an additional adaptation layer to combine the GD neural LM into the ID neural LM (Park et al., 2010; Ter-Sarkisov et al., 2014). However, an LM trained on all GD data is required. Curriculum learning (Bengio et al., 2009), which rearranges the training data in a particular order to improve generalization, is also applied on domain adaptation on"
C16-1131,D08-1089,0,0.0834468,"Missing"
C16-1131,W11-2123,0,0.120853,"Missing"
C16-1131,W06-1644,0,0.0270689,"50 and 100, we observe a sudden explosion in the evaluation perplexities with the decay factor setting described in Section 4.1. We experimentally set the learning rate to a decay factor of 0.5 after 8 iterations. In Table 6, the DAGRU approach can produce better perplexity results in all settings. 5 Related Work Domain adaptation for n-gram LMs is a well-studied research field. In general, there are approaches to select data which are similar to ID from GD (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Toral, 2013). There are also model mixture approaches (Bellegarda, 2004; Hsu and Glass, 2006; Allauzen and Riley, 2011), which try to find a weight to combine the ID LM and GD LM. In neural LM work, one approach to perform domain adaptation is to use an additional adaptation layer to combine the GD neural LM into the ID neural LM (Park et al., 2010; Ter-Sarkisov et al., 2014). However, an LM trained on all GD data is required. Curriculum learning (Bengio et al., 2009), which rearranges the training data in a particular order to improve generalization, is also applied on domain adaptation on a neural LM (Shi et al., 2013). In the work of Mikolov and Zweig (2012), word predictions are"
C16-1131,W04-3250,0,0.186791,"Missing"
C16-1131,J93-2004,0,0.0533767,"obtained from a very large GD data set. The word vectors in such a model are not domain-specific. By keeping it static, we interpret it as a “knowledge database”, and the knowledge should be consistent. Another practical reason for not updating the pre-trained GD word vector model is that fewer parameters need to be optimized in the network. 4 Experiments 4.1 Adaptation on Penn Treebank and News Corpus The typical setting of domain adaptation is small amount of ID training data and large amount of DG training data. Accordingly, we choose to use the availability of widely known Penn Treebank (Marcus et al., 1993) portion of the Wall Street Journal corpus in our LM adaptation experiment.5 The words outside the 10K vocabulary frequency list are mapped to the special unk token; sections 0-20 are used for training, and sections 21-22 are used for validation. We report the perplexity on data from sections 2324. More detailed data statistics are summarized in Table 1. We use the pre-trained word vector Google word2vec6 (Mikolov et al., 2013a) as the GD “look-up table”. It is trained on about 100 billion words, and consists of 3 million words and phrases. The word vectors are 300-dimensional in the word2vec"
C16-1131,N13-1090,0,0.352866,"on approach which has the ability to learn knowledge from huge corpora at speed. The question that arises here is how to make use of large amounts of GD data but avoiding long training times. In neural network training, words are represented as distributed representations, so-called “word vectors”, which can be pre-trained or trained with a specific task in mind. Although a pre-trained word vector model is also learned with a neural network, the training can be very fast. Recent optimized work shows learning word vectors can process more than 100 billion tokens in one day on a single machine (Mikolov et al., 2013c). Another advantage of a pre-trained word vector model is its flexibility, as it can be used later for different task-specific models. Furthermore, the pre-trained and the task-specific word vector models have no functional difference. Accordingly, we think it is very natural to use them This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1386 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1386–1397, Osaka, Japan, December 11-17"
C16-1131,P10-2041,0,0.58879,"are dissimilarities between the training and the testing environments. In research on domain adaptation, training data with the same style and topic (van der Wees et al., 2015) as the test data is often defined as in-domain (ID) data, with everything else called general-domain (GD) data. The ID data is often scarce and expensive to obtain, whereas the GD is plentiful and easy to access. One approach to address the situation of scarce ID training data is through data selection. For example, using the perplexity difference between ID and GD can select data that is close to ID and away from GD (Moore and Lewis, 2010). The selected data can then be concatenated with ID data for training. However, the drawback of using data selection is a threshold needs to be set, which often requires many models to be trained and evaluated. The situation will worsen if neural networks are used since the computational cost is immense in neural network training, where models often require days to converge even with the help of GPU-accelerated computing. Thus, simply adapting previous domain adaptation techniques into the neural network framework may not be efficient or effective. Ideally, we want to have an adaptation appro"
C16-1131,D14-1162,0,0.0800812,"both LMs. Already after training iteration 2, the DAGRU LM starts to outperform the baseline LM in terms of perplexity at every iteration. The plots flatten after 18 iterations, and the learning begins to converge for both the baseline LM and DAGRU LM. To demonstrate the scalability of the DAGRU adaptation approach, we also train LMs adapting from other freely available word vector models. SENNA (Semantic/syntactic Extraction using a Neural Network Architecture) is the word vector model received after a LM training (Collobert et al., 2011). The training data is obtained from Wikipedia. GloVe (Pennington et al., 2014) – Global Vectors for Word Representation – provides several versions of word vector models. The glove 6b model is trained on Wikipedia data and the English Gigaword Fifth Edition corpus;7 the glove 42b model is trained on the Common Crawl data; and the glove 840b model is trained on the the Common Crawl and additional web data. Table 6 presents the experimental results of DAGRU adaptation using different word vector models as GD data. The baseline models are trained on the Penn Treebank only. The word embedding numbers in Table 6 indicate the word vector size of the adapting word vector model"
C16-1131,W13-2803,0,0.0127122,"stems with embedding 7 https://catalog.ldc.upenn.edu/LDC2011T07 1394 size 50 and 100, we observe a sudden explosion in the evaluation perplexities with the decay factor setting described in Section 4.1. We experimentally set the learning rate to a decay factor of 0.5 after 8 iterations. In Table 6, the DAGRU approach can produce better perplexity results in all settings. 5 Related Work Domain adaptation for n-gram LMs is a well-studied research field. In general, there are approaches to select data which are similar to ID from GD (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Toral, 2013). There are also model mixture approaches (Bellegarda, 2004; Hsu and Glass, 2006; Allauzen and Riley, 2011), which try to find a weight to combine the ID LM and GD LM. In neural LM work, one approach to perform domain adaptation is to use an additional adaptation layer to combine the GD neural LM into the ID neural LM (Park et al., 2010; Ter-Sarkisov et al., 2014). However, an LM trained on all GD data is required. Curriculum learning (Bengio et al., 2009), which rearranges the training data in a particular order to improve generalization, is also applied on domain adaptation on a neural LM (S"
C16-1131,P15-2092,0,0.0359037,"Missing"
C16-1131,P07-2045,0,\N,Missing
C16-1170,P11-2080,0,0.0260277,"k Topic modelling has been applied successfully in many SMT works, especially in the domain adaptation literature. The motivation for introducing topic-model information in MT is that the translation performance decreases when there are dissimilarities between the training and the testing domains. A better approach is to make the use of the topic knowledge learned during training. Such knowledge can yield a better word or phrase choice in translation. Early work shows that the lexical translation table conditioned on the provenance of each domain can significantly improve translation quality (Chiang et al., 2011). Eidelman (2012) extends the provenance idea by including topic-dependent lexical weighting probabilities on the source side. Hasler (2012) successfully combines sparse word-pair and phrase-pair features with topic models. Later, Hasler (2014) also reports that translation performance is improved by using similarity features, which are computed from training phrase-pair and test-context vectors generated from the phrase-pair topic model. However, these ideas cannot be directly applied in NMT given the different training algorithms used in SMT and NMT. Despite the fact that neural network trai"
C16-1170,P05-1033,0,0.17334,"ailable (e.g. the word bank in Figure 1), it is clear that the translation in the Financial domain is more likely to be selected because many of the source words are from the same topic. Topic models have been applied successfully in many SMT works. For example, similar “topic consistent” behaviour is also observed by Su (2015). In his work, a context-aware topic model is integrated into SMT for better lexical selection. Xiao et al. (2012) and Zhang et al. (2014) focus on document translations and propose a topic-similarity model and a topic-sensitivity model for hierarchical phrasebased SMT (Chiang, 2005) on the document level. The topic-similarity model is used to encourage or penalize topic-sensitive rules, and the topic-sensitivity model is applied to balance topic-insensitive rules. However, these approaches cannot be directly used in NMT. In this work, we propose our novel topic-informed NMT model. In topic models, word-topic distributions can be viewed as a vector. In NMT, words are represented as vector-space representations. Thus, it is very natural to use them together. Word expressions in neural models are derived by its near context words while the topic vectors represent the docume"
C16-1170,D14-1179,0,0.0285064,"Missing"
C16-1170,P12-2023,0,0.0569223,"Missing"
C16-1170,D08-1089,0,0.0775974,"Missing"
C16-1170,2012.iwslt-papers.17,0,0.0411233,"Missing"
C16-1170,W14-3358,0,0.059939,"Missing"
C16-1170,W11-2123,0,0.0301983,"Missing"
C16-1170,2005.iwslt-1.8,0,0.120064,"Missing"
C16-1170,W04-3250,0,0.383993,"Missing"
C16-1170,P15-1002,0,0.0689874,"Missing"
C16-1170,W02-1018,0,0.126041,"Missing"
C16-1170,P00-1056,0,0.0398101,"Missing"
C16-1170,P03-1021,0,0.0206403,"Missing"
C16-1170,P02-1040,0,0.0951818,"oehn, 2004) improvements upon the baseline NMT using bootstrapping method at the level p = 0.01 and p = 0.05 level, respectively (with 1000 iterations). 36.5 36.5 36 36 35.5 35.5 Source Topic Number 150 100 80 50 40 30 20 150 33.5 100 33.5 80 34 50 34 40 34.5 30 34.5 20 35 10 35 10 BLEU 37 BLEU 37 Target Topic Number Figure 3: Topic numbers vs. translation BLEU scores on the NIST 2002 development dataset. and the models are saved at each 1,000 updates. The training takes approximately 3 days on an NVIDIA GeForce GTX TITAN X GM200 GPU machine. We then choose the final model based on the BLEU4 (Papineni et al., 2002) score on the development data. 5.2 Results Table 2 presents the experiment results on the development and test data. In Table 2, the number next to each topic-informed NMT system indicates the number of topics used in the system, i.e. we use 40 source topics in the source topic-informed NMT system, and 10 target topics in the target topic-informed NMT model. The source and target topic numbers are experimentally chosen from {10, 20, 30, 40, 50, 80, 100, 150} according to the development BLEU scores, as seen in Figure 3. We then leverage topic information on both source (with 40 topics) and ta"
C16-1170,W16-2209,0,0.0275087,"has been shown to be advantageous in many natural language processing tasks, little work has been proposed on using additional knowledge in NMT. Gulcehre et al. (2015) leverage monolingual corpora for NMT and propose two approaches to integrate a neural language model into the encoder-decoder architecture. He et al. (2016) integrate SMT features into NMT in order to solve the out-of-vocabulary problem. Furthermore, they use a n-gram language model trained on large monolingual data to enhance the local fluency of translation outputs. Linguistic information can also be used during NMT training (Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2016). There are also works that include topic modelling in neural language model training. Mikolov and Zweig (2012) use a contextual real-valued input vector associated with each word in a recurrent neural network (RNN) language model. 3 3.1 Neural Machine Translation Encoder-Decoder Architecture In a nutshell, the fundamental job of the encoder-decoder architecture (Cho et al., 2014) in NMT is to probabilistically decode a target sequence given the encoded source sequence, where the two sequences can be of different lengths. Given a sentence pair (S, T ), S is the"
C16-1170,P15-1023,0,0.0359834,"Missing"
C16-1170,I05-3027,0,0.0638,"Missing"
C16-1170,P12-1079,1,0.862252,"c in the translations during the decoding phase, and consequently better translations can be produced. Specifically, when a source word has more than one translation option available (e.g. the word bank in Figure 1), it is clear that the translation in the Financial domain is more likely to be selected because many of the source words are from the same topic. Topic models have been applied successfully in many SMT works. For example, similar “topic consistent” behaviour is also observed by Su (2015). In his work, a context-aware topic model is integrated into SMT for better lexical selection. Xiao et al. (2012) and Zhang et al. (2014) focus on document translations and propose a topic-similarity model and a topic-sensitivity model for hierarchical phrasebased SMT (Chiang, 2005) on the document level. The topic-similarity model is used to encourage or penalize topic-sensitive rules, and the topic-sensitivity model is applied to balance topic-insensitive rules. However, these approaches cannot be directly used in NMT. In this work, we propose our novel topic-informed NMT model. In topic models, word-topic distributions can be viewed as a vector. In NMT, words are represented as vector-space representa"
C16-1170,P07-2045,0,\N,Missing
C16-1243,D14-1179,0,0.0386727,"Missing"
C16-1243,P14-1129,0,0.0815892,"Missing"
C16-1243,P12-1092,0,0.0382067,"re: the phrase translation probability φ(e|f ), inverse phrase translation probability φ(f |e), lexical weighting lex(e|f ) and inverse lexical weighting lex(f |e). These scores are computed based on the co-occurrence of phrase pairs in training corpora and do not indicate any other information about phrases, their relation or context. Our goal in this research is to extend this set of features by incorporating semantic information of phrase pairs. Word embeddings are numerical representations of words which preserve semantic and syntactic information about words themselves and their context (Huang et al., 2012; Luong et al., 2013; Mikolov et al., 2013a; Mikolov et al., 2013b). They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al.,"
C16-1243,P07-2045,0,0.00688535,"he main set of bilingual features. Our goal is to enrich that set of features, as a better feature set should yield better translations. We propose new scores generated by a Convolutional Neural Network (CNN) which indicate the semantic relatedness of phrase pairs. We evaluate our model in different experimental settings with different language pairs. We observe significant improvements when the proposed features are incorporated into the PBSMT pipeline. 1 Introduction PBSMT models sentence-level translation with a phrase-based setting in which sentences are decomposed into different phrases (Koehn et al., 2007; Koehn, 2009). At each step, for a given source phrase the best candidate among the target phrases is selected as its translation. Phrasal translations are combined together to produce the sentence-level translation. This is a high-level view of PBSMT and there are many other processes involved in the main pipeline. Different bilingual and monolingual features are taken into account to make the final translation as adequate and fluent as possible. In this paper we only focus on the phrase-pairing process and try to enrich that part. The standard baseline bilingual features in the PBSMT pipeli"
C16-1243,W04-3250,0,0.126027,"K parallel sentences. All sentences are randomly selected from the En–Fr part of the Europarl (Koehn, 2005) collection. In our models we use 5-gram language models trained using the IRSTLM toolkit (Stolcke, 2002) and we tune models via MERT (Och, 2003). The extended systems those which include new scores within their phrase tables. In the extended systems we keep everything unchanged. only adding two new scores to the phrase table and the re-tune the PBSMT engine. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05 (Koehn, 2004). As Table 1 shows, adding new features considerably boosts the PBSMT model, especially for smallsize datasets. For example, the BLEU score reported for the En→Fr system trained on the 500K dataset is 35.2, while a better performance (35.5) is achievable on the smaller 200K dataset in the presence of the new features. Usually as the size of the phrase table grows, the impact of such models attenuate, as large(r) phrase tables are rich enough to cover different cases and do not need to be enriched. Accordingly, such models are more suitable for small/medium-size datasets or low-resource languag"
C16-1243,2005.mtsummit-papers.11,0,0.00906448,"k 34.5 35.2 +0.7 1M 35.4 35.5 +0.1 Table 1: Experimental results on the En–Fr pair. The numbers indicate the BLEU scores. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05. BLEU (Papineni et al., 2002) is used as the evaluation metric. We trained 3 baseline systems over datasets of 200K, 500K and 1 million (1M) parallel sentences. As the test set we use a corpus of 1.5K parallel sentences and the validation set includes 2K parallel sentences. All sentences are randomly selected from the En–Fr part of the Europarl (Koehn, 2005) collection. In our models we use 5-gram language models trained using the IRSTLM toolkit (Stolcke, 2002) and we tune models via MERT (Och, 2003). The extended systems those which include new scores within their phrase tables. In the extended systems we keep everything unchanged. only adding two new scores to the phrase table and the re-tune the PBSMT engine. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05 (Koehn, 2004). As Table 1 shows, adding new features considerably boosts the PBSMT model, especially for sma"
C16-1243,P09-5002,0,0.0148636,"gual features. Our goal is to enrich that set of features, as a better feature set should yield better translations. We propose new scores generated by a Convolutional Neural Network (CNN) which indicate the semantic relatedness of phrase pairs. We evaluate our model in different experimental settings with different language pairs. We observe significant improvements when the proposed features are incorporated into the PBSMT pipeline. 1 Introduction PBSMT models sentence-level translation with a phrase-based setting in which sentences are decomposed into different phrases (Koehn et al., 2007; Koehn, 2009). At each step, for a given source phrase the best candidate among the target phrases is selected as its translation. Phrasal translations are combined together to produce the sentence-level translation. This is a high-level view of PBSMT and there are many other processes involved in the main pipeline. Different bilingual and monolingual features are taken into account to make the final translation as adequate and fluent as possible. In this paper we only focus on the phrase-pairing process and try to enrich that part. The standard baseline bilingual features in the PBSMT pipeline by default"
C16-1243,W13-3512,0,0.0327847,"lation probability φ(e|f ), inverse phrase translation probability φ(f |e), lexical weighting lex(e|f ) and inverse lexical weighting lex(f |e). These scores are computed based on the co-occurrence of phrase pairs in training corpora and do not indicate any other information about phrases, their relation or context. Our goal in this research is to extend this set of features by incorporating semantic information of phrase pairs. Word embeddings are numerical representations of words which preserve semantic and syntactic information about words themselves and their context (Huang et al., 2012; Luong et al., 2013; Mikolov et al., 2013a; Mikolov et al., 2013b). They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 20"
C16-1243,P03-1021,0,0.0131858,"icate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05. BLEU (Papineni et al., 2002) is used as the evaluation metric. We trained 3 baseline systems over datasets of 200K, 500K and 1 million (1M) parallel sentences. As the test set we use a corpus of 1.5K parallel sentences and the validation set includes 2K parallel sentences. All sentences are randomly selected from the En–Fr part of the Europarl (Koehn, 2005) collection. In our models we use 5-gram language models trained using the IRSTLM toolkit (Stolcke, 2002) and we tune models via MERT (Och, 2003). The extended systems those which include new scores within their phrase tables. In the extended systems we keep everything unchanged. only adding two new scores to the phrase table and the re-tune the PBSMT engine. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05 (Koehn, 2004). As Table 1 shows, adding new features considerably boosts the PBSMT model, especially for smallsize datasets. For example, the BLEU score reported for the En→Fr system trained on the 500K dataset is 35.2, while a better performance (35.5)"
C16-1243,P02-1040,0,0.0987152,"Experimental Results To show the impact of the proposed scores we perform several experiments. In the first experiment we evaluate the model on the English–French (En–Fr) pair. Results are reported in Table 1. System Baseline Extended Improvement En→Fr 200K 34.4 35.5 +1.1 500k 35.2 36.0 +0.8 Fr→En 1M 35.7 36.0 +0.3 200k 33.8 35.1 +1.3 500k 34.5 35.2 +0.7 1M 35.4 35.5 +0.1 Table 1: Experimental results on the En–Fr pair. The numbers indicate the BLEU scores. The bold-faced scores indicate improvements are statistically significant according to paired bootstrap re-sampling with p = 0.05. BLEU (Papineni et al., 2002) is used as the evaluation metric. We trained 3 baseline systems over datasets of 200K, 500K and 1 million (1M) parallel sentences. As the test set we use a corpus of 1.5K parallel sentences and the validation set includes 2K parallel sentences. All sentences are randomly selected from the En–Fr part of the Europarl (Koehn, 2005) collection. In our models we use 5-gram language models trained using the IRSTLM toolkit (Stolcke, 2002) and we tune models via MERT (Och, 2003). The extended systems those which include new scores within their phrase tables. In the extended systems we keep everything"
C16-1243,W15-4911,1,0.892229,"Mikolov et al., 2013b). They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 2014; Gouws et al., 2015; Passban et al., 2015a; Zhao et al., 2015; Passban et al., 2016) with different architectures for different tasks such as MT and document classification. In our work we also try to follow the same research line. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Japan, December 11-17 2016. mixed embeddings. Using the propose"
C16-1243,W16-3403,1,0.859545,"e information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 2014; Gouws et al., 2015; Passban et al., 2015a; Zhao et al., 2015; Passban et al., 2016) with different architectures for different tasks such as MT and document classification. In our work we also try to follow the same research line. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Japan, December 11-17 2016. mixed embeddings. Using the proposed data structure, source and target words a"
C16-1243,D14-1162,0,0.0870383,"ine. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Japan, December 11-17 2016. mixed embeddings. Using the proposed data structure, source and target words are linked together, so we call our embeddings mixed. Our model is a bilingual extension to well-known embedding models (Mikolov et al., 2013a; Pennington et al., 2014) with a quite different architecture which is fine-tuned for MT tasks. The reminder of the paper is structured as follows. In Section 2 we briefly review some similar models which train bilingual embeddings. Section 3 discusses our neural model and how we incorporate results from our model into the PBSMT pipeline. Section 4 explains the results from several experiments to show the impact of the proposed model. Finally, Section 5 concludes the paper with some avenues for future work. 2 Background One of the most successful neural models proposed for training word embeddings is Word2Vec (Mikolov"
C16-1243,N15-1176,0,0.0840114,". They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 2014; Gouws et al., 2015; Passban et al., 2015a; Zhao et al., 2015; Passban et al., 2016) with different architectures for different tasks such as MT and document classification. In our work we also try to follow the same research line. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Japan, December 11-17 2016. mixed embeddings. Using the proposed data structure, so"
C16-1243,D13-1141,0,0.430929,"et al., 2012; Luong et al., 2013; Mikolov et al., 2013a; Mikolov et al., 2013b). They also preserve information about word order. These types of contextual, syntactic and semantic information can be quite useful for machine translation (MT), but being by its very nature a bilingual application, it requires bilingual (cross-lingual) information. Accordingly, we need methods to train bilingual embeddings via which we can access syntactic and semantic information about the source side as well as the target. Several papers have explored the training of bilingual embeddings (Mikolov et al., 2013b; Zou et al., 2013; Cho et al., 2014; Gouws et al., 2015; Passban et al., 2015a; Zhao et al., 2015; Passban et al., 2016) with different architectures for different tasks such as MT and document classification. In our work we also try to follow the same research line. We propose a multi-plane data structure and a CNN to train This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2582 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2582–2591, Osaka, Jap"
C16-1243,2015.eamt-1.12,1,\N,Missing
C18-1265,W17-4703,0,0.0312819,"Missing"
C18-1265,D14-1179,0,0.0310498,"Missing"
C18-1265,P16-1160,0,0.133621,"t et al. (2017) investigated the impact of different word representation models in the context of factored NMT. Our work is also an example of models which try to provide richer information when the source side is an MRL. Models reviewed so far address the problem of morphology on the source side. In contrast, there is a group of models which study the same problem for the target side. Huck et al. (2017) compared different word-segmentation models, including linguistically motivated as well as statistical techniques, to find the most appropriate segmentation scheme when translating into MRLs. Chung et al. (2016) tried to design a suitable architecture when the target language is an MRL. They benefit from using a character-based decoder which partially resolves the problem. Passban et al. (2018) proposed a similar approach in which they equipped the character-based decoder with an additional morphology table to inform the decoder with the target language’s morphological structures. 1 Comparing translation results generated by different word-, subword-, and character-based architectures on different language pairs shows that there is no character-based model which is able to outperform its word- and/or"
C18-1265,P16-2058,0,0.0395449,"Missing"
C18-1265,I17-1015,0,0.0203217,"rent language pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counterparts. Please see results at http://matrix.statmt.org/?mode=all 3136 Apart from these models, there are others that do not directly address the problem of morphology but their solutions could be quite useful to translate MRLs. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare and unseen words with the help of a post-translation phase to exchange unknown tokens with their potential translations. Dalvi et al. (2017) did not propose a new model but studied the impact of morphological information in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in monolingual and bilingual settings. 3 Proposed Approach We propose an NMT architecture with a double-source encoder and double-attentive decoder for translating from MRLs. Our"
C18-1265,N16-1101,0,0.310235,"morphological information in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in monolingual and bilingual settings. 3 Proposed Approach We propose an NMT architecture with a double-source encoder and double-attentive decoder for translating from MRLs. Our neural architecture is inspired by models proposed in Firat et al. (2016) and Zoph and Knight (2016). It takes inputs from two different channels: one channel which is referred to as the main channel sends stem information (main input), and the other one (the auxiliary channel) sends affix information. If the input is w0 , w1 , ..., wn for the (original) encoder-decoder model, our proposed architecture takes two sequences of ς0 , ς1 , ..., ςn and τ0 , τ1 , ..., τn through the main and auxiliary channels, respectively, where wi shows the surface form of a word whose stem is ςi , and affix information associated with wi is given by τi . Our new neural architecture is"
C18-1265,W17-4706,0,0.0194826,"cter- and word-based encoder to try to solve the out-of-vocabulary problem. Vylomova et al. (2016) tackled the problem by comparing the impact of different representation schemes on the encoder. Similarly, Burlot et al. (2017) investigated the impact of different word representation models in the context of factored NMT. Our work is also an example of models which try to provide richer information when the source side is an MRL. Models reviewed so far address the problem of morphology on the source side. In contrast, there is a group of models which study the same problem for the target side. Huck et al. (2017) compared different word-segmentation models, including linguistically motivated as well as statistical techniques, to find the most appropriate segmentation scheme when translating into MRLs. Chung et al. (2016) tried to design a suitable architecture when the target language is an MRL. They benefit from using a character-based decoder which partially resolves the problem. Passban et al. (2018) proposed a similar approach in which they equipped the character-based decoder with an additional morphology table to inform the decoder with the target language’s morphological structures. 1 Comparing"
C18-1265,P15-1001,0,0.0596598,"er-based decoder with an additional morphology table to inform the decoder with the target language’s morphological structures. 1 Comparing translation results generated by different word-, subword-, and character-based architectures on different language pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counterparts. Please see results at http://matrix.statmt.org/?mode=all 3136 Apart from these models, there are others that do not directly address the problem of morphology but their solutions could be quite useful to translate MRLs. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare and unseen words with the help of a post-translation phase to exchange unknown tokens with their potential translations. Dalvi et al. (2017) did not propose a new model but studied the impact of morphological information in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence lev"
C18-1265,W04-3250,0,0.0645315,"[ς]1 [pre+suf]2 [ς]1 [τ ]2 [ς]1 [τ ]2 char char char 26.11 26.74 26.29 22.95 23.44 23.40 23.62 23.81 23.74 Costa-juss`a and Fonollosa (2016) Firat et al. (2016) Lee et al. (2017) Sennrich and Haddow (2016)? Our model pre+suf affix-cς cτ affix-cτ Table 2: Source and Target indicate the data type for the encoder and decoder, respectively. ς is the stem, pre is the prefix and suf is the suffix. τ is the affix token. The bold-faced score is the best score for the direction and the score with * shows the best performance reported by other existing models. According to paired bootstrap re-sampling (Koehn, 2004) with p = 0.05, the bold-faced number is significantly better than the score with *. Brackets show different channels and the + mark indicates the summation, e.g. [ς]1 [pre+suf]2 means the first channel takes a stem at each step and the second channel takes the summation of the prefix and suffix of the associated stem. • is the concatenation operation and ? indicates that results were produced in our experimental setting using our re-implementation of the original model. The second row shows the model proposed by Costa-juss`a and Fonollosa (2016), in which a complicated convolutional module is"
C18-1265,Q17-1026,0,0.489613,"rds are consumed character by character. Usually, there is a convolutional module after the input layer to read characters, connect them to each other, and extract inter-character relations. This approach is helpful as it requires no preprocessing step, but there are two main problems with that. Similar to the previous group, the length of the input sequences could be an issue as by segmenting words into characters an input sequence with a limited number of words is changed to a long sequence of characters. Furthermore, the convolutional computation over characters could be quite costly, e.g. Lee et al. (2017) use 200 convolutional filters of width 1 along with 200 filters of width 2, 250 filters of width 3, and continue up to a filter size of 300 with width 8 to extract useful information on the source side. This amount of computation is carried out only in one layer (for one word), so in addition there are max-pooling This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 3135 License details: http:// layers followed by 4 highway layers (Srivastava et al., 2015), and then recurrent layers on top. This is a complex architecture wh"
C18-1265,L16-1147,0,0.0255189,"ic gradient descent with Adam (Kingma and Ba, 2015). The mini-batch size is 80, the beam search width is 20, and the norm of the gradient is clipped with the threshold 1.0. 4.1 Data Preparation Our models are trained to translate from German (De), Russian (Ru), and Turkish (Tr) into English (En). For German and Russian, we used WMT-15 datasets,2 where the De–En corpus includes 4.5M parallel sentences and the size of the Ru–En corpus is 2.1M. The newstest-2013 and newstest-2015 datasets are used as the development and test sets, respectively. For Tr–En, we used the OpenSubtitle2016 collection (Lison and Tiedemann, 2016).3 We randomly selected 3K sentences for each of the development and test sets, and 4M for training. 2 3 http://www.statmt.org/wmt15/translation-task.html http://opus.nlpl.eu/OpenSubtitles2016.php 3139 English–German En Sentence Token Type Stem Affix Prefix Suffix Char De 4.2M 103, 692, 553 96, 235, 845 103, 574 143, 329 21, 223 26, 301 13, 410 24, 054 3, 104 5, 208 3, 285 4, 974 355 302 English–Russian En Ru 2.1M 43, 944, 989 39, 694, 475 70, 376 119, 258 15, 964 19, 557 9, 320 17, 542 2, 959 3, 324 2, 219 3, 460 360 323 English–Turkish En Tr 4.0M 24, 875, 286 17, 915, 076 108, 699 178, 672 1"
C18-1265,P16-1100,0,0.033724,"portant issue in the field of sequence modeling as it directly affects the model’s architecture and its performance. There is a fair amount of research carried out to address this problem, which can be discussed in two main categories. One group of models tries to cope with the problem on the encoder side where the goal is to understand the rich morphology of the source language. Kim et al. (2016) proposed a convolutional module to process complex inputs for the problem of language modeling. Costa-juss`a and Fonollosa (2016) and Lee et al. (2017) adapted the same convolutional encoder to NMT. Luong and Manning (2016) designed a hybrid character- and word-based encoder to try to solve the out-of-vocabulary problem. Vylomova et al. (2016) tackled the problem by comparing the impact of different representation schemes on the encoder. Similarly, Burlot et al. (2017) investigated the impact of different word representation models in the context of factored NMT. Our work is also an example of models which try to provide richer information when the source side is an MRL. Models reviewed so far address the problem of morphology on the source side. In contrast, there is a group of models which study the same probl"
C18-1265,P15-1002,0,0.0498344,"der with the target language’s morphological structures. 1 Comparing translation results generated by different word-, subword-, and character-based architectures on different language pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counterparts. Please see results at http://matrix.statmt.org/?mode=all 3136 Apart from these models, there are others that do not directly address the problem of morphology but their solutions could be quite useful to translate MRLs. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare and unseen words with the help of a post-translation phase to exchange unknown tokens with their potential translations. Dalvi et al. (2017) did not propose a new model but studied the impact of morphological information in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in mono"
C18-1265,P02-1040,0,0.104755,"cond extension we can expect the neural network to learn our desired information. Basically, by defining the affix token we try to introduce an additional cell of memory, in which the NMT model and the affix encoder store the useful information it has learned 4 In these two extensions everything is the same but the attention mechanism. Equations (5) and (6) explain the attention mechanisms for affix-cς cτ and affix-cτ , respectively. 3140 (in addition to stem). 4.2 Experimental Results The results obtained from our experiments are reported in Table 2. The numbers in the table are BLEU scores (Papineni et al., 2002) of different neural models. We compare our models to all existing models which translate from MRLs or reported experimental results on our datasets. The first row of the table shows an encoder-decoder model where the decoder works at the character level and uses the architecture proposed in Chung et al. (2016). The first row can be considered as a baseline for all other models reported in the table, as it does not use any complicated neural architecture on the source side. For each word it simply sums stem, prefix, and suffix embeddings together and sends the summed vector as the word-level r"
C18-1265,N18-1006,1,0.442297,"ation when the source side is an MRL. Models reviewed so far address the problem of morphology on the source side. In contrast, there is a group of models which study the same problem for the target side. Huck et al. (2017) compared different word-segmentation models, including linguistically motivated as well as statistical techniques, to find the most appropriate segmentation scheme when translating into MRLs. Chung et al. (2016) tried to design a suitable architecture when the target language is an MRL. They benefit from using a character-based decoder which partially resolves the problem. Passban et al. (2018) proposed a similar approach in which they equipped the character-based decoder with an additional morphology table to inform the decoder with the target language’s morphological structures. 1 Comparing translation results generated by different word-, subword-, and character-based architectures on different language pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counterparts. Please see results at http://matrix.statmt.org/?mode=all 3136 Apart from these models, there are others that do not directly address the problem of morpholog"
C18-1265,W16-2209,0,0.0436963,"Missing"
C18-1265,P16-1162,0,0.0855822,"e an architectural design (manipulation) inspired by the nature of this particular problem, which is able to decompose MCWs or process them in decomposed forms. Subword-level NMT models are the most preferred alternatives to translate from MRLs, which can be discussed in two main categories. One group of models does not change the neural architecture but manipulates data by decomposing words into their subunits. In this approach either linguistically motivated morphological analyzers such as Morfessor (Smit et al., 2014) or purely statistical models such as the byte-pair encoding (bpe) model (Sennrich et al., 2016) are applied to process words. This approach, by its very nature, seems to be a promising solution as it changes the sparse surface formbased vocabulary set into a much smaller set of fundamental subunits. The size of a set including atomic subunits, especially for MRLs, is considerably smaller than that of the vocabulary set. Moreover, this solution partially solves the out-of-vocabulary word problem, as the chance of facing an unknown surface form is much higher than the chance of facing an unknown subunit. The aforementioned approach could help generate better translations, but there is a p"
C18-1265,E14-2006,0,0.036409,"Missing"
C18-1265,N16-1004,0,0.0288903,"n in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological information is learned via the model and how the model deals with complex structures. Passban (2018) extensively discussed the problem of morphology at the word and sequence level and proposed solutions for modeling and translating sequences in monolingual and bilingual settings. 3 Proposed Approach We propose an NMT architecture with a double-source encoder and double-attentive decoder for translating from MRLs. Our neural architecture is inspired by models proposed in Firat et al. (2016) and Zoph and Knight (2016). It takes inputs from two different channels: one channel which is referred to as the main channel sends stem information (main input), and the other one (the auxiliary channel) sends affix information. If the input is w0 , w1 , ..., wn for the (original) encoder-decoder model, our proposed architecture takes two sequences of ς0 , ς1 , ..., ςn and τ0 , τ1 , ..., τn through the main and auxiliary channels, respectively, where wi shows the surface form of a word whose stem is ςi , and affix information associated with wi is given by τi . Our new neural architecture is based on a hypothesis whic"
C18-1321,C14-1011,1,0.416417,"Missing"
C18-1321,J13-3008,0,0.0651524,"Missing"
C18-1321,E14-4001,0,0.0605679,"Missing"
C18-1321,P13-2028,0,0.032294,"Missing"
C18-1321,D10-1012,0,0.232371,"roach that computes the agreement of two partitions of objects into varied clusters was proposed in this paper. This is done depending on the information content related to the series of decisions made by the partitions on single pairs of objects. OPTIMSRC results demonstrated that meta clustering is far better than individual clustering techniques. Moreno et al. (2013), adapted the K-means algorithm to a third-order similarity measure and proposed a stopping criterion that determines the optimal number of clusters automatically. Experiments were conducted on two standard data sets, MORESQUE (Navigli and Crisafulli, 2010) and ODP-239 (Carpineto and Romano, 2010), and showed significant improvement over all existing text-based SRC techniques developed by then. Later, Acharya et al. (2014), first defined the SRC task as a multi-objective problem. They defined two objective functions ( separability & compactness), that are optimized parallely with the help of AMOSA (Bandyopadhyay et al., 2008). Their evaluations outperformed knowledge-driven exogenous strategies (Scaiella et al., 2012), text-based endogenous SRC approaches and algorithms. In another work, the multi-view clustering approach proposed by Wahid et al"
C18-1321,W16-2346,0,0.0363668,"Missing"
C18-1321,P17-1142,0,0.0155885,"creased attention over the past few years. These models have been used in various applications: image captioning (You et al., 2016); sentiment analysis (Poria et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. Licence details: 3793 Proceedings of the 27th International Conference on Computational Linguistics, pages 3793–3805 Santa Fe, New Mexico, USA, August 20-26, 2018. 2016); multimodal machine translation (Specia et al., 2016); visual question answering (Antol et al., 2015); combating human trafficking (Tong et al., 2017); and detection of Cyber-bullying (Zhong et al., 2016). In today’s online environment, the strategic use of multimedia (image, video etc.) has become increasingly important part of creating a successful website. Visual information embedded in web documents provides right-angled information that is free of ambiguities of natural language. It can also improve search ranking and Search engine optimization (SEO) scores on many levels that contribute to search visibility, find-ability, user satisfaction, experience and engagement. As a result, use of multimedia content in web documents has become m"
carl-etal-2002-toward,A94-1016,0,\N,Missing
carl-etal-2002-toward,W01-1406,0,\N,Missing
carl-etal-2002-toward,2001.mtsummit-ebmt.4,0,\N,Missing
carl-etal-2002-toward,W01-1401,0,\N,Missing
carl-etal-2002-toward,C92-2090,0,\N,Missing
carl-etal-2002-toward,langlais-simard-2002-merging,0,\N,Missing
carl-etal-2002-toward,1999.mtsummit-1.92,1,\N,Missing
D09-1039,J93-2003,0,0.0135897,"Missing"
D09-1039,P03-2041,0,0.0198529,"f a translation, DOT marginalizes over the scores of all derivations yielding the translation. From a parallel treebank aligned at the subsentential level, we extract all possible linked fragment pairs by first selecting all linked pairs of nodes in the treebank to be the roots of a new subtree pair, and then selecting a (possibly empty) set of linked node pairs that are descendants of the newly selected fragment roots and deleting all subtree pairs dominated by these nodes. Leaves of fragments can either be terminals, or non-terminal frontier nodes where we can compose other fragments (c.f. (Eisner, 2003)). We give example DOT fragment pairs in Figure 1. Given two subtree pairs hs1 , t1 i and hs2 , t2 i, we can compose them using the DOT composition operator ◦ if the leftmost non-terminal fronP (hds , dt i) = P |hds , dt i| r(us )=r(ds )∧r(ut )=r(dt ) |hus , ut i| (1) where |hds , dt i |is the number of times the fragment pair hds , dt i is found in the bitext, and r(d) is the root nonterminal of d. Essentially, the probability assigned to the fragment pair is the relative frequency of the fragment pair to the pair of nonterminals that root the fragments. Then, with the assumption that DOT fra"
D09-1039,P06-1121,0,0.0128267,"nt to score the derivations with weights that correlate well with the particular evaluation measure in mind. Much of the work in the MT literature has focused on the scoring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring f"
D09-1039,W96-0214,0,0.0683134,"ng Goodman reduction. frontier nodes. A fragment pair, then, is a pair of subtrees in which the root does not have an index, all internal nodes have indices, and all the leaves are either terminals or un-indexed nodes. We give an example Goodman reduction in Figure 2. While we store the source grammar and the target grammar separately, we also keep track of the correspondence between source and target Goodman indices and can easily identify the alignments according to the Goodman indices. Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996). In decoding with the Goodman reduction, we first find the n-best parses on the source side, and for each source fragment, we construct the k-best fragments on the target side. We finally compute the bilingual derivation probabilities by multiplying the source and target derivation probabilities by the target fragment relative frequencies conditioned on the source fragment. There are a few problems with a likelihoodbased scoring scheme. First, it is not clear that if a fragment is more likely to be seen in training data then it is more likely to be used in a correct translation of an unseen s"
D09-1039,2003.mtsummit-papers.22,1,0.959573,"coder in the direction of the optimal translation. Since we want 371 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 371–380, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP S S NP VP (a) tier node of s1 is equal to the root node of s2 , and the leftmost non-terminal frontier node of s1 ’s linked counterpart in t1 is equal to the root node of t2 . The resulting tree pair consists of a copy of s1 where s2 has been inserted at the leftmost frontier node, and a copy of t1 where t2 has been inserted at the node linked to s1 ’s leftmost frontier node (Hearne and Way, 2003). In Figure 1, fragment pair (a) is a fragment with two open substitution sites. If we compose this fragment pair with fragment pair (b), the source side composition must take place on the leftmost non-terminal frontier node (the leftmost NP). On the target side we compose on the frontier linked to the leftmost source side non-terminal frontier. The result is fragment pair (c). If we now compose the resulting fragment pair with fragment pair (d), we obtain a fragment pair with no open substitution sites whose source-side yield is John likes Mary and whose target-side yield is Mary plaˆıt a` Jo"
D09-1039,2006.eamt-1.8,1,0.855497,"re each αi is the weight of that term in the final score, and each fi (d) is a feature. In this work, we only consider f1 (d), an accuracy-based score, although in future work we will consider a wide variety of features in the scoring function, including combinations of the different scoring schemes described below, binary lexical features, binary source-side syntactic features, and local target side features. The score of a derivation is now given by (8): s(d) = s(hds , dt i1 ◦ . . . ◦ hds , dt iN ) X = s(hds , dt ii ) (8) i In order to disambiguate between candidate translations, we follow (Hearne and Way, 2006) by using Equation (5). 3.1 Structured Fragment Rescoring In all our approaches, we rescore fragments according to their contribution to the accuracy of a translation. We would like to give fragments that contribute to good translations relatively high scores, and give fragments that contribute to bad translations relatively low scores, so that during decoding fragments that are known to contribute to good translations would be chosen over those that are known to contribute to bad translations. Furthermore, we would like to score each fragment in a derivation independently, since bad translati"
D09-1039,J02-1005,0,0.0203117,"target derivation probabilities by the target fragment relative frequencies conditioned on the source fragment. There are a few problems with a likelihoodbased scoring scheme. First, it is not clear that if a fragment is more likely to be seen in training data then it is more likely to be used in a correct translation of an unseen sentence. In our analysis of the candidate translations of the DOT system, we observed that frequently, the highest-likelihood candidate translation output by the system was not the highest-accuracy candidate inferred. An additional problem is that, as described in (Johnson, 2002), the relative frequency estimator for DOP LHS+a → RHS1 RHS2 LHS+a → RHS1+b RHS2 LHS+a → RHS1 RHS2+c LHS+a → RHS1+b RHS2+c A category label which ends in a ‘+’ symbol followed by a Goodman index is fragment-internal and all other nodes are either fragment roots or 373 to the Goodman reduction for DOT. If we were to properly rescore each fragment, a new rule would need to be added to the grammar for each rule appearing in the fragment. Since the number of fragments is exponential, this would lead to a substantial increase in grammar size. Instead, we rescore the individual rules in the fragment"
D09-1039,P07-2045,0,0.00864901,"Missing"
D09-1039,W04-3250,0,0.14029,"Missing"
D09-1039,N09-2006,0,0.0156956,"ring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations. Similarly to MERT, Tillmann and Zhang estimate the parameters of a weight vector on a linear c"
D09-1039,2005.mtsummit-papers.11,0,0.00445852,"n (14) we are selecting the maximal score for hds , dt i from its comparison to all the possible corresponding oracle fragments. In this way, we are choosing to score hds , dt i against the oracle fragment it is closest to. 4 B LEU 8.78 2-8 SFR 10.30 NSFR 8.31 FSR 10.19 SFR 3.792 NSFR 3.431 FSR 3.784 SFR 40.92 NSFR 37.53 FSR 40.83 Baseline B LEU N IST Experiments F-S CORE For our pilot experiments, we tested all the rescoring methods in the previous section on Spanish-toEnglish translation against the relative-frequency baseline. We randomly selected 10,000 sentences from the Europarl corpus (Koehn, 2005), and parsed and aligned the bitext as described in (Tinsley et al., 2009). From the parallel treebank, we extracted a Goodman reduction DOT grammar, as described in (Hearne, 2005), although on an order of magnitude greater amount of training data. Unlike (Bod, 2007), we did not use the unsupervised version of DOT, and did not attempt to scale up our amount of training data to his levels, although in ongoing work we are optimizing our system to be able to handle that amount of training data. To perform the rescoring, we randomly chose an additional 30K sentence pairs from the Spanish-toEnglish"
D09-1039,lavie-etal-2004-significance,0,0.0248569,", dt i) = α0 l(hds , dt i) + k X αi fi (hds , dt i) This has the further advantage that we are allowing fragments that were unseen during tuning to be rescored according to previously seen fragment substructures. To implement this scheme, we select a set of oracle translations for each sentence in the tuning data by evaluating all the candidate translations against the gold standard translation using the Fscore (Turian et al., 2003), and selecting those with the highest F1 -measure, with exponent 1. We use GTM, rather than BLEU, because BLEU is not known to work well on a per-sentence level (Lavie et al., 2004) as needed for oracle selection. We then compare all the target-side fragments inferred in the translation process for each candidate translation against the fragments that yielded the oracles. There are two relevant parts of the fragments – the internal yields (i.e. the terminal leaves of the fragment) and the substitution sites (i.e. the frontiers where other fragments attach). We score the fragments rooted at the substitution sites separately from the parent fragment. We can uniquely identify the set of fragments that can be rooted at substitution sites by determining the span of the linked"
D09-1039,E09-1061,0,0.0362587,"Missing"
D09-1039,P03-1021,0,0.0521235,"he work in the MT literature has focused on the scoring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations. Similarly to MERT, Tillmann and Z"
D09-1039,P02-1040,0,0.0811029,"o a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations. Similarly to MERT, Tillmann and Zhang estimate the parameters of a weight vector on a linear combination of (binary) features using a global objective function correlated with BLEU (Papineni et al., 2002). In this work, we prototype some methods for moving directly towards incorporating a measure of the translation quality of each fragment used, bringing DOT more into the mainstream of current SMT research. In Section 2 we describe probability-based DOT fragment scoring. In Section 3 we describe our rescoring setup and the In this work we present a novel technique to rescore fragments in the Data-Oriented Translation model based on their contribution to translation accuracy. We describe three new rescoring methods, and present the initial results of a pilot experiment on a small subset of the"
D09-1039,C00-2092,0,0.720637,"s work we present a novel technique to rescore fragments in the Data-Oriented Translation model based on their contribution to translation accuracy. We describe three new rescoring methods, and present the initial results of a pilot experiment on a small subset of the Europarl corpus. This work is a proof-of-concept, and is the first step in directly optimizing translation decisions solely on the hypothesized accuracy of potential translations resulting from those decisions. 1 I. Dan Melamed AT&T Shannon Laboratory {lastname} @research.att.com Introduction The Data-Oriented Translation (DOT) (Poutsma, 2000) model is a tree-structured translation model, in which linked subtree fragments extracted from a parsed bitext are composed to cover a sourcelanguage sentence to be translated. Each linked fragment pair consists of a source-language side and a target-language side, similar to (Wu, 1997). Translating a new sentence involves composing the linked fragments into derivations so that a new source-language sentence is covered by the source tree fragments of the linked pairs, where the yields of the target-side derivations are the candidate translations. Derivations are scored according to their like"
D09-1039,P06-1091,0,0.079981,"mada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations. Similarly to MERT, Tillmann and Zhang estimate the parameters of a weight vector on a linear combination of (binary) features using a global objective function correlated with BLEU (Papineni et al., 2002). In this work, we prototype some methods for moving directly towards incorporating a measure of the translation quality of each fragment used, bringing DOT more into the mainstream of current SMT research. In Section 2 we describe probability-based DOT fragm"
D09-1039,2003.mtsummit-papers.51,1,0.810352,"hood features into the derivation scores. For all tree fragment pairs hds , dt i, let l(hds , dt i) = log(p(hds , dt i)) (6) The general form of a rescored tree fragment will be s(hds , dt i) = α0 l(hds , dt i) + k X αi fi (hds , dt i) This has the further advantage that we are allowing fragments that were unseen during tuning to be rescored according to previously seen fragment substructures. To implement this scheme, we select a set of oracle translations for each sentence in the tuning data by evaluating all the candidate translations against the gold standard translation using the Fscore (Turian et al., 2003), and selecting those with the highest F1 -measure, with exponent 1. We use GTM, rather than BLEU, because BLEU is not known to work well on a per-sentence level (Lavie et al., 2004) as needed for oracle selection. We then compare all the target-side fragments inferred in the translation process for each candidate translation against the fragments that yielded the oracles. There are two relevant parts of the fragments – the internal yields (i.e. the terminal leaves of the fragment) and the substitution sites (i.e. the frontiers where other fragments attach). We score the fragments rooted at th"
D09-1039,J97-3002,0,0.0123382,"rk is a proof-of-concept, and is the first step in directly optimizing translation decisions solely on the hypothesized accuracy of potential translations resulting from those decisions. 1 I. Dan Melamed AT&T Shannon Laboratory {lastname} @research.att.com Introduction The Data-Oriented Translation (DOT) (Poutsma, 2000) model is a tree-structured translation model, in which linked subtree fragments extracted from a parsed bitext are composed to cover a sourcelanguage sentence to be translated. Each linked fragment pair consists of a source-language side and a target-language side, similar to (Wu, 1997). Translating a new sentence involves composing the linked fragments into derivations so that a new source-language sentence is covered by the source tree fragments of the linked pairs, where the yields of the target-side derivations are the candidate translations. Derivations are scored according to their likelihood, and the translation is selected from the derivation pair with the highest score. However, we have no reason to believe that maximizing likelihood is the best way to maximize translation accuracy – likelihood and accuracy do not necessarily correlate well. We can frame the problem"
D09-1039,P01-1067,0,0.093028,"Missing"
D09-1039,P02-1039,0,0.016822,"o find the translation with the highest evaluation score, we would want to score the derivations with weights that correlate well with the particular evaluation measure in mind. Much of the work in the MT literature has focused on the scoring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (20"
D09-1039,2007.mtsummit-papers.8,0,\N,Missing
D09-1123,J99-2004,0,0.090166,"alizer’) is needed in order to compose a state Si with the previous states S0 . . . Si−1 in order to obtain a fully connected intermediate dependency structure at every position in the input. To implement the incremental parsing scheme described above we use the parser described in (Hassan et al., 2008b; Hassan et al., 2009), which is based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). We only briefly describe this parser as its full description is beyond the scope of this paper. The notions of a supertag as a lexical category and the process of supertagging are both crucial here (Bangalore and Joshi, 1999). Fortunately, CCG specifies the desired kind of lexical categories (supertags) sti for every word and a small set of combinatory operators oi that combine the supertag sti with a previous parse state Si−1 into the next parse state Si . In terms of CCG representations, the parse state is a CCG composite category which specifies either a functor and the arguments it expects to the right of the current word, or is itself an argument for a functor that will follow it to the right. At the first word in the sentence, the parse state consists solely of the supertag of that word. S0 Attacks rocked Ri"
D09-1123,P05-1033,0,0.0816104,"ther detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing parser to parse the target side of the parallel corpus in order to extract a syntactically motivated, bilingual synchronous grammar as in (Chiang, 2005). The above-mentioned approaches for incorporating syntax into Phrase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Grammar, which makes for"
D09-1123,R09-1025,1,0.886428,"Missing"
D09-1123,koen-2004-pharaoh,0,0.571843,"ities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntacticallyoriented feat"
D09-1123,W04-3250,0,0.478185,"ities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntacticallyoriented feat"
D09-1123,N03-1017,0,0.01714,"i () that capture the translation and language model effects, and (iii) the weights of the features λi that are estimated under MaxEnt (Berger et al., 1996), as in (1): P (T |S) = X P0 (T, J|S) exp λi φi (T, J, S) (1) Z i Here J is the skip reordering factor for the phrase pair captured by φi () and represents the jump from the previous source word, and Z is the per source sentence normalization term. The prior probability P0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional Phrase–based SMT systems, e.g., (Koehn et al., 2003). DTM2 differs from other Phrase–based SMT models in that it extracts from a word-aligned parallel corpus only a non-redundant set of minimal phrases in the sense that no two phrases overlap with each other. Baseline DTM2 Features: The baseline employs the following five types of features (beside the language model): • Lexical Micro Features examining source and target words of the phrases, The DTM2 approach based on MaxEnt provides a flexible framework for incorporating other available feature types as we demonstrate below. DTM2 Decoder: The decoder for the baseline is a beam search decoder s"
D09-1123,W06-1606,0,0.0241641,"to the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results. Section 7 provides translation output to shed further detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing"
D09-1123,P02-1040,0,0.0767533,"shed to perform at a parsing level close to state1182 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182–1191, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP of-the-art cubic-time parsers. Nevertheless, the parsing information it provides allows for significant improvement in translation quality. We test the new model, called the Dependencybased Direct Translation Model (DDTM), on standard Arabic–English translation tasks used in the community, including LDC and GALE data. We show that our DDTM system provides significant improvements in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores over the already extremely competitive DTM2 system. We also provide results of manual, qualitative analysis of the system output to provide insight into the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results."
D09-1123,P08-1066,0,0.0435443,"rate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized grammars, they never seek to build a full dependency tree or employ syntactic features in order to directly influence the reordering probabilities in the decoder. In the current work, we expand our previous work in (Hassan et al., 2007; Hassan et al., 2008a) to introduce the capabilities of building a full dependency structure and employing syntactic features to influence the decoding process. Recently, (Shen et al., 2008) introduced an approach for incorporating a dependency-based language model into SMT. They proposed to extract String-to-Dependency trees from the parallel corpus. As the dependency trees are not constituents by nature, they handle non-constituent phrases as well. While this work is in the same general direction as our work, namely aiming at incorporating dependency parsing into SMT, there remain three major differences. Firstly, (Shen et al., 2008) resorted to heuristics to extract the Stringto-Dependency trees, whereas our approach employs the well formalized CCG grammatical theory. Secondly"
D09-1123,2006.amta-papers.25,0,0.0267791,"el close to state1182 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182–1191, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP of-the-art cubic-time parsers. Nevertheless, the parsing information it provides allows for significant improvement in translation quality. We test the new model, called the Dependencybased Direct Translation Model (DDTM), on standard Arabic–English translation tasks used in the community, including LDC and GALE data. We show that our DDTM system provides significant improvements in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores over the already extremely competitive DTM2 system. We also provide results of manual, qualitative analysis of the system output to provide insight into the quantitative results. This paper is organized as follows. Section 2 reviews the related work. Section 3 discusses the DTM2 baseline model. Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2. Section 5 details our own DDTM, the dependencybased extension of the DTM2 model. Section 6 reports on extensive experiments and their results. Section 7 provides translation"
D09-1123,P07-1019,0,0.0181216,"are common within Phrase-based SMT. These approaches usually resort to ad hoc solutions to enrich the non-constituent phrases with syntactic structures. Secondly, they deploy chart-based decoders with a high computational cost compared with the phrase-based beam search decoders, e.g., (Tillmann and Ney, 2003; Koehn, 2004a). Thirdly, due to the large parse space, some of the proposed approaches are forced to employ small language models compared to what is usually used in phrase-based systems. To circumvent these computational limitations, various pruning techniques are usually needed, e.g., (Huang and Chiang, 2007). Other recent approaches, e.g., (Birch et al., 2007; Hassan et al., 2007; Hassan et al., 2008a) incorporate a linear-time supertagger into SMT to take the role of a syntactic language model alongside the standard language model. While these approaches share with our work the use of lexicalized grammars, they never seek to build a full dependency tree or employ syntactic features in order to directly influence the reordering probabilities in the decoder. In the current work, we expand our previous work in (Hassan et al., 2007; Hassan et al., 2008a) to introduce the capabilities of building a f"
D09-1123,J03-1005,0,0.493361,"e time and space complexities that are far beyond linear. Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms. This paper shows that this need not necessarily be the case. In this paper we extend the Direct Translation Model (DTM2) (Ittycheriah and Roukos, 2007) with target language syntax while maintaining linear-time decoding. With this extension we make three novel contributions to SMT. Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (Tillmann and Ney, 2003; Koehn, 2004a). At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder. The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure. This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG) (Steedman, 2000). Our second contribution lies in extending the DMT2 model with a novel set of syntactically"
D09-1123,W06-3119,0,0.0371993,"provides translation output to shed further detailed insight into the characteristics of the systems. Finally, Section 8 concludes, and discusses future work. 2 Related Work In (Marcu et al., 2006), it is demonstrated that ‘syntactified’ target language phrases can improve translation quality for Chinese–English. A stochastic, top-down transduction process is employed that assigns a joint probability to a source sentence and each of its alternative syntactified translations; this is done by specifying a rewriting process of the target parse-tree into a source sentence. Likewise, the model in (Zollmann and Venugopal, 2006) extends (Chiang, 2005) by augmenting the hierarchical phrases with syntactic categories derived from parsing the target side of a parallel corpus. They use an existing parser to parse the target side of the parallel corpus in order to extract a syntactically motivated, bilingual synchronous grammar as in (Chiang, 2005). The above-mentioned approaches for incorporating syntax into Phrase-based SMT (Marcu et al., 2006; Zollmann and Venugopal, 2006) share common drawbacks. Firstly, they are based on syntactic phrase-structure parse trees incorporated into a Synchronous CFG or TreeSubstitution Gr"
D09-1123,N07-1008,0,\N,Missing
D09-1123,J96-1002,0,\N,Missing
D09-1123,J90-2002,0,\N,Missing
D09-1123,W07-0702,0,\N,Missing
D10-1041,P05-1074,0,0.503744,"Missing"
D10-1041,N06-1003,0,0.699477,"r paraphrase lattice method and discusses how to set the weights for the edges in the lattice network. In Section 4, we report comparative experiments conducted on small, medium and largescale English-to-Chinese data sets. In Section 5, we analyse the influence of our paraphrase lattice method. Section 6 concludes and gives avenues for future work. 2 What Makes Translation Difficult? 2.1 Translation Difficulty We use the term “translation difficulty” to explain how difficult it is to translate the source-side sentence in three respects: • The OOV rates of the source sentences in the test set (Callison-Burch et al., 2006). • Translatability of a known phrase in the input 421 sentence. Some particular grammatical structures on the source side cannot be directly translated into the corresponding structures on the target side. Nakov (2008) presents an example showing how hard it is to translate an English construction into Spanish. Assume that an English-to-Spanish SMT system has an entry in its phrase table for “inequality of income”, but not for “income inequality”. He argues that the latter phrase is hard to translate into Spanish where noun compounds are rare: the correct translation in this case requires a s"
D10-1041,P05-1033,0,0.0297319,"Missing"
D10-1041,N03-1017,0,0.00616318,"Missing"
D10-1041,P07-2045,0,0.0137704,"Missing"
D10-1041,W08-0320,0,0.0251585,"ved in coverage and translation quality, especially in the case of unknown words which previously had been left untranslated. However, on a large-scale data set, they did not achieve improvements in terms of automatic evaluation. Introduction In recent years, statistical MT systems have been easy to develop due to the rapid explosion in data availability, especially parallel data. However, in reality there are still many language pairs which lack parallel data, such as Urdu–English, Chinese– Italian, where large amounts of speakers exist for both languages; of course, the problem is far worse Nakov (2008) proposed another way to use paraphrases in SMT. He generates nearly-equivalent syntactic paraphrases of the source-side training sentences, then pairs each paraphrased sentence with the target translation associated with the original sentence in the training data. Essentially, this method generates new training data using paraphrases to train a new model and obtain more useful 420 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 420–429, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics phrase pairs. Howev"
D10-1041,P03-1021,0,0.00595311,"s 4.1 System and Data Preparation For our experiments, we use Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. We also realise a paraphrase substitution-based system (Para-Sub)3 based on the method in (Callison-Burch, 2006) to compare with the baseline system and our proposed paraphrase lattice-based (Lattice) system. The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. The maximum phrase length is 10 words. Parameter tuning is performed using Minimum Error Rate Training (Och, 2003). The experiments are conducted on English-toChinese translation. In order to fully compare our proposed method with the baseline and the “ParaSub” system, we perform the experiments on three different sizes of training data: 20K, 200K and 2.1 million pairs of sentences. The former two sizes of data are derived from FBIS,4 and the latter size of data consists of part of HK parallel corpus,5 ISI parallel data,6 other news data and parallel dictionaries from LDC. All the language models are 5-gram which are trained on the monolingual part of parallel data. The development set (devset) and the te"
D10-1041,J03-1002,0,0.00373564,"ve them a prioritised weight, they would be severely penalised in the decoding process. So we do not need to distinguish unknown words when building and weighting the paraphrase lattice. 425 4 Experiments 4.1 System and Data Preparation For our experiments, we use Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. We also realise a paraphrase substitution-based system (Para-Sub)3 based on the method in (Callison-Burch, 2006) to compare with the baseline system and our proposed paraphrase lattice-based (Lattice) system. The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. The maximum phrase length is 10 words. Parameter tuning is performed using Minimum Error Rate Training (Och, 2003). The experiments are conducted on English-toChinese translation. In order to fully compare our proposed method with the baseline and the “ParaSub” system, we perform the experiments on three different sizes of training data: 20K, 200K and 2.1 million pairs of sentences. The former two sizes of data are derived from FBIS,4 and the latter size of data consists of part of HK parallel corpus,5 ISI paralle"
D10-1041,P02-1040,0,0.106419,"200K), the coverage of long phrases is still quite low. With respect to these three aspects of the translation difficulty, especially for data-limited language pairs, we propose a more effective method to make use of the paraphrases to facilitate translation process. 3 Paraphrase Lattice for Input Sentences In this Section, we propose a novel method to employ paraphrases to reduce the translation difficulty and in so doing increase the translation quality. • The method of paraphrase substitution does not show any significant improvement, especially on a large-scale data set in terms of BLEU (Papineni et al., 2002) scores (Callison-Burch et al., 2006); • Building a paraphrase lattice might provide more translation options to the decoder so that it can flexibly search for the best path. The major contributions of our method are: • We consider all N -gram phrases rather than only unknown phrases in the test set, where {1 &lt;= N &lt;= 10}; • We utilise lattices rather than simple substitution to facilitate the translation process; • We propose an empirical weight estimation method to set weights for edges in the word lattice, which is detailed in Section 3.4. 3.1 Motivation 3.2 Paraphrase Acquisition Our idea t"
D10-1041,E09-1082,0,0.0161055,"he original sentence in the training data. Essentially, this method generates new training data using paraphrases to train a new model and obtain more useful 420 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 420–429, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics phrase pairs. However, he reported that this method results in bad system performance. By contrast, real improvements can be achieved by merging the phrase tables of the paraphrase model and the original model, giving priority to the latter. Schroeder et al. (2009) presented the use of word lattices for multi-source translation, in which the multiple source input texts are compiled into a compact lattice, over which a single decoding pass is then performed. This lattice-based method achieved positive results across all data conditions. In this paper, we propose a novel method using paraphrases to facilitate translation, especially for resource-limited languages. Our method does not distinguish unknown words in the input sentence, but uses paraphrases of all possible words and phrases in the source input sentence to build a source-side lattice to provide"
D10-1041,2006.amta-papers.25,0,0.118978,"Missing"
D10-1041,W09-0441,0,0.0244824,"Missing"
D10-1041,2004.tmi-1.9,0,0.218658,"Missing"
D10-1041,W06-3119,0,0.024657,"Missing"
D15-1004,W08-0336,0,0.372538,"Missing"
D15-1004,W09-2307,0,0.173319,"n is produced. 5 Experiment We conduct experiments on Chinese–English and German–English translation tasks. 5.1 Settings Datasets 5.3 The Chinese–English training corpus is from LDC, including LDC2002E18, LDC2003E07, LDC2003E14, LDC2004T07, the Hansards portion of LDC2004T08 and LDC2005T06. NIST 2002 is taken as a development set to tune weights, and NIST 2004 (MT04) and NIST 2005 (MT05) are two test sets to evaluate systems. Table 1 provides a summary of this corpus. The Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a dependency graph in our model. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as a development set, while News-test 2012 (WMT12) and News-test 2013 (WMT13) are our test sets. Table 1 provides a summary of this corpus. We Results Table 2 shows the scores of all three metrics on all systems. Similar to Li et al. (2014), in our experiments Dep2Str has on average a comparable result with Moses HPB in terms of BLEU and METEOR scores. Howe"
D15-1004,C12-1083,0,0.234876,"Missing"
D15-1004,P96-1041,0,0.110194,"periments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. #words(EN) 55M+ 74,753 72,988 64,810 Table 1: Chinese–English (ZH–EN) and German– English (DE–EN) corpora. For the English side of dev and test sets, words counts are averaged across all references. for each fragment, the decoder finds rules to translate it. The translation of a large span can be obtained"
D15-1004,P13-1091,0,0.0258494,"ize and produce graphs. Following HRG, the graph we use in this 33 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 33–43, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Y X 1 Shijiebei Zai Y Nanfei 1 1 FIFA 1 Juxing Juxing Juxing S X Y Chenggong 2010 Shijiebei Chenggong Zai 2010 Zai 2010 Nanfei Chenggong FIFA Nanfei Figure 1: An example of a derivation in an ERG. Dark circles are external nodes. • S ∈ N is the start symbol. paper is connected, nodes ordered, acyclic and has edge labels but no node labels (Chiang et al., 2013). We provide some formal definitions on ERG. • V is a finite set of nodes. Figure 1 shows an example of a derivation in an ERG to produce a graph. Starting from the start symbol S, when a rule (A → R) is applied to an edge e, the edge is replaced by the graph fragment R. Just like in HRG, the ordering of nodes Ve in e and external nodes XR in R implies the mapping from Ve to XR (Chiang et al., 2013). • E ⊆ V 2 is a finite set of edges. 3 • φ : E → C assigns a label (drawn from C) to each edge. In SMT, we need a synchronous grammar to simultaneously parse an input graph and produce translations"
D15-1004,P07-2045,0,0.00801157,"n of an input graph, the decoder checks if it is a dependency-graph fragment. Then d∈D 37 (2) corpus train dev MT04 MT05 corpus train dev WMT12 WMT13 #sent. 1.5M+ 878 1,597 1,082 #sent. 2M+ 3,003 3,003 3,000 ZH–EN #words(ZH) 38M+ 22,655 43,719 29,880 DE–EN #words(DE) 52M+ 72,661 72,603 63,412 use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). #words(EN) ∼45M 26,905 52,705 35,326 5.2 In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portio"
D15-1004,P05-1033,0,0.313085,"Centre, School of Computing Dublin City University {liangyouli,away,qliu}@computing.dcu.ie Abstract As most available syntactic resources and tools are tree-based, in this paper we propose to convert dependency trees, which are usually taken as a kind of shallow semantic representation, to dependency graphs by labelling edges. We then use a synchronous version of edge replacement grammar (ERG) (Section 2), a special case of HRG, to translate these graphs. The resulting translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We defin"
D15-1004,P12-2007,0,0.0404576,"Missing"
D15-1004,W14-4014,1,0.750254,"ds(ZH) 38M+ 22,655 43,719 29,880 DE–EN #words(DE) 52M+ 72,661 72,603 63,412 use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). #words(EN) ∼45M 26,905 52,705 35,326 5.2 In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in"
D15-1004,W11-2107,0,0.0788531,"014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-scale experiments on Chinese–English and German–English tasks show th"
D15-1004,2005.mtsummit-ebmt.13,0,0.738381,"t allowing hyperedges or only using at most two external nodes reduces the phrase coverage in our model as well. Related Work Dependency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augme"
D15-1004,W02-1039,0,0.0791488,"re more suitable for longdistance reordering and translating long sentences. Although experiments show significant improvements over baselines, our model has limitations that can be avenues for future work. The restriction used in this paper reduces the time complexity but at the same time reduces the generative capacity of graph grammars. Without allowing hyperedges or only using at most two external nodes reduces the phrase coverage in our model as well. Related Work Dependency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to deci"
D15-1004,D13-1108,1,0.890369,"Missing"
D15-1004,P10-1064,1,0.888166,"Missing"
D15-1004,P05-1013,0,0.411158,"Missing"
D15-1004,W07-0706,1,0.902806,"dency structures have been used in SMT for a few years. Because of its better inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be beneficial to translation. Researchers have tried to use dependency structures on both target and source sides. Shen et al. (2010) propose a string-to-dependency model by using dependency fragments of neighbouring words on the target side, which makes the model easier to include a dependency-based language model. Menezes and Quirk (2005) and Quirk et al. (2005) propose the treelet approach which uses dependency structures on the source side. Xiong et al. (2007) extend this approach by allowing gaps in rules. However, their methods need a separate reordering model to decide the position of translated words (insertion problem). To avoid this problem, Xie et al. (2011) propose to use full head-dependent structures of a dependency tree and build a new dependency-to-string model. However, this model has difficulties in handling non-syntactic phrasal rules and ignores treelets. Meng et al. (2013) and Xie et al. (2014) further augment this model by incorporating constituent phrases and integrating fix/float structures (Shen et al., 2010), respectively, to"
D15-1004,P02-1038,0,0.101118,"tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, i"
D15-1004,J03-1002,0,0.00721639,"and Nilsson, 2005). #words(EN) ∼45M 26,905 52,705 35,326 5.2 In this paper, we mainly compare our system (DGST) with HPB in Moses (Koehn et al., 2007). We implement our model in Moses and take the same settings as Moses HPB in all experiments. In addition, translation results from a recently open-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. #words(EN) 55M+ 74,753 72,988 64,810 Table 1: Chinese–English (ZH–EN)"
D15-1004,J04-4002,0,0.356022,"initial pair of P , iff: i where φi are features defined on derivations and λi are feature weights. In our experiments, we use 9 features: • translation probabilities P (s|t) and P (t|s), where s is the source graph fragment and t is the target string. • lexical translation probabilities Plex (s|t) and Plex (t|s). 1. Gji is a dependency-graph fragment. That means it is a connected sub-graph and has at most two external nodes, nodes which connect with nodes outside or are the root. • language model lm(e) over translation e. • rule penalty exp(−1). 2. It is consistent with the word alignment ∼ (Och and Ney, 2004). • word penalty exp(|e|). • glue penalty exp(−1). The set of rules from P satisfies the following: 0 1. If hGji , eji0 i is an initial pair, then • unknown words penalty exp(u(g)), where u(g) is the number of unknown words in a source graph g. 0 hN (Gji ) → Gji , X → eji0 i is a rule, where N (G) defines the nonterminal symbol for G. Our decoder is based on the conventional chart parsing CYK algorithm (Kasami, 1965; Younger, 1967; Cocke and Schwartz, 1970). It searches for the best derivation d∗ among all possible derivations D, as in Equation (2): 2. If hN (R) → R, X → R0 i is a rule of P an"
D15-1004,P03-1021,0,0.0525651,"pen-source dependency tree-to-string system, Dep2Str4 (Li et al., 2014), which is implemented in Moses and improves the dependencybased model in Xie et al. (2011), are also reported. All systems use the same sets of features defined in Section 4. In all experiments, word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (MERT) (Och, 2003) is used to tune weights. To obtain more reliable results, in each experiment, we run MERT three times and report average scores. These scores are calculated by three widely used automatic metrics in case-insensitive mode: BLEU, METEOR and TER. #words(EN) 55M+ 74,753 72,988 64,810 Table 1: Chinese–English (ZH–EN) and German– English (DE–EN) corpora. For the English side of dev and test sets, words counts are averaged across all references. for each fragment, the decoder finds rules to translate it. The translation of a large span can be obtained by combining translations from its sub-span usin"
D15-1004,P02-1040,0,0.0927075,"handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-sca"
D15-1004,P05-1034,0,0.729257,"translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more"
D15-1004,2006.amta-papers.25,0,0.0415351,"(Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reordering and translating long sentences. Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-scale experiments on Chinese–E"
D15-1004,D11-1020,1,0.962756,"der of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). We also find that the rules in our model are more suitable for long-distance reor"
D15-1004,C14-1209,1,0.905946,"ion 2), a special case of HRG, to translate these graphs. The resulting translation model has the same order of magnitude in terms of time complexity with the hierarchical phrasebased model (HPB) (Chiang, 2005) under a certain restriction (Section 3). Compared to dependency tree-to-string models, using ERG for graph-to-string translation brings some benefits (Section 3). Thanks to the stronger generative capacity of the grammar, our model can naturally translate siblings in a tree structure, which are usually treated as non-syntactic phrases and handled by other techniques (Huck et al., 2014; Xie et al., 2014). Furthermore, compared to the known treelet approach (Quirk et al., 2005) and Dep2Str (Xie et al., 2011), our method not only uses treelets but also has a full capacity of reordering. We define our translation model (Section 4) in the log-linear framework (Och and Ney, 2002). Large-scale experiments (Section 5) on Chinese– English and German–English, two language pairs that have a high degree of syntactic reordering, show that our method significantly improves translation quality over both HPB and Dep2Str, as measured by BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denk"
D15-1004,J10-4005,0,\N,Missing
D15-1004,W14-3362,0,\N,Missing
D17-1301,D17-1105,1,0.644977,"Missing"
D17-1301,W12-3156,0,0.0426269,"ation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context. The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received relatively little attention fr"
D17-1301,P05-1066,0,0.303284,"Missing"
D17-1301,P15-1166,0,0.0160558,"de and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments. Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results. 5 Conclusion and Future Work We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduced context gate plays an important role."
D17-1301,D13-1176,0,0.00976873,"rical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points. 1 Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol en"
D17-1301,P07-2045,0,0.00724727,"Missing"
D17-1301,D15-1166,0,0.0309999,"s been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT model"
D17-1301,P02-1040,0,0.101177,"Missing"
D17-1301,E17-3017,0,0.0145815,"Missing"
D17-1301,P16-1008,1,0.843029,"NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points. 1 Introduction Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding m"
D17-1301,2011.mtsummit-papers.13,0,0.0990302,"ical machine translation (SMT) on various language pairs (Luong et al., 2015). The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of ∗ Corresponding Author: Zhaopeng Tu a word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context. The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received rel"
D17-1301,N16-1004,0,0.0544309,"or example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments. Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results. 5 Conclusion and Future Work We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduc"
D18-1245,S10-1006,0,0.0874417,"Missing"
D18-1245,P11-1055,0,0.853326,"the-art baselines in terms of PR curves, P@N and F1 measures. 1 Introduction Relation extraction is a fundamental task in information extraction (IE), which studies the issue of predicting semantic relations between pairs of entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhou et al., 2005). One crucial problem in RE is the relative lack of large-scale, high-quality labeled data. In recent years, one commonly used and effective technique for dealing with this challenge is the distant supervision method via knowledge bases (KBs) (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), which assumes that if one entity pair appearing in some sentences can be observed in a KB with a certain relationship, then these sentences will be labeled as the context of this entity pair and this relationship. The distant supervision strategy is an effective and efficient method for automatically labeling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two entities does not necessarily express their relation in a KB (Surdeanu et al., 2012; Zeng et al., 2015). Plenty of research work has been proposed to de"
D18-1245,P17-1004,0,0.477895,"Missing"
D18-1245,P16-1200,0,0.828618,"ionship. The distant supervision strategy is an effective and efficient method for automatically labeling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two entities does not necessarily express their relation in a KB (Surdeanu et al., 2012; Zeng et al., 2015). Plenty of research work has been proposed to deal with distantly supervised data and has achieved significant progress, especially with the rapid development of deep neural networks (DNN) for relation extraction in recent years (Zeng et al., 2014, 2015; Lin et al., 2016, 2017a; Wang et al., 2016; Zhou et al., 2016; Ji et al., 2017; Yang et al., 2017; Zeng et al., 2017). DNN models under an MIL framework for DSRE have become state-of-the-art, replacing statistical methods, such as feature-based and graphical models (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). In the MIL framework for distantly supervised RE, each entity pair often has multiple instances where some are noisy and some are valid. The attention mechanism in DNNs, such as convolutional (CNN) and recurrent neural networks (RNN), is an effective way to select valid instances"
D18-1245,P09-1113,0,0.326952,"hanism significantly outperform state-of-the-art baselines in terms of PR curves, P@N and F1 measures. 1 Introduction Relation extraction is a fundamental task in information extraction (IE), which studies the issue of predicting semantic relations between pairs of entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhou et al., 2005). One crucial problem in RE is the relative lack of large-scale, high-quality labeled data. In recent years, one commonly used and effective technique for dealing with this challenge is the distant supervision method via knowledge bases (KBs) (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), which assumes that if one entity pair appearing in some sentences can be observed in a KB with a certain relationship, then these sentences will be labeled as the context of this entity pair and this relationship. The distant supervision strategy is an effective and efficient method for automatically labeling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two entities does not necessarily express their relation in a KB (Surdeanu et al., 2012; Zeng et al., 2015). Pl"
D18-1245,D12-1042,0,0.756413,"via knowledge bases (KBs) (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), which assumes that if one entity pair appearing in some sentences can be observed in a KB with a certain relationship, then these sentences will be labeled as the context of this entity pair and this relationship. The distant supervision strategy is an effective and efficient method for automatically labeling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two entities does not necessarily express their relation in a KB (Surdeanu et al., 2012; Zeng et al., 2015). Plenty of research work has been proposed to deal with distantly supervised data and has achieved significant progress, especially with the rapid development of deep neural networks (DNN) for relation extraction in recent years (Zeng et al., 2014, 2015; Lin et al., 2016, 2017a; Wang et al., 2016; Zhou et al., 2016; Ji et al., 2017; Yang et al., 2017; Zeng et al., 2017). DNN models under an MIL framework for DSRE have become state-of-the-art, replacing statistical methods, such as feature-based and graphical models (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et a"
D18-1245,P16-1123,0,0.0713563,"Missing"
D18-1245,D15-1203,0,0.912373,"s) (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), which assumes that if one entity pair appearing in some sentences can be observed in a KB with a certain relationship, then these sentences will be labeled as the context of this entity pair and this relationship. The distant supervision strategy is an effective and efficient method for automatically labeling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two entities does not necessarily express their relation in a KB (Surdeanu et al., 2012; Zeng et al., 2015). Plenty of research work has been proposed to deal with distantly supervised data and has achieved significant progress, especially with the rapid development of deep neural networks (DNN) for relation extraction in recent years (Zeng et al., 2014, 2015; Lin et al., 2016, 2017a; Wang et al., 2016; Zhou et al., 2016; Ji et al., 2017; Yang et al., 2017; Zeng et al., 2017). DNN models under an MIL framework for DSRE have become state-of-the-art, replacing statistical methods, such as feature-based and graphical models (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). In the MI"
D18-1245,C14-1220,0,0.896428,"ntity pair and this relationship. The distant supervision strategy is an effective and efficient method for automatically labeling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two entities does not necessarily express their relation in a KB (Surdeanu et al., 2012; Zeng et al., 2015). Plenty of research work has been proposed to deal with distantly supervised data and has achieved significant progress, especially with the rapid development of deep neural networks (DNN) for relation extraction in recent years (Zeng et al., 2014, 2015; Lin et al., 2016, 2017a; Wang et al., 2016; Zhou et al., 2016; Ji et al., 2017; Yang et al., 2017; Zeng et al., 2017). DNN models under an MIL framework for DSRE have become state-of-the-art, replacing statistical methods, such as feature-based and graphical models (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). In the MIL framework for distantly supervised RE, each entity pair often has multiple instances where some are noisy and some are valid. The attention mechanism in DNNs, such as convolutional (CNN) and recurrent neural networks (RNN), is an effective way to"
D18-1245,D17-1186,0,0.535922,"beling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two entities does not necessarily express their relation in a KB (Surdeanu et al., 2012; Zeng et al., 2015). Plenty of research work has been proposed to deal with distantly supervised data and has achieved significant progress, especially with the rapid development of deep neural networks (DNN) for relation extraction in recent years (Zeng et al., 2014, 2015; Lin et al., 2016, 2017a; Wang et al., 2016; Zhou et al., 2016; Ji et al., 2017; Yang et al., 2017; Zeng et al., 2017). DNN models under an MIL framework for DSRE have become state-of-the-art, replacing statistical methods, such as feature-based and graphical models (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). In the MIL framework for distantly supervised RE, each entity pair often has multiple instances where some are noisy and some are valid. The attention mechanism in DNNs, such as convolutional (CNN) and recurrent neural networks (RNN), is an effective way to select valid instances by learning a weight distribution over multiple instances. However, there are two important represent"
D18-1245,P05-1053,0,0.106622,"learns a 2-D matrix where each row vector represents a weight distribution on selection of different valid instances. Experiments conducted on two publicly available DS-RE datasets show that the proposed framework with a multi-level structured self-attention mechanism significantly outperform state-of-the-art baselines in terms of PR curves, P@N and F1 measures. 1 Introduction Relation extraction is a fundamental task in information extraction (IE), which studies the issue of predicting semantic relations between pairs of entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhou et al., 2005). One crucial problem in RE is the relative lack of large-scale, high-quality labeled data. In recent years, one commonly used and effective technique for dealing with this challenge is the distant supervision method via knowledge bases (KBs) (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011), which assumes that if one entity pair appearing in some sentences can be observed in a KB with a certain relationship, then these sentences will be labeled as the context of this entity pair and this relationship. The distant supervision strategy is an effective and efficient method for aut"
D18-1245,P16-2034,0,0.552071,"an effective and efficient method for automatically labeling large-scale training data. However, it also introduces a severe mislabelling problem due to the fact that a sentence that mentions two entities does not necessarily express their relation in a KB (Surdeanu et al., 2012; Zeng et al., 2015). Plenty of research work has been proposed to deal with distantly supervised data and has achieved significant progress, especially with the rapid development of deep neural networks (DNN) for relation extraction in recent years (Zeng et al., 2014, 2015; Lin et al., 2016, 2017a; Wang et al., 2016; Zhou et al., 2016; Ji et al., 2017; Yang et al., 2017; Zeng et al., 2017). DNN models under an MIL framework for DSRE have become state-of-the-art, replacing statistical methods, such as feature-based and graphical models (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). In the MIL framework for distantly supervised RE, each entity pair often has multiple instances where some are noisy and some are valid. The attention mechanism in DNNs, such as convolutional (CNN) and recurrent neural networks (RNN), is an effective way to select valid instances by learning a weight distribution over multip"
D18-1333,N18-1008,0,0.0193306,"rn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotated source senˆ = {ˆ tence x x1 , x ˆ2 , . . . , x ˆT } word by word, and the reconstruction score is computed by R(ˆ x"
D18-1333,P05-1066,0,0.120726,"tperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Experiment 4.1 Setup To compare our work with the results reported by previous work (Wang et al., 2018), we conducted experiments on their released Chinese⇒English TV Subtitle corpus.2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by Wang et al. (2018).3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018), since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Results Table 2 shows the translation results. It is clear that the proposed m"
D18-1333,P15-1166,0,0.0298915,"ch higher, which provides the chance to alleviate the error propagation problem. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotat"
D18-1333,N16-1101,0,0.0216158,"r. Fortunately, the accuracy of predicting DP positions (DPPs) is much higher, which provides the chance to alleviate the error propagation problem. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction scor"
D18-1333,W10-1737,0,0.232395,"Missing"
D18-1333,P02-1040,0,0.100689,"the DPP-annotated data (“Baseline (+DPPs)”, Row 4) outperforms the other two counterparts, indicating that the error propagation problem does affect the performance of translating DPs. It suggests the necessity of jointly learning to translate and predict DPs. Experiment 4.1 Setup To compare our work with the results reported by previous work (Wang et al., 2018), we conducted experiments on their released Chinese⇒English TV Subtitle corpus.2 The training, validation, and test sets contain 2.15M, 1.09K, and 1.15K sentence pairs, respectively. We used case-insensitive 4-gram NIST BLEU metrics (Papineni et al., 2002) for evaluation, and sign-test (Collins et al., 2005) to test for statistical significance. We implemented our models on the code repository released by Wang et al. (2018).3 We used the same configurations (e.g. vocabulary size = 30K, hidden size = 1000) and reproduced their reported results. It should be emphasized that we did not use the pre-train strategy as done in Wang et al. (2018), since we found training from scratch achieved a better performance in the shared reconstructor setting. 2 https://github.com/longyuewangdcu/ tvsub 3 https://github.com/tuzhaopeng/nmt Results Table 2 shows the"
D18-1333,D17-1301,1,0.910434,"Missing"
D18-1333,N16-1113,1,0.686079,"Missing"
D18-1333,P13-1081,0,0.46132,"Missing"
D18-1333,N16-1004,0,0.0282224,"em. Intuitively, we can learn to generate DPs at the predicted positions using a jointly trained DP predictor, which is fed with informative representations in the reconstructor. 1 Unless otherwise indicated, in the paper, the terms “DP” and “DP word” are identical. Approach Shared Reconstructor Recent work shows that NMT models can benefit from sharing a component across different tasks and languages. Taking multi-language translation as an example, Firat et al. (2016) share an attention model across languages while Dong et al. (2015) share an encoder. Our work is most similar to the work of Zoph and Knight (2016) and Anastasopoulos and Chiang (2018), which share a decoder and two separate attention models to read from two different sources. In contrast, we share information at the level of reconstructed frames. The architectures of our proposed shared reconstruction model are shown in Figure 2(a). Formally, the reconstructor reads from both the encoder and decoder hidden states, as well as the DP-annotated source sentence, and outputs a reconstruction score. It uses two separate attention models to reconstruct the annotated source senˆ = {ˆ tence x x1 , x ˆ2 , . . . , x ˆT } word by word, and the reco"
D18-1334,P11-2031,0,0.0692818,"ed systems performed on the general test set compared to the baseline. In Table 2, the BLEU scores for 10 baseline and 10 gender-enhanced NMT systems are presented. Systems FR ES EL IT PT DA DE FI SV NL EN 37.82 42.47 31.38 31.46 36.11 36.69 28.28 21.82 35.42 28.35 EN-TAG 39.26* 42.28 31.54 31.75* 36.33 37.00* 28.05 21.35* 35.19 28.22 Table 2: BLEU scores for the 10 baseline (denoted with EN) and the 10 gender-enhanced NMT (denoted with ENTAG) systems. Entries labeled with * present statistically significant differences (p < 0.05). Statistical significance was computed with the MultEval tool (Clark et al., 2011). While most of the BLEU-scores (Papineni et al., 2002) in Table 2 are consistent with our hypothesis, showing (significant) improvements for the NMT systems enriched with a gender tag (EN-TAG) over the baseline systems (EN) for French, Italian, Portuguese and Greek, the Spanish enriched system surprisingly does not (–0.19 BLEU). As hypothesized, the Dutch, German, Finnish and Swedish systems do not improve. However, the Danish (EN–DA) enriched NMT system does achieve a significant +0.31 BLEU improvement. We expected to see the strongest improvements in sentences uttered by female speakers as,"
D18-1334,P17-4012,0,0.0290805,"(Sennrich et al., 2016), etc. (1) “FEMALE Madam President, as a...” For each of these language pairs we trained two NMT systems: a baseline and a tagged one. We evaluated the performance of all our systems on a randomly selected 2K general test set. Moreover, we further evaluated the EN–FR systems on 2K male-only and female-only test sets to have a look at the system performance with respect to genderrelated issues. We also looked at two additional male and female test sets in which the first person singular pronoun appeared. 4.2 Description of the NMT Systems We used the OpenNMT-py toolkit (Klein et al., 2017) to train the NMT models. The models are sequence-to-sequence encoder-decoders with LSTMs as the recurrent unit (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014) trained with the default parameters. In order to bypass the OOV problem and reduce the number of dictionary entries, we use word-segmentation with BPE (Sennrich, 2015). We ran the BPE algorithm with 89,500 operations (Sennrich, 2015). All systems are trained for 13 epochs and the best model is selected for evaluation. 5 Age groups Experimental Setup Results In this section we discuss some of the results obtained. We hy"
D18-1334,2005.mtsummit-papers.11,0,0.207863,"ystems is finding large enough annotated parallel datasets with speaker information. Rabinovich et al. (2017) published an annotated parallel dataset for EN–FR and EN–DE. However, for many other language pairs no sufficiently large annotated datasets are available. To address the aforementioned problem, we published online a collection of parallel corpora licensed under the Creative Commons Attribution 4.0 International License for 20 language pairs (Vanmassenhove and Hardmeier, 2018).2 We followed the approach described by Rabinovich et al. (2017) and tagged parallel sentences from Europarl (Koehn, 2005) with speaker information (name, gender, age, date of birth, euroID and date of the session) by retrieving speaker information provided by tags in the Europarl source files. The Europarl source files contain information about the speaker on the paragraph level and the filenames contain the data of the session. By retrieving the names of the speakers together with meta-information on the members of the European Parliament (MEPs) released by Rabinovich et al. (2017) (which includes among others name, country, date of birth and gender predictions per MEP), we were able to retrieve demographic ann"
D18-1334,D15-1130,0,0.439655,"syntax (Coates, 2015). The increasing amount of work on automatic author classification (or ‘author profiling’) reaching relatively high accuracies on domain-specific data corroborates these findings (Rangel et al., 2013; Santosh et al., 2013). However, determining the gender of an author based solely on text is not a solved issue. Likewise, the selection of the most informative features for gender classification remains a difficult task (Litvinova et al., 2016). When translating from one language into another, original author traits are partially lost, both in human and machine translations (Mirkin et al., 2015; Rabinovich et al., 2017). However, in the field of Machine Translation (MT) one of the most observable consequences of this missing information are morphologically incorrect variants due to a lack of agreement in number and gender with the subject. Such errors harm the overall fluency and adequacy of the translated sentence. Furthermore, gender-related errors are not just harming the quality of the translation as getting the gender right is also a matter of basic politeness. Current systems have a tendency to perpetuate a male bias which amounts to negative discrimination against half the po"
D18-1334,E17-1101,0,0.472965,". The increasing amount of work on automatic author classification (or ‘author profiling’) reaching relatively high accuracies on domain-specific data corroborates these findings (Rangel et al., 2013; Santosh et al., 2013). However, determining the gender of an author based solely on text is not a solved issue. Likewise, the selection of the most informative features for gender classification remains a difficult task (Litvinova et al., 2016). When translating from one language into another, original author traits are partially lost, both in human and machine translations (Mirkin et al., 2015; Rabinovich et al., 2017). However, in the field of Machine Translation (MT) one of the most observable consequences of this missing information are morphologically incorrect variants due to a lack of agreement in number and gender with the subject. Such errors harm the overall fluency and adequacy of the translated sentence. Furthermore, gender-related errors are not just harming the quality of the translation as getting the gender right is also a matter of basic politeness. Current systems have a tendency to perpetuate a male bias which amounts to negative discrimination against half the population and this has been"
D18-1334,Q15-1013,0,0.0206909,"a look at the system performance with respect to genderrelated issues. We also looked at two additional male and female test sets in which the first person singular pronoun appeared. 4.2 Description of the NMT Systems We used the OpenNMT-py toolkit (Klein et al., 2017) to train the NMT models. The models are sequence-to-sequence encoder-decoders with LSTMs as the recurrent unit (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014) trained with the default parameters. In order to bypass the OOV problem and reduce the number of dictionary entries, we use word-segmentation with BPE (Sennrich, 2015). We ran the BPE algorithm with 89,500 operations (Sennrich, 2015). All systems are trained for 13 epochs and the best model is selected for evaluation. 5 Age groups Experimental Setup Results In this section we discuss some of the results obtained. We hypothesized that the male/female tags would be particularly helpful for French, Portuguese, Italian, Spanish and Greek, where adjectives and even verb forms can be marked by the gender of the speaker. Since, according to the literature, women and men also make use of different syntactic constructions and make different 3005 word choices, we als"
D18-1334,N16-1005,0,0.296255,"MT (SMT) systems as a domain-adaptation task treating the female and male gender as two different domains. They applied two common simple domain-adaptation techniques in order to create personalized SMT: (1) using gender-specific phrase-tables and language models, and (2) using a gender-specific tuning set. Although their models did not improve over the baseline, their work provides a detailed analysis of gender traits in human and machine translation. Our work is, to the best of our knowledge, the first to attempt building a speaker-informed NMT system. Our approach is similar to the work of Sennrich et al. (2016) on controlling politeness, where some sentence of the training data are followed with an ‘informal’ or ‘polite’ tag indicating the level of politeness expressed. 3 Compilation of Datasets One of the main obstacles for more personalized MT systems is finding large enough annotated parallel datasets with speaker information. Rabinovich et al. (2017) published an annotated parallel dataset for EN–FR and EN–DE. However, for many other language pairs no sufficiently large annotated datasets are available. To address the aforementioned problem, we published online a collection of parallel corpora l"
D18-1334,P02-1040,0,0.101067,"d to the baseline. In Table 2, the BLEU scores for 10 baseline and 10 gender-enhanced NMT systems are presented. Systems FR ES EL IT PT DA DE FI SV NL EN 37.82 42.47 31.38 31.46 36.11 36.69 28.28 21.82 35.42 28.35 EN-TAG 39.26* 42.28 31.54 31.75* 36.33 37.00* 28.05 21.35* 35.19 28.22 Table 2: BLEU scores for the 10 baseline (denoted with EN) and the 10 gender-enhanced NMT (denoted with ENTAG) systems. Entries labeled with * present statistically significant differences (p < 0.05). Statistical significance was computed with the MultEval tool (Clark et al., 2011). While most of the BLEU-scores (Papineni et al., 2002) in Table 2 are consistent with our hypothesis, showing (significant) improvements for the NMT systems enriched with a gender tag (EN-TAG) over the baseline systems (EN) for French, Italian, Portuguese and Greek, the Spanish enriched system surprisingly does not (–0.19 BLEU). As hypothesized, the Dutch, German, Finnish and Swedish systems do not improve. However, the Danish (EN–DA) enriched NMT system does achieve a significant +0.31 BLEU improvement. We expected to see the strongest improvements in sentences uttered by female speakers as, according to our initial analysis, the male data was o"
D18-1334,Q17-1024,0,\N,Missing
E09-1063,J93-2003,0,0.0193936,"target) language into account. Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) requires a certain amount of bilingual corpora as training data in order to achieve competitive results. The only assumption of most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words. Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required. However, this segmentation is normally performed as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainProceedings of the 12th Conference of the European Chapter of the ACL, pages 549–557, c Athens, Greece, 30 March"
E09-1063,W08-0336,0,0.49856,"Missing"
E09-1063,H05-1022,0,0.101442,"this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) requires a certain amount of bilingual corpora as training data in order to achieve competitive results. The only assumption of most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words. Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required. However, this segmentation is normally performed as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainProceedings of the 12th Conference of the European Chapter of the ACL, pages 549–557, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Compu"
E09-1063,N06-4004,0,0.0126545,"interested in the performance of our segmentation ap160K 12.47 14.85 13.88 15.26 12.58 13.74 640K 14.40 17.20 15.86 16.94 14.11 15.33 Table 10: Scalability of BS on NIST task 555 6.5 Using different word aligners ation given that our segmentation is driven by the bilingual dictionary. The above experiments rely on G IZA ++ to perform word alignment. We next show that our approach is not dependent on the word aligner given that we have a conservative reliability estimation procedure. Table 11 shows the results obtained on the IWSLT data set using the MTTK alignment tool (Deng and Byrne, 2005; Deng and Byrne, 2006). CS ICT LDC Stanford BS-SingleBest BS-WordLattice IWSLT06 21.04 20.48 20.79 17.84 19.22 21.76 8 Conclusions and Future Work In this paper, we introduced a bilingually motivated word segmentation approach for SMT. The assumption behind this motivation is that the language to be segmented can be tokenised into basic writing units. Firstly, we extract 1-to-n word alignments using statistical word aligners to construct a bilingual dictionary in which each entry indicates a correspondence between one English word and n Chinese characters. This dictionary is then filtered using a few simple associa"
E09-1063,P08-1115,0,0.643192,"National Centre for Language Technology School of Computing Dublin City University Dublin 9, Ireland {yma, away}@computing.dcu.ie Abstract specific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in a monolingual context. Consequently, such segmenters cannot produce consistently good results when used across different domains. A substantial amount of research has been carried out to address the problems of word segmentation. However, most research focuses on combining various segmenters either in SMT training or decoding (Dyer et al., 2008; Zhang et al., 2008). One important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre. Segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models or PB-SMT decoding (Ma et al., 2007). The resulting segmentation will consequently influence the performance of an SMT system. In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT. We utilise a small bilingual corpus with the relevant"
E09-1063,N03-1017,0,0.00418847,"ically evaluate the performance of our approach via the Chinese–English translation task, i.e. we measure the influence of the segmentation process on the final translation output. The quality of the translation output is mainly evaluated using B LEU, with NIST (Doddington, 2002) and M ETEOR (Banerjee and Lavie, 2005) as complementary metrics. 5.2 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: G IZA ++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single wor"
E09-1063,P07-2045,0,0.0128045,"corpora should be segmented into sequences of tokens that are meant to be words. Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required. However, this segmentation is normally performed as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainProceedings of the 12th Conference of the European Chapter of the ACL, pages 549–557, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 549 system—Moses (Koehn et al., 2007). The performance of PB-SMT system is measured with B LEU score (Papineni et al., 2002). We firstly measure the influence of word segmentation on in-domain data with respect to the three above mentioned segmenters, namely UN data from the NIST 2006 evaluation campaign. As can be seen from Table 1, using monolingual segmenters achieves consistently better SMT performance than character-based segmentation (CS) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large. We can also observe that the ICT and Stanford s"
E09-1063,P07-1039,1,0.847676,"ults when used across different domains. A substantial amount of research has been carried out to address the problems of word segmentation. However, most research focuses on combining various segmenters either in SMT training or decoding (Dyer et al., 2008; Zhang et al., 2008). One important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre. Segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models or PB-SMT decoding (Ma et al., 2007). The resulting segmentation will consequently influence the performance of an SMT system. In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT. We utilise a small bilingual corpus with the relevant language segmented into basic writing units (e.g. characters for Chinese or kana for Japanese). Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidate “words”. We evaluate the reliability of these candidates using simple metrics based on co-occurrence frequencies, similar to those used in associativ"
E09-1063,J00-2004,0,0.168103,"l consequently influence the performance of an SMT system. In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT. We utilise a small bilingual corpus with the relevant language segmented into basic writing units (e.g. characters for Chinese or kana for Japanese). Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidate “words”. We evaluate the reliability of these candidates using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment (Melamed, 2000). We then modify the segmentation of the respective sentences in the parallel corpus according to these candidate words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the segmentation process on Chinese-to-English Machine Translation (MT) tasks in two different domains. The remainder of this paper is organised as folWe introduce a word segmentation approach to languages where word boundaries are not orthographically marked, with application to Phrase-Based Statistical Machi"
E09-1063,J03-1002,0,0.00503637,"ty in terms of B LEU score (Zhang et al., 2008). Consequently, we chose to extrinsically evaluate the performance of our approach via the Chinese–English translation task, i.e. we measure the influence of the segmentation process on the final translation output. The quality of the translation output is mainly evaluated using B LEU, with NIST (Doddington, 2002) and M ETEOR (Banerjee and Lavie, 2005) as complementary metrics. 5.2 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: G IZA ++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training"
E09-1063,P03-1021,0,0.00562528,"ia the Chinese–English translation task, i.e. we measure the influence of the segmentation process on the final translation output. The quality of the translation output is mainly evaluated using B LEU, with NIST (Doddington, 2002) and M ETEOR (Banerjee and Lavie, 2005) as complementary metrics. 5.2 5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: G IZA ++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices. 6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters. From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single word, using the method presented in section"
E09-1063,P02-1040,0,0.108666,"refore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required. However, this segmentation is normally performed as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainProceedings of the 12th Conference of the European Chapter of the ACL, pages 549–557, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 549 system—Moses (Koehn et al., 2007). The performance of PB-SMT system is measured with B LEU score (Papineni et al., 2002). We firstly measure the influence of word segmentation on in-domain data with respect to the three above mentioned segmenters, namely UN data from the NIST 2006 evaluation campaign. As can be seen from Table 1, using monolingual segmenters achieves consistently better SMT performance than character-based segmentation (CS) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large. We can also observe that the ICT and Stanford segmenter consistently outperform the LDC segmenter. Even using 3M sentence pairs for tr"
E09-1063,J96-3004,0,0.151699,"Missing"
E09-1063,I05-3027,0,0.0420538,"cerns consistency of performance across different domains. From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain. Our pilot investigation into the influence of word segmentation on SMT involves three offthe-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1 , LDC segmenter2 and Stanford segmenter version 2006-05113 . Both ICTCLAS and Stanford segmenters utilise machine learning techniques, with Hidden Markov Models for ICT (Zhang et al., 2003) and conditional random fields for the Stanford segmenter (Tseng et al., 2005). Both segmentation models were trained on news domain data with named entity recognition functionality. The LDC segmenter is dictionary-based with word frequency information to help disambiguation, both of which are collected from data in the news domain. We used Chinese character-based and manual segmentations as contrastive segmentations. The experiments were carried out on a range of data sizes from news and dialogue domains using a state-of-the-art Phrase-Based SMT (PB-SMT) CS ICT LDC Stanford 40K 8.33 10.17 9.37 10.45 160K 12.47 14.85 13.88 15.26 640K 14.40 17.20 15.86 16.94 3M 17.80 20."
E09-1063,C96-2141,0,0.0594596,"o account. Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains. We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions. 1 Introduction State-of-the-art Statistical Machine Translation (SMT) requires a certain amount of bilingual corpora as training data in order to achieve competitive results. The only assumption of most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words. Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required. However, this segmentation is normally performed as a preprocessing step using various word segmenters. Moreover, most of these segmenters are usually trained on a manually segmented domainProceedings of the 12th Conference of the European Chapter of the ACL, pages 549–557, c Athens, Greece, 30 March – 3 April 2009. 200"
E09-1063,W04-1118,0,0.753321,"roach can easily be scaled up to larger data sets and achieves competitive results if the small data used is a representative sample. As for future work, firstly we plan to integrate some named entity recognisers into our approach. We also plan to try our approach in more domains and on other language pairs (e.g. Japanese– English). Finally, we intend to explore the correlation between vocabulary size and the amount of training data needed in order to achieve good results using our approach. IWSLT07 31.41 31.11 30.51 29.35 29.75 31.75 Table 11: BS on IWSLT data sets using MTTK 7 Related Work (Xu et al., 2004) were the first to question the use of word segmentation in SMT and showed that the segmentation proposed by word alignments can be used in SMT to achieve competitive results compared to using monolingual segmenters. Our approach differs from theirs in two aspects. Firstly, (Xu et al., 2004) use word aligners to reconstruct a (monolingual) Chinese dictionary and reuse this dictionary to segment Chinese sentences as other monolingual segmenters. Our approach features the use of a bilingual dictionary and conducts a different segmentation. In addition, we add a process which optimises the biling"
E09-1063,2005.iwslt-1.18,0,0.48927,"s tCOOC and tAC , which allow for the control of the size of the dictionary and the quality of its contents. Some other measures (including the Dice coefficient) could be considered; however, it has to be noted that we are more interested here in the filtering than in the discovery of alignments per se, since our method builds upon an existing aligner. Moreover, we will see that even these simple measures can lead to an improvement in the alignment process in an MT context. 3.4 4.2 Word Lattice Generation Previous research on generating word lattices relies on multiple monolingual segmenters (Xu et al., 2005; Dyer et al., 2008). One advantage of our approach is that the bilingually motivated segmentation process facilitates word lattice generation without relying on other segmenters. As described in section 3.4, the update of the training corpus based on the constructed bilingual dictionary requires that the sentence pair meets the bilingual constraints. Such a segmentation process in the training stage facilitates the utilisation of word lattice decoding. Bootstrapped word segmentation Once the candidates are extracted, we perform word segmentation using the bilingual dictionaries constructed us"
E09-1063,W03-1730,0,0.0195884,"d the mechanisms of the segmenters themselves. A current research interest concerns consistency of performance across different domains. From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain. Our pilot investigation into the influence of word segmentation on SMT involves three offthe-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1 , LDC segmenter2 and Stanford segmenter version 2006-05113 . Both ICTCLAS and Stanford segmenters utilise machine learning techniques, with Hidden Markov Models for ICT (Zhang et al., 2003) and conditional random fields for the Stanford segmenter (Tseng et al., 2005). Both segmentation models were trained on news domain data with named entity recognition functionality. The LDC segmenter is dictionary-based with word frequency information to help disambiguation, both of which are collected from data in the news domain. We used Chinese character-based and manual segmentations as contrastive segmentations. The experiments were carried out on a range of data sizes from news and dialogue domains using a state-of-the-art Phrase-Based SMT (PB-SMT) CS ICT LDC Stanford 40K 8.33 10.17 9.3"
E09-1063,W08-0335,0,0.33881,"r Language Technology School of Computing Dublin City University Dublin 9, Ireland {yma, away}@computing.dcu.ie Abstract specific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in a monolingual context. Consequently, such segmenters cannot produce consistently good results when used across different domains. A substantial amount of research has been carried out to address the problems of word segmentation. However, most research focuses on combining various segmenters either in SMT training or decoding (Dyer et al., 2008; Zhang et al., 2008). One important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre. Segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models or PB-SMT decoding (Ma et al., 2007). The resulting segmentation will consequently influence the performance of an SMT system. In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT. We utilise a small bilingual corpus with the relevant language segmented i"
E09-1063,W05-0909,0,\N,Missing
E17-2095,P96-1041,0,0.0522628,"ificantly better than GBMT at p ≤ 0.01. ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than TBMT at p ≤ 0.01. Following Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingl"
E17-2095,N12-1047,0,0.0158415,"1. ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than TBMT at p ≤ 0.01. Following Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingly, we name the system TBMT. All system"
E17-2095,P11-2031,0,0.022121,"wing Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingly, we name the system TBMT. All systems are implemented in Moses. 3.2 Results and Discussion Table 1 shows BLEU scores of all systems. We found that GBMTctx is better th"
E17-2095,N10-1140,0,0.291447,"raphs by taking source context into consideration. Translations are generated by combining subgraph translations leftto-right using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-"
E17-2095,W09-2301,0,0.0813015,"the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-based reordering function which penalizes discontinuous phrases that have relatively long gaps (Galley and Manning, 2010). The model translates an input graph by segmenting it into subgraphs and generates a complete translation by combining subgraph translations left-to-right. However, the model treats different graph segmentations equally. Therefore, in this paper we propose a contextaware graph"
E17-2095,N03-1017,0,0.0108859,"School of Computing Dublin City University, Ireland {liangyou.li,andy.way,qun.liu}@adaptcentre.ie Abstract In this paper, we present an improved graph-based translation model which segments an input graph into node-induced subgraphs by taking source context into consideration. Translations are generated by combining subgraph translations leftto-right using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al"
E17-2095,P07-2045,0,0.0137392,"bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingly, we name the system TBMT. All systems are implemented in Moses. 3.2 Results and Discussion Table 1 shows BLEU scores of all systems. We found that GBMTctx is better than PBMT across all test sets. Specifically, the improvements are +2.0/+0.7 BLEU on average on ZH–EN and DE– EN, respectively. This improvemen"
E17-2095,P16-1010,1,0.525151,"e of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-based reordering function which penalizes discontinuous phrases that have relatively long gaps (Galley and Manning, 2010). The model translates an input graph by segmenting it into subgraphs and generates a complete translation by combining subgraph translations left-to-right. However, the model treats different graph segmentations equally. Therefore, in this paper we propose a contextaware graph segmentation (Section 2): (i) we add contextual information to each translation rule during traini"
E17-2095,2005.mtsummit-ebmt.13,0,0.0951289,"ht using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-based reordering function which penalizes discontinuous phrases that have relatively long gaps (Galley and Manning, 2010)"
E17-2095,P02-1038,0,0.0278917,"ti i to R ; 4 2010 FIFA 2010 FIFA // segmenting and selecting rules 5 6 7 8 9 10 for q = hsai+1 , ti+1 i in P do c is the set of edges between g ai and sai+1 ; add hg ai , c, ti i to R ; end end end Shijiebei Zai Nanfei Chenggong Juxing r2 : Shijiebei Juxing x h2 : World Cup was held 2010 FIFA World Cup was held Zai Nanfei Chenggong rules are then used to generate segmenting and selecting rules by extending them with contextual connections (Lines 5–8). 2.3 r3 : Zai Nanfei Chenggong h3 : Model and Decoding Following Li et al. (2016), we define our model in the well-known log-linear framework (Och and Ney, 2002). In our experiments, we use the following standard features: two translation probabilities p(g, c|t) and p(t|g, c), two lexical translation probabilities plex (g, c|t) and plex (t|g, c), a language model p(t), a rule penalty, a word penalty, and a distortion function as defined in Galley and Manning (2010). In addition, we add one more feature into our system: a basic-rule penalty to distinguish basic rules from segmenting and selecting rules. Our decoder is very similar to the one in the conventional graph-based model, which generates hypotheses left-to-right using beam search. A hypothesis"
E17-2095,J03-1002,0,0.013059,"BMT GBMT GBMTctx 33.2 33.8∗ 34.7∗+ 35.4∗+ 19.5 19.6 19.8∗+ 20.1∗+ Basic Rule Segmenting Rule Selecting Rule Total 31.8 31.7 32.4∗+ 33.7∗+ 21.9 22.1∗ 22.4∗+ 22.8∗+ Table 1: BLEU scores of all systems. Bold figures mean GBMTctx is significantly better than GBMT at p ≤ 0.01. ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than TBMT at p ≤ 0.01. Following Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system describ"
E17-2095,J04-4002,0,0.0964544,"with each other. A selecting rule is used to select one of them. For example, rule (6) can be applied to (7) to translate 2010Nian FIFA to 2010 FIFA. In this example, the x in rule (6) matches with Chenggong Juxing (in the dashed rectangle) in (7). During training, given a word-aligned graph– string pair hg, t, ai, we extract translation rules hg ai , cai , ti i, each of which consists of a continuous target phrase ti , a source subgraph gai aligned to ti , and a source context cai . We first find initial pairs. h˜ sai , ti i is an initial pair, iff it is consistent with the word alignment a (Och and Ney, 2004). s˜aj is a set of source words which are aligned to ti . Then, the set of rules satisfies the following: 2010Nian FIFA Shijiebei (4) where dashed links are contextual connections. During decoding, when the context matches, rule (4) translates a subgraph over 2010Nian FIFA into a target phrase 2010 FIFA. For example, it can be applied to graph (5) where Shijiebei Zai Nanfei (in the dashed rectangle) is treated as x: Training 1. If h˜ sai , ti i is an initial pair and s˜ai is covered by a subgraph g ai which is connected, then hg ai , ∗, ti i is a basic rule. cai = ∗ means that a basic rule is"
E17-2095,P02-1040,0,0.098098,"a system is significantly better than TBMT at p ≤ 0.01. Following Li et al. (2016), Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune feature weights. We report BLEU (Papineni et al., 2002) scores averaged on three runs of MIRA (Clark et al., 2011). We compare our system GBMTctx with several other systems. A system PBMT is built using the phrase-based model in Moses (Koehn et al., 2007). GBMT is the graph-based translation system described in Li et al. (2016). To examine the influence of bigram links, GBMT is also used to translate dependency trees where treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) are the basic translation units. Accordingly, we name the system TBMT. All systems are implemented in Moses. 3.2 Results and Discussion Table 1 shows BLE"
E17-2095,P05-1034,0,0.384189,"to node-induced subgraphs by taking source context into consideration. Translations are generated by combining subgraph translations leftto-right using beam search. Experiments on Chinese–English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgra"
E17-2095,W07-0706,1,0.838957,"English and German–English demonstrate that the context-aware segmentation significantly improves the baseline graph-based model. 1 Introduction The well-known phrase-based statistical translation model (Koehn et al., 2003) extends the basic translation units from single words to continuous phrases to capture local phenomena. However, one of its significant weaknesses is that it cannot learn generalizations (Quirk et al., 2005; Galley and Manning, 2010). To allow discontinuous phrases (any subset of words of an input sentence), dependency treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007) can be used, which are connected subgraphs on trees. However, continuous phrases which are not connected on trees and thus excluded could in fact be extremely important to system performance (Koehn et al., 2003; Hanneman and Lavie, 2009). To make use of the merits of both phrase-based models and treelet-based models, Li et al. (2016) proposed a graph-based translation model as in Equation (1): source subgraph g ai .1 d is a distance-based reordering function which penalizes discontinuous phrases that have relatively long gaps (Galley and Manning, 2010). The model translates an input graph by"
E17-2101,W16-3210,0,0.179052,"Missing"
E17-2101,N16-1101,0,0.0298551,"been successfully tackled by different groups using the sequence-to-sequence framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). However, multi-modal MT has just recently been addressed by the MT community in a shared task (Specia et al., 2016). In NMT, Bahdanau et al. (2015) first proposed to use an attention mechanism in the decoder. Their decoder learns to attend to the relevant source-language words as it generates each word of the target sentence. Since then, many authors have proposed different ways to incorporate attention into MT (Luong et al., 2015; Firat et al., 2016; Tu et al., 2016). In the context of image description generation (IDG), Vinyals et al. (2015) proposed an influential neural IDG model based on the sequence-to-sequence framework and trained end-to-end. Elliott et al. (2015) put forward a model to generate multilingual descriptions of images by learning and transferring features between two independent, non-attentive neural image description models. Finally, Xu et al. (2015) proposed an attention-based model where a model learns to attend to specific areas of an image representation as it Acknowledgements The ADAPT Centre for Digital Content"
E17-2101,D16-1025,0,0.137277,"efore, we explore the potential of multi-modal, multilingual MT of auction listings’ titles and product images from English into German. To that end, we compare eBay’s production system, due to service-level agreements a classic phrase-based statistical MT (PBSMT) system, with two neural MT (NMT) systems. One of the NMT models is a text-only attentional NMT and the other is a multi-modal attentional NMT model trained using the product images as additional data. PBSMT still outperforms both text-only and multimodal NMT models in the translation of product listings, contrary to recent findings (Bentivogli et al., 2016). Under the hypothesis that the amount of training data could be the culprit and since curated multilingual, multi-modal in-domain data is very expensive to obtain, we back-translate monolingual listings and incorporate them as additional synthetic training data. Utilising synthetic data leads to big gains in performance and ultimately brings NMT models closer to bridging the gap with an optimized PBSMT system. We also use multi-modal NMT models to rescore the output of a PBSMT system and show significant improvements in TER (Snover et al., 2006). This paper is structured as follows. In §2 we"
E17-2101,P13-2121,0,0.0541374,"Missing"
E17-2101,W15-3001,0,0.0299657,"tion of the English description into German. Translating user-generated product listings has particular challenges; they are often ungrammatical and can be difficult to interpret in isolation even by a native speaker of the language, as can be seen in the examples in Table 1. To further demonstrate this issue, in Table 2 we show the number of running words as well as the perplexity scores obtained with LMs trained on three sets of different German corpora: the Multi30k, eBay’s in-domain data and a concatenation of the WMT 20152 Europarl (Koehn, 2005), Common Crawl and News Commentary corpora (Bojar et al., 2015).3 LM training #words Perplexity (×1000) corpus (×1000) eBay Multi30k WMT’15 Multi30k eBay 4310.0 29.0 99.0 60.1 25.2 1.8 0.5 0.05 4.2 Table 2: Perplexity on eBay and Multi30k’s test sets for LMs trained on different corpora. WMT’15 is the concatenation of the Europarl, Common Crawl and News Commentary corpora (the German side of the parallel English–German corpora). We see that different LM perplexities on eBay’s test set are high even for an LM trained on eBay in-domain data. LMs trained on mixed-domain corpora such as the WMT 2015 corpora or the Multi30k have perplexities below 500 on the M"
E17-2101,P16-1227,0,0.110564,"duct Listings Iacer Calixto1 , Daniel Stein2 , Evgeny Matusov2 , Pintu Lohar1 , Sheila Castilho1 and Andy Way1 1 ADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland 2 eBay Inc., Aachen, Germany {iacer.calixto,pintu.lohar,sheila.castilho,andy.way}@adaptcentre.ie {danstein,ematusov}@ebay.com Abstract metric. Nevertheless, when presenting humans with images of the product which come along with the auction titles, the listings are perceived as somewhere between “easy” and “neutral” to understand. Images can bring useful complementary information to MT (Calixto et al., 2012; Hitschler et al., 2016; Huang et al., 2016). Therefore, we explore the potential of multi-modal, multilingual MT of auction listings’ titles and product images from English into German. To that end, we compare eBay’s production system, due to service-level agreements a classic phrase-based statistical MT (PBSMT) system, with two neural MT (NMT) systems. One of the NMT models is a text-only attentional NMT and the other is a multi-modal attentional NMT model trained using the product images as additional data. PBSMT still outperforms both text-only and multimodal NMT models in the translation of product listings, co"
E17-2101,W16-2358,0,0.397223,"Missing"
E17-2101,W16-2360,0,0.312031,"xto1 , Daniel Stein2 , Evgeny Matusov2 , Pintu Lohar1 , Sheila Castilho1 and Andy Way1 1 ADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland 2 eBay Inc., Aachen, Germany {iacer.calixto,pintu.lohar,sheila.castilho,andy.way}@adaptcentre.ie {danstein,ematusov}@ebay.com Abstract metric. Nevertheless, when presenting humans with images of the product which come along with the auction titles, the listings are perceived as somewhere between “easy” and “neutral” to understand. Images can bring useful complementary information to MT (Calixto et al., 2012; Hitschler et al., 2016; Huang et al., 2016). Therefore, we explore the potential of multi-modal, multilingual MT of auction listings’ titles and product images from English into German. To that end, we compare eBay’s production system, due to service-level agreements a classic phrase-based statistical MT (PBSMT) system, with two neural MT (NMT) systems. One of the NMT models is a text-only attentional NMT and the other is a multi-modal attentional NMT model trained using the product images as additional data. PBSMT still outperforms both text-only and multimodal NMT models in the translation of product listings, contrary to recent find"
E17-2101,D13-1176,0,0.052574,"xplore in future work. We also found that NMT models trained on small in-domain data sets can still be successfully used to rescore a standard PBSMT system with significant improvements in TER. Since we know from our experiments with LM perplexities that these are very high for e-commerce data. i.e. fluency is quite low, it seems fitting that BLEU scores do not improve as much. In future work, we will also conduct a human evaluation of the translations generated by the various systems. Related work NMT has been successfully tackled by different groups using the sequence-to-sequence framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). However, multi-modal MT has just recently been addressed by the MT community in a shared task (Specia et al., 2016). In NMT, Bahdanau et al. (2015) first proposed to use an attention mechanism in the decoder. Their decoder learns to attend to the relevant source-language words as it generates each word of the target sentence. Since then, many authors have proposed different ways to incorporate attention into MT (Luong et al., 2015; Firat et al., 2016; Tu et al., 2016). In the context of image description generation (IDG), Vinyals et al. (2015) propo"
E17-2101,W16-2359,1,0.899956,"s listed on the eBay main site1 . Product listings contain extremely high trigram perplexities even if trained (and applied) on in-domain data, which is a challenge not only for proper language models but also for automatic evaluation metrics such as the n-gram precision-based BLEU (Papineni et al., 2002) 1 2 Model We first briefly introduce the two text-only baselines used in this work: a PBSMT model (§2.1) and a textonly attentive NMT model (§2.2). We then discuss the doubly-attentive multi-modal NMT model that we use in our experiments (§2.3), which is comparable to the model introduced by Calixto et al. (2016). http://www.ebay.com/ 637 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 637–643, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Figure 1: Decoder RNN with attention over source sentence and image features. This decoder learns to independently attend to image patches and source-language words when generating translations. 2.1 Statistical Machine Translation (SMT) tion mechanism. The attention mechanism computes a context vector ct for each time step t of the decoder"
E17-2101,W14-4012,0,0.177509,"Missing"
E17-2101,P07-2045,0,0.00850619,"uistics: Volume 2, Short Papers, pages 637–643, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Figure 1: Decoder RNN with attention over source sentence and image features. This decoder learns to independently attend to image patches and source-language words when generating translations. 2.1 Statistical Machine Translation (SMT) tion mechanism. The attention mechanism computes a context vector ct for each time step t of the decoder where this vector is a weighted sum of the source annotation vectors C: We use a PBSMT model built with the Moses SMT Toolkit (Koehn et al., 2007). The language model (LM) is a 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) for tuning the model parameters for BLEU scores. 2.2 src T src src esrc t,i = (va ) tanh(Ua st−1 + Wa hi ), αsrc t,i = Text-only Neural Machine Translation (NMTt ) ct = We use the attentive NMT model introduced by Bahdanau et al. (2015) as our text-only NMT baseline. It is based on the encoder–decoder framework and it implements an attention mechanism over the sourcesentence words. Being X = (x1 , x2 , · · · , xN ) and Y = (y1 , y2 , · · · , yM ) a"
E17-2101,P11-2031,0,0.169281,"s in different languages. Altogether, we have a strong indication that images can indeed help an MT model translate product listings, especially for translations into German. 4 Model Table 4: Comparative results with PBSMT, NMTt and multi-modal models NMTm evaluated on eBay’s test set. Best PBSMT and NMT results in bold. best lists to be able to evaluate these two scenarios independently. We evaluate our models quantitatively using BLEU4 (Papineni et al., 2002) and TER (Snover et al., 2006) and report statistical significance computed using approximate randomisation with the Multeval toolkit (Clark et al., 2011). 5 Results In Table 4 we present quantitative results obtained with the two text-only baselines SMT and NMTt and one multi-modal model NMTm . It is clear that the gains from adding more data are much more apparent to the multi-modal NMTm model than to the two text-only ones. This can be attributed to the fact that this model has access to more data, i.e. image features, and consequently can learn better representations derived from them. The PBSMT model’s improvements are inconsistent; its TER score even deteriorates by 0.5 with the additional data. The same does not happen with the NMT model"
E17-2101,2005.mtsummit-papers.11,0,0.0574817,"Flickr, one description in English and one human translation of the English description into German. Translating user-generated product listings has particular challenges; they are often ungrammatical and can be difficult to interpret in isolation even by a native speaker of the language, as can be seen in the examples in Table 1. To further demonstrate this issue, in Table 2 we show the number of running words as well as the perplexity scores obtained with LMs trained on three sets of different German corpora: the Multi30k, eBay’s in-domain data and a concatenation of the WMT 20152 Europarl (Koehn, 2005), Common Crawl and News Commentary corpora (Bojar et al., 2015).3 LM training #words Perplexity (×1000) corpus (×1000) eBay Multi30k WMT’15 Multi30k eBay 4310.0 29.0 99.0 60.1 25.2 1.8 0.5 0.05 4.2 Table 2: Perplexity on eBay and Multi30k’s test sets for LMs trained on different corpora. WMT’15 is the concatenation of the Europarl, Common Crawl and News Commentary corpora (the German side of the parallel English–German corpora). We see that different LM perplexities on eBay’s test set are high even for an LM trained on eBay in-domain data. LMs trained on mixed-domain corpora such as the WMT 20"
E17-2101,W16-2361,0,0.129799,"Missing"
E17-2101,D15-1166,0,0.0619019,"Related work NMT has been successfully tackled by different groups using the sequence-to-sequence framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). However, multi-modal MT has just recently been addressed by the MT community in a shared task (Specia et al., 2016). In NMT, Bahdanau et al. (2015) first proposed to use an attention mechanism in the decoder. Their decoder learns to attend to the relevant source-language words as it generates each word of the target sentence. Since then, many authors have proposed different ways to incorporate attention into MT (Luong et al., 2015; Firat et al., 2016; Tu et al., 2016). In the context of image description generation (IDG), Vinyals et al. (2015) proposed an influential neural IDG model based on the sequence-to-sequence framework and trained end-to-end. Elliott et al. (2015) put forward a model to generate multilingual descriptions of images by learning and transferring features between two independent, non-attentive neural image description models. Finally, Xu et al. (2015) proposed an attention-based model where a model learns to attend to specific areas of an image representation as it Acknowledgements The ADAPT Centre"
E17-2101,P03-1021,0,0.0568175,"th attention over source sentence and image features. This decoder learns to independently attend to image patches and source-language words when generating translations. 2.1 Statistical Machine Translation (SMT) tion mechanism. The attention mechanism computes a context vector ct for each time step t of the decoder where this vector is a weighted sum of the source annotation vectors C: We use a PBSMT model built with the Moses SMT Toolkit (Koehn et al., 2007). The language model (LM) is a 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) for tuning the model parameters for BLEU scores. 2.2 src T src src esrc t,i = (va ) tanh(Ua st−1 + Wa hi ), αsrc t,i = Text-only Neural Machine Translation (NMTt ) ct = We use the attentive NMT model introduced by Bahdanau et al. (2015) as our text-only NMT baseline. It is based on the encoder–decoder framework and it implements an attention mechanism over the sourcesentence words. Being X = (x1 , x2 , · · · , xN ) and Y = (y1 , y2 , · · · , yM ) a one-hot representation of a sentence in a source language and its translation into a target language, respectively, the model is trained to maximi"
E17-2101,Q14-1006,0,0.016563,"t sets used in our experiments consist of 480 and 444 triples, respectively. The curation of parallel product listings with an accompanying product image is costly and timeconsuming, so the in-domain data is rather small. More easily accessible are monolingual German listings accompanied by the product image where the source text input can be emulated by back-translating the target listing. For this set of experiments, we use 83, 832 tuples, henceforth mono. Finally, we also use the publicly available Multi30k dataset (Elliott et al., 2016), a multilingual expansion of the original Flickr30k (Young et al., 2014) with ∼30k pictures from Flickr, one description in English and one human translation of the English description into German. Translating user-generated product listings has particular challenges; they are often ungrammatical and can be difficult to interpret in isolation even by a native speaker of the language, as can be seen in the examples in Table 1. To further demonstrate this issue, in Table 2 we show the number of running words as well as the perplexity scores obtained with LMs trained on three sets of different German corpora: the Multi30k, eBay’s in-domain data and a concatenation of"
E17-2101,P02-1040,0,0.102634,"slation (MT). Among the challenges in automatic processing are the specialized language and grammar for listing titles, as well as a high percentage of user-generated content for nonbusiness sellers, who often are not native speakers themselves. We investigate the nature of user-generated auction listings’ titles as listed on the eBay main site1 . Product listings contain extremely high trigram perplexities even if trained (and applied) on in-domain data, which is a challenge not only for proper language models but also for automatic evaluation metrics such as the n-gram precision-based BLEU (Papineni et al., 2002) 1 2 Model We first briefly introduce the two text-only baselines used in this work: a PBSMT model (§2.1) and a textonly attentive NMT model (§2.2). We then discuss the doubly-attentive multi-modal NMT model that we use in our experiments (§2.3), which is comparable to the model introduced by Calixto et al. (2016). http://www.ebay.com/ 637 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 637–643, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Figure 1: Decoder RNN with"
E17-2101,P16-1009,0,0.0443197,"tt et al., 2016) data sets. We also did not use any additional parallel, but out-of-domain data that had been used to train eBay’s PBSMT production system (see Section 5). Training our text-only NMTt baseline on this large corpus would not help shed more light on how multi-modality helps MT, since it has no images available and thus cannot be used to train the multimodal model NMTm . Rather, we report results of reranking experiments using n-best lists generated by eBay’s best-performing PBSMT production system. In order to measure the impact of the training data size on MT quality, we follow Sennrich et al. (2016) and back-translate the mono German product listings using our baseline NMTt model trained on the original 23, 697 German→English corpus (- images). These additional synthetic data (including images) are added to the original’s 23, 697 triples and used in our translation experiments. We do not include the back-translated data set when training NMT models for re-ranking n640 Model Training data baseline N BLEU oracle TER oracle Translation length — 29.0 — 53.0 — 13.60 ±2.59 NMTt NMTm 100k in-domain orig. + Multi30k 10 10 29.3 ↑ 0.3 29.4 ↑ 0.4 35.4 35.4 52.4 † ↓ 0.6 52.1 † ↓ 0.9 46.4 46.4 13.48"
E17-2101,W16-2363,0,0.0802193,"no significant difference between the length of translations for the baseline and the reranked models. 6 generates its description in natural language with a softattention mechanism. Although no purely neural multi-modal model to date has significantly improved on both text-only NMT and SMT models on the Multi30k data set (Specia et al., 2016), different research groups have proposed to include images in re-ranking n-best lists generated by an SMT system or directly in a NMT framework with some success (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick´y et al., 2016; Shah et al., 2016). To the best of our knowledge, we are the first to study multi-modal NMT applied to the translation of product listings, i.e. for the e-commerce domain. 7 Conclusions and Future work In this paper, we investigate the potential impact of multi-modal NMT in the context of e-commerce product listings. With only a limited amount of multimodal and multilingual training data available, both text-only and multi-modal NMT models still fail to outperform a productive SMT system, contrary to recent findings (Bentivogli et al., 2016). However, the introduction of back-translated data leads to substantia"
E17-2101,2006.amta-papers.25,0,0.423403,"duct listings, contrary to recent findings (Bentivogli et al., 2016). Under the hypothesis that the amount of training data could be the culprit and since curated multilingual, multi-modal in-domain data is very expensive to obtain, we back-translate monolingual listings and incorporate them as additional synthetic training data. Utilising synthetic data leads to big gains in performance and ultimately brings NMT models closer to bridging the gap with an optimized PBSMT system. We also use multi-modal NMT models to rescore the output of a PBSMT system and show significant improvements in TER (Snover et al., 2006). This paper is structured as follows. In §2 we describe the text-only and multi-modal MT models we evaluate and in §3 the data sets we used, also introducing and discussing interesting findings. In §4 we discuss how we structure our quantitative evaluation, and in §5 we analyse and discuss our results. In §6 we discuss some relevant related work and in §7 we draw conclusions and devise future work. In this paper we study the impact of using images to machine-translate user-generated ecommerce product listings. We study how a multi-modal Neural Machine Translation (NMT) model compares to two t"
E17-2101,W16-2346,0,0.347903,"Missing"
gough-etal-2002-example,1999.tc-1.8,0,\N,Missing
gough-etal-2002-example,2000.tc-1.7,0,\N,Missing
I17-1093,P14-2134,0,0.135901,"Missing"
I17-1093,Q17-1010,0,0.0922808,"Missing"
I17-1093,D14-1121,0,0.0723739,"Missing"
I17-1093,P15-1073,0,0.433513,"Natural Language Processing, pages 926–936, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP pose a sentence-level rule-based semantic filtering approach, which utilizes grammatical relations among words to remove offensive contents in a sentence from Youtube comments. Although this may be a valuable work, its scope deviates from our specific goal, which aims at automatically detecting racist online messages. ence to money, banking, and media (Warner and Hirschberg, 2012). Second, the demographic context of the speaker may greatly influence word choices, syntax, and even semantics (Hovy, 2015). For instance, a young female rapper from Australia may not use the exact same language as an elder male economist from South Africa to express her racists thoughts (Figure 1). The first identified studies that can directly match our objectives are proposed by Kwok and Wang (2013) and Warner and Hirschberg (2012). In Kwok and Wang (2013), the authors propose a na¨ıve Bayes classifier based on unigram features to classify tweets as racist or non-racist. It is important to notice that the standard data sets of racist tweets were selected from Twitter accounts that were self-classified as racist"
I17-1093,P15-1150,0,0.0280118,"Missing"
I17-1093,W12-2103,0,0.836702,"tter (Silva et al., 2016). Racist speech relates to a socially constructed idea about differences between social groups based on phenotype, ancestry, culture or religion (Paradies, 2006a) and covers the categories of race (e.g. black people), ethnicity (e.g. Chinese people), and religion (e.g. jewish people) introduced in Silva et al. (2016). Racism is often expressed through negative and inaccurate stereotypes with one-word epithets (e.g. tiny), phrases (e.g. big nose), concepts (e.g. head bangers), metaphors (e.g. coin slot), and juxtapositions (e.g. yellow cab) that convey hateful intents (Warner and Hirschberg, 2012). As such, its automatic identification is a challenging task. Moreover, the racist language is not uniform. First, it highly depends on contextual features of the targeted community. For example, anti-african-american messages often refer to unemployment or single parent upbringing whereas anti-semitic language predominantly makes referMost social media platforms grant users freedom of speech by allowing them to freely express their thoughts, beliefs, and opinions. Although this represents incredible and unique communication opportunities, it also presents important challenges. Online racism"
I17-1093,N16-2013,0,0.190977,"fiers for each of the four hatefulspeech categories using combinations of feature types: ngrams (1 to 5), slur words and typeddependencies. Overall, the highest F1 scores are 927 achieved by combining ngrams and hateful terms, although the inclusion of typed dependencies reduces false positive rates. In a second step, the authors build a data-driven blended model where more than one protected characteristic is taken at a time and show that hateful speech is a domain dependent problem. The important finding of this work is the relevance of the simple dictionary lookup feature of slur words. In Waseem and Hovy (2016), the authors study the importance of demographic features (gender and location) for hateful speech (racism, sexism and others) classification from Twitter as well as proposed to deal with data sparseness by substituting word ngrams with character ngram (1 to 4) features. 10-fold cross validation results with a logistic regression show that the demographic features (individually or combined) do to not lead to any improvement, while character-based classification outperforms by at least 5 points the F1 score of word-based classification. Although nonconclusive results are obtained with demograp"
I17-3009,P07-2045,0,0.0193009,"Missing"
I17-3009,P03-1021,0,0.0209504,"SLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30.70 31.10 Evaluation of Task-Oriented Named Entity and Translation We manually annotated Chinese and English sentences in the test set to evaluate the proposed taskoriented NE recognition and translation in terms of accuracy, recall and F1. In Table 4, Recog indicates NE recognition on the"
I17-3009,J03-1002,0,0.0059894,"20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30.70 31.10 Evaluation of Task-Oriented Named Entity and Translation We manually annotated Chinese and English sentences in the test set to evaluate the proposed taskoriented NE recognition and translation in terms of accuracy, recall and F1. In Table 4,"
I17-3009,D17-1301,1,0.67952,"Missing"
I17-3009,N16-1113,1,0.838057,"he selected pseudo in-domain data can improve the performance by at most +1.09 and +1.24 on EN-ZH (top-50K) and ZH-EN (top50K), respectively. However, bring more pseudo in-domain data (&gt; top − 250K), the performance drops sharply. 4 ZH-EN 17.90 20.30 20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30"
I17-3009,L16-1436,1,0.749687,"he selected pseudo in-domain data can improve the performance by at most +1.09 and +1.24 on EN-ZH (top-50K) and ZH-EN (top50K), respectively. However, bring more pseudo in-domain data (&gt; top − 250K), the performance drops sharply. 4 ZH-EN 17.90 20.30 20.20 Table 3: Overall performance. From the IWSLT DIALOG corpus,we select 1,023 and 1053 hotel booking sentences (34/36 dialogues) as development set and test set, respectively. We combine our home-made travel domain corpora as in-domain training data (180K). We also use domain adaptation techniques to select in-domain data from movie subtitles (Wang et al., 2016b). We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on Chinese (ZH)–English (EN). Furthermore, we train a 5-gram language model using the SRI Language Toolkit (Stolcke, 2002). We run GIZA++ (Och and Ney, 2003) for alignment and use MERT (Och, 2003) to optimize the feature weights. We develop TODAY on the basis of an open-source live support application Mibew4 by integrating our semantics-enhanced SMT system and the semantic form filling. 3.2 4 +6.5 +6.9 Table 2: Performance with task-oriented NE recognition. Experiments and Analysis 3.1 EN-ZH 24.20 30"
I17-4027,pak-paroubek-2010-twitter,0,0.0239702,"roß (2013) detect the individual product aspects reviewers have commented on and to decide whether the comments are rather positive or negative. They focus on the two main subtasks of aspect-oriented review mining: (i) identifying relevant product aspects, and (ii) determining and classifying expressions of sentiment. Gr¨abner et al. (2012) propose a system that performs the classification of customer reviews of hotels by means of a sentiment analysis. They elaborate on a process to extract a domain-specific lexicon of semantically relevant words based on a given corpus (Scharl et al. (2003); Pak and Paroubek (2010)). The resulting lexicon backs the sentiment analysis for generating a classification of the reviews. 3 Customer feedback analysis 4 Most app companies treat the contents of these reports as confidential materials and also regard things such as categorisation of customer feedback as business secrets. To the best of our knowledge, there are only few openly available categorisations from these app companies. One of them is the commonly used categorisation which could be found in many websites, i.e., the five-class Excellent-Good-Average-Fair-Poor Experiments Statistics of the whole data sets in"
J03-3004,ahrenberg-etal-2002-system,0,0.0582241,"Missing"
J03-3004,T75-2013,0,0.0511091,"and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: • Learnability (Zernik and Dyer 1987) • Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996) • Speech generation (Rayner and Carter 1997) • Localization (Sch¨aler 1996) More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at a subsentential level, while Carl, Way, and Sch¨aler (2002) and Sch¨aler, Way, and Carl (2003, pages 108–109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment. This, they suggest, may result in a paradigm shift f"
J03-3004,C00-1019,0,0.014354,"V]   [das V was], [which V what]  .. . (26)  [V ist was Sie], [V is what you]  .. .  [das ist was V wollten], [which is what V wanted]  .. . Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and Guvenir ¨ (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into their generalized equivalents, as in (28): (28) <DET> good man : bon homme That is, where Block (2000) substitutes variables for various words in his templates, we replace certain lexical items with thei"
J03-3004,1999.mtsummit-1.37,0,0.146819,"[das V], [which V]   [das V was], [which V what]  .. . (26)  [V ist was Sie], [V is what you]  .. .  [das ist was V wollten], [which is what V wanted]  .. . Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and Guvenir ¨ (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into their generalized equivalents, as in (28): (28) <DET> good man : bon homme That is, where Block (2000) substitutes variables for various words in his templates, we replace certain lexic"
J03-3004,carl-etal-2002-toward,1,0.82836,"Missing"
J03-3004,A94-1016,0,0.0616601,"Missing"
J03-3004,1994.amta-1.10,0,0.157134,"Missing"
J03-3004,W97-0119,0,0.00906536,"guistics Volume 29, Number 3 2. Deriving Translation Resources from Web-Based MT Systems All EBMT systems, from the initial proposal by Nagao (1984) to the recent collection of Carl and Way (2003), are premised on the availability of subsentential alignments derived from the input bitext. There is a wealth of literature on trying to establish subsentential translations from a bilingual corpus.3 Kay and Roscheisen ¨ (1993) attempt to extract a bilingual dictionary using a hybrid method of sentence and word alignment on the assumption that the source, target words have a similar distribution. Fung and McKeown (1997) attempt to translate technical terms using word relation matrices, although the resource from which such relations are derived is a pair of nonparallel corpora. Somers (1998) replicates the work of Fung and McKeown with different language pairs using the simpler metric of Levenshtein distance. Boutsis and Piperidis (1998) use a tagged parallel corpus to extract translationally equivalent English-Greek clauses on the basis of word occurrence and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersso"
J03-3004,gough-etal-2002-example,1,0.690137,"Missing"
J03-3004,1999.tc-1.8,0,0.10511,"Missing"
J03-3004,hogan-frederking-1998-evaluation,0,0.0341364,"Missing"
J03-3004,C92-2101,0,0.140584,"Missing"
J03-3004,2000.tc-1.7,0,0.0523599,"Missing"
J03-3004,1999.tmi-1.10,0,0.0104655,"(25):  [V ist], [V is]   [das V], [which V]   [das V was], [which V what]  .. . (26)  [V ist was Sie], [V is what you]  .. .  [das ist was V wollten], [which is what V wanted]  .. . Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and Guvenir ¨ (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into their generalized equivalents, as in (28): (28) <DET> good man : bon homme That is, where Block (2000) substitutes variables for various words in his templates, we replace"
J03-3004,C90-3044,0,0.0496343,"20 over both knowledge sources. Similarly, if we wish to consider translations produced by all three MT systems, then we add the weights of common translations and divide the weights of all proposed translations by six. When translated phrases have been retrieved for each chunk of the input string, they must then be combined to produce an output string. In order to calculate a ranking for each TL sentence produced, we multiply the weights of each chunk used in its construction. Note that this ensures that greater importance is attributed to longer chunks, as is usual in most EBMT systems (cf. Sato and Nagao 1990; Veale and Way 1997; Carl 1999).7 As an example, consider the translation into French of the house collapsed. Assume the conditional probabilities in (33): 7 Note that approaches that prefer the greatest context to be taken into account are not limited to EBMT. Research in the area of data-oriented parsing (cf. Bod, Scha, and Sima’an, 2003) also shows that unless the corpus is inherently biased, derivations constructed using the smallest number of subtrees have a higher probability than those built with a larger number of smaller subtrees. 435 Computational Linguistics (33) Volume 29, Number"
J03-3004,1996.eamt-1.3,0,0.0291291,"Missing"
J03-3004,2003.mtsummit-papers.18,1,0.553506,"Missing"
J03-3004,2001.mtsummit-papers.60,0,0.0166788,"and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: • Learnability (Zernik and Dyer 1987) • Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996) • Speech generation (Rayner and Carter 1997) • Localization (Sch¨aler 1996) More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at a subsentential level, while Carl, Way, and Sch¨aler (2002) and Sch¨aler, Way, and Carl (2003, pages 108–109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment. This, they suggest, may result in a paradigm shift from TM to EBMT via the phrasal lexicon: Translators are on the whole wary of MT technology, but once subsentential alignment is enabled, translators will become aware of the benefits to be gained from source, target phrasal segments, and from there they suggest that “it i"
J03-3004,1993.tmi-1.25,0,0.0145492,"what you wanted]  431 Computational Linguistics Volume 29, Number 3 Using the algorithm described above, the patterns in (26) are derived from the chunks in (25):  [V ist], [V is]   [das V], [which V]   [das V was], [which V what]  .. . (26)  [V ist was Sie], [V is what you]  .. .  [das ist was V wollten], [which is what V wanted]  .. . Of course, many other researchers also try to extract generalized templates. Kaji, Kida, and Morimoto (1992) identify translationally equivalent phrasal segments and replace such equivalents with variables to generate a set of translation patterns. Watanabe (1993) combines lexical and dependency mappings to form his generalizations. Other similar approaches include those of Cicekli and Guvenir ¨ (1996), McTait and Trujillo (1999), Carl (1999), and Brown (2000), inter alia. In our system, in some cases the smallest chunk obtainable via the marker-based segmentation process may be something like (27): (27) <DET> the good man : le bon homme In such cases, if our system were confronted with a good man, it would not be able to translate such a phrase, assuming this to be missing from the marker lexicon. Accordingly, we convert examples such as (27) into the"
J03-3004,J87-3009,0,0.0297397,"Missing"
J03-3004,macklovitch-russell-2000-whats,0,\N,Missing
J03-3004,J93-1006,0,\N,Missing
J03-3004,W98-1503,0,\N,Missing
J05-3003,P87-1027,0,0.109086,"and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar"
J05-3003,J93-2002,0,0.800674,"ter detail in Section 6, in which we compare our results with those reported elsewhere in the literature. We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input. Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames. The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based"
J05-3003,A97-1052,0,0.911163,"and the constituents with which they co-occur. He assumes 19 different subcategorization 334 O’Donovan et al. Large-Scale Induction and Evaluation of Lexical Resources frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb cla"
J05-3003,Y04-1016,1,0.884078,"Missing"
J05-3003,P04-1041,1,0.777592,"Missing"
J05-3003,2000.iwpt-1.9,0,0.104136,"e (Carroll and Rooth 1998). Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004; Nakanishi, Miyao, and Tsujii 2004). In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches. In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate. Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically"
J05-3003,P97-1003,0,0.14526,"n of Collins’s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner. The first step is to label each node as either a head,"
J05-3003,C94-1042,0,0.276371,"Missing"
J05-3003,J93-1005,0,0.22634,"Missing"
J05-3003,W03-2401,0,0.0288161,"Missing"
J05-3003,kingsbury-palmer-2002-treebank,0,0.025675,"Missing"
J05-3003,kinyon-prolo-2002-identifying,0,0.479907,"Missing"
J05-3003,P98-1115,0,0.0235922,"Missing"
J05-3003,H94-1003,0,0.0569859,"Missing"
J05-3003,P95-1037,0,0.00938586,"has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins’s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the app"
J05-3003,P93-1032,0,0.746649,"is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction). Lexicons, including subcategorization details, were traditionally produced by hand. However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998). Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (Chen and Vijay-Shanker 2000; Xia 1999; Hoc"
J05-3003,H94-1020,0,0.0194078,"-structure information. F-structures are attribute–value structures which represent abstract syntactic information, approximating to basic predicate–argument–modifier structures. Most of the early work on automatic f-structure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept. However, more recent work (Cahill et al. 2002; Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences. We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations. The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter, for which an amended version of Magerman (1994) is used. The head is annotated with the LFG equati"
J05-3003,P04-1047,1,0.624486,"Missing"
J05-3003,P98-2184,0,0.0423901,"in higher recall scores than those achieved when we (effectively) reversed the mapping (COMLEX-LFG Mapping II [Section 6.3]). The first mapping is essentially a conflation of our more fine-grained LFG grammatical functions with the more generic COMLEX functions, while the second mapping tries to maintain as many distinctions as possible. Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains. We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt,"
J05-3003,C00-2100,0,0.138208,"ts of verb occurrences in the Penn-II Treebank. This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument. Arguments were then mapped to traditional syntactic functions. For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject. In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 335 Computational Linguistics Volume 31, Number 3 1998). Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesi"
J05-3003,schulte-im-walde-2002-subcategorisation,0,0.146073,"Missing"
J05-3003,C02-1145,0,0.0658598,"Missing"
J05-3003,W98-1505,0,\N,Missing
J05-3003,C98-2179,0,\N,Missing
J05-3003,C98-1111,0,\N,Missing
J08-1003,J97-4005,0,0.0398419,"Missing"
J08-1003,baldwin-etal-2004-road,0,0.0160361,"btain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive effo"
J08-1003,H91-1060,0,0.0647814,"Missing"
J08-1003,E03-1005,0,0.0139868,"tensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage"
J08-1003,J93-1002,0,0.305911,"0], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 howev"
J08-1003,P06-2006,0,0.0594671,"Missing"
J08-1003,W02-1503,0,0.0261788,"uages do not always interpret linguistic material locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf"
J08-1003,P04-1041,1,0.839632,"Missing"
J08-1003,C02-1013,0,0.117113,"al locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been success"
J08-1003,A00-2018,0,0.01612,"Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically"
J08-1003,P04-1014,0,0.0567308,"Missing"
J08-1003,P07-1032,0,0.0390822,"Missing"
J08-1003,P97-1003,0,0.151407,"arsers). The article also reports on a task-based evaluation experiment to rank the parsers using the grammatical relations as input to an anaphora resolution system. Preiss concluded that parser ranking using grammatical relations reﬂected the absolute ranking (between treebank-induced parsers) using traditional tree-based metrics, but that the difference between the performance of the parsing algorithms narrowed when they carried out the anaphora resolution task. Her results show that the hand-crafted deep uniﬁcation parser (Briscoe and Carroll 1993) outperforms the machine-learned parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision and recall on grammatical relations.4 Kaplan et al. (2004) compare their deep, handcrafted, LFG-based XLE parsing system (Riezler et al. 2002) with Collins’s (1999) model 3 using a simple conversion-based approach, capturing dependencies from the tree output of the machine-learned parser, and evaluating both parsers against the PARC 700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep grammar outperforms the state-of-the-art treebank-based shallow parser on the level of dependency representation, at th"
J08-1003,W03-1005,0,0.0353103,"Missing"
J08-1003,C86-1129,0,0.453477,"ion, as well as traces and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II CFG trees with LFG f-structure information. Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse the tree and deterministically add f-structure equations to the phrasal and leaf nodes of the tree, resulting in an f-structure annotated version of the tree. The annotations are then collected and passed on to a constraint solver which generates an f-structure (if the constraints are satisﬁable). We use a simple graph-uniﬁcation-based constraint solver ¨ (Eisele and Dorre 1986), extended to handle path, set-valued, disjunctive, and existential constraints. Given parser output without Penn-II style annotations and traces, the same algorithm is used to assign annotations to each node in the tree, whereas a separate module is applied at the level of f-structure to resolve any long-distance dependencies (see Section 2.3). 5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms (QLFs), and Underspeciﬁed Discourse Representation Structures (UDRSs). 86 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Tabl"
J08-1003,W03-1008,0,0.0100689,"resent research on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are somewhat partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual information, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks.31 7. Conclusions Parser compa"
J08-1003,P03-1046,0,0.0243758,"results show that machine-learning-based resources can outperform deep, state-of-the-art hand-crafted resources with respect to the quality of dependencies generated. Treebank-based, deep and wide-coverage constraint-based grammar acquisition has become an important research topic: Starting with the seminal TAG-based work of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao, Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present research on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automati"
J08-1003,P02-1043,0,0.347852,"and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia 1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii 2003], LFG [Cahill et al. 2002b, 2004]). To a ﬁrst approximation, these approaches can be classiﬁed as “conversion”- or “annotation”-based. TAG-based approaches convert 1 Our use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations"
J08-1003,P02-1018,0,0.00917585,"s Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, ﬁne-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) ele"
J08-1003,N04-1013,1,0.397922,"is (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate strings to informatio"
J08-1003,W03-2401,1,0.923661,"ular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep gram"
J08-1003,P03-1054,0,0.0412189,"ime-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treeba"
J08-1003,P04-1042,0,0.0169367,"pendencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches convert trees into CCG derivations from which CCG categories can be extracted. HPSGand LFG-based grammar induction methods automatically annotate treebank trees with (typed) attribute-value structure information for the extraction of constraint-based grammars and lexical resources. Two recent papers (Preiss 2003; Kaplan et al"
J08-1003,H94-1020,0,0.0187269,"of (related) drawbacks: 1. Bracketed trees do not always provide NLP applications with enough information to carry out the required tasks: Many applications involve a deeper analysis of the input in the form of semantically motivated information such as deep dependency relations, predicate–argument structures, or simple logical forms. 2. A number of alternative, but equally valid tree representations can potentially be given for the same input. To give just a few examples: In English, VPs containing modals and auxiliaries can be analyzed using (predominantly) binary branching rules (Penn-II [Marcus et al. 1994]), or employ ﬂatter analyses where modals and auxiliaries are sisters of the main verb (AP treebank [Leech and Garside 1991]), or indeed do without a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank bracketing guidelines can use “traditional” CFG categories such as S, NP, and so on (Penn-II) or a maximal projection-inspired analysis with IPs and DPs (Chinese Penn Treebank [Xue et al. 2004]). 3. Because a tree-based gold standard for parser evaluation must adopt a particular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), e"
J08-1003,E06-1011,0,0.0298537,"use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, ﬁne-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume"
J08-1003,C04-1204,0,0.0126897,"I treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are somewhat partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual information, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks.31 7. Conclusions Parser comparison is a non-trivial and t"
J08-1003,P04-1047,1,0.924795,"Missing"
J08-1003,E03-1025,0,0.233427,"uistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate s"
J08-1003,P02-1035,1,0.159909,"RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are sha"
J08-1003,C96-1045,1,0.742182,"Missing"
J08-1003,P97-1052,1,0.798137,"Missing"
J08-1003,C00-2137,0,0.0191341,"able tuples between the two systems. Approximate randomization produces shufﬂes by random assignments instead of evaluating all 2S possible assignments. Signiﬁcance levels are computed as the percentage of trials where the pseudo statistic, that is the test statistic computed on the shufﬂed data, is greater than or equal to the actual statistic, that is the test statistic computed on the test data. A sketch of an algorithm for approximate randomization testing is given in Figure 12. 21 Applications of this test to natural language processing problems can be found in Chinchor et al. (1993) and Yeh (2000). 104 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Table 10 Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for approximate randomization test for 10,000,000 randomizations. PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 .0003 <.0001 <.0001 - Table 10 gives the p-values (the smallest ﬁxed level at which the null hypothesis can be rejected) for comparing each parser against all of the other"
J08-1003,J98-4004,0,\N,Missing
J08-1003,W96-0213,0,\N,Missing
J08-1003,J03-4003,0,\N,Missing
J08-1003,J93-3001,0,\N,Missing
K16-1003,N15-1044,0,0.0125157,"niversité de Caen Normandie, Caen, France 2 GREYC UMR 6072, Caen, France 3 ADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland Abstract is one of the most popular smartphones at the present day”, no temporal mention is found, although one may expect that within this context currently and present day share some equivalent temporal dimension. Such systems would certainly benefit from the existence of a temporal resource enumerating a large set of possible time variants (Kuzey et al., 2016). In parallel, new trends have emerged in the context of human temporal orientation (Schwartz et al., 2015). The underlying idea is to understand how past, present, and future emphasis in text may affect people’s finances, health, and happiness. For that purpose, temporal classifiers are built to detect the overall temporal dimension of a given sentence. For instance, the following Facebook post “can’t wait to get a pint tonight” would be tagged as FUTURE. Successful features include timexes, specific temporal (past, present, future) words from a commercial dictionary, but also ngrams, thus indicating that temporality may be embodied by multi-word terms, whose temporal orientation is unknown. As a"
K16-1003,chang-manning-2012-sutime,0,0.0321031,"a semi-supervised minimum cuts strategy that makes use of WordNet glosses and semantic relations to supplement WordNet entries with temporal information. Intrinsic and extrinsic evaluations show that our approach outperforms prior semi-supervised non-graph classifiers. 1 Introduction Recognizing temporal information can significantly improve the functionality of information retrieval (Campos et al., 2014) and natural language processing (Mani et al., 2005) applications. Most text applications have been relying on rule-based time taggers such as HeidelTime (Strötgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and normalize time mentions in texts. Although interesting levels of performance have been seen (UzZaman et al., 2013), their coverage is limited to the finite number of rules they implement. Let’s take the following sentence: “Apple’s iPhone is currently one of the most popular smartphone”. When labeled by SUTime1 or HeidelTime2 , the adverb currently is correctly tagged with the PRESENT_REF value. However, if we change the sentence to “Apple’s iPhone 1 http://nlp.stanford.edu:8080/sutime/ process 2 http://heideltime.ifi.uni-heidelberg. de/heideltime/ 22 Proceedings of the 20th S"
K16-1003,D15-1063,0,0.0181765,"in texts. In this paper, we propose a semi-supervised minimum cuts strategy that makes use of WordNet glosses and semantic relations to supplement WordNet entries with temporal information. Intrinsic and extrinsic evaluations show that our approach outperforms prior semi-supervised non-graph classifiers. 1 Introduction Recognizing temporal information can significantly improve the functionality of information retrieval (Campos et al., 2014) and natural language processing (Mani et al., 2005) applications. Most text applications have been relying on rule-based time taggers such as HeidelTime (Strötgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and normalize time mentions in texts. Although interesting levels of performance have been seen (UzZaman et al., 2013), their coverage is limited to the finite number of rules they implement. Let’s take the following sentence: “Apple’s iPhone is currently one of the most popular smartphone”. When labeled by SUTime1 or HeidelTime2 , the adverb currently is correctly tagged with the PRESENT_REF value. However, if we change the sentence to “Apple’s iPhone 1 http://nlp.stanford.edu:8080/sutime/ process 2 http://heideltime.ifi.uni-heidelberg. de/heid"
K16-1003,E14-4002,1,0.825871,"temporality may be embodied by multi-word terms, whose temporal orientation is unknown. As a consequence, discovering the temporal orientation of words is a challenging issue that may benefit many text applications. Whereas most prior studies have focused on temporal expressions and events, there has been a lack of work looking at the temporal orientation of word senses. In this paper, we focus on automatically timetagging word senses in WordNet (Miller, 1995) as past, present, future, or atemporal based on their glosses and relational semantic structures in the line of Dias et al. (2014) and Hasanuzzaman et al. (2014b). In particular, we propose a semi-supervised graph-based strategy that relies on the max-flow min-cut theorem (Papadimitriou and Steiglitz, 1998; Blum and Chawla, 2001), that finds successive minimum cuts in a connected graph to time-tag each synset as one of the four The ability to capture time information is essential to many natural language processing and information retrieval applications. Therefore, a lexical resource associating word senses to their temporal orientation might be crucial for the computational tasks aiming at the interpretation of language of time in texts. In this pap"
K16-1003,S13-2001,0,0.164793,"mporal information. Intrinsic and extrinsic evaluations show that our approach outperforms prior semi-supervised non-graph classifiers. 1 Introduction Recognizing temporal information can significantly improve the functionality of information retrieval (Campos et al., 2014) and natural language processing (Mani et al., 2005) applications. Most text applications have been relying on rule-based time taggers such as HeidelTime (Strötgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and normalize time mentions in texts. Although interesting levels of performance have been seen (UzZaman et al., 2013), their coverage is limited to the finite number of rules they implement. Let’s take the following sentence: “Apple’s iPhone is currently one of the most popular smartphone”. When labeled by SUTime1 or HeidelTime2 , the adverb currently is correctly tagged with the PRESENT_REF value. However, if we change the sentence to “Apple’s iPhone 1 http://nlp.stanford.edu:8080/sutime/ process 2 http://heideltime.ifi.uni-heidelberg. de/heideltime/ 22 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 22–30, c Berlin, Germany, August 7-12, 2016. 2016 Associ"
K16-1003,S13-2015,0,0.0313903,"er a unigram or a synonym of any temporal word contained in the sentence/tweet and its value is the tf.idf. Note that word sense disambiguation is performed using the Lesk algorithm (Lesk, 1986). Amount of labeled data 100 200 400 600 800 1000 1264 (all) TWnH 59.8 62.6 65.5 67.4 67.9 68.0 68.4 5.4 Temporal Relation Annotation Finally, we focus on the problem of classifying temporal relations as proposed in TempEval3, assuming that the identification of events and timexes is already performed. In order to produce comparative results with the best-performing system at TempEval-3, namely UTTime (Laokulrat et al., 2013) for the above task, we follow the guidelines and use the same datasets provided by the organizers (UzZaman et al., 2013). In particular, we restrict our experiment to a subset of relations, namely BEFORE (past), AFTER (future), and INCLUDES (present), with all other relations mapped to the NA−RELATION for the following two subtasks: event to document creation time and event to same sentence event. This choice is motivated by the complexity of mapping the 14 relations of TempEval-3 into three temporal classes (past, present, future). As such, we test a simpler configuration of the original pro"
L16-1002,D11-1033,0,0.0199566,"how that BabelNet is helpful in improving translation performance of an SMT system. 3. As mentioned, BabelNet integrates many knowledge resources, so its entries come from many different domains. However, for practical use, we often use domain-specific data sets to build SMT systems, e.g. Newswire, Europarl (Koehn, 2005), DGT etc. In order to make good use of the knowledge in the BabelNet dictionary rather than simply adding it to the training data, we propose to use different domain adaptation methods, namely the entry-match strategy and Cross Entropy Difference (CED) (Moore and Lewis, 2010; Axelrod et al., 2011; Haque et al., 2014), to select in-domain entries. The entry match strategy uses a straightforward way to select in-domain entries, i.e. if either the source side or the target side of one entry-pair occurs in the initial training data, then this entry-pair will be selected. The advantage of this method is that it can increase the probability estimation of existing words. However, the disadvantage is that it cannot handle OOVs. We also use a domain-adaptation method which has a good performance record – CED – to facilitate entry selection from the bilingual dictionary. In this method, given a"
L16-1002,N06-1003,0,0.300119,"uent translations are produced. The problem is exacerbated when bilingual data are scarce, or if the text to be translated is not from the same domain as the training data. OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a very large network of semantic relations. These ar"
L16-1002,P11-2071,0,0.0230385,"ple, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a very large network of semantic relations. These are called Babel synsets, each of which represents a given meaning and contains all the synonyms which express that meaning in a range of different languages (Navigli and Ponzetto, 2010; Roberto Navigli, 2012; Navigli, 2012). The most recent version o"
L16-1002,D10-1041,1,0.926228,"OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a very large network of semantic relations. These are called Babel synsets, each of which represents a given meaning and contains all the synonyms which express that meaning in a range of different languages (Nav"
L16-1002,ehrmann-etal-2014-representing,0,0.0174578,"uage pairs show that domain adaptation can better utilize BabelNet knowledge and performs better than other methods. The results also demonstrate that BabelNet is a really useful tool for improving translation performance of SMT systems. Keywords: BabelNet, SMT, unknown words, OOVs, domain adaptation 1. geNet etc.1 BabelNet has been applied in many natural language processing tasks, such as multilingual lexicon extraction, crosslingual word-sense disambiguation, annotation, and information extraction, all with good performance (Elbedweihy et al., 2013; Jadidinejad, 2013; Navigli et al., 2013; Ehrmann et al., 2014; Moro et al., 2014). However, to the best of our knowledge, to date there is no comprehensive work applying BabelNet knowledge to SMT tasks. In this paper, we present three different strategies to utilize BabelNet resources in SMT systems, namely using direct training data, domain adaptation and OOV post-processing approaches. Specifically, the first strategy very straightforwardly appends the bilingual dictionary extracted from BabelNet to the initial training data, and then verifies the impact on translation performance; the second uses domainadaptation methods to select in-domain entries f"
L16-1002,W09-1117,0,0.0220751,"rse, in both cases, erroneous and disfluent translations are produced. The problem is exacerbated when bilingual data are scarce, or if the text to be translated is not from the same domain as the training data. OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a ver"
L16-1002,P08-2015,0,0.155215,"gether. Of course, in both cases, erroneous and disfluent translations are produced. The problem is exacerbated when bilingual data are scarce, or if the text to be translated is not from the same domain as the training data. OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and n"
L16-1002,P08-1088,0,0.0119874,"nd neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a very large network of semantic relations. These are called Babel synsets, each of which represents a given meaning and contains all the synonyms which express that meaning in a range of different languages (Navigli and Ponzetto, 2010; Roberto Navigli, 2012; Navigli,"
L16-1002,W14-4806,1,0.857435,"elpful in improving translation performance of an SMT system. 3. As mentioned, BabelNet integrates many knowledge resources, so its entries come from many different domains. However, for practical use, we often use domain-specific data sets to build SMT systems, e.g. Newswire, Europarl (Koehn, 2005), DGT etc. In order to make good use of the knowledge in the BabelNet dictionary rather than simply adding it to the training data, we propose to use different domain adaptation methods, namely the entry-match strategy and Cross Entropy Difference (CED) (Moore and Lewis, 2010; Axelrod et al., 2011; Haque et al., 2014), to select in-domain entries. The entry match strategy uses a straightforward way to select in-domain entries, i.e. if either the source side or the target side of one entry-pair occurs in the initial training data, then this entry-pair will be selected. The advantage of this method is that it can increase the probability estimation of existing words. However, the disadvantage is that it cannot handle OOVs. We also use a domain-adaptation method which has a good performance record – CED – to facilitate entry selection from the bilingual dictionary. In this method, given an indomain (ID) corpu"
L16-1002,P07-2045,0,0.00463242,"Table 2: Statistics of Europarl EN–PL data for the test set Chinese – Training Data #sen #word #entry 270,794 9,582,189 102,035 English – Training Data #sen #word #entry 270,794 10,319,019 81,036 Table 3: Statistics of FBIS ZH–EN data for model training Chinese – Test Set #sen #word #entry 1,082 30,489 5,684 English – Test Set (4 Refs) #sen #word #entry 1,082 142,794 7,552 Table 4: Statistics of FBIS ZH–EN data for the test set 4. 4.1. Experiments and Analysis • if East Asian characters are included in either the English or Polish side, we remove this pair; Experimental Settings We use Moses (Koehn et al., 2007) as the SMT system and configure the argument ‘–mark-unknown’ to mark up the OOVs in the translation. Experiments are conducted on English–Polish (EN–PL), English–Chinese (EN–ZH) and Chinese–English (ZH–EN) translation tasks. • if the English side contains symbols which are neither letters nor digits, then we remove this pair; 4.2. • if the English side is the same as the Polish side, then we remove this pair; • if either side contains punctuation, then we remove this pair; Experiments on Strategy 1: Direct Training Data (DTD) In this section, we verify the impact of Strategy 1 on system perfo"
L16-1002,W04-3250,0,0.248766,"Missing"
L16-1002,2005.mtsummit-papers.11,0,0.0143837,"me-based model. Different from the methods above, we utilize an extra semantic resource to handle the problem of OOVs. Specifically, we use BabelNet (i) as direct parallel data, or (ii) to retrieve the translations of OOVs via an API call. Experimental results on different language pairs show that BabelNet is helpful in improving translation performance of an SMT system. 3. As mentioned, BabelNet integrates many knowledge resources, so its entries come from many different domains. However, for practical use, we often use domain-specific data sets to build SMT systems, e.g. Newswire, Europarl (Koehn, 2005), DGT etc. In order to make good use of the knowledge in the BabelNet dictionary rather than simply adding it to the training data, we propose to use different domain adaptation methods, namely the entry-match strategy and Cross Entropy Difference (CED) (Moore and Lewis, 2010; Axelrod et al., 2011; Haque et al., 2014), to select in-domain entries. The entry match strategy uses a straightforward way to select in-domain entries, i.e. if either the source side or the target side of one entry-pair occurs in the initial training data, then this entry-pair will be selected. The advantage of this met"
L16-1002,C12-2062,0,0.059162,"scarce, or if the text to be translated is not from the same domain as the training data. OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a very large network of semantic relations. These are called Babel synsets, each of which represents a given meaning and contai"
L16-1002,Y13-1041,0,0.0396257,"to be translated is not from the same domain as the training data. OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a very large network of semantic relations. These are called Babel synsets, each of which represents a given meaning and contains all the synonyms"
L16-1002,D09-1040,0,0.125577,"as the training data. OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a very large network of semantic relations. These are called Babel synsets, each of which represents a given meaning and contains all the synonyms which express that meaning in a range of differ"
L16-1002,P10-2041,0,0.0295359,"ferent language pairs show that BabelNet is helpful in improving translation performance of an SMT system. 3. As mentioned, BabelNet integrates many knowledge resources, so its entries come from many different domains. However, for practical use, we often use domain-specific data sets to build SMT systems, e.g. Newswire, Europarl (Koehn, 2005), DGT etc. In order to make good use of the knowledge in the BabelNet dictionary rather than simply adding it to the training data, we propose to use different domain adaptation methods, namely the entry-match strategy and Cross Entropy Difference (CED) (Moore and Lewis, 2010; Axelrod et al., 2011; Haque et al., 2014), to select in-domain entries. The entry match strategy uses a straightforward way to select in-domain entries, i.e. if either the source side or the target side of one entry-pair occurs in the initial training data, then this entry-pair will be selected. The advantage of this method is that it can increase the probability estimation of existing words. However, the disadvantage is that it cannot handle OOVs. We also use a domain-adaptation method which has a good performance record – CED – to facilitate entry selection from the bilingual dictionary. I"
L16-1002,moro-etal-2014-annotating,0,0.0206669,"omain adaptation can better utilize BabelNet knowledge and performs better than other methods. The results also demonstrate that BabelNet is a really useful tool for improving translation performance of SMT systems. Keywords: BabelNet, SMT, unknown words, OOVs, domain adaptation 1. geNet etc.1 BabelNet has been applied in many natural language processing tasks, such as multilingual lexicon extraction, crosslingual word-sense disambiguation, annotation, and information extraction, all with good performance (Elbedweihy et al., 2013; Jadidinejad, 2013; Navigli et al., 2013; Ehrmann et al., 2014; Moro et al., 2014). However, to the best of our knowledge, to date there is no comprehensive work applying BabelNet knowledge to SMT tasks. In this paper, we present three different strategies to utilize BabelNet resources in SMT systems, namely using direct training data, domain adaptation and OOV post-processing approaches. Specifically, the first strategy very straightforwardly appends the bilingual dictionary extracted from BabelNet to the initial training data, and then verifies the impact on translation performance; the second uses domainadaptation methods to select in-domain entries from the extracted bi"
L16-1002,P10-1023,0,0.0327516,"10) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects concepts and named entities in a very large network of semantic relations. These are called Babel synsets, each of which represents a given meaning and contains all the synonyms which express that meaning in a range of different languages (Navigli and Ponzetto, 2010; Roberto Navigli, 2012; Navigli, 2012). The most recent version of BabelNet is 3.5 which integrates knowledge from WordNet, Wikipedia, Microsoft Terminology, Ima2. Related Work There has been a long line of research on handling OOVs in SMT. In this section, we briefly introduce some representative work in terms of the methods of processing OOVs. Lexicon-induction-based and morpho-syntactic methods are commonly used for handling unknown words by creating a bilingual lexicon for OOVs. By extending this work, 1 9 http://babelnet.org/about 3.2. Habash (2008) presents techniques for online treatme"
L16-1002,P12-3012,0,0.0209779,"ze this influence, we use bigrams to build the language models. 3.3. OOV Post-processing: BabelNet API There is a lot of noise in the extracted dictionary. For example, on the Chinese side of the English–Chinese dictionary, an entry might occur in Simplified or Traditional Chinese, or an entry might be segmented into words or characters. More importantly, there are many possible target terms for a given source term which come from different domains. In our experiments, using BabelNet bilingual entries as the training data does not perform well, so we propose to directly call the BabelNet API (Navigli and Ponzetto, 2012) to post-process OOVs in the translation of a source sentence. We use the BabelNet API to call the precompiled index bundle (version: 2.5.1)2 to retrieve the translation for an OOV. Specifically, an OOV in the source sentence is automatically marked in the output of the SMT decoder, and then we recognize this OOV and retrieve its 1-best candidate by calling the BabelNet API. Finally, we replace the OOV in the target side by the candidate translation. Strategies for Using BabelNet in SMT In this section, we introduce three different strategies to utilize BabelNet resources in SMT. 3.1. Domain A"
L16-1002,S13-2040,0,0.0398379,"Missing"
L16-1002,P02-1040,0,0.0945378,"Missing"
L16-1002,popovic-ney-2004-towards,0,0.0518349,"is’ on the target side; or (ii) to omit it altogether. Of course, in both cases, erroneous and disfluent translations are produced. The problem is exacerbated when bilingual data are scarce, or if the text to be translated is not from the same domain as the training data. OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million"
L16-1002,P06-1001,0,0.0269381,"or (ii) to omit it altogether. Of course, in both cases, erroneous and disfluent translations are produced. The problem is exacerbated when bilingual data are scarce, or if the text to be translated is not from the same domain as the training data. OOVs are often named entities, specialized terms and neologisms. For example, some person names or technical terms are phonetically transliterated from other languages. In the past, plenty of work has been done to alleviate the impact of OOVs, including orthographic (lexicon-inductionbased) and morphosyntactic preprocessing (Popovic and Ney, 2004; Sadat and Habash, 2006; Habash, 2008; Garera et al., 2009), pivot languages (Callison-Burch et al., 2006), grapheme-based model for phonetic transliterations (Lehal and Saini, 2012; Luo et al., 2013), paraphrases (Habash, 2008; Marton et al., 2009; Du et al., 2010) and contextbased semantic models (Haghighi et al., 2008; Daume-III and Jagarlamudi, 2011; Zhang et al., 2012). In this paper, we propose to use BabelNet to handle OOVs. BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic coverage of terms, as well as a semantic network comprising 14 million entries which connects"
L16-1002,2006.amta-papers.25,0,0.167614,"Missing"
L16-1003,W09-1904,0,0.0383851,"in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-goals that require human intervention in order to meet its high-quality output standards against upcoming challenges, including the large number of targeted languages, the fragmentary or weak SMT infrastructure support for the majority of the languages, and the multiple domains and text genres involved. The CrowdFlower 14 platform was chosen for the implementation of the crowdsourcing activities because of (a) its configurability, (b) its robust infrastructure, (c) its densely populated crowd channels and the evaluation and ranking process the"
L16-1003,2015.mtsummit-papers.19,1,0.892142,"i et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-g"
L16-1003,2005.mtsummit-papers.11,0,0.0298517,"onsist of (a) translation experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are ta"
L16-1003,P02-1040,0,0.0956189,"EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (millio"
L16-1003,P11-1138,0,0.0602764,"Missing"
L16-1003,steinberger-etal-2006-jrc,0,0.0272335,"experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are taken like  access control using quiz da"
L16-1003,tiedemann-2012-parallel,0,0.0663706,"known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality assurance measures are taken like  access control using quiz data that are far from straightforward to"
L16-1003,abdelali-etal-2014-amara,0,0.284209,"Missing"
L16-1003,D09-1030,0,0.252764,"Missing"
L16-1003,2012.eamt-1.60,0,0.0756545,"king field. The targeted crowds consist of (a) translation experts, (b) an internal group of workers with a known background in linguistics and/or translation, and (c) a group of external Table 1: Size of parallel data for all language pairs 4. Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev Tuned Mixed Tuned TraMOOC Dev TraMOOC Dev 25.5 27.9 34.1 36.5 34.1 35.9 Initial Translation Results For the initial TraMOOC prototypes we focused on three language pairs: EN-IT, EN-PT and EN-EL. Phrase-based models were trained on a large amount of parallel and monolingual data, including TED (Cettolo et al., 2012), Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), various OPUS corpora (Tiedemann, 2012), WMT News Commentary and Common Crawl 12, and SETimes 10 https://www.translectures.eu/web/project-summary/ http://videolectures.net/ 12 http://www.statmt.org/wmt15/translation-task.html 11 13 14 18 http://dumps.wikimedia.org (April 2015) www.crowdflower.com contributors from the platform's crowd channels. For the latter crowd category, apart from the standard channel evaluation processes applied by the platform to isolate spammers and contributors with poor language skills, further quality a"
L16-1003,N12-1047,0,0.0685374,"Missing"
L16-1003,N13-1001,0,0.0129109,"TraMOOC’s industrial partner, Iversity, are also included since student forums will also be automatically translated for the purposes of implicit translation evaluation. Language pair EN-DE EN-BG EN-PT EN-EL EN-NL EN-CZ EN-RU EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the highest possible general performance system (Huck et al., 20"
L16-1003,W10-0713,0,0.0292656,", and News to result in the highest possible general performance system (Huck et al., 2015). As expected, however, it is outperformed by using the domain-specific test set. System EN-EL EN-PT EN-IT TraMOOC Test 28.0 28.5 27.9 29.1 32.6 33.0 Table 2: BLEU scores for the initial translation prototypes 5. Size (million words) 2.7 1.5 4.8 2.4 1.3 1.5 1.4 0.2 1.7 2.3 8.7 Crowdsourcing Crowdsourcing has been employed extensively for the implementation of human intelligence natural language processing (NLP) tasks in recent years (Callison, 2009; Zaidan & Burch, 2011; Zbib et al., 2012; Ambati, 2012; Finin et al., 2010; Hsueh et al., 2009). TraMOOC involves crowdsourcing for realizing sub-goals that require human intervention in order to meet its high-quality output standards against upcoming challenges, including the large number of targeted languages, the fragmentary or weak SMT infrastructure support for the majority of the languages, and the multiple domains and text genres involved. The CrowdFlower 14 platform was chosen for the implementation of the crowdsourcing activities because of (a) its configurability, (b) its robust infrastructure, (c) its densely populated crowd channels and the evaluation an"
L16-1003,D08-1089,0,0.0114402,"ments, slides, and other course materials. The forum data of TraMOOC’s industrial partner, Iversity, are also included since student forums will also be automatically translated for the purposes of implicit translation evaluation. Language pair EN-DE EN-BG EN-PT EN-EL EN-NL EN-CZ EN-RU EN-CR EN-PL EN-IT EN-ZH (Tyers and Alperen, 2010). These were supplemented with monolingual Wikipedia corpora13 for all three target languages. The phrase-based models include many features which make them strong baselines. These models include standard features plus a hierarchical lexicalised reordering model (Galley & Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013), binary features indicating absolute occurrence count classes of phrase pairs, sparse phrase length features, and sparse lexical features for the top-200 words. The models were optimised to maximise BLEU (Papineni et al., 2002) with batch MIRA (Cherry & Foster, 2012) on 1000-best lists. In Table 1 we compare the BLEU score performance of the systems on the TraMOOC test sets, when tuned on a mixed domain tuning set, or with the TraMOOC tuning set. The mixed tuning set includes tuning sets from TED, Europarl, and News to result in the hi"
L16-1003,N12-1006,0,0.0606257,"Missing"
L16-1003,ambati-etal-2010-active,0,\N,Missing
L16-1153,2014.eamt-1.34,1,0.921802,"Missing"
L16-1153,W08-0509,0,0.0573643,"stics of MT training, development and test data available to build our systems. 4.2. Testing the Models For all of the different techniques used in this paper, the language model was built using the KenLM2 toolkit (Heafield, 2011) with Kneser-Ney smoothing (Kneser and Ney, 1995) and default backoff. For the SM T _cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). It is constructed as follows. First, word alignments in both directions are calculated, using GIZA++ (Gao and Vogel, 2008). Phrases are extracted using the default settings in the Moses toolkit. The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our different systems in order to test the impact of successive corrections using different models. Our different techniques do not always correct the same errors, so a combination can be beneficial. Two paths were pursued to explore the effect of the combination of the SM T _cor and LM _cor methods. In the first one, we begin by generating the corrected output of the LM _cor which is then processed"
L16-1153,W11-2123,0,0.0732123,"e French from the 17th century using the ABBYY OCR tool and manually corrected the text output in order to provide a correction reference for the evaluation. The statistics of all corpora used in our experiments can be seen in Table 1. 1 bitexts smt_17 smt_18 smt_19 dev17 test17 # OCR tokens 1.98 M 33.49 M 23.08 M 8638 1660 # ref tokens 1.96 M 33.40 M 22.9 M 8638 1660 Table 1: Statistics of MT training, development and test data available to build our systems. 4.2. Testing the Models For all of the different techniques used in this paper, the language model was built using the KenLM2 toolkit (Heafield, 2011) with Kneser-Ney smoothing (Kneser and Ney, 1995) and default backoff. For the SM T _cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). It is constructed as follows. First, word alignments in both directions are calculated, using GIZA++ (Gao and Vogel, 2008). Phrases are extracted using the default settings in the Moses toolkit. The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our different"
L16-1153,N03-1017,0,0.0164518,"s can be seen in Table 1. 1 bitexts smt_17 smt_18 smt_19 dev17 test17 # OCR tokens 1.98 M 33.49 M 23.08 M 8638 1660 # ref tokens 1.96 M 33.40 M 22.9 M 8638 1660 Table 1: Statistics of MT training, development and test data available to build our systems. 4.2. Testing the Models For all of the different techniques used in this paper, the language model was built using the KenLM2 toolkit (Heafield, 2011) with Kneser-Ney smoothing (Kneser and Ney, 1995) and default backoff. For the SM T _cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). It is constructed as follows. First, word alignments in both directions are calculated, using GIZA++ (Gao and Vogel, 2008). Phrases are extracted using the default settings in the Moses toolkit. The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our different systems in order to test the impact of successive corrections using different models. Our different techniques do not always correct the same errors, so a combination can be beneficial. Two paths were pursued to e"
L16-1153,P07-2045,0,0.0129777,"t_19 dev17 test17 # OCR tokens 1.98 M 33.49 M 23.08 M 8638 1660 # ref tokens 1.96 M 33.40 M 22.9 M 8638 1660 Table 1: Statistics of MT training, development and test data available to build our systems. 4.2. Testing the Models For all of the different techniques used in this paper, the language model was built using the KenLM2 toolkit (Heafield, 2011) with Kneser-Ney smoothing (Kneser and Ney, 1995) and default backoff. For the SM T _cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). It is constructed as follows. First, word alignments in both directions are calculated, using GIZA++ (Gao and Vogel, 2008). Phrases are extracted using the default settings in the Moses toolkit. The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our different systems in order to test the impact of successive corrections using different models. Our different techniques do not always correct the same errors, so a combination can be beneficial. Two paths were pursued to explore the effect of the combination of the SM T _co"
L16-1153,W06-1648,0,0.175074,"e, it still has various limitations that prevent it from being the perfect solution for OCR error correction (Hong, 1995). It requires a wide-ranging dictionary that covers every word in the language. Existing linguistic resources can usually target a single specific language in a given period, but cannot, therefore, support historical documents. The second type of approach in OCR post-processing is context-based error correction. Those techniques are founded on statistical language modelling and word ngrams. It aims at calculating the likelihood that a word sequence appears Tillenius (1996); Magdy and Darwish (2006). Applying this technique on historical documents might be challenging because work on building corpora for this kind of task (on old-style languages) has been very limited. Furthermore, when many consecutive corrupted words are encountered in a sentence, it is difficult to choose good candidate words. This is illustrated in FigStatistical Machine Translation method The idea centres on using an SMT system trained on OCR output texts which have been post-edited and manually corrected. SMT systems handle the translation process as the transformation of a sequence of symbols in a source language"
L16-1153,P03-1021,0,0.0374296,"g the KenLM2 toolkit (Heafield, 2011) with Kneser-Ney smoothing (Kneser and Ney, 1995) and default backoff. For the SM T _cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). It is constructed as follows. First, word alignments in both directions are calculated, using GIZA++ (Gao and Vogel, 2008). Phrases are extracted using the default settings in the Moses toolkit. The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our different systems in order to test the impact of successive corrections using different models. Our different techniques do not always correct the same errors, so a combination can be beneficial. Two paths were pursued to explore the effect of the combination of the SM T _cor and LM _cor methods. In the first one, we begin by generating the corrected output of the LM _cor which is then processed by the SM T _cor system. We call this combination LM _cor +SM T _cor. In the second one, we reverse the use of the two systems and we call this combination SM T _cor + LM _cor. 4.3. R"
L16-1153,P02-1038,0,0.068549,"n is (1): t∗ = arg max P r(t|s) = arg max P r(s|t)P r(t) t t (1) It can be decomposed, as in the original work of (Brown et al., 1993), into a language model probability P r(t), and a translation model probability P r(s|t). The language model is trained on a large quantity of correct French data, and the translation model is trained using a bilingual text aligned at sentence (segment) level, i.e. an OCR output for a segment and its ground-truth obtained manually, so-called bitexts. As in most current state-of-the-art systems, the translation probability is modelled using the log-linear model (Och and Ney, 2002) in (2): P (t|s) = N X λi hi (s, t) (2) i=0 where hi (s, t) is the ith feature function and λi its weight (determined by an optimization process). We call this method ""SMT_cor "" in the rest of this paper. We used only Word-based model for this task because Afli et al. (2015) demonstrated in a similar work that it improves the results better than the character-based model. In the special case of OCR correction, the source and target languages are the same (French in this case) which should not require the use of a reordering model. 3.3. Language Modelling Language Modelling is a flexible method"
L16-1153,W12-3212,0,0.328237,"Missing"
L16-1352,W14-3302,0,0.0356516,"Missing"
L16-1352,N06-1003,0,0.0246612,"tors easily visualise the relations between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and const"
L16-1352,P05-1033,0,0.495203,"rt” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive performance. However, MT systems which make use of SCFG tend to generate an enormous phrase table containing many erroneous rules. This huge search space not only leads to unreliable output, but also restricts the input sentence length that the system can handle. Other treebased SMT models such as Liu et al. (2006) and Shen et al. (2008) depend heavily on the accuracy of the parsing algorithm which introduces noise upstream to the MT system. Our method, ProphetMT, allows monolin"
L16-1352,D10-1041,1,0.800129,"ns between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and constraints. Sentences in"
L16-1352,2003.mtsummit-papers.18,1,0.692067,"e to further improve the productivity of post-editors by explicitly featuring the relations between the source and target rules. Keywords: Controlled Language, Authoring Tool, Post-Editing, Statistical Machine Translation 1. Introduction A Controlled Language (CL) can be defined as “an explicitly defined restriction of a natural language that specifies constraints on lexicon, grammar, and style” (Huijsen, 1998). CL is widely used in professional writing where the aim is to write for a certain standard and style demanded by a particular profession, such as law, medicine, patent, technique etc (Gough and Way, 2003; Gough and Way, 2004). For multilingual documents, CL has been shown to improve the quality of the translation output, whether the translation is done by humans or machines (Nyberg et al., 2003). The advantages of applying CL are self-evident: clear and consistent composition guidelines as well as less ambiguity in translation. However, the problems are also obvious: designing the rules usually requires human linguists, and rules may be difficult for end-users to grasp. In addition, the sentences that can be generated are often limited in length and complexity (O’Brien, 2003). To the best of"
L16-1352,2004.eamt-1.9,1,0.708592,"the productivity of post-editors by explicitly featuring the relations between the source and target rules. Keywords: Controlled Language, Authoring Tool, Post-Editing, Statistical Machine Translation 1. Introduction A Controlled Language (CL) can be defined as “an explicitly defined restriction of a natural language that specifies constraints on lexicon, grammar, and style” (Huijsen, 1998). CL is widely used in professional writing where the aim is to write for a certain standard and style demanded by a particular profession, such as law, medicine, patent, technique etc (Gough and Way, 2003; Gough and Way, 2004). For multilingual documents, CL has been shown to improve the quality of the translation output, whether the translation is done by humans or machines (Nyberg et al., 2003). The advantages of applying CL are self-evident: clear and consistent composition guidelines as well as less ambiguity in translation. However, the problems are also obvious: designing the rules usually requires human linguists, and rules may be difficult for end-users to grasp. In addition, the sentences that can be generated are often limited in length and complexity (O’Brien, 2003). To the best of our knowledge, most of"
L16-1352,J07-1006,0,0.027759,"les usually requires human linguists, and rules may be difficult for end-users to grasp. In addition, the sentences that can be generated are often limited in length and complexity (O’Brien, 2003). To the best of our knowledge, most of the existing computer-aided authoring methods (Acrolinx, for example) employ a kind of interactive paradigm with a CL together with a grammar checker which provides user feedback. Users have to follow the ‘compose, check, revise’ loop until the sentence is consistent according to a given parser. Exceptions usually follow an approach called conceptual authoring (Hallett et al., 2007; Hart et al., 2008) where texts are created by short cycles of language generation and user-triggered modification actions. Obviously, these methods severely restrict a user’s expressiveness in the authoring process. Even with the help of CL, current state-of-the-art machine translation (MT) methods still fail to produce reliable outputs. For example, for English-to-Chinese translation, given the sentence “allows the client computers that connect through a token ring adapter to access the network”, it is very hard for computers to figure out: (i) the skeleton of this sentence is ‘allow someth"
L16-1352,D09-1127,1,0.81682,"s use of auto-suggestion both for syntactic templates and for terms. A shift-reduce-like (Aho, 2003) authoring interface, which allows users to easily parse the “already composed part” of the sentence, is also applied to maintain the structural correctness and unambiguous parsing while the source sentence is being composed. 3. 3.1. The main components are: A: the source-side auto-suggestion server, which stores the source-side rules obtained from the HPB server (component C) sorted according to their occurrence. B: the main UI for users to compose text. Note that we employ the ‘shift-reduce’ (Huang et al., 2009) manner to ensure the ‘left-to-right’ (or ‘right-to-left’, for Arabic) writing style which is more natural. C: modified HPB Moses server, with two main modifications: Firstly, the decoder is constrained to use the structural metadata provided by users, i.e. the parse tree is automatically constructed when the user composes the source sentence. Note that the gluegrammar (Chiang, 2005) is applied when the decoder is incapable of deriving a sound parse tree, which in our case is not an option. Therefore, the use of the glue-grammar is also prevented (i.e. the glue-grammar can be applied only at t"
L16-1352,P07-2045,0,0.00860239,"ce pairs and the test set has 943 sentence pairs. From the test set, we randomly select 150 sentences for our evaluation. Our baseline is a standard HPB-SMT model with all the default settings: maximum 2 non-terminals; maximum 5 tokens for each rule; max-chart span is 10; etc. As for ProphetMT, we modify the decoder as described in Figure 2 (C). We use the GIZA++ (Och and Ney, 2003) implementation of IBM word alignment 4, 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke and Laboratory, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) to decode. 1 https://code.google.com/p/berkeleyparser/ http://www.statmt.org/moses/ 3 http://www.cis.upenn.edu/ ccb/ppdb/ 2 Writing instructions The ultimate goal of ProphetMT is to allow users to easily compose sentences in a subset of language that is ‘understood’ by the computer. Ideally this can be conducted by either letting different users describe the same picture or to paraphrase the same sentence. For simplicity, our experiment only requires users to rewrite the sentences in the test set using ProphetMT. According to Wang et al. (2007), which systematically investigates English-Chine"
L16-1352,2014.amta-researchers.19,1,0.750394,"decoder, the normal restrictions of tree-based models, such as the maximum span (which is <= 20) and the NT numbers (which is usually <= 2), can be removed. 4.4. Paraphrase Auto-suggestions If the user inputs an OOV, a paraphrase engine will be queried to try to suggest terms within the current SMT model. Paraphrases are obtained from PPDB3 . If the OOV is not found in PPDB, then the user will be forced to choose another word. 5. Experiments This section describes the preliminary experiments conducted. 5.1. Experimental Settings Our raw data set is the English-to-Chinese translation memory in Li et al. (2014), consisting of 86k sentence pairs. The average sentence length of the training set are 13.2 and 13.5 for English and Chinese, respectively. The development set has 762 sentence pairs and the test set has 943 sentence pairs. From the test set, we randomly select 150 sentences for our evaluation. Our baseline is a standard HPB-SMT model with all the default settings: maximum 2 non-terminals; maximum 5 tokens for each rule; max-chart span is 10; etc. As for ProphetMT, we modify the decoder as described in Figure 2 (C). We use the GIZA++ (Och and Ney, 2003) implementation of IBM word alignment 4,"
L16-1352,P06-1077,1,0.756691,"vertheless, it lacks syntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive performance. However, MT systems which make use of SCFG tend to generate an enormous phrase table containing many erroneous rules. This huge search space not only leads to unreliable output, but also restricts the input sentence length that the system can handle. Other treebased SMT models such as Liu et al. (2006) and Shen et al. (2008) depend heavily on the accuracy of the parsing algorithm which introduces noise upstream to the MT system. Our method, ProphetMT, allows monolingual users to easily and naturally write correct in-domain sentences while also providing the structural metadata needed to make the parsing of the sentence unambiguous. The set of structural templates is provided by the tree-based MT system itself, meaning that highly reliable MT results can be generated directly from the user’s composition. Syntactic annotation is a tedious task which has traditionally required specialised trai"
L16-1352,P13-4015,0,0.01856,"r translations. Finally, ProphetMT also employs a useful colour scheme to help post-editors easily visualise the relations between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an"
L16-1352,1999.mtsummit-1.8,0,0.152233,"A user-friendly interface that facilitates the composition of text which is suitable (with respect to both terminal and non-terminal phrases) for an existing treebased MT model; 3. Allows users to easily attach structural metadata while authoring; 4. The metadata can help the MT decoder to find better translations. Finally, ProphetMT also employs a useful colour scheme to help post-editors easily visualise the relations between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring T"
L16-1352,J03-1002,0,0.00501789,"English-to-Chinese translation memory in Li et al. (2014), consisting of 86k sentence pairs. The average sentence length of the training set are 13.2 and 13.5 for English and Chinese, respectively. The development set has 762 sentence pairs and the test set has 943 sentence pairs. From the test set, we randomly select 150 sentences for our evaluation. Our baseline is a standard HPB-SMT model with all the default settings: maximum 2 non-terminals; maximum 5 tokens for each rule; max-chart span is 10; etc. As for ProphetMT, we modify the decoder as described in Figure 2 (C). We use the GIZA++ (Och and Ney, 2003) implementation of IBM word alignment 4, 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke and Laboratory, 2002) on the Chinese side of the training data, and Moses (Koehn et al., 2007) to decode. 1 https://code.google.com/p/berkeleyparser/ http://www.statmt.org/moses/ 3 http://www.cis.upenn.edu/ ccb/ppdb/ 2 Writing instructions The ultimate goal of ProphetMT is to allow users to easily compose sentences in a subset of language that is ‘understood’ by the computer. Ideally this can be conducted by either letting different users describe the same"
L16-1352,P03-1021,0,0.0460033,"d Translatability. 4.2. Non-Terminal Rule (NTR) Auto-suggestions In order to extract NTRs which are meaningful to humans, we parse the source side of the training corpus with the Berkley Parser.1 Then we wrap the parsed result with the xml for Moses2 hierarchical phrase-based model to extract rules. The ranking of NTR suggestions follows the same methodology that was employed for phrase suggestions. The automatic evaluation metric we employ is character based 5-gram BLEU (Papineni et al., 2002), which is one of the standards in Chinese-as-target evaluation.4 After minimum-error-rate training (Och, 2003), the baseline obtains 49.0 BLEU score on the test set. Note that while the BLEU score might be viewed as rather high for this language pair, it is actually quite typical of the scores seen when using TMs from industry, which show much more repetition than training data used in most ‘academic’ MT papers. 5.2. 4.3. Filtering To further reduce the amount of rules, the NTRs containing content words like nouns, pronouns and numbers can be removed. This filtering is based on the observation that the structure of a sentence is primarily dictated by function words as well as verbs. The phrase-level a"
L16-1352,P02-1040,0,0.0953931,"inal rank of the proposed phrases is based on the minimization of the Semantic Distance and maiximization of the Fluency and Translatability. 4.2. Non-Terminal Rule (NTR) Auto-suggestions In order to extract NTRs which are meaningful to humans, we parse the source side of the training corpus with the Berkley Parser.1 Then we wrap the parsed result with the xml for Moses2 hierarchical phrase-based model to extract rules. The ranking of NTR suggestions follows the same methodology that was employed for phrase suggestions. The automatic evaluation metric we employ is character based 5-gram BLEU (Papineni et al., 2002), which is one of the standards in Chinese-as-target evaluation.4 After minimum-error-rate training (Och, 2003), the baseline obtains 49.0 BLEU score on the test set. Note that while the BLEU score might be viewed as rather high for this language pair, it is actually quite typical of the scores seen when using TMs from industry, which show much more repetition than training data used in most ‘academic’ MT papers. 5.2. 4.3. Filtering To further reduce the amount of rules, the NTRs containing content words like nouns, pronouns and numbers can be removed. This filtering is based on the observatio"
L16-1352,2003.eamt-1.13,0,0.0642068,"phrases) for an existing treebased MT model; 3. Allows users to easily attach structural metadata while authoring; 4. The metadata can help the MT decoder to find better translations. Finally, ProphetMT also employs a useful colour scheme to help post-editors easily visualise the relations between the source and target rules. 2. Related Work All existing computer-aided authoring tools within a translation context employ a kind of interactive paradigm with a CL. Mitamura (1999) allows users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be c"
L16-1352,P08-1066,0,0.0248572,"yntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive performance. However, MT systems which make use of SCFG tend to generate an enormous phrase table containing many erroneous rules. This huge search space not only leads to unreliable output, but also restricts the input sentence length that the system can handle. Other treebased SMT models such as Liu et al. (2006) and Shen et al. (2008) depend heavily on the accuracy of the parsing algorithm which introduces noise upstream to the MT system. Our method, ProphetMT, allows monolingual users to easily and naturally write correct in-domain sentences while also providing the structural metadata needed to make the parsing of the sentence unambiguous. The set of structural templates is provided by the tree-based MT system itself, meaning that highly reliable MT results can be generated directly from the user’s composition. Syntactic annotation is a tedious task which has traditionally required specialised training. In order to maint"
L16-1352,C12-3058,0,0.0882714,"users to compose from scratch, and discusses the issues in designing a CL for rule based machine translation. Power et al. (2003) describe a CL authoring tool for multilingual generation. Marti et al. (2010) present a rule-based rewriting tool which performs syntactic analysis. Mirkin et al. (2013) introduce a confidence-driven rewriting tool which is inspired by Callison-Burch et al. (2006) and Du et al. (2010) 2214 that paraphrases the out-of-vocabulary words (OOV) or the “hard-to-translate-part” of the source side in order to improve SMT performance. Figure 1: SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive"
L16-1352,D07-1077,0,0.0157802,"e Chinese side of the training data, and Moses (Koehn et al., 2007) to decode. 1 https://code.google.com/p/berkeleyparser/ http://www.statmt.org/moses/ 3 http://www.cis.upenn.edu/ ccb/ppdb/ 2 Writing instructions The ultimate goal of ProphetMT is to allow users to easily compose sentences in a subset of language that is ‘understood’ by the computer. Ideally this can be conducted by either letting different users describe the same picture or to paraphrase the same sentence. For simplicity, our experiment only requires users to rewrite the sentences in the test set using ProphetMT. According to Wang et al. (2007), which systematically investigates English-Chinese reordering, we define the writing instructions as follows (note: words in parentheses are the expanded non-terminals): 1. verbs must be used in non-terminal rules. 2. prefer the longest phrase that composes a constituent, e.g. in Figure 3, “the firewall”, “the client computer” are two phrases that act as a noun; “(the firewall) is not running” is preferable to “(the firewall) is not (running)” or “(the firewall) is (not (running))”. 3. prefer a noun phrase attacheing its preposition at the right adjacent place, e.g. in Figure 3, “outbound tra"
L16-1352,J97-3002,0,0.169314,": SMT-driven Authoring Tool by Venkatapathy and Mirkin (2012) To the best of our knowledge, Venkatapathy and Mirkin (2012) is the first interface that could be called an SMTdriven CL authoring tool, shown in Figure 1. Their tool provides users with the word, phrase, even sentence-level auto-suggestions which are obtained from an existing translation model. Nevertheless, it lacks syntactically-informed suggestions and constraints. Sentences in all languages contain recursive structure. Synchronous context-free grammars (SCFG) (Chiang, 2005) and stochastic inversion transduction grammars (ITG) (Wu, 1997) have been widely used in SMT and achieve impressive performance. However, MT systems which make use of SCFG tend to generate an enormous phrase table containing many erroneous rules. This huge search space not only leads to unreliable output, but also restricts the input sentence length that the system can handle. Other treebased SMT models such as Liu et al. (2006) and Shen et al. (2008) depend heavily on the accuracy of the parsing algorithm which introduces noise upstream to the MT system. Our method, ProphetMT, allows monolingual users to easily and naturally write correct in-domain sente"
L16-1436,P12-2040,0,0.0758794,"consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Bu"
L16-1436,W11-0609,0,0.0490398,"2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing."
L16-1436,itamar-itai-2008-using,0,0.207421,"MT) of conversational material by exploiting their internal structure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discou"
L16-1436,P07-2045,0,0.0127474,"Missing"
L16-1436,matsubara-etal-2002-bilingual,0,0.0781911,"Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged the existence of bilingual subtitles as a source of parallel data for the Chinese-English language pair to improve the MT systems in the movie domain. However, their work only considers sentence-level data instead of extracting more useful information for dialogues. Besides, Japanese researchers constructed a speech dialogue corpus for a machine interpretation system (Aizawa et al., 2000; Matsubara et al., 2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ consecutive/simultaneous interpreting in the booth. The German VERBMOBIL speech-to-speech translation programme (Wahlster, 2013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Dane"
L16-1436,W12-0117,0,0.0160222,"ed Work In the specific case of dialogue MT system, data acquisition can impose challenges including data scarcity, translation quality and scalability. The release of the Penn Discourse Treebank (PDTB)2 (Prasad et al., 2008) helped bring about 1 We release our DCU English-Chinese Dialogue Corpus in http://computing.dcu.ie/˜lwang/resource. html. 2 Available at https://www.seas.upenn.edu/˜pdtb. 2748 a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on PDTB, some have applied the insights to MT (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue corpus construction. One is parallel corpora construction for dialogue or conversation MT (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Tiedemann, 2008; Itamar and Itai, 2008; Xiao and Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged"
L16-1436,J03-1002,0,0.00968861,"Missing"
L16-1436,P03-1021,0,0.0215298,"Missing"
L16-1436,prasad-etal-2008-penn,0,0.0125033,"ue MT system. The rest of the paper is organized as follows. In Section 2, we describe related work. Section 3 describes in detail our approaches to build a dialogue corpus as well as the structure of the generated database. The experimental results for both corpus annotation and translation are reported in Section 4. Finally, Section 5 presents our conclusions and future work plans. 2. Related Work In the specific case of dialogue MT system, data acquisition can impose challenges including data scarcity, translation quality and scalability. The release of the Penn Discourse Treebank (PDTB)2 (Prasad et al., 2008) helped bring about 1 We release our DCU English-Chinese Dialogue Corpus in http://computing.dcu.ie/˜lwang/resource. html. 2 Available at https://www.seas.upenn.edu/˜pdtb. 2748 a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on PDTB, some have applied the insights to MT (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue co"
L16-1436,schmitt-etal-2012-parameterized,0,0.0221098,"g three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Building A Parallel Dialogue Corpus 4 (1) given a monolingual movie/episode script, we identify dialogue boundaries and speaker tags using clues such as format and story structure tags in the script; (2) for a bilingual subtitle, we align each sentence with its tra"
L16-1436,tiedemann-2008-synchronizing,0,0.197336,"aterial by exploiting their internal structure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discourse corpus by bri"
L16-1436,tiedemann-2012-parallel,0,0.140544,"tructure. This lack of research on the dialogue MT is a surprising fact, since dialogue exhibits more cohesiveness than single sentence and at least as much than textual discourse. Although there are a number of papers on corpus construction for various natural language processing (NLP) tasks, dialogue corpora are still scarce for MT. Some work regarding bilingual subtitles as parallel corpora exists, but it lacks rich information between utterances (sentence-level corpus) (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Itamar and Itai, 2008; Tiedemann, 2008; Xiao and Wang, 2009; Tiedemann, 2012; Zhang et al., 2014). Other work focuses on mining the internal structure in dialogue data from movie scripts. However, these are monolingual data which cannot used for MT (DanescuNiculescu-Mizil and Lee, 2011; Banchs, 2012; Walker et al., 2012; Schmitt et al., 2012). In general, the fact is that bilingual subtitles are ideal resources to extract parallel sentence-level utterances, and movie scripts contain rich information such as dialogue boundaries and speaker tags. Inspired by these facts, our initial idea was to build dialogue discourse corpus by bridging the information in these two kin"
L16-1436,walker-etal-2012-annotated,0,0.171098,"013) also collected and transcribed task-oriented dialogue data. This related work focused on speech-to-speech translation including three modules of automatic speech recognition (ASR), MT and textto-speech(TTS). The other one is mining rich information from other resources such as movie scripts. Danescu-Niculescu-Mizil and Lee (2011) created a conversation corpus containing large metadata-rich collections of fictional conversations extracted from raw movie scripts. Both Banchs (2012) and CMU released dialogue corpora extracted from the Internet Movie Script Database (IMSDb).3 Based on IMSDb, Walker et al. (2012) annotated 862 film scripts to learn and characterize the character style for an interactive story system, and Schmitt et al. (2012) annotated 347 dialogues to explore a spoken dialogue system. The resource of movie scripts, such as IMSDb, is good enough to generate conversational discourse for dialogue processing. However, monolingual movie scripts are not enough for MT which needs a large-scale bilingual dialogue corpus to train and tune translation models. 3. Building A Parallel Dialogue Corpus 4 (1) given a monolingual movie/episode script, we identify dialogue boundaries and speaker tags"
L16-1436,O12-1015,1,0.894007,"change is made to accommodate the size of the TV screen. It is a big challenge to deal with these changed, missing or duplicated terms during matching. All the above problems make the task a complex N to-N matching where N ≥ 0. ith utterance Di in the script is represented as a vector Di = [w1,i , w2,i , ...wk,i ], in which k is the size of the term vocabulary. Many similarity functions can be employed to calculate the similarity between two utterance vectors (Cha, 2007). Here we apply the cosine distance: sim(di , dj ) = N X k=1 Therefore, we regard the matching and projection as an IR task (Wang et al., 2012a). The Vector Space Model (VSM) (Salton et al., 1975) is a state-of-the-art IR model in which each document is represented as a vector of identifiers (here we describe each identifier as a term). The wi,k · wj,k v uN uX t w i,k k=1 v uN uX ·t w j,k (1) k=1 where N is the number of terms in an utterance vector, and wi,k and wj,k represent the weight of the ith/jth term in the utterance Di /Dj respectively. Technically, the distance between documents in VSM is calculated by comparing the 2750 Item Total number of scripts processed Total number of dialogues Total number of speakers Total number"
L16-1436,W12-6310,1,0.929649,"change is made to accommodate the size of the TV screen. It is a big challenge to deal with these changed, missing or duplicated terms during matching. All the above problems make the task a complex N to-N matching where N ≥ 0. ith utterance Di in the script is represented as a vector Di = [w1,i , w2,i , ...wk,i ], in which k is the size of the term vocabulary. Many similarity functions can be employed to calculate the similarity between two utterance vectors (Cha, 2007). Here we apply the cosine distance: sim(di , dj ) = N X k=1 Therefore, we regard the matching and projection as an IR task (Wang et al., 2012a). The Vector Space Model (VSM) (Salton et al., 1975) is a state-of-the-art IR model in which each document is represented as a vector of identifiers (here we describe each identifier as a term). The wi,k · wj,k v uN uX t w i,k k=1 v uN uX ·t w j,k (1) k=1 where N is the number of terms in an utterance vector, and wi,k and wj,k represent the weight of the ith/jth term in the utterance Di /Dj respectively. Technically, the distance between documents in VSM is calculated by comparing the 2750 Item Total number of scripts processed Total number of dialogues Total number of speakers Total number"
L16-1436,W14-3331,1,0.893281,"Missing"
L16-1436,zhang-etal-2014-dual,0,0.0684567,"eyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in dialogue or conversation as well. There are two directions of work related to dialogue corpus construction. One is parallel corpora construction for dialogue or conversation MT (Lavecchia et al., 2007; Tiedemann, 2007a; Tiedemann, 2007b; Tiedemann, 2008; Itamar and Itai, 2008; Xiao and Wang, 2009; Tiedemann, 2012). Thanks to the effects of crowdsourcing and fan translation in audiovisual translation (O’Hagan, 2012), we can regard subtitles as parallel corpora. Zhang et al. (2014) leveraged the existence of bilingual subtitles as a source of parallel data for the Chinese-English language pair to improve the MT systems in the movie domain. However, their work only considers sentence-level data instead of extracting more useful information for dialogues. Besides, Japanese researchers constructed a speech dialogue corpus for a machine interpretation system (Aizawa et al., 2000; Matsubara et al., 2002; Ryu et al., 2003; Takezawa, 2003). They collected speech dialogue corpora for machine interpretation research via recording and transcribing Japanese/English interpreters’ c"
L18-1422,W14-3348,0,0.0828275,"Missing"
L18-1422,W13-1109,0,0.818739,"example of our recent work on sentiment translation system using a the sentiment classification approach (Lohar et al. (2017)). Finally, we conclude and point out some possible future work in Section 6. 2. Related work Parallel data for Twitter is scarcely available on the Internet. One of the available corpora is “microtopia”, a parallel corpus of microblogs (Ling et al. (2014)). Recently, TweetMT (Vicente et al. (2016)) has been introduced as a parallel corpus of tweets in four language pairs that combine five languages (Spanish from/to Basque, Catalan, Galician and Portuguese). Ling et al. (2013) present a framework to crawl parallel data from microblogs in order to find parallel resources from single posts, with translations of the same sentence in two languages. Hajjem et al. (2013) create an Arabic–French comparable corpus, the first comparable corpus collected from Twitter. Despite this apparent lack of data, some work has been carried out in the area of tweet translation. Kaufmann and Kalita (2010) combine a statistical machine translation (SMT) system with a preprocessor and successfully remove the majority of noise from a tweet, which results in increasing its readability in th"
L18-1422,2012.amta-commercial.8,1,0.920064,"he area of tweet translation. Kaufmann and Kalita (2010) combine a statistical machine translation (SMT) system with a preprocessor and successfully remove the majority of noise from a tweet, which results in increasing its readability in the target lanrecently expanded to 280 2666 guage. The work in Gotti et al. (2013) reports experimental results obtained from translating Twitter feeds published by agencies and organizations, using an SMT system. They mine parallel web pages linked from the URLs contained in English–French pairs of tweets in order to create the tuning and training material. Jiang et al. (2012) propose strategies to handle shortforms, acronyms, typos, punctuation errors, non-dictionary slang, wordplay, censor avoidance and emoticons. 3. Translation guidelines This section describes the main guidelines we followed during the manual translation process. As Tweets may contain shortforms, typos, wordplays etc., all of which are often deliberately introduced especially due to the character limitation. Such characteristics of tweets pose challenges in translations into another language. We therefore place an emphasis on the following three strategies while translating the tweets: (i) info"
L18-1422,W14-3356,0,0.0175757,"his field. In Section 3, we discuss some translation guidelines followed during the corpus development. The sentiment-annotation procedure is explained in Section 4. In Section 5, we briefly discuss the usefulness of this corpus with an example of our recent work on sentiment translation system using a the sentiment classification approach (Lohar et al. (2017)). Finally, we conclude and point out some possible future work in Section 6. 2. Related work Parallel data for Twitter is scarcely available on the Internet. One of the available corpora is “microtopia”, a parallel corpus of microblogs (Ling et al. (2014)). Recently, TweetMT (Vicente et al. (2016)) has been introduced as a parallel corpus of tweets in four language pairs that combine five languages (Spanish from/to Basque, Catalan, Galician and Portuguese). Ling et al. (2013) present a framework to crawl parallel data from microblogs in order to find parallel resources from single posts, with translations of the same sentence in two languages. Hajjem et al. (2013) create an Arabic–French comparable corpus, the first comparable corpus collected from Twitter. Despite this apparent lack of data, some work has been carried out in the area of tweet"
L18-1422,P03-1021,0,0.059359,"Experiments As the corpus is small in size, a very small subset of 50 tweets per sentiment (negative, neutral and positive) is held out for tuning and testing purposes in order to maintain as large an amount as possible for training purpose. The remaining 3, 700 tweet pairs are considered as the training data and are similarly divided into negative, neutral and positive tweet pairs. The translation models are built using the Moses SMT tool (Koehn et al. (2007)) using Giza++ (Och and Ney (2003)) for word and phrase alignment. Afterwards, the models are tuned using minimum error rate training (Och (2003)). The additional resources used are English–German parallel Flickr data2 and “NewsCommentary (News)” data3 in order to build larger MT engines. The evaluation process consists of two different types of measurements: (i) MT quality and (ii) sentiment preservation. For MT evaluation, the automatic evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. 2 http://www.statmt.org/wmt16/multimodaltask.html#task1 3 http://data.statmt.org/wmt16/translationtask/training-parallel-nc-v11.tgz 2668 Translation model Twitter Twitter (Baseline) Twitter +"
L18-1422,J03-1002,0,0.00820811,"er to be able to use the sentiment analysis tool in English especially designed for tweets (Afli et al. (2017)). 5.1.1. Experiments As the corpus is small in size, a very small subset of 50 tweets per sentiment (negative, neutral and positive) is held out for tuning and testing purposes in order to maintain as large an amount as possible for training purpose. The remaining 3, 700 tweet pairs are considered as the training data and are similarly divided into negative, neutral and positive tweet pairs. The translation models are built using the Moses SMT tool (Koehn et al. (2007)) using Giza++ (Och and Ney (2003)) for word and phrase alignment. Afterwards, the models are tuned using minimum error rate training (Och (2003)). The additional resources used are English–German parallel Flickr data2 and “NewsCommentary (News)” data3 in order to build larger MT engines. The evaluation process consists of two different types of measurements: (i) MT quality and (ii) sentiment preservation. For MT evaluation, the automatic evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. 2 http://www.statmt.org/wmt16/multimodaltask.html#task1 3 http://data.statmt.org/"
L18-1422,P02-1040,0,0.100282,"Missing"
L18-1422,2006.amta-papers.25,0,0.082055,"Missing"
L18-1422,L16-1469,0,0.733599,"Missing"
L18-1422,P07-2045,0,0.0229953,"e German tweets into English in order to be able to use the sentiment analysis tool in English especially designed for tweets (Afli et al. (2017)). 5.1.1. Experiments As the corpus is small in size, a very small subset of 50 tweets per sentiment (negative, neutral and positive) is held out for tuning and testing purposes in order to maintain as large an amount as possible for training purpose. The remaining 3, 700 tweet pairs are considered as the training data and are similarly divided into negative, neutral and positive tweet pairs. The translation models are built using the Moses SMT tool (Koehn et al. (2007)) using Giza++ (Och and Ney (2003)) for word and phrase alignment. Afterwards, the models are tuned using minimum error rate training (Och (2003)). The additional resources used are English–German parallel Flickr data2 and “NewsCommentary (News)” data3 in order to build larger MT engines. The evaluation process consists of two different types of measurements: (i) MT quality and (ii) sentiment preservation. For MT evaluation, the automatic evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. 2 http://www.statmt.org/wmt16/multimodaltask.htm"
L18-1422,P13-1018,0,0.0637854,"pus with an example of our recent work on sentiment translation system using a the sentiment classification approach (Lohar et al. (2017)). Finally, we conclude and point out some possible future work in Section 6. 2. Related work Parallel data for Twitter is scarcely available on the Internet. One of the available corpora is “microtopia”, a parallel corpus of microblogs (Ling et al. (2014)). Recently, TweetMT (Vicente et al. (2016)) has been introduced as a parallel corpus of tweets in four language pairs that combine five languages (Spanish from/to Basque, Catalan, Galician and Portuguese). Ling et al. (2013) present a framework to crawl parallel data from microblogs in order to find parallel resources from single posts, with translations of the same sentence in two languages. Hajjem et al. (2013) create an Arabic–French comparable corpus, the first comparable corpus collected from Twitter. Despite this apparent lack of data, some work has been carried out in the area of tweet translation. Kaufmann and Kalita (2010) combine a statistical machine translation (SMT) system with a preprocessor and successfully remove the majority of noise from a tweet, which results in increasing its readability in th"
N16-1113,P11-2037,0,0.0300195,"Missing"
N16-1113,D13-1135,0,0.169061,"Missing"
N16-1113,D10-1062,0,0.769955,"hat there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our"
N16-1113,P07-2045,0,0.0104744,"ing the approach described in Section 2.1. There are two different language models for the DP annotation (Section 2.1) and translation tasks, respectively: one is trained on the 2.13TB Chinese Web Page Collection Corpus5 while the other one is trained on all extracted 7M English subtitle data (Wang et al., 2016). Corpus Lang. Sentents Train Dev Test ZH EN ZH EN ZH EN 1,037,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5,"
N16-1113,D10-1086,0,0.132,"Missing"
N16-1113,W10-1737,0,0.380169,"Missing"
N16-1113,D09-1106,1,0.87907,"Missing"
N16-1113,P13-2064,1,0.842545,"missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our task is that ZP cont"
N16-1113,C14-1003,0,0.0608436,"Missing"
N16-1113,J03-1002,0,0.0251063,"on. Related work is described in Section 3. The experimental results for both the DP generator and translation are reported in Section 4. Section 5 analyses some real examples which is followed by our conclusion in Section 6. 2 Methodology The architecture of our proposed method is shown in Figure 2, which can be divided into three phases: DP corpus annotation, DP generation, and SMT integration. 2.1 DP Training Corpus Annotation We propose an approach to automatically annotate DPs by utilizing alignment information. Given a parallel corpus, we first use an unsupervised word alignment method (Och and Ney, 2003; Tu et al., 2012) to produce a word alignment. From observing of the alignment matrix, we found it is possible to detect DPs by projecting misaligned pronouns from the non-pro-drop target side (English) to the pro-drop source side (Chinese). In this work, we focus on nominative and accusative pronouns including personal, possessive and reflexive instances, as listed in Table 1. Figure 2: Architecture of proposed method. Category Subjective Personal Objective Personal Possessive Objective Possessive Reflexive Pronouns 我 (I), 我们 (we), 你/你们 (you), 他 (he), 她 (she), 它 (it), 他们/她们/它 们 (they). 我 (me"
N16-1113,P03-1021,0,0.094528,"37,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5, the size of the single hidden layer = 200, iterations = 10, embeddings = 200. The MLP classifier use random initialized embeddings, with the following settings: the size of the single hidden layer = 200, embeddings = 100, iterations = 200. For end-to-end evaluation, case-insensitive BLEU (Papineni et al., 2002) is used to measure 5 Available at http://www.sogou"
N16-1113,P02-1040,0,0.0971952,"on, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, the contributions of this paper include the following: • We propose an automatic method to build a large-scale DP training corpus. Given that the DPs are annotated in the parallel corpus, models trained on this data are more appropriate to the translation task; • Benefit"
N16-1113,W12-4501,0,0.0351916,"Missing"
N16-1113,N07-1029,0,0.0127834,"anslation of missing pronouns by explicitly recalling DPs for both parallel data and monolingual input sentences. More specifically, we extract an additional rule table from the DP-inserted parallel corpus to produce a “pronoun-complete” translation model. In addition, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, th"
N16-1113,W12-4213,0,0.348159,"Missing"
N16-1113,C10-1123,1,0.824671,"et language, so that the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The di"
N16-1113,I11-1145,1,0.817536,"hat the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our t"
N16-1113,C12-2122,1,0.905413,"Missing"
N16-1113,L16-1436,1,0.843791,"Missing"
N16-1113,P13-1081,0,0.692401,"se pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our DP generator. We div"
N16-1113,N13-1125,0,0.0308356,"Missing"
N16-1113,C10-2158,0,0.0510881,"Missing"
N16-1113,P15-2051,0,0.709643,"consists of 1M sentence pairs extracted from movie and TV episode subtitles. We found that there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. Aft"
N16-1113,zhang-etal-2014-dual,0,0.0371089,"Missing"
N16-1113,D07-1057,0,0.339977,"Missing"
N18-1006,P17-1175,1,0.886944,"Missing"
N18-1006,W04-3250,0,0.227175,"Missing"
N18-1006,L16-1147,0,0.0402812,"work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20. Our models are trained using stochastic gradient descent with Adam (Kingma and Ba, 2015). Chung et al. (2016) and Sennrich et al. (2016) demonstrated that bpe boosts NMT, so similar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora1 to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English–Turkish (En–Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016). The training side of the English–German (En–De), English–Russian (En– Ru), and En–Tr corpora include 4.5, 2.1, and 4 million parallel sentences, respectively. We randomly select 3K sentences for each of the development and test sets for En–Tr. For all language pairs we keep the 400 most frequent characters as the target-side character set and replace the remainder (infrequent characters) with a specific character. One of the key modules in our architecture is the morphology table. In order to implement it we use a look-up table whose columns include embeddings for the target language’s affix"
N18-1006,D14-1179,0,0.0539509,"Missing"
N18-1006,P16-1100,0,0.0311023,"dels. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by Kim et al. (2016). Each word is represented by a convolutional combination of its characters. Luong and Manning (2016) used a hybrid model for representing words. In their model, unseen and complex words are encoded with a character-based representation, with other words encoded via the usual surface-form embeddings. Vylomova et al. (2016) compared differCharacter-level decoding limits the search space by dramatically reducing the size of the target vocabulary, but at the same time widens the search space by working with characters whose sampling seems to be harder than words. The freedom in selection and sampling of characters can mislead the decoder, which prevents us from taking the maximum advantages of c"
N18-1006,P16-1160,0,0.189931,"V problem. For these reasons we propose an NMT engine which works at the character level. Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches. However, its performance drops considerably in the presence of morphologically rich languages (MRLs). Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based models to translate this set of languages. In this paper, we propose an extension to the state-of-the-art model of Chung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information. In our architecture, an additional morphology table is plugged into the model. Each time the decoder samples from a target vocabulary, the table sends auxiliary signals from the most relevant affixes in order to enrich the decoder’s current state and constrain it to provide better predictions. We evaluated our model to translate English into German, Russian, and Turkish as three MRLs and observed significant improvements. 1 Word Translation terbiye terbiye.siz terbiye.siz.lik terbiye.siz.lik"
N18-1006,P15-1002,0,0.0387071,"tly better than their wordbased versions. This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models. 2 NMT for MRLs There are several models for NMT of MRLs which are designed to deal with morphological complexities. García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by Kim et al. (2016). Each word is represented by a convolutional combination of its characters. Luong and Manning (2016) used a hybrid model for representing words. In their model, unseen and"
N18-1006,P16-2058,0,0.0359841,"ned to deal with morphological complexities. García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by Kim et al. (2016). Each word is represented by a convolutional combination of its characters. Luong and Manning (2016) used a hybrid model for representing words. In their model, unseen and complex words are encoded with a character-based representation, with other words encoded via the usual surface-form embeddings. Vylomova et al. (2016) compared differCharacter-level decoding limits the search space by dramatically reducing the size of the target vocabulary, but at the same ti"
N18-1006,P02-1040,0,0.106636,"ults. We report results for the bpe→char setting, which means the source token is a bpe unit and the decoder samples a character at each time step. CDNMT is the baseline model. Table 3 includes scores reported from the original CDNMT model (Chung et al., 2016) as well as the scores from our reimplementation. To make our work comparable and show the impact of the new architecture, we tried to replicate CDNMT’s results in our experimental setting, we kept everything (parameters, iterations, epochs etc.) unchanged and evaluated the extended model in the same setting. Table 3 reports BLEU scores (Papineni et al., 2002) of our NMT models. Model CDNMT CDNMT∗ CDNMT∗m CDNMT∗o CDNMT∗mo En→De En→Ru En→Tr 21.33 21.01 21.27 21.39 21.48 26.00 26.23 26.78 26.39 26.84 18.01 18.44 18.59 18.70 • The combination of the morphology table and the extra output channel provides the best result for all languages. Figure 3 depicts the impact of the morphology table and the extra output channel for each language. 0.8 0.7 0.61 0.6 0.55 0.47 0.4 Table 3: CDNMT∗ is our implementation of CDNMT. m and o indicates that the base model is extended with the morphology table and the additional output channel, respectively. mo is the combi"
N18-1006,W17-4727,0,0.019935,"entation of the input sequence. ent representation models (word-, morpheme, and character-level models) which try to capture complexities on the source side, for the task of translating from MRLs. Chung et al. (2016) proposed an architecture which benefits from different segmentation schemes. On the encoder side, words are segmented into subunits with the byte-pair segmentation model (bpe) (Sennrich et al., 2016), and on the decoder side, one target character is produced at each time step. Accordingly, the target sequence is treated as a long chain of characters without explicit segmentation. Grönroos et al. (2017) focused on translating from English into Finnish and implicitly incorporated morphological information into NMT through multi-task learning. Passban (2018) comprehensively studied the problem of translating MRLs and addressed potential challenges in the field. Among all the models reviewed in this section, the network proposed by Chung et al. (2016) could be seen as the best alternative for translating into MRLs as it works at the character level on the decoder side and it was evaluated in different settings on different languages. Consequently, we consider it as a baseline model in our exper"
N18-1006,W16-2360,0,0.0233384,"formulated as in (2): cm i = |A| X βiu fu u=1 exp (em m iu ) ; em βiu = P |A| iu = a (fu , hi−1 ) v=1 exp (eiv ) (2) where fu represents the embedding of the u-th affix (u-th column) in the morphology/affix table A, βiu is the weight assigned to fu when predicting the i-th target token, and am is a feed-forward connection between the morphology table and the decoder. The attention module in general can be considered as a search mechanism, e.g. in the original encoder-decoder architecture the basic attention module finds the most relevant input words to make the prediction. In multi-modal NMT (Huang et al., 2016; Calixto et al., 2017) an extra attention module is added to the basic one in order to search the image input to find the most relevant image segments. In our case we have a similar additional attention module which searches the morphology table. In this scenario, the morphology table including the target language’s affixes can be considered as an external knowledge repository that sends auxiliary signals which accompany the main input sequence at all time steps. Such a table certainly includes useful information for the decoder. As we are not sure which affix preserves those pieces of useful"
N18-1006,W16-2209,0,0.033435,"we consider the decoder as a classifier, it should in principle be able to perform much better over hundreds of classes (characters) rather than thousands (words), but the performance of character-based models is almost the same as or slightly better than their wordbased versions. This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models. 2 NMT for MRLs There are several models for NMT of MRLs which are designed to deal with morphological complexities. García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source wor"
N18-1006,P15-1001,0,0.0626427,"he performance of character-based models is almost the same as or slightly better than their wordbased versions. This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models. 2 NMT for MRLs There are several models for NMT of MRLs which are designed to deal with morphological complexities. García-Martínez et al. (2016) and Sennrich and Haddow (2016) adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. Jean et al. (2015) proposed a model to handle very large vocabularies. Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. Sennrich et al. (2016) used subword units for NMT. The model relies on frequent subword units instead of words. Costa-jussà and Fonollosa (2016) designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by Kim et al. (2016). Each word is represented by a convolutional combination of its characters. Luong and Manning (2016)"
N18-1006,E14-2006,0,0.0656194,"constituents on the decoder side is that of Chung et al. (2016), which should be an appropriate baseline for our comparisons. Moreover, it outperforms other existing NMT models, so we prefer to compare our network to the best existing model. This model is referred to as CDNMT in our experiments. In the next sections first we explain our experimental setting, corpora, and how we build the morphology table (Section 4.1), and then report experimental results (Section 4.2). 62 4.1 Experimental Setting to manipulate the training corpus to decompose words into morphemes for which we use Morfessor (Smit et al., 2014), an unsupervised morphological analyzer. Using Morfessor each word is segmented into different subunits where we consider the longest part as the stem of each word; what appears before the stem is taken as a member of the set of prefixes (there might be one or more prefixes) and what follows the stem is considered as a member of the set of suffixes. In order to make our work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20. Our models are trained using stochastic gradient des"
N18-1006,P16-1162,0,\N,Missing
N18-1061,P11-2008,0,0.0727691,"Missing"
N18-1061,P06-4018,0,0.0411422,"tional LSTMs (Schuster and Paliwal, 1997) train two LSTMs, instead of one, on the input sequence. The first on the input sequence and the second on a reversed copy of the input sequence. It is designed to capture information of the sequential dataset and maintain the contextual features from the past and the future. This can provide an additional context to the network and result in faster and even fuller learning on the problem without keeping the redundant context information. 3.2 Sentiment View of Temporal Orientation We use an existing sentiment classifier available with the NLTK toolkit (Bird, 2006) to classify the user-level tweets into positive, negative or neutral.3 Sentiment is added at the fine-grained level of the temporal orientation. Given the tweets of a user, the sentiment view of temporal orientation of that user is defined by the following equation: orientations,t (user) = |tweetss/t (user)| (1) |tweetst (user)| where, (t ∈ { past, present, or future}), and (s ∈ { positive, negative, or neutral}), in equation (1). Here, we first classify each user’s tweet into the past, present or future temporal category. Then for each temporal category, we find the percentage of each sentim"
N18-1061,chang-manning-2012-sutime,0,0.0350988,"Park et al., 2015; Schwartz et al., 2015; Park et al., 2017). The underlying idea is to understand how the past, present, and future emphasis in the text may affect people’s finances, health, and happiness. For that purpose, the temporal classifiers are built to detect the overall temporal dimension of a given sentence. For instance, the following sentence “can’t wait to get a pint tonight” would be tagged as future. In summary, most of the temporal text processing applications have been mainly relying on the rule-based time taggers, for e.g. HeidelTime (Str¨otgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and normalize time mentions in the texts. Although interesting results have been reported (UzZaman et al., 2013), but the coverage is limited to the finite number of rules they implement. The time perspective and its importance in various social science and psychological studies is well established in literature. It plays a fundamental role in our interpersonal relation influenced by cognitive process (Zimbardo and Boyd, 2015). • We introduce the sentiment dimensions in the human temporal orientation to infer the social media users’ psycho-demographic attributes on a large-scale."
N18-1061,D14-1121,0,0.0818986,"Missing"
N18-1061,P14-5010,0,0.00412872,"ucation, intelligence, optimism, and relationship using a linear regression (LR) classifier (Neter et al., 1996). 4 the filtering method, we filter out the tweets which do not contain a verb. The verb part-of-speech tag is determined using the CMU tweet-tagger (Gimpel et al., 2011). In the second pass of the filtering method, we removed the tweets having tense as past from the tweets of the present and future events. The CMU tweet-tagger does not provide verbs in different sub-categories. For this reason, we also retrieve the Part-of-Speech (PoS) tag information from the Standford PoS-tagger (Manning et al., 2014) for all the tweets to get the subcategories of verb (i.e. VB, VBD, VBG, VBN, VBP, VBZ). We observed that although Standford PoS-tagger assigned the required verb subcategories, it also incorrectly tagged some nonverbs as verbs. This is the reason why we considered only those verbs for sub-categorization which were identified (as verbs) by the CMU tweettagger. We varied the training set starting from 3K (equally distributed) to 30K and observed that the accuracy on the gold standard test set did not improve after 27K training instances. Few example tweets with the trending topics are depicted"
N18-1061,N15-1044,0,0.361498,"e current research trends and presented a number of interesting applications along with the open problems. The shared task like the NTCIR-11 Temporalia task (Joho et al., 2014) further pushed this idea and proposed to distinguish whether a given query is related to past, recency, future or atemporal. It is the first such challenge, which is organized to provide a common platform for designing and analyzing the time-aware information access systems. In parallel, new trends have emerged in the context of the human temporal orientation (Schwartz et al., 2013; Sap et al., 2014; Park et al., 2015; Schwartz et al., 2015; Park et al., 2017). The underlying idea is to understand how the past, present, and future emphasis in the text may affect people’s finances, health, and happiness. For that purpose, the temporal classifiers are built to detect the overall temporal dimension of a given sentence. For instance, the following sentence “can’t wait to get a pint tonight” would be tagged as future. In summary, most of the temporal text processing applications have been mainly relying on the rule-based time taggers, for e.g. HeidelTime (Str¨otgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and"
N18-1061,D14-1162,0,0.0809289,"Missing"
N18-1061,D15-1063,0,0.0446812,"Missing"
N18-1061,S13-2001,0,0.0975751,"ibutes, namely age, eduction, relationship, intelligence, and optimism. Our contributions are summarised as below: • We define a way to find a novel association between the sentiment view of temporal orientation and the different psychodemographic factors of the tweet users. 2 Related Background The temporal study has recently received an increased attention in several application domains of Natural Language Processing (NLP) and Information Retrieval (IR). The introduction of the TempEval task (Verhagen et al., 2009) and the subsequent challenges i.e. TempEval-2 and -3 (Verhagen et al., 2010; UzZaman et al., 2013) in the Semantic Evaluation workshop series have clearly established the importance of time in dealing with the different NLP tasks. Alonso et al. (2011) reviewed the current research trends and presented a number of interesting applications along with the open problems. The shared task like the NTCIR-11 Temporalia task (Joho et al., 2014) further pushed this idea and proposed to distinguish whether a given query is related to past, recency, future or atemporal. It is the first such challenge, which is organized to provide a common platform for designing and analyzing the time-aware informatio"
P04-1041,J97-4005,0,0.150696,"Missing"
P04-1041,P03-1046,0,0.0771796,"the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources provide the backbone of many state-of-the-art probabilistic parsing approaches (Charniak, 1996; Collins, 1999; Charniak, 1999; Hockenmaier, 2003; Klein and Manning, 2003). Such approaches are attractive as they achieve robustness, coverage and performance while incurring very low grammar development cost. However, with few notable exceptions (e.g. Collins’ Model 3, (Johnson, 2002), (Hockenmaier, 2003) ), treebank-based probabilistic parsers return fairly simple “surfacey” CFG trees, without deep syntactic or semantic information. The grammars used by such systems are sometimes re2 Or dependency banks. ferred to as “half” (or “shallow”) grammars (Johnson, 2002), i.e. they do not resolve LDDs but interpret linguistic material purely loc"
P04-1041,P02-1018,0,0.541974,"lative clauses and interrogative sentences, however, there is an important difference between the location of the (surface) realisation of linguistic material and the location where this material should be interpreted semantically. Resolution of such long-distance dependencies (LDDs) is therefore crucial in the determination of accurate predicate-argument struc1 Manually constructed f-structures for 105 randomly selected trees from Section 23 of the WSJ section of the Penn-II Treebank ture, deep dependency relations and the construction of proper meaning representations such as logical forms (Johnson, 2002). Modern unification/constraint-based grammars such as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is"
P04-1041,N04-1013,0,0.0502222,"105 (Cahill et al., 2002) 2. The full 2,416 f-structures automatically generated by the f-structure annotation algorithm for the original Penn-II trees, in a CCG-style (Hockenmaier, 2003) evaluation experiment # Parses Lab. F-Score Unlab. F-Score All GFs F-Score (before LDD resolution) All GFs F-Score (after LDD resolution) Preds only F-Score (before LDD resolution) Preds only F-Score (after LDD resolution) All GFs F-Score (before LDD resolution) All GFs F-Score (after LDD resolution) Preds only F-Score (before LDD resolution) Preds only F-Score (after LDD resolution) Subset of GFs following (Kaplan et al., 2004) Pipeline Integrated PCFG P-PCFG A-PCFG PA-PCFG 2416 Section 23 trees 2416 2416 2416 2414 75.83 80.80 79.17 81.32 78.28 82.70 81.49 83.28 DCU 105 F-Strs 79.82 79.24 81.12 81.20 83.79 84.59 86.30 87.04 70.00 71.57 73.45 74.61 73.78 77.43 78.76 80.97 2416 F-Strs 81.98 81.49 83.32 82.78 84.16 84.37 86.45 86.00 72.00 73.23 75.22 75.10 74.07 76.12 78.36 78.40 PARC 700 Dependency Bank 77.86 80.24 77.68 78.60 Table 7: Parser Evaluation 3. A subset of 560 dependency structures of the PARC 700 Dependency Bank following (Kaplan et al., 2004) The results are given in Table 7. The parenttransformed gramma"
P04-1041,P03-1054,0,0.116149,"f parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources provide the backbone of many state-of-the-art probabilistic parsing approaches (Charniak, 1996; Collins, 1999; Charniak, 1999; Hockenmaier, 2003; Klein and Manning, 2003). Such approaches are attractive as they achieve robustness, coverage and performance while incurring very low grammar development cost. However, with few notable exceptions (e.g. Collins’ Model 3, (Johnson, 2002), (Hockenmaier, 2003) ), treebank-based probabilistic parsers return fairly simple “surfacey” CFG trees, without deep syntactic or semantic information. The grammars used by such systems are sometimes re2 Or dependency banks. ferred to as “half” (or “shallow”) grammars (Johnson, 2002), i.e. they do not resolve LDDs but interpret linguistic material purely locally where it occurs in th"
P04-1041,H94-1003,0,0.210445,"Missing"
P04-1041,H94-1020,0,0.0697514,"as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank (Marcus et al., 1994) with discriminative (loglinear) parameter estimation techniques. The last 20 years have seen continuously increasing efforts in the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources"
P04-1041,P04-1047,1,0.744247,"Missing"
P04-1041,P02-1035,0,0.0980593,"proper meaning representations such as logical forms (Johnson, 2002). Modern unification/constraint-based grammars such as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank (Marcus et al., 1994) with discriminative (loglinear) parameter estimation techniques. The last 20 years have seen continuously increasing efforts in the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, sta"
P04-1041,W98-0141,0,0.0579954,"Missing"
P04-1041,C96-1045,1,0.860707,"Missing"
P04-1041,A00-2018,0,\N,Missing
P04-1041,J98-4004,0,\N,Missing
P04-1041,J03-4003,0,\N,Missing
P04-1047,J93-2002,0,0.193808,"Missing"
P04-1047,P04-1041,1,0.781736,"Missing"
P04-1047,W98-1505,0,0.100785,"Missing"
P04-1047,2000.iwpt-1.9,0,0.252147,"Missing"
P04-1047,P97-1003,0,0.0933422,"Missing"
P04-1047,kinyon-prolo-2002-identifying,0,0.119169,"Missing"
P04-1047,P98-1115,0,0.0908158,"Missing"
P04-1047,H94-1003,0,0.925761,"ticle verbs. Our method does not predefine the frames to be extracted. In contrast to many other approaches, it discriminates between active and passive frames, properly reflects long distance dependencies and assigns conditional probabilities to the semantic forms associated with each predicate. Section 2 reviews related work in the area of automatic subcategorisation frame extraction. Our methodology and its implementation are presented in Section 3. Section 4 presents the results of our lexical extraction. In Section 5 we evaluate the complete extracted lexicon against the COMLEX resource (MacLeod et al., 1994). To our knowledge, this is the largest evaluation of subcategorisation frames for English. In Section 6, we conclude and give suggestions for future work. 2 Related Work Creating a (subcategorisation) lexicon by hand is time-consuming, error-prone, requires considerable linguistic expertise and is rarely, if ever, complete. In addition, a system incorporating a manually constructed lexicon cannot easily be adapted to specific domains. Accordingly, many researchers have attempted to construct lexicons automatically, especially for English. (Brent, 1993) relies on local morphosyntactic cues (su"
P04-1047,P93-1032,0,0.217958,"Missing"
P04-1047,W93-0109,0,0.205245,"Missing"
P04-1047,W00-1307,0,0.0866381,"Missing"
P04-1047,A97-1052,0,\N,Missing
P04-1047,C98-1111,0,\N,Missing
P07-1037,koen-2004-pharaoh,0,0.0266213,"tory operator violations in a sequence of supertags (cf. Figure 2). For a supertag sequence of length (L) which has (V ) operator violations (as measured by the CCG system), the language model P will be adjusted as P ∗ = P × (1 − VL ). This is of course no longer a simple smoothed maximum-likelihood estimate nor is it a true probability. Nevertheless, this mechanism provides a simple, efficient integration of a global compositionality (grammaticality) measure into the n-gram language model over supertags. Decoder The decoder used in this work is Moses, a log-linear decoder similar to Pharaoh (Koehn, 2004), modified to accommodate supertag phrase probabilities and supertag language models. 5 Experiments In this section we present a number of experiments that demonstrate the effect of lexical syntax on translation quality. We carried out experiments on the NIST open domain news translation task from Arabic into English. We performed a number of experiments to examine the effect of supertagging approaches (CCG or LTAG) with varying data sizes. Data and Settings The experiments were conducted for Arabic to English translation and tested on the NIST 2005 evaluation set. The systems were trained on"
P07-1037,N03-1017,0,0.337572,"operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robust Within the field of Machine Translation, by far the 1 most dominant paradigm is Phrase-based Statistical These operators neither carry nor presuppose further linMachine Translation (PBSMT) (Koehn et al., 2003; guistic knowledge beyond what the lexicon contains. 288 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288–295, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and efficient supertagging approach: like standard taggers, supertaggers employ probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore & Joshi, 1999). There are currently two supertagging approaches available: LTAG-based (Bangalore & Joshi, 1999) and CCG-based (Clark & Curran, 2004"
P07-1037,W06-1606,0,0.00887724,"Related Work Until very recently, the experience with adding syntax to PBSMT systems was negative. For example, (Koehn et al., 2003) demonstrated that adding syntax actually harmed the quality of their SMT system. Among the first to demonstrate improvement when adding recursive structure was (Chiang, 2005), who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashion. Chiang’s derived grammar does not rely on any linguistic annotations or assumptions, so that the ‘syntax’ induced is not linguistically motivated. Coming right up to date, (Marcu et al., 2006) demonstrate that ‘syntactified’ target language phrases can improve translation quality for Chinese– 289 English. They employ a stochastic, top-down transduction process that assigns a joint probability to a source sentence and each of its alternative translations when rewriting the target parse-tree into a source sentence. The rewriting/transduction process is driven by “xRS rules”, each consisting of a pair of a source phrase and a (possibly only partially) lexicalized syntactified target phrase. In order to extract xRS rules, the word-to-word alignment induced from the parallel training co"
P07-1037,W02-1018,0,0.0579825,"Missing"
P07-1037,P03-1021,0,0.0204594,"riments, we used the LTAG English supertagger5 (Bangalore 4 5 http://www.speech.sri.com/projects/srilm/ http://www.cis.upenn.edu/˜xtag/gramrelease.html & Joshi, 1999) to tag the English part of the parallel data and the supertag language model data. For the CCG supertag experiments, we used the CCG supertagger of (Clark & Curran, 2004) and the Edinburgh CCG tools6 to tag the English part of the parallel corpus as well as the CCG supertag language model data. The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003). Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3. We built two baseline systems with two different-sized training sets: ‘Base-SMALL’ (5 million words) and ‘Base-LARGE’ (50 million words) as described above. Both systems use a trigram language model built using 250 million words from the English GigaWord Corpus. Table 1 presents the BLEU scores (Papineni et al., 2002) of both systems on the NIST 2005 MT Evaluation test set. System Base-SMALL Base-LARGE BLEU Score 0.4008 0.4418 Table 1: Baseline systems’ BLEU scores 5.1 Baseline vs. Supertags on S"
P07-1037,J99-2004,0,0.857417,"5) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi & Schabes, 1992; Bangalore & Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000). In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robus"
P07-1037,P05-1033,0,0.0735743,"on enriching PBSMT with syntactic structure. In section 3, we describe the baseline PBSMT system which our work extends. In section 4, we detail our approach. Section 5 describes the experiments carried out, together with the results obtained. Section 6 concludes, and provides avenues for further work. 2 Related Work Until very recently, the experience with adding syntax to PBSMT systems was negative. For example, (Koehn et al., 2003) demonstrated that adding syntax actually harmed the quality of their SMT system. Among the first to demonstrate improvement when adding recursive structure was (Chiang, 2005), who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashion. Chiang’s derived grammar does not rely on any linguistic annotations or assumptions, so that the ‘syntax’ induced is not linguistically motivated. Coming right up to date, (Marcu et al., 2006) demonstrate that ‘syntactified’ target language phrases can improve translation quality for Chinese– 289 English. They employ a stochastic, top-down transduction process that assigns a joint probability to a source sentence and each of its alternative translations when rewriting the targ"
P07-1037,C04-1041,0,0.660761,"(Koehn et al., 2003; guistic knowledge beyond what the lexicon contains. 288 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288–295, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics and efficient supertagging approach: like standard taggers, supertaggers employ probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore & Joshi, 1999). There are currently two supertagging approaches available: LTAG-based (Bangalore & Joshi, 1999) and CCG-based (Clark & Curran, 2004). Both the LTAG (Chen et al., 2006) and the CCG supertag sets (Hockenmaier, 2003) were acquired from the WSJ section of the Penn-II Treebank using handbuilt extraction rules. Here we test both the LTAG and CCG supertaggers. We interpolate (log-linearly) the supertagged components (language model and phrase table) with the components of a standard PBSMT system. Our experiments on the Arabic– English NIST 2005 test suite show that each of the supertagged systems significantly improves over the baseline PBSMT system. Interestingly, combining the two taggers together diminishes the benefits of sup"
P07-1037,C92-2066,0,0.329956,"tem. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi & Schabes, 1992; Bangalore & Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000). In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory ope"
P07-1037,P02-1040,0,0.106815,"corpus as well as the CCG supertag language model data. The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003). Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3. We built two baseline systems with two different-sized training sets: ‘Base-SMALL’ (5 million words) and ‘Base-LARGE’ (50 million words) as described above. Both systems use a trigram language model built using 250 million words from the English GigaWord Corpus. Table 1 presents the BLEU scores (Papineni et al., 2002) of both systems on the NIST 2005 MT Evaluation test set. System Base-SMALL Base-LARGE BLEU Score 0.4008 0.4418 Table 1: Baseline systems’ BLEU scores 5.1 Baseline vs. Supertags on Small Data Sets We compared the translation quality of the baseline systems with the LTAG and CCG supertags systems (LTAG-SMALL and CCG-SMALL). The results are System Base-SMALL LTAG-SMALL CCG-SMALL BLEU Score 0.4008 0.4205 0.4174 Table 2: LTAG and CCG systems on small data given in Table 2. All systems were trained on the same parallel data. The LTAG supertag-based system outperforms the baseline by 1.97 BLEU point"
P07-1037,N03-2036,0,0.0279423,"Missing"
P07-1037,J03-1002,0,\N,Missing
P07-1039,P06-1002,0,0.0102905,"(Och and Ney, 2003), that compares a system’s alignment output to a set of gold-standard alignment. While this method gives a direct evaluation of the quality of word alignment, it is faced with several limitations. First, it is really difficult to build a reliable and objective gold-standard set, especially for languages as different as Chinese and English. Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). The relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Dev. Eval. Sentences Running words Vocabulary size Sentences Running words Vocabulary size Sentences Running words Vocabulary size Chinese English 41,465 361,780 375,938 11,427 9,851 289 (7 refs.) 3,350 26,223 897 1,331 200 (7 refs.) 1,864 14,437 569 1,081 Data T"
P07-1039,W06-3123,0,0.00441411,"ication of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentioned in (Tiedemann, 2003), and related issues are reported in (Wu, 1997). 2.2 Notation While in this paper, we focus on Chinese–English, the method proposed is applicable to any language 3 Note that a 1: 0 alignment may denote a failure to capture a 1: n alignment with n &gt; 1. 4 Interestingly, this is actually even the case for approaches that directly model alignments between phrases (Marcu and Wong, 2002; Birch et al., 2006). pair – even for closely related languages, we expect improvements to be seen. The notation however assume Chinese–English MT. Given a Chinese sentence cJ1 consisting of J words {c1 , . . . , cJ } and an English sentence eI1 consisting of I words {e1 , . . . , eI }, AC→E (resp. AE→C ) will denote a Chinese-to-English (resp. an English-to-Chinese) word alignment between cJ1 and eI1 . Since we are primarily interested in 1-to-n alignments, AC→E can be represented as a set of pairs aj = hcj , Ej i denoting a link between one single Chinese word cj and a few English words Ej (and similarly for AE"
P07-1039,J93-2003,0,0.0272075,"tion 5, we evaluate the influence of our method on the alignment process on a Chinese to English MT task, and experimental results are presented. Section 6 concludes the paper and gives avenues for future work. The Case of 1-to-n Alignment 2 The same concept can be expressed in different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n ≥ 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-"
P07-1039,H05-1022,0,0.152346,"Missing"
P07-1039,P02-1040,0,0.111613,"Missing"
P07-1039,W96-0107,0,0.474963,"k of automatic word aligners by packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (Kitamura and Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003). We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the alignment process on a Chinese-to-English Machine Translation (MT) task. The remainder of this paper is organized as follows. In Section 2, we study the case of 1-ton word alignment. Section 3 introduces an automatic method to pack together groups of consecutive Proceedings of the 45t"
P07-1039,N03-1017,0,0.0292406,"nNLP toolkit, and case information was removed. For Chinese, the data provided were tokenized according to the output format of ASR systems, and human-corrected (Paul, 2006). Since segmentations are human-corrected, we are sure that they are good from a monolingual point of view. Table 2 contains the various corpus statistics. 4.3 Train Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003),8 the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training 7 More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8 Training is performed using the same number of iterations as in Section 2. 308 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional pro"
P07-1039,koen-2004-pharaoh,0,0.0247571,"n et al., 1993; Och and Ney, 2003),8 the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training 7 More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8 Training is performed using the same number of iterations as in Section 2. 308 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 5.1 Experimental Results Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each direction), and the training corpus is updated by repacking the words in the dictionaries, using the method presented in Section 2. As previously mentioned, this process can be repeated several"
P07-1039,J96-1001,0,0.0568498,"nments. Our method for repacking words is very simple: whenever a single word is aligned with several consecutive words, they are considered candidates for repacking. Formally, given an alignment AC→E between cJ1 and eI1 , if 306 Candidate Reliability Estimation AC(aj ) = C(aj ) , COOC(cj , Ej ) where C(aj ) denotes the number of alignments proposed by the word aligner that are identical to aj . In other words, AC(aj ) measures how often the 5 Consequently, if we compare our approach to the problem of collocation identification, we may say that we are more interested in precision than recall (Smadja et al., 1996). However, note that our goal is not recognizing specific sequences of words such as compounds or collocations; it is making (bilingually motivated) changes that simplify the alignment process. aligner aligns cj and Ej when they co-occur. We also impose that |Ej |≤ k, where k is a fixed integer that may depend on the language pair (between 3 and 5 in practice). The rationale behind this is that it is very rare to get reliable alignment between one word and k consecutive words when k is high. The candidates are included in our bilingual dictionary if and only if their measures are above some fi"
P07-1039,2005.mtsummit-papers.11,0,0.0522491,"different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n ≥ 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-entropy based tokenizer of OpenNLP2 ; the Chinese sentences were human segmented (Paul, 2006). In Table 1, we report the frequencies of the different types of alignments for the various languages and directions. As expected, the number of 1: n 1 More specifically, we perf"
P07-1039,takezawa-etal-2002-toward,0,0.433061,"avenues for future work. The Case of 1-to-n Alignment 2 The same concept can be expressed in different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n ≥ 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-entropy based tokenizer of OpenNLP2 ; the Chinese sentences were human segmented (Paul, 2006). In Table 1, we report the frequencies of the different types of alignments for the various l"
P07-1039,N06-1014,0,0.0714853,"tionary 4 4.1 Experimental Setting Evaluation The intrinsic quality of word alignment can be assessed using the Alignment Error Rate (AER) metric (Och and Ney, 2003), that compares a system’s alignment output to a set of gold-standard alignment. While this method gives a direct evaluation of the quality of word alignment, it is faced with several limitations. First, it is really difficult to build a reliable and objective gold-standard set, especially for languages as different as Chinese and English. Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). The relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Dev. Eval. Sentences Running words Vocabulary size Sentences Running words Vocabulary size Sentences Running words Vo"
P07-1039,E03-1026,0,0.39535,"al consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (Kitamura and Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003). We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the alignment process on a Chinese-to-English Machine Translation (MT) task. The remainder of this paper is organized as follows. In Section 2, we study the case of 1-ton word alignment. Section 3 introduces an automatic method to pack together groups of consecutive Proceedings of the 45th Annual Meeting of the Associati"
P07-1039,2006.amta-papers.11,0,0.0100724,"that compares a system’s alignment output to a set of gold-standard alignment. While this method gives a direct evaluation of the quality of word alignment, it is faced with several limitations. First, it is really difficult to build a reliable and objective gold-standard set, especially for languages as different as Chinese and English. Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). The relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Dev. Eval. Sentences Running words Vocabulary size Sentences Running words Vocabulary size Sentences Running words Vocabulary size Chinese English 41,465 361,780 375,938 11,427 9,851 289 (7 refs.) 3,350 26,223 897 1,331 200 (7 refs.) 1,864 14,437 569 1,081 Data The experiments were carr"
P07-1039,2006.iwslt-papers.7,0,0.00990746,"Missing"
P07-1039,W02-1018,0,0.029928,"benefit from a simplification of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentioned in (Tiedemann, 2003), and related issues are reported in (Wu, 1997). 2.2 Notation While in this paper, we focus on Chinese–English, the method proposed is applicable to any language 3 Note that a 1: 0 alignment may denote a failure to capture a 1: n alignment with n &gt; 1. 4 Interestingly, this is actually even the case for approaches that directly model alignments between phrases (Marcu and Wong, 2002; Birch et al., 2006). pair – even for closely related languages, we expect improvements to be seen. The notation however assume Chinese–English MT. Given a Chinese sentence cJ1 consisting of J words {c1 , . . . , cJ } and an English sentence eI1 consisting of I words {e1 , . . . , eI }, AC→E (resp. AE→C ) will denote a Chinese-to-English (resp. an English-to-Chinese) word alignment between cJ1 and eI1 . Since we are primarily interested in 1-to-n alignments, AC→E can be represented as a set of pairs aj = hcj , Ej i denoting a link between one single Chinese word cj and a few English words Ej"
P07-1039,C96-2141,0,0.216558,"he case of 1-to-n alignments is, therefore, obviously an important issue when dealing with Chinese–English word alignment.3 2.1 The Treatment of 1-to-n Alignments Fertility-based models such as IBM models 3, 4, and 5 allow for alignments between one word and several words (1-to-n or 1: n alignments in what follows), in particular for the reasons specified above. They can be seen as extensions of the simpler IBM models 1 and 2 (Brown et al., 1993). Similarly, Deng and Byrne (2005) propose an HMM framework capable of dealing with 1-to-n alignment, which is an extension of the original model of (Vogel et al., 1996). However, these models rarely question the monolingual tokenization, i.e. the basic unit of the alignment process is the word.4 One alternative to extending the expressivity of one model (and usually its complexity) is to focus on the input representation; in particular, we argue that the alignment process can benefit from a simplification of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentioned in (Tiedemann, 2003), and related issues are reported in (Wu, 1997). 2"
P07-1039,W97-0311,0,0.0330193,"we want to change the input to give to the word aligner, we need to make sure that we are not making harmful modifications.5 We thus additionally evaluate the reliability of the candidates we extract and filter them before inclusion in our bilingual dictionary. To perform this filtering, we use two simple statistical measures. In the following, aj = hcj , Ej i denotes a candidate. The first measure we consider is co-occurrence frequency (COOC(cj , Ej )), i.e. the number of times cj and Ej co-occur in the bilingual corpus. This very simple measure is frequently used in associative approaches (Melamed, 1997; Tiedemann, 2003). The second measure is the alignment confidence, defined as Candidate Extraction In the following, we assume the availability of an automatic word aligner that can output alignments AC→E and AE→C for any sentence pair (cJ1 , eI1 ) in a parallel corpus. We also assume that AC→E and AE→C contain 1: n alignments. Our method for repacking words is very simple: whenever a single word is aligned with several consecutive words, they are considered candidates for repacking. Formally, given an alignment AC→E between cJ1 and eI1 , if 306 Candidate Reliability Estimation AC(aj ) = C(aj"
P07-1039,J97-3002,0,0.167763,"sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words – a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of words (see e.g. (Wu, 1997)). Moreover, a 304 segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models. Although some statistical alignment models allow for 1-to-n word alignments for those reasons, they rarely question the monolingual tokenization and the basic unit of the alignment process remains the word. In this paper, we focus on 1-to-n alignments with the goal of simplifying the task of automatic word aligners by packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough suc"
P07-1039,J00-2004,0,0.0129116,"y packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (Kitamura and Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003). We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the alignment process on a Chinese-to-English Machine Translation (MT) task. The remainder of this paper is organized as follows. In Section 2, we study the case of 1-ton word alignment. Section 3 introduces an automatic method to pack together groups of consecutive Proceedings of the 45th Annual Meetin"
P07-1039,W04-1118,0,0.154211,"Missing"
P07-1039,J03-1002,0,0.134718,"n the alignment process on a Chinese to English MT task, and experimental results are presented. Section 6 concludes the paper and gives avenues for future work. The Case of 1-to-n Alignment 2 The same concept can be expressed in different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n ≥ 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-entropy based tokenizer of OpenNLP2 ; the Chines"
P07-1039,P03-1021,0,0.00672862,"pus statistics. 4.3 Train Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003),8 the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training 7 More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8 Training is performed using the same number of iterations as in Section 2. 308 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 5.1 Experimental Results Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each dire"
P07-1039,W06-3121,0,0.021584,"Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003),8 the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training 7 More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8 Training is performed using the same number of iterations as in Section 2. 308 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 5.1 Experimental Results Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each direction), and the training corpus is up"
P07-1039,N04-1033,0,0.02351,"ence from the 7 references and the Chinese sentence to construct new sentence pairs. 8 Training is performed using the same number of iterations as in Section 2. 308 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 5.1 Experimental Results Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each direction), and the training corpus is updated by repacking the words in the dictionaries, using the method presented in Section 2. As previously mentioned, this process can be repeated several times; at each step, we can also choose to exploit only one of the two available dictionaries, if so desired. We then extract aligned phrases using the same procedure as for the baseline system; the only diffe"
P07-1039,J07-3002,0,\N,Missing
P07-1039,2006.amta-papers.2,0,\N,Missing
P07-1039,2006.iwslt-evaluation.1,0,\N,Missing
P10-1064,quirk-2004-training,0,0.104681,"ifiers that classify an input instance based on decision rules which minimize the regularized error function in (1): Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less rele"
P10-1064,C04-1046,0,0.243124,"thout a TM in the background. 3 Support Vector Machines for Translation Quality Estimation Related Work SVMs (Cortes and Vapnik, 1995) are binary classifiers that classify an input instance based on decision rules which minimize the regularized error function in (1): Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there"
P10-1064,2009.mtsummit-papers.14,0,0.553395,"rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this paper. A third strand of research tries to incorporate confidence measures into a post-editing environ∑ 1 T w w+C ξi 2 i=1 l min w,b,ξ s. t. yi (wT ϕ(xi ) + b) &gt; 1 − ξi ξi &gt; 0 (1) where (xi , yi ) ∈ Rn × {+1, −1} are l training instances that are mapped by the function ϕ to a higher dimensional space. w is the weight vector, ξ is the relaxation variable and C &gt; 0 is the penalty parameter. Solving SVMs is viable using the ‘kernel trick’: finding a kernel function K in (1) with K(xi , xj ) ="
P10-1064,J93-2003,0,0.0180988,"Missing"
P10-1064,D09-1030,0,0.0312648,"d the one recommended to edit, we can measure the true accuracy of our recommendation, as well as the post-editing time we save for the post-editors; Finally, we analyze the characteristics of the integrated outputs. We present results to show that, if measured by number, type and content of edits in TER, the recommended sentences produced by the classification model would bring about less post-editing effort than the TM outputs. • Apply the presented method on open domain data and evaluate it using crowdsourcing. It has been shown that crowdsourcing tools, such as the Amazon Mechanical Turk (Callison-Burch, 2009), can help developers to obtain good human judgements on MT output quality both cheaply and quickly. Given that our problem is related to MT quality estimation in nature, it can potentially benefit from such tools as well. 7 This work can be extended in the following ways. Most importantly, it is useful to test the model in user studies, as proposed in Section 6.3. A user study can serve two purposes: 1) it can validate the effectiveness of the method by measuring the amount of edit effort it saves; and 2) the byproduct of the user study – post-edited sentences – can be used to generate HTER s"
P10-1064,2006.amta-papers.25,0,0.293373,"mization, employ posterior probability-based confidence estimation to support user-based tuning for precision and recall, experiment with feature sets involving MT-, TM- and system-independent features, and use automatic MT evaluation metrics to simulate post-editing effort. The rest of the paper is organized as follows: we first briefly introduce related research in Section 2, and review the classification SVMs in Section 3. We formulate the classification model in Section 4 and present experiments in Section 5. In Section 6, we analyze the post-editing effort approximated by the TER metric (Snover et al., 2006). Section 7 concludes the paper and points out avenues for future research. 2 ment. To the best of our knowledge, the first paper in this area is (Specia et al., 2009a). Instead of modeling on translation quality (often measured by automatic evaluation scores), this research uses regression on both the automatic scores and scores assigned by post-editors. The method is improved in (Specia et al., 2009b), which applies Inductive Confidence Machines and a larger set of features to model post-editors’ judgement of the translation quality between ‘good’ and ‘bad’, or among three levels of post-edi"
P10-1064,2009.eamt-1.5,0,0.270491,"everal simple reasons for this: 1) TMs are useful; 2) TMs represent considerable effort and investment by a company or (even more so) an individual translator; 3) the fuzzy match score used in TMs offers a good approximation of post-editing effort, which is useful both for translators and translation cost estimation and, 4) current SMT translation confidence estimation measures are not as robust as TM fuzzy match scores and professional translators are thus not ready to replace fuzzy match scores with SMT internal quality measures. There has been some research to address this issue, see e.g. (Specia et al., 2009a) and (Specia et al., 2009b). However, to date most of the research has focused on better confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors (cf. Section 2). In this paper, we try to address the problem from a different perspective. Given that most postediting work is (still) based on TM output, we propose to recommend MT outputs which are better than TM hits to post-editors. In this framework, post-editors still work with the TM while benefiting from (better) SMT outputs; the assets in TMs are not wasted an"
P10-1064,2009.mtsummit-papers.16,0,0.417056,"everal simple reasons for this: 1) TMs are useful; 2) TMs represent considerable effort and investment by a company or (even more so) an individual translator; 3) the fuzzy match score used in TMs offers a good approximation of post-editing effort, which is useful both for translators and translation cost estimation and, 4) current SMT translation confidence estimation measures are not as robust as TM fuzzy match scores and professional translators are thus not ready to replace fuzzy match scores with SMT internal quality measures. There has been some research to address this issue, see e.g. (Specia et al., 2009a) and (Specia et al., 2009b). However, to date most of the research has focused on better confidence measures for MT, e.g. based on training regression models to perform confidence estimation on scores assigned by post-editors (cf. Section 2). In this paper, we try to address the problem from a different perspective. Given that most postediting work is (still) based on TM output, we propose to recommend MT outputs which are better than TM hits to post-editors. In this framework, post-editors still work with the TM while benefiting from (better) SMT outputs; the assets in TMs are not wasted an"
P10-1064,N03-1017,0,0.0127258,"Missing"
P10-1064,P07-2045,0,0.00728275,"Missing"
P10-1064,2005.eamt-1.35,0,0.019082,"fy an input instance based on decision rules which minimize the regularized error function in (1): Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive translation system. The second strand of research focuses on combining TM information with an SMT system, so that the SMT system can produce better target language output when there is an exact or close match in the TM (Simard and Isabelle, 2009). This line of research is shown to help the performance of MT, but is less relevant to our task in this pap"
P10-1064,2003.mtsummit-papers.52,0,0.0136704,"cause of this, the precision and recall scores reported in this paper are not directly comparable to those in (Specia et al., 2009b) as the latter are computed on a pure SMT system without a TM in the background. 3 Support Vector Machines for Translation Quality Estimation Related Work SVMs (Cortes and Vapnik, 1995) are binary classifiers that classify an input instance based on decision rules which minimize the regularized error function in (1): Previous research relating to this work mainly focuses on predicting the MT quality. The first strand is confidence estimation for MT, initiated by (Ueffing et al., 2003), in which posterior probabilities on the word graph or N-best list are used to estimate the quality of MT outputs. The idea is explored more comprehensively in (Blatz et al., 2004). These estimations are often used to rerank the MT output and to optimize it directly. Extensions of this strand are presented in (Quirk, 2004) and (Ueffing and Ney, 2005). The former experimented with confidence estimation with several different learning algorithms; the latter uses word-level confidence measures to determine whether a particular translation choice should be accepted or rejected in an interactive t"
P10-1064,P02-1038,0,0.265413,"Missing"
P10-1064,P03-1021,0,\N,Missing
P11-1124,W05-0909,0,0.0230239,"of the TM itself; a selfcontained coherent TM would facilitate consistent translations. In the future, we plan to investigate the impact of TM quality on translation consistency when using our approach. Furthermore, we will explore methods to promote translation consistency at document level. Moreover, we also plan to experiment with phrase-by-phrase classification instead of sentenceby-sentence classification presented in this paper, in order to obtain more stable classification results. We also plan to label the training examples using other sentence-level evaluation metrics such as Meteor (Banerjee and Lavie, 2005), and to incorporate features that can measure syntactic similarities in training the classifier, in the spirit of (Owczarzak et al., 2007). Currently, only a standard phrase-based SMT system is used, so we plan to test our method on a hierarchical system (Chiang, 2005) to facilitate direct comparison with (Koehn and Senellart, 2010). We will also carry out experiments on other data sets and for more language pairs. Acknowledgments This work is supported by Science Foundation Ireland (Grant No 07/CE/I1142) and part funded under FP7 of the EC within the EuroMatrix+ project (grant No 231720). Th"
P11-1124,P05-1033,0,0.397873,"Missing"
P11-1124,P10-1064,1,0.848606,"Missing"
P11-1124,2010.jec-1.4,0,0.713736,"f translation, as the translation of new input sentences is closely informed and guided (or constrained) by previously translated sentences. There are several different ways of using the translation information derived from fuzzy matches, with the following two being the most widely adopted: 1) to add these translations into a phrase table as in (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009), or 2) to mark up the input sentence using the relevant chunk translations in the fuzzy match, and to use an MT system to translate the parts that are not marked up, as in (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010). It is worth mentioning that translation consistency was not explicitly regarded as their primary motivation in this previous work. Our research follows the direction of the second strand given that consistency can no longer be guaranteed by constructing another phrase table. However, to categorically reuse the translations of matched chunks without any differentiation could generate inferior translations given the fact that the context of these matched chunks in the input sentence could be completely different from the source side of the fuzzy match. To addre"
P11-1124,N03-1017,0,0.080917,"Missing"
P11-1124,P07-2045,0,0.0210111,"Missing"
P11-1124,C08-1064,0,0.0128297,"urces of improvements by marking up the input sentence, we performed some manual analysis of the output. We observe that the improvements can broadly be attributed to two reasons: 1) the use of long phrase pairs which are missing in the phrase table, and 2) deterministically using highly reliable phrase pairs. Phrase-based SMT systems normally impose a limit on the length of phrase pairs for storage and speed considerations. Our method can overcome 1246 this limitation by retrieving and reusing long phrase pairs on the fly. A similar idea, albeit from a different perspective, was explored by (Lopez, 2008), where he proposed to construct a phrase table on the fly for each sentence to be translated. Differently from his approach, our method directly translates part of the input sentence using fuzzy matches retrieved on the fly, with the rest of the sentence translated by the pre-trained MT system. We offer some more insights into the advantages of our method by means of a few examples. Example 1 shows translation improvements by using long phrase pairs. Compared to the reference translation, we can see that for the underlined phrase, the translation without markup contains (i) word ordering erro"
P11-1124,J03-1002,0,0.0258011,"ations of the matched phrases in the input sentence. The remaining words without specified translations will be translated by an MT system. For example, given an input sentence e1 e2 · · · ei ei+1 · · · eI , and a phrase pair < e¯, f¯′ >, e¯ = ′ ei ei+1 , f¯′ = fj′ fj+1 derived from the fuzzy match, we can mark up the input sentence as: ′ ”> e e e1 e2 · · · <tm=“fj′ fj+1 i i+1 < /tm> · · · eI . Our method to constrain the translations using TM fuzzy matches is similar to (Koehn and Senellart, 2010), except that the word alignment between e′ and f ′ is the intersection of bidirectional GIZA++ (Och and Ney, 2003) posterior alignments. We use the intersected word alignment to minimise the noise introduced by word alignment of only one direction in marking up the input sentence. 3.2 Discriminative Learning Whether the translation information from the fuzzy matches should be used or not (i.e. whether the input sentence should be marked up) is determined using a discriminative learning procedure. The translation information refers to the “phrase pairs” derived using the method described in Section 3.1. We cast this problem as a binary classification problem. 3.2.1 SVMs (Cortes and Vapnik, 1995) are binary"
P11-1124,P03-1021,0,0.0304164,"Missing"
P11-1124,W07-0714,1,0.707741,"Missing"
P11-1124,P02-1040,0,0.0964413,"Missing"
P11-1124,2009.mtsummit-papers.14,0,0.673198,"Missing"
P11-1124,2006.amta-papers.25,0,0.0931137,"input sentence should be marked up using relevant phrase pairs derived from the fuzzy match before sending it to the SMT system for translation. The classifier uses features such as the fuzzy match score, the phrase and lexical translation probabilities of these relevant phrase pairs, and additional syntactic dependency features. Ideally the classifier will decide to mark up the input sentence if the translations of the marked phrases are accurate when taken contextual information into account. As large-scale manually annotated data is not available for this task, we use automatic TER scores (Snover et al., 2006) as the measure for training data annotation. We label the training examples as in (3): y= Support Vector Machines l X 1 T ξi w w+C 2 i=1 yi (wT φ(xi ) + b) > 1 − ξi ξi > 0 ( +1 if T ER(w. markup) < T ER(w/o markup) −1 if T ER(w/o markup) ≥ T ER(w. markup) (3) Each instance is associated with a set of features which are discussed in more detail in Section 4. 3.2.2 Classification Confidence Estimation We use the techniques proposed by (Platt, 1999) and improved by (Lin et al., 2007) to convert classification margin to posterior probability, so that we can easily threshold our classifier (cf. Se"
P11-1124,2009.mtsummit-papers.16,0,0.0118041,"e design in Section 4. We report the experimental results in Section 5 and conclude the paper and point out avenues for future research in Section 6. 2 Related Research Despite the fact that TM and MT integration has long existed as a major challenge in the localisation industry, it has only recently received attention in main-stream MT research. One can loosely combine TM and MT at sentence (called segments in TMs) level by choosing one of them (or both) to recommend to the translators using automatic classifiers (He et al., 2010), or simply using fuzzy match score or MT confidence measures (Specia et al., 2009). One can also tightly integrate TM with MT at the sub-sentence level. The basic idea is as follows: given a source sentence to translate, we firstly use a TM system to retrieve the most similar ‘example’ source sentences together with their translations. If matched chunks between input sentence and fuzzy matches can be detected, we can directly re-use the corresponding parts of the translation in the fuzzy matches, and use an MT system to translate the remaining chunks. As a matter of fact, implementing this idea is pretty straightforward: a TM system can easily detect the word alignment betw"
P11-1124,W10-3806,1,0.891749,"Missing"
P16-1010,J93-2003,0,0.0574552,"explicitly guide the selection of subgraphs. In experiments, this model further improves our system. In the future, we will extend this model to allow discontinuity on target sides and explore the possibility of directly encoding reordering information in translation rules. We are also interested in using graphs for neural machine translation to see how it can translate and benefit from graphs. Related Work Starting from sequence-based models, SMT has been benefiting increasingly from complex structures. Sequence-based MT: Since the breakthrough made by IBM on word-based models in the 1990s (Brown et al., 1993), SMT has developed rapidly. The PB model (Koehn et al., 2003) advanced the state-of-the-art by translating multi-word units, which makes it better able to capture local phenomena. However, a major drawback in PBMT is that only continuous phrases are considered. Galley and Manning (2010) extend PBMT by allowing discontinuity. However, without linguistic structure information such as syntax trees, sequence-based models can learn a large amount of phrases which may be unreliable. Tree-based MT: Compared to sequences, trees provide recursive structures over sentences and can handle long-distance"
P16-1010,W08-0336,0,0.0790836,"Missing"
P16-1010,N04-1035,0,0.419182,"duction Statistical machine translation (SMT) starts from sequence-based models. The well-known phrasebased (PB) translation model (Koehn et al., 2003) has significantly advanced the progress of SMT by extending translation units from single words to phrases. By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion. However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . . . pas to English not cannot be learned. To solve this, syntax-based models (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Marcu et al., 2006) take tree structures into consideration to learn translation patterns by using non-terminals for generalization. • We propose to use a graph structure to combine a sequence and a tree (Section 3.1). The 97 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics graph contains both local relations between words from the sequence and long-distance relations from the tree. • • • • We present a translation model to"
P16-1010,W09-2307,0,0.0255999,"C in C out P out P in C in C out C in C out P in Figure 4: An illustration of extracting sparse features for each node in a subgraph during decoding. The decoder segments the graph in Figure 2 into three subgraphs (solid rectangles) and produces a complete translation by combining translations of each subgraph (dashed rectangles). In this figure, the class of a word is randomly assigned. 2004 (MT04) and NIST 2005 (MT05) are two test sets used to evaluate the systems. The Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a graph by adding bigram relations. The DE–EN training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-Test 2011 (WMT11) is taken as a development set while News-Test 2012 (WMT12) and News-Test 2013 (WMT13) are test sets. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then, MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.1 2007). Treelet extends PBMT by taking treelets as the basic tr"
P16-1010,W09-2301,0,0.815184,"add hG(˜ s), ti to R; 8 end 9 if |s˜ |< L then 10 for each unaligned word si adjacent to s˜ do 11 s˜0 = extend s˜ with si ; 12 add s˜0 to Q; 13 end 14 end 15 end 16 end successfully Chenggong South Africa Nanfei Figure 2: An example graph for a Chinese sentence. Each node includes a Chinese word and its English meaning. Dashed red lines are bigram relations. Solid lines are dependency relations. Dotted blue lines are shared by bigram and dependency relations. of continuous words. Phrases connected by bigram relations (i.e. continuous phrases) are known to be useful to improve phrase coverage (Hanneman and Lavie, 2009). By contrast, dependency relations come from dependency structures which model syntactic and semantic relations between words. Phrases whose words are connected by dependency relations (also known as treelets) are linguistic-motivated and thus more reliable (Quirk et al., 2005). By combining these two relations together in graphs, we can make use of both continuous and linguistic-informed discontinuous phrases as long as they are connected subgraphs. 3.2 (lines 1–2), and outputs hG(˜ s), ti if s˜ is covered by a connected subgraph G(˜ s) (lines 6–8). A source phrase can be extended with unali"
P16-1010,P96-1041,0,0.206429,"to right and uses beam search for decoding. DTU extends the PB model by allowing discontinuous phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation of DTU in Moses makes it easier to meaningfully compare systems under the same settings. http://code.google.com/p/mate-tools/ http://www.maltparser.org/ 101 Metric BLEU ↑ METEOR ↑ TER ↓ System ZH–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Tree"
P16-1010,N12-1047,0,0.0597396,"coding. DTU extends the PB model by allowing discontinuous phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation of DTU in Moses makes it easier to meaningfully compare systems under the same settings. http://code.google.com/p/mate-tools/ http://www.maltparser.org/ 101 Metric BLEU ↑ METEOR ↑ TER ↓ System ZH–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT"
P16-1010,C12-1083,0,0.0570256,"Missing"
P16-1010,N13-1003,0,0.0137791,"d by translating an uncovered subgraph instead of a phrase. Positions covered by the subgraph are then marked as translated. 4 ZH–EN Graph Segmentation Model where n.w and n.c are the word and class of the current node n, and n0 .w and n0 .c are the word and class of a node n0 connected to n. C, P , and H denote that the node n0 is in the current subgraph Gi or the adjacent previous subgraph Gi−1 or other previous subgraphs, respectively. Note that we treat the adjacent previous subgraph differently from others since information from the last previous unit is quite useful (Xiong et al., 2006; Cherry, 2013). in and out denote that the edge is an incoming edge or outgoing edge for the current node n. Figure 4 shows an example of extracting sparse features for a subgraph. Inspired by success in using sparse features in SMT (Cherry, 2013), in this paper we lexicalize only on the top-100 most frequent words. In addition, we group source words into 50 classes by using mkcls which should provide useful generalization (Cherry, 2013) for our model. Each derivation in our graph-based translation model implies a sequence of subgraphs (also called a segmentation). By default, similar to PB translation, our"
P16-1010,N03-1017,0,0.406976,"ich combine bigram and dependency relations and propose a graph-based translation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experiments on Chinese–English and German– English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmentation, our system obtains further improvement, especially on German–English. 1 Model C D S (Koehn et al., 2003) (Galley and Manning, 2010) (Quirk et al., 2005) and (Menezes and Quirk, 2005) This work • • • sequence sequence • tree • graph • Table 1: Comparison between our work and previous work in terms of three aspects: keeping continuous phrases (C), allowing discontinuous phrases (D), and input structures (S). However, the expressiveness of these models is confined by hierarchical constraints of the grammars used (Galley and Manning, 2010) since these patterns still cover continuous spans of an input sentence. By contrast, Quirk et al. (2005), Menezes and Quirk (2005) and Xiong et al. (2007) take tr"
P16-1010,P05-1033,0,0.593295,"achine translation (SMT) starts from sequence-based models. The well-known phrasebased (PB) translation model (Koehn et al., 2003) has significantly advanced the progress of SMT by extending translation units from single words to phrases. By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion. However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . . . pas to English not cannot be learned. To solve this, syntax-based models (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Marcu et al., 2006) take tree structures into consideration to learn translation patterns by using non-terminals for generalization. • We propose to use a graph structure to combine a sequence and a tree (Section 3.1). The 97 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics graph contains both local relations between words from the sequence and long-distance relations from the tree. • • • • We present a translation model to translate a g"
P16-1010,2005.iwslt-1.8,0,0.0448647,"r to those models, our model is weak at phrase reordering as well. However, we are interesting in the potential power of our model by incorporating lexical reordering (LR) models and comparing it with syntax-based models. Table 5 shows BLEU scores of the hierarchical phrase-based (HPB) system (Chiang, 2005) in Moses6 and GBMT combined with a word-based System GBMT+LR HPB ZH–EN MT04 MT05 36.0 36.1 33.9 34.1 DE–EN WMT12 WMT13 20.6 20.3 23.6 22.8 Table 5: BLEU scores of a Moses hierarchical phrase-based system (HPB) and our system (GBMT) with a word-based lexical reordering model (LR). LR model (Koehn et al., 2005). We find that the LR model significantly improves our system. GBMT+LR is comparable with the Moses HPB model on Chinese–English and better than HPB on German–English. 5.3 Examples Figure 6 shows three examples from MT04 to better explain the differences of each system. Example 1 shows that systems which allow discontinuous phrases (namely Treelet, DTU, GBMT, and GSM) successfully translate a Chinese collocation “Yu . . . Wuguan” to “have nothing to do with” while PBMT fails to catch the generalization since it only allows continuous phrases. In Example 2, Treelet translates a discontinuous ph"
P16-1010,P11-2031,0,0.0430053,"–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM 33.2 33.8∗ 34.5∗ 34.7∗ 34.9∗+ 32.1 31.9 32.3∗ 32.4∗+ 32.7∗+ 60.6 60.1∗ 60.0∗ 59.8∗+ 60.5 19.5 19.6 19.8∗ 19.8∗ 20.3∗+ 28.0 28.0 28.2∗ 28.2∗ 28.5∗+ 63.7 63.2∗ 63.5∗ 63.5∗ 63.1∗+ 31.8 31.7 32.3∗ 32.4∗ 32.7∗+ 32.3 31.8 32.4 32.5∗ 32.6∗+ 61.6 61.4 61.5 61.3∗ 62.1 21.9 22.1∗ 22.3∗ 22.4∗ 22.9∗+ 29.2 29.1 29.5∗ 29.4∗ 29.8∗+ 60.2 59.6∗ 59.8∗ 59.8∗ 59.3∗+ Table 3: Metric scores for all systems on Chinese–English (ZH–EN) and German–English (DE–EN). Each score is an average over three MIRA runs (Clark et al., 2011). ∗ means a system is significantly better than PBMT at p ≤ 0.01. Bold figures mean a system is significantly better than Treelet at p ≤ 0.01. + means a system is significantly better than DTU at p ≤ 0.01. In this table, we mark a system by comparing it with previous ones. 5.2 Results and Discussion Table 3 shows our evaluation results. We find that our GBMT system is significantly better than PBMT as measured by all three metrics across all test sets. Specifically, the improvements are up to +1.5/+0.5 BLEU, +0.3/+0.2 METEOR, and -0.8/0.4 TER on ZH–EN and DE–EN, respectively. This improvement"
P16-1010,P07-2045,0,0.0121936,"Missing"
P16-1010,J03-1002,0,0.0243166,"(Nivre and Nilsson, 2005). 5.1 2007). Treelet extends PBMT by taking treelets as the basic translation units (Quirk et al., 2005; Menezes and Quirk, 2005). We implement a Treelet model in Moses which produces translations from left to right and uses beam search for decoding. DTU extends the PB model by allowing discontinuous phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation"
P16-1010,J10-4005,0,0.0182637,"parse features to explicitly model the graph segmentation (Section 4). These features are based on edges in the input graph, each of which is either inside a subgraph or connects the subgraph with a previous subgraph. 2 •• I p(t1 |G(˜ sI1 )) I Y = p(ti |G(˜ sai ))d(G(˜ sai ), G(˜ sai−1 )) (1) i=1 The target sentence T is broken into I phrases t1 · · · tI , each of which is a translation of a source phrase sai . d is a distance-based reordering model. Note that in the basic PB model, the phrase segmentation is not explicitly modeled which means that different segmentations are treated equally (Koehn, 2010). The performance of PB translation relies on the quality of phrase pairs in a translation table. Conventionally, a phrase pair hs, ti has two properties: (i) s and t are continuous phrases. (ii) hs, ti is consistent with a word alignment A (Och and Ney, 2004): ∀(i, j) ∈ A, si ∈ s ⇔ tj ∈ t and ∃si ∈ s, tj ∈ t, (i, j) ∈ A. PB decoders generate hypotheses (partial translations) from left to right. Each hypothesis maintains a coverage vector to indicate which source words have been translated so far. A hypothesis can be extended on the right by translating an ≈ i=1 I Y (2) sai ))d(˜ sai , s˜ai−1"
P16-1010,J04-4002,0,0.324209,"G(˜ sai ))d(G(˜ sai ), G(˜ sai−1 )) (1) i=1 The target sentence T is broken into I phrases t1 · · · tI , each of which is a translation of a source phrase sai . d is a distance-based reordering model. Note that in the basic PB model, the phrase segmentation is not explicitly modeled which means that different segmentations are treated equally (Koehn, 2010). The performance of PB translation relies on the quality of phrase pairs in a translation table. Conventionally, a phrase pair hs, ti has two properties: (i) s and t are continuous phrases. (ii) hs, ti is consistent with a word alignment A (Och and Ney, 2004): ∀(i, j) ∈ A, si ∈ s ⇔ tj ∈ t and ∃si ∈ s, tj ∈ t, (i, j) ∈ A. PB decoders generate hypotheses (partial translations) from left to right. Each hypothesis maintains a coverage vector to indicate which source words have been translated so far. A hypothesis can be extended on the right by translating an ≈ i=1 I Y (2) sai ))d(˜ sai , s˜ai−1 ) p(ti |G(˜ i=1 where G(˜ si ) denotes a connected source subgraph which covers a (discontinuous) phrase s˜i . 3.1 Building Graphs As a more powerful and natural structure for sentence modeling, a graph can model various kinds of word-relations together in a u"
P16-1010,W14-4014,1,0.764896,"hat only continuous phrases are considered. Galley and Manning (2010) extend PBMT by allowing discontinuity. However, without linguistic structure information such as syntax trees, sequence-based models can learn a large amount of phrases which may be unreliable. Tree-based MT: Compared to sequences, trees provide recursive structures over sentences and can handle long-distance relations. Typically, trees used in SMT are either phrasal structures (Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006) or dependency structures (Menezes and Quirk, 2005; Xiong et al., 2007; Xie et al., 2011; Li et al., 2014). However, conventional treebased models only use linguistically well-formed phrases. Although they are more reliable in theory, discarding all phrase pairs which are not linguistically motivated is an overly harsh decision. Therefore, exploring more translation rules usually can significantly improve translation performance (Marcu et al., 2006; DeNeefe et al., 2007; Wang et al., 2007; Mi et al., 2008). Graph-based MT: Compared to sequences and trees, graphs are more general and can represent more relations between words. In recent years, Acknowledgments This research has received funding from"
P16-1010,P02-1040,0,0.106811,"us phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation of DTU in Moses makes it easier to meaningfully compare systems under the same settings. http://code.google.com/p/mate-tools/ http://www.maltparser.org/ 101 Metric BLEU ↑ METEOR ↑ TER ↓ System ZH–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM 33.2 33.8∗ 34.5∗ 34.7∗"
P16-1010,D15-1004,1,0.652183,"tors of the community Guangyi . and draw on collective wisdom . Figure 6: Translation examples from MT04 produced by different systems. Each source sentence is annotated by dependency relations and additional bigram relations (dotted red edges). We also annotate phrase alignments produced by our system GSM. 104 graphs have been drawing quite a lot of attention from researchers. Jones et al. (2012) propose a hypergraph-based translation model where hypergraphs are taken as a meaning representation of sentences. However, large corpora with annotated hypergraphs are not readily available for MT. Li et al. (2015) use an edge replacement grammar to translate dependency graphs which are converted from dependency trees by labeling edges. However, their model only focuses on subgraphs which cover continuous phrases. linguistically motivated and could be unreliable. By disallowing phrases which are not connected in the input graph, GBMT and GSM produce better translations. Example 3 illustrates that our graph segmentation model helps to select better subgraphs. After obtaining a partial translation “the government must”, GSM chooses to translate a subgraph which covers a discontinuous phrase “Jixu . . . Zu"
P16-1010,P05-1034,0,0.874736,"propose a graph-based translation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experiments on Chinese–English and German– English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmentation, our system obtains further improvement, especially on German–English. 1 Model C D S (Koehn et al., 2003) (Galley and Manning, 2010) (Quirk et al., 2005) and (Menezes and Quirk, 2005) This work • • • sequence sequence • tree • graph • Table 1: Comparison between our work and previous work in terms of three aspects: keeping continuous phrases (C), allowing discontinuous phrases (D), and input structures (S). However, the expressiveness of these models is confined by hierarchical constraints of the grammars used (Galley and Manning, 2010) since these patterns still cover continuous spans of an input sentence. By contrast, Quirk et al. (2005), Menezes and Quirk (2005) and Xiong et al. (2007) take treelets from dependency trees as the basic transl"
P16-1010,D14-1209,0,0.013343,"search strategy to reduce the size of the decoding space. Hypotheses which cover the same number of source words are grouped in a stack. Hypotheses can be pruned according to their partial translation cost and an estimated future cost. • Experiments (Section 5) on Chinese–English and German–English tasks show that our model is significantly better than the PB model. After incorporating the segmentation model, our system achieves still further improvement. p(t1 |sI1 ) = •• ••• Figure 1: Beam search for phrase-based MT. • denotes a covered source position while indicates an uncovered position (Liu and Huang, 2014). • We present a set of sparse features to explicitly model the graph segmentation (Section 4). These features are based on edges in the input graph, each of which is either inside a subgraph or connects the subgraph with a previous subgraph. 2 •• I p(t1 |G(˜ sI1 )) I Y = p(ti |G(˜ sai ))d(G(˜ sai ), G(˜ sai−1 )) (1) i=1 The target sentence T is broken into I phrases t1 · · · tI , each of which is a translation of a source phrase sai . d is a distance-based reordering model. Note that in the basic PB model, the phrase segmentation is not explicitly modeled which means that different segmentati"
P16-1010,2006.amta-papers.25,0,0.0271408,"iscontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function growdiag-final-and. We use SRILM (Stolcke, 2002) to train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Batch MIRA (Cherry and Foster, 2012) is used to tune weights. BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006) are used for evaluation. Settings In this paper, we mainly report results from five systems under the same configuration. PBMT is built by the PB model in Moses (Koehn et al., 2 3 4 The re-implementation of DTU in Moses makes it easier to meaningfully compare systems under the same settings. http://code.google.com/p/mate-tools/ http://www.maltparser.org/ 101 Metric BLEU ↑ METEOR ↑ TER ↓ System ZH–EN MT04 MT05 DE–EN WMT12 WMT13 PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM PBMT Treelet DTU GBMT GSM 33.2 33.8∗ 34.5∗ 34.7∗ 34.9∗+ 32.1 31.9 32.3∗ 32.4∗+ 32.7∗+ 60.6 60.1∗ 60.0∗ 59.8∗+ 60.5 1"
P16-1010,P06-1077,1,0.875045,"tion (SMT) starts from sequence-based models. The well-known phrasebased (PB) translation model (Koehn et al., 2003) has significantly advanced the progress of SMT by extending translation units from single words to phrases. By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion. However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . . . pas to English not cannot be learned. To solve this, syntax-based models (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Marcu et al., 2006) take tree structures into consideration to learn translation patterns by using non-terminals for generalization. • We propose to use a graph structure to combine a sequence and a tree (Section 3.1). The 97 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics graph contains both local relations between words from the sequence and long-distance relations from the tree. • • • • We present a translation model to translate a graph (Section 3)."
P16-1010,W06-1606,0,0.201369,"from sequence-based models. The well-known phrasebased (PB) translation model (Koehn et al., 2003) has significantly advanced the progress of SMT by extending translation units from single words to phrases. By using phrases, PB models can capture local phenomena, such as word order, word deletion, and word insertion. However, one of the significant weaknesses in conventional PB models is that only continuous phrases are used, so generalizations such as French ne . . . pas to English not cannot be learned. To solve this, syntax-based models (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Marcu et al., 2006) take tree structures into consideration to learn translation patterns by using non-terminals for generalization. • We propose to use a graph structure to combine a sequence and a tree (Section 3.1). The 97 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 97–107, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics graph contains both local relations between words from the sequence and long-distance relations from the tree. • • • • We present a translation model to translate a graph (Section 3). The model segments th"
P16-1010,D07-1078,0,0.0867248,"Missing"
P16-1010,2005.mtsummit-ebmt.13,0,0.681184,"nslation model. The model segments an input graph into connected subgraphs, each of which may cover a discontinuous phrase. We use beam search to combine translations of each subgraph left-to-right to produce a complete translation. Experiments on Chinese–English and German– English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores. By explicitly modeling the graph segmentation, our system obtains further improvement, especially on German–English. 1 Model C D S (Koehn et al., 2003) (Galley and Manning, 2010) (Quirk et al., 2005) and (Menezes and Quirk, 2005) This work • • • sequence sequence • tree • graph • Table 1: Comparison between our work and previous work in terms of three aspects: keeping continuous phrases (C), allowing discontinuous phrases (D), and input structures (S). However, the expressiveness of these models is confined by hierarchical constraints of the grammars used (Galley and Manning, 2010) since these patterns still cover continuous spans of an input sentence. By contrast, Quirk et al. (2005), Menezes and Quirk (2005) and Xiong et al. (2007) take treelets from dependency trees as the basic translation units. These treelets ar"
P16-1010,D11-1020,1,0.811514,"wback in PBMT is that only continuous phrases are considered. Galley and Manning (2010) extend PBMT by allowing discontinuity. However, without linguistic structure information such as syntax trees, sequence-based models can learn a large amount of phrases which may be unreliable. Tree-based MT: Compared to sequences, trees provide recursive structures over sentences and can handle long-distance relations. Typically, trees used in SMT are either phrasal structures (Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006) or dependency structures (Menezes and Quirk, 2005; Xiong et al., 2007; Xie et al., 2011; Li et al., 2014). However, conventional treebased models only use linguistically well-formed phrases. Although they are more reliable in theory, discarding all phrase pairs which are not linguistically motivated is an overly harsh decision. Therefore, exploring more translation rules usually can significantly improve translation performance (Marcu et al., 2006; DeNeefe et al., 2007; Wang et al., 2007; Mi et al., 2008). Graph-based MT: Compared to sequences and trees, graphs are more general and can represent more relations between words. In recent years, Acknowledgments This research has rec"
P16-1010,P08-1023,1,0.780891,"ees used in SMT are either phrasal structures (Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006) or dependency structures (Menezes and Quirk, 2005; Xiong et al., 2007; Xie et al., 2011; Li et al., 2014). However, conventional treebased models only use linguistically well-formed phrases. Although they are more reliable in theory, discarding all phrase pairs which are not linguistically motivated is an overly harsh decision. Therefore, exploring more translation rules usually can significantly improve translation performance (Marcu et al., 2006; DeNeefe et al., 2007; Wang et al., 2007; Mi et al., 2008). Graph-based MT: Compared to sequences and trees, graphs are more general and can represent more relations between words. In recent years, Acknowledgments This research has received funding from the People Programme (Marie Curie Actions) of the European Union’s Framework Programme (FP7/20072013) under REA grant agreement no 317471 and the European Union’s Horizon 2020 research and innovation programme under grant agreement no 645452 (QT21). The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European"
P16-1010,P06-1066,1,0.759455,"ypothesis is extended by translating an uncovered subgraph instead of a phrase. Positions covered by the subgraph are then marked as translated. 4 ZH–EN Graph Segmentation Model where n.w and n.c are the word and class of the current node n, and n0 .w and n0 .c are the word and class of a node n0 connected to n. C, P , and H denote that the node n0 is in the current subgraph Gi or the adjacent previous subgraph Gi−1 or other previous subgraphs, respectively. Note that we treat the adjacent previous subgraph differently from others since information from the last previous unit is quite useful (Xiong et al., 2006; Cherry, 2013). in and out denote that the edge is an incoming edge or outgoing edge for the current node n. Figure 4 shows an example of extracting sparse features for a subgraph. Inspired by success in using sparse features in SMT (Cherry, 2013), in this paper we lexicalize only on the top-100 most frequent words. In addition, we group source words into 50 classes by using mkcls which should provide useful generalization (Cherry, 2013) for our model. Each derivation in our graph-based translation model implies a sequence of subgraphs (also called a segmentation). By default, similar to PB t"
P16-1010,P05-1013,0,0.0618967,"8) is used to segment Chinese sentences. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into a projective dependency tree which is then converted to a graph by adding bigram relations. The DE–EN training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-Test 2011 (WMT11) is taken as a development set while News-Test 2012 (WMT12) and News-Test 2013 (WMT13) are test sets. We use mate-tools2 to perform morphological analysis and parse German sentences (Bohnet, 2010). Then, MaltParser3 converts a parse result into a projective dependency tree (Nivre and Nilsson, 2005). 5.1 2007). Treelet extends PBMT by taking treelets as the basic translation units (Quirk et al., 2005; Menezes and Quirk, 2005). We implement a Treelet model in Moses which produces translations from left to right and uses beam search for decoding. DTU extends the PB model by allowing discontinuous phrases (Galley and Manning, 2010). We implement DTU with source discontinuity in Moses.4 GBMT is our basic graph-based translation system while GSM adds the graph segmentation model into GBMT. Both systems are implemented in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with th"
P16-1010,P02-1038,0,0.0689957,"A source phrase can be extended with unaligned source words which are adjacent to the phrase (lines 9– 14). We use a queue Q to store all phrases which are consistently aligned to the same target phrase (line 3). Training Different from PB translation, the basic translation units in our model are subgraphs. Thus, during training, we extract subgraph–phrase pairs instead of phrase pairs on parallel graph–string sentences associated with word alignments.1 An example of a translation rule is as follows: FIFA Shijiebei Juxing 3.3 Model and Decoding We define our model in the log-linear framework (Och and Ney, 2002) over a derivation D = r1 r2 · · · rN , as in Equation (3): Y p(D) ∝ φi (D)λi (3) FIFA World Cup was held i Note that the source side of a rule in our model is a graph which can be used to cover either a continuous phrase or a discontinuous phrase according to its match in an input graph during decoding. The algorithm for extracting translation rules is shown in Algorithm 1. This algorithm traverses each phrase pair h˜ s, ti, which is within a length limit and consistent with a given word alignment where ri are translation rules, φi are features defined on derivations and λi are feature weight"
P16-1010,W07-0706,1,0.855471,"C D S (Koehn et al., 2003) (Galley and Manning, 2010) (Quirk et al., 2005) and (Menezes and Quirk, 2005) This work • • • sequence sequence • tree • graph • Table 1: Comparison between our work and previous work in terms of three aspects: keeping continuous phrases (C), allowing discontinuous phrases (D), and input structures (S). However, the expressiveness of these models is confined by hierarchical constraints of the grammars used (Galley and Manning, 2010) since these patterns still cover continuous spans of an input sentence. By contrast, Quirk et al. (2005), Menezes and Quirk (2005) and Xiong et al. (2007) take treelets from dependency trees as the basic translation units. These treelets are connected and may cover discontinuous phrases. However, their models lack the ability to handle continuous phrases which are not connected in trees but could in fact be extremely important to system performance (Koehn et al., 2003). Galley and Manning (2010) directly extract discontinuous phrases from input sequences. However, without imposing additional restrictions on discontinuity, the amount of extracted rules can be very large and unreliable. Different from previous work (as shown in Table 1), in this"
P16-1010,N10-1140,0,\N,Missing
P16-1010,W11-2107,0,\N,Missing
P16-2045,N12-1047,0,0.0457588,"Missing"
P16-2045,J03-1002,0,0.00586571,"pair applied to translate an input phrase, WANG finds its corresponding phrase pairs in a TM instance and then extracts features which are directly added to the loglinear framework (Och and Ney, 2002) as sparse features. We build three systems based on our approach: CWLadd only uses constraints from addition; CWLsub only uses constraints from subtraction; CWLboth uses constraints from both. Table 1 shows a summary of our datasets. The EN–ZH dataset is a translation memory from Symantec. Our EN–FR dataset is from the publicly available JRC-Acquis corpus.4 Word alignment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and. After obtaining the segmentation, we create a constrained input for each f i using subtraction and add it to the lattice by creating a path covering f i . The path contains one or more edges, each of which is labeled either by an input word or a constraint in the constrained input. 2.3 Sentences Decoding The decoder for integrating word lattices into the phrase-based model (Koehn et al., 2003) works similarly to the phrase-based decoder, except that it tracks nodes instead of words (Dyer et al., 2008): given the topological order of nodes in a latt"
P16-2045,P08-1115,0,0.0279922,"corpus.4 Word alignment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and. After obtaining the segmentation, we create a constrained input for each f i using subtraction and add it to the lattice by creating a path covering f i . The path contains one or more edges, each of which is labeled either by an input word or a constraint in the constrained input. 2.3 Sentences Decoding The decoder for integrating word lattices into the phrase-based model (Koehn et al., 2003) works similarly to the phrase-based decoder, except that it tracks nodes instead of words (Dyer et al., 2008): given the topological order of nodes in a lattice, the decoder builds a translation hypothesis from left to right by selecting a range of untranslated nodes. The decoder for a constrained lattice works similarly except that, for a constrained edge, the decoder can only build its translation directly from the constraint. For example, in Figure 2, the translation of the edge “1 → 5” is “, le texte du deuxi`eme alin´ea”. 4 http://ipsc.jrc.ec.europa.eu/index. php?id=198 277 EN–ZH BLEU↑ TER↓ Systems PB EN–FR BLEU↑ TER↓ 44.3 40.0 65.7 Sentence-Level Combination 25.9 45.6* 39.2* 64.2 49.4* 36.3* 64"
P16-2045,J04-4002,0,0.0292856,"ntences are taken from Koehn and Senellart (2010). 1. Building an initial lattice for an input sentence. This produces a chain. 2.2 Subtraction In addition, matched input words are directly replaced by their translations from a retrieved TM, which means that addition follows the word order of an input sentence. This property makes it easy to obtain constraints for an input phrase. For an input phrase f , we firstly find its matched phrase f 0 from f 0 via string edits3 between f and f 0 , so that f = f 0 . Then, we extract its translation e0 from e0 , which is consistent with the alignment A (Och and Ney, 2004). To build a lattice using addition, we directly add a new edge to the lattice which covers f and is labeled by e0 . For example, dash-dotted lines in Figure 2 are labeled by constraints from addition. In subtraction, mismatched input words in f are inserted into e0 and mismatched words in e0 are removed. The inserted position is determined by A. The advantage of subtraction is that it keeps the word order of e0 . This is important since the reordering of target words is one of the fundamental problems in SMT, especially for language pairs which have a high degree of syntactic reordering. Howe"
P16-2045,P10-1064,1,0.868718,"Missing"
P16-2045,P02-1040,0,0.0950648,"Missing"
P16-2045,2010.jec-1.4,0,0.405533,"g a constrained word lattice, which encodes input phrases and TM constraints together, to combine SMT and TM at phrase-level. Experiments on English– Chinese and English–French show that our approach is significantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination. 1 Introduction The combination of statistical machine translation (SMT) and translation memory (TM) has proven to be beneficial in improving translation quality and has drawn attention from many researchers (Bic¸ici and Dymetman, 2008; He et al., 2010; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013; Li et al., 2014). Among various combination approaches, constrained translation (Koehn and Senellart, 2010; Ma et al., 2011) is a simple one and can be readily adopted. Given an input sentence, constrained translation retrieves similar TM instances and uses matched segments to constrain the translation space of the input by generating a constrained input. Then an SMT engine is used to search for a complete translation of the constrained input. Despite its effectiveness in improving SMT, previous constrained translation works at the sentence-level, which me"
P16-2045,2006.amta-papers.25,0,0.0794351,"Missing"
P16-2045,N03-1017,0,0.0356054,"–ZH dataset is a translation memory from Symantec. Our EN–FR dataset is from the publicly available JRC-Acquis corpus.4 Word alignment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and. After obtaining the segmentation, we create a constrained input for each f i using subtraction and add it to the lattice by creating a path covering f i . The path contains one or more edges, each of which is labeled either by an input word or a constraint in the constrained input. 2.3 Sentences Decoding The decoder for integrating word lattices into the phrase-based model (Koehn et al., 2003) works similarly to the phrase-based decoder, except that it tracks nodes instead of words (Dyer et al., 2008): given the topological order of nodes in a lattice, the decoder builds a translation hypothesis from left to right by selecting a range of untranslated nodes. The decoder for a constrained lattice works similarly except that, for a constrained edge, the decoder can only build its translation directly from the constraint. For example, in Figure 2, the translation of the edge “1 → 5” is “, le texte du deuxi`eme alin´ea”. 4 http://ipsc.jrc.ec.europa.eu/index. php?id=198 277 EN–ZH BLEU↑ T"
P16-2045,P07-2045,0,0.0129872,"ted position of a mismatched unaligned word depends on the alignment of the word before it. Train Dev Test Train Dev Test EN–FR W/S (EN) W/S (ZH) 84,871 734 943 13.5 14.3 17.4 13.8 14.5 17.4 Sentences W/S (EN) W/S (FR) 751,548 2,665 2,655 26.9 26.8 27.1 29.3 29.2 29.4 Table 1: Summary of English–Chinese (EN–ZH) and English–French (EN–FR) datasets 3 4. No smaller tuples may be extracted without violating restrictions 1–3. This allows us to obtain a unique segmentation where each tuple is minimal. Experiment In our experiments, a baseline system PB is built with the phrase-based model in Moses (Koehn et al., 2007). We compare our approach with three other combination methods. ADD combines PB with addition (Ma et al., 2011), while SUB combines PB with subtraction (Koehn and Senellart, 2010). WANG combines SMT and TM at phraselevel during decoding (Wang et al., 2013; Li et al., 2014). For each phrase pair applied to translate an input phrase, WANG finds its corresponding phrase pairs in a TM instance and then extracts features which are directly added to the loglinear framework (Och and Ney, 2002) as sparse features. We build three systems based on our approach: CWLadd only uses constraints from addition"
P16-2045,P13-1002,0,0.0680504,"input phrases and TM constraints together, to combine SMT and TM at phrase-level. Experiments on English– Chinese and English–French show that our approach is significantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination. 1 Introduction The combination of statistical machine translation (SMT) and translation memory (TM) has proven to be beneficial in improving translation quality and has drawn attention from many researchers (Bic¸ici and Dymetman, 2008; He et al., 2010; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013; Li et al., 2014). Among various combination approaches, constrained translation (Koehn and Senellart, 2010; Ma et al., 2011) is a simple one and can be readily adopted. Given an input sentence, constrained translation retrieves similar TM instances and uses matched segments to constrain the translation space of the input by generating a constrained input. Then an SMT engine is used to search for a complete translation of the constrained input. Despite its effectiveness in improving SMT, previous constrained translation works at the sentence-level, which means that matched segments in a TM in"
P16-2045,2014.amta-researchers.19,1,0.627439,"M constraints together, to combine SMT and TM at phrase-level. Experiments on English– Chinese and English–French show that our approach is significantly better than previous combination methods, including sentence-level constrained translation and a recent phrase-level combination. 1 Introduction The combination of statistical machine translation (SMT) and translation memory (TM) has proven to be beneficial in improving translation quality and has drawn attention from many researchers (Bic¸ici and Dymetman, 2008; He et al., 2010; Koehn and Senellart, 2010; Ma et al., 2011; Wang et al., 2013; Li et al., 2014). Among various combination approaches, constrained translation (Koehn and Senellart, 2010; Ma et al., 2011) is a simple one and can be readily adopted. Given an input sentence, constrained translation retrieves similar TM instances and uses matched segments to constrain the translation space of the input by generating a constrained input. Then an SMT engine is used to search for a complete translation of the constrained input. Despite its effectiveness in improving SMT, previous constrained translation works at the sentence-level, which means that matched segments in a TM instance are either"
P16-2045,P11-1124,1,0.820404,"Missing"
P16-2045,P02-1038,0,0.264534,"minimal. Experiment In our experiments, a baseline system PB is built with the phrase-based model in Moses (Koehn et al., 2007). We compare our approach with three other combination methods. ADD combines PB with addition (Ma et al., 2011), while SUB combines PB with subtraction (Koehn and Senellart, 2010). WANG combines SMT and TM at phraselevel during decoding (Wang et al., 2013; Li et al., 2014). For each phrase pair applied to translate an input phrase, WANG finds its corresponding phrase pairs in a TM instance and then extracts features which are directly added to the loglinear framework (Och and Ney, 2002) as sparse features. We build three systems based on our approach: CWLadd only uses constraints from addition; CWLsub only uses constraints from subtraction; CWLboth uses constraints from both. Table 1 shows a summary of our datasets. The EN–ZH dataset is a translation memory from Symantec. Our EN–FR dataset is from the publicly available JRC-Acquis corpus.4 Word alignment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and. After obtaining the segmentation, we create a constrained input for each f i using subtraction and add it to the lattice by creating a p"
P18-3010,P16-1078,0,0.0211633,"ords independently) does not perform better than the baseline system. A similar observation was made by Li et al (2017), who incorporate the linearized parse trees of the source sentences into ZH–EN NMT systems. They propose three different sorts of encoders: (a) a parallel RNN, (b) a hierarchical RNN, and (c) a mixed RNN. Like Nadejde et al. (2017), Li et al (2017) observe that the mixed RNN (the simplest RNN encoder), where words and label annotation vectors are simply stitched together in the input sequences, yields the best performance with a significant improvement (1.4 BLEU). Similarly, Eriguchi et al. (2016) integrated syntactic information in the form of linearized parse trees by using an encoder that computes vector representations for each phrase in the source tree. They focus on source-side syntactic information based on Head-Driven Phrase Structure Grammar (Sag et al., 1999) where target words are aligned not only with the corresponding source words but with the entire source phrase. Wu et al. (2017) focus on incorporating sourceside long distance dependencies by enriching each source state with global dependency structure. To the best of our knowledge, there has not been any work on explici"
P18-3010,P17-2012,0,0.0171593,"ine Translation (SMT), the previous state-of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (Bentivogli et al., 2016). Although NMT seems to partially ‘learn’ or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic (Bentivogli et al., 2016). More recent work showed that explicitly (Sennrich and Haddow, 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017) or implicitly (Eriguchi et al., 2017) modeling extra syntactic information into an NMT system on the source (and/or target) side could lead to improvements in translation quality. When integrating linguistic information into an MT system, following the central role assigned to syntax by many linguists, the focus has been mainly on the integration of syntactic features. Although there has been some work on semantic features for SMT (Banchs and Costa-Juss`a, 2011), so 67 Proceedings of ACL 2018, Student Research Workshop, pages 67–73 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tures le"
P18-3010,P17-2021,0,0.0145135,"et al., 2014). Compared to Statistical Machine Translation (SMT), the previous state-of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (Bentivogli et al., 2016). Although NMT seems to partially ‘learn’ or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic (Bentivogli et al., 2016). More recent work showed that explicitly (Sennrich and Haddow, 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017) or implicitly (Eriguchi et al., 2017) modeling extra syntactic information into an NMT system on the source (and/or target) side could lead to improvements in translation quality. When integrating linguistic information into an MT system, following the central role assigned to syntax by many linguists, the focus has been mainly on the integration of syntactic features. Although there has been some work on semantic features for SMT (Banchs and Costa-Juss`a, 2011), so 67 Proceedings of ACL 2018, Student Research Workshop, pages 67–73 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association"
P18-3010,E12-1068,0,0.0184641,"an be beneficial for NMT providing it with an extra ability to generalize and thus better learn more complex semanticosyntactic patters. The remainder of this paper is structured as follows: First, in Section 2, the related work is discussed. Next, Section 3 presents the semantic and syntactic features used. The experimental set-up is described in Section 4 followed by the results in Section 5. Finally, We conclude and present some of the ideas for future work in Section 6. 2 Related Work In SMT, various linguistic features such as stems (Toutanova et al., 2008) lemmas (Mareˇcek et al., 2011; Fraser et al., 2012), POStags (Avramidis and Koehn, 2008), dependency labels (Avramidis and Koehn, 2008) and supertags (Hassan et al., 2007; Haque et al., 2009) are integrated using pre- or post-processing techniques often involving factored phrase-based models (Koehn and Hoang, 2007). Compared to factored NMT models, factored SMT models have some disadvantages: (a) adding factors increases the sparsity of the models, (b) the n-grams limit the size of context that is taken into account, and (c) features are assumed to be independent of each other. However, adding syntactic features to SMT systems led to improveme"
P18-3010,N06-2001,0,0.0595315,"features are assumed to be independent of each other. However, adding syntactic features to SMT systems led to improvements with respect to word order and morphological agreement (Williams and Koehn, 2012; Sennrich, 2015). One of the main strengths of NMT is its strong ability to generalize. The integration of linguistic features can be handled in a flexible way without creating sparsity issues or limiting context information (within the same sentence). Furthermore, the encoder and attention layers can be shared between features. By representing the encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006), Sennrich and Haddow (2016) generalized the embedding layer in such a way that an arbitrary number of linguistic features can be explicitly integrated. They then investigated whether features such as lemmas, subword tags, morphological features, POS tags and dependency labels could be useful for NMT systems or whether their inclusion is redundant. Similarly, on the syntax level, Shi et al. (2016) show that although NMT systems are able to partially learn syntactic information, more complex patterns remain problematic. Furthermore, sometimes information is present in the encoding vectors but i"
P18-3010,2009.eamt-1.32,1,0.479386,"remainder of this paper is structured as follows: First, in Section 2, the related work is discussed. Next, Section 3 presents the semantic and syntactic features used. The experimental set-up is described in Section 4 followed by the results in Section 5. Finally, We conclude and present some of the ideas for future work in Section 6. 2 Related Work In SMT, various linguistic features such as stems (Toutanova et al., 2008) lemmas (Mareˇcek et al., 2011; Fraser et al., 2012), POStags (Avramidis and Koehn, 2008), dependency labels (Avramidis and Koehn, 2008) and supertags (Hassan et al., 2007; Haque et al., 2009) are integrated using pre- or post-processing techniques often involving factored phrase-based models (Koehn and Hoang, 2007). Compared to factored NMT models, factored SMT models have some disadvantages: (a) adding factors increases the sparsity of the models, (b) the n-grams limit the size of context that is taken into account, and (c) features are assumed to be independent of each other. However, adding syntactic features to SMT systems led to improvements with respect to word order and morphological agreement (Williams and Koehn, 2012; Sennrich, 2015). One of the main strengths of NMT is i"
P18-3010,P08-1087,0,0.037525,"ing it with an extra ability to generalize and thus better learn more complex semanticosyntactic patters. The remainder of this paper is structured as follows: First, in Section 2, the related work is discussed. Next, Section 3 presents the semantic and syntactic features used. The experimental set-up is described in Section 4 followed by the results in Section 5. Finally, We conclude and present some of the ideas for future work in Section 6. 2 Related Work In SMT, various linguistic features such as stems (Toutanova et al., 2008) lemmas (Mareˇcek et al., 2011; Fraser et al., 2012), POStags (Avramidis and Koehn, 2008), dependency labels (Avramidis and Koehn, 2008) and supertags (Hassan et al., 2007; Haque et al., 2009) are integrated using pre- or post-processing techniques often involving factored phrase-based models (Koehn and Hoang, 2007). Compared to factored NMT models, factored SMT models have some disadvantages: (a) adding factors increases the sparsity of the models, (b) the n-grams limit the size of context that is taken into account, and (c) features are assumed to be independent of each other. However, adding syntactic features to SMT systems led to improvements with respect to word order and mo"
P18-3010,P07-1037,1,0.61147,"Missing"
P18-3010,W11-1014,0,0.0723889,"Missing"
P18-3010,P14-1062,0,0.010358,"ags have the potential to be useful especially in combination with syntactic information. In this paper we incorporate semantic supersensetags and syntactic supertag features into EN–FR and EN–DE factored NMT systems. In experiments on various test sets, we observe that such features (and particularly when combined) help the NMT model training to converge faster and improve the model quality according to the BLEU scores. 1 Introduction Neural Machine Translation (NMT) models have recently become the state-of-the art in the field of Machine Translation (Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner et al., 2014; Sutskever et al., 2014). Compared to Statistical Machine Translation (SMT), the previous state-of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (Bentivogli et al., 2016). Although NMT seems to partially ‘learn’ or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic (Bentivogli et al., 2016). More recent work showed that explicitly (Sennrich and Haddow, 2016; Nadejde et al., 2017; Bastings et"
P18-3010,D17-1209,0,0.0288475,"Missing"
P18-3010,2005.mtsummit-papers.11,0,0.0351352,"n the newstest2013 than on the Europarl test set. Figure 1 compares the BPE-ed baseline system (BPE) with the supertag-supersensetag system (CCG– SST) automatically evaluated on the newstest2013 (in terms of BLEU (Papineni et al., 2002)) over all 150,000 iterations. From the graph, it can also be observed that the system has a more robust, consistent learning curve. (6) It|NP is|(S[dcl]NP)/NP a|NP/N modern|N/N form|N/PP of|PP/NP colonialism|N .|. 25 4.1 20 Experimental Set-Up 15 Data sets BLEU 4 Our NMT systems are trained on 1M parallel sentences of the Europarl corpus for EN–FR and EN– DE (Koehn, 2005). We test the systems on 5K sentences (different from the training data) extracted from Europarl and the newstest2013. Two different test sets are used in order to show how additional semantic and syntactic features can help the NMT system translate different types of test sets and thus evaluate the general effect of our improvement. 4.2 10 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 Iterations BPE SST_CCG Figure 1: Baseline (BPE) vs Combined (SST–CCG) NMT Systems for EN–FR, evaluated on the newstest"
P18-3010,D16-1025,0,0.0157731,"observe that such features (and particularly when combined) help the NMT model training to converge faster and improve the model quality according to the BLEU scores. 1 Introduction Neural Machine Translation (NMT) models have recently become the state-of-the art in the field of Machine Translation (Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner et al., 2014; Sutskever et al., 2014). Compared to Statistical Machine Translation (SMT), the previous state-of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (Bentivogli et al., 2016). Although NMT seems to partially ‘learn’ or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic (Bentivogli et al., 2016). More recent work showed that explicitly (Sennrich and Haddow, 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017) or implicitly (Eriguchi et al., 2017) modeling extra syntactic information into an NMT system on the source (and/or target) side could lead to improvements in translation quality. When integrating linguistic informati"
P18-3010,D07-1091,0,0.0635319,"ents the semantic and syntactic features used. The experimental set-up is described in Section 4 followed by the results in Section 5. Finally, We conclude and present some of the ideas for future work in Section 6. 2 Related Work In SMT, various linguistic features such as stems (Toutanova et al., 2008) lemmas (Mareˇcek et al., 2011; Fraser et al., 2012), POStags (Avramidis and Koehn, 2008), dependency labels (Avramidis and Koehn, 2008) and supertags (Hassan et al., 2007; Haque et al., 2009) are integrated using pre- or post-processing techniques often involving factored phrase-based models (Koehn and Hoang, 2007). Compared to factored NMT models, factored SMT models have some disadvantages: (a) adding factors increases the sparsity of the models, (b) the n-grams limit the size of context that is taken into account, and (c) features are assumed to be independent of each other. However, adding syntactic features to SMT systems led to improvements with respect to word order and morphological agreement (Williams and Koehn, 2012; Sennrich, 2015). One of the main strengths of NMT is its strong ability to generalize. The integration of linguistic features can be handled in a flexible way without creating spa"
P18-3010,W06-1670,0,0.01648,"Missing"
P18-3010,N13-1090,0,0.0230389,"Missing"
P18-3010,D16-1159,0,0.0254726,"t information (within the same sentence). Furthermore, the encoder and attention layers can be shared between features. By representing the encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006), Sennrich and Haddow (2016) generalized the embedding layer in such a way that an arbitrary number of linguistic features can be explicitly integrated. They then investigated whether features such as lemmas, subword tags, morphological features, POS tags and dependency labels could be useful for NMT systems or whether their inclusion is redundant. Similarly, on the syntax level, Shi et al. (2016) show that although NMT systems are able to partially learn syntactic information, more complex patterns remain problematic. Furthermore, sometimes information is present in the encoding vectors but is lost during the decoding phase (Vanmassenhove et al., 2017). Sennrich and Haddow (2016) show that the inclusion of linguistic fea3 3.1 Semantics and Syntax in NMT Supersense Tags The novelty of our work is the integration of explicit semantic features supersenses into an NMT system. Supersenses are a term which refers to 68 the top-level hypernyms in the WordNet (Miller, 1995) taxonomy, sometime"
P18-3010,W17-4707,0,0.0737246,", 2014; Kalchbrenner et al., 2014; Sutskever et al., 2014). Compared to Statistical Machine Translation (SMT), the previous state-of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (Bentivogli et al., 2016). Although NMT seems to partially ‘learn’ or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic (Bentivogli et al., 2016). More recent work showed that explicitly (Sennrich and Haddow, 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017) or implicitly (Eriguchi et al., 2017) modeling extra syntactic information into an NMT system on the source (and/or target) side could lead to improvements in translation quality. When integrating linguistic information into an MT system, following the central role assigned to syntax by many linguists, the focus has been mainly on the integration of syntactic features. Although there has been some work on semantic features for SMT (Banchs and Costa-Juss`a, 2011), so 67 Proceedings of ACL 2018, Student Research Workshop, pages 67–73 c Melbour"
P18-3010,P02-1040,0,0.101454,"the lexical level. This kind of information can help resolve ambiguity in terms of prepositional attachment, among others. An example of a CCGtagged sentence is given in (6): Results 5.1 English–French For both test sets, the NMT system with supersenses (SST) converges faster than the baseline (BPE) NMT system. As we hypothesized, the benefits of the features added, was more clear on the newstest2013 than on the Europarl test set. Figure 1 compares the BPE-ed baseline system (BPE) with the supertag-supersensetag system (CCG– SST) automatically evaluated on the newstest2013 (in terms of BLEU (Papineni et al., 2002)) over all 150,000 iterations. From the graph, it can also be observed that the system has a more robust, consistent learning curve. (6) It|NP is|(S[dcl]NP)/NP a|NP/N modern|N/N form|N/PP of|PP/NP colonialism|N .|. 25 4.1 20 Experimental Set-Up 15 Data sets BLEU 4 Our NMT systems are trained on 1M parallel sentences of the Europarl corpus for EN–FR and EN– DE (Koehn, 2005). We test the systems on 5K sentences (different from the training data) extracted from Europarl and the newstest2013. Two different test sets are used in order to show how additional semantic and syntactic features can help"
P18-3010,N03-1033,0,0.120011,"Input: SST: Output: 3.2 “a number of” “a number of” “a|mwe number|mwe of|mwe ” Supertags and POS-tags We hypothesize that more general semantic information can be particularly useful for NMT in combination with more detailed syntactic information. To support our hypothesis we also experimented 1 https://github.com/nschneid/ pysupersensetagger 2 All the examples are extracted from our data used later on to train the NMT systems 69 5 with syntactic features (separately and in combination with the semantic ones): POS tags and CCG supertags. The POS tags are generated by the Stanford POS-tagger (Toutanova et al., 2003); for the supertags we used the EasySRL tool (Lewis et al., 2015) which annotates words with CCG tags. CCG tags provide global syntactic information on the lexical level. This kind of information can help resolve ambiguity in terms of prepositional attachment, among others. An example of a CCGtagged sentence is given in (6): Results 5.1 English–French For both test sets, the NMT system with supersenses (SST) converges faster than the baseline (BPE) NMT system. As we hypothesized, the benefits of the features added, was more clear on the newstest2013 than on the Europarl test set. Figure 1 comp"
P18-3010,P08-1059,0,0.0380446,"t semantic features in the form of semantic ‘classes’ can be beneficial for NMT providing it with an extra ability to generalize and thus better learn more complex semanticosyntactic patters. The remainder of this paper is structured as follows: First, in Section 2, the related work is discussed. Next, Section 3 presents the semantic and syntactic features used. The experimental set-up is described in Section 4 followed by the results in Section 5. Finally, We conclude and present some of the ideas for future work in Section 6. 2 Related Work In SMT, various linguistic features such as stems (Toutanova et al., 2008) lemmas (Mareˇcek et al., 2011; Fraser et al., 2012), POStags (Avramidis and Koehn, 2008), dependency labels (Avramidis and Koehn, 2008) and supertags (Hassan et al., 2007; Haque et al., 2009) are integrated using pre- or post-processing techniques often involving factored phrase-based models (Koehn and Hoang, 2007). Compared to factored NMT models, factored SMT models have some disadvantages: (a) adding factors increases the sparsity of the models, (b) the n-grams limit the size of context that is taken into account, and (c) features are assumed to be independent of each other. However, addin"
P18-3010,N15-1177,0,0.347545,"a higher level of abstraction beneficial to learn more complex constructions. Furthermore, a combination of both syntactic and semantic features would provide the NMT system with a way of learning semantico-syntactic patterns. To apply semantic abstractions at the word-level that enable a characterisation beyond that what can be superficially derived, coarse-grained semantic classes can be used. Inspired by Named Entity Recognition which provides such abstractions for a limited set of words, supersense-tagging uses an inventory of more general semantic classes for domain-independent settings (Schneider and Smith, 2015). We investigate the effect of integrating supersense features (26 for nouns, 15 for verbs) into an NMT system. To obtain these features, we used the AMALGrAM 2.0 tool (Schneider et al., 2014; Schneider and Smith, 2015) which analyses the input sentence for Multi-Word Expressions as well as noun and verb supersenses. The features are integrated using the framework of Sennrich et al. (2016), replicating the tags for every subword unit obtained by byte-pair encoding (BPE). We further experiment with a combination of semantic supersenses and syntactic supertag features (CCG syntactic categories ("
P18-3010,W12-3150,0,0.0172011,"ramidis and Koehn, 2008) and supertags (Hassan et al., 2007; Haque et al., 2009) are integrated using pre- or post-processing techniques often involving factored phrase-based models (Koehn and Hoang, 2007). Compared to factored NMT models, factored SMT models have some disadvantages: (a) adding factors increases the sparsity of the models, (b) the n-grams limit the size of context that is taken into account, and (c) features are assumed to be independent of each other. However, adding syntactic features to SMT systems led to improvements with respect to word order and morphological agreement (Williams and Koehn, 2012; Sennrich, 2015). One of the main strengths of NMT is its strong ability to generalize. The integration of linguistic features can be handled in a flexible way without creating sparsity issues or limiting context information (within the same sentence). Furthermore, the encoder and attention layers can be shared between features. By representing the encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006), Sennrich and Haddow (2016) generalized the embedding layer in such a way that an arbitrary number of linguistic features can be explicitly integrated. They then investig"
P18-3010,Q15-1013,0,0.133863,"nd supertags (Hassan et al., 2007; Haque et al., 2009) are integrated using pre- or post-processing techniques often involving factored phrase-based models (Koehn and Hoang, 2007). Compared to factored NMT models, factored SMT models have some disadvantages: (a) adding factors increases the sparsity of the models, (b) the n-grams limit the size of context that is taken into account, and (c) features are assumed to be independent of each other. However, adding syntactic features to SMT systems led to improvements with respect to word order and morphological agreement (Williams and Koehn, 2012; Sennrich, 2015). One of the main strengths of NMT is its strong ability to generalize. The integration of linguistic features can be handled in a flexible way without creating sparsity issues or limiting context information (within the same sentence). Furthermore, the encoder and attention layers can be shared between features. By representing the encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006), Sennrich and Haddow (2016) generalized the embedding layer in such a way that an arbitrary number of linguistic features can be explicitly integrated. They then investigated whether feat"
P18-3010,E17-3017,0,0.0827683,"Missing"
P18-3010,W16-2209,0,0.484389,"au et al., 2014; Cho et al., 2014; Kalchbrenner et al., 2014; Sutskever et al., 2014). Compared to Statistical Machine Translation (SMT), the previous state-of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (Bentivogli et al., 2016). Although NMT seems to partially ‘learn’ or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic (Bentivogli et al., 2016). More recent work showed that explicitly (Sennrich and Haddow, 2016; Nadejde et al., 2017; Bastings et al., 2017; Aharoni and Goldberg, 2017) or implicitly (Eriguchi et al., 2017) modeling extra syntactic information into an NMT system on the source (and/or target) side could lead to improvements in translation quality. When integrating linguistic information into an MT system, following the central role assigned to syntax by many linguists, the focus has been mainly on the integration of syntactic features. Although there has been some work on semantic features for SMT (Banchs and Costa-Juss`a, 2011), so 67 Proceedings of ACL 2018, Student Research Workshop,"
petukhova-etal-2012-sumat,W09-4610,1,\N,Missing
petukhova-etal-2012-sumat,P07-2045,0,\N,Missing
petukhova-etal-2012-sumat,2005.eamt-1.29,0,\N,Missing
petukhova-etal-2012-sumat,R11-1014,0,\N,Missing
petukhova-etal-2012-sumat,2010.eamt-1.23,1,\N,Missing
R09-1025,J99-2004,0,0.126659,"Missing"
R09-1025,J96-1002,0,0.015203,"Missing"
R09-1025,A00-2018,0,0.274514,"Missing"
R09-1025,C04-1041,0,0.130527,"Missing"
R09-1025,J07-4004,0,0.0961213,"Missing"
R09-1025,P04-1015,0,0.140279,"Missing"
R09-1025,P02-1002,0,0.0725069,"Missing"
R09-1025,J07-3004,0,0.0293549,"Missing"
R09-1025,E95-1017,0,0.15406,"Missing"
R09-1025,W04-0308,0,0.0627621,"Missing"
R09-1025,P06-2089,0,0.038448,"Missing"
R09-1025,H05-1102,0,0.0378422,"Missing"
R09-1025,W03-3023,0,0.207811,"Missing"
R09-1025,P83-1020,0,0.586462,"Missing"
R09-1025,J03-4003,0,\N,Missing
R09-1025,P05-1033,0,\N,Missing
R09-1025,J05-1004,0,\N,Missing
R19-1052,P07-2045,0,0.0121297,"a swathe of research in the field of MT, unraveling the strengths, weaknesses, impacts and commercialisation aspects of the classical (i.e. PB-SMT) and emerging (i.e. NMT) methods (e.g. (Bentivogli et al., 2016; Toral and Way, 2018)). In brief, the NMT systems are often able to produce better translations than the PB-SMT systems. Interestingly, terminology translation, a crucial factor in industrial translation workflows (TWs), is one of the less explored areas in MT research. In this context, a few studies (Burchardt et al., 2 MT Systems To build our PB-SMT systems we used the Moses toolkit (Koehn et al., 2007). For LM training we combine a large monolingual corpus with the target-side of the parallel training corpus. Additionally, we trained a neural LM with the NPLM toolkit (Vaswani et al., 2013) on the target side of the parallel training corpus alone. We considered the standard PB-SMT log-linear features for training. We call the English-to-Hindi and Hindito-English PB-SMT systems EHPS and HEPS, respectively. Our NMT systems are Google Transformer models (Vaswani et al., 2017). In our experiments we followed the recommended best setup from Vaswani et al. (2017). We call our the English-to-Hindi"
R19-1052,E17-2045,0,0.0329626,"udice translation (NMT) nyaay - nirnay keea koee bhoomika halbury ke kaanoonon poorva vaad vidvat edeeje kaabil edeeje mrtp adhiniyam testrex rahasyon nyaayik roop vichaaraadheen class (NMT) PE TD PE RE TD CTR CTV CTR OE OE OE CTV ing the strength of the open-vocabulary translation technique (Sennrich et al., 2016). However, this method also has down-sides. For example, some of the term translations under the OE category in the NMT task are non-existent wordforms of the target language, for which the open-vocabulary translation technique is responsible. This phenomenon is also corroborated by Farajian et al. (2017) while translating technical domain terms. We discuss the OE class further below. We see from Table 5 that the human evaluator has marked 24 term translations with OE in NMT. In this category we observed that the translations of the source terms are usually either strange words that have no relation to the meaning of the source term, repetitions of other translated words or terms, entities that are non-existent wordforms of the target language, or words with typographical errors. As far as PB-SMT is concerned, we see from Table 5 that the evaluator also tagged 12 term translations with OE, mos"
R19-1052,P02-1040,0,0.104214,"Missing"
R19-1052,P16-1162,0,0.0761391,"ng errors seen in the Hindi-to-English PB-SMT task compared to 5 in the Hindi-to-English NMT task. As 442 Table 4: STC in English-to-Hindi PB-SMT and NMT. STC (PB-SMT) adjudicatory role decretal halsbury ’s laws presuit adjudicatory learned adj learned adj mrtp act testatrix concealments res judicata subjudice translation (NMT) nyaay - nirnay keea koee bhoomika halbury ke kaanoonon poorva vaad vidvat edeeje kaabil edeeje mrtp adhiniyam testrex rahasyon nyaayik roop vichaaraadheen class (NMT) PE TD PE RE TD CTR CTV CTR OE OE OE CTV ing the strength of the open-vocabulary translation technique (Sennrich et al., 2016). However, this method also has down-sides. For example, some of the term translations under the OE category in the NMT task are non-existent wordforms of the target language, for which the open-vocabulary translation technique is responsible. This phenomenon is also corroborated by Farajian et al. (2017) while translating technical domain terms. We discuss the OE class further below. We see from Table 5 that the human evaluator has marked 24 term translations with OE in NMT. In this category we observed that the translations of the source terms are usually either strange words that have no re"
R19-1052,tiedemann-2012-parallel,0,0.0352328,"In our experiments we followed the recommended best setup from Vaswani et al. (2017). We call our the English-to-Hindi and Hindi-to-English NMT systems EHNS and HENS, respectively. 437 Proceedings of Recent Advances in Natural Language Processing, pages 437–446, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_052 For experimentation we used the IIT Bombay English-Hindi parallel corpus (Kunchukuttan et al., 2017). For building additional LMs for Hindi and English we use the HindEnCorp monolingual corpus (Bojar et al., 2014) and monolingual data from the OPUS project (Tiedemann, 2012), respectively. Corpus statistics are shown in Table 1. We selected 2,000 sentences (test set) for the evaluation of the MT systems and 996 sentences (development set) for validation from the Judicial parallel corpus (cf. Table 1) which is a juridical domain corpus (i.e. proceedings of legal judgments). The MT systems were built with the training set shown in Table 1 that includes the remaining sentences of the Judicial parallel corpus. concerned, HEPS and HENS produce moderate BLEU scores (34.1 BLEU and 39.9 BLEU) on the test set. As expected, translation quality in the morphologically-rich t"
R19-1052,W04-3250,0,0.649584,"Missing"
R19-1052,D13-1140,0,0.034409,"ntivogli et al., 2016; Toral and Way, 2018)). In brief, the NMT systems are often able to produce better translations than the PB-SMT systems. Interestingly, terminology translation, a crucial factor in industrial translation workflows (TWs), is one of the less explored areas in MT research. In this context, a few studies (Burchardt et al., 2 MT Systems To build our PB-SMT systems we used the Moses toolkit (Koehn et al., 2007). For LM training we combine a large monolingual corpus with the target-side of the parallel training corpus. Additionally, we trained a neural LM with the NPLM toolkit (Vaswani et al., 2013) on the target side of the parallel training corpus alone. We considered the standard PB-SMT log-linear features for training. We call the English-to-Hindi and Hindito-English PB-SMT systems EHPS and HEPS, respectively. Our NMT systems are Google Transformer models (Vaswani et al., 2017). In our experiments we followed the recommended best setup from Vaswani et al. (2017). We call our the English-to-Hindi and Hindi-to-English NMT systems EHNS and HENS, respectively. 437 Proceedings of Recent Advances in Natural Language Processing, pages 437–446, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org"
R19-1052,C00-2137,0,0.0777439,"Missing"
R19-1107,W05-0909,0,0.0591163,"ompare German-to-English translation hypotheses generated by systems trained (i) only on authentic data, (ii) only on synthetic data, and (iii) on authentic data enhanced with different types of BT data: SMT, NMT. We exploit two types of synthetic and authentic data combinations: (a) randomly selected half of target sentences backtranslated by SMT and another half by NMT system, and (b) joining all BT data (thus repeating each target segment). The translation hypotheses are compared in terms of four automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005) and CHRF (Popovi´c, 2015). These metrics give an overall estimate of the quality of the translations with respect to the reference (human translation of the test set). In addition, the translation hypotheses are analyzed in terms of five error categories, lexical variety and syntactic variety. 1. Models trained with 1M auth + 2M synth sentences using the default settings, including 13 training epochs. 2. Models trained on 1M auth data only, trained either: (a) using the default settings, including 13 training epochs. (b) Trained for 39 epochs, to obtain a same amount of training effort as for"
R19-1107,P17-4012,0,0.0677521,"of base. • NMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an NMT model (with the same configuration as described in Section 5 but in the English to German direction) trained with the base set. • hybrNMTSMT: Synthetic parallel corpus combining NMTsynth and SMTsynth sets. It has been built by maintaining the same target side of auth, and as source side we alternate between NMTsynth and SMTsynth each 500K sentences. Experimental Settings For the experiments we have built German-toEnglish NMT models using the Pytorch port of OpenNMT (Klein et al., 2017). We use the default parameters: 2-layer LSTM with 500 hidden units. The models are trained for the same number of epochs. As the model trained with all authentic data converges after 13 epochs, we use that many iterations to train the models (we use the same amount of epochs). As optimizer we use stochas• fullhybrNMTSMT: Synthetic parallel corpus combining all segments from NMTsynth and SMTsynth sets (double size, each original target sentence repeated twice with both an NMT and SMT back-translation-generated translation). 924 7 Experiments The results show that adding synthetic data has a po"
R19-1107,W04-3250,0,0.494404,". As optimizer we use stochas• fullhybrNMTSMT: Synthetic parallel corpus combining all segments from NMTsynth and SMTsynth sets (double size, each original target sentence repeated twice with both an NMT and SMT back-translation-generated translation). 924 7 Experiments The results show that adding synthetic data has a positive impact on the performance of the models as all of them achieve improvements when compared to that built only with authentic data 1M base. These improvements are statistically significant at p=0.01 (computed with multeval (Clark et al., 2011) using Bootstrap Resampling (Koehn, 2004)). However, the increases of quality are different depending on the approach followed to create the BT data. In our experiments, we build models on different portions of the datasets described in Section 6. First, we train an initial NMT model using the base data set. Then, in order to investigate how much the models benefit from using synthetic data generated by different approaches, we build models with increasing sizes of data (from the data sets described in Section 6). The models explored are built with data that ranges from 1M sentences (built with only authentic data from base data set)"
R19-1107,W18-6315,0,0.14104,"s (Poncelas et al., 2019). Edunov et al. (2018) confirmed that synthetic data can sometimes match the performance of authentic data. In addition, a comprehensive analysis of different methods to generate synthetic source sentences was carried out. This analysis revealed that sampling from the model distribution or noising beam outputs out-performs pure beam search, which is typically used in NMT. Their analysis shows that synthetic data based on sampling and noised beam search provides a stronger training signal than synthetic data based on argmax inference. One of the experiments reported in Burlot and Yvon (2018) is comparing performance between models trained with NMT and SMT BT data. The best Moses system (Koehn et al., 2007) is almost as good as the NMT system trained with the same (authentic) data, and much faster to train. Improvements obtained with the Moses system trained with a small training corpus are much smaller; this system even decreases the performance for the out-of-domain test. The authors also investigated some properties of BT data and found out that the back-translated sources are on average shorter than authentic ones, syntactically simpler than authentic ones, and contain smaller"
R19-1107,L18-1004,0,0.0339369,"Missing"
R19-1107,P03-1021,0,0.173985,"n their translation hypotheses. 5 6 Data The parallel data used for the experiments has been obtained from WMT 2015 (Bojar et al., 2015). We build two parallel sets with these sentences: base (1M sentences) and auth (3M sentences). We use the target side of auth to create the following datasets: • SMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an SMT model trained with base set in the English to German direction. It has been built using the Moses toolkit with default settings, using GIZA++ for word alignment and tuned using MERT (Och, 2003)). The language model (of order 8) is built with the KenLM toolkit (Heafield, 2011) using the German side of base. • NMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an NMT model (with the same configuration as described in Section 5 but in the English to German direction) trained with the base set. • hybrNMTSMT: Synthetic parallel corpus combining NMTsynth and SMTsynth sets. It has been built by maintaining the same target side of auth, and as source side we alternate between NMTsynth and SMTsynth each 500K sentences. Experimental"
R19-1107,P11-2031,0,0.0665491,"in the models (we use the same amount of epochs). As optimizer we use stochas• fullhybrNMTSMT: Synthetic parallel corpus combining all segments from NMTsynth and SMTsynth sets (double size, each original target sentence repeated twice with both an NMT and SMT back-translation-generated translation). 924 7 Experiments The results show that adding synthetic data has a positive impact on the performance of the models as all of them achieve improvements when compared to that built only with authentic data 1M base. These improvements are statistically significant at p=0.01 (computed with multeval (Clark et al., 2011) using Bootstrap Resampling (Koehn, 2004)). However, the increases of quality are different depending on the approach followed to create the BT data. In our experiments, we build models on different portions of the datasets described in Section 6. First, we train an initial NMT model using the base data set. Then, in order to investigate how much the models benefit from using synthetic data generated by different approaches, we build models with increasing sizes of data (from the data sets described in Section 6). The models explored are built with data that ranges from 1M sentences (built wit"
R19-1107,P02-1040,0,0.104339,"of our knowledge, this has not been investigated yet. We compare German-to-English translation hypotheses generated by systems trained (i) only on authentic data, (ii) only on synthetic data, and (iii) on authentic data enhanced with different types of BT data: SMT, NMT. We exploit two types of synthetic and authentic data combinations: (a) randomly selected half of target sentences backtranslated by SMT and another half by NMT system, and (b) joining all BT data (thus repeating each target segment). The translation hypotheses are compared in terms of four automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005) and CHRF (Popovi´c, 2015). These metrics give an overall estimate of the quality of the translations with respect to the reference (human translation of the test set). In addition, the translation hypotheses are analyzed in terms of five error categories, lexical variety and syntactic variety. 1. Models trained with 1M auth + 2M synth sentences using the default settings, including 13 training epochs. 2. Models trained on 1M auth data only, trained either: (a) using the default settings, including 13 training epochs. (b) Trained fo"
R19-1107,D18-1045,0,0.0216509,"oint onwards. 923 tic gradient descent (SGD), in combination with learning rate decay, halving the learning rate starting from the 8th epoch. In order to build the models, all data sets are tokenized and truecased and segmented with BytePair Encoding (BPE) (Sennrich et al., 2016b) built on the joint vocabulary using 89500 merge operations. For testing the models we use the test set provided in the WMT 2015 News Translation Task (Bojar et al., 2015). As development set, we use 5K randomly sampled sentences from development sets provided in previous years of WMT. models (Poncelas et al., 2019). Edunov et al. (2018) confirmed that synthetic data can sometimes match the performance of authentic data. In addition, a comprehensive analysis of different methods to generate synthetic source sentences was carried out. This analysis revealed that sampling from the model distribution or noising beam outputs out-performs pure beam search, which is typically used in NMT. Their analysis shows that synthetic data based on sampling and noised beam search provides a stronger training signal than synthetic data based on argmax inference. One of the experiments reported in Burlot and Yvon (2018) is comparing performance"
R19-1107,W18-2703,0,0.0607276,"anguage sentences using a Machine Translation (MT) model. Generally, NMT models are used for backtranslation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmented with back-translated data created by merging different MT approaches. 1 The work presented in Poncelas et al. (2018) draws an early-stage empirical roadmap to investigating the effects of BT data. In particular, it looks at how the amount of BT data impacts the performance of the final NMT system. In Sennrich et al. (2016a) and Poncelas et al. (2018), the systems used to generate the BT data are neural. However, it has been noted that often different paradigms can contribute differently to a given task. For example, it has been shown that applying an APE system based on NMT technology improves statistical machine translation (SMT) output, but has lower impact on NMT output (Bojar et al., 2017; Chatterjee et"
R19-1107,W11-2123,0,0.0532852,"ments has been obtained from WMT 2015 (Bojar et al., 2015). We build two parallel sets with these sentences: base (1M sentences) and auth (3M sentences). We use the target side of auth to create the following datasets: • SMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an SMT model trained with base set in the English to German direction. It has been built using the Moses toolkit with default settings, using GIZA++ for word alignment and tuned using MERT (Och, 2003)). The language model (of order 8) is built with the KenLM toolkit (Heafield, 2011) using the German side of base. • NMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an NMT model (with the same configuration as described in Section 5 but in the English to German direction) trained with the base set. • hybrNMTSMT: Synthetic parallel corpus combining NMTsynth and SMTsynth sets. It has been built by maintaining the same target side of auth, and as source side we alternate between NMTsynth and SMTsynth each 500K sentences. Experimental Settings For the experiments we have built German-toEnglish NMT models using the Py"
R19-1107,W16-2378,0,0.038863,"Missing"
R19-1107,W15-3049,1,0.923108,"Missing"
R19-1107,2011.eamt-1.36,1,0.735645,"Missing"
R19-1107,P16-1009,0,0.670634,"APE) (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018). However, the effects of various parameters for creating back-translated (BT) data have not been investigated enough as to indicate what are the optimal conditions in not only creating but also employing such data to train high-quality neural machine translation (NMT) systems. Neural Machine Translation (NMT) models achieve their best performance when large sets of parallel data are used for training. Consequently, techniques for augmenting the training set have become popular recently. One of these methods is back-translation (Sennrich et al., 2016a), which consists on generating synthetic sentences by translating a set of monolingual, target-language sentences using a Machine Translation (MT) model. Generally, NMT models are used for backtranslation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmen"
R19-1107,P16-1162,0,0.838573,"APE) (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018). However, the effects of various parameters for creating back-translated (BT) data have not been investigated enough as to indicate what are the optimal conditions in not only creating but also employing such data to train high-quality neural machine translation (NMT) systems. Neural Machine Translation (NMT) models achieve their best performance when large sets of parallel data are used for training. Consequently, techniques for augmenting the training set have become popular recently. One of these methods is back-translation (Sennrich et al., 2016a), which consists on generating synthetic sentences by translating a set of monolingual, target-language sentences using a Machine Translation (MT) model. Generally, NMT models are used for backtranslation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmen"
R19-1107,2006.amta-papers.25,0,0.0605592,"ot been investigated yet. We compare German-to-English translation hypotheses generated by systems trained (i) only on authentic data, (ii) only on synthetic data, and (iii) on authentic data enhanced with different types of BT data: SMT, NMT. We exploit two types of synthetic and authentic data combinations: (a) randomly selected half of target sentences backtranslated by SMT and another half by NMT system, and (b) joining all BT data (thus repeating each target segment). The translation hypotheses are compared in terms of four automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005) and CHRF (Popovi´c, 2015). These metrics give an overall estimate of the quality of the translations with respect to the reference (human translation of the test set). In addition, the translation hypotheses are analyzed in terms of five error categories, lexical variety and syntactic variety. 1. Models trained with 1M auth + 2M synth sentences using the default settings, including 13 training epochs. 2. Models trained on 1M auth data only, trained either: (a) using the default settings, including 13 training epochs. (b) Trained for 39 epochs, to obtain a sa"
R19-1107,W17-4717,0,\N,Missing
S14-2085,S12-1059,0,0.102397,"Missing"
S14-2085,P98-1013,0,0.0501381,"Missing"
S14-2085,S13-1034,1,0.805331,"Missing"
S14-2085,S13-2098,1,0.882393,"Missing"
S14-2085,W14-3339,1,0.567803,"Missing"
S14-2085,J93-2003,0,0.0283776,"he distribution of alignment probabilities for S P ( s∈S −p log p for p = p(t|s) where s and t are tokens) and T, their average for S and T, the number of entries with p ≥ 0.2 and p ≥ 0.01, the entropy of the word alignment between S and T and its average, and word alignment log probability and its value in terms of bits per word. We also compute word alignment percentage as in (Camargo de Souza et al., 2013) and potential BLEU, F1 , WER, PER scores for S and T. • IBM1 Translation Probability {4, 12}: Calculates the translation probability of test sentences using the selected training set, I (Brown et al., 1993). • Feature Vector Similarity {8, 8}: Calculates similarities between vector representations. Figure 1: RTM depiction. Our encouraging results in the semantic similarity tasks increase our understanding of the acts of translation we ubiquitously use when communicating and how they can be used to predict the semantic similarity of text. RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable in different domains and tasks. RTM expands the applicability of MTPP by making it feasible when making monolingual quality and similarity judgment"
S14-2085,W11-2131,1,0.91684,"Missing"
S14-2085,W13-2243,0,0.062769,"Missing"
S14-2085,W11-2137,1,0.926086,"Missing"
S14-2085,W14-3303,1,0.773403,"Missing"
S14-2085,S14-2003,0,0.248829,"two sentences, produce a relatedness score indicating the extent to which the sentences express a related meaning: a number in the range [1, 5]. We introduce a fully automated judge for semantic similarity that performs well in three semantic similarity tasks at SemEval-2014, Semantic Evaluation Exercises - International Workshop on Semantic Evaluation (Nakov and Zesch, 2014). RTMs provide a language independent solution for the semantic textual similarity (STS) task (Task 10) (Agirre et al., 2014), achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) (Jurgens et al., 2014), We model the problem as a translation performance prediction task where one possible interpretation is obtained by translating S1 (the source to translate, S) to S2 (the target translation, T). Since linguistic processing can reveal deeper similarity relationships, we also look at the translation task at different granularities of information: plain 487 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 487–496, Dublin, Ireland, August 23-24, 2014. text (R for regular) and after lemmatization (L). We lowercase all text. Task 3 Cross-Level Semantic Simi"
S14-2085,levy-andrew-2006-tregex,0,0.0132666,",3,4,5,6) obtained for S and T (B¨ar et al., 2012). • Minimum Bayes Retrieval Risk {0, 4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {0, 3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • LIX {1, 1}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for S and T. 1 2.3 Many fund managers argue that now ’s the time to buy . We use Tregex (Levy and Andrew, 2006) for visualizing the output parse trees presented on the left. The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 SemEval-14 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2014 with the RTM-DCU team name. The interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and the LM corpora provided by LDC for ˆ English (Parke"
S14-2085,W13-2242,1,0.905045,"Missing"
S14-2085,J93-2004,0,0.0468325,"Missing"
S14-2085,S14-2001,0,0.495765,"ts, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs provide a language independent approach to all similarity tasks and achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) and good results in semantic relatedness and entailment (Task 1) and multilingual semantic textual similarity (STS) (Task 10). RTMs remove the need to access any task or domain specific information or resource. 1 and achieve good results in the semantic relatedness and entailment task (Task 1) (Marelli et al., 2014a). Referential translation machine (Section 2) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic¸ici, 2008). Each RTM model is a data translation and translation prediction model betwe"
S14-2085,marelli-etal-2014-sick,0,0.329715,"ts, the difficulty of translating them, and the presence of the acts of translation, which may ubiquitously be observed in communication. RTMs provide a language independent approach to all similarity tasks and achieve top performance when predicting monolingual cross-level semantic similarity (Task 3) and good results in semantic relatedness and entailment (Task 1) and multilingual semantic textual similarity (STS) (Task 10). RTMs remove the need to access any task or domain specific information or resource. 1 and achieve good results in the semantic relatedness and entailment task (Task 1) (Marelli et al., 2014a). Referential translation machine (Section 2) is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, training data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic¸ici, 2008). Each RTM model is a data translation and translation prediction model betwe"
S14-2085,P02-1040,0,0.091463,"of co-occurring features in the training set (Bic¸ici et al., 2013). • Synthetic Translation Performance {3, 3}: Calculates translation scores achievable according to the n-gram coverage. • Character n-grams {5}: Calculates cosine between character n-grams (for n=2,3,4,5,6) obtained for S and T (B¨ar et al., 2012). • Minimum Bayes Retrieval Risk {0, 4}: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance {0, 3}: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • LIX {1, 1}: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for S and T. 1 2.3 Many fund managers argue that now ’s the time to buy . We use Tregex (Levy and Andrew, 2006) for visualizing the output parse trees presented on the left. The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 SemEval-14 Results We develop individual RTM mode"
S14-2085,2009.eamt-1.5,0,0.0491984,"R+L L We use ridge regression (RR), support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004), and extremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic¸ici et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic¸ici et al., 2013; Bic¸ici et al., 2014). We optimize the learning parameters by selecting ε close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for ε is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). At testing tim"
S14-2085,N03-1033,0,0.0323644,"tatistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 SemEval-14 Results We develop individual RTM models for each task and subtask that we participate at SemEval-2014 with the RTM-DCU team name. The interpretants are selected from the LM corpora distributed by the translation task of WMT14 (Bojar et al., 2014) and the LM corpora provided by LDC for ˆ English (Parker et al., 2011) and Spanish (Angelo 4 Mendonc¸a, 2011) . We use the Stanford POS tagger (Toutanova et al., 2003) to obtain the lemmatized corpora for the SRE task. For each RTM Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing structure. We derive 5 statistics based on the geometric properties of the parse trees: number of brackets used (numB), depth (depthB), average depth (avg 2 For nodes with uneven number of children, the nodes in the odd child contribute to the right branches. 3 Wall Street Journal (WSJ) corpus section 23, distributed with Penn Treebank version 3 (Marcus et al., 1993). 4 English Gigaword 5th, Spanish Gigawor"
S14-2085,C98-1013,0,\N,Missing
S14-2085,S14-2010,0,\N,Missing
S14-2085,W14-3302,0,\N,Missing
W00-2040,C00-1010,0,0.021914,"Missing"
W00-2040,P98-1022,0,0.046265,"Missing"
W00-2040,1992.tmi-1.20,0,0.134597,"Missing"
W00-2040,E95-1038,0,0.026776,"Missing"
W05-0833,1999.tmi-1.3,0,0.402267,"ing with new input. 2.1 2. Determining the sub-sentential translation links in those retrieved examples; 3. Recombining relevant parts of the target translation links to derive the translation. Searching for the best matches involves determining a similarity metric based on word occurrences and part-of-speech labels, generalised templates and bilingual dictionaries. The recombination process depends on the nature of the examples used in the first place, which may include aligning phrasestructure (sub-)trees (Hearne & Way, 2003) or dependency trees (Watanabe et al., 2003), or using placeables (Brown, 1999) as indicators of chunk boundaries. Another method—and the one used in the EBMT system used in our experiments—is to use a set of closed-class words to segment aligned source and target sentences and to derive an additional set of lexical and phrasal resources. (Gough & Way, 2004b) base their work on the ‘Marker Hypothesis’ (Green, 1979), a universal psycholinguistic constraint which posits that languages are ‘marked’ for syntactic structure at surface level by a closed set of specific lexemes and morphemes. In a preprocessing stage, (Gough & Way, 2004b) use 7 sets of marker words for English"
W05-0833,2004.eamt-1.9,1,0.0532277,"EBMT system outperformed the SMT system by a factor of two to one. Nevertheless, they did not test their EBMT system against a phrase-based SMT system. Obtaining their training and test data for English–French, we carry out a number of experiments using the Pharaoh SMT Decoder. While better results are seen when Pharaoh is seeded with Giza++ word- and phrase-based data compared to EBMT sub-sentential alignments, in general better results are obtained when combinations of this ‘hybrid’ data is used to construct the translation and probability models. While for the most part the EBMT system of (Gough & Way, 2004b) outperforms any flavour of the phrasebased SMT systems constructed in our experiments, combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French–English. 183 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 183–190, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 in (Way and Gough, 2005), and evaluated a number of SMT systems which use the Pharaoh decoder1 against the Marker-Based EBMT system of (Gough & Way, 2004b), for French–English and"
W05-0833,2004.tmi-1.11,1,0.0645737,"EBMT system outperformed the SMT system by a factor of two to one. Nevertheless, they did not test their EBMT system against a phrase-based SMT system. Obtaining their training and test data for English–French, we carry out a number of experiments using the Pharaoh SMT Decoder. While better results are seen when Pharaoh is seeded with Giza++ word- and phrase-based data compared to EBMT sub-sentential alignments, in general better results are obtained when combinations of this ‘hybrid’ data is used to construct the translation and probability models. While for the most part the EBMT system of (Gough & Way, 2004b) outperforms any flavour of the phrasebased SMT systems constructed in our experiments, combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French–English. 183 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 183–190, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 in (Way and Gough, 2005), and evaluated a number of SMT systems which use the Pharaoh decoder1 against the Marker-Based EBMT system of (Gough & Way, 2004b), for French–English and"
W05-0833,W04-3250,0,0.0488001,"Missing"
W05-0833,N03-1017,0,0.143216,"phrases) in the most likely target word order. The language model is trained by determining all bigram and/or trigram frequency distributions occurring in the training data, while the translation model takes into account source and target word (and phrase) co-occurrence frequencies, sentence lengths and the relative sentence positions of source and target words. Until quite recently, SMT models of translation were based on the simple word alignment models of (Brown et al., 1990). Nowadays, however, SMT practitioners also get their systems to learn phrasal as well as lexical alignments (e.g. (Koehn et al., 2003); (Och, 2003)). Unsurprisingly, the quality obtained by today’s phrase-based SMT systems is considerably better than that obtained by the poorer word-based models. 3 Comparing EBMT and Word-Based SMT (Way and Gough, 2005) obtained a large translation memory from Sun Microsystems containing 207,468 English–French sentence pairs, of which 3,939 sentence pairs were randomly extracted as a test set, with the remaining 203,529 sentences used as training data. The average sentence length for the English test set was 13.1 words and 15.2 words for the corresponding French test set. The EBMT system use"
W05-0833,P03-1021,0,0.0409001,"likely target word order. The language model is trained by determining all bigram and/or trigram frequency distributions occurring in the training data, while the translation model takes into account source and target word (and phrase) co-occurrence frequencies, sentence lengths and the relative sentence positions of source and target words. Until quite recently, SMT models of translation were based on the simple word alignment models of (Brown et al., 1990). Nowadays, however, SMT practitioners also get their systems to learn phrasal as well as lexical alignments (e.g. (Koehn et al., 2003); (Och, 2003)). Unsurprisingly, the quality obtained by today’s phrase-based SMT systems is considerably better than that obtained by the poorer word-based models. 3 Comparing EBMT and Word-Based SMT (Way and Gough, 2005) obtained a large translation memory from Sun Microsystems containing 207,468 English–French sentence pairs, of which 3,939 sentence pairs were randomly extracted as a test set, with the remaining 203,529 sentences used as training data. The average sentence length for the English test set was 13.1 words and 15.2 words for the corresponding French test set. The EBMT system used was their M"
W05-0833,J03-1002,0,0.0577411,"dels. 3 Comparing EBMT and Word-Based SMT (Way and Gough, 2005) obtained a large translation memory from Sun Microsystems containing 207,468 English–French sentence pairs, of which 3,939 sentence pairs were randomly extracted as a test set, with the remaining 203,529 sentences used as training data. The average sentence length for the English test set was 13.1 words and 15.2 words for the corresponding French test set. The EBMT system used was their Marker-based system as described in section 2.1 above. In order to create the necessary SMT language and translation models, they used: • Giza++ (Och & Ney, 2003);2 • the CMU-Cambridge statistical toolkit;3 • the ISI ReWrite Decoder.4 Word- and Phrase-Based SMT SMT systems require two large probability tables in order to generate translations of new input: 1. a translation model induced from a large amount of bilingual data; Translation was performed from English–French and French–English, and the resulting translations were evaluated using a range of automatic metrics: BLEU (Papineni et al., 2002), Precision and Recall 2 2. a target language model induced from a(n even) large(r) quantity of separate monolingual text. 185 http://www.isi.edu/∼och/Giza++"
W05-0833,P02-1040,0,0.112046,"combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French–English. 183 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 183–190, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 in (Way and Gough, 2005), and evaluated a number of SMT systems which use the Pharaoh decoder1 against the Marker-Based EBMT system of (Gough & Way, 2004b), for French–English and English– French. We provide results using a range of automatic evaluation metrics: BLEU (Papineni et al., 2002), Precision and Recall (Turian et al., 2003), and Word- and Sentence Error Rates. (Way and Gough, 2005) observe that EBMT tends to outperform a word-based SMT model, and our experiments show that a number of different phrase-based SMT systems still tend to fall short of the quality obtained via EBMT for these evaluation metrics. However, when Pharaoh is seeded with the data sets automatically induced by both Giza++ and their EBMT system, better results are seen for French–English than for the EBMT system per se. The remainder of the paper is constructed as follows. In section 2, we summarize t"
W05-0833,2003.mtsummit-papers.51,0,0.291555,"by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French–English. 183 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 183–190, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 in (Way and Gough, 2005), and evaluated a number of SMT systems which use the Pharaoh decoder1 against the Marker-Based EBMT system of (Gough & Way, 2004b), for French–English and English– French. We provide results using a range of automatic evaluation metrics: BLEU (Papineni et al., 2002), Precision and Recall (Turian et al., 2003), and Word- and Sentence Error Rates. (Way and Gough, 2005) observe that EBMT tends to outperform a word-based SMT model, and our experiments show that a number of different phrase-based SMT systems still tend to fall short of the quality obtained via EBMT for these evaluation metrics. However, when Pharaoh is seeded with the data sets automatically induced by both Giza++ and their EBMT system, better results are seen for French–English than for the EBMT system per se. The remainder of the paper is constructed as follows. In section 2, we summarize the main ideas behind typical models of SMT a"
W05-0833,J90-2002,0,\N,Missing
W05-0833,2003.mtsummit-papers.22,1,\N,Missing
W06-3112,W05-0909,0,0.0735864,"data set in order to obtain a reliable score for their system. Recently a number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics. Some of them concentrate mainly on the word reordering aspect, like Maximum Matching String (Turian et al., 2003) or Translation Error Rate (Snover et al., 2005). Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. A closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation, in that it requires considerable external knowledge resources like WordNet, verb class databases, and extensive text preparation: stemming, tagging, etc. The advantage of our method is that it produc"
W06-3112,P05-1074,0,0.125146,"Missing"
W06-3112,P02-1033,0,0.0803152,"Missing"
W06-3112,N03-1017,0,0.0209544,"Missing"
W06-3112,koen-2004-pharaoh,0,0.0138884,"s the resources available. Therefore, it would be desirable to find a way to automatically generate legitimate translation alternatives not present in the reference(s) already available. 86 Proceedings of the Workshop on Statistical Machine Translation, pages 86–93, c New York City, June 2006. 2006 Association for Computational Linguistics In this paper, we present a novel method that automatically derives paraphrases using only the source and reference texts involved in for the evaluation of French-to-English Europarl translations produced by two MT systems: statistical phrase-based Pharaoh (Koehn, 2004) and rulebased Logomedia.1 In using what is in fact a miniature bilingual corpus our approach differs from the mainstream paraphrase generation based on monolingual resources. We show that paraphrases produced in this way are more relevant to the task of evaluating machine translation than the use of external lexical knowledge resources like thesauri or WordNet2, in that our paraphrases contain both lexical equivalents and low-level syntactic variants, and in that, as a side-effect, evaluation bitextderived paraphrasing naturally yields domainspecific paraphrases. The paraphrases generated fro"
W06-3112,2005.mtsummit-papers.11,0,0.0709031,"the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation. On the other hand, in practice even a number of references do not capture the whole potential variability of the translation. Moreover, often it is the case that multiple references are not available or are too difficult and expensive to produce: when designing a statistical machine translation system, the need for large amounts of training data limits the researcher to collections of parallel corpora like Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, often exceeds the resources available. Therefore, it would be desirable to find a way to automatically generate legitimate translation alternatives not present in the reference(s) already available. 86 Proceedings of the Workshop on Statistical Machine Translation, pages 86–93, c New York City, June 2006. 2006 Association for Computational Linguistics In this paper, we present a novel method that automatically derives pa"
W06-3112,E06-1031,0,0.0320273,"tion is to some extent counterbalanced by the time spent by developers on producing a sufficiently large test data set in order to obtain a reliable score for their system. Recently a number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics. Some of them concentrate mainly on the word reordering aspect, like Maximum Matching String (Turian et al., 2003) or Translation Error Rate (Snover et al., 2005). Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. A closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation, in that it requires considerable external knowledge resources like WordNet, verb clas"
W06-3112,N03-1024,0,0.0729959,"admire the reply mrs parly gave this morning however we have turned a blind eye to that Paraphrase 5: i admire the answer mrs parly gave this morning however we have turned a blind eye to it Paraphrase 6: i admire the reply mrs parly gave this morning but we have turned a blind eye to it This can potentially prevent higher n-grams being successfully matched if two or more equivalent expressions find themselves within the range of ngrams being tested by BLEU and NIST. To avoid combinatorial problems, implementing multiple simultaneous substitutions could be done using a lattice, much like in (Pang et al., 2003). 4 Results As expected, the use of multiple references produced by our method raises both the BLEU and NIST scores for translations produced by Pharaoh (test set PH) and Logomedia (test set LM). The results are presented in Table 1. PH single ref PH multi ref LM single ref LM multi ref BLEU 0.2131 0.2407 0.1782 0.2043 NIST 6.1625 7.0068 5.5406 6.3834 The hypothesis that the multiple-reference scores reflect better human judgment is also confirmed. For 100-sentence subsets (Subset PH and Subset LM) randomly extracted from our test sets PH and LM, we calculated Pearson’s correlation between the"
W06-3112,P02-1040,0,0.113715,"during automatic MT evaluation using only the source and reference texts, which are necessary for the evaluation, and word and phrase alignment software. Using target language paraphrases produced through word and phrase alignment a number of alternative reference sentences are constructed automatically for each candidate translation. The method produces lexical and lowlevel syntactic paraphrases that are relevant to the domain in hand, does not use external knowledge resources, and can be combined with a variety of automatic MT evaluation system. 1 Introduction Since their appearance, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) have been the standard tools used for evaluating the quality of machine translation. They both score candidate translations on the basis of the number of n-grams it shares with one or more reference translations provided. Such automatic measures are indispensable in the development of machine translation systems, because they allow the developers to conduct frequent, cost-effective, and fast evaluations of their evolving models. These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translat"
W06-3112,C04-1031,0,0.038569,"source language phrase fpi are often paraphrases of each other. For example, in our experiment, for the French word question the most probable automatically aligned English translations are question, matter, and issue, which in English are practically synonyms. Section 3.2 presents more examples of such equivalent expressions. 3.1 We used the GIZA++ word alignment software3 to produce initial word alignments for our miniature bilingual corpus consisting of the source French file and the English reference file, and the refined word alignment strategy of (Och and Ney, 2003; Koehn et al., 2003; Tiedemann, 2004) to obtain improved word and phrase alignments. For each source word or phrase fi that is aligned with more than one target words or phrases, its possible translations ei1, ..., ein were placed in a list as equivalent expressions (i.e. synonyms, near-synonyms, or paraphrases of each other). A few examples are given in (1). (1) agreement - accordance adopted - implemented matter - lot - case funds - money arms - weapons area - aspect question – issue – matter we would expect - we certainly expect bear on - are centred around Experimental design For our experiment, we used two test sets, each co"
W06-3112,2003.mtsummit-papers.51,0,0.109197,"noted by (Och et al., 2003) and (Russo-Lassner et al., 2005). A side effect of this phenomenon is that BLEU is less reliable for smaller data sets, so the advantage it provides in the speed of evaluation is to some extent counterbalanced by the time spent by developers on producing a sufficiently large test data set in order to obtain a reliable score for their system. Recently a number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics. Some of them concentrate mainly on the word reordering aspect, like Maximum Matching String (Turian et al., 2003) or Translation Error Rate (Snover et al., 2005). Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. A closer examination of these metrics suggests t"
W06-3112,2004.tmi-1.9,0,0.101216,"ny divergence from them. In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical choices and syntactic structure it contains, even though perfectly legitimate, are not present in at least one of the references. Necessarily, this score would not reflect a much more favourable human judgment that such a translation would receive. The limitations of string comparison are the reason why it is advisable to provide multiple references for a candidate translation in the BLEU- or NIST-based evaluation in the first place. While (Zhang and Vogel, 2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation. On the other hand, in practice even a number of references do not capture the whole potential variability of the translation. Moreover, often it is the case that multiple references are not available or are too difficult and expensive to produce: when designing a statistical machine translation system, the need for large amounts of training data limits the researcher to collections"
W06-3112,E06-1032,0,\N,Missing
W06-3112,J03-1002,0,\N,Missing
W07-0411,N06-1058,0,0.264045,"zation and provide a “normalized” representation of (some) syntactic variants of a given sentence. The translation and reference files are analyzed by a treebank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al., 2004), which produces a set of dependency triples for each input. The translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al. (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment. The remainder of this paper is organ"
W07-0411,W05-0909,0,0.10468,"ion, recall, and f-score for that particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al. (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment. The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consortium’s (LDC) Multiple Translation project; Section 5 discusses ongoing work; Section 6 conclu"
W07-0411,P04-1041,1,0.683222,"Missing"
W07-0411,koen-2004-pharaoh,0,0.0155325,"Missing"
W07-0411,2005.mtsummit-papers.11,0,0.0926597,"ces for a candidate translation in BLEU- or NIST-based evaluations. While Zhang and Vogel (2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation. In addition, in practice even a number of references do not capture the whole potential variability of the translation. Moreover, when designing a statistical MT system, the need for large amounts of training data limits the researcher to collections of parallel corpora such as Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, is often prohibitive. Therefore, it would be desirable to find an evaluation method that accepts legitimate syntactic and lexical differences 80 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80–87, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics between the translation and the reference, thus better mirroring human assessmen"
W07-0411,2004.tmi-1.8,0,0.241948,"Missing"
W07-0411,E06-1031,0,0.0544376,"lopment of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2005), which computes the number of substitutions, inserts, deletions, and shifts necessary to transform the translation text to match the reference. Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and Owczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet5 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al. (2006). Another metric making use of synonyms is the linear regression model dev"
W07-0411,J03-1002,0,0.00159553,"U scored Pharaoh 0.0349 points higher, NIST scored Pharaoh 0.6219 points higher, but human judges scored Logomedia output 0.19 points higher (on a 5-point scale). 4.1.1 Experimental design In order to check for the existence of a bias in the dependency-based metric, we created a set of 4,000 sentences drawn randomly from the SpanishEnglish subset of Europarl (Koehn, 2005), and we produced two translations: one by a rule-based system Logomedia, and the other by the standard phrase-based statistical decoder Pharaoh, using alignments produced by GIZA++8 and the refined word alignment strategy of Och and Ney (2003). The translations were scored with a range of metrics: BLEU, NIST, GTM, TER, METEOR, and the dependency-based method. 4.1.2 Adding synonyms Besides the ability to allow syntactic variants as valid translations, a good metric should also be able to accept legitimate lexical variation. We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al. (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). Bitext-derived paraphrases Owczar"
W07-0411,P02-1040,0,0.117147,"the translation. In addition to allowing for legitimate syntactic differences, we use paraphrases in the evaluation process to account for lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. An experiment with two translations of 4,000 sentences from Spanish-English Europarl shows that, in contrast to most other metrics, our method does not display a high bias towards statistical models of translation. 1 Introduction Since their appearance, string-based evaluation metrics such as BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) have been the standard tools used for evaluating MT quality. Both score a candidate translation on the basis of the number of n-grams shared with one or more reference translations. Automatic measures are indispensable in the development of MT systems, because they allow MT developers to conduct frequent, costeffective, and fast evaluations of their evolving models. These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any dive"
W07-0411,2006.amta-papers.25,0,0.0752774,"es is calculated, giving the precision, recall, and f-score for that particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al. (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment. The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consortium’s (LDC) Multiple Translation project; Section 5"
W07-0411,2003.mtsummit-papers.51,0,0.639646,"mpared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al. (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment. The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consort"
W07-0411,2004.tmi-1.9,0,0.058185,"nce strings, and will penalize any divergence from them. In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical and syntactic choices it contains, even though perfectly legitimate, are not present in at least one of the references. Necessarily, this score would differ from a much more favourable human judgement that such a translation would receive. The limitations of string comparison are the reason why it is advisable to provide multiple references for a candidate translation in BLEU- or NIST-based evaluations. While Zhang and Vogel (2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation. In addition, in practice even a number of references do not capture the whole potential variability of the translation. Moreover, when designing a statistical MT system, the need for large amounts of training data limits the researcher to collections of parallel corpora such as Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creatin"
W07-0411,E06-1032,0,\N,Missing
W07-0411,W06-3112,1,\N,Missing
W07-0714,W05-0909,0,0.110351,"nd the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium’s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes. terms of functional a"
W07-0714,P04-1041,1,0.860716,"Missing"
W07-0714,N06-1058,0,0.342387,"a treebank-based, probabilistic LFG parser (Cahill et al., 2004), which produces a set of dependency triples for each input. The translation set is compared to the reference set, and the number of matches is calculated, giving the 104 Proceedings of the Second Workshop on Statistical Machine Translation, pages 104–111, c Prague, June 2007. 2007 Association for Computational Linguistics precision, recall, and f-score for each particular translation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium’s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining de"
W07-0714,koen-2004-pharaoh,0,0.0345475,"Missing"
W07-0714,2005.mtsummit-papers.11,0,0.017666,"Missing"
W07-0714,2004.tmi-1.8,0,0.0233568,"Kauchak and Barzilay (2006) and Owczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al. (2006). Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al. (2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation’s distance from human-level quality. 3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering. By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information. Two of these metrics are based on matching syntactic subtrees between the translation and the reference, and one 3 A demo of the parser can be found at http"
W07-0714,E06-1031,0,0.0428548,"ment of other automatic MT evaluation metrics. Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference. Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. Kauchak and Barzilay (2006) and Owczarzak et al. (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al. (2006). Another metric making use of synonyms is the linear regression model dev"
W07-0714,W05-0904,0,0.794276,"nces between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references. A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference. Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. Dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide a “normalized” representation of (some) syntactic variants of a given sentence. While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency stru"
W07-0714,J03-1002,0,0.00266914,"Missing"
W07-0714,P02-1040,0,0.111228,"e present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser. Our dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. 1 Introduction Since the creation of BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), the subject of automatic evaluation metrics for MT has been given quite a lot of attention. Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level (Callison-Burch et al., 2006). As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the tra"
W07-0714,2006.amta-papers.25,0,0.245971,"fferences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium’s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Se"
W07-0714,2003.mtsummit-papers.51,0,0.380941,"on, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium’s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Tr"
W07-0714,2004.tmi-1.9,0,0.0676059,"Missing"
W07-0714,E06-1032,0,\N,Missing
W07-0714,J03-4003,0,\N,Missing
W07-0714,W06-3112,1,\N,Missing
W08-0326,P07-1037,1,0.912581,"Missing"
W08-0326,P07-2045,0,0.0164865,"he Czech–English News Commentary and News tasks. This language pair • Chunking Module: outputs a set of chunks given an input corpus, • Chunk Alignment Module: outputs aligned chunk pairs given source and target chunks extracted from comparable corpora, • Decoder: returns optimal translation given a set of aligned sentence, chunk/phrase and word pairs. In some cases, these modules may comprise wrappers around pre-existing software. For example, our system configuration for the shared task incorporates a wrapper around G IZA ++ (Och and Ney, 2003) for word alignment and a wrapper around Moses (Koehn et al., 2007) for decoding. It 171 Proceedings of the Third Workshop on Statistical Machine Translation, pages 171–174, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics should be noted, however, that the complete system is not limited to using only these specific module choices. The following subsections describe those modules unique to our system. 2.1 Marker-Based Chunking The chunking module used for the shared task is based on the Marker Hypothesis, a psycholinguistic constraint which posits that all languages are marked for surface syntax by a specific closed set of lexe"
W08-0326,N03-1017,0,0.0074248,"le 1. 3.2 B LEU (-EBMT) 0.3283 0.2768 0.2235 System Configuration As mentioned in Section 2, our word alignment module employs a wrapper around G IZA ++. We built a 5-gram language model based the target side of the training data. This was done using the SRI Language Modelling toolkit (Stolcke, 2002) employing linear interpolation and modified KneserNey discounting (Chen and Goodman, 1996). Our phrase-table comprised a combination of marker-based chunk pairs2 , extracted as described in Sections 2.1 and 2.2, and word-alignment-based phrase pairs extracted using the “grow-diag-final” method of Koehn et al. (2003), with a maximum phrase length of 7 words. Phrase translation probabilities were estimated by relative frequency over all phrase pairs and were combined with other features, 2 This module was omitted from the Czech–English system as we have yet to verify whether marker-based chunking is appropriate for Czech. 173 such as a reordering model, in a log-linear combination of functions. We tuned our system on the development set devtest2006 for the EuroParl tasks and on nc-test2007 for Czech–English, using minimum error-rate training (Och, 2003) to optimise BLEU score. Finally, we carried out decod"
W08-0326,P07-1039,1,0.84642,"ge are combined in a log-linear framework. The weights of the log-linear model are not optimised; we experimented with different sets of parameters and did not find any significant difference as long as the weights stay in the interval [0.5 − 1.5]. Outside this interval, the quality of the model decreases. More details about the combination of knowledge sources can be found in (Stroppa and Way, 2006). 2.3 Unused Modules There are numerous other features available in our system which, due to time constraints, were not exploited for the purposes of the shared task. They include: • Word packing (Ma et al., 2007): a bilingually motivated packing of words that changes the basic unit of the alignment process in order to simplify word alignment. a We first consider alignments such as those obtained by an edit-distance algorithm, i.e. a = (t1 , s1 )(t2 , s2 ) . . . (tn , sn ), with ∀k ∈ J1, nK, tk ∈ J0, IK and sk ∈ J0, JK, and ∀k < k′ : tk ≤ tk′ or tk′ = 0, sk ≤ sk′ or sk′ = 0, where tk = 0 (resp. sk = 0) denotes a non-aligned target (resp. source) chunk. We then assume the following model: P(a, e|f ) = Πk P(tk , sk , e|f ) = Πk P(etk |fsk ), • Supertagging (Hassan et al., 2007b): incorporating lexical sy"
W08-0326,P03-1021,0,0.00966521,"racted using the “grow-diag-final” method of Koehn et al. (2003), with a maximum phrase length of 7 words. Phrase translation probabilities were estimated by relative frequency over all phrase pairs and were combined with other features, 2 This module was omitted from the Czech–English system as we have yet to verify whether marker-based chunking is appropriate for Czech. 173 such as a reordering model, in a log-linear combination of functions. We tuned our system on the development set devtest2006 for the EuroParl tasks and on nc-test2007 for Czech–English, using minimum error-rate training (Och, 2003) to optimise BLEU score. Finally, we carried out decoding using a wrapper around the Moses decoder. 3.3 Post-processing Case restoration was carried out by training the system outlined above - without the EBMT chunk extraction - to translate from the lowercased version of the applicable target language training data to the truecased version. We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al., 2007a). The translations were then detokenised. 4 Results The system output is evaluated with respect to BLEU score. Results on the develo"
W08-0326,J03-1002,0,0.0154199,"chunks into a single translation model. We also participated in the Czech–English News Commentary and News tasks. This language pair • Chunking Module: outputs a set of chunks given an input corpus, • Chunk Alignment Module: outputs aligned chunk pairs given source and target chunks extracted from comparable corpora, • Decoder: returns optimal translation given a set of aligned sentence, chunk/phrase and word pairs. In some cases, these modules may comprise wrappers around pre-existing software. For example, our system configuration for the shared task incorporates a wrapper around G IZA ++ (Och and Ney, 2003) for word alignment and a wrapper around Moses (Koehn et al., 2007) for decoding. It 171 Proceedings of the Third Workshop on Statistical Machine Translation, pages 171–174, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics should be noted, however, that the complete system is not limited to using only these specific module choices. The following subsections describe those modules unique to our system. 2.1 Marker-Based Chunking The chunking module used for the shared task is based on the Marker Hypothesis, a psycholinguistic constraint which posits that all langu"
W08-0326,J93-2003,0,0.00801712,"dit-distance-style” dynamic programming alignment algorithm. In the following, a denotes an alignment between a target sequence e consisting of I chunks and a source sequence f consisting of J chunks. Given these sequences of chunks, we are looking for the most likely alignment a ˆ: a ˆ = argmax P(a|e, f ) = argmax P(a, e|f ). a Assuming that the parameters P(etk |fsk ) are known, the most likely alignment is computed by a simple dynamic-programming algorithm.1 Instead of using an Expectation-Maximization algorithm to estimate these parameters, as commonly done when performing word alignment (Brown et al., 1993; Och and Ney, 2003), we directly compute these parameters by relying on the information contained within the chunks. The conditional probability P(etk |fsk ) can be computed in several ways. In our experiments, we have considered three main sources of knowledge: (i) word-to-word translation probabilities, (ii) word-to-word cognates, and (iii) chunk labels. These sources of knowledge are combined in a log-linear framework. The weights of the log-linear model are not optimised; we experimented with different sets of parameters and did not find any significant difference as long as the weights s"
W08-0326,P96-1041,0,0.0242566,"ch data was already tokenised) and removed blank lines. We then filtered out sentence pairs based on length (>100 words) and fertility (9:1 word ratio). Finally we lowercased the data. Details of this preprocessing are given in Table 1. 3.2 B LEU (-EBMT) 0.3283 0.2768 0.2235 System Configuration As mentioned in Section 2, our word alignment module employs a wrapper around G IZA ++. We built a 5-gram language model based the target side of the training data. This was done using the SRI Language Modelling toolkit (Stolcke, 2002) employing linear interpolation and modified KneserNey discounting (Chen and Goodman, 1996). Our phrase-table comprised a combination of marker-based chunk pairs2 , extracted as described in Sections 2.1 and 2.2, and word-alignment-based phrase pairs extracted using the “grow-diag-final” method of Koehn et al. (2003), with a maximum phrase length of 7 words. Phrase translation probabilities were estimated by relative frequency over all phrase pairs and were combined with other features, 2 This module was omitted from the Czech–English system as we have yet to verify whether marker-based chunking is appropriate for Czech. 173 such as a reordering model, in a log-linear combination of"
W08-0326,2007.tmi-papers.28,1,0.892037,"Missing"
W08-0326,2004.tmi-1.11,1,0.786387,"extendible and re-implementable modules, the most significant of which are: • Word Alignment Module: outputs a set of word alignments given a parallel corpus, 1 Introduction In this paper, we present the Data-Driven MT systems developed at DCU, M AT R E X (Machine Translation using Examples). This system is a hybrid system which exploits EBMT and SMT techniques to build a combined translation model. We participated in both the French–English and Spanish–English EuroParl tasks. In these two tasks, we monolingually chunk both source and target sides of the dataset using a marker-based chunker (Gough and Way, 2004). We then align these chunks using a dynamic programming, edit-distance-style algorithm and combine them with phrase-based SMT-style chunks into a single translation model. We also participated in the Czech–English News Commentary and News tasks. This language pair • Chunking Module: outputs a set of chunks given an input corpus, • Chunk Alignment Module: outputs aligned chunk pairs given source and target chunks extracted from comparable corpora, • Decoder: returns optimal translation given a set of aligned sentence, chunk/phrase and word pairs. In some cases, these modules may comprise wrapp"
W08-0326,2006.iwslt-evaluation.4,1,0.839207,"computed in several ways. In our experiments, we have considered three main sources of knowledge: (i) word-to-word translation probabilities, (ii) word-to-word cognates, and (iii) chunk labels. These sources of knowledge are combined in a log-linear framework. The weights of the log-linear model are not optimised; we experimented with different sets of parameters and did not find any significant difference as long as the weights stay in the interval [0.5 − 1.5]. Outside this interval, the quality of the model decreases. More details about the combination of knowledge sources can be found in (Stroppa and Way, 2006). 2.3 Unused Modules There are numerous other features available in our system which, due to time constraints, were not exploited for the purposes of the shared task. They include: • Word packing (Ma et al., 2007): a bilingually motivated packing of words that changes the basic unit of the alignment process in order to simplify word alignment. a We first consider alignments such as those obtained by an edit-distance algorithm, i.e. a = (t1 , s1 )(t2 , s2 ) . . . (tn , sn ), with ∀k ∈ J1, nK, tk ∈ J0, IK and sk ∈ J0, JK, and ∀k < k′ : tk ≤ tk′ or tk′ = 0, sk ≤ sk′ or sk′ = 0, where tk = 0 (resp"
W08-0409,ayan-etal-2004-multi,0,0.0134731,"rately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If 69 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics we can first obtain a set of reliable anchor links, we could take advantage of the synt"
W08-0409,P06-1009,0,0.0556713,"ased on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each ot"
W08-0409,J93-2003,0,0.0259015,"ically significant improvement in Chinese–English machine translation in comparison with the baseline word alignment. 1 Introduction Automatic word alignment can be defined as the problem of determining translational correspondences at word level given a parallel corpus of aligned sentences. Bilingual word alignment is a fundamental component of most approaches to statistical machine translation (SMT). Dominant approaches to word alignment can be classified into two main schools: generative and discriminative word alignment models. Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment. However, it is very difficult to incorporate new features into these models. Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual langu"
W08-0409,P06-2014,0,0.127701,"d alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If 69 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencies relating unaligned words to aligned anchor words to expand the alignment. Figure 1 give"
W08-0409,P07-1001,0,0.0901465,"f I words e1 , ..., eI , we define the alignment A between cJ1 and eI1 as a subset of the Cartesian product of the word positions: A ⊆ {(j, i) : j = 1, ..., J; i = 1, ..., I} 70 2.3 Anchor Alignment Model The anchor alignment model pǫ (A∆ ) aims to find a set of high precision links. Various approaches can be used for this purpose. In this paper we adopted the following two approaches. 2.3.1 Heuristics-based Approach The problem of word alignment is regarded as a process of word linkage disambiguation, i.e. choosing the correct links between words from all competing hypothesis (Melamed, 2000; Deng and Gao, 2007). We constrain the link probabilities in such a way that: p((j, i)) &gt; ǫ1 p((j, i′ )) p((j, i)) ∀j ′ ∈ {1, ..., J}, j ′ = 6 j: &gt; ǫ2 p((j ′ , i)) ∀i′ ∈ {1, ..., I}, i′ 6= i : (5) (6) Condition (5) implies that for the source word cj , the link with the target word ei is more probable (with reliability threshold ǫ1 ) than the link with any other target word. Condition (6) guarantees that for the target word ei , cj is the only most probable (with threshold ǫ2 ) source word to be linked to. 2.3.2 Intersected Generative Word Alignment Models We can use the asymmetric IBM models for bidirectional wo"
W08-0409,J93-1003,0,0.013538,"sentence eI1 , we search for the alignment aj such that: 3.1 Statistics-based Features 3.1.1 IBM model 1 score IBM model 1 is a position-independent word alignment model which is often used to bootstrap parameters for more complex models. Model 1 models the conditional distribution and uses a uniform distribution for the dependencies between source word positions and target word positions. P r(cJ1 , aJ1 |eI1 ) = J p(J|I) Y p(cj |eaj ) (I + 1)J (8) j=1 3.1.2 Log-likelihood ratio The log-likelihood ratio statistic has been found to be accurate for modeling the associations between rare events (Dunning, 1993). It has also been successfully used to measure the associations between word pairs (Melamed, 2000; Moore, 2005). Given the following contingency table: cj ¬cj ei a b ¬ei c d the log-likelihood ratio can be defined as: G2 (cj , ei ) = −2log B(a|a + b, p1 )B(c|c + d, p2 ) B(a|a + b, p)B(c|c + d, p) where B(k|n, p) = (nk )pk (1 − p)n−k are binomial aˆj = argmax{pλM (aj |cJ1 , eI1 , a1j−1 , A∆ )} (7) probabilities. The probability parameters can be ob1 aj tained using maximum likelihood estimates: PM j J I = argmax{ m=1 λm hm (c1 , e1 , a1 , A∆ , Tc , Te )} aj a c p1 = , p2 = (9) a+b c+d In this"
W08-0409,P02-1050,0,0.0299532,"Missing"
W08-0409,H05-1012,0,0.0630996,"(1) A Figure 1: How syntactic dependencies can help word alignment: an example The remainder of this paper is organized as follows. In Section 2, we introduce our syntaxenhanced discriminative word alignment approach. The feature functions used are described in Section 3. Experimental setting and results are presented in Section 4 and 5 respectively. In Section 6, we compare our approach with other related word alignment approaches. Section 7 concludes the paper and gives avenues for future work. We use a model (2) that directly models the linkage between source and target words similarly to (Ittycheriah and Roukos, 2005). We decompose this model into an anchor alignment model (3) and a syntax-enhanced model (4) by distinguishing the anchor alignment from the non-anchor alignment. p(A|cJ1 , eI1 ) = p(aj |cJ1 , eI1 , a1j−1 ) (2) j=0 = 2 Word Alignment Model 2.1 J Y 1 · pǫ (A∆ |cJ1 , eI1 ) · (3) Z Y p(aj |cJ1 , eI1 , a1j−1 , A∆ ) (4) ¯ j∈∆ Notation While in this paper we focus on Chinese–English, the method proposed is applicable to any language pair. The notation will assume Chinese–English word alignment and Chinese–English MT. Here we adopt a notation similar to (Brown et al., 1993). Given a Chinese sentence"
W08-0409,N03-1017,0,0.0217447,"Missing"
W08-0409,P07-2045,0,0.0601009,"Missing"
W08-0409,N06-1014,0,0.0799062,"Missing"
W08-0409,P05-1057,0,0.169804,"is a fundamental component of most approaches to statistical machine translation (SMT). Dominant approaches to word alignment can be classified into two main schools: generative and discriminative word alignment models. Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment. However, it is very difficult to incorporate new features into these models. Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide"
W08-0409,W05-0812,0,0.0184919,", which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If 69 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics we can first obtain a set of reliable anchor"
W08-0409,J00-2004,0,0.165168,"I1 consisting of I words e1 , ..., eI , we define the alignment A between cJ1 and eI1 as a subset of the Cartesian product of the word positions: A ⊆ {(j, i) : j = 1, ..., J; i = 1, ..., I} 70 2.3 Anchor Alignment Model The anchor alignment model pǫ (A∆ ) aims to find a set of high precision links. Various approaches can be used for this purpose. In this paper we adopted the following two approaches. 2.3.1 Heuristics-based Approach The problem of word alignment is regarded as a process of word linkage disambiguation, i.e. choosing the correct links between words from all competing hypothesis (Melamed, 2000; Deng and Gao, 2007). We constrain the link probabilities in such a way that: p((j, i)) &gt; ǫ1 p((j, i′ )) p((j, i)) ∀j ′ ∈ {1, ..., J}, j ′ = 6 j: &gt; ǫ2 p((j ′ , i)) ∀i′ ∈ {1, ..., I}, i′ 6= i : (5) (6) Condition (5) implies that for the source word cj , the link with the target word ei is more probable (with reliability threshold ǫ1 ) than the link with any other target word. Condition (6) guarantees that for the target word ei , cj is the only most probable (with threshold ǫ2 ) source word to be linked to. 2.3.2 Intersected Generative Word Alignment Models We can use the asymmetric IBM models"
W08-0409,H05-1011,0,0.172055,"component of most approaches to statistical machine translation (SMT). Dominant approaches to word alignment can be classified into two main schools: generative and discriminative word alignment models. Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment. However, it is very difficult to incorporate new features into these models. Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant info"
W08-0409,J03-1002,0,0.011799,"p://svmlight.joachims.org/ 5 More specifically, we performed 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. ment and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a trigram language model with KneserNey smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode. 4.4 Evaluation We evaluate the intrinsic quality of predicted alignment A with precision, recall and alignment error rate (AER). Slightly differently from (Och and Ney, 2003), we use possible alignments in computing recall. |A ∩ P | |A ∩ P | , precision = |P | |A| |A ∩ S |+ |A ∩ P | AER(S, P ; A) = 1 − |A |+ |S| recall = f-score 0.7811 0.7959 0.7964 0.8102 recall 0.4047 0.5011 0.5327 0.5677 f-measure 0.5724 0.6563 0.6903 0.7179 AER 0.3947 0.3157 0.2809 0.2533 of the anchor alignment model, we first obtained the intersection of the words left unaligned after anchoring using each of the anchor alignment models. We evaluate the alignment of these words against the gold-standard alignments involving these words. The influence of anchor alignment on the performance of"
W08-0409,P03-1021,0,0.00765498,"Missing"
W08-0409,W04-2207,1,0.822112,"ances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If 69 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencie"
W08-0409,P02-1040,0,0.105628,"l HMM refined Syntax-HMM Model 4 refined Syntax-Model 4 The Influence of Anchor Alignment Quality As we can see in Table 2, our precision-oriented approach to acquire anchor alignments was accomplished quite well. All four different anchor alignment models achieved high precision. However, the recall differs dramatically, with model 4 achieving the highest recall and the heuristics-based approach receiving the lowest. To investigate the influence (20) We also extrinsically measure the word alignment quality via a Chinese–English translation task. The translation output is measured using BLEU (Papineni et al., 2002). 5.1 5.1.1 AER 0.2059 0.1845 0.1929 0.1730 Table 1: Comparing syntax-enhanced approach with generative word alignment 74 anchor model Heuristics Model 1 HMM Model 4 precision 0.4505 0.5538 0.5932 0.5660 recall 0.3270 0.3894 0.3611 0.4216 f-score 0.3790 0.4573 0.4489 0.4832 AER 0.6210 0.5427 0.5511 0.5168 Table 3: Influence of anchor alignment in syntaxenhanced model 5.1.2 The Influence of Syntactic Dependencies on Word Alignment The influence of incorporating syntactic dependencies into the word alignment process is shown in Table 4. Syntax plays a positive role in all different anchor alignm"
W08-0409,W96-0213,0,0.10914,"Missing"
W08-0409,2007.mtsummit-papers.52,0,0.020595,"e more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If 69 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation"
W08-0409,W00-1314,0,0.0646624,"Missing"
W08-0409,takezawa-etal-2002-toward,0,0.0670154,"Missing"
W08-0409,2006.iwslt-papers.7,0,0.0269839,"ase in AER. model Heuristics no syntax w. syntax Model 1 no syntax w. syntax HMM no syntax w. syntax Model 4 no syntax w. syntax precision recall f-score AER 0.8362 0.8376 0.6751 0.6894 0.7470 0.7563 0.2302 0.2240 0.8759 0.8542 0.6902 0.7160 0.7720 0.7790 0.2045 0.2011 0.8655 0.8744 0.7168 0.7304 0.7841 0.7959 0.1952 0.1845 0.8697 0.8566 0.7340 0.7685 0.7961 0.8102 0.1832 0.1730 Table 4: Influence of syntactic dependencies on word alignment 5.1.3 5.2 Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006). Hereafter, we used a Chinese–English MT task to extrinsically evaluate the quality of our word alignment. Table 6 shows the influence of our word alignment approach on MT quality.6 On development set, we achieved statistically significant improvement using both our syntax-enhanced models—SyntaxHMM (p&lt;0.002) and Syntax-Model 4 (p&lt;0.008). On the test set, we observed that the MT output based on our alignment model tends to be shorter than the reference translations and the BLEU score is considerably penalized. If we ignore the length penalty (‘BP’ in Table 6) in significance testing, the impro"
W08-0409,C96-2141,0,0.416874,"lation in comparison with the baseline word alignment. 1 Introduction Automatic word alignment can be defined as the problem of determining translational correspondences at word level given a parallel corpus of aligned sentences. Bilingual word alignment is a fundamental component of most approaches to statistical machine translation (SMT). Dominant approaches to word alignment can be classified into two main schools: generative and discriminative word alignment models. Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment. However, it is very difficult to incorporate new features into these models. Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance of the system. Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alig"
W08-0409,W04-3226,0,0.128088,"of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If 69 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics we can first obtain"
W08-0409,J97-3002,0,0.0603969,"ce for word alignment tasks. For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006). Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000). Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006). In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies. Our approach is motivated by the fact that words tend to be dependent on each other. If 69 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 69–77, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencies relating unaligned words to aligned anchor words to expand the a"
W08-0409,2007.iwslt-1.1,0,\N,Missing
W09-0416,P96-1041,0,0.0133032,"However the sentence was dropped if the length ratio between English and French was larger than 1.5 or less than 0.67. 3.3 Monolingual Corpus In this evaluation, we trained a small 4-gram language model using data in Table 1 and a large 4gram language model using data in Table 2. We configured these two LMs for Baseline and EBMT systems while HPB only used the large one. Language English French Sen 9,966,838 9,966,838 Token 240,849,221 260,520,313 Pre-Processing System Configuration The two language models were done using the SRILM employing linear interpolation and modified K-N discounting (Chen and Goodman, 1996). The configuration for the three systems is listed in Table 3. System Baseline-E Baseline-G EBMT HPB Source E/N/NC E/N/NC Table 2: Statistics of Monolingual Data P-Table 55.9M 58.4M 59.4M 122M Length 7 7 7 5 LM 2 2 2 1 Features 15 15 15 8 Table 3: Statistics of MT Systems In the above table, E/N/NC refers to Europarl/News/New Commentary corpus. In this table, E indicates the Europarl corpus 97 System Baseline-E Baseline-G EBMT HPB Combination Official-Auto2 which is used for all three systems, and G stands for the Giga corpus which is only used for the Baseline system. We can see from Table 3"
W09-0416,2006.iwslt-evaluation.4,1,0.189534,"ackbone which determines the word order of the combination. The other hypotheses are aligned against the backbone based on the TER metric. NULL words are allowed in the alignment. Each arc in the CN represents an alternative word at that position in the sentence and the number of votes for each word is counted when constructing the network. The features we used are as follows: Figure 1: System Framework We then align these segments using an editdistance-style algorithm, in which the insertion and deletion probabilities depend on word-toword translation probabilities and word-to-word cognates (Stroppa and Way, 2006). We extracted phrases of at most 7 words on each side. We then merged these phrases with the phrases extracted by the baseline system adding word alignment information, and used this system seeded with this additional information. 2.3 Hierarchical Machine Translation • word posterior probability (Fiscus, 1997); HPB translation system is a re-implementation of the hierarchical phrase translation model which is based on PSCFG (Chiang, 2005). We generate recursively PSCFG rules from the initial rules as • 3, 4-gram target language model; • word length penalty; • Null word length penalty; N → f1"
W09-0416,W06-3110,0,0.0160589,"add some new global features in rescore model. The features we used are as follows: where 1 ≤ i ≤ j ≤ m and 1 ≤ u ≤ v ≤ n, at which point a new rule can be obtained, named, m Xk env+1 /eu−1 N → f1i−1 Xk fj+1 1 • Direct and inverse IBM model; where k is an index for the nonterminal X. The number of nonterminals permitted in a rule is no more than two. When extracting hierarchical rules,we set some limitations that initial rules are of no more than • 3, 4-gram target language model; • 3, 4, 5-gram POS language model (Ratnaparkhi, 1996; Schmid, 1994); 96 • Sentence length posterior probability (Zens and Ney, 2006); 3.2 We preprocessed both Europarl and Giga Release 1 corpus. For the Europarl corpus, we removed the reserved characters in GIZA++ and tokenized and lowercased the corpus with tools provided by WMT09. The Giga corpus was too large for our resource, so we performed sentence selection before cleaning, in the following steps. • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; • We split the Giga corpus into even segments, each segment consisting of 20 lines. The weights are optimized"
W09-0416,2004.tmi-1.11,1,0.777111,"erated by the combination module. See Figure 1 as a detailed illustration. Introduction In this paper, we present a multi-engine MT system developed at DCU, M AT R E X (Machine Translation using Examples). This system exploits EBMT, SMT and system combination techniques to build a cascaded translation framework. We participated in both the French–English and English-French News tasks. In these two tasks, we employ three individual MT system which are 1) Baseline: phrase-based system (PB); 2) EBMT: Monolingually chunking both source and target sides of the dataset using a marker-based chunker (Gough and Way, 2004). 3) HPB: a typical hierarchical phrase-based system (Chiang, 2005). Meanwhile, we also use a word-level combination framework (Rosti et al., 2007) to combine the multiple translation hypotheses and employ a new rescoring model to generate the final result. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and 2.2 Example-Based Machine Translation EBMT obtains resources using the Marker Hypothesis (Green, 1979), a psycholinguistic constraint which posits that all languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signi"
W09-0416,N04-1022,0,0.212599,"Missing"
W09-0416,P03-1021,0,0.0132018,"on each side. We then merged these phrases with the phrases extracted by the baseline system adding word alignment information, and used this system seeded with this additional information. 2.3 Hierarchical Machine Translation • word posterior probability (Fiscus, 1997); HPB translation system is a re-implementation of the hierarchical phrase translation model which is based on PSCFG (Chiang, 2005). We generate recursively PSCFG rules from the initial rules as • 3, 4-gram target language model; • word length penalty; • Null word length penalty; N → f1 . . . fm /e1 . . . en Also, we use MERT (Och, 2003) to tune the weights of confusion network. where N is a rule which is initial or includes nonterminals. 2.5 M → fi . . . fj /eu . . . ev Rescore Rescore is a very important part in post-processing which can select a better hypothesis from the N best list. We add some new global features in rescore model. The features we used are as follows: where 1 ≤ i ≤ j ≤ m and 1 ≤ u ≤ v ≤ n, at which point a new rule can be obtained, named, m Xk env+1 /eu−1 N → f1i−1 Xk fj+1 1 • Direct and inverse IBM model; where k is an index for the nonterminal X. The number of nonterminals permitted in a rule is no mor"
W09-0416,P02-1040,0,0.0759286,"M) (Stolcke, 2002) is used in the cube-pruning process. The search space is pruned with a chart cell size limit of 50. EBMT TestSet Dev/MERT Rescore Rescore/MERT Decoding Mutiple 1-best Mutiple 1-best MBR Decoder CN/MERT MBR Decoder System Combination CN Decoder Rescore/MERT Rescore Recaser Recaser 2.4 System Combination For multiple system combination, we implement an MBR-CN framework as shown in Figure 1. Instead of using a single system output as the skeleton, we employ a minimum Bayes-risk decoder to select the best single system output from the merged N -best list by minimizing the BLEU (Papineni et al., 2002) loss. The confusion network is built by the output of MBR as the backbone which determines the word order of the combination. The other hypotheses are aligned against the backbone based on the TER metric. NULL words are allowed in the alignment. Each arc in the CN represents an alternative word at that position in the sentence and the number of votes for each word is counted when constructing the network. The features we used are as follows: Figure 1: System Framework We then align these segments using an editdistance-style algorithm, in which the insertion and deletion probabilities depend o"
W09-0416,W96-0213,0,0.111922,"post-processing which can select a better hypothesis from the N best list. We add some new global features in rescore model. The features we used are as follows: where 1 ≤ i ≤ j ≤ m and 1 ≤ u ≤ v ≤ n, at which point a new rule can be obtained, named, m Xk env+1 /eu−1 N → f1i−1 Xk fj+1 1 • Direct and inverse IBM model; where k is an index for the nonterminal X. The number of nonterminals permitted in a rule is no more than two. When extracting hierarchical rules,we set some limitations that initial rules are of no more than • 3, 4-gram target language model; • 3, 4, 5-gram POS language model (Ratnaparkhi, 1996; Schmid, 1994); 96 • Sentence length posterior probability (Zens and Ney, 2006); 3.2 We preprocessed both Europarl and Giga Release 1 corpus. For the Europarl corpus, we removed the reserved characters in GIZA++ and tokenized and lowercased the corpus with tools provided by WMT09. The Giga corpus was too large for our resource, so we performed sentence selection before cleaning, in the following steps. • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; • We split the Giga corpus in"
W09-0416,N07-1029,0,0.0280315,"at DCU, M AT R E X (Machine Translation using Examples). This system exploits EBMT, SMT and system combination techniques to build a cascaded translation framework. We participated in both the French–English and English-French News tasks. In these two tasks, we employ three individual MT system which are 1) Baseline: phrase-based system (PB); 2) EBMT: Monolingually chunking both source and target sides of the dataset using a marker-based chunker (Gough and Way, 2004). 3) HPB: a typical hierarchical phrase-based system (Chiang, 2005). Meanwhile, we also use a word-level combination framework (Rosti et al., 2007) to combine the multiple translation hypotheses and employ a new rescoring model to generate the final result. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and 2.2 Example-Based Machine Translation EBMT obtains resources using the Marker Hypothesis (Green, 1979), a psycholinguistic constraint which posits that all languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signify context. Given a set of closed-class words we segment each sentence into chunks, creating a chunk at each new occurrence of a marker word, with"
W09-0416,2006.amta-papers.25,0,0.0616424,"Missing"
W09-0416,P05-1033,0,\N,Missing
W09-1509,1983.tc-1.13,0,0.368785,"n be ascertained by other elements in the workflow – allowing them to react to the progress (or lack thereof) in the task and, for example, to allow the job to be redirected to another process when it is not progressing satisfactorily. Our service design is focused on supporting crowd-sourcing, but it is intended to extend it to offer general-purpose support for the integration of human-tasks into a BPEL workflow. It serves as a testbed and proof of concept for the development of a generic localization human task interface. The initial specification has been derived from the TWS specification [tws], but incorporates several important changes. Firstly, it is greatly simplified by removing all the quote-related functions and replacing them with the RequestJob and SubmitJob functions and combining all of the job control functions into a single updateJob function and combining the two job list functions into one. TWS, as a standard focused on support for localization outsourcing – hence the concentration on negotiating ‗quotes‘ between partners. Our requirements are quite different – we cannot assume that there is any price, or even any formal agreement which governs crowd-sourcing. Indeed,"
W09-1509,2008.iwslt-evaluation.3,1,\N,Missing
W09-3523,kang-choi-2000-automatic,0,0.0293469,"ss problems.We carried out experiments both at character and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage & Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvo"
W09-3523,P02-1038,0,0.046724,"sults obtained, together with some analysis. Section 5 concludes the paper. 2 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . eI of a source sentence f1 J = f1 . . . fJ is chosen to maximize (1): arg max P(e1I |f1J )  arg max P( f1J |e1I ).P(e1I ) (1) I , e1I I , e1I where P ( f1J |e1I ) and P (e1I ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): m log P ( e1I |f 1 J )   m h m ( f 1 J , e1I , s1K ) m 1   LM log P ( e1I ) (2) K 1 where s  s1 ...s k denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ1 ,..., eˆk ) and ( fˆ1 ,..., fˆk ) such that (we set i0 = 0) (3):  1  k  K , sk = (ik ; bk, jk), eˆk  ei k 1 1...ei k , fˆk  fbk ... f j k (3) The translational features involved depend only on a pair of source/target phrases and do not take into account any context of these phrases. This mea"
W09-3523,W06-1607,0,0.0128369,"ation is normalised to estimate P( eˆk |fˆk ,CI( fˆk )). Therefore our expected feature is derived as in (8): hˆm ( fˆk ,CI( fˆk ), eˆk , sk) = log P( eˆk |fˆk , CI( fˆk )) (7) 3.1 Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating P( eˆk |fˆk , CI( fˆk )) using relative frequencies is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of contextinformed features, since the context is also taken Implementation Issues hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (8) As for the standard phrase-based approach, their weights are optimized using Minimum Error Rate Training (MERT) of (Och, 2003) for each of the experiments. As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrasetable represented as a list of aligned phrases accompanied with several features. Since these fea1 An implementation of IGTree, IB1 and TRIBL is available in the TiMBL software p"
W09-3523,P04-1021,0,0.138344,"se features while avoiding data sparseness problems.We carried out experiments both at character and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage & Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterleve"
W09-3523,P07-2045,0,0.00641492,"troduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage & Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvowel cluster as one transliteration unit. The remainder of the paper is organized as follows. In section 2 we give a brief overview of PB-SMT. Section 3 describes how contextinformed features are incorporated i"
W09-3523,2009.eamt-1.32,1,0.844095,"., 2005). When predicting a target phrase given a source phrase and its context, the source phrase is intuitively the feature with the highest prediction power; in all our experiments, it is the feature with the highest gain ratio (GR). In order to build the set of examples required to train the classifier, we modify the standard phrase-extraction method of (Koehn et al., 2003) to extract the context of the source phrases at the same time as the phrases themselves. Importantly, therefore, the context extraction comes at no extra cost. We refer interested readers to (Stroppa et al., 2007) and (Haque et al., 2009) as well as the references therein for more details of how MemoryBased Learning (MBL) is used for classification of source examples for use in the log-linear MT framework. K hm ( f1J , e1I , s1K )   hˆm ( fˆk , eˆk , sk ) (4) k 1 where hˆm is a feature that applies to a single phrase-pair. Thus (2) can be rewritten as: m K K m 1 k 1 k 1  m  hˆm ( fˆk , eˆk , sk )   hˆ( fˆk , eˆk , sk ) where, hˆ  m  (5) hˆ . In this context, the translam m m 1 tion process amounts to: (i) choosing a segmentation of the source sentence, (ii) translating each source phrase. 3 Source Context Featur"
W09-3523,2007.tmi-papers.28,1,0.868563,"Missing"
W09-3523,N04-1033,0,0.0196323,"xperiments on both character-level (C-L) and TUlevel (TU-L) data. We use a 5-gram language model for all our experiments. The Moses PBSMT system serves as our baseline system. The distribution of target phrases given a source phrase and its contextual information is normalised to estimate P( eˆk |fˆk ,CI( fˆk )). Therefore our expected feature is derived as in (8): hˆm ( fˆk ,CI( fˆk ), eˆk , sk) = log P( eˆk |fˆk , CI( fˆk )) (7) 3.1 Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating P( eˆk |fˆk , CI( fˆk )) using relative frequencies is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of contextinformed features, since the context is also taken Implementation Issues hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (8) As for the standard phrase-based approach, their weights are optimized using Minimum Error Rate Training (MERT) of (Och, 2003) for each of the experiments. As (Stroppa et al., 2007) point out, PB-SMT deco"
W09-3523,J93-2003,0,0.0102579,"e a brief overview of PB-SMT. Section 3 describes how contextinformed features are incorporated into state-ofart log-linear PB-SMT. Section 4 includes the results obtained, together with some analysis. Section 5 concludes the paper. 2 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . eI of a source sentence f1 J = f1 . . . fJ is chosen to maximize (1): arg max P(e1I |f1J )  arg max P( f1J |e1I ).P(e1I ) (1) I , e1I I , e1I where P ( f1J |e1I ) and P (e1I ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): m log P ( e1I |f 1 J )   m h m ( f 1 J , e1I , s1K ) m 1   LM log P ( e1I ) (2) K 1 where s  s1 ...s k denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ1 ,..., eˆk ) and ( fˆ1 ,..., fˆk ) such that (we set i0 = 0) (3):  1  k  K , sk = (ik ; bk, jk), eˆk  ei k 1 1...ei k , fˆk  fbk ... f j"
W09-3523,N03-1017,0,0.0324828,"s of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage & Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvowel cluster as one transliteration unit. The remainder of the paper is organized as follows. In section 2 we give a brief overview of PB-SMT. Section 3 describes how contextinf"
W09-3523,koen-2004-pharaoh,0,0.0229922,"ation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of contextinformed features, since the context is also taken Implementation Issues hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (8) As for the standard phrase-based approach, their weights are optimized using Minimum Error Rate Training (MERT) of (Och, 2003) for each of the experiments. As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrasetable represented as a list of aligned phrases accompanied with several features. Since these fea1 An implementation of IGTree, IB1 and TRIBL is available in the TiMBL software package (http://ilk.uvt.nl/timbl). 105 tures do not express the context in which those phrases occur, no context information is kept in the phrase-table, and there is no way to recover this information from the phrase-table. In order to take into account the contextinformed features for use with such decoders, the devset and testset that need to be translated are pre-proces"
W09-3523,J98-4003,0,\N,Missing
W10-1720,P96-1041,0,0.070396,"M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages, we used the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model using all monolingual data provided. However, for en–es we used the IRSTLM toolkit (Federico and Cettolo, 2007) to train a 5-gram language model using the es Gigaword corpus. Both language models use modified Kneser-Ney smoothing (Chen and Goodman, 1996). Statistics for the monolingual corpora are given in Table 2. Corpus E/N/NC/UN Gigaword News Language es es cs Sentences 9,6M 40M 13M Tokens 290M 1,2G 210M Table 2: Statistics of Monolingual Data. E/N/NC/UN refers to Europarl/News/News Commentary/United Nations corpora. For all the systems except Apertium, we first lowercase and tokenize all the monolingual and bilingual data using the tools provided by the WMT10 organizers. After translation, system 145combination output is detokenised and true-cased. 3.2 English–Czech (en–cs) Experiments ˇ The CzEng corpus (Bojar and Zabokrtsk´ y, 2009) is"
W10-1720,J07-2003,0,0.0791111,"ngually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we use a word-level combination framework (Rosti et al., 2007) to combine the 143which edit costs depend on word-to-word translaProceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143–148, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the above three translation factors: an SF to SF decoding path and a path which maps lemma to l"
W10-1720,W07-0712,0,0.00757178,"Corpus Langs. Sent. Europarl News-comm UN News-Comm CzEng en–es en–es en–es en–cs en–cs 1.6M 97k 5.9M 85k 7.8M Source tokens 43M 2.4M 160M 1.8M 80M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages, we used the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model using all monolingual data provided. However, for en–es we used the IRSTLM toolkit (Federico and Cettolo, 2007) to train a 5-gram language model using the es Gigaword corpus. Both language models use modified Kneser-Ney smoothing (Chen and Goodman, 1996). Statistics for the monolingual corpora are given in Table 2. Corpus E/N/NC/UN Gigaword News Language es es cs Sentences 9,6M 40M 13M Tokens 290M 1,2G 210M Table 2: Statistics of Monolingual Data. E/N/NC/UN refers to Europarl/News/News Commentary/United Nations corpora. For all the systems except Apertium, we first lowercase and tokenize all the monolingual and bilingual data using the tools provided by the WMT10 organizers. After translation, system 1"
W10-1720,2009.freeopmt-1.3,1,0.877177,"Missing"
W10-1720,2004.tmi-1.11,1,0.0884387,"English–Spanish (en– phrase-based and tree-based MT. es) and English–Czech (en–cs) translation The combination structure uses the MBR and tasks. For these two tasks, we employ several CN decoders, and is based on a word-level comindividual MT systems: 1) Baseline: phrasebination strategy (Du et al., 2009). In the final based SMT (Koehn et al., 2007); 2) EBMT: stage, we use a new rescoring module to process Monolingually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we"
W10-1720,Y09-1019,1,0.818274,"oding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI may include any feature (lexical, syntactic, etc.), which can provide useful information to disambiguate a given source phrase. In addition to using local words and PoS-tags as features, as in (Stroppa et al., 2007), we incorporate grammatical dependency relations (Haque et al., 2009a) and supertags (Haque et al., 2009b) as syntactic source context features in the log-linear PB-SMT model. In addition to the above feature, we derived a ˆ best , defined in (2): simple binary feature h ( 1 if eˆk maximizes P (ˆ ek |fˆk , CI(fˆk )) 0 otherwise (2) We performed experiments by integrating these ˆ MBL and h ˆ best , directly into the two features, h log-linear framework of Moses. ˆ best = h 2.6 Hierarchical PB-SMT model For the en–cs translation task, we built a weighted synchronous context-free grammar model (Chiang, 2007) of translation that uses the bilingual phrase pairs of"
W10-1720,2009.eamt-1.32,1,0.0893869,"oding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI may include any feature (lexical, syntactic, etc.), which can provide useful information to disambiguate a given source phrase. In addition to using local words and PoS-tags as features, as in (Stroppa et al., 2007), we incorporate grammatical dependency relations (Haque et al., 2009a) and supertags (Haque et al., 2009b) as syntactic source context features in the log-linear PB-SMT model. In addition to the above feature, we derived a ˆ best , defined in (2): simple binary feature h ( 1 if eˆk maximizes P (ˆ ek |fˆk , CI(fˆk )) 0 otherwise (2) We performed experiments by integrating these ˆ MBL and h ˆ best , directly into the two features, h log-linear framework of Moses. ˆ best = h 2.6 Hierarchical PB-SMT model For the en–cs translation task, we built a weighted synchronous context-free grammar model (Chiang, 2007) of translation that uses the bilingual phrase pairs of"
W10-1720,W04-3250,0,0.127192,"Missing"
W10-1720,2005.mtsummit-papers.11,0,0.0279389,"gth posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we used the Europarl(Koehn, 2005), News Commentary and United Nations parallel data. We used a maximum sentence length of 80 for en–es and 40 for en–cs. Detailed statistics are shown in Table 1. Corpus Langs. Sent. Europarl News-comm UN News-Comm CzEng en–es en–es en–es en–cs en–cs 1.6M 97k 5.9M 85k 7.8M Source tokens 43M 2.4M 160M 1.8M 80M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages"
W10-1720,P07-2045,0,0.0171046,"Examples). This system exploits example-based pects of both the EBMT and SMT paradigms. MT, statistical MT (SMT), and system combinaThe architecture includes various individual systion techniques. tems: phrase-based, example-based, hierarchical We participated in the English–Spanish (en– phrase-based and tree-based MT. es) and English–Czech (en–cs) translation The combination structure uses the MBR and tasks. For these two tasks, we employ several CN decoders, and is based on a word-level comindividual MT systems: 1) Baseline: phrasebination strategy (Du et al., 2009). In the final based SMT (Koehn et al., 2007); 2) EBMT: stage, we use a new rescoring module to process Monolingually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based"
W10-1720,N04-1022,0,0.0614202,"ribe the modular design of our multi-engine machine translation (MT) system with particular focus on the components used in this participation. We participated in the English– Spanish and English–Czech translation tasks, in which we employed our multiengine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder. 1 Introduction multiple translation hypotheses and employ a new rescoring model to generate the final translation. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the confusion network (CN) (Mangu et al., 2000). We then build the CN using the TER metric (Snover et al., 2006), and finally search for the best translation. The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task. In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set. Section 4 concludes the paper. 2 The M AT R E X System 2.1 System Architect"
W10-1720,P03-1021,0,0.0273052,"output of each individual system. The CN is built by aligning other hypotheses against the backbone, based on the TER metric. Null words are allowed in the alignment. Either votes or different confidence measures are assigned to each word in the network. Each arc in the CN represents an alternative word at that position in the sentence and the number of votes for each word is counted when constructing the network. The features we used are as follows: • • • • word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length rat"
W10-1720,P02-1038,0,0.00732471,"xtracted by the baseline system adding word alignment information. 2.3 Apertium RBMT Apertium1 is a free/open-source platform for RBMT. The current version of the en–es system in Apertium was used for the system combination task (section 2.7), and its morphological analysers and part-of-speech taggers were used to build a factored Moses model. 2.4 Factored Translation Model We also used a factored model for the en–es translation task. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). We used three factors in our factored translation model, which are used in two different decoding paths: a surface form (SF) to SF translation factor, a lemma to lemma translation factor, and a part-ofspeech (PoS) to PoS translation factor. Finally, we used two decoding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI"
W10-1720,P02-1040,0,0.113346,"Missing"
W10-1720,2006.amta-papers.25,0,0.0454791,"and English–Czech translation tasks, in which we employed our multiengine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder. 1 Introduction multiple translation hypotheses and employ a new rescoring model to generate the final translation. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the confusion network (CN) (Mangu et al., 2000). We then build the CN using the TER metric (Snover et al., 2006), and finally search for the best translation. The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task. In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set. Section 4 concludes the paper. 2 The M AT R E X System 2.1 System Architecture In this paper, we present the DCU multi-engine The M AT R E X system is a combination-based MT system M AT R E X (Machine Translation using multi-engine architecture, which explo"
W10-1720,2007.tmi-papers.28,1,0.632786,"Missing"
W10-1720,2006.iwslt-evaluation.4,1,0.38824,"tion factors: an SF to SF decoding path and a path which maps lemma to lemma, PoS to PoS, and an SF generated using the TL lemma and PoS. The lemmas and PoS for en and es were obtained using Apertium (section 2.3). 2.5 Source-Side Context-informed PB-SMT One natural way to express a context-informed ˆ MBL ) is to view it as the conditional feature (h probability of the target phrases (ˆ ek ) given the ˆ source phrase (fk ) and its source-side context information (CI): ˆ MBL = log P (ˆ h ek |fˆk , CI(fˆk )) Figure 1: System Framework. tion probabilities and the amount of word-to-word cognates (Stroppa and Way, 2006). Once these phrase pairs were obtained they were merged with the phrase pairs extracted by the baseline system adding word alignment information. 2.3 Apertium RBMT Apertium1 is a free/open-source platform for RBMT. The current version of the en–es system in Apertium was used for the system combination task (section 2.7), and its morphological analysers and part-of-speech taggers were used to build a factored Moses model. 2.4 Factored Translation Model We also used a factored model for the en–es translation task. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it"
W10-1720,W06-3110,0,0.03498,"word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we used the Europarl(Koehn, 2005), News Commentary and United Natio"
W10-1720,W96-0213,0,0.0699016,"ng the network. The features we used are as follows: • • • • word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we us"
W10-1720,N07-1029,0,0.00800063,"d Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we use a word-level combination framework (Rosti et al., 2007) to combine the 143which edit costs depend on word-to-word translaProceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143–148, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the above three translation factors: an SF to SF decoding path and a path which maps lemma to lemma, PoS to PoS, and an SF generated using the TL lemma and PoS. The lemmas and PoS for en and es were obtained using Apertium (section 2.3). 2.5 Source-Side Context-informed PB-SMT One natural way to express a context-informed ˆ MBL ) is to view it"
W10-1720,D07-1091,0,\N,Missing
W10-1720,2003.mtsummit-systems.3,0,\N,Missing
W10-1720,W10-1742,1,\N,Missing
W10-1742,N04-1022,0,0.159363,"Missing"
W10-1742,E06-1005,0,0.0258044,". Section 3 details the steps for building our augmented three-pass combination framework. In Section 4, a rescoring model with rich features is described. Then, Sections 5 and 6 respectively report the experimental settings and experimental results on English-to-Czech and Frenchto-English combination tasks. Section 7 gives our conclusions. 2 Hypothesis Alignment Methods Hypothesis alignment plays a vital role in the CN, as the backbone sentence determines the skeleton and the word order of the consensus output. In the combination evaluation task, we integrated TER (Snover et al., 2006), HMM (Matusov et al., 2006) and TERp (Snover et al., 2009) into our augmented three-pass combination framework. In this section, we briefly describe these three methods. 2.1 TER The TER (Translation Edit Rate) metric measures the ratio of the number of edit operations between the hypothesis E 0 and the reference Eb to the total number of words in Eb . Here the backbone Eb is assumed to be the reference. The allowable edits include insertions (Ins), deletions (Del), substitutions (Sub), and phrase shifts (Shft). The TER of E 0 compared to Eb is computed as in (1): where Nb is the total number of words in Eb . The differe"
W10-1742,P03-1021,0,0.0351504,"hmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N best list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N -best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N -best list using B LEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 We participated in the English–Czech and French–English system combination tasks. In our system combination framework, we use a large-scale monolingual data to train language models and carry out POS-tagging. 5.1 English-Czech Training Data The statistics of the data used for language models training are shown in Table 1. Monolingual tokens (Cz) 2,214,757 81,161,278 205,600,053 288,976,088 Number of sentences 84,706 8,027,391 13,042,040 21,154,137 Table 1: Statistics of data in the En–Cz task All the data are provided by the workshop organisers. 1 In Table 1, “News-Comm” indicates the data"
W10-1742,P02-1040,0,0.0871932,"nd we used the default configuration of optimised edit costs. 3 Augmented Three-Pass Combination Framework The construction of the augmented three-pass combination framework is shown in Figure 1. N Single MT Systems BLEU TER Top M Single TERp BLEU TER TERp Alignment HMM TER TERp Pass 1 Individual CNs Nbest Re-ranking Super CN Networks Pass 1: Specific Metric-based Single Networks 1. Merge all the 1-best hypotheses from single MT systems into a new N -best set Ns . 2. Utilise the standard MBR decoder to select one from the Ns as the backbone given some specific loss function such as TER, BLEU (Papineni et al., 2002) and TERp; Additionally, in order to increase the diversity of candidates used for Pass 2 and Pass 3, we also use the 1-best hypotheses from the top M single MT systems as the backbone. Add the backbones generated by MBR into Ns . 3. Perform the word alignment between the different backbones and the other hypotheses via the TER, HMM, TERp (only for English) metrics. 4. Carry out word reordering based on word alignment (TER and TERp have completed the reordering in the process of scoring) and build individual CNs (Rosti et al., 2007); 5. Decode the single networks and export the 1best outputs a"
W10-1742,W96-0213,0,0.659158,"oding to search for the The lines with arrows pointing to “mConMBR” best final result from Ncon . In this step, we represent adding outputs into the mConMBR deset a uniform distribution between the candicoding component. “Top M Single” indicates that dates in Ncon . the 1-best results from the best M individual MT 292 4 Rescoring Model We adapted our previous rescoring model (Du et al., 2009) to larger-scale data. The features we used are as follows: • Direct and inverse IBM model; • 4-gram and 5-gram target language model; • 3, 4, and 5-gram Part-of-Speech (POS) language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N best list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N -best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N -best list using B LEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 We participated"
W10-1742,P07-1040,0,0.0900202,"et of hypotheses, and then component which is used to re-rank the the remaining hypotheses are aligned against the N -best lists generated from the individual backbone by a specific alignment approach. CurCNs and the super network, 2) a new hyrently, most research in system combination has pothesis alignment metric – TERp – that focused on hypothesis alignment due to its signifis used to carry out English-targeted hyicant influence on combination quality. pothesis alignment, and 3) more differA multiple CN or “super-network” framework ent backbone-based CNs which are emwas firstly proposed in Rosti et al. (2007) who ployed to increase the diversity of the used each of all individual system results as the mConMBR decoding phase. We took backbone to build CNs based on the same alignpart in the combination tasks of Englishment metric, TER (Snover et al., 2006). A consento-Czech and French-to-English. Expersus network MBR (ConMBR) approach was preimental results show that our proposed sented in (Sim et al., 2007), where MBR decodcombination framework achieved 2.17 abing is employed to select the best hypothesis with solute points (13.36 relative points) and the minimum cost from the original single syste"
W10-1742,2006.amta-papers.25,0,0.404187,", most research in system combination has pothesis alignment metric – TERp – that focused on hypothesis alignment due to its signifis used to carry out English-targeted hyicant influence on combination quality. pothesis alignment, and 3) more differA multiple CN or “super-network” framework ent backbone-based CNs which are emwas firstly proposed in Rosti et al. (2007) who ployed to increase the diversity of the used each of all individual system results as the mConMBR decoding phase. We took backbone to build CNs based on the same alignpart in the combination tasks of Englishment metric, TER (Snover et al., 2006). A consento-Czech and French-to-English. Expersus network MBR (ConMBR) approach was preimental results show that our proposed sented in (Sim et al., 2007), where MBR decodcombination framework achieved 2.17 abing is employed to select the best hypothesis with solute points (13.36 relative points) and the minimum cost from the original single system 1.52 absolute points (5.37 relative points) outputs compared to the consensus output. in terms of BLEU score on English-toDu and Way (2009) proposed a combination Czech and French-to-English tasks restrategy that employs MBR, super network, and spe"
W10-1742,W09-0441,0,0.01692,"ranslation and MetricsMATR, pages 290–295, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pass, all the 1-best hypotheses coming from single MT systems, individual networks, and the super network are combined to select the final result using the mConMBR decoder. In the system combination task of WMT 2010, we adopted an augmented framework by extending the strategy in (Du and Way, 2009). In addition to the basic three-pass architecture, we augment our combination system as follows: • We add a rescoring component in Pass 1 and Pass 2. • We introduce the TERp (Snover et al., 2009) alignment metric for the English-targeted combination. • We employ different backbones and hypothesis alignment metrics to increase the diversity of candidates for our mConMBR decoding. The remainder of this paper is organised as follows. In Section 2, we introduce the three hypothesis alignment methods used in our framework. Section 3 details the steps for building our augmented three-pass combination framework. In Section 4, a rescoring model with rich features is described. Then, Sections 5 and 6 respectively report the experimental settings and experimental results on English-to-Czech and"
W10-1742,W06-3110,0,0.0584497,"onMBR” best final result from Ncon . In this step, we represent adding outputs into the mConMBR deset a uniform distribution between the candicoding component. “Top M Single” indicates that dates in Ncon . the 1-best results from the best M individual MT 292 4 Rescoring Model We adapted our previous rescoring model (Du et al., 2009) to larger-scale data. The features we used are as follows: • Direct and inverse IBM model; • 4-gram and 5-gram target language model; • 3, 4, and 5-gram Part-of-Speech (POS) language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N best list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N -best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N -best list using B LEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 We participated in the English–Czech and French–English system combination tas"
W10-1742,W10-1720,1,\N,Missing
W10-1753,E06-1032,0,0.12591,"ncy-based metric’s correlation with human judgement. 1 Introduction String-based automatic evaluation metrics such as B LEU (Papineni et al., 2002) have led directly to quality improvements in machine translation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include M ETEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TER P 349 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorpor"
W10-1753,W05-0904,0,0.135213,"ation Localisation School of Computing Dublin City University Dublin 9, Ireland {yhe,jdu,away,josef}@computing.dcu.ie Abstract (Snover et al., 2010), both of which now utilize stemming, WordNet and paraphrase information. Experimental and evaluation campaign results have shown that these metrics can obtain better correlation with human judgements than metrics that only use surface-level information. Given that many of today’s MT systems incorporate some kind of syntactic information, it was perhaps natural to use syntax in automatic MT evaluation as well. This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. Owczarzak et al. (2007) extended this line of research with the use of a term-based encoding of Lexical Functional Grammar (LFG:(Kaplan and Bresnan, 1982)) labelled dependency graphs into unordered sets of dependency triples, and calculating precision, recall, and F-score on the triple sets corresponding to the translation and reference sentences. With the addition of partial matching and n-best parses, Owczarzak et al. (2007)’s method considerably outperforms Liu and Gildea’s (2005) w.r.t. corre"
W10-1753,nivre-etal-2006-maltparser,0,0.159655,"nslation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include M ETEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TER P 349 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorporate stemming, synonym and paraphrase information as in (Kahn et al., 2010), and at the same time introduce a chunk penalty in the spirit of M ETEOR to penalize discontinuous matches. We sort the matches according to the match level and the dependency type, and weight the matches to maximize correlation with human judgement. The remainder of the paper is organized as follows. Section 2 reviews the dependency-based metric. Sections 3, 4, 5 and 6 introduce our improvements on this me"
W10-1753,W07-0714,1,0.869034,"Missing"
W10-1753,P09-1034,0,0.275084,"abels derived from a Probabilistic Context-Free Grammar (PCFG) parse to replace the LFG labels, showing that a PCFG parser is sufficient for preprocessing, compared to a dependency parser in (Liu and Gildea, 2005) and (Owczarzak et al., 2007). E DPM also incorporates more information sources: e.g. the parser confidence, the Porter stemmer, WordNet synonyms and paraphrases. Besides the metrics that rely solely on the dependency structures, information from the dependency parser is a component of some other metrics that use more diverse resources, such as the textual entailment-based metric of (Pado et al., 2009). In this paper we extend the work of (Owczarzak et al., 2007) in a different manner: we use an We describe DCU’s LFG dependencybased metric submitted to the shared evaluation task of WMT-MetricsMATR 2010. The metric is built on the LFG F-structurebased approach presented in (Owczarzak et al., 2007). We explore the following improvements on the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penal"
W10-1753,P02-1040,0,0.0808056,"the original metric: 1) we replace the in-house LFG parser with an open source dependency parser that directly parses strings into LFG dependencies; 2) we add a stemming module and unigram paraphrases to strengthen the aligner; 3) we introduce a chunk penalty following the practice of M ETEOR to reward continuous matches; and 4) we introduce and tune parameters to maximize the correlation with human judgement. Experiments show that these enhancements improve the dependency-based metric’s correlation with human judgement. 1 Introduction String-based automatic evaluation metrics such as B LEU (Papineni et al., 2002) have led directly to quality improvements in machine translation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and lingui"
W10-1753,W05-0909,0,0.365379,"anslation (MT). These metrics provide an alternative to expensive human evaluations, and enable tuning of MT systems based on automatic evaluation results. However, there is widespread recognition in the MT community that string-based metrics are not discriminative enough to reflect the translation quality of today’s MT systems, many of which have gone beyond pure string-based approaches (cf. (Callison-Burch et al., 2006)). With that in mind, a number of researchers have come up with metrics which incorporate more sophisticated and linguistically motivated resources. Examples include M ETEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) and TER P 349 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349–353, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics adapted version of the Malt parser (Nivre et al., 2006) to produce 1-best LFG dependencies and allow triple matches where the dependency labels are different. We incorporate stemming, synonym and paraphrase information as in (Kahn et al., 2010), and at the same time introduce a chunk penalty in the spirit of M ETEOR to penalize discontinuous matches. We sort the matc"
W10-1753,P04-1041,1,0.877558,"Missing"
W10-3707,C04-1114,0,0.441408,"ither. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration method. We rely on these automatically aligned NEs and treat them as translation examples. Adding bilingual dictionaries, which in effect are instances of atomic translation pairs, to the parallel corpus is a well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008). We modify the parallel corpus by converting the MWEs into single tokens and adding the aligned NEs in the parallel corpus in a bid to improve the word alignment, and hence the phrase alignment quality. This 46 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46–54, Beijing, August 2010 preprocessing results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. In section 2 we discuss related work. The System is described in Section 3. Section 4 includes the results obtai"
W10-3707,W09-3539,1,0.741895,"Missing"
W10-3707,W05-0909,0,0.209211,"Missing"
W10-3707,W04-3248,0,0.280025,"Missing"
W10-3707,J93-2003,0,0.0150925,"ed MWEs can bring about any further improvement on top of that. We carried out our experiments on an English—Bangla translation task, a relatively hard task with Bangla being a morphologically richer language. 3 System Description 3.1 PB-SMT Translation is modeled in SMT as a decision process, in which the translation e1I = e1 . . . ei . . . eI of a source sentence f1J = f1 . . . fj . . . fJ is chosen to maximize (1): (1) arg max P(e1I |f1J ) = arg max P( f1J |e1I ).P(e1I ) I ,e1I I ,e1I where P ( f1J |e1I ) and P (e1I ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modeled as a log-linear combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P(e1I |f 1J ) = ∑ λ m hm ( f 1J , e1I , s1K ) m =1 + λLM log P(e1I ) (2) where s = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ1 ,..., eˆk ) and ( fˆ1 ,..., fˆk ) such that (we set i0 = 0) (3): k 1 ∀1 ≤ k ≤ K , sk = (ik, bk, jk), eˆk = eik −1 +1...eik , fˆ = f ... f . k bk jk (3) and eac"
W10-3707,W03-1502,0,0.128214,"r simultaneous NE identification and translation. He uses capitalization cues for identifying NEs on the English side, and then he applies statistical techniques to decide which portion of the target language corresponds to the specified English NE. Feng et al. (2004) proposed a Maximum Entropy model based approach for English— Chinese NE alignment which significantly outperforms IBM Model4 and HMM. They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical NEs in the same sentence. Huang et al. (2003) proposed a method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilin"
W10-3707,N10-1029,0,0.108076,"reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT, it could well be the case that constituents of an 47 MWE are marked and aligned as parts of consecutive phrases, since PB-SMT (or any other approaches to SMT) does not generally treat MWEs as special tokens. Another problem SMT suffers from is that verb phrases are often wrongly translated, or even sometimes deleted in the output in or"
W10-3707,C08-2007,0,0.104429,"Missing"
W10-3707,P07-2045,0,0.0291762,"gual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT, it could well be the case that constituents of an 47 MWE are marked and aligned as parts of consecutive phrases, since"
W10-3707,2006.amta-papers.25,0,0.0728121,"Missing"
W10-3707,C96-2141,0,0.388305,"(or spaces)” (Sag et al., 2002). Traditional approaches to word alignment following IBM Models (Brown et al., 1993) do not work well with multi-word expressions, especially with NEs, due to their inability to handle manyto-many alignments. Firstly, they only carry out alignment between words and do not consider the case of complex expressions, such as multiword NEs. Secondly, the IBM Models only allow at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al., 2003). In another well-known word alignment approach, Hidden Markov Model (HMM: Vogel et al., 1996), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration method. We rely on these automatically aligned NEs and treat them as translation examples. Adding bilingual dictionar"
W10-3707,W04-3250,0,0.31373,"Missing"
W10-3707,W06-1204,0,0.435823,"Missing"
W10-3707,P01-1050,0,0.00961498,"pus. Multi-word expressions (MWE) are defined as “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2002). Traditional approaches to word alignment following IBM Models (Brown et al., 1993) do not work well with multi-word expressions, especially with NEs, due to their inability to handle manyto-many alignments. Firstly, they only carry out alignment between words and do not consider the case of complex expressions, such as multiword NEs. Secondly, the IBM Models only allow at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al., 2003). In another well-known word alignment approach, Hidden Markov Model (HMM: Vogel et al., 1996), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration me"
W10-3707,C08-1125,0,0.154066,"Missing"
W10-3707,E03-1035,0,0.114084,"igned NEs in the parallel corpus in a bid to improve the word alignment, and hence the phrase alignment quality. This 46 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46–54, Beijing, August 2010 preprocessing results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. In section 2 we discuss related work. The System is described in Section 3. Section 4 includes the results obtained, together with some analysis. Section 5 concludes, and provides avenues for further work. 2 Related Work Moore (2003) presented an approach for simultaneous NE identification and translation. He uses capitalization cues for identifying NEs on the English side, and then he applies statistical techniques to decide which portion of the target language corresponds to the specified English NE. Feng et al. (2004) proposed a Maximum Entropy model based approach for English— Chinese NE alignment which significantly outperforms IBM Model4 and HMM. They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical N"
W10-3707,P03-1021,0,0.0160755,"guages to Indian Languages Machine Translation (ILILMT) System”. NEs in Bangla are identified using the NER system of Ekbal and Bandyopadhyay (2008). We use the Stanford Parser, Stanford NER and the NER for Bangla along with the default model files provided, i.e., with no additional training. The effectiveness of the MWE-aligned parallel corpus developed in the work is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model with Kneser-Ney smoothing (Kneser and 1 The EILMT and ILILMT projects are funded by the Department of Information Technology (DIT), Ministry of Communications and Information Technology (MCIT), Government of India. 2 http://nlp.stanford.edu/software/lex-parser.shtml 3 4 Ney, 1995) trained with SRILM (Stolcke, 2002), and Moses decoder (Koehn et al., 2007). http://crfchunker.sourceforge.net/ http://nlp.stanford.edu/software/CRF-NER.shtml 50 Experiments and Results We randomly extracted 500 sentences each for the development set and testset fr"
W10-3707,P02-1040,0,0.0819899,"Missing"
W10-3707,W09-2907,0,0.376728,"re, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical NEs in the same sentence. Huang et al. (2003) proposed a method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWE"
W10-3707,N03-1017,0,\N,Missing
W10-3803,P06-1067,0,0.0575735,"Missing"
W10-3803,J82-2005,0,0.757263,"Missing"
W10-3803,W09-2307,0,0.209074,"d testset before being fed into the SMT systems, so that only the reordered source sentences are dealt with while building during the SMT system. In this case, most work is focused on methods to extract and to apply syntactic reordering patterns which come from manually created rules (Collins et al., 2005; Wang et al., 2007a), or via an automatic extraction process taking advantage of parse trees (Collins et al., 2005; Habash, 2007). Because reordered source sentence cannot be undone by the SMT decoders (AlOnaizan et al., 2006), which implies a systematic error for this approach, classifiers (Chang et al., 2009b; Du & Way, 2010) are utilized to obtain high-performance reordering for some specialized syntactic structures (e.g. DE construction in Chinese). On the other hand, the non-deterministic approach leaves the decisions to the decoders to choose appropriate source-side reorderings. This is more flexible because both the original and reordered source sentences are presented in the inputs. Word lattices generated from syntactic structures for N-gram-based SMT is presented in (Crego et al., 2007). In (Zhang et al., 2007a; Zhang et al., 2007b), chunks and POS tags are used to extract reordering rule"
W10-3803,2010.eamt-1.32,1,0.694098,"g fed into the SMT systems, so that only the reordered source sentences are dealt with while building during the SMT system. In this case, most work is focused on methods to extract and to apply syntactic reordering patterns which come from manually created rules (Collins et al., 2005; Wang et al., 2007a), or via an automatic extraction process taking advantage of parse trees (Collins et al., 2005; Habash, 2007). Because reordered source sentence cannot be undone by the SMT decoders (AlOnaizan et al., 2006), which implies a systematic error for this approach, classifiers (Chang et al., 2009b; Du & Way, 2010) are utilized to obtain high-performance reordering for some specialized syntactic structures (e.g. DE construction in Chinese). On the other hand, the non-deterministic approach leaves the decisions to the decoders to choose appropriate source-side reorderings. This is more flexible because both the original and reordered source sentences are presented in the inputs. Word lattices generated from syntactic structures for N-gram-based SMT is presented in (Crego et al., 2007). In (Zhang et al., 2007a; Zhang et al., 2007b), chunks and POS tags are used to extract reordering rules, while the gener"
W10-3803,P06-1055,0,0.0268839,"Missing"
W10-3803,W08-0406,0,0.0264985,"Missing"
W10-3803,2009.eamt-1.20,1,0.900503,"Missing"
W10-3803,W09-0809,0,0.0243834,"Missing"
W10-3803,D07-1077,0,0.134529,"inally, we give our conclusion and avenues for future work in section 6. ing, 2009) uses syntactic rules to score the output word order, both on English–Danish and English– Arabic tasks. Syntactic reordering information is also considered as an extra feature to improve PBSMT in (Chang et al., 2009b) for the Chinese– English task. These results confirmed the effectiveness of syntactic reorderings. However, for the particular case of Chinese source inputs, although the DE construction has been addressed for both PBSMT and HPBSMT systems in (Chang et al., 2009b; Du & Way, 2010), as indicated by (Wang et al., 2007a), there are still lots of unexamined structures that imply source-side reordering, especially in the nondeterministic approach. As specified in (Xue, 2005), these include the bei-construction, baconstruction, three kinds of de-construction (including DE construction) and general preposition constructions. Such structures are referred with functional words in this paper, and all the constructions can be identified by their corresponding tags in the Penn Chinese TreeBank. It is interesting to investigate these functional words for the syntactic reordering task since most of them tend to produc"
W10-3803,2007.mtsummit-papers.29,0,0.0612271,"Missing"
W10-3803,2010.eamt-1.26,1,0.902679,"c parser are also utilized to form weighted n-best lists which are fed into the decoder (Li et al., 2007). Furthermore, (Elming, 2008; Elm1 Introduction Previous work has shown that the problem of structural differences between language pairs in SMT can be alleviated by source-side syntactic reordering. Taking account for the integration with SMT systems, these methods can be divided into two different kinds of approaches (Elming, 19 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19–27, COLING 2010, Beijing, August 2010. tice scoring approach (Jiang et al., 2010) is utilized to discover non-monotonic phrase alignments, and then syntactic reordering patterns are extracted from source-side parse trees. After that, functional word tags specified in (Xue, 2005) are adopted to perform pattern filtering. Finally, both the unfiltered pattern set and the filtered one are used to transform inputs into word lattices to present potential reorderings for improving PBSMT system. A comparison between the three systems is carried out to examine the performance of syntactic reordering as well as the usefulness of functional words for pattern filtering. The rest of th"
W10-3803,C04-1073,0,0.071259,"Missing"
W10-3803,2002.tmi-tutorials.2,0,0.0785608,"Missing"
W10-3803,P07-1091,0,0.0375729,"Missing"
W10-3803,W07-0401,0,0.0417763,"Missing"
W10-3803,ma-2006-champollion,0,0.021906,"on 5 Both the devset and testset are transformed into word lattices by the extracted patterns to incorporate potential reorderings. Figure 2 illustrates this process: treelet T ′ is matched with a pattern, then its leaf nodes {a1 , · · · am } ∈ LA (spanning {w1 , · · · , wp }) are swapped with leaf nodes {b1 , · · · , bn } ∈ LB (spanning {v1 , · · · , vq }) on the generated paths in the word lattice. We conducted our experiments on a medium-sized corpus FBIS (a multilingual paragraph-aligned corpus with LDC resource number LDC2003E14) for the Chinese–English SMT task. The Champollion aligner (Ma, 2006) is utilized to perform sentence alignment. A total number of 256,911 sentence pairs are obtained, while 2,000 pairs for devset and 2,000 pairs for testset are randomly selected, which we call FBIS set. The rest of the data is used as the training corpus. The baseline system is Moses (Koehn et al., 2007), and GIZA++1 is used to perform word alignment. Minimum error rate training (MERT) (Och, 2003) is carried out for tuning. A 5-gram language model built via SRILM2 is used for all the experiments in this paper. Experiments results are reported on two different sets: the FBIS set and the NIST se"
W10-3803,2007.iwslt-1.3,0,0.0411723,"Missing"
W10-3803,P02-1040,0,\N,Missing
W10-3803,P07-2045,0,\N,Missing
W10-3803,P05-1066,0,\N,Missing
W10-3803,W09-0436,0,\N,Missing
W10-3803,P03-1021,0,\N,Missing
W10-3813,P09-1088,0,0.0139562,"far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English. The weakness of most generative models often lies in the incapability of addressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) alignments. Some research directly addresses m-to-n alignment with phrase alignment models (Marcu and Wong, 2002). However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). The most widely used word alignment models, such as IBM models 3 and 4, can only model 1-to-n alignment; these models are often called “asymmetric” models. IBM models 3 and 4 model 1-to-n alignments using the notion of “fertility”, which is associated with a “deficiency” problem despite its high performance in practice. On the other hand, the HMM word-to-phrase a"
W10-3813,J93-2003,0,0.0250734,"he Backward procedure efficiently. Posterior probability can be calculated based on the Forward and Backward probabilities. 3.4 EM Parameter Updates The Expectation step accumulates fractional counts using the posterior probabilities for each parameter during the Forward-Backward passes, and the Maximisation step normalises the counts in order to generate updated parameters. The E-step for the syntactic coherence model proceeds as follows: X X γj (i, φ, ε = 1) c(r ′ ; f, φ′ ) = (f ,e)∈T i,j,φ,fi =f 4.2 δ(φ, φ′ )δ(ϕj (e, φ), r ′ ) The G IZA ++ (Och and Ney, 2003) implementation of IBM Model 4 (Brown et al., 1993) is used as the baseline for word alignment. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. We compared our model against the MTTK (Deng and Byrne, 2006) implementation of the HMM word-to-phrase alignment model. The model training includes 10 iterations of Model 1, 5 iterations of Model 2, 5 iterations of HMM wordto-word alignment, 20 iterations (5 iterations respectively for phrase lengths 2, 3 and 4 with unigram translation probability, and phrase length 4 with bigram translation probability)"
W10-3813,P06-2014,0,0.0191912,". Therefore, this model sets a good example of addressing the tradeoffs between modelling power and modelling complexity. This model can also be seen as a more generalised 101 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101–109, COLING 2010, Beijing, August 2010. case of the HMM word-to-word model (Vogel et al., 1996; Och and Ney, 2003), since this model can be reduced to an HMM word-to-word model by restricting the generated target phrase length to one. One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). However, most research focuses on the incorporation of syntactic constraints into discriminative alignment models. Introducing syntactic information into generative alignment models is shown to be more challenging mainly due to the absence of appropriate modelling of syntactic constraints and the “inflexibility” of these generative models. In this paper, we extend the HMM word-tophrase alignment model with syntactic dependencies by presenting a model that can incorporate syntactic information while maintaining the efficiency of the model. This model is based on the observation that in 1-to-"
W10-3813,P08-2007,0,0.0184203,"le translation tasks. However, the quality of the alignment yielded from these models is still far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English. The weakness of most generative models often lies in the incapability of addressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) alignments. Some research directly addresses m-to-n alignment with phrase alignment models (Marcu and Wong, 2002). However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). The most widely used word alignment models, such as IBM models 3 and 4, can only model 1-to-n alignment; these models are often called “asymmetric” models. IBM models 3 and 4 model 1-to-n alignments using the notion of “fertility”, which is associated with a “deficiency”"
W10-3813,N06-4004,0,0.0793721,"taining the efficiency of the model. This model is based on the observation that in 1-to-n alignments, the n words bear some syntactic dependencies. Leveraging such information in the model can potentially further aid the model in producing more fine-grained word alignments. The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. As a syntactic extension of the open-source MTTK implementation (Deng and Byrne, 2006) of the HMM word-to-phrase alignment model, its source code will also be released as open source in the near future. The remainder of the paper is organised as follows. Section 2 describes the HMM word-tophrase alignment model. In section 3, we present the details of the incorporation of syntactic dependencies. Section 4 presents the experimental setup, and section 5 reports the experimental results. In section 6, we draw our conclusions and point out some avenues for future work. phrases: e = v1K , where vk represents the kth phrase in the target sentence. The assumption that each phrase vk g"
W10-3813,P07-2045,0,0.0129381,"used for Minimum Error-Rate Training (MERT) (Och, 2003), and MTC2, 3 and 4 were used as development 105 4.3 Word Alignment MT system The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the method described in section 4.2, we perform phrase-extraction using heuristics described in (Koehn et al., 2003), Minimum Error-Rate Training (MERT) (Och, 2003) optimising the B LEU metric, a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and M OSES (Koehn et al., 2007) for decoding. A Hiero-style decoder Joshua (Li et al., 2009) is also used in our experiments. All significance tests are performed using approximate randomisation (Noreen, 1989) at p = 0.05. 5 Experimental Results 5.1 Alignment Model Tuning In order to find the value of ζ in the SSH model that yields the best MT performance, we used three development test sets using a PB-SMT system trained on the small data condition. Figure 1 shows the results on each development test set using different configurations of the alignment models. For each system, we obtain the mean of the BLEU scores (Papineni"
W10-3813,W09-0424,0,0.0138783,"2, 3 and 4 were used as development 105 4.3 Word Alignment MT system The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the method described in section 4.2, we perform phrase-extraction using heuristics described in (Koehn et al., 2003), Minimum Error-Rate Training (MERT) (Och, 2003) optimising the B LEU metric, a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and M OSES (Koehn et al., 2007) for decoding. A Hiero-style decoder Joshua (Li et al., 2009) is also used in our experiments. All significance tests are performed using approximate randomisation (Noreen, 1989) at p = 0.05. 5 Experimental Results 5.1 Alignment Model Tuning In order to find the value of ζ in the SSH model that yields the best MT performance, we used three development test sets using a PB-SMT system trained on the small data condition. Figure 1 shows the results on each development test set using different configurations of the alignment models. For each system, we obtain the mean of the BLEU scores (Papineni et al., 2002) on the three development test sets, and derive"
W10-3813,W02-1018,0,0.0241074,"is widespread use can be attributed to their robustness and high performance particularly on largescale translation tasks. However, the quality of the alignment yielded from these models is still far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English. The weakness of most generative models often lies in the incapability of addressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) alignments. Some research directly addresses m-to-n alignment with phrase alignment models (Marcu and Wong, 2002). However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). The most widely used word alignment models, such as IBM models 3 and 4, can only model 1-to-n alignment; these models are often called “asymmetric” models. IBM models 3"
W10-3813,J03-1002,0,0.0206545,"m despite its high performance in practice. On the other hand, the HMM word-to-phrase alignment model tackles 1-to-n alignment problems with simultaneous segmentation and alignment while maintaining the efficiency of the models. Therefore, this model sets a good example of addressing the tradeoffs between modelling power and modelling complexity. This model can also be seen as a more generalised 101 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101–109, COLING 2010, Beijing, August 2010. case of the HMM word-to-word model (Vogel et al., 1996; Och and Ney, 2003), since this model can be reduced to an HMM word-to-word model by restricting the generated target phrase length to one. One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). However, most research focuses on the incorporation of syntactic constraints into discriminative alignment models. Introducing syntactic information into generative alignment models is shown to be more challenging mainly due to the absence of appropriate modelling of syntactic constraints and the “inflexibility” of these generative models. In this paper, we extend"
W10-3813,P03-1021,0,0.00745032,"of GALE data (LDC2006E26) consisting of 103K (2.9 million English running words) sentence pairs was firstly used as a proof of concept (“small”), and FBIS data containing 238K sentence pairs (8 million English running words) was added to construct a “medium” scale experiment. To investigate the intrinsic quality of the alignment, a collection of parallel sentences (12K sentence pairs) for which we have manually annotated word alignment was added to both “small” and “medium” scale experiments. Multiple-Translation Chinese Part 1 (MTC1) from LDC was used for Minimum Error-Rate Training (MERT) (Och, 2003), and MTC2, 3 and 4 were used as development 105 4.3 Word Alignment MT system The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the method described in section 4.2, we perform phrase-extraction using heuristics described in (Koehn et al., 2003), Minimum Error-Rate Training (MERT) (Och, 2003) optimising the B LEU metric, a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and M OSES (Koehn et al., 2007) for decoding. A Hiero-style decoder"
W10-3813,W02-1039,0,0.029975,"ve models. In this paper, we extend the HMM word-tophrase alignment model with syntactic dependencies by presenting a model that can incorporate syntactic information while maintaining the efficiency of the model. This model is based on the observation that in 1-to-n alignments, the n words bear some syntactic dependencies. Leveraging such information in the model can potentially further aid the model in producing more fine-grained word alignments. The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. As a syntactic extension of the open-source MTTK implementation (Deng and Byrne, 2006) of the HMM word-to-phrase alignment model, its source code will also be released as open source in the near future. The remainder of the paper is organised as follows. Section 2 describes the HMM word-tophrase alignment model. In section 3, we present the details of the incorporation of syntactic dependencies. Section 4 presents the experimental setup, and section 5 reports the experimental results. In secti"
W10-3813,D07-1006,0,0.0149147,"mpared to IBM model 4 using a PB-SMT system, and a comparable performance compared to IBM model 4 using a Hiero system. 5.3 the SH model. Both SH and SSH lead to gains over H for both ZH–EN and EN–ZH directions, while gains in the EN–ZH direction appear to be more pronounced. IBM model 4 achieves significantly higher P over other models while the gap in R is narrow. Relating Table 2 to Table 1, we observe that the HMM word-to-word alignment model (H) can still achieve good MT performance despite the lower P and R compared to other models. This provides additional support to previous findings (Fraser and Marcu, 2007b) that the intrinsic quality of word alignment does not necessarily correlate with the performance of the resulted MT system. 5.4 In order to further understand the characteristics of the alignment that each model produces, we investigated several statistics of the alignment results which can hopefully reveal the capabilities and limitations of each model. 5.4.1 Intrinsic Evaluation In order to further investigate the intrinsic quality of the word alignment, we compute the Precision (P), Recall (R) and F-score (F) of the alignments obtained using different alignment models. As the models inve"
W10-3813,P02-1040,0,0.0788725,"Missing"
W10-3813,J07-3002,0,0.0142498,"mpared to IBM model 4 using a PB-SMT system, and a comparable performance compared to IBM model 4 using a Hiero system. 5.3 the SH model. Both SH and SSH lead to gains over H for both ZH–EN and EN–ZH directions, while gains in the EN–ZH direction appear to be more pronounced. IBM model 4 achieves significantly higher P over other models while the gap in R is narrow. Relating Table 2 to Table 1, we observe that the HMM word-to-word alignment model (H) can still achieve good MT performance despite the lower P and R compared to other models. This provides additional support to previous findings (Fraser and Marcu, 2007b) that the intrinsic quality of word alignment does not necessarily correlate with the performance of the resulted MT system. 5.4 In order to further understand the characteristics of the alignment that each model produces, we investigated several statistics of the alignment results which can hopefully reveal the capabilities and limitations of each model. 5.4.1 Intrinsic Evaluation In order to further investigate the intrinsic quality of the word alignment, we compute the Precision (P), Recall (R) and F-score (F) of the alignments obtained using different alignment models. As the models inve"
W10-3813,N03-1017,0,0.0256969,"Missing"
W10-3813,W96-0213,0,0.429198,"Missing"
W10-3813,C96-2141,0,0.141764,"“deficiency” problem despite its high performance in practice. On the other hand, the HMM word-to-phrase alignment model tackles 1-to-n alignment problems with simultaneous segmentation and alignment while maintaining the efficiency of the models. Therefore, this model sets a good example of addressing the tradeoffs between modelling power and modelling complexity. This model can also be seen as a more generalised 101 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101–109, COLING 2010, Beijing, August 2010. case of the HMM word-to-word model (Vogel et al., 1996; Och and Ney, 2003), since this model can be reduced to an HMM word-to-word model by restricting the generated target phrase length to one. One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). However, most research focuses on the incorporation of syntactic constraints into discriminative alignment models. Introducing syntactic information into generative alignment models is shown to be more challenging mainly due to the absence of appropriate modelling of syntactic constraints and the “inflexibility” of these generative models. In th"
W10-4006,P04-1023,0,0.0433803,"sed combined with IBM Models 3 and 4. Although theoretically it is possible to incorporate partial annotation with a small change in its code, Graca et al. do not mention it. Secondly, Talbot (2005) introduces a constrained EM method which constrains the E-step to incorporate partial alignment into word alignment,1 which is in a similar manner to Graca et al. (2007). He conducted experiments using partial alignment annotation based on cognate relations, a bilingual dictionary, domain-specific bilingual semantic annotation, and numerical pattern matching. He did not incorporate BMWEs. Thirdly, Callison-Burch et al. (2004) replace the likelihood maximization in the M-step with mixed likelihood maximization, which is a convex combination of negative log likelihood of known links and unknown links. The remainder of this paper is organized as follows: in Section 2 we define the anchor word alignment problem. In Section 3 we include a review of the EM algorithm with IBM Models 1-5, and the HMM Model. Section 4 describes our own algorithm based on the combination of BMWE extraction and the modified word alignment which incorporates the groupings of BMWEs and enforces their alignment links; we explain the EM algorith"
W10-4006,P91-1023,0,0.194889,"on semantic space that incorporate a notion of order in the bag-of-words model (e.g. co-occurences). Some aligned corpora include implicit partial alignment annotation, while for other corpora a partial alignment can be extracted by state-ofthe-art techniques. For example, implicit tags such as reference number within the patent corpus of Fujii et al. (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al. (1993), extract terminology pairs using linguistically predefined POS patterns. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Resnik and Melamed (1997) automatically extract domainspecific lexica. Moore (2003) extracts namedentities. In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporat"
W10-4006,N03-1017,0,0.00316695,"Word Alignment Tsuyoshi Okita1 , Alfredo Maldonado Guerra2 , Yvette Graham3 , Andy Way1 {CNGL1 , NCLT3 } / School of Computing / Dublin City University, CNGL / School of Computer Science and Statistics / Trinity College Dublin2 {tokita,ygraham,away}@computing.dcu.ie, maldonaa@scss.tcd.ie Abstract 2007) remains key to providing high-quality translations as all subsequent training stages rely on its performance. It alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diagfinal (Koehn et al., 2003), to resolve them. This paper presents a new word alignment method which incorporates knowledge about Bilingual Multi-Word Expressions (BMWEs). Our method of word alignment first extracts such BMWEs in a bidirectional way for a given corpus and then starts conventional word alignment, considering the properties of BMWEs in their grouping as well as their alignment links. We give partial annotation of alignment links as prior knowledge to the word alignment process; by replacing the maximum likelihood estimate in the M-step of the IBM Models with the Maximum A Posteriori (MAP) estimate, prior k"
W10-4006,2005.mtsummit-papers.11,0,0.00743896,"We then perform MERT while a 5-gram language model is trained with SRILM. Our implementation is based on a modified version of GIZA++ (Och and Ney, 2003a). This modification is on the function that reads a bilingual terminology file, the function that calculates priors, the M-step in IBM Models 1-5, and the forward-backward algorithm in the HMM Model. Other related software tools are written in Python and Perl: terminology concatenation, terminology numbering, and so forth. 6 Experimental Results We conduct an experimental evaluation on the NTCIR-8 corpus (Fujii et al., 2010) and on Europarl (Koehn, 2005). Firstly, MWEs are extracted from both corpora, as shown in Table 3. In the second step, we apply our modified version of GIZA++ in which we incorporate the results of 6 This is because it needs to maintain potentially an ℓ × m matrix, where ℓ denotes the number of English tokens in the corpus and m denotes the number of foreign tokens, even if the matrix is sparse. Prior Model I only requires an ℓˆ × m ˆ matrix where ℓˆ is the number of English tokens in a sentence and m ˆ is the number of foreign tokens in a sentence, which is only needed until this information is incorporated in a posterio"
W10-4006,P07-2045,0,0.00825313,"Missing"
W10-4006,J10-4005,0,0.00742961,"apping objects. Table 1 shows two example phrase pairs for French to English c’est la vie and that is life, and la vie en rose and rosy life with the initial value for the EM algorithm, the prior value and the fi1 Although the code may be similar in practice to our Prior Model I, his explanation to modify the E-step will not be applied to IBM Models 3 and 4. Our view is to modify the M-step due to the same reason above, i.e. GIZA++ searches only over the alignment space around the Viterbi alignment. 27 bilities t(ei |fj ). It is noted that we use e|f rather than f |e following the notation of Koehn (2010). One important remark is that the Viterbi alignment of the sentence pair (˘ e, f˘) = (eJ1 , f1I ), which is obtained as in (1): Statistical MWE extraction method 97 groupe socialiste socialist group 26 26 101 monsieur poettering mr poettering 1 4 103 monsieur poettering mr poettering 1 11 110 monsieur poettering mr poettering 1 9 117 explication de vote explanation of vote 28 26 Eviterbi : aˆJ1 = arg max pθˆ(f, a|e) (1) Heuristic-based MWE extraction method aJ 1 28 the wheel 2 車輪 ２ 25 5 28 the primary-side fixed armature 13 １ 次 側 固 定 電機 子 １ ３ 13 9 28 the secondary-side rotary magnet 7 ２ 次 側 回"
W10-4006,P93-1003,0,0.929921,"ld ) + log p(t) t 3.2 HMM A first-order Hidden Markov Model (Vogel et al., 1996) uses the sentence length probability p(J|I), the mixture alignment probability p(i|j, I), and the translation probability, as in (4): p(f |e) = p(J|I) J Y j=1 p(fj |ei ) 4.1 (4) PI r(i − j JI ) ′ i′ =1 r(i − j JI ) (5) The HMM alignment probabilities p(i|i′ , I) depend only on the jump width (i − i′ ). Using a set of non-negative parameters s(i − i′ ), we have (6): p(i|i′ , I) = s(i − i′ ) PI l=1 s(l − i′ ) MWE Extraction Our algorithm of extracting MWEs is a statistical method which is a bidirectional version of Kupiec (1993). Firstly, Kupiec presents a method to extract bilingual MWE pairs in a unidirectional manner based on the knowledge about typical POS patterns of noun phrases, which is languagedependent but can be written down with some ease by a linguistic expert. For example in French they are N N, N prep N, and N Adj. Secondly, we take the intersection (or union) of extracted bilingual MWE pairs.2 Suppose we have a training set of R observation sequences Xr , where r = 1, · · · , R, each of which is labelled according to its class m, where m = 1, · · · , M , as in (5): p(i|j, I) = Our Approach 2 In word a"
W10-4006,W06-2402,0,0.714233,"it tags such as reference number within the patent corpus of Fujii et al. (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al. (1993), extract terminology pairs using linguistically predefined POS patterns. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Resnik and Melamed (1997) automatically extract domainspecific lexica. Moore (2003) extracts namedentities. In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate or the Maximum Penalized Likelihood (MPL) estimate (McLach1 Introduction Word alignment (Brown et al., 1993; Vogel et al., 1996; Och"
W10-4006,E03-1035,0,0.0189154,"extracted by state-ofthe-art techniques. For example, implicit tags such as reference number within the patent corpus of Fujii et al. (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al. (1993), extract terminology pairs using linguistically predefined POS patterns. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Resnik and Melamed (1997) automatically extract domainspecific lexica. Moore (2003) extracts namedentities. In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate or the Maximum Penalized Likelihood (MPL) estimate (McLach1"
W10-4006,W04-3243,0,0.0276884,"Missing"
W10-4006,J03-1002,0,0.0111181,"006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate or the Maximum Penalized Likelihood (MPL) estimate (McLach1 Introduction Word alignment (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003a; Graca et al., 26 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 26–34, Beijing, August 2010 pair EN-FR is NULL rosy en that . life la . c‘ that c‘ is est life vie rosy rose lan and Krishnan, 1997; Bishop, 2006). Then, the MAP estimate allows us to incorporate the prior, a probability used to reflect the degree of prior belief about the occurrences of the events. A small number of studies have been carried out that use partial alignment annotation for word alignment. Firstly, Graca et al. (2007) introduce a posterior regularization to"
W10-4006,A97-1050,0,0.0505851,"lignment annotation, while for other corpora a partial alignment can be extracted by state-ofthe-art techniques. For example, implicit tags such as reference number within the patent corpus of Fujii et al. (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al. (1993), extract terminology pairs using linguistically predefined POS patterns. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Resnik and Melamed (1997) automatically extract domainspecific lexica. Moore (2003) extracts namedentities. In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate o"
W10-4006,P03-1010,0,0.0224125,"cuments corpora,3 we can use heuristics to extract the “noun phrase” + “reference number” from both sides. This is due to the fact that terminology is often labelled with a unique reference number, which is labelled on both the SL and TL sides. 4.2 Prior Model I Prior for Exhaustive Alignment Space IBM Models 1 and 2 implement a prior for all possible 3 Unlike other language pairs, the availability of Japanese–English parallel corpora is quite limited: the NTCIR patent corpus (Fujii et al., 2010) of 3 million sentence pairs (the latest NTCIR-8 version) for the patent domain and JENAAD corpus (Utiyama and Isahara, 2003) of 150k sentence pairs for the news domain. In this regard, the patent domain is particularly important for this particular language pair. 30 Algorithm 3 Prior Model I for IBM Model 1 Given: parallel corpus e˘, f˘, anchor words biT erm initialize t(e|f ) uniformly do until convergence set count(e|f ) to 0 for all e,f set total(f) to 0 for all f for all sentence pairs (˘ es ,f˘s ) prior(e|f )s = getPriorModelI(˘ e, f˘, biT erm) for all words e in e˘s totals (e) = 0 for all words f in f˘s totals (e) += t(e|f ) for all words e in e˘s for all words f in f˘s count(e|f )+=t(e|f )/totals (e)× prior("
W10-4006,J93-1004,0,\N,Missing
W10-4006,J93-2003,0,\N,Missing
W10-4006,C96-2141,0,\N,Missing
W11-1004,P05-1074,0,0.0408865,"application on paraphrases in SMT. Section 4 presents the experiments and results of the proposed method as well as discussions. Conclusions and future work are then given in Section 5. 2 Word-lattice-based method Compared with translation model augmentation with paraphrases (Callison-Burch et al., 2006), word-lattice-based paraphrasing for PBSMT is introduced in (Du et al., 2010). A brief overview of this method is given in this section. 2.1 Lattice construction from paraphrases The first step of the word-lattice-based method is to generate paraphrases from parallel corpus. The algorithm in (Bannard and Callison-Burch, 2005) is used for this purpose by pivoting through phrases in the source- and the target- languages: for each source phrase, all occurrences of its target phrases are found, and all the corresponding source phrases of these target phrases are considered as the potential paraphrases of the original source phrase (CallisonBurch et al., 2006). A paraphrase probability p(e2 |e1 ) is defined to reflect the similarities between two phrases, as in (1): p(e2 |e1 ) = X p(f |e1 )p(e2 |f ) (1) f where the probability p(f |e1 ) is the probability that the original source phrase e1 translates as a particular ph"
W11-1004,2008.iwslt-papers.2,0,0.0182548,"ditions. To alleviate this problem, paraphrase-enriched SMT systems have been proposed to show the effectiveness of incorporating paraphrase information. In terms of the position at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories: • Translation model augmentation with paraphrases (Callison-Burch et al., 2006; Marton et al., 2009). Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases. • Training corpus augmentation with paraphrases (Bond et al., 2008; Nakov, 2008a; Nakov, 2008b). Paraphrases are incorporated into the MT systems by expanding the training data. • Word-lattice-based method with paraphrases (Du et al., 2010; Onishi et al., Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 31–40, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics 2010). Instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the SMT system. Another directly related work is to use word lattices to deal w"
W11-1004,N06-1003,0,0.428493,"the parallel corpus is high; then the “exact phrase match” translation method could bring a good translation. However, for some language pairs, it is not easy to obtain a huge amount of parallel data, so it is not that easy to satisfy these two conditions. To alleviate this problem, paraphrase-enriched SMT systems have been proposed to show the effectiveness of incorporating paraphrase information. In terms of the position at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories: • Translation model augmentation with paraphrases (Callison-Burch et al., 2006; Marton et al., 2009). Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases. • Training corpus augmentation with paraphrases (Bond et al., 2008; Nakov, 2008a; Nakov, 2008b). Paraphrases are incorporated into the MT systems by expanding the training data. • Word-lattice-based method with paraphrases (Du et al., 2010; Onishi et al., Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 31–40, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Ass"
W11-1004,D10-1041,1,0.413496,"ion at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories: • Translation model augmentation with paraphrases (Callison-Burch et al., 2006; Marton et al., 2009). Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases. • Training corpus augmentation with paraphrases (Bond et al., 2008; Nakov, 2008a; Nakov, 2008b). Paraphrases are incorporated into the MT systems by expanding the training data. • Word-lattice-based method with paraphrases (Du et al., 2010; Onishi et al., Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 31–40, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics 2010). Instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the SMT system. Another directly related work is to use word lattices to deal with multi-source translation (Schroeder et al., 2009), in which paraphrases are actually generated from the alignments of difference source sentences. Comparing these three"
W11-1004,P07-2045,0,0.0112278,"Missing"
W11-1004,C10-1069,0,0.012722,"f this paper is to show that this compromise also works for SMT systems incorporating sourcelanguage paraphrases in the inputs. Regarding the use of paraphrases SMT system, there are still other two categories of work that are related to this paper: • Using paraphrases to improve system optimization (Madnani et al., 2007). With an English– English MT system, this work utilises paraphrases to reduce the number of manually translated references that are needed in the parameter tuning process of SMT, while preserved a similar translation quality. • Using paraphrases to smooth translation models (Kuhn et al., 2010; Max, 2010). Either cluster-based or example-based methods are proposed to obtain better estimation on phrase translation probabilities with paraphrases. q1q2 … qm ... wx-1 wx wx+1 ... wy wy+1 ... The rest of this paper is organized as follows: In section 2, we present an overview of the wordlattice-based method and its drawbacks. Section 3 proposes the CN-based method, including the building process and its application on paraphrases in SMT. Section 4 presents the experiments and results of the proposed method as well as discussions. Conclusions and future work are then given in Section 5. 2"
W11-1004,ma-2006-champollion,0,0.0200854,"ing the method proposed in Section 3.1; 3) Tune the PBSMT model on the CNs via 36 the development set. Note that the overhead of the evaluation steps are: transform each test set sentence into a word lattice, and also transform them into a CN, then feed them into the SMT decoder to obtain decoding results. 4 Experiments 4.1 Experimental setup Experiments were carried out on three English– Chinese translation tasks. The training corpora comprise 20K, 200K and 2.1 million sentence pairs, where the former two corpora are derived from FBIS corpus1 which is sentence-aligned by Champollion aligner (Ma, 2006), the latter corpus comes from HK parallel corpus,2 ISI parallel corpus,3 other news data and parallel dictionaries from LDC. The development set and the test set for the 20K and 200K corpora are randomly selected from the FBIS corpus, each of which contains 1,200 sentences, with one reference. For the 2.1 million corpus, the NIST 2005 Chinese–English current set (1,082 sentences) with one reference is used as the development set, and NIST 2003 English–Chinese current set (1,859 sentences) with four references is used as the test set. Three baseline systems are built for comparison: Moses PBSM"
W11-1004,W07-0716,0,0.0207045,"., 2005), word lattices are transformed into CNs to obtain compact representations of multiple aligned ASR hypotheses in speech understanding; in (Bertoldi et al., 2008), CNs are also adopted instead of word lattices as the source-side inputs for speech translation systems. The main contribution of this paper is to show that this compromise also works for SMT systems incorporating sourcelanguage paraphrases in the inputs. Regarding the use of paraphrases SMT system, there are still other two categories of work that are related to this paper: • Using paraphrases to improve system optimization (Madnani et al., 2007). With an English– English MT system, this work utilises paraphrases to reduce the number of manually translated references that are needed in the parameter tuning process of SMT, while preserved a similar translation quality. • Using paraphrases to smooth translation models (Kuhn et al., 2010; Max, 2010). Either cluster-based or example-based methods are proposed to obtain better estimation on phrase translation probabilities with paraphrases. q1q2 … qm ... wx-1 wx wx+1 ... wy wy+1 ... The rest of this paper is organized as follows: In section 2, we present an overview of the wordlattice-base"
W11-1004,D09-1040,0,0.0191726,"then the “exact phrase match” translation method could bring a good translation. However, for some language pairs, it is not easy to obtain a huge amount of parallel data, so it is not that easy to satisfy these two conditions. To alleviate this problem, paraphrase-enriched SMT systems have been proposed to show the effectiveness of incorporating paraphrase information. In terms of the position at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories: • Translation model augmentation with paraphrases (Callison-Burch et al., 2006; Marton et al., 2009). Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases. • Training corpus augmentation with paraphrases (Bond et al., 2008; Nakov, 2008a; Nakov, 2008b). Paraphrases are incorporated into the MT systems by expanding the training data. • Word-lattice-based method with paraphrases (Du et al., 2010; Onishi et al., Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 31–40, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computati"
W11-1004,D10-1064,0,0.0113647,"show that this compromise also works for SMT systems incorporating sourcelanguage paraphrases in the inputs. Regarding the use of paraphrases SMT system, there are still other two categories of work that are related to this paper: • Using paraphrases to improve system optimization (Madnani et al., 2007). With an English– English MT system, this work utilises paraphrases to reduce the number of manually translated references that are needed in the parameter tuning process of SMT, while preserved a similar translation quality. • Using paraphrases to smooth translation models (Kuhn et al., 2010; Max, 2010). Either cluster-based or example-based methods are proposed to obtain better estimation on phrase translation probabilities with paraphrases. q1q2 … qm ... wx-1 wx wx+1 ... wy wy+1 ... The rest of this paper is organized as follows: In section 2, we present an overview of the wordlattice-based method and its drawbacks. Section 3 proposes the CN-based method, including the building process and its application on paraphrases in SMT. Section 4 presents the experiments and results of the proposed method as well as discussions. Conclusions and future work are then given in Section 5. 2 Word-lattic"
W11-1004,W08-0320,0,0.014891,"te this problem, paraphrase-enriched SMT systems have been proposed to show the effectiveness of incorporating paraphrase information. In terms of the position at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories: • Translation model augmentation with paraphrases (Callison-Burch et al., 2006; Marton et al., 2009). Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases. • Training corpus augmentation with paraphrases (Bond et al., 2008; Nakov, 2008a; Nakov, 2008b). Paraphrases are incorporated into the MT systems by expanding the training data. • Word-lattice-based method with paraphrases (Du et al., 2010; Onishi et al., Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 31–40, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics 2010). Instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the SMT system. Another directly related work is to use word lattices to deal with multi-sou"
W11-1004,P03-1021,0,0.0831114,"ed to keep the highest-ranked path in the CN (note there is one  edge between node 9 and 10 to accomplish the merging operation). Furthermore, each edge in the CN is assigned a weight by formula (6). This weight assignment procedure penalizes paths from paraphrases according to the paraphrase probabilities, in a similar manner to the aforementioned word-lattice-based method. 3.2 Modified MT pipeline By transforming word lattices into CNs, duplicate paths are merged. Furthermore the new features on the edges are introduced by formula (6), which is then tuned on the development set using MERT (Och, 2003) in the log-linear model (Och and Ney, 2002). Since the SMT decoders are able to perform CN decoding (Bertoldi et al., 2008) in an efficient multi-stack decoding way, decoding time is drastically reduced compared to lattice decoding. The training steps are then modified as follows: 1) Extract phrase table, reordering table, and build target-side language models from parallel and monolingual corpora respectively for the PBSMT model; 2) Transform source sentences in the development set into word lattices, and then transform them into CNs using the method proposed in Section 3.1; 3) Tune the PBSM"
W11-1004,P02-1038,0,0.0755706,"n the CN (note there is one  edge between node 9 and 10 to accomplish the merging operation). Furthermore, each edge in the CN is assigned a weight by formula (6). This weight assignment procedure penalizes paths from paraphrases according to the paraphrase probabilities, in a similar manner to the aforementioned word-lattice-based method. 3.2 Modified MT pipeline By transforming word lattices into CNs, duplicate paths are merged. Furthermore the new features on the edges are introduced by formula (6), which is then tuned on the development set using MERT (Och, 2003) in the log-linear model (Och and Ney, 2002). Since the SMT decoders are able to perform CN decoding (Bertoldi et al., 2008) in an efficient multi-stack decoding way, decoding time is drastically reduced compared to lattice decoding. The training steps are then modified as follows: 1) Extract phrase table, reordering table, and build target-side language models from parallel and monolingual corpora respectively for the PBSMT model; 2) Transform source sentences in the development set into word lattices, and then transform them into CNs using the method proposed in Section 3.1; 3) Tune the PBSMT model on the CNs via 36 the development se"
W11-1004,J03-1002,0,0.00251353,"2.1 million corpus, the NIST 2005 Chinese–English current set (1,082 sentences) with one reference is used as the development set, and NIST 2003 English–Chinese current set (1,859 sentences) with four references is used as the test set. Three baseline systems are built for comparison: Moses PBSMT baseline system (Koehn et al., 2007), a realization of the translation model augmentation system described in (Callison-Burch et al., 2006) (named “Para-Sub” hereafter), and the word-lattice based system proposed in (Du et al., 2010). Word alignments on the parallel corpus are performed using GIZA++ (Och and Ney, 2003) with the “grow-diag-final” refinement. Maximum phrase length is set to 10 words and the parameters in the log-linear model are tuned by MERT (Och, 2003). All the language models are 5-gram built with the SRILM toolkit (Stolcke, 2002) on the monolingual part of the parallel corpora. 4.2 Paraphrase acquisition The paraphrases data for all paraphrase-enriched system is derived from the “Paraphrase Phrase Ta1 Paragraph-aligned corpus LDC2003E14. 2 LDC number: LDC2004T08. 3 LDC number: LDC2007T09. with LDC number Figure 3: An example of a real CN converted from a paraphrase lattice. Note that it i"
W11-1004,P10-2001,0,0.217384,"nments of difference source sentences. Comparing these three methods, the word-latticebased method has the least overheads because: • The translation model augmentation method has to re-run the whole MT pipeline once the inputs are changed, while the word-latticebased method only need to transform the new input sentences into word lattices. • The training corpus augmentation method requires corpus-scale expansion, which drastically increases the computational complexity on large corpora, while the word-lattice-based method only deals with the development set and test set. In (Du et al., 2010; Onishi et al., 2010), it is also observed that the word-lattice-based method performed better than the translation model augmentation method on different scales and two different language pairs in several translation tasks. Thus they concluded that the word-lattice-based method is preferable for this task. However, there are still some drawbacks for the word-lattice-based method: • In the lattice construction processing, duplicated paths are created and fed into SMT decoders. This decreases the paraphrase capacity in the word lattices. Note that we use the phrase “paraphrase capacity” to represent the amount of p"
W11-1004,E09-1082,0,0.293669,"Missing"
W11-1004,2006.amta-papers.25,0,0.0689156,"Missing"
W11-1004,W09-0441,0,0.0167427,"the log-linear model are tuned by MERT (Och, 2003). All the language models are 5-gram built with the SRILM toolkit (Stolcke, 2002) on the monolingual part of the parallel corpora. 4.2 Paraphrase acquisition The paraphrases data for all paraphrase-enriched system is derived from the “Paraphrase Phrase Ta1 Paragraph-aligned corpus LDC2003E14. 2 LDC number: LDC2004T08. 3 LDC number: LDC2007T09. with LDC number Figure 3: An example of a real CN converted from a paraphrase lattice. Note that it is a subsection of the whole CN that is converted from the word lattice in Figure 2. ble”4 of TER-Plus (Snover et al., 2009). Furthermore, the following two steps are taken to filter out noise paraphrases as described in (Du et al., 2010): 1. Filter out paraphrases with probabilities lower than 0.01. 2. Filter out paraphrases which are not observed in the phrase table. This objective is to guarantee that no extra out-of-vocabulary words are introduced into the paraphrase systems. The filtered paraphrase table is then used to generate word lattices and CNs. 4.3 Experimental results The results are reported in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Table 1 compares the performance of four"
W11-1004,2004.tmi-1.9,0,0.124198,"Missing"
W11-1004,P02-1040,0,\N,Missing
W12-0106,J10-4005,0,0.0486746,"m of a hybrid SMTEBMT approach. It is often the case that an EBMT system produces a good translation where SMT systems fail and vice versa (Dandapat et al., 2011). Introduction An EBMT system relies on past translations to derive the target output for a given input. Runtime EBMT approaches generally do not include any training stage, which has the advantage of not having to depend on time-consuming preprocessing. On the other hand, their runtime complexity can be considerable. This is due to the time-consuming matching stage at runtime that finds the example State-of-the-art phrase-based SMT (Koehn, 2010a) is the most successful MT approach in many large scale evaluations, such as WMT,1 IWSLT2 etc. At the same time, work continues in the area of EBMT. Some recent EBMT systems include Cunei (Phillips, 1 2 http://www.statmt.org/wmt11/ http://www.iwslt2011.org/ 48 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 48–58, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics (or set of examples) which most closely matches the source-language sentence to be translated. This matching step often uses some"
W12-0106,P07-2045,0,0.0153836,"Missing"
W12-0106,W04-3250,0,0.354876,"Missing"
W12-0106,J03-1002,0,0.00542541,"anually or automatically (e.g. using the marker-hypothesis (Groves et al., 2006)). Usually, TUs are linguistically motivated translation units. In this paper however, we explore a different route, as manual construction of high-quality TMs is time consuming and expensive. Furthermore, only considering linguistically motivated TUs may limit the matching potential of a TM. Because of this, we used SMT technology to automatically create the subsentential part of our TM at the phrase (i.e. no longer necessarily linguistically motivated) and word level. Based on Moses word alignment (using GIZA++ (Och and Ney, 2003)) and phrase table construction, we construct the additional TM for further use within an EBMT approach. Firstly, we add entries to the TM based on the aligned phrase pairs from the Moses phrase table using the following two scores: 1. Direct phrase translation probabilities: φ(t|s) 2. Direct lexical weight: lex(t|s) Table 1 shows an example of phrase pairs with the associated probabilities learned by Moses. We keep all target equivalents in a sorted order based on the 3.2 EBMT Engine The overview of the three stages of the EBMT engine is given below: Matching: In this stage, we find a sentenc"
W12-0106,P02-1040,0,0.0827422,"Missing"
W12-0106,2006.tc-1.9,0,0.0376592,"Missing"
W12-0106,2011.eamt-1.28,1,0.879031,"lore two methods to make the system scalable at runtime. First, we use an heuristic-based approach. Secondly, we use an IR-based indexing technique to speed up the time-consuming matching procedure of the EBMT system. The index-based matching procedure substantially improves run-time speed without affecting translation quality. 1 Keeping these in mind, our objective is to develop a good quality MT system choosing the best approach for each input in the form of a hybrid SMTEBMT approach. It is often the case that an EBMT system produces a good translation where SMT systems fail and vice versa (Dandapat et al., 2011). Introduction An EBMT system relies on past translations to derive the target output for a given input. Runtime EBMT approaches generally do not include any training stage, which has the advantage of not having to depend on time-consuming preprocessing. On the other hand, their runtime complexity can be considerable. This is due to the time-consuming matching stage at runtime that finds the example State-of-the-art phrase-based SMT (Koehn, 2010a) is the most successful MT approach in many large scale evaluations, such as WMT,1 IWSLT2 etc. At the same time, work continues in the area of EBMT."
W12-0106,2006.eamt-1.15,1,0.838534,"Missing"
W12-0106,2010.eamt-1.21,0,0.0234162,"for Quality and Scale Sandipan Dandapat1 , Sara Morrissey1 , Andy Way2 , Joseph van Genabith1 1 CNGL, School of Computing Dublin City University, Glasnevin, Dublin 9, Ireland {sdandapat,smorri,josef}@computing.dcu.ie 2 Applied Language Solutions, Delph, UK andy.way@appliedlanguage.com Abstract 2011), CMU-EBMT (Brown, 2011) and OpenMaTrEx (Dandapat et al., 2010). The success of an SMT system often depends on the amount of parallel training corpora available for the particular language pair. However, low translation accuracy has been observed for language pairs with limited training resources (Islam et al., 2010; Khalilov et al., 2010). SMT systems effectively discard the actual training data once the models (translation model and language model) have been estimated. This can lead to their inability to guarantee good quality translation for sentences closely matching those in the training corpora. By contrast, EBMT systems usually maintain a linked relationship between the full sentence pairs in source and target texts. Because of this EBMT systems can often capture long range dependencies and rich morphology at runtime. In contrast to SMT, however, most EBMT models lack a wellformed probability mode"
W12-0106,2009.mtsummit-papers.14,0,0.0557551,"nguage translation pairs for effective reuse of previous translations originally created by human translators. TMs are often used to store examples for EBMT systems. After retrieving a set of examples with associated translations, EBMT systems automatically extract translations of suitable fragments and combine them to produce a grammatical target output. Phrase-based SMT systems (Koehn, 2010a), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to be used in a CAT environment (Simard, 2003; Bic¸ici and Dymetman, 2008; Bourdaillet et al., 2009; Simard and Isabelle, 2009). Koehn and Senellart (2010b) use SMT to produce the translation of the non-matched fragments after obtaining the TMbased match. EBMT phrases have also been used to populate the knowledge database of an SMT system (Groves et al., 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when no sufficiently well matching translation unit (TU) is found in the TM. The TransTy"
W12-0106,W03-0313,0,0.0351912,"anslation memory (TM). A TM essentially stores source- and target-language translation pairs for effective reuse of previous translations originally created by human translators. TMs are often used to store examples for EBMT systems. After retrieving a set of examples with associated translations, EBMT systems automatically extract translations of suitable fragments and combine them to produce a grammatical target output. Phrase-based SMT systems (Koehn, 2010a), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to be used in a CAT environment (Simard, 2003; Bic¸ici and Dymetman, 2008; Bourdaillet et al., 2009; Simard and Isabelle, 2009). Koehn and Senellart (2010b) use SMT to produce the translation of the non-matched fragments after obtaining the TMbased match. EBMT phrases have also been used to populate the knowledge database of an SMT system (Groves et al., 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when n"
W14-3303,J03-1002,0,0.00990406,"ble at http://github.com/bicici/FDAOptimization. 2.2 Language Model Data Selection Results We run ParFDA5 SMT experiments for all language pairs in both directions in the WMT14 translation task (Bojar et al., 2014), which include English-Czech (en-cs), English-German (en-de), English-French (en-fr), English-Hindi (en-hi), and English-Russian (en-ru). We true-case all of the corpora, use 150-best lists during tuning, set the LM order to a value between 7 and 10 for all language pairs, and train the LM using SRILM (Stolcke, 2002). We set the maximum sentence length filter to 126 and for GIZA++ (Och and Ney, 2003), Parallel FDA5 Parallel FDA5 (ParFDA5) is presented in Algorithm 1, which first shuffles the training sentences, U and runs individual FDA5 models on the multiple splits from which equal number of sentences, 1 (Cormen et al., 2009), question 6.5-9. Merging k sorted lists into one sorted list using a min-heap for k-way merging. 2 Unless the translation is a verbatim copy of the source. 60 S→T en-cs en-cs cs-en cs-en en-de en-de de-en de-en en-fr en-fr fr-en fr-en en-hi en-hi hi-en hi-en en-ru en-ru ru-en ru-en Data C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5 C ParFDA5"
W14-3303,W13-2206,1,0.833312,"Missing"
W14-3303,W14-3332,0,0.151794,"e the diversity. Algorithm 1: Parallel FDA5 Input: Parallel training sentences U, test set features F, and desired number of training instances N . Output: Subset of the parallel sentences to be used as the training data L ⊆ U. 1 U ← shuffle(U) 2 U , M ← split(U, N ) 3 L ← {} 4 foreach Ui ∈ U do 5 hLi , si i ← FDA5(Ui , F, M ) 6 L ← L ∪ hLi , si i 7 L ← merge(L) 2.3 We select the LM training data with ParFDA5 based on the following observation (Bic¸ici, 2013): 3.22 BLEU points compared to random selection. FDA5 is also used for selecting the training set in the WMT14 medical translation task (Calixto et al., 2014) and the tuning set in the WMT14 German-English translation task (Li et al., 2014). FDA5 has 5 parameters that effect the instance scores based on the three formulas used: No word not appearing in the training set can appear in the translation. It is impossible for an SMT system to translate a word unseen in the training corpus nor can it translate it with a word not found in the target side of the training set 2 . Thus we are only interested in correctly ordering the words appearing in the training corpus and collecting the sentences that contain them for building the LM. At the same time, a"
W14-3303,P07-2045,0,0.0130258,"Deployment of Accurate Statistical Machine Translation Systems Ergun Bic¸ici Qun Liu Andy Way Centre for Next Generation Localisation Centre for Next Generation Localisation Centre for Next Generation Localisation School of Computing School of Computing School of Computing Dublin City University Dublin City University Dublin City University ergun.bicici@computing.dcu.ie qliu@computing.dcu.ie away@computing.dcu.ie Abstract Parallel FDA5 runs separate FDA5 models on randomized subsets of the training data and combines the selections afterwards. We run parallel FDA5 SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT14 (Bojar et al., 2014) and obtain SMT performance close to the top constrained Moses systems training using all of the training material. Parallel FDA5 allows rapid prototyping of SMT systems for a given target domain or task and can be very useful for MT in target domains with limited resources or in disaster and crisis situations (Lewis et al., 2011). We use parallel FDA5, an efficiently parameterized and optimized parallel implementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a"
W14-3303,W11-2164,0,0.0270653,"ting.dcu.ie away@computing.dcu.ie Abstract Parallel FDA5 runs separate FDA5 models on randomized subsets of the training data and combines the selections afterwards. We run parallel FDA5 SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT14 (Bojar et al., 2014) and obtain SMT performance close to the top constrained Moses systems training using all of the training material. Parallel FDA5 allows rapid prototyping of SMT systems for a given target domain or task and can be very useful for MT in target domains with limited resources or in disaster and crisis situations (Lewis et al., 2011). We use parallel FDA5, an efficiently parameterized and optimized parallel implementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a day for each translation direction. We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of 3.49 BLEU points difference using significantly less resources for training and development. 1 2 2.1 Introduction Parallel FDA5 for Instance Selection FDA5 FDA is developed mai"
W14-3303,W14-3314,1,0.812886,"Missing"
W14-3303,W11-2131,1,\N,Missing
W14-3303,W14-3302,0,\N,Missing
W14-3314,P11-1105,0,0.06307,"nfiguration for submitted system will be presented. And we conclude in the last section. Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model (Koehn et al., 2007). For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models (Axelrod et al., 2005; Galley and Manning, 2008), a 9gram Operation Sequence Model (Durrani et al., 2011) and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA (Cherry and Foster, 2012) on development set. We set the maximum iteration to be 25. The Language Models in our systems are trained with SRILM (Stolcke, 2002). We trained 2 Pre-processing We use all the training data for German-English translation, including Europarl, News Commentary and Common Crawl. The first thing we noticed is that some Non-German and Non-English sentences are i"
W14-3314,N13-1073,0,0.0337443,"ases, are designed to select the right translation. But different with (He et al., 2008), we use sparse features to model the context. And instead of using syntactic POS, we adopt independent POS-like features: cluster ID of word. In our experiment mkcls was used to cluster words into 50 groups. And all features are generalized to cluster ID. 4 • Multi-alignment Selection: We also try to use multi-alignment selection (Tu et al., 2012) to generate a ”better” alignment from three alignmens: MGIZA++ with function growdiag-final-and, SyMGIZA++ with function grow-diag-final-and and fast alignment (Dyer et al., 2013). Although this method show comparable or better result on development set, it fails on test set. Since we build a few systems with different setting on Moses phrase-based model, a straightforward thinking is to obtain the better translation from several different translation systems. So we use system combination (Heafield and Lavie, 2010) on the 1-best outputs of three systems (indicated with ∗ in table 4). And this results in our best system so far, as shown in Table 4. In our final submission, this result is taken as primary. Submission Based on our preliminary experiments in the section ab"
W14-3314,D08-1089,0,0.526005,"s task and the experiment results. In section 4, our final configuration for submitted system will be presented. And we conclude in the last section. Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model (Koehn et al., 2007). For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models (Axelrod et al., 2005; Galley and Manning, 2008), a 9gram Operation Sequence Model (Durrani et al., 2011) and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA (Cherry and Foster, 2012) on development set. We set the maximum iteration to be 25. The Language Models in our systems are trained with SRILM (Stolcke, 2002). We trained 2 Pre-processing We use all the training data for German-English translation, including Europarl, News Commentary and Common Crawl. The first thing we notice"
W14-3314,W08-0509,0,0.030245,"selected by FDA is better than the baseline tuned with all the development data. 3.2 The Operation Sequence Model (OSM) (Durrani et al., 2011) explains the translation procedure as a linear sequence of operations which generates source and target sentences in parallel. Durrani et al. (2011) defined four translation operations: Generate(X,Y), Continue Source Concept, Generate Source Only (X) and Generate Identical, as well as three reordering operations: Insert Gap, Jump Back(W) and Jump Forward. These operations are described as follows. In this section, alignment model is trained by MGIZA++ (Gao and Vogel, 2008) with grow-diag-final-and heuristic function. And other settings are mostly default values in Moses. 3.1 Operation Sequence Model Lexicalized Reordering Model • Generate(X,Y) make the words in Y and the first word in X added to target and source string respectively. German and English have different word order which brings a challenge in German-English machine translation. In our system, we adopt three Lexicalized Reordering Models (LRMs) for addressing this problem. They are word-based LRM (wLRM), phrase-based LRM (pLRM) and hierarchal LRM (hLRM). • Continue Source Concept adds the word in th"
W14-3314,2005.iwslt-1.8,0,0.0531592,"we have tried for this task and the experiment results. In section 4, our final configuration for submitted system will be presented. And we conclude in the last section. Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model (Koehn et al., 2007). For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models (Axelrod et al., 2005; Galley and Manning, 2008), a 9gram Operation Sequence Model (Durrani et al., 2011) and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA (Cherry and Foster, 2012) on development set. We set the maximum iteration to be 25. The Language Models in our systems are trained with SRILM (Stolcke, 2002). We trained 2 Pre-processing We use all the training data for German-English translation, including Europarl, News Commentary and Common Crawl"
W14-3314,C08-1041,1,0.816799,"gressively. • SyMGIZA++: Better alignment could lead to better translation. So we carry out some experiments on SyMGIZA++ aligner (JunczysDowmunt and Sza, 2012), which modifies the original IBM/GIZA++ word alignment models to allow to update the symmetrized models between chosen iterations of the original training algorithms. Experiment shows this new alignment improves translation quality. • Sparse Features: For each source phrase, there is usually more than one corresponding translation option. Each different translation may be optimal in different contexts. Thus in our systems, similar to (He et al., 2008) which proposed a Maximum Entropy-based rule selection for the hierarchical phrasebased model, features which describe the context of phrases, are designed to select the right translation. But different with (He et al., 2008), we use sparse features to model the context. And instead of using syntactic POS, we adopt independent POS-like features: cluster ID of word. In our experiment mkcls was used to cluster words into 50 groups. And all features are generalized to cluster ID. 4 • Multi-alignment Selection: We also try to use multi-alignment selection (Tu et al., 2012) to generate a ”better” a"
W14-3314,P96-1041,0,0.259229,"Missing"
W14-3314,D07-1091,0,0.0381084,"· · oJ ) is: p(O) = J Y p(oj |oj−n+1 · · · oj−1 ) (1) j=1 where n indicates the number of previous operations used. In this paper we train a 9-gram OSM on training data and integrate this model directly into loglinear framework (OSM is now available to use in Moses). Our experiment shows OSM improves our system by about 0.8 BLEU (see Table 2). 3.3 3.4 Other Tries In addition to the techniques mentioned above, we also try some other approaches. Unfortunately all of these methods described in this section are non-effective in our experiments. The results are shown in Table 2. • Factored Model (Koehn and Hoang, 2007): We tried to integrate a target POS factored model into our system with a 9-gram POS language model to address the problem of word selection and word order. But experiment doesn’t show improvement. The English POS is from Stanford POS Tagger (Toutanova et al., 2003). Language Model Interpolation In our baseline, Language Model (LM) is trained on all the monolingual data provided. In this section, we try to build a large language model by including data from English Gigaword fifth edition (only taking partial data with size of 1.6G), English side of UN corpus and English side of 109 French-Eng"
W14-3314,E03-1076,0,0.139113,"Missing"
W14-3314,P07-2045,0,0.00785319,"detection: percentage of filtered out sentences a 5-gram model with Kneser-Ney discounting (Chen and Goodman, 1996). In the next sections, we will describe our system in detail. In section 2, we will explain our preprocessing steps on corpus. Then in section 3, we will describe some techniques we have tried for this task and the experiment results. In section 4, our final configuration for submitted system will be presented. And we conclude in the last section. Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model (Koehn et al., 2007). For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models (Axelrod et al., 2005; Galley and Manning, 2008), a 9gram Operation Sequence Model (Durrani et al., 2011) and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA (Cher"
W14-3314,W13-2221,0,0.0180035,"d the results are shown in Table 4. to minimize perplexity measured on development set. We try to split the training data in two ways. One is according to data source, resulting in three subsets: Europarl, News Commentary and Common Crawl. Another one is to use data selection. We use FDA to select 200K sentence pairs as in-domain data and the rest as out-domain data. Unfortunately both experiments failed. In Table 2, we only report results of phrase table combination on FDA-based data sets. • Development Set Selection: Instead of using FDA which is dependent on test set, we use the method of (Nadejde et al., 2013) to select tuning set from newstest 2008-2013 for the final system. We only keep 2K sentences which have more than 30 words and higher BLEU score. The experiment result is shown in Table 4 ( The system is indicated as Baseline). • OSM Interpolation: Since OSM in our system could be taken as a special language model, we try to use the idea of interpolation similar with language model to make OSM adapted to some data. Training data are splitted into two subsets with FDA. We train 9-gram OSM on each subsets and interpolate them according to OSM trained on the development set. • Pre-processing: In"
W14-3314,E12-1055,0,0.0274196,"line, Language Model (LM) is trained on all the monolingual data provided. In this section, we try to build a large language model by including data from English Gigaword fifth edition (only taking partial data with size of 1.6G), English side of UN corpus and English side of 109 French-English corpus. Instead of training a single model on all data, we interpolate language models trained on each subset (monolingual data provided is splitted into three parts: News 20072013, Europarl and News Commentary) by tuning • Translation Model Combination: In this experiment, we try to use the method of (Sennrich, 2012) to combine phrase tables or reordering tables from different subsets of data 138 final submission. And the results are shown in Table 4. to minimize perplexity measured on development set. We try to split the training data in two ways. One is according to data source, resulting in three subsets: Europarl, News Commentary and Common Crawl. Another one is to use data selection. We use FDA to select 200K sentence pairs as in-domain data and the rest as out-domain data. Unfortunately both experiments failed. In Table 2, we only report results of phrase table combination on FDA-based data sets. •"
W14-3314,N03-1033,0,0.00982984,"). Our experiment shows OSM improves our system by about 0.8 BLEU (see Table 2). 3.3 3.4 Other Tries In addition to the techniques mentioned above, we also try some other approaches. Unfortunately all of these methods described in this section are non-effective in our experiments. The results are shown in Table 2. • Factored Model (Koehn and Hoang, 2007): We tried to integrate a target POS factored model into our system with a 9-gram POS language model to address the problem of word selection and word order. But experiment doesn’t show improvement. The English POS is from Stanford POS Tagger (Toutanova et al., 2003). Language Model Interpolation In our baseline, Language Model (LM) is trained on all the monolingual data provided. In this section, we try to build a large language model by including data from English Gigaword fifth edition (only taking partial data with size of 1.6G), English side of UN corpus and English side of 109 French-English corpus. Instead of training a single model on all data, we interpolate language models trained on each subset (monolingual data provided is splitted into three parts: News 20072013, Europarl and News Commentary) by tuning • Translation Model Combination: In this"
W14-3314,C12-2122,1,0.756888,"Missing"
W14-3314,N12-1047,0,\N,Missing
W14-3319,D11-1033,0,0.199736,"nt sets, to obtain our final SMT system. Our submission for the English to French translation task was ranked second amongst nine teams and a total of twenty submissions. 1 To train the LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding pa"
W14-3319,P11-1105,0,0.180288,"encodes a different degree of generalisation over the particular example it has been extracted from. Finally, the minimum set of rules which correctly reproduces all the bilingual phrases is found based on integer linear programming search (Garfinkel and Nemhauser, 1972). Once the rules have been inferred, the phrase table is built from them and the original rulebased MT dictionaries, following the method by S´anchez-Cartagena et al. (2011), which was one of winning systems4 (together with two online SMT systems) in the pairwise manual evaluation of the WMT11 English–Spanish translation task (Callison-Burch et al., 2011). This phrasetable is then interpolated with the baseline TM and the results are presented in Table 5. A slight improvement over the baseline is observed, which motivates the use of synthetic rules in our final MT system. This small improvement may be related to the small coverage of the Apertium dictionaries: the English–French bilingual dictionary has a low number of entries compared to more mature language pairs in Apertium which have around 20 times more bilingual entries. System the number of n-bests used by MERT. Results obtained on the development set newstest2013 are reported in Table"
W14-3319,W13-2212,0,0.0160957,"except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in M OSES Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 News Commentary v8 304.2 News Shuffled 2007 3,782.5 News Shuffled 2008 12,954.5 News Shuffled 2009 14,680.0 News Shuffled 2010 6,797.2 News Shuffled 2011 15,437.7 News Shuffled 2012 14,869.7 News Shuffled 2013 21,688.4 LDC afp 7,184.9 LDC apw 8,829.4 LDC cna 618.4 LDC ltw 986.9 LDC nyt 5,327.7 LDC wpb 108.8 LDC xin 5,121.9 59.9 7.4 90.2 308.1 347.0 157.8 358.1 345.5"
W14-3319,W08-0509,0,0.170818,"test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we ma"
W14-3319,P13-2121,0,0.0606903,"Missing"
W14-3319,P07-2045,0,0.0136523,"with the focus on the English to French direction. Language models (LMs) and translation 171 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171–177, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2 Datasets and Tools Corpus We use all the monolingual and parallel datasets in English and French provided by the shared task organisers, as well as the LDC Gigaword for the same languages1 . For each language, a true-case model is trained using all the data, using the traintruecaser.perl script included in the M OSES toolkit (Koehn et al., 2007). Punctuation marks of all the monolingual and parallel corpora are then normalised using the script normalize-punctuation.perl provided by the organisers, before being tokenised and true-cased using the scripts distributed with the M OSES toolkit. The same pre-processing steps are applied to the development and test sets. As development sets, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and wit"
W14-3319,W13-2235,0,0.294214,"he LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding parameters. Introduction The reminder of this paper is organised as follows: the datasets and tools used in our experiments are described in Section 2. Then, details about the LMs and TMs a"
W14-3319,P10-2041,0,0.0671112,"ces and (ii) reduce the total amount of out-of-domain data. Second, a novel approach for the automatic extraction of translation rules and their use to enrich the phrase table is detailed. 10 Parallel Data Filtering and Vocabulary Saturation Bilingual Cross-Entropy Difference 4.1 Amongst the parallel corpora provided by the shared task organisers, only News Commentary can be considered as in-domain regarding the development and test sets. We use this training corpus to build our baseline SMT system. The other parallel corpora are individually filtered using bilingual cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011). This data filtering method relies on four LMs, two in the source and two in the target language, which aim to model particular features of in and out-ofdomain sentences. We build the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 8 6 4 2 0 Common Crawl Europarl 10^9 UN -2 -4 0 2k 4k 6k Sentence Pairs 8k"
W14-3319,J03-1002,0,0.01001,"ts, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich"
W14-3319,P03-1021,0,0.0469427,"rter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use K EN LM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use M OSES (Koehn et al., 2007) version 2.1 with MGIZA ++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over se"
W14-3319,P02-1040,0,0.0911366,"d the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 8 6 4 2 0 Common Crawl Europarl 10^9 UN -2 -4 0 2k 4k 6k Sentence Pairs 8k 10k Figure 1: Sample of ranked sentence-pairs (10k) of each of the out-of-domain parallel corpora with bilingual cross-entropy difference The results obtained using the pseudo indomain data show B LEU (Papineni et al., 2002) scores superior or equal to the baseline score. Only the Europarl subset is slightly lower than the baseline, while the subset taken from the 109 corpus reaches the highest B LEU compared to the other systems (30.29). This is mainly due to the 3 The subsets contain the same number of sentences and the same vocabulary as News Commentary. 173 size of this subset which is ten times larger than the one taken from Europarl. The last row of Table 3 shows the B LEU score obtained after interpolating the four pseudo in-domain translation models. This system outperforms the best pseudo indomain one by"
W14-3319,2011.mtsummit-papers.64,1,0.889124,"Missing"
W14-3319,E12-1055,0,0.0325549,"Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is M OSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in M OSES Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 News Commentary v8 304.2 News Shuffled 2007 3,782.5 News Shuffled 2008 12,954.5 News Shuffled 2009 14,680.0 News Shuffled 2010 6,797.2 News Shuffled 2011 15,437.7 News Shuffled 2012 14,869.7 News Shuffled 2013 21,688.4 LDC afp 7,184.9 LDC apw 8,829.4 LDC cna 618.4 L"
W14-3319,2006.amta-papers.25,0,0.0323304,"r detruecasing and de-tokenising using the scripts distributed with M OSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of B LEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation leads to an overall improvement of 0.1 B LEU absolute. The results on newstest2014 show that adding synthetic rules did not help improving B LEU and degraded slightly TER (Snover et al., 2006) scores. In addition to our English→French submission, we submitted a French→English translation. Our French→English MT system is built on the alignments obtained from the English→French direction. The training processes between the two sys27.76 28.06 Table 5: BLEU scores reported by MERT on English–French newstest2013 for the baseline SMT system standalone and with automatically extracted translation rules. 5 27.76 31.93 31.90 32.21 32.10 Table 6: B LEU scores reported by MERT on English–French newstest2013 development set. BLEUdev Baseline Baseline+Rules BLEUdev Tuning and Decoding We presen"
W14-3319,D07-1080,0,0.0658355,"experiments. Adding the synthetic translation rules degrades B LEU (as indicated by the last row in the Table), thus we decide to submit two systems to the shared task: one without and one with synthetic rules. By submitting a system without synthetic rules, we also ensure that our SMT system is constrained according to the shared task guidelines. System Baseline + pseudo in + pseudo out + OSM + MERT 200-best + Rules As MERT is not suitable when a large number of features are used (our system uses 19 fetures), we switch to the Margin Infused Relaxed Algorithm (MIRA) for our submitted systems (Watanabe et al., 2007). The development set used is newstest2012, as we aim to select the best decoding parameters according to the scores obtained when decoding the newstest2013 corpus, after detruecasing and de-tokenising using the scripts distributed with M OSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of B LEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation lea"
W14-3319,W14-3320,1,\N,Missing
W14-3325,bojar-etal-2014-hindencorp,0,0.146073,"Missing"
W14-3325,P96-1041,0,0.232301,"including data from the English Gigaword fifth edition, the English side of the UN corpus, the English side of the 109 French–English corpus and the English side of the Hindi–English parallel data provided by the organisers. We interpolate language models trained using each dataset, with the monolingual data provided split into three parts (news 2007-2013, Europarl (?) and news commentary) and the weights tuned to minimize perplexity on the target side of the devset. The language models in our systems are trained with SRILM (Stolcke, 2002). We train a 5-gram model with Kneser-Ney discounting (Chen and Goodman, 1996). 3.4 (2) (3) For the Hindi-to-English translation task, we use part-of-speech (PoS) tags4 of the source phrase and the neighbouring words as the contextual feature, owing to the fact that supertaggers are readily available only for English. We use a memory-based machine learning (MBL) classifier (TRIBL: (Daelemans, 2005))5 that is able to estimate P(ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. Thus, we derive the feature ˆ mbl defined in Equation (2). In addition to h ˆ mbl , h 4 In order to obtain PoS tags"
W14-3325,P11-1105,0,0.0954256,"anslation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms stand for (Left|Right) (Round|Square|Curly) Bracket. 2 215 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics problem, where a source phrase with given additional context information is classified into a distribution over possible target phrases. The size of this distribution needs to be limited, and would ideally omit irrelevant target ph"
W14-3325,D08-1089,0,0.0630626,"tokens on either side and Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms stand for (Left|Right) (Round|Square|Curly) Bracket. 2 215 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics problem, where a source phrase with given additional context information is classified into a distribution over"
W14-3325,2005.iwslt-1.8,0,0.0390493,"ntaining more than 80 tokens on either side and Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms stand for (Left|Right) (Round|Square|Curly) Bracket. 2 215 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics problem, where a source phrase with given additional context information is classifie"
W14-3325,N03-1017,0,0.0101267,"tp://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms stand for (Left|Right) (Round|Square|Curly) Bracket. 2 215 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215–220, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics problem, where a source phrase with given additional context information is classified into a distribution over possible target phrases. The size of this distribution needs to be limited, and would ideally omit irrelevant target phrase translations that the standard PB-SMT (Koehn et al., 2003) approach would normally include. Following Haque et al. (2011), we derive a context-informed feature ˆ mbl that is expressed as the conditional probabilh ity of the target phrase eˆk given the source phrase fˆk and its context information (CI), as in (2): sentence pairs with length difference larger than 3 times. 3 3.1 Techniques Deployed Combination of Various Lexical Reordering Model (LRM) Clearly, Hindi and English have quite different word orders, so we adopt three lexical reordering models to address this problem. They are wordbased LRM and phrase-based LRM, which mainly focus on local r"
W14-3325,I08-2099,0,0.075816,"Missing"
W14-3325,P07-2045,0,0.0107113,"-, -LSB-, LCB-, -RRB-, -RSB-, and -RCB-.3 For consistency, those character sequences in the training data were replaced by the corresponding brackets. For English – both monolingual and the target side of the bilingual data – we perform tokenization, normalization of punctuation, and truecasing. For parallel training data, we filter sentences pairs containing more than 80 tokens on either side and Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org"
W14-3325,bojar-etal-2010-data,0,0.0144511,"experimental results and resultant discussion. This paper describes the DCU-Lingo24 submission to WMT 2014 for the HindiEnglish translation task. We exploit miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), MultiAlignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task. 1 2 Pre-processing Steps We use all the training data provided for Hindi– English translation. Following Bojar et al. (2010), we apply a number of normalisation methods on the Hindi corpus. The HindEnCorp parallel corpus compiles several sources of parallel data. We observe that the source-side (Hindi) of the TIDES data source contains font-related noise, i.e. many Hindi sentences are a mixture of two different encodings: UTF-81 and WX2 notations. We prepared a WX-to-UTF-8 font conversion script for Hindi which converts all WX encoded characters into UTF-8, thus removing all WX encoding appearing in the TIDES data. We also observe that a portion of the English training corpus contained the following bracketlike seq"
W14-3325,W04-3230,0,0.0852863,"Missing"
W14-3325,N06-1014,0,0.117013,"Missing"
W14-3325,J03-1002,0,0.00674523,"anguage model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. The following algorithm shows the steps involved: 1. Prepare the monolingual source and target sentences. Multi-Alignment Combination (MAC) Word alignment is a critical component of MT systems. Various methods for word alignment have been proposed, and different models can produce signicantly different outputs. For example, Tu et al. (2012) demonstrates that the alignment agreement between the two best-known alignment tools, namely Giza++(Och and Ney, 2003) and 2. Prepare the dictionary which consists of U entries of source and target sentences comprising non-stop-words. 3. Train the neural network language model on the source side and obtain the real vectors of X dimensions for each word. 6 Suffixes were separated and completely removed from the training data. 7 217 http://code.google.com/p/berkeleyaligner/ 4. Train the neural network language model on the target side and obtain the real vectors of X dimensions for each word. 4.2 We employ a standard Moses PB-SMT model as our baseline. The Hindi side is preprocessed but unstemmed. We use Giza++"
W14-3325,P03-1021,0,0.0686331,"For consistency, those character sequences in the training data were replaced by the corresponding brackets. For English – both monolingual and the target side of the bilingual data – we perform tokenization, normalization of punctuation, and truecasing. For parallel training data, we filter sentences pairs containing more than 80 tokens on either side and Introduction This paper describes the DCU-Lingo24 submission to WMT 2014 for the Hindi-English translation task. All our experiments on WMT 2014 are built upon the Moses phrase-based model (PB-SMT) (Koehn et al., 2007) and tuned with MERT (Och, 2003). Starting from this baseline system, we exploit various methods including ContextInformed PB-SMT (CIPBSMT), zero-shot learning (Palatucci et al., 2009) using neural networkbased language modelling (Bengio et al., 2000; Mikolov et al., 2013) for OOV word conversion, various lexical reordering models (Axelrod et al., 2005; Galley and Manning, 2008), various Multiple Alignment Combination (MAC) (Tu et al., 2012), Operation Sequence Model (OSM) (Durrani et al., 2011) and Language Model Interpolation(LMI). 1 http://en.wikipedia.org/wiki/UTF-8 http://en.wikipedia.org/wiki/WX_notation 3 The acronyms"
W14-3325,W14-3329,1,0.619071,", word stemming on the Hindi side usually results in too many English words being aligned to one stemmed Hindi word, i.e. we encounter the problem of phrase overextraction. Therefore, we conduct word alignment with the stemmed version of Hindi, and then at the phrase extraction step, we replace the stemmed form with the original Hindi form. 3.8 OOV Word Conversion Method Our algorithm for OOV word conversion uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013). The same technique is used in (Okita et al., 2014). This method requires neither parallel nor comparable corpora, but rather two monolingual corpora. In our context, we prepare two monolingual corpora on both sides, which are neither parallel nor comparable, and a small amount of already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). Then, we train both sides with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. The following algorithm shows the"
W14-3325,C12-2122,1,0.901635,"Missing"
W14-3329,2005.mtsummit-papers.11,0,0.0124348,"term = {eterm1 , . . . , etermn } and Fterm = {fterm1 , . . . , ftermn }. We search for possible alignment links between the term-pair only when they co-occur in the same sentence. One obvious advantage of this approach is the computational complexity which is fairly low. Note that the result of (Okita et al., 2010) shows that the frequency-based approach of (Kupiec, 1993) worked well for NTCIR patent terminology (Fujii et al., 2010), which otherwise would have been difficult to capture via the traditional SMT/GIZA++ method. In contrast, however, this did not work well on the Europarl corpus (Koehn, 2005). 240 However, even among European languages, this mechanism makes it possible to find possible translation counterparts for a given term. In this query task, we did this only for the French-to-English direction and only for words containing accented characters (by rule-based conversion). 2.3 Terminology Dictionaries Terminology dictionaries themselves are obviously among the most important resources for bilingual term-pairs. In this medical query translation subtask, two corpora are provided for this purpose: (i) Unified Medical Language System corpus (UMLS corpus),1 and (ii) Wiki entries.2 2"
W14-3329,J10-4005,0,0.0130232,"small subset of sentences includes a few terms, such that term-pairs become sparse with regard to sentences. Term-pairs can be found rather easily if a candidate term-pair cooccurs on the source and the target sides and on the condition that the items in the term-pair actually correspond with one another. 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation mod"
W14-3329,P93-1003,0,0.0136536,"is evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi , ftermj |st ) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the entire P corpus N t=1 c(etermi , ftermj |st ). Then, the algorithm adjusts the length of etermi and ftermj . It can be said that this algorithm captures termpairs which occur rather frequently. However, this We assume two predefined sets of terms at the outset, Eterm = {eterm1 , . . . , etermn } and Fterm = {fterm1 , . . . , ftermn }. We search for pos"
W14-3329,J93-2003,0,0.023956,". Accordingly, if our aim changes to capture only those less frequent pairs, the situation changes dramatically. The number of terms we need to capture is considerably decreased. Many sentences do not include any terminology at all, and only a relatively small subset of sentences includes a few terms, such that term-pairs become sparse with regard to sentences. Term-pairs can be found rather easily if a candidate term-pair cooccurs on the source and the target sides and on the condition that the items in the term-pair actually correspond with one another. 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pair"
W14-3329,N13-1090,0,0.12842,"i) “What can we do if the terminology does not occur in a corpus?” These two problems require computationally quite different approaches than what is usually done in the standard statistical approach. Furthermore, the medical query task in WMT14 provides a wide range of corpora: parallel and monolingual corpora, as well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). It is possible to approach this in a reverse manner: “less frequent pairs can be outstanding term candidates”. Accordingly, if our aim changes to capture only those less frequent pairs, the situation changes dramatically. The number of terms we need to capture is considerably decreased. Many sentences do not include any terminology at all, and only a relatively small subset of sentences includes a few terms, such that term-pairs become sparse with regard to sentences. Term-pairs can be found rather easily if a candidate term-pair cooccurs on the source and the target sides and on the condi"
W14-3329,P09-1104,0,0.0127763,"nslation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi , ftermj |st ) on the source a"
W14-3329,J03-1002,0,0.0203194,"o translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi , ftermj |st ) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the entire P corpus N t=1 c(etermi , ftermj |st ). Then, the algorithm adjusts the length of etermi and ftermj . It can be said that this algorithm captures termpairs which occur rather frequently. However, this We assume two"
W14-3329,P03-1021,0,0.108968,"Missing"
W14-3329,N03-1017,0,0.0289087,"Missing"
W14-3329,P07-2045,0,0.0230485,"rs on the source and the target sides and on the condition that the items in the term-pair actually correspond with one another. 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The clas"
W14-3329,W10-4006,1,0.807417,"h a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi , ftermj |st ) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the"
W14-3329,W04-3250,0,0.0737557,"Missing"
W14-3329,P02-1040,0,0.0884738,"Missing"
W14-3329,H05-1059,0,0.0191079,"Missing"
W14-3329,2013.tc-1.12,1,0.769408,"Missing"
W14-3339,W11-2131,1,0.344982,"Missing"
W14-3339,W11-2137,1,0.934164,"Missing"
W14-3339,S14-2010,0,0.0176795,"use and how they can be used to predict the performance of translation and judging the semantic similarity between text. RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable in different domains and tasks. 2013), top performance when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014), and a language independent solution and good performance when judging the semantic similarity of sentences (Agirre et al., 2014; Bic¸ici and Way, 2014). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic¸ici, 2008). Each RTM model is a data translation model between the instances in the training set and the test set."
W14-3339,S12-1059,0,0.00925526,"Missing"
W14-3339,W14-3303,1,0.693268,"Missing"
W14-3339,W13-2242,1,0.379343,"Missing"
W14-3339,S14-2003,0,0.0223763,", 313 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 313–321, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation and judging the semantic similarity between text. RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable in different domains and tasks. 2013), top performance when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014), and a language independent solution and good performance when judging the semantic similarity of sentences (Agirre et al., 2014; Bic¸ici and Way, 2014). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, data close to both the training set and the test set, which allow shared semantics by provi"
W14-3339,P07-2045,0,0.00543926,"contain 4.5M sentences for de-en with 110M words for de and 116M words for en and 15.1M sentences for en-es with 412M words for en and 462M words for es. We do not use any resources provided by QET including data, software, or baseline features. Instance selection for the training set and the language model (LM) corpus is handled by parallel FDA5 (Bic¸ici et al., 2014), whose parameters are optimized for each translation task. LM are trained using SRILM (Stolcke, 2002). We tokenize and true-case all of the corpora. The true-caser is trained on all of the available training corpus using Moses (Koehn et al., 2007). Table 2 lists the number of sentences in the training and test sets for each task. For each task or subtask, we select 375 thousand (K) training instances from the available parallel training corpora as interpretants for the individual RTM models using parallel FDA5. We add the selected training set to the 3 million (M) sentences selected from the available monolingual corpora for each LM corpus. The statistics of the training data selected by and used as interpretants in the 4 Train 3816 1050 1400 1050 896 650 1957 900 715 350 Test 600 450 600 450 208 208 382 150 150 100 Table 2: Number of"
W14-3339,levy-andrew-2006-tregex,0,0.00873579,"g depthB), number of brackets on the right branches over the number of brackets on the left (R/L) 2 , average right to left branching over all internal tree nodes (avg R/L). The ratio of the number of right to left branches shows the degree to which the sentence is right branching or not. Additionally, we capture the different types of branching present in a given parse tree identified by the number of nodes in each of its children. Table 1 depicts the parsing output obtained by CCL for the following sentence from WSJ23 3 : Many fund managers argue that now ’s the time to buy . We use Tregex (Levy and Andrew, 2006) for visualizing the output parse trees presented on the left. The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 RTM in the Quality Estimation Task We participate in all of the four challenges of the quality estimation task (QET) (Bojar et al., 2014), which include English to Spanish (en-es), Spanish to English (es-en), English to German (ende), and German to English (de-en) translation directions. There are two main catego"
W14-3339,J93-2003,0,0.0271665,"the distribution of alignment probabilities for S P ( s∈S −p log p for p = p(t|s) where s and t are tokens) and T, their average for S and T, the number of entries with p ≥ 0.2 and p ≥ 0.01, the entropy of the word alignment between S and T and its average, and word alignment log probability and its value in terms of bits per word. We also compute word alignment percentage as in (Camargo de Souza et al., 2013) and potential BLEU, F1 , WER, PER scores for S and T. IBM1 Translation Probability {4, 12}: Calculates the translation probability of test sentences using the selected training set, I (Brown et al., 1993). Feature Vector Similarity {8, 8}: Calculates similarities between vector representations. Entropy {2, 8}: Calculates the distributional similarity of test sentences to the training set over top N retrieved sentences (Bic¸ici et al., 2013). Length {6, 3}: Calculates the number of words and characters for S and T and their average token lengths and their ratios. Diversity {3, 3}: Measures the diversity of co-occurring features in the training set. Synthetic Translation Performance {3, 3}: Calculates translation scores achievable according to the n-gram coverage. Character n-grams {5}: Calculat"
W14-3339,J93-2004,0,0.0464769,"Missing"
W14-3339,W12-3102,0,0.141222,"Missing"
W14-3339,S14-2001,0,0.0421146,", June 26–27, 2014. 2014 Association for Computational Linguistics greater understanding of the acts of translation we ubiquitously use and how they can be used to predict the performance of translation and judging the semantic similarity between text. RTM and MTPP models are not data or language specific and their modeling power and good performance are applicable in different domains and tasks. 2013), top performance when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014), and a language independent solution and good performance when judging the semantic similarity of sentences (Agirre et al., 2014; Bic¸ici and Way, 2014). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the rea"
W14-3339,W13-2243,0,0.0378063,"Missing"
W14-3339,W02-1001,0,0.013786,"o select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic¸ici et al., 2013; Bic¸ici et al., 2014). We optimize the learning parameters by selecting ε close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for ε is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). We select the top 2 systems according to their performance on the training set. For Task 2, we use both Global Linear Models (GLM) (Collins, 2002) and GLM with dynamic learning (GLMd) we developed last year (Bic¸ici, 2013). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions. GLMd extends the GLM framework by parallel perceptron training (McDonald et al., 2010) and dynamic learning with adaptive weight updates in the perceptron learning algorithm: w = w + α (Φ(xi , yi ) − Φ(xi , ˆ y)) , 3.2 Training Results We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) to evaluate (Bic¸ici, 2013). DeltaAvg (Callison-Burch et al., 2012) calculates the"
W14-3339,2006.amta-papers.25,0,0.347327,"Penn Treebank version 3 (Marcus et al., 1993). 315 numB 24.0 2 1 1 1 1 3 depthB 9.0 1 1 13 1 1 3 CCL avg depthB 0.375 1 1 2 1 4 5 R/L 2.1429 1 avg R/L 3.401 1 8 2 10 1 1 7 15 Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP). Task Task 1.1 (en-es) Task 1.1 (es-en) Task 1.1 (en-de) Task 1.1 (de-en) Task 1.2 (en-es) Task 1.3 (en-es) Task 2 (en-es) Task 2 (es-en) Task 2 (en-de) Task 2 (de-en) word-level prediction (Task 2). Task 1.1 is about predicting post-editing effort (PEE), Task 1.2 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of translations, Task 1.3 is about predicting post-editing time (PET), and Task 2 is about binary, ternary, or multi-class classification of word-level quality. For each task, we develop individual RTM models using the parallel corpora and the LM corpora distributed by the translation task (WMT14) (Bojar et al., 2014) and the LM corpora provided by LDC for English (Parker et al., 2011) and Spanˆ ish (Angelo Mendonc¸a, 2011) 4 . The parallel corpora contain 4.5M sentences for de-en with 110M words for de and 116M words for en and 15.1M sentences for en-es with 412M words for en and 462M"
W14-3339,W09-0441,0,0.00839904,"e results on the test set are given in Table 5 where QuEst (Shah et al., 2013) SVR lists the baseline system results. Rank lists the overall ranking in the task out of about 10 submissions. We obtain the rankings by sorting according to the predicted scores and randomly assigning ranks in case of ties. RTMs with SVR PLS learning is able to achieve the top rank in this task. Task 1.2: Predicting HTER of Sentence Translations Task 1.2 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006), where case insensitive translation edit rate (TER) scores obtained by TERp (Snover et al., 2009) and their ranking. We derive features over sentences that are true-cased. The results on the test set are given in Table 6 where the ranks are out of about 11 submissions. We are also able to achieve the top ranking in this task. (1) where Φ returns a global representation for instance i and the weights are updated by α, which 317 Ranking Translations TREE en-es PLS-TREE QuEst SVR PLS-RR es-en FS-RR QuEst SVR TREE en-de PLS-TREE QuEst SVR RR de-en PLS-RR QuEst SVR Scoring Translations TREE en-es PLS-TREE QuEst SVR FS-RR es-en PLS-RR QuEst SVR TREE en-de PLS-TREE QuEst SVR RR de-en PLS-RR QuEs"
W14-3339,2009.eamt-1.5,0,0.160666,"different tasks. dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates. tremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic¸ici et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic¸ici et al., 2013; Bic¸ici et al., 2014). We optimize the learning parameters by selecting ε close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for ε is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). We select the"
W14-3339,P02-1040,0,\N,Missing
W14-3339,S14-2085,1,\N,Missing
W14-3339,N10-1069,0,\N,Missing
W14-3339,W14-3302,0,\N,Missing
W14-3339,W13-2201,0,\N,Missing
W14-4014,W08-0336,0,0.118081,"e NIST 2002 as the development set to tune weights, and NIST 2004 (MT04) and NIST 2005 (MT05) as the test data to evaluate the systems. Table 1 provides a summary of the Chinese–English corpus. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as the development set, while News-test 2012 (test12) and News-test 2013 (test13) are our test sets. Table 2 provides a summary of the German–English corpus. 5.2 MT05 33.91 33.79 5.3 Baseline Chinese–English In the Chinese–English translation task, the Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences into words. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into the projective dependency tree. For both language pairs, we filter sentence pairs longer than 80 words and keep the length ratio less than or equal to 3. English sentences are tokenized with scripts in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and (Koehn et al., 2003). We use SRILM (Stolcke, 2002) to 5 In this paper, the use of phrasal rules is similar to that of the HPB model, so they can be hand"
W14-4014,W09-2307,0,0.178172,"test data to evaluate the systems. Table 1 provides a summary of the Chinese–English corpus. The German–English training corpus is from WMT 2014, including Europarl V7 and News Commentary. News-test 2011 is taken as the development set, while News-test 2012 (test12) and News-test 2013 (test13) are our test sets. Table 2 provides a summary of the German–English corpus. 5.2 MT05 33.91 33.79 5.3 Baseline Chinese–English In the Chinese–English translation task, the Stanford Chinese word segmenter (Chang et al., 2008) is used to segment Chinese sentences into words. The Stanford dependency parser (Chang et al., 2009) parses a Chinese sentence into the projective dependency tree. For both language pairs, we filter sentence pairs longer than 80 words and keep the length ratio less than or equal to 3. English sentences are tokenized with scripts in Moses. Word alignment is performed by GIZA++ (Och and Ney, 2003) with the heuristic function grow-diag-final-and (Koehn et al., 2003). We use SRILM (Stolcke, 2002) to 5 In this paper, the use of phrasal rules is similar to that of the HPB model, so they can be handled by Moses directly. 128 Systems Moses HPB D2S +pseudo-forest +sub-structural rules +pseudo-forest"
W14-4014,P96-1041,0,0.223319,"Missing"
W14-4014,P99-1065,0,0.058104,"Missing"
W14-4014,W13-3710,0,0.0271875,"Missing"
W14-4014,W02-1039,0,0.189689,"Missing"
W14-4014,P03-1021,0,0.0691294,"Missing"
W14-4014,W06-3601,0,0.0254702,"hnology Chinese Academy of Sciences, Beijing, China {liangyouli,away,qliu}@computing.dcu.ie junxie@ict.ac.cn † Abstract semantic information and has the best inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be helpful to translation. In recent years, dependency structure has been widely used in SMT. For example, Shen et al. (2010) present a string-to-dependency model by using the dependency fragments of the neighbouring words on the target side, which makes it easier to integrate a dependency language model. However such string-to-tree systems run slowly in cubic time (Huang et al., 2006). Another example is the treelet approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words."
W14-4014,J03-1002,0,0.0230289,"Missing"
W14-4014,P02-1040,0,0.0894681,"el before (XJ) and after (D2S) dependency tree being transformed. Systems are trained on a selected 1.2M Chinese–English corpus. Table 1: Chinese–English corpus. For the English dev and test sets, words counts are averaged across 4 references. corpus train dev test12 test13 sentences 2,037,209 3,003 3,003 3,000 words(de) 52,671,991 72,661 72,603 63,412 train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (Och, 2003) is used to tune weights. Caseinsensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement the baseline Dep2Str model in Moses with methods described in this paper, which is denoted as D2S. The first experiment we do is to sanity check our implementation. Thus we take a separate system (denoted as XJ) for comparison which implements the Dep2Str model based on (Xie et al., 2011). As shown in Table 3, using the transformation of dependency trees, the Dep2Str model implemented in Moses (D2S) is comparable with the stan"
W14-4014,P05-1034,0,0.182818,"u.ie junxie@ict.ac.cn † Abstract semantic information and has the best inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be helpful to translation. In recent years, dependency structure has been widely used in SMT. For example, Shen et al. (2010) present a string-to-dependency model by using the dependency fragments of the neighbouring words on the target side, which makes it easier to integrate a dependency language model. However such string-to-tree systems run slowly in cubic time (Huang et al., 2006). Another example is the treelet approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependency-to-string (Dep2Str)"
W14-4014,W04-3250,0,0.0311272,"selected 1.2M Chinese–English corpus. Table 1: Chinese–English corpus. For the English dev and test sets, words counts are averaged across 4 references. corpus train dev test12 test13 sentences 2,037,209 3,003 3,003 3,000 words(de) 52,671,991 72,661 72,603 63,412 train a 5-gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser-Ney discounting (Chen and Goodman, 1996). Minimum Error Rate Training (Och, 2003) is used to tune weights. Caseinsensitive BLEU (Papineni et al., 2002) is used to evaluate the translation results. Bootstrap resampling (Koehn, 2004) is also performed to compute statistical significance with 1000 iterations. We implement the baseline Dep2Str model in Moses with methods described in this paper, which is denoted as D2S. The first experiment we do is to sanity check our implementation. Thus we take a separate system (denoted as XJ) for comparison which implements the Dep2Str model based on (Xie et al., 2011). As shown in Table 3, using the transformation of dependency trees, the Dep2Str model implemented in Moses (D2S) is comparable with the standard implementation (XJ). In the rest of this section, we describe experiments w"
W14-4014,H01-1014,0,0.061167,"= {2 → 2} where “H1” denotes the position of the head word is 1, “R1” indicates the first right dependent of the head word, “X” is the general label for the target side and φ is the set of alignments (the index-correspondences between s and t). The format has been described in detail at http://www.statmt.org/ moses/?n=Moses.SyntaxTutorial. Guohui For the internal node “ 国会” in the HD fragment Guohui Xuanju “(国会) 选举”, we create two constituent nodes 125 In addition, our transformation is different from other works which transform a dependency tree into a constituent tree (Collins et al., 1999; Xia and Palmer, 2001). In this paper, the produced constituent tree still preserves dependency relations between words, and the phrasal structure is directly derived from the dependency structure without refinement. Accordingly, the constituent tree may not be a linguistically well-formed syntactic structure. However, it is not a problem for our model, because in this paper what matters is the dependency structure which has already been encoded into the (ill-formed) constituent tree. 4 smart/JJ She/PRP is/VBZ very/RB smart/JJ + She/PRP is/VBZ very/RB Figure 4: An example of decomposition on a headdependent fragmen"
W14-4014,N03-1017,0,0.0792329,"Missing"
W14-4014,D11-1020,1,0.942012,"let approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependency-to-string (Dep2Str) model, which extracts head-dependent (HD) rules from word-aligned source dependency trees and target strings. As this model specifies reordering information in the HD rules, during translation only the substitution operation is needed, because words are reordered simultaneously with the rule being applied. Meng et al. (2013) and Xie et al. (2014) extend the model by augmenting HD rules with the help of either constituent tree or fixed/float structure (Shen et al., 2010). Augmented rules are created by the combination of two or more nodes in Dependency"
W14-4014,2005.mtsummit-ebmt.13,0,0.78139,"i,away,qliu}@computing.dcu.ie junxie@ict.ac.cn † Abstract semantic information and has the best inter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be helpful to translation. In recent years, dependency structure has been widely used in SMT. For example, Shen et al. (2010) present a string-to-dependency model by using the dependency fragments of the neighbouring words on the target side, which makes it easier to integrate a dependency language model. However such string-to-tree systems run slowly in cubic time (Huang et al., 2006). Another example is the treelet approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependenc"
W14-4014,C14-1209,1,0.89908,"lets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependency-to-string (Dep2Str) model, which extracts head-dependent (HD) rules from word-aligned source dependency trees and target strings. As this model specifies reordering information in the HD rules, during translation only the substitution operation is needed, because words are reordered simultaneously with the rule being applied. Meng et al. (2013) and Xie et al. (2014) extend the model by augmenting HD rules with the help of either constituent tree or fixed/float structure (Shen et al., 2010). Augmented rules are created by the combination of two or more nodes in Dependency structure provides grammatical relations between words, which have shown to be effective in Statistical Machine Translation (SMT). In this paper, we present an open source module in Moses which implements a dependency-to-string model. We propose a method to transform the input dependency tree into a corresponding constituent tree for reusing the tree-based decoder in Moses. In our experi"
W14-4014,D13-1108,1,0.913216,"Missing"
W14-4014,W07-0706,1,0.884483,"ter-lingual phrasal cohesion properties (Fox, 2002), it is believed to be helpful to translation. In recent years, dependency structure has been widely used in SMT. For example, Shen et al. (2010) present a string-to-dependency model by using the dependency fragments of the neighbouring words on the target side, which makes it easier to integrate a dependency language model. However such string-to-tree systems run slowly in cubic time (Huang et al., 2006). Another example is the treelet approach (Menezes and Quirk, 2005; Quirk et al., 2005), which uses dependency structure on the source side. Xiong et al. (2007) extend the treelet approach to allow dependency fragments with gaps. As the treelet is defined as an arbitrary connected sub-graph, typically both substitution and insertion operations are adopted for decoding. However, as translation rules based on the treelets do not encode enough reordering information directly, another heuristic or separate reordering model is usually needed to decide the best target position of the inserted words. Different from these works, Xie et al. (2011) present a dependency-to-string (Dep2Str) model, which extracts head-dependent (HD) rules from word-aligned source"
W14-4014,P08-1023,1,0.907825,"Missing"
W14-4014,P05-1013,0,0.453403,"Missing"
W14-4014,J10-4005,0,\N,Missing
W14-4014,P07-2045,0,\N,Missing
W14-4806,C94-2167,0,0.424799,"Missing"
W14-4806,W95-0110,0,0.603914,"Missing"
W14-4806,C94-1084,0,0.345379,"Missing"
W14-4806,P98-1074,0,0.1073,"algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, rather than on obta"
W14-4806,ha-etal-2008-mutual,0,0.0185747,"et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, rather than on obtaining a bilingual"
W14-4806,Y06-1020,0,0.0327178,"h can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in p"
W14-4806,2005.mtsummit-papers.11,0,0.0439874,"Missing"
W14-4806,P07-2045,0,0.0069973,"ulti-word terms up to 3-grams, whereas they focused solely on extracting single word terms. 3.2 Creating a Bilingual Termbank We obtained source and target termlists from the bilingual domain corpus using the approach described in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the extracted source and target termlists. This section provides a mathematical derivation of the PB-SMT model to show how we scored candidate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability P(eI1 |f1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (5): log P(eI1 |f1J ) = M X m=1 I λm hm (f1J , eI1 , sK 1 ) + λLM log P(e1 ) (5) where eI1 = e1 , ..., eI is the probable candidate translation for the given input sentence f1J = f1 , ..., fJ and sK 1 = s1 , ..., sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (fˆ1 , ..., fˆk ) and (ˆ e1 , ..., eˆk ) such that (we set i0 := 0):"
W14-4806,N03-1017,0,0.217859,"ue (Ananiadou et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which is composed of two consecutive and independent processes: 1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences) independently from the source and target sides of a parallel corpus, 2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003). We then evaluate our novel bilingual terminology extraction model on various domain corpora considering English-to-Spanish and low-resourced and less-explored English-to-Hindi language-pairs and see excellent performance for all data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 42 Proceedings of the 4th International Workshop on Computational Terminology, pages 42–51, Dublin, Ireland, August 23 2014. The remainder of the"
W14-4806,P93-1003,0,0.71563,"resources using human experts are, however, expensive and time-consuming tasks. In contrast, automatic terminology extraction is much faster and less expensive, but cannot be guaranteed to be error-free. Accordingly, in real NLP applications, a manual inspection is required to amend or discard anomalous items from an automatically extracted terminology list. The automatic terminology extraction task starts with selecting candidate terms from the input domain corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word sequences are regarded as candidate terms (Deane, 2005). Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which is composed of two consecutive and independent processes: 1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences) independently from"
W14-4806,E09-1057,0,0.0194097,"ex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, rather than on obtaining a bilingual term list. In contrast, we"
W14-4806,P03-1021,0,0.0218959,"et i0 := 0): ∀k ∈ [1, K] sk := (ik ; bk , jk ), (bk corresponds to starting index of fk ) eˆk := eˆik−1 +1 , ..., eˆik , fˆk := fˆbk , ..., fˆjk Each feature hm in (5) can be rewritten as in (6): hm (f1J , eI1 , sK 1 )= K X ˆ m (fˆk , eˆk , sk ) h (6) k=1 Therefore, the translational features in (5) can be rewritten as in (7): M X m=1 λm hm (f1J , eI1 , sK 1 )= M X m=1 λm K X ˆ m (fˆk , eˆk , sk ) h (7) k=1 ˆ m is a feature defined on phrase-pairs (fˆk , eˆk ), and λm is the feature weight of h ˆ m. In equation (7), h These weights (λm ) are optimized using minimum error-rate training (MERT) (Och, 2003) on a held-out 500 sentence-pair development set for each of the experiments. We create a list of probable source–target term-pairs by taking each source and target term from the source and target termlists, respectively, provided that those source–target term-pairs are present in the PB-SMT phrase-table. We calculate a weight (w) for each source–target term-pair (essentially, a phrasepair, i.e. (ˆ ek , fˆk )) using (8):2 M X ˆ ˆ m (fˆk , eˆk ) w(ˆ ek , fk ) = λm h (8) m=1 2 Equation (8) is derived from the right-hand side of equation (7) for a single source–target phrase-pair. 44 ˆ m ), namel"
W14-4806,P02-1038,0,0.14282,"ed source and target termlists from the bilingual domain corpus using the approach described in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the extracted source and target termlists. This section provides a mathematical derivation of the PB-SMT model to show how we scored candidate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability P(eI1 |f1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (5): log P(eI1 |f1J ) = M X m=1 I λm hm (f1J , eI1 , sK 1 ) + λLM log P(e1 ) (5) where eI1 = e1 , ..., eI is the probable candidate translation for the given input sentence f1J = f1 , ..., fJ and sK 1 = s1 , ..., sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (fˆ1 , ..., fˆk ) and (ˆ e1 , ..., eˆk ) such that (we set i0 := 0): ∀k ∈ [1, K] sk := (ik ; bk , jk ), (bk corresponds to starting index of fk ) eˆk := eˆik−1 +1 , ..., eˆik , fˆk := fˆbk , ..., fˆjk Ea"
W14-4806,bojar-etal-2014-hindencorp,0,0.0605815,"Missing"
W14-4806,W00-0901,0,0.901235,"ous items from an automatically extracted terminology list. The automatic terminology extraction task starts with selecting candidate terms from the input domain corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word sequences are regarded as candidate terms (Deane, 2005). Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which is composed of two consecutive and independent processes: 1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences) independently from the source and target sides of a parallel corpus, 2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003). We then evaluate our novel bilingual terminology extraction model on various domain corpora c"
W14-4806,zhang-etal-2008-comparative,0,0.0451364,"Missing"
W14-4806,P05-1075,0,\N,Missing
W14-4806,C98-1071,0,\N,Missing
W15-0714,C88-1016,0,0.18011,"this paper we explore the feasibility of applying the current state-of-the-art MT technology to literary texts, what might be considered to be the last bastion of human translation. The perceived wisdom is that MT is of no use for the translation of literature. We challenge that view, despite the fact that – to the best of our knowledge – the applicability of MT to literature has to date been only partially studied from an empirical point of view. Introduction The field of Machine Translation (MT) has evolved very rapidly since the emergence of statistical approaches almost three decades ago (Brown et al., 1988; Brown et al., 1990). MT is nowadays a growing reality throughout the industry, which continues to adopt this technology as it results in demonstrable improvements in translation productivity, at least In this paper we aim to measure the translatability of literary text. Our empirical methodology relies on the fact that the applicability of MT to a given type of text can be assessed by analysing parallel corpora of that particular type and measuring (i) the degree of freedom of the translations (how literal the translations are), and (ii) the narrowness of the domain (how specific or general"
W15-0714,J90-2002,0,0.813076,"e the feasibility of applying the current state-of-the-art MT technology to literary texts, what might be considered to be the last bastion of human translation. The perceived wisdom is that MT is of no use for the translation of literature. We challenge that view, despite the fact that – to the best of our knowledge – the applicability of MT to literature has to date been only partially studied from an empirical point of view. Introduction The field of Machine Translation (MT) has evolved very rapidly since the emergence of statistical approaches almost three decades ago (Brown et al., 1988; Brown et al., 1990). MT is nowadays a growing reality throughout the industry, which continues to adopt this technology as it results in demonstrable improvements in translation productivity, at least In this paper we aim to measure the translatability of literary text. Our empirical methodology relies on the fact that the applicability of MT to a given type of text can be assessed by analysing parallel corpora of that particular type and measuring (i) the degree of freedom of the translations (how literal the translations are), and (ii) the narrowness of the domain (how specific or general that text is). Hence,"
W15-0714,P96-1041,0,0.179931,"apertium/files/apertium-es-ca/1.2.1/ 10 http://sourceforge.net/projects/ apertium/files/apertium-en-es/0.8.0/ 11 Manual evaluation for English, French and Greek concluded that 0.4 was an adequate threshold for Hunalign’s confidence score (Pecina et al., 2012). Narrowness of the Domain As previously mentioned, we use LM perplexity as a proxy to measure the narrowness of the domain. We take two random samples without replacement for the Spanish side of each dataset, to be used for training (200,000 tokens) and testing (20,000 tokens). We train an LM of order 3 and improved Kneser-Ney smoothing (Chen and Goodman, 1996) with IRSTLM (Federico et al., 2008). For each LM we report the perplexity on the testset built from the same dataset in Figure 1. The two novels considered (perplexities in the range [230.61, 254.49]) fall somewhere between news ([359.73, 560.62]) and technical domain ([127.30, 228.38]). Our intuition is that novels cover a narrow domain, like technical texts, but the vocabulary and language used in novels is richer, thus leading to higher perplexity than technical texts. News, on the contrary, covers a large variety of topics. Hence, despite novels possibly using more complex linguistic cons"
W15-0714,W14-3348,0,0.132007,"Missing"
W15-0714,D10-1016,0,0.650978,"Missing"
W15-0714,E14-1047,0,0.0425261,"Missing"
W15-0714,D10-1051,0,0.583547,"Missing"
W15-0714,C08-1048,0,0.0175521,"d the author concludes that such a workflow can be a useful low-cost alternative for translating literary works, albeit at the expense of sacrificing translation quality. According to the opinion of a professional translator, the main errors had to do with using English syntactic structures and expressions instead of their French equivalents and not taking into account certain cultural references. Finally, there are some works that use MT techniques in literary text, but for generation rather than for translation. He et al. (2012) used SMT to generate poems in Chinese given a set of keywords. Jiang and Zhou (2008) used SMT to generate the second line of Chinese couplets given the first line. In a similar fashion, Wu et al. (2013) used transduction grammars to generate rhyming responses in hip-hop given the original challenges. This paper contributes to the current state-of-theart in two dimensions. On the one hand, we conduct a comparative analysis on the translatability of literary text according to narrowness of the domain and freedom of translation. This can be seen as a more general and complementary analysis to the one conducted by Voigt and Jurafsky (2012). On the other hand, and related to Besac"
W15-0714,W04-3250,0,0.286519,"Missing"
W15-0714,2005.mtsummit-papers.11,0,0.053497,"to as news1) for Spanish–Catalan, and newscommentary v84 (referred to as news2) for Spanish– English. For technical documentation we use four datasets: DOGC,5 a corpus from the official journal of the Catalan Goverment, for Spanish–Catalan; EMEA,6 a corpus from the European Medicines Agency, for Spanish–English; JRC-Acquis (henceforth referred as JRC) (Steinberger et al., 2006), made of legislative text of the European Union, for Spanish– English; and KDE4,7 a corpus of localisation files of the KDE desktop environment, for the two language pairs. Finally, we consider the Europarl corpus v7 (Koehn, 2005), given it is widely used in the MT community, for Spanish–English. All the datasets are pre-processed as follows. First they are tokenised and truecased with Moses’ (Koehn et al., 2007) scripts. Truecasing is carried out with a model trained on the caWaC corpus for Catalan (Ljubeˇsi´c and Toral, 2014) and News Crawl 20128 both for English and Spanish. Parallel datasets not available in a sentence-split format (novel1 and novel2) are sentence-split using Freeling (Padr´o and Stanilovsky, 2012). All parallel datasets are then sentence aligned. We use Hunalign (Varga et al., 2005) and keep only"
W15-0714,ljubesic-toral-2014-cawac,1,0.882326,"Missing"
W15-0714,J03-1002,0,0.00673519,"Equation 1, as a proxy to measure the degree of translation freedom. Word alignment perplexity gives an indication of how well the model fits the data. log2 P P = − X s log2 p(es |fs ) (1) The assumption is that the freer the translations are for a given parallel corpus, the higher the perplexity of the word alignment model learnt from such dataset, as the word alignment algorithms would have more difficulty to find suitable alignments. For each parallel dataset, we randomly select a set of sentence pairs whose overall size accounts for 500,000 tokens. We then run word alignment with GIZA++ (Och and Ney, 2003) in both directions, with the default parameters used in Moses. For each dataset and language pair, we report in Figure 2 the perplexity of the word alignment after the last iteration for each direction. The most important discriminating variable appears to be the level of relatedness of the languages involved, i.e. all the perplexities for Spanish–Catalan are below 10 while all the perplexities for Spanish–English are well above this number. 50 45 40 Perplexity 35 30 es-ca ca-es es-en en-es 25 20 15 10 5 0 novel1 novel2 news1 news2 DOGC Acquis KDE4 EMEA ep Dataset Figure 2: Word alignment per"
W15-0714,P03-1021,0,0.0109664,"domain-adapted systems. The domain adaptation is carried out by using two previous novels from the same author that were translated by the same translator (cf. the dataset novel1 in Section 3.1). We explore their use for tuning (+inDev), LM (concatenated +inLM and interpolated +IinLM) and TM (concatenated +inTM and interpolated +IinTM). The testset is made of a set of randomly selected sentence pairs from The Prisoner of Heaven. Table 1 provides an overview of the datasets used for MT. We train phrase-based SMT systems with Moses v2.1 using default parameters. Tuning is carried out with MERT (Och, 2003). LMs are linearly interpolated with SRILM (Stolcke et al., 2011) by means of perplexity minimisation on the development set from the novel1 dataset. Similarly, TMs are linearly interpolated, also by means of perplexity minimisation (Sennrich, 2012). 4.1 Automatic Evaluation Our systems are evaluated with a set of state-ofthe-art automatic metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR 1.5 (Denkowski and Lavie, 2014). Table 2 shows the results obtained by each of the systems built. For each domain-adapted system System baseline +inDev +inDev+inLM +inDev+IinLM +inDe"
W15-0714,padro-stanilovsky-2012-freeling,0,0.0885312,"Missing"
W15-0714,P02-1040,0,0.0933569,"Missing"
W15-0714,2012.eamt-1.38,1,0.904327,"Missing"
W15-0714,2014.eamt-1.39,0,0.0137405,"predictable domains such as news (cf. WMT translation task series).2 We propose to study the applicability of SMT to literary text by comparing the degree of freedom and narrowness of parallel corpora for literature to other domains widely studied in the area of MT (technical documentation and news). Such a corpus study can be carried out by using a set of automatic measures. The perplexity of the word alignment can be used as a proxy to measure the degree of freedom of the translation. The narrowness of the domain can be assessed by measuring perplexity with respect to a language model (LM) (Ruiz and Federico, 2014). Therefore, in order to assess the translatability of literary text with MT, we contextualise the problem by comparing it to the translatability of other widely studied types of text. Instead of considering the 2 http://www.statmt.org/wmt14/ translation-task.html translatability of literature as a whole, we root the study along two axes: • Relatedness of the language pair: from pairs of languages that belong to the same family (e.g. Romance languages), through languages that belong to the same group (e.g. Romance and Germanic languages of the Indo-European group) to unrelated languages (e.g."
W15-0714,E12-1055,0,0.042822,"Missing"
W15-0714,2006.amta-papers.25,0,0.213719,"Missing"
W15-0714,W12-2503,0,0.293951,"real terms. Their system was trained and evaluated with WMT-09 data1 for French–English. Greene et al. (2010) also translated poetry, choosing target realisations that conform to the desired rhythmic patterns. Specifically, they translated Dante’s Divine Comedy from Italian sonnets into English iambic pentameter. Instead of constraining the SMT system, they passed its output lattice through a FST that maps words to sequences of stressed and unstressed syllables. These sequences are finally filtered with a iambic pentameter acceptor. Their output translations are evaluated qualitatively only. Voigt and Jurafsky (2012) examined how referential cohesion is expressed in literary and nonliterary texts, and how this cohesion affects trans1 http://www.statmt.org/wmt09/ translation-task.html 124 lation. They found that literary texts have more dense reference chains and conclude that incorporating discourse features beyond the level of the sentence is an important direction for applying MT to literary texts. Jones and Irvine (2013) used existing MT systems to translate samples of French literature (prose and poetry) into English. They then used qualitative analysis grounded in translation theory on the MT output"
W15-0714,2013.mtsummit-papers.14,0,0.0183302,"the expense of sacrificing translation quality. According to the opinion of a professional translator, the main errors had to do with using English syntactic structures and expressions instead of their French equivalents and not taking into account certain cultural references. Finally, there are some works that use MT techniques in literary text, but for generation rather than for translation. He et al. (2012) used SMT to generate poems in Chinese given a set of keywords. Jiang and Zhou (2008) used SMT to generate the second line of Chinese couplets given the first line. In a similar fashion, Wu et al. (2013) used transduction grammars to generate rhyming responses in hip-hop given the original challenges. This paper contributes to the current state-of-theart in two dimensions. On the one hand, we conduct a comparative analysis on the translatability of literary text according to narrowness of the domain and freedom of translation. This can be seen as a more general and complementary analysis to the one conducted by Voigt and Jurafsky (2012). On the other hand, and related to Besacier (2014), we evaluate MT output for literary text. There are two differences though; first, they translated a short"
W15-0714,2012.amta-wptp.10,0,0.0567945,"Missing"
W15-0714,steinberger-etal-2006-jrc,0,\N,Missing
W15-0714,P07-2045,0,\N,Missing
W15-3005,W14-3303,1,0.877323,"Missing"
W15-3005,W13-2206,1,0.885723,"Missing"
W15-3005,P07-2045,0,0.0118755,"eployment of Accurate Statistical Machine Translation Systems, Benchmarks, and Statistics Ergun Bic¸ici Qun Liu Andy Way ADAPT Research Center ADAPT Research Center ADAPT Research Center School of Computing School of Computing School of Computing Dublin City University, Ireland Dublin City University, Ireland Dublin City University, Ireland ergun.bicici@computing.dcu.ie qliu@computing.dcu.ie away@computing.dcu.ie Abstract randomized subsets of the training data and combines the selections afterwards. FDA5 is available at http://github.com/bicici/FDA. We run ParFDA SMT experiments using Moses (Koehn et al., 2007) in all language pairs in WMT15 (Bojar et al., 2015) and obtain SMT performance close to the top constrained Moses systems. ParFDA allows rapid prototyping of SMT systems for a given target domain or task. We use ParFDA for selecting parallel training data and LM data for building SMT systems. We select the LM training data with ParFDA based on the following observation (Bic¸ici, 2013): We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and ob"
W15-3005,J03-1002,0,0.0147758,"2013; Bic¸ici et al., 2014) runs separate FDA5 (Bic¸ici and Yuret, 2015) models on 2 Results We run ParFDA SMT experiments for all language pairs in both directions in the WMT15 translation task (Bojar et al., 2015), which include English-Czech (en-cs), English-German (en-de), English-Finnish (en-fi), English-French (en-fr), and English-Russian (en-ru). We truecase all of the corpora, set the maximum sentence length to 126, use 150-best lists during tuning, set the LM order to a value in [7, 10] for all language pairs, and train the LM using SRILM (Stolcke, 2002) with -unk option. For GIZA++ (Och and Ney, 2003), max-fertility is set to 10, with the number of iterations set to 7,3,5,5,7 for IBM models 1,2,3,4, and the HMM model, and 70 word 74 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 74–78, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. S→T en-cs en-cs cs-en cs-en en-de en-de de-en de-en en-fi en-fi fi-en fi-en en-fr en-fr fr-en fr-en en-ru en-ru ru-en ru-en Data C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA C ParFDA #word S (M) 253.8 49.0 224.1 42.0 116.3 37.6 109.8 33.3 52.8 37.2 37.9 25"
W15-3005,P09-2087,1,0.85904,"Missing"
W15-3005,W15-3001,0,\N,Missing
W15-3035,W15-3001,0,0.153862,"ed learning models. We develop RTM models for each WMT15 QET (QET15) subtask and obtain improvements over QET14 results. RTMs achieve top performance in QET15 ranking 1st in document- and sentence-level prediction tasks and 2nd in word-level prediction task. 1 • with extended learning models including bayesian ridge regression (Tan et al., 2015), which did not obtain better performance than support vector regression in training results (Section 2.2). We present top results with Referential Translation Machines (Bic¸ici, 2015; Bic¸ici and Way, 2014) at quality estimation task (QET15) in WMT15 (Bojar et al., 2015). RTMs pioneer a computational model for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bic¸ici and Yuret, 2015) as interpretants for reaching shared semantics. RTMs use Machine Translation Performance Prediction (MTPP) System (Bic¸ici et al., 2013; Bic¸ici, 2015), which is a state-of-the-art performance predictor of translation even without using the translation by using only the source. We use ParFDA for selecting the interpretants (Bic¸ici et al., 2015; Bic¸ici and Yuret, 2015) and build an MTPP model. MTPP derives"
W15-3035,2009.eamt-1.5,0,0.0365708,"for Computational Linguistics. 2.1 We present results using support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004) for sentence and document translation prediction tasks and Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) (Bic¸ici, 2013; Bic¸ici and Way, 2014) for word-level translation performance prediction. We also use these learning models after a feature subset selection (FS) with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), or PLS after FS (FS+PLS). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions. GLMd extends the GLM framework by parallel perceptron training (McDonald et al., 2010) and dynamic learning with adaptive weight updates in the perceptron learning algorithm: Figure 1: RTM depiction. Task Task 1 (en-es) Task 2 (en-es) Task 3 (en-de) Task 3 (de-en) Train 12271 12271 800 800 Test 1817 1817 415 415 Table 1: Number of sentences in different tasks. 2 w = w + α (Φ(xi , yi ) − Φ(xi , ˆy)) , (1) where Φ returns a global representation for instance i and the weights are up"
W15-3035,W12-3102,0,0.0987149,"Missing"
W15-3035,W02-1001,0,0.0614078,"ed ParFDA instance selection model (Bic¸ici et al., 2015) allowing better language models (LM) in which similarity judgments are made to be built with improved optimization and selection of the LM data, 304 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 304–308, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. 2.1 We present results using support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Sch¨olkopf, 2004) for sentence and document translation prediction tasks and Global Linear Models (GLM) (Collins, 2002) with dynamic learning (GLMd) (Bic¸ici, 2013; Bic¸ici and Way, 2014) for word-level translation performance prediction. We also use these learning models after a feature subset selection (FS) with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), or PLS after FS (FS+PLS). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions. GLMd extends the GLM framework by parallel perceptron training (McDonald et al., 2010) and dynamic learning with adaptive weight"
W15-3035,S15-2015,0,0.052246,"Missing"
W15-3035,P07-2045,0,0.00372441,"error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known (Bic¸ici, 2015). RTM test performance on various tasks sorted according to MRAER can help identify which tasks and subtasks may require more work. DeltaAvg (Callison-Burch et al., Instance selection for the training set and the language model (LM) corpus is handled by ParFDA (Bic¸ici et al., 2015), whose parameters are optimized for each translation task. LM are trained using SRILM (Stolcke, 2002). We tokenize and truecase all of the corpora using code released with Moses (Koehn et al., 2007) 1 . Table 1 lists the number of sentences in the training and test sets for each task. 1 RTM Prediction Models and Optimization mosesdecoder/scripts/ 305 Task Task1 Task3 Translation en-es en-es en-de en-de de-en de-en Model FS SVR FS+PLS SVR FS SVR SVR FS SVR FS+PLS SVR r 0.355 0.362 0.517 0.503 0.479 0.391 MAE 0.1387 0.1389 0.0737 0.0765 0.0473 0.0515 RAE 0.895 0.896 0.734 0.761 0.738 0.804 MAER 0.782 0.784 0.289 0.307 0.267 0.288 MRAER 0.821 0.824 0.678 0.737 0.665 0.81 Table 2: Training performance of the top 2 individual RTM models prepared for different tasks. 2.0 1.8 learning rate 1.6"
W15-3035,W07-0734,0,0.47327,"Estimation Task We participate in all of the three subtasks of the quality estimation task (QET) (Bojar et al., 2015), which include English to Spanish (en-es), English to German (en-de), and German to English (deen) translation directions. There are three subtasks: sentence-level prediction (Task 1), wordlevel prediction (Task 2), and document-level prediction (Task 3). Task 1 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of sentence translations, Task 2 is about binary classification of word-level quality, and Task 3 is about predicting METEOR (Lavie and Agarwal, 2007) scores of document translations. f (x) = (loga b − 1)x2 + 1 (2) Learning rate curve for a = 0.5 and b = 1.0 is provided in Figure 2: 2.2 Training Results We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) as well as relative MAE (MAER) and relative RAE (MRAER) to evaluate (Bic¸ici, 2015; Bic¸ici, 2013). MAER is mean absolute error relative to the magnitude of the target and MRAER is mean absolute error relative to the absolute error of a predictor always predicting the target mean assuming that target mean is known (Bic¸ici, 20"
W15-3035,2006.amta-papers.25,0,0.0613992,"learning rate updates the weight values with weights in the range [a, b] using the following function taking error rate as the input: RTM in the Quality Estimation Task We participate in all of the three subtasks of the quality estimation task (QET) (Bojar et al., 2015), which include English to Spanish (en-es), English to German (en-de), and German to English (deen) translation directions. There are three subtasks: sentence-level prediction (Task 1), wordlevel prediction (Task 2), and document-level prediction (Task 3). Task 1 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of sentence translations, Task 2 is about binary classification of word-level quality, and Task 3 is about predicting METEOR (Lavie and Agarwal, 2007) scores of document translations. f (x) = (loga b − 1)x2 + 1 (2) Learning rate curve for a = 0.5 and b = 1.0 is provided in Figure 2: 2.2 Training Results We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) as well as relative MAE (MAER) and relative RAE (MRAER) to evaluate (Bic¸ici, 2015; Bic¸ici, 2013). MAER is mean absolute error relative to the magnitude of the target an"
W15-3035,S15-2010,1,\N,Missing
W15-3035,W13-2242,1,\N,Missing
W15-3035,N10-1069,0,\N,Missing
W15-3035,W14-3339,1,\N,Missing
W15-4906,P11-1103,0,0.0204333,"a large-scale Engish-to-Farsi translation task. The rest of this paper is organised as follows. Section 2 reviews the related work and contextualises our work. Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi. Section 4 presents our reordering model, which is then evaluated in Section 5. Finally, Section 6 concludes the paper and outlines avenues of future work. 2 Related Work Phrase-based systems can perform local (short distance) reordering inside the phrases but they are inherently weak at non-local (medium and long distance) reordering (Birch and Osborne, 2011). Previous work to address reordering in PB-SMT can generally be categorised into two groups. Approaches in the ﬁrst group perform reordering in a pre-processing step (i.e. before decoding) by applying some reordering rules to the source sentences to make them in order more similar to that of the target language (Xia and McCord, 2004; Collins et al., 2005; Genzel, 2010). Although all these approaches have reported improvements, there is a fundamental problem with separating the reordering task into a pre-processing component as every faulty decision in the pre-processing step will be passed al"
W15-4906,W09-0434,0,0.0219166,"re long- and even medium-distance reorderings, since they try to ﬁnd suitable reorderings only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this lin"
W15-4906,D14-1082,0,0.0446672,"Missing"
W15-4906,P05-1033,0,0.19675,"f lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to ﬁnd suitable reorderings only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our w"
W15-4906,P05-1066,0,0.347083,"Missing"
W15-4906,W14-3348,0,0.0422701,"irs and features examined (cf. Table 6) and two additional systems that model the reordering for both types of constituent pairs (rows all) with (ws) and without (wos) surface forms. We compare our systems to two baselines, a standard HPB-SMT system (HPB) and a HPBSMT system with added swap glue grammar rule (HPB sgg) as in Equation 4. The swap glue rule allows adjacent phrases to be reversed. X → (X1 X2 , X2 X1 ) (4) Table 7 shows the results obtained by each of the MT systems according to four automatic evaluation metrics: BLEU, NIST (Doddington, 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014). For each system and evaluation metric we show its relative improvement over the baseline HPB (columns diff). The scores obtained by systems that implement our novel reordering between pairs of dependents (columns dd) are better than those of the baseline, both with (ws) and without (wos) surface forms, accross all the four evaluation metrics. The same is true for models that implement reordering between both pairs of constituent types (columns all), except for the system all wos according to BLEU. The results for systems that perform reordering between pairs of head and dependent offer a mix"
W15-4906,D08-1089,0,0.0302952,"Despite the satisfactory performance of lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to ﬁnd suitable reorderings only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanne"
W15-4906,D11-1079,0,0.16187,"rs from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the ﬁrst paper in this line of work to be applied to a language pair ot"
W15-4906,C10-1043,0,0.0213204,"d outlines avenues of future work. 2 Related Work Phrase-based systems can perform local (short distance) reordering inside the phrases but they are inherently weak at non-local (medium and long distance) reordering (Birch and Osborne, 2011). Previous work to address reordering in PB-SMT can generally be categorised into two groups. Approaches in the ﬁrst group perform reordering in a pre-processing step (i.e. before decoding) by applying some reordering rules to the source sentences to make them in order more similar to that of the target language (Xia and McCord, 2004; Collins et al., 2005; Genzel, 2010). Although all these approaches have reported improvements, there is a fundamental problem with separating the reordering task into a pre-processing component as every faulty decision in the pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering mo"
W15-4906,J03-1002,0,0.0143311,"g. Table 4 presents the details about this dataset. We parsed the source side (English) of the corpus using the Stanford dependency parser (Chen 1 tribes wandered http://dadegan.ir/catalog/mizan 47 wife wandered unit English Farsi sentences 1,016,758 1,016,758 Train words 13,919,071 14,043,499 sentences 3,000 3,000 Tune words 40,831 41,670 sentences 1,000 1,000 Test words 13,165 13,444 Table 4: Mizan parallel corpus statistics and Manning, 2014) and used the “collapsed representation” of the parser output to obtain direct dependencies between the words in the source sentences. We used GIZA++ (Och and Ney, 2003) to align the words in the corpus. Then we extracted 6,391,255 head−dependent pairs and 5,247,137 dependent−dependent pairs from train dataset and determined the orientation for each pair based on Equation 1. In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classiﬁer with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classiﬁer separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of H"
W15-4906,P03-1021,0,0.00762563,"on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classiﬁer with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classiﬁer separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of HPM model with default settings (Hoang et al., 2009). We used a 5-gram target language model trained on the Farsi side of the training data. In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT (Och, 2003). 5.2 Impact of different features Since the proposed reordering model has to classify the head−dependent and dependent−dependent pairs into their correct monotone or swap orientation classes, its task can be seen as a binary classiﬁcation task. We used the Naive Bayes algorithm to build such an orientation classiﬁer. We then used different feature sets in each classiﬁcation experiment to determine their impact on the accuracy of the model. The features that were examined in this paper are shown in Table 5. All of these features are entirely based on the source sentence and source dependency p"
W15-4906,P05-1034,0,0.185607,"lement pairs presented by Dryer (1992). Dryer has shown that these pairs can be used to distinguish SOV and SVO languages. 4 Dependency-based Reordering Model Our reordering model is based on the source dependency tree, an example of which is shown in Figure 1. The dependency tree of a sentence shows the grammatical relations between the head and dependent words of that sentence. For example in Figure 1, the arrow from “he” to “bought” with label “nsubj”, expresses that the 45 dependent word “he” is the subject of the head word “bought”. Under the assumption that constituents move as a whole (Quirk et al., 2005), our proposed reordering model aims to predict the orientation of each dependent word with respect to its head (head−dependent), and also with respect to the other dependents of that head (dependent−dependent orientation). For example, for the sentence in Figure 1 we try to predict the appropriate orientations between the headdependent and dependent-dependent pairs shown in Tables 2 and 3, respectively. Our motivation for using dependency structure as the basis of our reordering model is based on the assumption that, if it is the case that a reordering pattern is employed for one English–Fars"
W15-4906,2006.amta-papers.25,0,0.0607029,"r according to the constituent pairs and features examined (cf. Table 6) and two additional systems that model the reordering for both types of constituent pairs (rows all) with (ws) and without (wos) surface forms. We compare our systems to two baselines, a standard HPB-SMT system (HPB) and a HPBSMT system with added swap glue grammar rule (HPB sgg) as in Equation 4. The swap glue rule allows adjacent phrases to be reversed. X → (X1 X2 , X2 X1 ) (4) Table 7 shows the results obtained by each of the MT systems according to four automatic evaluation metrics: BLEU, NIST (Doddington, 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014). For each system and evaluation metric we show its relative improvement over the baseline HPB (columns diff). The scores obtained by systems that implement our novel reordering between pairs of dependents (columns dd) are better than those of the baseline, both with (ws) and without (wos) surface forms, accross all the four evaluation metrics. The same is true for models that implement reordering between both pairs of constituent types (columns all), except for the system all wos according to BLEU. The results for systems that perform reordering between"
W15-4906,N04-4026,0,0.16978,"pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering model that can be used in combination with the other models in SMT to ﬁnd the best translation. These approaches range from distortion models (Koehn et al., 2003) to lexical reordering models (Tillmann, 2004). 44 Distortion models generally prefer monotone translation which, while may work for related languages, is not a realistic assumption for translating between languages with different grammatical structure. On top of this limitation, these models do not take the content into consideration, and thus they do not generalise well. Lexical reordering models take content into account and condition reordering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexical models, they have two important limitations"
W15-4906,N13-1029,0,0.0134009,"2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pai"
W15-4906,J10-2004,0,0.0160499,"ccess to the necessary structural information to perform long-distance reordering. However, due to the complexity of the decoding algorithm, they have very low performance on large-scale translations. In order to overcome some of these deﬁciencies, we propose a dependency-based reordering model for HPB-SMT. Our model uses the dependency structure of the source sentence to capture the medium- and long-distance reorderings between the dependent parts of the sentence. Unlike the syntax-based models that impose harsh syntactic limits on rule extraction and require serious efforts to be optimised (Wang et al., 2010), we use syntactic information only in the reordering model and augment the HPB model with soft dependency constraints. We report experimental results on a large-scale Engish-to-Farsi translation task. The rest of this paper is organised as follows. Section 2 reviews the related work and contextualises our work. Section 3 outlines the main reordering issues due to syntactic differences between English and Farsi. Section 4 presents our reordering model, which is then evaluated in Section 5. Finally, Section 6 concludes the paper and outlines avenues of future work. 2 Related Work Phrase-based s"
W15-4906,2009.iwslt-papers.4,0,0.100382,"s. Then we extracted 6,391,255 head−dependent pairs and 5,247,137 dependent−dependent pairs from train dataset and determined the orientation for each pair based on Equation 1. In order to measure the impact of different features on the accuracy of our reordering model (as will be described in Section 5.2), we used the Naive Bayes classiﬁer with standard settings from the Weka machine learning toolkit (Hall et al., 2009). We trained the classiﬁer separately for head−dependent and dependent−dependent pairs. Our baseline MT system was the Moses implementation of HPM model with default settings (Hoang et al., 2009). We used a 5-gram target language model trained on the Farsi side of the training data. In all experiments, the weights of our reordering feature-function and the builtin feature-functions was tuned with MERT (Och, 2003). 5.2 Impact of different features Since the proposed reordering model has to classify the head−dependent and dependent−dependent pairs into their correct monotone or swap orientation classes, its task can be seen as a binary classiﬁcation task. We used the Naive Bayes algorithm to build such an orientation classiﬁer. We then used different feature sets in each classiﬁcation e"
W15-4906,D13-1053,0,0.0230185,"anslation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the ﬁ"
W15-4906,N03-1017,0,0.0445723,"cessing component as every faulty decision in the pre-processing step will be passed along as a hard decision to the translation system. This also violates the main principle behind statistical modelling in SMT, i.e. to avoid any hard choices and having the ability to reverse early faulty choices. Approaches in the second group try to handle reordering in the decoding step, as a part of the translation process. They implement a probabilistic reordering model that can be used in combination with the other models in SMT to ﬁnd the best translation. These approaches range from distortion models (Koehn et al., 2003) to lexical reordering models (Tillmann, 2004). 44 Distortion models generally prefer monotone translation which, while may work for related languages, is not a realistic assumption for translating between languages with different grammatical structure. On top of this limitation, these models do not take the content into consideration, and thus they do not generalise well. Lexical reordering models take content into account and condition reordering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexica"
W15-4906,W04-3250,0,0.203099,"Missing"
W15-4906,P06-1066,0,0.0259177,"dering on actual phrases. They try to learn local orientations for each adjacent phrase from training data. Despite the satisfactory performance of lexical models, they have two important limitations (Birch, 2011). First, since these models are conditioned on actual phrases, they have no ability to be generalised to unseen phrases. Second, these models still fail to capture long- and even medium-distance reorderings, since they try to ﬁnd suitable reorderings only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre"
W15-4906,P12-1095,0,0.0196735,"ssing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source elements, pairs of words (Huang et al., 2013), constituents such as head and dependent words (Gao et al., 2011) and predicate-argument structures (Xiong et al., 2012; Li et al., 2013). It is worth noting that all these approaches have been applied solely to one language pair so far, Chinese-to-English. This paper contributes to this research line on two dimensions. First, we extend the work of (Gao et al., 2011), who studied reordering of headdependent pairs (i.e. parent and child elements in the dependency tree), and consider also the reordering of pairs of dependents (i.e. sibling elements in the dependency tree). Second, this is the ﬁrst paper in this line of work to be applied to a language pair other than Chinese-to-English. Our language pair, Englis"
W15-4906,N09-1028,0,0.0188487,"only between adjacent phrases. The ﬁrst limitation can be alleviated by using features of the phrase pair instead of the phrase itself (Xiong et al., 2006) while the second limitation can be tackled with hierarchical phrase reordering models (Galley and Manning, 2008). HPB models (Chiang, 2005) should lead to better reordering than PB-SMT models by allowing phrases to contain gaps. In fact, this approach outperforms PB-SMT in medium-distance reordering, but it is equally weak in long-distance reordering (Birch et al., 2009). Common approaches to reordering in HPB models include preprocessing (Xu et al., 2009) and adding syntax to translation rules. The ﬁrst approach results in improvements but suffers from the same issues presented above for pre-processing reordering in PBSMT. The second introduces additional complexities and increases data sparsity (Hanneman and Lavie, 2013). Our work falls into the recent research line that uses an external reordering model in hierarchical SMT. These models use source syntax to improve reordering without having to annotate translation rules with source syntax. Work in this line has so far looked at predicting the translation order of different types of source el"
W15-4906,C04-1073,0,\N,Missing
W15-4906,N13-1060,0,\N,Missing
W15-4911,2013.iwslt-evaluation.20,0,0.0154076,"s a commercial rule-based engine for English–Farsi translation. It contains 1.5 million words in its database and includes speciﬁc dictionaries for 33 different ﬁelds of science. Another English–Farsi MT system was developed by the Iran Supreme Council of Information.4 Postchi5 is a bidirectional system listed among the EuroMatrix6 systems for the Farsi language. These systems are not terribly robust or precise examples of Farsi SMT and are usually the by-products of research or commercial projects. The only system that has ofﬁcially been reported for the purpose of Farsi SMT is FBK’s system (Bertoldi et al., 2013). It was tested on a publicly available dataset and from this viewpoint is the most important system for our purposes.7 2.2 Parallel Corpora for Farsi SMT The ﬁrst attempts at generating Farsi–English parallel corpora are documented in the Shiraz project (Zajac et al., 2000). The authors constructed a corpus of 3000 parallel sentences, which were translated manually from monolingual online Farsi doc3 http://mabnasoft.com/english/parstrans/index.htm http://www.machinetranslation.ir/ 5 http://www.postchi.com/ 6 http://matrix.statmt.org/resources/pair?l1=fa&l2=en#pair 7 However other Farsi MT eng"
W15-4911,2012.eamt-1.60,0,0.0397015,"Missing"
W15-4911,erjavec-2010-multext,0,0.0282231,"nces, which were translated manually from monolingual online Farsi doc3 http://mabnasoft.com/english/parstrans/index.htm http://www.machinetranslation.ir/ 5 http://www.postchi.com/ 6 http://matrix.statmt.org/resources/pair?l1=fa&l2=en#pair 7 However other Farsi MT engines like the Shiraz system (Amtrup et al., 2000) or that of Mohaghegh (2012) use their own in-house datasets. As we are not able to replicate them we do not include them in our comparisons. 4 83 uments at New Mexico State University. More recently Qasemizadeh et al. (2007) participated in the Farsi part of MULTEXT-EAST8 project (Erjavec, 2010) and developed about 6000 sentences. There is also a corpus available in ELRA9 consisting of about 3,500,000 English and Farsi words aligned at sentence level (about 100,000 sentences). This is a mixed domain dataset including a variety of text types such as art, law, culture, literature, poetry, proverbs, religion etc. PEN (Parallel English–Persian News corpus) is another small corpus (Farajian, 2011) generated semi-automatically. It includes almost 30,000 sentences. Farajian developed a method to ﬁnd similar sentence pairs and for quality assurance used Google Translate.10 All these corpora"
W15-4911,N03-1017,0,0.0397858,"Missing"
W15-4911,P07-2045,0,0.0310231,"scuss the problems with Mizan in Section 4.1 and perform error analysis on the output translations, where it is used as the SMT training data. In the second part using TEP and TEP++ we carry out monolingual translation between SF and FF (SF2FF) and discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default conﬁguration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sent"
W15-4911,W04-3250,0,0.590562,"Missing"
W15-4911,P03-1021,0,0.0511009,"nd discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default conﬁguration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sentences. Training set sizes are reported in tables. For all experiments BLEU scores for Google Translate are reported as a baseline. LM3 LM5 Google Translate Training set Corpus EN–FA Before After 8.24 10.47 8.54 10.53 FA–EN Before After 1"
W15-4911,P02-1040,0,0.0915203,"we carry out monolingual translation between SF and FF (SF2FF) and discuss some use-cases for this type of translation task. Finally in the last part we show how SF2FF boosts the SMT quality for Farsi and report our results on the IWSLT–2013 dataset providing a comparison with FBK’s system. 4.1 Mizan, TEP and TEP++ To test the performance of our engines, they were trained using Mizan, TEP and TEP++. We used Moses (Koehn et al., 2007) with the default conﬁguration for phrase-based translation. For the language modeling, SRILM (Stolcke and others, 2002) was used. The evaluation measure is BLEU (Papineni et al., 2002) and to tune the models, we applied MERT (Och, 2003). Table 1 summarizes our experimental results for the Mizan dataset. We evaluated with two types of language models, 3gram (LM3) and 5-gram (LM5). Numbers for both before and after tuning are reported. For all experiments training, tuning and test sets were selected randomly from the main corpus. The size of the test set is 1,000 and the tuning set is 2000 sentences. Training set sizes are reported in tables. For all experiments BLEU scores for Google Translate are reported as a baseline. LM3 LM5 Google Translate Training set Corpus EN–FA Bef"
W15-4911,I13-1144,0,0.0380416,"Missing"
W15-4944,E06-1032,0,\N,Missing
W15-4944,W10-1751,0,\N,Missing
W15-4944,W14-3301,0,\N,Missing
W15-4944,P02-1040,0,\N,Missing
W15-4944,W14-3319,1,\N,Missing
W15-4944,P11-1105,0,\N,Missing
W15-4944,P10-2041,0,\N,Missing
W15-4944,W05-0909,0,\N,Missing
W15-4944,P07-2045,0,\N,Missing
W15-4944,W07-0718,0,\N,Missing
W15-4944,C14-1111,0,\N,Missing
W15-4944,P12-3005,0,\N,Missing
W15-4944,2012.eamt-1.67,1,\N,Missing
W15-4944,2014.eamt-1.4,1,\N,Missing
W15-4944,W14-3320,0,\N,Missing
W15-4944,2005.mtsummit-papers.11,0,\N,Missing
W15-4944,ljubesic-etal-2014-tweetcat,1,\N,Missing
W15-4944,W15-3036,1,\N,Missing
W15-4944,rubino-etal-2014-quality,1,\N,Missing
W15-4944,W15-3022,1,\N,Missing
W15-4944,W15-4903,1,\N,Missing
W15-4944,2015.eamt-1.4,1,\N,Missing
W15-4944,espla-gomis-etal-2014-comparing,1,\N,Missing
W15-4944,W15-3001,0,\N,Missing
W15-4944,ljubesic-toral-2014-cawac,1,\N,Missing
W15-4944,W14-0405,1,\N,Missing
W15-4944,2005.iwslt-1.8,0,\N,Missing
W15-4944,W16-3421,1,\N,Missing
W15-4944,D07-1078,0,\N,Missing
W15-4944,W08-0509,0,\N,Missing
W15-4944,W11-2123,0,\N,Missing
W15-4944,P14-1129,0,\N,Missing
W15-4944,W16-2347,0,\N,Missing
W15-4944,W16-2375,1,\N,Missing
W15-4944,W16-2367,1,\N,Missing
W15-4944,W16-3423,1,\N,Missing
W16-0602,N12-1047,0,0.0147445,"DE–EN) training corpus (2M+ sentence pairs) is from WMT 2014. GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT. DTU extends the PB model by allowing source discontinuous phrases (Galley and Manning, 2010). All systems are implemented in Moses (Koehn et al., 2007). 11 System PBMT DTU GBMT GSM ZH–EN NIST04 NIST05 32.8 31.4 33.4∗ 31.5 33.7∗+ 31.7 33.8∗+ 32.0∗+ DE–EN WMT12 WMT13 19.6 21.9 19.8∗ 22.3∗ ∗ 19.8 22.4∗ ∗+ 20.3 22.9∗+ Table 1: BLEU (Papineni et al., 2002) scores for all systems on two datasets. Each score is the average score over three MIRA (Cherry and Foster, 2012) runs (Clark et al., 2011). ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than DTU at p ≤ 0.01. System DTU GBMT # Rules ZH–EN DE–EN 224M+ 352M+ 99M+ 153M+ Table 2: The number of rules in DTU and GBMT. Table 1 shows our main results. Our system GBMT is better than PBMT as measured by all three metrics across all testsets. This improvement is reasonable as GBMT allows discontinuous phrases which can reduce data sparsity and handle longdistance relations (Galley and Manning, 2010). Since phrases from syntactic structures are fewer in numb"
W16-0602,N13-1003,0,0.0178214,"define a set of sparse features to explicitly model a graph segmentation. Given previous subgraphs, for each node in a current subgraph, we extract the following features:     0  C     in n .w n.w × P × × 0 out n .c n.c   H where n.w and n.c are the word and class of a current node n, and n0 is a node connected to n. C, P , and H denote that n0 is in the current subgraph or the last previous subgraph or other previous subgraphs, respectively. in and out denote that an edge is an in-coming edge or out-going edge of n. In this paper we lexicalize only on the top-100 frequent words (Cherry, 2013). In addition, we group source words into 50 classes by using mkcls. 3 Experiments and Results Our Chinese–English (ZH–EN) training corpus contains 1.5M+ sentence pairs from LDC. Our German– English (DE–EN) training corpus (2M+ sentence pairs) is from WMT 2014. GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT. DTU extends the PB model by allowing source discontinuous phrases (Galley and Manning, 2010). All systems are implemented in Moses (Koehn et al., 2007). 11 System PBMT DTU GBMT GSM ZH–EN NIST04 NIST05 32.8 31.4 33.4∗ 31.5 33.7∗+ 31.7 33.8∗+ 32"
W16-0602,P05-1033,0,0.136346,"raphs. Experiments on Chinese–English and German–English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfu"
W16-0602,P11-2031,0,0.0130664,"tence pairs) is from WMT 2014. GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT. DTU extends the PB model by allowing source discontinuous phrases (Galley and Manning, 2010). All systems are implemented in Moses (Koehn et al., 2007). 11 System PBMT DTU GBMT GSM ZH–EN NIST04 NIST05 32.8 31.4 33.4∗ 31.5 33.7∗+ 31.7 33.8∗+ 32.0∗+ DE–EN WMT12 WMT13 19.6 21.9 19.8∗ 22.3∗ ∗ 19.8 22.4∗ ∗+ 20.3 22.9∗+ Table 1: BLEU (Papineni et al., 2002) scores for all systems on two datasets. Each score is the average score over three MIRA (Cherry and Foster, 2012) runs (Clark et al., 2011). ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than DTU at p ≤ 0.01. System DTU GBMT # Rules ZH–EN DE–EN 224M+ 352M+ 99M+ 153M+ Table 2: The number of rules in DTU and GBMT. Table 1 shows our main results. Our system GBMT is better than PBMT as measured by all three metrics across all testsets. This improvement is reasonable as GBMT allows discontinuous phrases which can reduce data sparsity and handle longdistance relations (Galley and Manning, 2010). Since phrases from syntactic structures are fewer in number but more reliable (Koeh"
W16-0602,N04-1035,0,0.0481273,"ents on Chinese–English and German–English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South A"
W16-0602,N03-1017,0,0.141924,"away,qliu}@computing.dcu.ie Abstract In this paper, we propose a graph-based translation model which takes advantage of discontinuous phrases. The model segments a graph which combines bigram and dependency relations into subgraphs and produces translations by combining translations of these subgraphs. Experiments on Chinese–English and German–English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgrap"
W16-0602,P07-2045,0,0.00860634,"ming edge or out-going edge of n. In this paper we lexicalize only on the top-100 frequent words (Cherry, 2013). In addition, we group source words into 50 classes by using mkcls. 3 Experiments and Results Our Chinese–English (ZH–EN) training corpus contains 1.5M+ sentence pairs from LDC. Our German– English (DE–EN) training corpus (2M+ sentence pairs) is from WMT 2014. GBMT is our graphbased translation system and GSM adds the graph segmentation model into GBMT. DTU extends the PB model by allowing source discontinuous phrases (Galley and Manning, 2010). All systems are implemented in Moses (Koehn et al., 2007). 11 System PBMT DTU GBMT GSM ZH–EN NIST04 NIST05 32.8 31.4 33.4∗ 31.5 33.7∗+ 31.7 33.8∗+ 32.0∗+ DE–EN WMT12 WMT13 19.6 21.9 19.8∗ 22.3∗ ∗ 19.8 22.4∗ ∗+ 20.3 22.9∗+ Table 1: BLEU (Papineni et al., 2002) scores for all systems on two datasets. Each score is the average score over three MIRA (Cherry and Foster, 2012) runs (Clark et al., 2011). ∗ means a system is significantly better than PBMT at p ≤ 0.01. + means a system is significantly better than DTU at p ≤ 0.01. System DTU GBMT # Rules ZH–EN DE–EN 224M+ 352M+ 99M+ 153M+ Table 2: The number of rules in DTU and GBMT. Table 1 shows our main r"
W16-0602,P06-1077,1,0.69701,"sh and German–English tasks show that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South Africa Nanfei Figure"
W16-0602,2005.mtsummit-ebmt.13,0,0.0186176,"w that our system is significantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South Africa Nanfei Figure 1: An example of constructing a grap"
W16-0602,P02-1040,0,0.0963065,"Missing"
W16-0602,P05-1034,0,0.0704469,"ificantly better than the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South Africa Nanfei Figure 1: An example of constructing a graph for a Chinese sent"
W16-0602,W07-0706,1,0.783162,"n the phrase-based model. By explicitly modeling the graph segmentation, our system gains further improvement. 1 Introduction One significant weakness of conventional phrasebased (PB) models (Koehn et al., 2003) is that it only uses continuous phrases and thus cannot learn generalizations, such as French ne. . . pas to English not (Galley and Manning, 2010). Although using tree structures is believed to be a promising way to solve this problem by learning either translation patterns (Chiang, 2005; Galley et al., 2004; Liu et al., 2006) or treelets (Menezes and Quirk, 2005; Quirk et al., 2005; Xiong et al., 2007), handling non-syntactic phrases is still a big challenge. In this paper, we propose a graph-based translation model which translates a graph into a string by segmenting the graph into subgraphs. Each subgraph is connected and may cover discontinuous phrases. Experiments show that our model is significantly better than the PB model. Explicitly modeling the graph segmentation further improves our system. 10 held Juxing FIFA FIFA World Cup Shijiebei 2010 2010Nian in Zai successfully Chenggong South Africa Nanfei Figure 1: An example of constructing a graph for a Chinese sentence. Each node inclu"
W16-0602,N10-1140,0,\N,Missing
W16-2372,C04-1151,0,0.118778,"Missing"
W16-2372,2005.mtsummit-papers.11,0,0.15388,"WMT16). We propose a technique based on sourceto-target sentence- and word-based scores and the fraction of matched source named entities. We performed our experiments on English-to-French document alignments for this bilingual task. 1 Introduction Parallel corpora (or “bitexts”), comprising bilingual/multilingual texts extracted from parallel documents, are crucial resources for building SMT systems. Unfortunately, parallel documents are a scarce resource for many language pairs with the exception of English, French, Spanish, Arabic, Chinese and some European languages included in Europarl1 (Koehn, 2005) and OPUS (Tiedemann, 2012).2 Furthermore, these existing available corpora do not cover some special domains or subdomains. For the field of SMT, this can be problematic, because MT systems trained on data from a specific domain (e.g. parliamentary proceedings) perform poorly when applied to other domains, e.g. 1 2 http://www.statmt.org/europarl/ http://opus.lingfil.uu.se/ 717 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 717–723, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics tracting parallel data from"
W16-2372,C10-1073,0,0.0270691,"ences from an English–Japanese comparable corpus. They identify similar article pairs, and having considered them as parallel texts, then align sentences using a sentence-pair similarity score and use DP to find the least-cost alignment over the document pair. Munteanu and Marcu (2005) use a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using IR techniques. There have been only a few studies trying to investigate the formal quantification of how similar two comparable documents are. Li and Gaussier (2010) presented one of the first works on developing a comparability measure based on the expectation of finding translation word pairs in the corpus. Our approach follows this line of work based on a method developed by Sennrich and Volk (2010). ods is still much less than expected. We contend that the main problem comes from the document alignment of such comparable corpora. One of the challenges of our research is to build data and techniques for some under-resourced domains. We propose to investigate the improvement of alignment of bilingual comparable documents in order to solve this problem."
W16-2372,J05-4003,0,0.16149,"the most reliable Chinese translation of an English word. One of the main methods relies on cross-lingual information retrieval (CLIR), with different techniques for transferring the request into the target language (using a bilingual dictionary or a full SMT system). Utiyama and Isahara (2003) use CLIR techniques and DP to extract sentences from an English–Japanese comparable corpus. They identify similar article pairs, and having considered them as parallel texts, then align sentences using a sentence-pair similarity score and use DP to find the least-cost alignment over the document pair. Munteanu and Marcu (2005) use a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using IR techniques. There have been only a few studies trying to investigate the formal quantification of how similar two comparable documents are. Li and Gaussier (2010) presented one of the first works on developing a comparability measure based on the expectation of finding translation word pairs in the corpus. Our approach follows this line of work based on a method developed by Sennrich and Volk (2010). ods is still much less"
W16-2372,J03-3002,0,0.301378,"ugust 11-12, 2016. 2016 Association for Computational Linguistics tracting parallel data from such corpora requires special algorithms. Many papers use the Web as a comparable corpus. An adaptive approach, proposed by Zhao and Vogel (2002), aims at mining parallel sentences from a bilingual comparable news collection collected from the Web. A maximum likelihood criterion was used by combining sentence-length models with lexicon-based models. The translation lexicon is iteratively updated using the mined parallel data to obtain better vocabulary coverage and translation probability estimation. Resnik and Smith (2003) propose a web-mining-based system called STRAND and show that their approach is able to find large numbers of similar document pairs. Yang and Li (2003) present an alignment method at different levels (title, word and character) based on dynamic programming (DP). The goal is to identify one-to-one title pairs in an English–Chinese corpus collected from the Web. They apply the longest common sub-sequence to find the most reliable Chinese translation of an English word. One of the main methods relies on cross-lingual information retrieval (CLIR), with different techniques for transferring the r"
W16-2372,2010.amta-papers.14,0,0.576002,"ver the document pair. Munteanu and Marcu (2005) use a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using IR techniques. There have been only a few studies trying to investigate the formal quantification of how similar two comparable documents are. Li and Gaussier (2010) presented one of the first works on developing a comparability measure based on the expectation of finding translation word pairs in the corpus. Our approach follows this line of work based on a method developed by Sennrich and Volk (2010). ods is still much less than expected. We contend that the main problem comes from the document alignment of such comparable corpora. One of the challenges of our research is to build data and techniques for some under-resourced domains. We propose to investigate the improvement of alignment of bilingual comparable documents in order to solve this problem. Accordingly, in this paper we describe an experimental framework designed to address a situation when we have large quantities of non-aligned parallel or comparable documents in different languages that we need to exploit. Our document alig"
W16-2372,tiedemann-2012-parallel,0,0.0207317,"chnique based on sourceto-target sentence- and word-based scores and the fraction of matched source named entities. We performed our experiments on English-to-French document alignments for this bilingual task. 1 Introduction Parallel corpora (or “bitexts”), comprising bilingual/multilingual texts extracted from parallel documents, are crucial resources for building SMT systems. Unfortunately, parallel documents are a scarce resource for many language pairs with the exception of English, French, Spanish, Arabic, Chinese and some European languages included in Europarl1 (Koehn, 2005) and OPUS (Tiedemann, 2012).2 Furthermore, these existing available corpora do not cover some special domains or subdomains. For the field of SMT, this can be problematic, because MT systems trained on data from a specific domain (e.g. parliamentary proceedings) perform poorly when applied to other domains, e.g. 1 2 http://www.statmt.org/europarl/ http://opus.lingfil.uu.se/ 717 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 717–723, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics tracting parallel data from such corpora requires speci"
W16-2372,P03-1010,0,0.075717,"to find large numbers of similar document pairs. Yang and Li (2003) present an alignment method at different levels (title, word and character) based on dynamic programming (DP). The goal is to identify one-to-one title pairs in an English–Chinese corpus collected from the Web. They apply the longest common sub-sequence to find the most reliable Chinese translation of an English word. One of the main methods relies on cross-lingual information retrieval (CLIR), with different techniques for transferring the request into the target language (using a bilingual dictionary or a full SMT system). Utiyama and Isahara (2003) use CLIR techniques and DP to extract sentences from an English–Japanese comparable corpus. They identify similar article pairs, and having considered them as parallel texts, then align sentences using a sentence-pair similarity score and use DP to find the least-cost alignment over the document pair. Munteanu and Marcu (2005) use a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using IR techniques. There have been only a few studies trying to investigate the formal quantification of"
W16-2372,I13-1033,1,\N,Missing
W16-3403,W14-4001,0,0.0172345,"d in document-level MT to disambiguate the word selection. Tran et al. (2014) used bilingual word embeddings to compute the semantic similarity of phrases. To extend the application of text embedding beyond single words, Gao et al. (2013) proposed learning embeddings for source and target phrases by training a network to maximize the sentence-level BLEU score. Costa-jussa et al. (2014) worked at the sentence-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the OOV word problem (Zhao et al., 2014). Our network makes use of some ideas from existing models, but also extends the information available to the embedding model. We train embeddings in the joint space using both source and target side information simultaneously, using a model which is similar to that of Devlin et al. (2014) and Passban et al. (2015b). Similar to Gao et al. (2013) we make embeddings for phrases and sentences and add their similarity as feature functions to the SMT model. 3 Proposed Method In order to train our bilingual em"
W16-3403,W14-3306,0,0.0133549,"he parallel training data.The work by Garcia and Tiedemann (2014) is another model follows that the same paradigm. However, machine translation (MT) is more than word-level translation. In Mart´ınez et al. (2015) word embeddings were used in document-level MT to disambiguate the word selection. Tran et al. (2014) used bilingual word embeddings to compute the semantic similarity of phrases. To extend the application of text embedding beyond single words, Gao et al. (2013) proposed learning embeddings for source and target phrases by training a network to maximize the sentence-level BLEU score. Costa-jussa et al. (2014) worked at the sentence-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the OOV word problem (Zhao et al., 2014). Our network makes use of some ideas from existing models, but also extends the information available to the embedding model. We train embeddings in the joint space using both source and target side information simultaneously, using a model which is similar to that of Devli"
W16-3403,P14-1129,0,0.338501,"2014) worked at the sentence-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the OOV word problem (Zhao et al., 2014). Our network makes use of some ideas from existing models, but also extends the information available to the embedding model. We train embeddings in the joint space using both source and target side information simultaneously, using a model which is similar to that of Devlin et al. (2014) and Passban et al. (2015b). Similar to Gao et al. (2013) we make embeddings for phrases and sentences and add their similarity as feature functions to the SMT model. 3 Proposed Method In order to train our bilingual embedding model, we start by creating a large bilingual corpus. Each line of the corpus may include: – a source or target sentence, – a source or target phrase, – a concatenation of a phrase pair (source and target phrases which are each other’s translation), – a tuple of source and target words (each other’s translation). Sentences of the bilingual corpus are taken from the SMT t"
W16-3403,W14-4015,0,0.0216153,"source embedding space into the target space. The transformation function was approximated using a small set of word pairs extracted using an unsupervised alignment model trained with a parallel corpus. This approach allows the construction of a word-level translation engine with very large monolingual data and only a small number of bilingual word pairs. The cross-lingual transformation mechanism allows the engine to search for translations for OOV (out-of-vocabulary) words by consulting a monolingual index which contains words that were not observed in the parallel training data.The work by Garcia and Tiedemann (2014) is another model follows that the same paradigm. However, machine translation (MT) is more than word-level translation. In Mart´ınez et al. (2015) word embeddings were used in document-level MT to disambiguate the word selection. Tran et al. (2014) used bilingual word embeddings to compute the semantic similarity of phrases. To extend the application of text embedding beyond single words, Gao et al. (2013) proposed learning embeddings for source and target phrases by training a network to maximize the sentence-level BLEU score. Costa-jussa et al. (2014) worked at the sentence-level and incorp"
W16-3403,P14-6007,0,0.025054,"detail. In Section 4, experimental results are reported. We also have a separate section to discuss different aspects of embeddings and the model. Finally, in the last section we present our conclusions along with some avenues for future work. 2 Background Several models such as He et al. (2008), Liu et al. (2008) and Shen et al. (2009) studied the use of contextual information for statistical machine translation (SMT). The idea is to go beyond the phrase level and enhance the phrase representation by taking surrounding phrases into account. This line of research is referred as discourse SMT (Hardmeier, 2014; Meyer, 2014). Because NNs can provide distributed representations for words and phrases, they are ideally suited to the task of comparing semantic similarity. Unsupervised models such as Word2Vec2 (Mikolov et al., 2013a) or Paragraph Vectors (Le & Mikolov, 2014) have shown that distributional information is often enough to learn high-quality word and sentence embeddings. A large body of recent work has evaluated the use of embeddings in machine translation. A successful usecase was reported in (Mikolov et al., 2013b). They separately 1 2 Although the features contributed by the language mode"
W16-3403,C08-1041,1,0.788524,"formation using the neural features. We search in the source and target spaces and retrieve the closest constituent to the phrase pair in our bilingual embedding space. The structure of the paper is as follows. Section 2 gives an overview of related work. Section 3 explains our pipeline and the network architecture in detail. In Section 4, experimental results are reported. We also have a separate section to discuss different aspects of embeddings and the model. Finally, in the last section we present our conclusions along with some avenues for future work. 2 Background Several models such as He et al. (2008), Liu et al. (2008) and Shen et al. (2009) studied the use of contextual information for statistical machine translation (SMT). The idea is to go beyond the phrase level and enhance the phrase representation by taking surrounding phrases into account. This line of research is referred as discourse SMT (Hardmeier, 2014; Meyer, 2014). Because NNs can provide distributed representations for words and phrases, they are ideally suited to the task of comparing semantic similarity. Unsupervised models such as Word2Vec2 (Mikolov et al., 2013a) or Paragraph Vectors (Le & Mikolov, 2014) have shown that"
W16-3403,W04-3250,0,0.199529,"sed MERT (Och, 2003) for tuning. In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM (Stolcke et al., 2002). We used BLEU (Papineni et al., 2002) as the evaluation metric. We added our features to the phrase table and tuned the translation models. Table 2 shows the impact of each feature. We also estimated the translation quality in the presence of the all features (we run MERT for each row of Table 2). Bold numbers are statistically significant according to the results of paired bootstrap re-sampling with p=0.05 for 1000 samples (Koehn, 2004). Arrows indicate whether the new features increased or decreased the quality over the baseline. 4 http://www.statmt.org/europarl/ 136 Passban et al. Table 2. Impact of the proposed features. Feature Baseline sp2tp sp2sm sp2tm tp2tm tp2sm sm2tm All En–Fa 21.03 21.46 21.32 21.40 20.40 21.93 21.18 21.84 ↑↓ 0.00 0.43 ↑ 0.29 ↑ 0.37 ↑ 0.63 ↓ 0.90 ↑ 0.15 ↑ 0.81 ↑ Fa–En 29.21 29.71 29.74 29.56 29.56 29.26 30.08 30.26 ↑↓ En–Cz ↑↓ 0.00 28.35 0.00 0.50 ↑ 28.72 0.37 ↑ 0.53 ↑ 28.30 0.05 ↓ 0.35 ↑ 28.52 0.17 ↑ 0.35 ↑ 28.00 0.35 ↓ 0.05 ↑ 28.94 0.59↑ 0.87 ↑ 28.36 0.01 ↑ 1.05 ↑ 29.01 0.66 ↑ Cz–En 39.63 40.34 3"
W16-3403,2005.mtsummit-papers.11,0,0.00908734,"cab. Fig. 2. Network architecture. The input document is S = w1 w2 w3 w4 w5 w6 and the target word is w3 . 4 Experimental Results We evaluated our new features on two language pairs: En–Fa and En–Cz. Both Farsi and Czech are morphologically rich languages; therefore, translation to/from these languages can be more difficult than it is for languages where words tend to be discrete semantic units. Farsi is also a low-resource language, so we are interested in working with these pairs. For the En–Fa pair we used the TEP++ corpus (Passban et al., 2015a) and for Czech we used the Europarl4 corpus (Koehn, 2005). TEP++ is a collection of 600,000 parallel sentences. We used 1000 and 2000 sentences for testing and tuning, respectively and the rest of the corpus for training. From the Czech dataset we selected the same number of sentences for training, testing and tuning. The baseline system is a PBSMT engine built using Moses (Koehn et al., 2007) with the default configuration. We used MERT (Och, 2003) for tuning. In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM (Stolcke et al., 2002). We used BLEU (Papineni et al., 2002) as the evaluati"
W16-3403,P07-2045,0,0.0103536,"Missing"
W16-3403,D08-1010,1,0.729984,"e neural features. We search in the source and target spaces and retrieve the closest constituent to the phrase pair in our bilingual embedding space. The structure of the paper is as follows. Section 2 gives an overview of related work. Section 3 explains our pipeline and the network architecture in detail. In Section 4, experimental results are reported. We also have a separate section to discuss different aspects of embeddings and the model. Finally, in the last section we present our conclusions along with some avenues for future work. 2 Background Several models such as He et al. (2008), Liu et al. (2008) and Shen et al. (2009) studied the use of contextual information for statistical machine translation (SMT). The idea is to go beyond the phrase level and enhance the phrase representation by taking surrounding phrases into account. This line of research is referred as discourse SMT (Hardmeier, 2014; Meyer, 2014). Because NNs can provide distributed representations for words and phrases, they are ideally suited to the task of comparing semantic similarity. Unsupervised models such as Word2Vec2 (Mikolov et al., 2013a) or Paragraph Vectors (Le & Mikolov, 2014) have shown that distributional info"
W16-3403,P03-1021,0,0.0122874,"Farsi is also a low-resource language, so we are interested in working with these pairs. For the En–Fa pair we used the TEP++ corpus (Passban et al., 2015a) and for Czech we used the Europarl4 corpus (Koehn, 2005). TEP++ is a collection of 600,000 parallel sentences. We used 1000 and 2000 sentences for testing and tuning, respectively and the rest of the corpus for training. From the Czech dataset we selected the same number of sentences for training, testing and tuning. The baseline system is a PBSMT engine built using Moses (Koehn et al., 2007) with the default configuration. We used MERT (Och, 2003) for tuning. In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM (Stolcke et al., 2002). We used BLEU (Papineni et al., 2002) as the evaluation metric. We added our features to the phrase table and tuned the translation models. Table 2 shows the impact of each feature. We also estimated the translation quality in the presence of the all features (we run MERT for each row of Table 2). Bold numbers are statistically significant according to the results of paired bootstrap re-sampling with p=0.05 for 1000 samples (Koehn, 2004). Arrows"
W16-3403,P02-1038,0,0.00763426,"rch problem where the score at each step of exploration is formulated as a log-linear model (Koehn, 2010). For each candidate phrase, the set of features is combined with a set of learned weights to find the best target counterpart of the provided source sentence. Because an exhaustive search of the candidate space is not computationally feasible, the space is typically pruned via some heuristic search, such as beam search (Koehn, 2010). The discriminative log-linear model allows the incorporation of arbitrary context-dependent and context-independent features. Thus, features such as those in Och and Ney (2002) or Chiang et al. (2009) can be combined to improve translation performance. The standard baseline bilingual features included in Moses (Koehn et al., 2007) by default are: the phrase translation 130 Passban et al. probability φ(e|f ), inverse phrase translation probability φ(f |e), direct lexical weighting lex(e|f ) and inverse lexical weighting lex(f |e).1 The scores in the phrase table are computed directly from the co-occurrence of aligned phrases in training corpora. A large body of recent work evaluates the hypothesis that co-occurrence information alone cannot capture contextual informa"
W16-3403,P02-1040,0,0.0971356,"sed the Europarl4 corpus (Koehn, 2005). TEP++ is a collection of 600,000 parallel sentences. We used 1000 and 2000 sentences for testing and tuning, respectively and the rest of the corpus for training. From the Czech dataset we selected the same number of sentences for training, testing and tuning. The baseline system is a PBSMT engine built using Moses (Koehn et al., 2007) with the default configuration. We used MERT (Och, 2003) for tuning. In the experiments we trained 5-gram language models on the monolingual parts of the bilingual corpora using SRILM (Stolcke et al., 2002). We used BLEU (Papineni et al., 2002) as the evaluation metric. We added our features to the phrase table and tuned the translation models. Table 2 shows the impact of each feature. We also estimated the translation quality in the presence of the all features (we run MERT for each row of Table 2). Bold numbers are statistically significant according to the results of paired bootstrap re-sampling with p=0.05 for 1000 samples (Koehn, 2004). Arrows indicate whether the new features increased or decreased the quality over the baseline. 4 http://www.statmt.org/europarl/ 136 Passban et al. Table 2. Impact of the proposed features. Feat"
W16-3403,W15-4911,1,0.558797,"nce-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the OOV word problem (Zhao et al., 2014). Our network makes use of some ideas from existing models, but also extends the information available to the embedding model. We train embeddings in the joint space using both source and target side information simultaneously, using a model which is similar to that of Devlin et al. (2014) and Passban et al. (2015b). Similar to Gao et al. (2013) we make embeddings for phrases and sentences and add their similarity as feature functions to the SMT model. 3 Proposed Method In order to train our bilingual embedding model, we start by creating a large bilingual corpus. Each line of the corpus may include: – a source or target sentence, – a source or target phrase, – a concatenation of a phrase pair (source and target phrases which are each other’s translation), – a tuple of source and target words (each other’s translation). Sentences of the bilingual corpus are taken from the SMT training corpus. According"
W16-3403,D09-1008,0,0.0168738,"search in the source and target spaces and retrieve the closest constituent to the phrase pair in our bilingual embedding space. The structure of the paper is as follows. Section 2 gives an overview of related work. Section 3 explains our pipeline and the network architecture in detail. In Section 4, experimental results are reported. We also have a separate section to discuss different aspects of embeddings and the model. Finally, in the last section we present our conclusions along with some avenues for future work. 2 Background Several models such as He et al. (2008), Liu et al. (2008) and Shen et al. (2009) studied the use of contextual information for statistical machine translation (SMT). The idea is to go beyond the phrase level and enhance the phrase representation by taking surrounding phrases into account. This line of research is referred as discourse SMT (Hardmeier, 2014; Meyer, 2014). Because NNs can provide distributed representations for words and phrases, they are ideally suited to the task of comparing semantic similarity. Unsupervised models such as Word2Vec2 (Mikolov et al., 2013a) or Paragraph Vectors (Le & Mikolov, 2014) have shown that distributional information is often enough"
W16-3403,D15-1167,0,0.0347918,"e level similarities. Retrieved instances are semantically related to the given queries. Table 3. The top 10 most similar vectors for the given English query. Recall that the retrieved vectors could belong to words, phrases or sentences in either English or Farsi and word or phrase pairs. The items that were originally in Farsi have been translated into English, and are indicated with italics. Query 1 2 3 4 5 6 7 8 9 10 sadness <apprehension, nervous> emotion <ill,sick> pain <money,money> benignity <may he was punished,punished harshly> is really gonna hurt i know tom ’ s dying <bitter,angry> Tang et al. (2015) proposed that a sentence embedding could be generated by averaging/concatenating embeddings of the words in that sentence. In our case the model by Tang et al. was not as beneficial as ours for both Farsi and Czech. As an example if the sp2tp is computed using their model, it degrades the En–Fa direction’s BLEU from 21.03 to 20.97 and its improvement for the Fa–En direction is only +0.11 points (almost 5 times less than ours). Our goal is not to compare our model to that of Tang et al.. We only performed a simple comparison on the most important feature to see the difference. Furthermore, acc"
W16-3403,D14-1175,0,0.0133728,"ranslation engine with very large monolingual data and only a small number of bilingual word pairs. The cross-lingual transformation mechanism allows the engine to search for translations for OOV (out-of-vocabulary) words by consulting a monolingual index which contains words that were not observed in the parallel training data.The work by Garcia and Tiedemann (2014) is another model follows that the same paradigm. However, machine translation (MT) is more than word-level translation. In Mart´ınez et al. (2015) word embeddings were used in document-level MT to disambiguate the word selection. Tran et al. (2014) used bilingual word embeddings to compute the semantic similarity of phrases. To extend the application of text embedding beyond single words, Gao et al. (2013) proposed learning embeddings for source and target phrases by training a network to maximize the sentence-level BLEU score. Costa-jussa et al. (2014) worked at the sentence-level and incorporated the source side information into the decoding phase by finding the similarities between phrases and source embeddings. Some other models re-scored the phrase table (Alkhouli et al., 2014) or generated new phrase pairs in order to address the"
W16-3403,N09-1025,0,\N,Missing
W16-3403,N15-1176,0,\N,Missing
W16-3404,2013.mtsummit-user.3,1,0.799285,"Missing"
W16-3404,2015.mtsummit-papers.14,1,0.870149,"Missing"
W16-3404,2014.amta-wptp.5,1,0.819014,"Missing"
W16-3404,D14-1130,0,0.0689897,"Missing"
W16-3404,2010.amta-papers.27,1,0.825004,"Missing"
W16-3404,W15-4933,0,0.0350222,"r TM outputs”. They note that this indicates not only that “phrase-based SMT system[s] [are] able to produce outputs that are … grammatically acceptable enough to be recognized as human translations in the TM”, but also “how much the post-editors subconsciously trust the TM [which] may be an explanation for the relatively low acceptance of MT technology in the localization industry and demonstrates the need for TM–MT integration”. In a similar line of work, we note here the recent effort by STAR to combine TM and MT in an interesting way, where MT matches are used to reinforce fuzzy matching (Hofmann, 2015). Participants in this study were reasonably satisfied with the quality of MT output despite the use of a generic engine for a difficult language pair. This suggests that, Comparing Translator Acceptability of TM and SMT Outputs 149 contrary to perceived wisdom in the field, quality is not the sole barrier for widespread MT acceptance. The results of Moorkens et al. (2015) showed that the addition of onscreen MT confidence indication alone does not immediately lead to behavioural changes for post-editors. Users need to learn to trust measures of quality or confidence, but also need to be prese"
W16-3404,W15-4910,1,0.879945,"Missing"
W16-3404,2013.tc-1.13,0,0.0634124,"Missing"
W16-3404,2009.mtsummit-papers.14,0,0.142171,"Missing"
W16-3404,2009.eamt-1.5,0,0.0384134,"nusable and eschewed this option. 148 Moorkens and Way 4. Discussion The strong association between fuzzy match percentages and participant ratings, despite the fact that percentages were not displayed onscreen, demonstrates an advantage that TM has over MT: fuzzy matches are reasonably accurate gauges of quality that correlate with human judgement, whereas “the correlation between human judges and all [contemporary] automatic measures of MT quality” is “quite low” (Turian et al., 2003). Some progress has been made in research on MT confidence estimation without use of reference translations. Specia et al. (2009), in a study that aimed to eliminate very poor MT results, identified 84 segment-level features that could be used to estimate MT quality, with results that correlated far better with human judgements than several commonly-used automatic evaluation metrics. Accurately gauging the quality of timeconsuming mid-ranking MT output will be a more onerous task. Turchi et al. (2013) noted the subjectivity of human judgements and the associated difficulty in confidence estimation using machine learning based on human annotation. Specia (2011) suggested machine-learning models based on user post-edits a"
W16-3404,2011.eamt-1.12,0,0.021805,"nce estimation without use of reference translations. Specia et al. (2009), in a study that aimed to eliminate very poor MT results, identified 84 segment-level features that could be used to estimate MT quality, with results that correlated far better with human judgements than several commonly-used automatic evaluation metrics. Accurately gauging the quality of timeconsuming mid-ranking MT output will be a more onerous task. Turchi et al. (2013) noted the subjectivity of human judgements and the associated difficulty in confidence estimation using machine learning based on human annotation. Specia (2011) suggested machine-learning models based on user post-edits as a route for accurate MT confidence estimation. It is less likely that users would have accepted TM were it not possible to impose a quality threshold that users can confidently consider accurate and personalise to their own requirements based on years of experience. Once this threshold is removed, participants in this study commented on the low-quality match proposals and rated many segments poorly. For this reason, we consider the answer to the research question presented in Section 1 to be answered – at least in part – in the aff"
W16-3404,W13-2231,0,0.0493928,"between human judges and all [contemporary] automatic measures of MT quality” is “quite low” (Turian et al., 2003). Some progress has been made in research on MT confidence estimation without use of reference translations. Specia et al. (2009), in a study that aimed to eliminate very poor MT results, identified 84 segment-level features that could be used to estimate MT quality, with results that correlated far better with human judgements than several commonly-used automatic evaluation metrics. Accurately gauging the quality of timeconsuming mid-ranking MT output will be a more onerous task. Turchi et al. (2013) noted the subjectivity of human judgements and the associated difficulty in confidence estimation using machine learning based on human annotation. Specia (2011) suggested machine-learning models based on user post-edits as a route for accurate MT confidence estimation. It is less likely that users would have accepted TM were it not possible to impose a quality threshold that users can confidently consider accurate and personalise to their own requirements based on years of experience. Once this threshold is removed, participants in this study commented on the low-quality match proposals and"
W16-3404,2003.mtsummit-papers.51,0,0.0951676,"e ‘activates and deactivates’ phrase could have been leveraged by users, but all participants considered the segment as a whole unusable and eschewed this option. 148 Moorkens and Way 4. Discussion The strong association between fuzzy match percentages and participant ratings, despite the fact that percentages were not displayed onscreen, demonstrates an advantage that TM has over MT: fuzzy matches are reasonably accurate gauges of quality that correlate with human judgement, whereas “the correlation between human judges and all [contemporary] automatic measures of MT quality” is “quite low” (Turian et al., 2003). Some progress has been made in research on MT confidence estimation without use of reference translations. Specia et al. (2009), in a study that aimed to eliminate very poor MT results, identified 84 segment-level features that could be used to estimate MT quality, with results that correlated far better with human judgements than several commonly-used automatic evaluation metrics. Accurately gauging the quality of timeconsuming mid-ranking MT output will be a more onerous task. Turchi et al. (2013) noted the subjectivity of human judgements and the associated difficulty in confidence estima"
W16-3404,2013.tc-1.12,1,0.87183,"Missing"
W16-3404,2015.eamt-1.11,1,\N,Missing
W16-4015,L16-1153,1,0.69056,"in order to verify the applicability of our new OCR-to-MT framework. 2.2 Translation method This technique centres on using an SMT system trained on the OCR output texts which have been postedited and manually corrected. SMT systems handle the translation process as the transformation of a sequence of symbols in a source language into another sequence of symbols in a target language. Generally the symbols dealt with are the words in the two languages. We consider that our SMT system will translate OCR output to corrected text in the same language following the work of (Fancellu et al., 2014; Afli et al., 2016). In fact, using the standard approach of SMT we are given a sentence (a sequence of OCR output words) sM = s1 ...sM of size M which is to be translated into a corrected sentence tN = t1 ...tN of size N in the same language (French in our case). The statistical approach aims at determining the translation t∗ which maximizes the posterior probability given the source sentence. Formally, by using Bayes’ rule, the fundamental equation is (1): t∗ = arg max P r(t|s) = arg max P r(s|t)P r(t) t t 110 (1) It can be decomposed, as in the original work of (Brown et al., 1993), into a language model prob"
W16-4015,J93-2003,0,0.0394854,"k of (Fancellu et al., 2014; Afli et al., 2016). In fact, using the standard approach of SMT we are given a sentence (a sequence of OCR output words) sM = s1 ...sM of size M which is to be translated into a corrected sentence tN = t1 ...tN of size N in the same language (French in our case). The statistical approach aims at determining the translation t∗ which maximizes the posterior probability given the source sentence. Formally, by using Bayes’ rule, the fundamental equation is (1): t∗ = arg max P r(t|s) = arg max P r(s|t)P r(t) t t 110 (1) It can be decomposed, as in the original work of (Brown et al., 1993), into a language model probability P r(t), and a translation model probability P r(s|t). The language model is trained on a large quantity of French texts and the translation model is trained using a bilingual text aligned at sentence (segment) level, i.e. an OCR output for a segment and its ground-truth obtained manually. As in most current state-of-the-art systems, the translation probability is modelled using the log-linear model in (2): P (t|s) = N X λi hi (s, t) (2) i=0 where hi (s, t) is the it h feature function and λi its weight (determined by an optimization process). We call this me"
W16-4015,2014.eamt-1.34,1,0.848541,"Missing"
W16-4015,W08-0509,0,0.0281339,"3.08 M 9013 # ref tokens 1.96 M 33.40 M 22.9 M 8946 Table 1: Statistics of MT training, development and test data available to build our systems. For all of the different techniques used in this paper, the language model was built using the KenLM toolkit with Kneser-Ney smoothing and default backoff. For the SM T cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). Word alignments in both directions are calculated, using a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our two systems using a Confusion Network (CN) combination system based on the work of (Wu et al., 2012). We call this combination LM cor + SM T cor. 5.2 Results In order to evaluate the effectiveness of error correction, we used Word Error Rate (WER) which is derived from Levenshtein distance (Levenshtein, 1966). We compare results on the test data of the two different methods used in our experiments and their combination, against the baseline results which represent s"
W16-4015,N03-1017,0,0.0123894,"century, manually corrected. The statistics of all corpora used in our experiments can be seen in Table 1. 112 bitexts smt 17 smt 18 smt 19 dev17 # OCR tokens 1.98 M 33.49 M 23.08 M 9013 # ref tokens 1.96 M 33.40 M 22.9 M 8946 Table 1: Statistics of MT training, development and test data available to build our systems. For all of the different techniques used in this paper, the language model was built using the KenLM toolkit with Kneser-Ney smoothing and default backoff. For the SM T cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). Word alignments in both directions are calculated, using a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our two systems using a Confusion Network (CN) combination system based on the work of (Wu et al., 2012). We call this combination LM cor + SM T cor. 5.2 Results In order to evaluate the effectiveness of error correction, we used Word Error Rate (WER) which is derived from Levenshtein distance (Le"
W16-4015,P07-2045,0,0.0202219,"orpora used in our experiments can be seen in Table 1. 112 bitexts smt 17 smt 18 smt 19 dev17 # OCR tokens 1.98 M 33.49 M 23.08 M 9013 # ref tokens 1.96 M 33.40 M 22.9 M 8946 Table 1: Statistics of MT training, development and test data available to build our systems. For all of the different techniques used in this paper, the language model was built using the KenLM toolkit with Kneser-Ney smoothing and default backoff. For the SM T cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). Word alignments in both directions are calculated, using a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our two systems using a Confusion Network (CN) combination system based on the work of (Wu et al., 2012). We call this combination LM cor + SM T cor. 5.2 Results In order to evaluate the effectiveness of error correction, we used Word Error Rate (WER) which is derived from Levenshtein distance (Levenshtein, 1966). We compare results on the test dat"
W16-4015,P04-1077,0,0.0346542,"cor system improves the results more than LM cor. Nonetheless, both underperform compared to the CN Comb. system. This is due to the fact that the two methods are not always correcting the same errors, so the CN combination can be beneficial in this case. Systems Baseline LM cor SMT cor CN Comb. LM cor + SMT cor Correctness 83.92 84.82 87.64 Accuracy 81.68 82.57 86.06 WER 18.32 17.43 13.94 89.10 87.07 12.93 Table 2: Word Error Rate (WER), Accuracy and Correctness results on on dev17 OCR-corrected data. For the translation evaluation we used BLEU-4 score (Papineni et al., 2002), Smoothed BLEU (Lin and Och, 2004) and TER (Snover et al., 2006) calculated between the output of Exp. 1 (our reference) and Exp. 2 output (the baseline) or Exp. 3 output (our proposed framework). Table 3 lists the results of the two translation outputs from Exp. 1 and Exp. 2. It shows that our proposed framework is very capable of correcting the final translation of the OCR-ed documents. 5.3 Analysis and Discussion In order to better understand the impact of the error correction process and the problems of OCR’ed historical document translation, we prepared a manual human translation of our test set based on the https://kheaf"
W16-4015,W06-1648,0,0.040841,"ll has various limitations that prevent it from being the perfect solution for OCR error correction (Hong, 1995). It requires a wide-ranging dictionary that covers every word in the language. Existing linguistic resources can usually target a single specific language in a given period, but cannot therefore support historical documents. The second type of approach in OCR post-processing is context-based error correction. These techniques are founded on statistical language modelling and word n-grams, and aims to calculate the likelihood that a particular word sequence appears (Tillenius, 1996; Magdy and Darwish, 2006). Applying this technique on historical documents is challenging because the works on building corpora for this kind of task has been very limited. Furthermore, when many consecutive corrupted words are encountered in a sentence, it is difficult to choose the good candidate words. In this paper we conducted our experiments using a corpus of old-style French OCR-ed data from the 17th , 18th and 19th centuries in order to verify the applicability of our new OCR-to-MT framework. 2.2 Translation method This technique centres on using an SMT system trained on the OCR output texts which have been po"
W16-4015,P12-2059,0,0.0147416,"a translation model probability P r(s|t). The language model is trained on a large quantity of French texts and the translation model is trained using a bilingual text aligned at sentence (segment) level, i.e. an OCR output for a segment and its ground-truth obtained manually. As in most current state-of-the-art systems, the translation probability is modelled using the log-linear model in (2): P (t|s) = N X λi hi (s, t) (2) i=0 where hi (s, t) is the it h feature function and λi its weight (determined by an optimization process). We call this method ”SMT cor ” in the rest of this paper. As (Nakov and Tiedemann, 2012; Tiedemann and Nakov, 2013) demonstrated, closely related languages largely overlap in vocabulary and have a strong syntactic and lexical similarities. We assume that we do not need to use the reordering model in the task of error correction in the same language. 2.3 Language Modelling Language Modelling is the field of creating models for writing text so that we can assign a probability to a sequence of n consecutive words. Using this technique, the candidate correction of an error might be successfully found using the Noisy Channel Model (Mays et al., 1991). Considering the sentence ’I drin"
W16-4015,P03-1021,0,0.0777586,"e to build our systems. For all of the different techniques used in this paper, the language model was built using the KenLM toolkit with Kneser-Ney smoothing and default backoff. For the SM T cor method, an SMT system is trained on all available parallel data. Our SMT system is a phrase-based system (Koehn et al., 2003) based on the Moses SMT toolkit (Koehn et al., 2007). Word alignments in both directions are calculated, using a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). The parameters of our system were tuned on a development corpus, using Minimum Error Rate Training (Och, 2003). We combined our two systems using a Confusion Network (CN) combination system based on the work of (Wu et al., 2012). We call this combination LM cor + SM T cor. 5.2 Results In order to evaluate the effectiveness of error correction, we used Word Error Rate (WER) which is derived from Levenshtein distance (Levenshtein, 1966). We compare results on the test data of the two different methods used in our experiments and their combination, against the baseline results which represent scores between OCR output and the corrected reference (called OCR-Baseline). Table 2 reports on the percentage of"
W16-4015,P02-1040,0,0.0947742,"It can also be observed that the SM T cor system improves the results more than LM cor. Nonetheless, both underperform compared to the CN Comb. system. This is due to the fact that the two methods are not always correcting the same errors, so the CN combination can be beneficial in this case. Systems Baseline LM cor SMT cor CN Comb. LM cor + SMT cor Correctness 83.92 84.82 87.64 Accuracy 81.68 82.57 86.06 WER 18.32 17.43 13.94 89.10 87.07 12.93 Table 2: Word Error Rate (WER), Accuracy and Correctness results on on dev17 OCR-corrected data. For the translation evaluation we used BLEU-4 score (Papineni et al., 2002), Smoothed BLEU (Lin and Och, 2004) and TER (Snover et al., 2006) calculated between the output of Exp. 1 (our reference) and Exp. 2 output (the baseline) or Exp. 3 output (our proposed framework). Table 3 lists the results of the two translation outputs from Exp. 1 and Exp. 2. It shows that our proposed framework is very capable of correcting the final translation of the OCR-ed documents. 5.3 Analysis and Discussion In order to better understand the impact of the error correction process and the problems of OCR’ed historical document translation, we prepared a manual human translation of our"
W16-4015,W12-3212,0,0.0692542,"Missing"
W16-4015,2006.amta-papers.25,0,0.025304,"lts more than LM cor. Nonetheless, both underperform compared to the CN Comb. system. This is due to the fact that the two methods are not always correcting the same errors, so the CN combination can be beneficial in this case. Systems Baseline LM cor SMT cor CN Comb. LM cor + SMT cor Correctness 83.92 84.82 87.64 Accuracy 81.68 82.57 86.06 WER 18.32 17.43 13.94 89.10 87.07 12.93 Table 2: Word Error Rate (WER), Accuracy and Correctness results on on dev17 OCR-corrected data. For the translation evaluation we used BLEU-4 score (Papineni et al., 2002), Smoothed BLEU (Lin and Och, 2004) and TER (Snover et al., 2006) calculated between the output of Exp. 1 (our reference) and Exp. 2 output (the baseline) or Exp. 3 output (our proposed framework). Table 3 lists the results of the two translation outputs from Exp. 1 and Exp. 2. It shows that our proposed framework is very capable of correcting the final translation of the OCR-ed documents. 5.3 Analysis and Discussion In order to better understand the impact of the error correction process and the problems of OCR’ed historical document translation, we prepared a manual human translation of our test set based on the https://kheafield.com/code/kenlm The source"
W16-4015,R13-1088,0,0.0153927,"ility P r(s|t). The language model is trained on a large quantity of French texts and the translation model is trained using a bilingual text aligned at sentence (segment) level, i.e. an OCR output for a segment and its ground-truth obtained manually. As in most current state-of-the-art systems, the translation probability is modelled using the log-linear model in (2): P (t|s) = N X λi hi (s, t) (2) i=0 where hi (s, t) is the it h feature function and λi its weight (determined by an optimization process). We call this method ”SMT cor ” in the rest of this paper. As (Nakov and Tiedemann, 2012; Tiedemann and Nakov, 2013) demonstrated, closely related languages largely overlap in vocabulary and have a strong syntactic and lexical similarities. We assume that we do not need to use the reordering model in the task of error correction in the same language. 2.3 Language Modelling Language Modelling is the field of creating models for writing text so that we can assign a probability to a sequence of n consecutive words. Using this technique, the candidate correction of an error might be successfully found using the Noisy Channel Model (Mays et al., 1991). Considering the sentence ’I drink a baer‘, the error correct"
W16-4015,W12-5704,0,0.0690117,"Missing"
W17-1313,W08-0509,0,0.0225439,"Missing"
W17-1313,N13-1044,0,0.0428744,"Missing"
W17-1313,N03-1017,0,0.00707196,"or the query topic set, we use a modified monolingual adhoc version of the 60 different original English topics developed within the MediaEval 2012 Search and Hyperlinking task5 which was developed by Khwileh et al. (2016). To setup the CLIR system, similar to the procedure adopted in our earlier investigation (2015), we used two native Arabic (AR) speakers who are also fluent on English (EN) to write their equivalent versions of the queries in Arabic for each of these EN topics. We configured and trained an AR-to-EN MT system to translate each AR query to EN. Our MT system is a phrase-based (Koehn et al., 2003), that is developed using the Moses Statistical Machine Translation (SMT) toolkit (Koehn et al., 2007). Word alignments in both directions were calculated using a multi-threaded version of the GIZA++ 6 tool (Gao and Vogel, 2008). The parameters of our MT system were tuned on a development corpus using Minimum Error Rate Training (Och, 2003). The AR-to-En MT system was trained using the bilingual training corpora listed in Table 1 from LDC for MSA (Modern Standard Arabic) training. The size of the tuning set is 111.8K and 138.2K of Arabic and English tokens. All AR data are tokenised using MADA"
W17-1313,P07-2045,0,0.0197594,"Missing"
W17-1313,P03-1021,0,0.0129249,"Missing"
W17-1313,C12-1164,0,0.0429978,"Missing"
W17-1608,P15-1174,0,0.0311308,"nked systems. • Potential ‘gaming the system’. Another concern is the impact of the results of the shared task beyond the shared task itself (e.g. real-world applications, end-users). Shared tasks are evaluated against a common test set under the auspices of a ‘fair’ comparison among systems. However, as the ultimate goal of most participating teams is to obtain the highest positions in the ranking, there is a risk of focusing on winning, rather than on the task itself. Of course, accurate evaluation is crucial when reporting results of NLP tasks (e.g. Summarisation (Mackie et al., 2014); MT (Graham, 2015)). As evaluation metrics play a crucial role in determining who is the winner of a shared task, many participating teams will tune their systems so that they achieve the highest possible score for the objective function at hand, as opposed to focusing on whether this approach is actually the best way to solve the problem. This, in turn, impacts directly on the real-world applications for which solving that challenge is particularly relevant, as it may be the case that the ‘winning’ systems are not necessarily the best ones to be used in practice. • Unequal playing field. Another potential risk"
W17-1608,E03-1076,0,0.0237189,"ined results. 2.1 Why are shared tasks important in our field? Shared tasks are important because they help boost the pace of development in our field and encourage a culture of improving upon the state-of-theart. Shared tasks have an additional advantage: by using the same data, all systems can be evaluated objectively and comparisons across systems could be made easier. At the same time, some best practices and de facto standards have evolved from shared tasks, e.g. the widely used CoNLL format used in parsing and many other NLP tasks, and the splitting of German compounds in MT proposed by Koehn and Knight (2003). A by-product of these shared tasks are the new datasets that are made available for use by the Shared Tasks in NLP As mentioned in Section 1, shared tasks are competitions to which researchers or teams of researchers submit systems that address a particular challenge. In the field of NLP, the first shared tasks were initiated in the United States by NIST in collaboration with DARPA (Mariani et al., 2014). Paroubek et al. (2007) report that the first shared tasks – then called evaluation campaigns – focused 5 See Pallett (2003) for an overview of these first shared tasks and the role that NIS"
W17-1608,mariani-etal-2014-rediscovering,0,0.0460867,"s and de facto standards have evolved from shared tasks, e.g. the widely used CoNLL format used in parsing and many other NLP tasks, and the splitting of German compounds in MT proposed by Koehn and Knight (2003). A by-product of these shared tasks are the new datasets that are made available for use by the Shared Tasks in NLP As mentioned in Section 1, shared tasks are competitions to which researchers or teams of researchers submit systems that address a particular challenge. In the field of NLP, the first shared tasks were initiated in the United States by NIST in collaboration with DARPA (Mariani et al., 2014). Paroubek et al. (2007) report that the first shared tasks – then called evaluation campaigns – focused 5 See Pallett (2003) for an overview of these first shared tasks and the role that NIST played in them. 6 http://www.senseval.org/ 7 http://www.cnts.ua.ac.be/conll99/npb/ 67 task on unseen data. In some cases, the shared task organisers will distinguish between two different tracks for the same shared task depending on the source of the data being used to train the systems. In most cases, all teams in an evaluation test their systems on the same datasets to allow for easier across-the-board"
W17-1608,J08-3010,0,\N,Missing
W17-2004,W14-3348,0,0.107156,"Missing"
W17-2004,W16-3210,0,0.0970905,"Missing"
W17-2004,D16-1025,0,0.0343357,"Missing"
W17-2004,N16-1101,0,0.0938844,"Missing"
W17-2004,W16-2358,0,0.172904,"Missing"
W17-2004,P13-2121,0,0.0480622,"Missing"
W17-2004,W16-2359,1,0.854604,"of product listings, which is in 4 digit numbers even for language models (LMs) trained on in-domain data, as we discuss in §3. This is not only a challenge for LMs but also for automatic evaluation metrics such as the n-gram precisionbased BLEU metric (Papineni et al., 2002). 1 2 MT Models evaluated in this work We first introduce the two text-only baselines used in this work: a PBSMT model (§2.1) and a text-only attention-based NMT model (§2.2). We then briefly discuss the doubly-attentive multi-modal NMT model we use in our experiments (§2.3), which is comparable to the model evaluated by Calixto et al. (2016) and further detailed and analysed in Calixto et al. (2017a). http://www.ebay.com/ 31 Proceedings of the 6th Workshop on Vision and Language, pages 31–37, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics The decoder is also an RNN, more specifically a neural LM (Bengio et al., 2003) conditioned upon its past predictions via its previous hidden state st−1 and the word emitted in the previous time step yt−1 , as well as the source sentence via an attention mechanism. The attention computes a context vector ct for each time step t of the decoder where this vector i"
W17-2004,P16-1227,0,0.120661,"Missing"
W17-2004,P17-1175,1,0.422189,"Missing"
W17-2004,E17-2101,1,0.556858,"eural Machine Translation: a Case Study on E-commerce Listing Titles Iacer Calixto1 , Daniel Stein2 , Evgeny Matusov2 , Sheila Castilho1 and Andy Way1 1 ADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland 2 eBay Inc., Aachen, Germany {iacer.calixto,sheila.castilho,andy.way}@adaptcentre.ie {danstein,ematusov}@ebay.com Abstract The majority of listings are accompanied by a product image, often (but not always) a user-generated shot. Moreover, images are known to bring useful complementary information to MT (Calixto et al., 2012; Hitschler et al., 2016; Huang et al., 2016; Calixto et al., 2017b). Therefore, in order to explore whether product images can benefit the machine translation of auction titles, we evaluate a multi-modal neural MT (NMT) system to eBay’s production system, specifically a phrase-based statistical MT (PBSMT) one. We additionally train a text-only attention-based NMT baseline, so as to be able to measure eventual gains from the additional multi-modal data independently of the MT architecture. According to a quantitative evaluation using a combination of four automatic MT evaluation metrics, a PBSMT system outperforms both text-only and multimodal NMT models in"
W17-2004,W16-2360,0,0.141847,"ion of Multi-modal Neural Machine Translation: a Case Study on E-commerce Listing Titles Iacer Calixto1 , Daniel Stein2 , Evgeny Matusov2 , Sheila Castilho1 and Andy Way1 1 ADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland 2 eBay Inc., Aachen, Germany {iacer.calixto,sheila.castilho,andy.way}@adaptcentre.ie {danstein,ematusov}@ebay.com Abstract The majority of listings are accompanied by a product image, often (but not always) a user-generated shot. Moreover, images are known to bring useful complementary information to MT (Calixto et al., 2012; Hitschler et al., 2016; Huang et al., 2016; Calixto et al., 2017b). Therefore, in order to explore whether product images can benefit the machine translation of auction titles, we evaluate a multi-modal neural MT (NMT) system to eBay’s production system, specifically a phrase-based statistical MT (PBSMT) one. We additionally train a text-only attention-based NMT baseline, so as to be able to measure eventual gains from the additional multi-modal data independently of the MT architecture. According to a quantitative evaluation using a combination of four automatic MT evaluation metrics, a PBSMT system outperforms both text-only and mul"
W17-2004,W14-4012,0,0.131326,"Missing"
W17-2004,P11-2031,0,0.247347,"Missing"
W17-2004,W16-2361,0,0.080277,"Missing"
W17-2004,D15-1166,0,0.0819838,". In their model, local visual features were used instead. In both cases, as well as in this work and in most of the state-of-the-art models in the field, models transferred learning from CNNs pre-trained for image classification on ImageNet (Russakovsky et al., 2015). In NMT, Bahdanau et al. (2015) was the first to propose to use an attention mechanism in the decoder. Their decoder learns to attend to the relevant source-language words as it generates a sentence in the target language, again word by word. Since then, many authors have proposed different ways to incorporate attention into MT. Luong et al. (2015) proposed among other things a local attention mechanism that was less costly than the original global attention; Firat et al. (2016) proposed a model to translate from many source and into many target languages, which involved a shared attention mechanism strategy; Tu et al. (2016) proposed an attention coverage strategy, so that 7 Conclusions and Future Work In this paper, we investigate the potential impact of multi-modal NMT in the context of e-commerce product listings. Images bring important information to NMT models in this context; in fact, translations obtained with a multi-modal NMT"
W17-2004,P03-1021,0,0.0188721,"on about that specific area of the image. The visual attention mechanism computes a context vector it for each time step t of the decoder similarly to the textual attention mechanism described in §2.2: Figure 1: Decoder RNN with attention over source sentence and image features. This decoder learns to independently attend to image patches and source-language words when generating translations. 2.1 Statistical Machine Translation (SMT) We use a PBSMT model where the language model (LM) is a 5–gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) for tuning the model parameters using BLEU as the objective function. 2.2 Multi-modal NMT (NMTm ) img T img img eimg t,l = (va ) tanh(Ua st−1 + Wa al ), Text-only NMT (NMTt ) We use the attention-based NMT model introduced by Bahdanau et al. (2015) as our text-only NMT baseline. It is based on the encoder–decoder framework and it implements an attention mechanism over the source-sentence words X = (x1 , x2 , · · · , xN ), where Y = (y1 , y2 , · · · , yM ) is its target-language translation. A model is trained to maximise the log-likelihood of the target given the source. The encoder is a bidi"
W17-2004,P02-1040,0,0.0997019,"listings’ titles as listed on the eBay main site1 . Among the challenges for MT are the specialized language and grammar for listing titles, as well as a high percentage of user-generated content for non-business sellers, who are often not native speakers themselves. This is reflected on the data by means of extremely high trigram perplexities of product listings, which is in 4 digit numbers even for language models (LMs) trained on in-domain data, as we discuss in §3. This is not only a challenge for LMs but also for automatic evaluation metrics such as the n-gram precisionbased BLEU metric (Papineni et al., 2002). 1 2 MT Models evaluated in this work We first introduce the two text-only baselines used in this work: a PBSMT model (§2.1) and a text-only attention-based NMT model (§2.2). We then briefly discuss the doubly-attentive multi-modal NMT model we use in our experiments (§2.3), which is comparable to the model evaluated by Calixto et al. (2016) and further detailed and analysed in Calixto et al. (2017a). http://www.ebay.com/ 31 Proceedings of the 6th Workshop on Vision and Language, pages 31–37, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics The decoder is also"
W17-2004,Q14-1006,0,0.0228342,"ch containing (i) a product listing in English, (ii) a product listing in German and (iii) a product image. In ∼6k training tuples, the original user-generated product listing was given in English and was manually translated into German by in-house experts. The same holds for validation and test sets, which contain 480 and 444 triples, respectively. In the remaining training tuples (∼18k), the original listing was given in German and manually translated into English. We also use the publicly available Multi30k dataset (Elliott et al., 2016), a multilingual expansion of the original Flickr30k (Young et al., 2014) with ∼30k pictures from Flickr, each accompanied by one description in English and one human translation of the English description into German. Although the curation of in-domain parallel product listings with an associated product image is costly and time-consuming, monolingual German listings with an image are far simpler to obtain. In order to increase the small amount of training data, we train the text-only model NMTt on the German–English eBay24k and Multi30k data sets (without images) and back-translate 83, 832 German in-domain product listings into English. We use the synthetic Engli"
W17-2004,W15-3049,0,0.11233,"Missing"
W17-2004,P16-1162,0,0.0703878,"Missing"
W17-2004,W16-2363,0,0.12724,"aluation of translations generated with the different text-only and multi-modal models. To the best of our knowledge, along with Calixto et al. (2017b) we are the first to study multi-modal NMT applied to the translation of product listings, i.e. for the e-commerce domain. Multi-modal MT has just recently been addressed by the MT community in a shared task (Specia et al., 2016), where many different groups proposed techniques for multi-modal translation using different combinations of NMT and SMT models (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Libovick´y et al., 2016; Shah et al., 2016). In the multimodal translation task, participants are asked to train models to translate image descriptions from one natural language into another, while also taking the image itself into consideration. This effectively bridges the gap between two well-established tasks: image description generation (IDG) and MT. There is an important body of research conducted in IDG. We highlight the work of Vinyals et al. (2015), who proposed an influential neural IDG model based on the sequence-to-sequence framework. They used global visual features to initialise an RNN LM decoder, used to generate the im"
W17-2004,2006.amta-papers.25,0,0.343206,"Missing"
W17-2004,W16-2346,0,0.210848,"Missing"
W17-2004,P16-1008,0,0.0758416,"Missing"
W17-5602,2005.mtsummit-papers.11,0,0.100026,"ain the raw data (e.g., web pages) Although many Natural Language Processing (NLP) applications can be developed by using existing corpora, there are many areas where NLP could be useful if there was a suitable corpus available. For example, Multimodal Machine Translation and Crosslingual Image Description Generation tasks 2 are becoming interested in developing methods that can use not only the texts but also their relations with images. Such information can neither be obtained from standard computer vision data sets such as the COREL collection 3 nor from NLP collections such as Europarl 4 (Koehn, 2005). Similarly, although the image near a text article on a website may provide cues about finding more (ii) align the articles (document alignment) (iii) extract the texts (iv) prepare the corpus for NLP applications (normalisation, tokenisation) (v) map sentences/phrases in one language sentences in the other language (parallel data extraction) In the following, we will describe in detail the acquisition of the Euronews corpus from the website of Euronews. In this work, data is extracted from the available news (image and text modalities) on the Euronews website.5 Figure 1 shows an example of m"
W17-5602,C10-1073,0,0.0277783,"nces from an English–Japanese comparable corpus. They identify similar article pairs, and having considered them as parallel texts, then align sentences using a sentence-pair similarity score and use DP to find the least-cost alignment over the document pair. (Munteanu and Marcu, 2005) use a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using IR techniques. There have been only a few studies trying to investigate the formal quantification of how similar two comparable documents are. Li and Gaussier (2010) presented one of the first works on developing a comparability measure based on the expectation of finding translation word pairs in the corpus. Our approach follows this line of work based on a method developed by Sennrich and Volk (2010). Table 2: Results of bilingual aligned image-text MMEuronews data used in our experiments. publicly for the computer vision and NLP community. 5 Related Work on Document Alignment In the “Big Data” world that we now live in, it is widely believed that there is no better data than more data (e.g. Mayer-Schönberger and Cukier (2013)). In line with this idea,"
W17-5602,J05-4003,0,0.0652639,"the most reliable Chinese translation of an English word. One of the main methods relies on cross-lingual information retrieval (CLIR), with different techniques for transferring the request into the target language (using a bilingual dictionary or a full SMT system). Utiyama and Isahara (2003) use CLIR techniques and DP to extract sentences from an English–Japanese comparable corpus. They identify similar article pairs, and having considered them as parallel texts, then align sentences using a sentence-pair similarity score and use DP to find the least-cost alignment over the document pair. (Munteanu and Marcu, 2005) use a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using IR techniques. There have been only a few studies trying to investigate the formal quantification of how similar two comparable documents are. Li and Gaussier (2010) presented one of the first works on developing a comparability measure based on the expectation of finding translation word pairs in the corpus. Our approach follows this line of work based on a method developed by Sennrich and Volk (2010). Table 2: Results of bi"
W17-5602,J03-3002,0,0.148354,"aired, they are not literal translations one of each other. Thus, extracting parallel data from such corpora requires special algorithms. Many works use the Web as a comparable corpus. An adaptive approach, proposed by Zhao and Vogel (2002), aims at mining parallel sentences from a bilingual comparable news collection collected from the Web. A maximum likelihood criterion was used by combining sentencelength models with lexicon-based models. The translation lexicon is iteratively updated using the mined parallel data to obtain better vocabulary coverage and translation probability estimation. Resnik and Smith (2003) propose a web-miningbased system called STRAND and show that their approach is able to find large numbers of similar document pairs. In (Yang and Li, 2003), an align6 Conclusion Despite the fact that many researchers have investigated the use of comparable corpora to generate initial training data for NLP, we still have a lack of corpus in different modalities. In this paper, we seek to build a corpus that combine aligned images and texts in different languages. We use Euronews website as source of our crawled raw data. We propose a new techniques to align bilingual documents. Our method is b"
W17-5602,2010.amta-papers.14,0,0.134392,"s in French (image and text modalities). These documents can be used to extract comparable documents and parallel data. Euronews web site clusters news into several categories including languages and sub-domains (e.g. Sport, Politics, etc.). Table 2 shows the statistics of our MMEuronews corpus created from news article data from 2013, 2014 and 2105 in 9 languages including: fr(French), ar(Arabic), en(English), de(German), es(Spanish), it(Italian), tr(Turkish), ua(Ukrainian), and pt(Portuguese). 3 3.1 Aligning Comparable documents Basic Idea We propose an extension of the method described in (Sennrich and Volk, 2010) to align our corpus. The basic system architecture is described in Figure 2. We begin by removing the documents that have very little contents in order to reduce the total number of all possible comparisons. Such documents are very rarely considered as candidates for being comparable document because they consist of only few sentences or words and it is observed that in the reference for training data provided, these kind of documents are not included in the reference set. Subsequently, we introduced three methods as follows: (i) sentence-level scoring, (ii) word-level scoring, and (iii) name"
W17-5602,P03-1010,0,0.0276343,"37021 # Aligned 35761 36114 36178 36762 36003 35863 35901 35922 288 504 ment method is presented at different levels (title, word and character) based on dynamic programming (DP). The goal is to identify one-to-one title pairs in an English–Chinese corpus collected from the Web. They apply the longest common subsequence to find the most reliable Chinese translation of an English word. One of the main methods relies on cross-lingual information retrieval (CLIR), with different techniques for transferring the request into the target language (using a bilingual dictionary or a full SMT system). Utiyama and Isahara (2003) use CLIR techniques and DP to extract sentences from an English–Japanese comparable corpus. They identify similar article pairs, and having considered them as parallel texts, then align sentences using a sentence-pair similarity score and use DP to find the least-cost alignment over the document pair. (Munteanu and Marcu, 2005) use a bilingual lexicon to translate some of the words of the source sentence. These translations are then used to query the database to find matching translations using IR techniques. There have been only a few studies trying to investigate the formal quantification o"
W18-1808,W14-3348,0,0.0183833,"roceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 84 4.2 Resources and tools All the translation models are built by using the widely used open source Moses SMT toolkit (Koehn et al. (2007)). The word and phrase alignments are obtained by using the Giza++ tool (Och and Ney (2003)). Once the translation models are built, we tune all the sentiment translation systems for both experimental set-ups (Exp1 and Exp2) via minimum error rate training (Och (2003)). 4.3 Evaluation process We use the automatic MT evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. (2006)) to evaluate the absolute translation quality obtained. We measure the sentiment preservation score by calculating what percentage of the tweets belongs to the same sentiment class before and after translation. 5 Results Table 2 shows the results obtained from the ﬁrst experimental setup (Exp1) which is comparable with the previous results obtained in the work of Lohar et al. (2017) because the data distribution is the same (i.e. 150 tweet pairs for each of the development and test data sets). System (Exp1) Twitter Baseline Twitter SentClass Twitter NearSent BLE"
W18-1808,W13-1109,0,0.0336,"concludes together with some avenues for future work. 2 Related work Translating UGC creates new challenges in the area of MT. Jiang et al. (2012) describe how to handle shortforms, acronyms, typos, punctuation errors, non-dictionary slang, wordplay, censor avoidance and emoticons, phenomena which are characteristic of UGC but not of ‘normal’ written forms in language. The combination of statistical machine translation (SMT) and a preprocessor was also applied to remove a signiﬁcant amount of noise from tweets in order to convert them into a more readable format (Kaufmann and Kalita (2010)). Gotti et al. (2013) use an SMT system to translate Twitter feeds published by agencies and organisations. They create tuning and training sets by mining parallel web pages linked from the URLs contained in English–French pairs of tweets. There exists quite a lot of research in the area of sentiment analysis of UGC. For example, 1 Recently released in Lohar et al. (2018) and available at: https://github.com/HAfli/FooTweets_ Corpus Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 82 Fang and Zhan (2015) analyse the sentiment polarity of online product reviews extracted from Ama"
W18-1808,2012.amta-commercial.8,1,0.741844,"tive to neutral (or vice-versa) or from positive to neutral (or vice-versa)), but rarely from positive to negative (or vice-versa). The remainder of this paper is organised as follows. We brieﬂy describe some relevant related work in Section 2. In Section 3, we provide an architectural overview of our sentiment classiﬁcation MT system. The experimental set ups are discussed in Section 4, followed by a detailed discussion of the results in Section 5. Finally, Section 6 concludes together with some avenues for future work. 2 Related work Translating UGC creates new challenges in the area of MT. Jiang et al. (2012) describe how to handle shortforms, acronyms, typos, punctuation errors, non-dictionary slang, wordplay, censor avoidance and emoticons, phenomena which are characteristic of UGC but not of ‘normal’ written forms in language. The combination of statistical machine translation (SMT) and a preprocessor was also applied to remove a signiﬁcant amount of noise from tweets in order to convert them into a more readable format (Kaufmann and Kalita (2010)). Gotti et al. (2013) use an SMT system to translate Twitter feeds published by agencies and organisations. They create tuning and training sets by m"
W18-1808,P07-2045,0,0.032631,"ental set-ups (Exp1 and Exp2). We hope that by slightly increasing the size of the development and test sets, our analysis of the performance of the proposed system with these two different set-ups will be somewhat more informative. Exp. setup Exp1 Exp2 Train 3, 700 3, 400 #neg. 50 100 Development #neu. 50 100 #pos. 50 100 #neg. 50 100 Test #neu. 50 100 #pos. 50 100 Table 1: Data statistics Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 84 4.2 Resources and tools All the translation models are built by using the widely used open source Moses SMT toolkit (Koehn et al. (2007)). The word and phrase alignments are obtained by using the Giza++ tool (Och and Ney (2003)). Once the translation models are built, we tune all the sentiment translation systems for both experimental set-ups (Exp1 and Exp2) via minimum error rate training (Och (2003)). 4.3 Evaluation process We use the automatic MT evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. (2006)) to evaluate the absolute translation quality obtained. We measure the sentiment preservation score by calculating what percentage of the tweets belongs to the same s"
W18-1808,L18-1422,1,0.801994,"forms in language. The combination of statistical machine translation (SMT) and a preprocessor was also applied to remove a signiﬁcant amount of noise from tweets in order to convert them into a more readable format (Kaufmann and Kalita (2010)). Gotti et al. (2013) use an SMT system to translate Twitter feeds published by agencies and organisations. They create tuning and training sets by mining parallel web pages linked from the URLs contained in English–French pairs of tweets. There exists quite a lot of research in the area of sentiment analysis of UGC. For example, 1 Recently released in Lohar et al. (2018) and available at: https://github.com/HAfli/FooTweets_ Corpus Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 82 Fang and Zhan (2015) analyse the sentiment polarity of online product reviews extracted from Amazon.com using both sentence-level and review-level categorization techniques. Gr¨abner et al. (2012) classify customer reviews of hotels by extracting a domain-speciﬁc lexicon of semantically relevant words based on a given corpus (Scharl et al. (2003); Pak and Paroubek (2010)). Broß (2013) focus on the following two main subtasks of aspect-oriented r"
W18-1808,P03-1021,0,0.0873752,"g. 50 100 Development #neu. 50 100 #pos. 50 100 #neg. 50 100 Test #neu. 50 100 #pos. 50 100 Table 1: Data statistics Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 84 4.2 Resources and tools All the translation models are built by using the widely used open source Moses SMT toolkit (Koehn et al. (2007)). The word and phrase alignments are obtained by using the Giza++ tool (Och and Ney (2003)). Once the translation models are built, we tune all the sentiment translation systems for both experimental set-ups (Exp1 and Exp2) via minimum error rate training (Och (2003)). 4.3 Evaluation process We use the automatic MT evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. (2006)) to evaluate the absolute translation quality obtained. We measure the sentiment preservation score by calculating what percentage of the tweets belongs to the same sentiment class before and after translation. 5 Results Table 2 shows the results obtained from the ﬁrst experimental setup (Exp1) which is comparable with the previous results obtained in the work of Lohar et al. (2017) because the data distribution is the same (i.e."
W18-1808,J03-1002,0,0.0102216,"nt and test sets, our analysis of the performance of the proposed system with these two different set-ups will be somewhat more informative. Exp. setup Exp1 Exp2 Train 3, 700 3, 400 #neg. 50 100 Development #neu. 50 100 #pos. 50 100 #neg. 50 100 Test #neu. 50 100 #pos. 50 100 Table 1: Data statistics Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 84 4.2 Resources and tools All the translation models are built by using the widely used open source Moses SMT toolkit (Koehn et al. (2007)). The word and phrase alignments are obtained by using the Giza++ tool (Och and Ney (2003)). Once the translation models are built, we tune all the sentiment translation systems for both experimental set-ups (Exp1 and Exp2) via minimum error rate training (Och (2003)). 4.3 Evaluation process We use the automatic MT evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. (2006)) to evaluate the absolute translation quality obtained. We measure the sentiment preservation score by calculating what percentage of the tweets belongs to the same sentiment class before and after translation. 5 Results Table 2 shows the results obtained f"
W18-1808,pak-paroubek-2010-twitter,0,0.0560778,"h in the area of sentiment analysis of UGC. For example, 1 Recently released in Lohar et al. (2018) and available at: https://github.com/HAfli/FooTweets_ Corpus Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 82 Fang and Zhan (2015) analyse the sentiment polarity of online product reviews extracted from Amazon.com using both sentence-level and review-level categorization techniques. Gr¨abner et al. (2012) classify customer reviews of hotels by extracting a domain-speciﬁc lexicon of semantically relevant words based on a given corpus (Scharl et al. (2003); Pak and Paroubek (2010)). Broß (2013) focus on the following two main subtasks of aspect-oriented review mining: (i) identiﬁcation of the relevant product aspects, and (ii) determining and classifying the expressions of the sentiment. Some existing work applies MT for the task of sentiment analysis. For example, Mohammad et al. (2016) show that the sentiment analysis of English translations of Arabic text produces competitive results compared to Arabic sentiment analysis per se. In a similar vein, Araujo et al. (2016) reveal that simply translating the non-English input text into English and using the English sentim"
W18-1808,P02-1040,0,0.102399,"50 100 Table 1: Data statistics Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 84 4.2 Resources and tools All the translation models are built by using the widely used open source Moses SMT toolkit (Koehn et al. (2007)). The word and phrase alignments are obtained by using the Giza++ tool (Och and Ney (2003)). Once the translation models are built, we tune all the sentiment translation systems for both experimental set-ups (Exp1 and Exp2) via minimum error rate training (Och (2003)). 4.3 Evaluation process We use the automatic MT evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. (2006)) to evaluate the absolute translation quality obtained. We measure the sentiment preservation score by calculating what percentage of the tweets belongs to the same sentiment class before and after translation. 5 Results Table 2 shows the results obtained from the ﬁrst experimental setup (Exp1) which is comparable with the previous results obtained in the work of Lohar et al. (2017) because the data distribution is the same (i.e. 150 tweet pairs for each of the development and test data sets). System (Exp1) Twitter Baseline T"
W18-1808,2006.amta-papers.25,0,0.0329862,"esearch Track Boston, March 17 - 21, 2018 |Page 84 4.2 Resources and tools All the translation models are built by using the widely used open source Moses SMT toolkit (Koehn et al. (2007)). The word and phrase alignments are obtained by using the Giza++ tool (Och and Ney (2003)). Once the translation models are built, we tune all the sentiment translation systems for both experimental set-ups (Exp1 and Exp2) via minimum error rate training (Och (2003)). 4.3 Evaluation process We use the automatic MT evaluation metrics BLEU (Papineni et al. (2002)), METEOR (Denkowski and Lavie (2014)) and TER (Snover et al. (2006)) to evaluate the absolute translation quality obtained. We measure the sentiment preservation score by calculating what percentage of the tweets belongs to the same sentiment class before and after translation. 5 Results Table 2 shows the results obtained from the ﬁrst experimental setup (Exp1) which is comparable with the previous results obtained in the work of Lohar et al. (2017) because the data distribution is the same (i.e. 150 tweet pairs for each of the development and test data sets). System (Exp1) Twitter Baseline Twitter SentClass Twitter NearSent BLEU 50.3 48.2 49.0 METEOR 60.9 59"
W18-2202,L16-1090,0,0.304988,"Missing"
W18-2202,D16-1025,0,0.0317377,"human translators), and as such, progress in this area in terms of advances in state-of-the-art approaches is of interest to us. For many years, there have been extensive studies to show how the integration of MT within such a workflow (often complementary to the use of translation memory tools) improves productivity, both in industry-based and in academic-based research (e.g. Etchegoyhen et al. (2014); Arenas (2008)). With the introduction of NMT methods, there have been subsequent studies examining the differences between the impact that SMT and NMT have within such a setting. For example, Bentivogli et al. (2016) carried out a small scale study on post-editing of English→German translated TED talks, and concluded that NMT had made significantly positive changes in the field. Bojar et al. (2016) report a significant step forward using NMT instead of SMT in the automatic post-editing tasks at the Conference on Statistical Machine Translation (WMT16). More recently, Castilho et al. (2017) carried out a more extensive quantitative and qualitative comparative evaluation of PBSMT and NMT using automatic metrics and professional translators. Results were mixed overall. They varied from showing positive resul"
W18-2202,W16-2301,0,0.0131159,"w the integration of MT within such a workflow (often complementary to the use of translation memory tools) improves productivity, both in industry-based and in academic-based research (e.g. Etchegoyhen et al. (2014); Arenas (2008)). With the introduction of NMT methods, there have been subsequent studies examining the differences between the impact that SMT and NMT have within such a setting. For example, Bentivogli et al. (2016) carried out a small scale study on post-editing of English→German translated TED talks, and concluded that NMT had made significantly positive changes in the field. Bojar et al. (2016) report a significant step forward using NMT instead of SMT in the automatic post-editing tasks at the Conference on Statistical Machine Translation (WMT16). More recently, Castilho et al. (2017) carried out a more extensive quantitative and qualitative comparative evaluation of PBSMT and NMT using automatic metrics and professional translators. Results were mixed overall. They varied from showing positive results for NMT in terms of improved (perceived) fluency and errors, to achieving no particular gains over SMT at document level for post-editing. While these studies were carried out on bet"
W18-2202,W17-4123,0,0.0233514,"ditional processing has been carried out to date. In future experiments, we hope to investigate methods for tailoring NMT to this particular domain and language pair. A possible avenue of research to explore is the inclusion of linguistic features in NMT such as the work carried out by Sennrich and Haddow (2016). We wish to address over-translation issues discussed in Section 5, possibly with the use of coverage vectors (Tu et al., 2016). Another approach worth considering is addressing the divergent word order in the EN-GA language pair with a pre-reordering approach such as the one taken by Du and Way (2017). Methods which address data sparsity will also be investigated – options include the use of back translation (Sennrich et al., 2016a) and/or data augmentation (Fadaee et al., 2017). In addition, it will be important in the future to include human evaluation in our studies to ensure that the MT systems designed for public administration use will be optimised to enhance the task of a human translator, and will not merely be tuned to automatic metrics. Finally, the derogation on the production of Irish language documents within the EU is Proceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, Ma"
W18-2202,etchegoyhen-etal-2014-machine,0,0.0577894,"Missing"
W18-2202,W11-2123,0,0.0608162,"ege Dublin Proceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, March 17 - 21, 2018 |Page 14 Data–set Paradocs UT† # of words 1,596,569 15,377 # of sentences 98,758 598 Table 2: Additional monolingual (GA) text used for training the SMT language model 4 Experiment Set–Up 4.1 SMT To attain the most up-to-date results for this use-case, we train a phrase-based SMT system using Moses (Koehn et al., 2007) with the training data described in Section 3. Earlier findings showed that a 6-gram language model helps address divergent word order in EN-GA (Dowling et al., 2015). We therefore use KenLM (Heafield, 2011) to train a 6-gram language model with the monolingual data outlined in table 1. In addition, we implement hierarchical re-ordering tables to address issues surrounding word order. Our earlier system was tailored to address some consistent errors that arose from data sparsity, which resulted from inflectional variations. We took steps to reduce the repetitive task of the translator in correcting these slight orthographic changes at the token level. Our approach involved the introduction of an automated post-editing (APE) module in the pipeline, which consists of hand-coded grammar rules (Dowli"
W18-2202,P17-4012,0,0.0566675,"anslator in correcting these slight orthographic changes at the token level. Our approach involved the introduction of an automated post-editing (APE) module in the pipeline, which consists of hand-coded grammar rules (Dowling et al., 2016). In order to maximise consistency with our previous work, we chose to include this APE module in our MT experiments. 4.2 NMT Baseline In order to provide a preliminary NMT baseline for EN-GA in this domain, we implement a ‘vanilla’ NMT system, i.e. using default parameters where possible (this system is referred to as NMT-base in Figure 1). We use OpenNMT (Klein et al., 2017), which is an implementation of the popular NMT approach that uses an attentional encoder-decoder network (Bahdanau et al., 2014). We train a 2-layer LSTM with 500 hidden layers for 13 epochs. For the sake of comparison we use the same training data as used in the SMT system (see Table 1). The resulting vocabulary size is 50,002 (English) and 50,004 (Irish). Note that we also apply the APE module to the output of the NMT system. Further NMT experiments To add to this baseline system, we also perform a few preliminary experiments to investigate the affect that altering parameters or using other"
W18-2202,P07-2045,0,0.00634013,"sc.ie (Website for the state agency providing research, advisory and education in agriculture, horticulture, food and rural development in Ireland) 6 The University Times is a university newspaper in Trinity College Dublin Proceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, March 17 - 21, 2018 |Page 14 Data–set Paradocs UT† # of words 1,596,569 15,377 # of sentences 98,758 598 Table 2: Additional monolingual (GA) text used for training the SMT language model 4 Experiment Set–Up 4.1 SMT To attain the most up-to-date results for this use-case, we train a phrase-based SMT system using Moses (Koehn et al., 2007) with the training data described in Section 3. Earlier findings showed that a 6-gram language model helps address divergent word order in EN-GA (Dowling et al., 2015). We therefore use KenLM (Heafield, 2011) to train a 6-gram language model with the monolingual data outlined in table 1. In addition, we implement hierarchical re-ordering tables to address issues surrounding word order. Our earlier system was tailored to address some consistent errors that arose from data sparsity, which resulted from inflectional variations. We took steps to reduce the repetitive task of the translator in corr"
W18-2202,W17-3204,0,0.265819,"ing et al., 2015).4 The question remains whether NMT can 1 Most (if not all) Irish speakers have fluency in English. 2 http://publications.europa.eu/code/en/en-370204.htm 3 https://ec.europa.eu/cefdigital/wiki/display/CEFDIGITAL/Machine+Translation 4 Results: BLEU .43/ TER .46 Proceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, March 17 - 21, 2018 |Page 12 achieve a similar level of usability for Irish in this setting. While the introduction of deep learning methods to the field of MT has witnessed a breakthrough in recent years, the positive impact of NMT is not felt across the board. As Koehn and Knowles (2017) highlight, current NMT systems can face a number of challenges when dealing with specific tasks. These challenges include low-resourced languages, low-frequency words arising from inflection, long sentences, and out-of-domain texts. The latter may not apply to our test case, as the success of our earlier SMT system lies in the closed domain nature of the use case (public administration data), yet the other factors are very real for the Irish language in general. In this study, we report on recent scores from the training of an updated Irish SMT engine, based on our latest data sets. We then p"
W18-2202,D15-1166,0,0.0696037,"can have in real-life translation scenarios. Aside from examining the impact on translator productivity, there has also been increased focus in addressing the shortcomings of NMT, such as those outlined by Koehn and Knowles (2017). As such, a number of innovative approaches have emerged to this end. The application of various transfer learning methods has proven successful for certain low-resourced language (Zoph et al., 2016; Passban et al., 2017), as has the inclusion of linguistic features when addressing data sparsity that faces morphologically rich languages (Sennrich and Haddow, 2016). Luong et al. (2015) show that the use of attention-based NMT can have positive results in many aspects of MT, including the handling of long sentences. In the case of Irish language, the lack of sufficient data, along with a lack of skilled reProceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, March 17 - 21, 2018 |Page 13 sources has resulted in limited progress in the area of English-Irish (EN-GA) MT to date: As discussed in Section 1, a domain specific (public administration) SMT system is currently in use by in-house translators in the Department of Culture, Heritage and the Gaeltacht (DCHG) (Dowling et a"
W18-2202,W13-2506,0,0.0203521,"f notices, annual reports, website content, press releases and official correspondence. We supplement the existing corpus with additional recently translated in-domain data provided by the DCHG. Parallel texts from two EU bodies: the Digital Corpus of the European Parliament (DCEP) and Directorate General for Translation, Translation Memories (DGT-TM) are included in the training data (referred to collectively as ‘EU’ in Table 1). In addition, we include data crawled from websites5 that were deemed to contain text from a domain similar to public administration (using the ILSP Focused Crawler (Papavassiliou et al., 2013)). Finally, we contribute a new parallel corpus, which was collected from Conradh na Gaeilge (CnaG), an Irish language organisation which promotes the Irish language in Ireland. Monolingual data – language model SMT engines require additional monolingual data in order to train a language model that helps to improve the fluency of the SMT output. This monolingual data does not necessarily need to be in-domain, and thus our language model is trained not only on the GA data used for the translation model, but also on a combination of two additional out-of-domain data sets: ‘Paradocs’, a corpus of"
W18-2202,W16-2209,0,0.128158,"the change in MT approaches can have in real-life translation scenarios. Aside from examining the impact on translator productivity, there has also been increased focus in addressing the shortcomings of NMT, such as those outlined by Koehn and Knowles (2017). As such, a number of innovative approaches have emerged to this end. The application of various transfer learning methods has proven successful for certain low-resourced language (Zoph et al., 2016; Passban et al., 2017), as has the inclusion of linguistic features when addressing data sparsity that faces morphologically rich languages (Sennrich and Haddow, 2016). Luong et al. (2015) show that the use of attention-based NMT can have positive results in many aspects of MT, including the handling of long sentences. In the case of Irish language, the lack of sufficient data, along with a lack of skilled reProceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, March 17 - 21, 2018 |Page 13 sources has resulted in limited progress in the area of English-Irish (EN-GA) MT to date: As discussed in Section 1, a domain specific (public administration) SMT system is currently in use by in-house translators in the Department of Culture, Heritage and the Gaeltacht"
W18-2202,P16-1009,0,0.303037,"stochastic optimisation (Kinga and Adam, 2015). This method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. We implement this method using the recommended learning rate for Adam (0.001) and denote this system in Table 3 as NMT+ADAM. • NMT+BPE In order to address the inflectional nature of the Irish language, we experiment with the use of byte-pair encoding (BPE). BPE is a technique presented by Gage (1994) Proceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, March 17 - 21, 2018 |Page 15 and adapted for NMT by Sennrich et al. (2016b). In terms of MT, it aims to increase vocabulary coverage by encoding rare and unknown words as sequences of subword units. As data sparsity is an issue especially relevant to a low-resourced inflectional language such as Irish, reducing out of vocabulary (OOV) words is a promising technique. This system is referred to as NMT+BPE in Table 3 and Figure 1. 5 Results and Preliminary Analysis 50 score 45 40 35 BLEU BLEU+APE Figure 1: Bar graph displaying the BLEU scores of the SMT and NMT systems, with and without the APE module applied. Both the SMT and NMT systems were tested on the same test"
W18-2202,P16-1162,0,0.450354,"stochastic optimisation (Kinga and Adam, 2015). This method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. We implement this method using the recommended learning rate for Adam (0.001) and denote this system in Table 3 as NMT+ADAM. • NMT+BPE In order to address the inflectional nature of the Irish language, we experiment with the use of byte-pair encoding (BPE). BPE is a technique presented by Gage (1994) Proceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, March 17 - 21, 2018 |Page 15 and adapted for NMT by Sennrich et al. (2016b). In terms of MT, it aims to increase vocabulary coverage by encoding rare and unknown words as sequences of subword units. As data sparsity is an issue especially relevant to a low-resourced inflectional language such as Irish, reducing out of vocabulary (OOV) words is a promising technique. This system is referred to as NMT+BPE in Table 3 and Figure 1. 5 Results and Preliminary Analysis 50 score 45 40 35 BLEU BLEU+APE Figure 1: Bar graph displaying the BLEU scores of the SMT and NMT systems, with and without the APE module applied. Both the SMT and NMT systems were tested on the same test"
W18-2202,P16-1008,0,0.136687,"ar, even to those unfamiliar with the Irish language, why the SMT output prevails in this case. The first phrase in this example is translated perfectly, when compared to the reference – meaning that it is likely that this exact phrase or very similar phrases are present in the training data, and the SMT system is therefore well-equipped to translate it. Looking at the NMT output we can see that a phenomenon, not uncommon in NMT, has occurred: the translations for ‘request’ and ‘receipt’ are repeated unnecessarily (‘iarratas’ and ‘faighte’). This is sometimes referred to as ‘overtranslation’ (Tu et al., 2016) and can pose problems for NMT quality. Examples 3 and 4 show cases where NMT produces translations with a higher BLEU score than that of the SMT system. In Example 3, NMT outputs a more accurate verb (cabhra´ıonn ‘assists’) as opposed to the SMT output (taca´ıonn ‘supports’), and in fact achieves an almost perfect translation (freisin ‘also’ being a synonym for chomh maith ‘as well’). It also chooses the correct inflection for haon ‘any’, which the SMT system fails to do (outputting aon). The h inflection is required following the vowel ending on the preceding preposition le ‘with’. In Exampl"
W18-2202,D16-1163,0,0.0260478,"out on better resourced language pairs (English→German, Portuguese, Russian and Greek), they are still highly relevant in indicating the potential impact that the change in MT approaches can have in real-life translation scenarios. Aside from examining the impact on translator productivity, there has also been increased focus in addressing the shortcomings of NMT, such as those outlined by Koehn and Knowles (2017). As such, a number of innovative approaches have emerged to this end. The application of various transfer learning methods has proven successful for certain low-resourced language (Zoph et al., 2016; Passban et al., 2017), as has the inclusion of linguistic features when addressing data sparsity that faces morphologically rich languages (Sennrich and Haddow, 2016). Luong et al. (2015) show that the use of attention-based NMT can have positive results in many aspects of MT, including the handling of long sentences. In the case of Irish language, the lack of sufficient data, along with a lack of skilled reProceedings of AMTA 2018 Workshop: LoResMT 2018 Boston, March 17 - 21, 2018 |Page 13 sources has resulted in limited progress in the area of English-Irish (EN-GA) MT to date: As discussed"
W18-6312,D16-1025,0,0.0446923,"nal translators against those of non-experts and discover that those of the experts result in higher inter-annotator agreement and better discrimination between human and machine translations. In addition, we analyse the human translations of the test set and identify important translation issues. Finally, based on these findings, we provide a set of recommendations for future human evaluations of MT. 1 Introduction Neural machine translation (NMT) has revolutionised the field of MT by overcoming many of the weaknesses of the previous state-of-the-art phrase-based machine translation (PBSMT) (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017). In only a few years since the first working models, this approach has led to a substantial improvement in translation quality, reported in terms of automatic metrics (Bojar et al., 2016, 2017; Sennrich et al., 2016). This has ignited higher levels of expectation, fuelled in part by hyperbolic claims from large MT developers. First we saw in Wu et al. (2016) that Google NMT was “bridging the gap between human and machine translation [quality]”. This was amplified 1 https://www.sdl.com/about/news-media/press/2018/sdlcracks-russian-to-english-neural-machine-"
W18-6312,D17-1301,1,0.828365,"we evaluate sentences in consecutive order (rather than randomly). This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences Context Hassan et al. (2018) evaluated the sentences in the testset in randomised order, meaning that sentences were evaluated in isolation. However, documents such as the news stories that make up the test set contain relations that go beyond the sentence level. To translate them correctly one needs to take this inter-sentential context into account (Voigt and Jurafsky, 2012; Wang et al., 2017a). The MT system by Hassan et al. (2018) translates sentences in isolation while humans naturally consider the wider context when conducting translation. Our hypothesis is that referential relations that go beyond the sentence-level were ignored in the evaluation as its setup considered sentences in isolation (randomised). This probably resulted in the evaluation missing some errors by the MT system that might have been caused by its lack of inter-sentential contextual knowledge. In contrast, our revised human evaluation takes intersentential context into account. Sentences are not randomised"
W18-6312,W17-4742,0,0.0316018,"we evaluate sentences in consecutive order (rather than randomly). This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences Context Hassan et al. (2018) evaluated the sentences in the testset in randomised order, meaning that sentences were evaluated in isolation. However, documents such as the news stories that make up the test set contain relations that go beyond the sentence level. To translate them correctly one needs to take this inter-sentential context into account (Voigt and Jurafsky, 2012; Wang et al., 2017a). The MT system by Hassan et al. (2018) translates sentences in isolation while humans naturally consider the wider context when conducting translation. Our hypothesis is that referential relations that go beyond the sentence-level were ignored in the evaluation as its setup considered sentences in isolation (randomised). This probably resulted in the evaluation missing some errors by the MT system that might have been caused by its lack of inter-sentential contextual knowledge. In contrast, our revised human evaluation takes intersentential context into account. Sentences are not randomised"
W18-6312,N15-1124,0,0.0458269,"sentences, they see them in order. We randomised the documents in the test set (169) and prepared one evaluation task per document, for the first 49 documents (503 sentences). Of these 49 documents, 41 were originally written in ZH (amounting to 299 sentences, with each document containing 7.3 sentences on average) and the remaining 8 were originally written in EN (204 sentences, average of 25.5 sentences per document). Evaluators were asked to annotate all the sentences of each document in one go, so that they can take intersentential context into account. Rather than direct assessment (DA) (Graham et al., 2015), as in Hassan et al. (2018), we conduct a relative ranking evaluation. While DA has some advantages over ranking and has replaced the latter at the WMT shared task since 2017 (Bojar et al., 2017), ranking is more appropriate for our evaluation due to the fact that we evaluate sentences in consecutive order (rather than randomly). This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences Context Hassan et al. (2018) evaluated the sentences in the testset in randomised order, meaning that sentences wer"
W18-6312,D18-1512,0,0.0886803,"Missing"
W18-6312,W16-2323,0,0.0468048,"and identify important translation issues. Finally, based on these findings, we provide a set of recommendations for future human evaluations of MT. 1 Introduction Neural machine translation (NMT) has revolutionised the field of MT by overcoming many of the weaknesses of the previous state-of-the-art phrase-based machine translation (PBSMT) (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017). In only a few years since the first working models, this approach has led to a substantial improvement in translation quality, reported in terms of automatic metrics (Bojar et al., 2016, 2017; Sennrich et al., 2016). This has ignited higher levels of expectation, fuelled in part by hyperbolic claims from large MT developers. First we saw in Wu et al. (2016) that Google NMT was “bridging the gap between human and machine translation [quality]”. This was amplified 1 https://www.sdl.com/about/news-media/press/2018/sdlcracks-russian-to-english-neural-machine-translation.html 2 http://aka.ms/Translator-HumanParityData 113 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 113–123 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computati"
W18-6312,E17-1100,1,0.881329,"Missing"
W18-6312,W12-2503,0,0.0411092,"ation due to the fact that we evaluate sentences in consecutive order (rather than randomly). This can be accommodated in ranking as we can show all three translations for each source sentence together with the previous and next source sentences Context Hassan et al. (2018) evaluated the sentences in the testset in randomised order, meaning that sentences were evaluated in isolation. However, documents such as the news stories that make up the test set contain relations that go beyond the sentence level. To translate them correctly one needs to take this inter-sentential context into account (Voigt and Jurafsky, 2012; Wang et al., 2017a). The MT system by Hassan et al. (2018) translates sentences in isolation while humans naturally consider the wider context when conducting translation. Our hypothesis is that referential relations that go beyond the sentence-level were ignored in the evaluation as its setup considered sentences in isolation (randomised). This probably resulted in the evaluation missing some errors by the MT system that might have been caused by its lack of inter-sentential contextual knowledge. In contrast, our revised human evaluation takes intersentential context into account. Sentences"
W18-6323,D11-1033,0,0.0843934,". Secondly, we could select data with the aim to make trained systems perform well for specific domains. In MT, models built with in-domain data perform better, as the vocabulary and sentence structures used in one domain (e.g. legal) differs from another unrelated domain (e.g. biotechnology). There are several studies on data selection methods for SMT, showing good improvements over the baselines in which the whole corpora were used for training (Chen et al., 2016). A popular data selection method is cross-entropy difference (CED) (Moore and Lewis, 2010). In particular its bilingual variant (Axelrod et al., 2011) showed a positive impact of data selection for MT. Term Frequency-Inverse Document Frequency (TF-IDF) (Salton and Yang, 1973) has also been used as a baseline data selection method in the literature. Data selection with cleaning was proposed to improve the robustness of training with divergent sentences (Carpuat et al., 2017). Feature Decay Algorithms (FDA) are data selection methods that try to extract the subset of sentences by which the coverage of target language features is maximized (Bic¸ici and Yuret, 2011). It has been used to select sentences from parallel data for SMT and NMT (Ponce"
W18-6323,W13-2206,0,0.108141,"Missing"
W18-6323,W11-2131,0,0.124156,"Missing"
W18-6323,W17-3209,0,0.0185578,"selection methods for SMT, showing good improvements over the baselines in which the whole corpora were used for training (Chen et al., 2016). A popular data selection method is cross-entropy difference (CED) (Moore and Lewis, 2010). In particular its bilingual variant (Axelrod et al., 2011) showed a positive impact of data selection for MT. Term Frequency-Inverse Document Frequency (TF-IDF) (Salton and Yang, 1973) has also been used as a baseline data selection method in the literature. Data selection with cleaning was proposed to improve the robustness of training with divergent sentences (Carpuat et al., 2017). Feature Decay Algorithms (FDA) are data selection methods that try to extract the subset of sentences by which the coverage of target language features is maximized (Bic¸ici and Yuret, 2011). It has been used to select sentences from parallel data for SMT and NMT (Poncelas et al., 2018) in order to obtain a subset of data that is more tailored to a given test set. Most of these results focused on comparing training of models from scratch for use in specific domains. The aforementioned papers do not include a focus on the impact of such techniques in finetuning the resulting trained model, wh"
W18-6323,2016.amta-researchers.8,0,0.0331314,"tly. In this study, we reviewed three data selection approaches for MT, namely Term Frequency– Inverse Document Frequency, Cross-Entropy Difference and Feature Decay Algorithm, and conducted experiments on Neural Machine Translation (NMT) with the selected data using the three approaches. The results showed that for NMT systems, using data selection also improved the performance, though the gain is not as much as for SMT systems. 1 Introduction Data selection is a technology used to improve Machine Translation (MT) performance by choosing a subset of the corpus for the training of MT systems (Chen et al., 2016). There are additional benefits using subsets instead of the whole corpus for MT training. Firstly, the training time could be reduced significantly. In some application scenarios, a much shorter training time would be very useful. Secondly, we could select data with the aim to make trained systems perform well for specific domains. In MT, models built with in-domain data perform better, as the vocabulary and sentence structures used in one domain (e.g. legal) differs from another unrelated domain (e.g. biotechnology). There are several studies on data selection methods for SMT, showing good i"
W18-6323,P13-2119,0,0.050181,"th out-of-domain data, and an in-domain language-model LMD , the method ranks sentences s using the cross-entropy difference in both language models, as in (2): CED(s) = HD (s) − HG (s) (2) Although different ranking methods have been introduced, this method still remains popular among data selection approaches, having been used in recent work such as for the selection of monolingual data (Junczys-Dowmunt and Grundkiewicz, 2016), and for the selection of conversational data (Lewis and Federmann, 2015). Some work was also published on the use of neural language models for this purpose, such as Duh et al. (2013), but this applied to Statistical Machine Translation. In our experiments, we built n-gram language models of order 5 using the KenLM tool1 (Heafield, 1 https://github.com/kpu/kenlm 2.2 TF-IDF data selection The TF-IDF (Salton and Yang, 1973) method is widely known for its use in several information retrieval applications. It is defined in (3), where tft,d is the term frequency in the document, i.e. the ratio between the number of times the term appears in the sentence and the total number of terms, and idft,d is the inverse document frequency, the ratio between the total number of documents a"
W18-6323,W11-2123,0,0.0569989,"Missing"
W18-6323,W16-2378,0,0.0281316,"ant of scoring by perplexity, since cross-entropy and perplexity are tightly coupled as shown in 1, where b is the used base. b− P x ·p(x) log q(x) = bH(p,q) (1) Given a general language model LMG , built with out-of-domain data, and an in-domain language-model LMD , the method ranks sentences s using the cross-entropy difference in both language models, as in (2): CED(s) = HD (s) − HG (s) (2) Although different ranking methods have been introduced, this method still remains popular among data selection approaches, having been used in recent work such as for the selection of monolingual data (Junczys-Dowmunt and Grundkiewicz, 2016), and for the selection of conversational data (Lewis and Federmann, 2015). Some work was also published on the use of neural language models for this purpose, such as Duh et al. (2013), but this applied to Statistical Machine Translation. In our experiments, we built n-gram language models of order 5 using the KenLM tool1 (Heafield, 1 https://github.com/kpu/kenlm 2.2 TF-IDF data selection The TF-IDF (Salton and Yang, 1973) method is widely known for its use in several information retrieval applications. It is defined in (3), where tft,d is the term frequency in the document, i.e. the ratio be"
W18-6323,2015.iwslt-papers.2,0,0.0248284,"shown in 1, where b is the used base. b− P x ·p(x) log q(x) = bH(p,q) (1) Given a general language model LMG , built with out-of-domain data, and an in-domain language-model LMD , the method ranks sentences s using the cross-entropy difference in both language models, as in (2): CED(s) = HD (s) − HG (s) (2) Although different ranking methods have been introduced, this method still remains popular among data selection approaches, having been used in recent work such as for the selection of monolingual data (Junczys-Dowmunt and Grundkiewicz, 2016), and for the selection of conversational data (Lewis and Federmann, 2015). Some work was also published on the use of neural language models for this purpose, such as Duh et al. (2013), but this applied to Statistical Machine Translation. In our experiments, we built n-gram language models of order 5 using the KenLM tool1 (Heafield, 1 https://github.com/kpu/kenlm 2.2 TF-IDF data selection The TF-IDF (Salton and Yang, 1973) method is widely known for its use in several information retrieval applications. It is defined in (3), where tft,d is the term frequency in the document, i.e. the ratio between the number of times the term appears in the sentence and the total n"
W18-6323,E17-3017,0,0.0687007,"Missing"
W18-6323,P16-1162,0,0.0961626,"ents comparable. 3.3 Neural Machine Translation The aim of this work is to assess the impact of data selection techniques on NMT. For this purpose, we use the Marian framework4 (Junczys-Dowmunt et al., 2018) to train models using the attentionbased encoder–decoder architecture as described in Sennrich et al. (2017). For all experiments a preprocessing routine similar to the one in Moses5 (Koehn et al., 2007) is used. The preprocessing consists of the following steps: entity replacement (on numbers, emails, urls and alphanumeric entities), tokenisation, truecasing and Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 89,500 merge operations. 4 Experiments Although each data selection method has provided its own approach to select subsets from large corpora, in practice they would better perform if given a good initial subset (i.e. seed) to start with. To prepare such an initial seed (the same seed is used in the three data selection algorithms), we remove noisy sentences considering punctuation and numerical character. In particular, we remove sentences where: We present MT results using the three data selection methods and then use the best of the three methods to conduct a series of experiments to"
W18-6323,2006.amta-papers.25,0,0.113898,"in practice they would better perform if given a good initial subset (i.e. seed) to start with. To prepare such an initial seed (the same seed is used in the three data selection algorithms), we remove noisy sentences considering punctuation and numerical character. In particular, we remove sentences where: We present MT results using the three data selection methods and then use the best of the three methods to conduct a series of experiments to assess the impact of data selection on NMT models. We present two evaluation scores, BLEU (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006), in the tables. These scores give an estimation of how good the translation is: For BLEU, higher scores indicate better translations, while for TER, as it measures an error rate, lower scores indicate better translation performance. We performed three different experiments: 1. a source (or target) sentence contains fewer than tchars non-punctuation characters, • A comparison of the three data selection methods introduced in this paper (Section 4.1). 3.2 2 3 Seed preparation 4 http://opus.nlpl.eu/EMEA.php http://opus.nlpl.eu/DGT.php 5 226 https://marian-nmt.github.io/ http://www.statmt.org/mos"
W18-6323,P10-2041,0,0.536436,"scenarios, a much shorter training time would be very useful. Secondly, we could select data with the aim to make trained systems perform well for specific domains. In MT, models built with in-domain data perform better, as the vocabulary and sentence structures used in one domain (e.g. legal) differs from another unrelated domain (e.g. biotechnology). There are several studies on data selection methods for SMT, showing good improvements over the baselines in which the whole corpora were used for training (Chen et al., 2016). A popular data selection method is cross-entropy difference (CED) (Moore and Lewis, 2010). In particular its bilingual variant (Axelrod et al., 2011) showed a positive impact of data selection for MT. Term Frequency-Inverse Document Frequency (TF-IDF) (Salton and Yang, 1973) has also been used as a baseline data selection method in the literature. Data selection with cleaning was proposed to improve the robustness of training with divergent sentences (Carpuat et al., 2017). Feature Decay Algorithms (FDA) are data selection methods that try to extract the subset of sentences by which the coverage of target language features is maximized (Bic¸ici and Yuret, 2011). It has been used t"
W18-6323,P02-1040,0,0.100682,"ed its own approach to select subsets from large corpora, in practice they would better perform if given a good initial subset (i.e. seed) to start with. To prepare such an initial seed (the same seed is used in the three data selection algorithms), we remove noisy sentences considering punctuation and numerical character. In particular, we remove sentences where: We present MT results using the three data selection methods and then use the best of the three methods to conduct a series of experiments to assess the impact of data selection on NMT models. We present two evaluation scores, BLEU (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006), in the tables. These scores give an estimation of how good the translation is: For BLEU, higher scores indicate better translations, while for TER, as it measures an error rate, lower scores indicate better translation performance. We performed three different experiments: 1. a source (or target) sentence contains fewer than tchars non-punctuation characters, • A comparison of the three data selection methods introduced in this paper (Section 4.1). 3.2 2 3 Seed preparation 4 http://opus.nlpl.eu/EMEA.php http://opus.nlpl.eu/DGT.php 5 226"
W18-6323,D17-1147,0,0.0728752,"Missing"
W18-6323,L16-1561,0,0.0167716,"16, 2017) that can obtain better results. However, in this work we used the default configuration of d = 0.5, c = 0.0 and used trigrams as features. 3 Experimental Setup 3.1 Data description For the experiments we use English–French parallel data from two different domains/corpora: EMEA2 and DGT3 from the Open Parallel Corpus (OPUS) (Tiedemann, 2009). The first consists of medical data and the second a translation memory in the legal domain. We chose these domains in particular because they are categories more distant from the generic data, which is comprised of news data. The MultiUN corpus (Ziemski et al., 2016) is used for the training of generic models. Moreover, we use only its 6-way subset corpora, to be able to run the experiments in a more comparable setting. where tchars , twords and tratio are thresholds. For both domains and language pairs, tchars =5, twords =2 and tratio =0.5 are used. We then removed duplicates using the source as reference and compile the remaining sentences into three parts: a validation set (2000 lines); a test set (2000 lines); and the remaining lines comprise the seed domain data. The EMEA domain corpus gave rise to a seed with 238K lines, and the DGT was truncated to"
W19-6622,D16-1025,0,0.0217491,"losely related to our work is the work of Klebanov and Flor (2013) who presented findings regarding the loss of associative texture by comparing original and backtranslated texts, references and system translations and a set of different MT systems. Although the destruction of the underlying networks of signification might be, to some extent, unavoidable in any translation process, the work of Klebanov and Flor (2013) shows that SMT specifically suffers from lexical loss, more than HT. Lexical diversity or the loss thereof has also been used as a feature to estimate the quality of MT systems. Bentivogli et al. (2016) used lexical diversity, measured by using the type-token ratio (TTR), as an indicator of the size of vocabulary as well as the variety of subject matter in a text. Their experiments compared SMT to NMT and the results suggested that NMT is better able to cope with lexical diversity than SMT. Dublin, Aug. 19-23, 2019 |p. 223 3 Hypothesis Data-driven statistical MT paradigms1 are concerned with (i) identifying the most probable target words, phrases, or sub-word units given a source-language input sentence and the preceding decoded information, via the translation model, and (ii) chaining those"
W19-6622,D17-1151,0,0.0283702,"RNN type: bidirectional LSTM, number of layers of the encoder and of the decoder: 4, attention type: mlp, dropout: 0.2, batch size: 128, learning optimizer: adam (Kingma and Ba, 2014) and learning rate: 0.0001. • Transformer: number of layers: 6, size: 512, transformer ff: 2048, number of heads: 8, dropout: 0.1, batch size: 4096, batch type: tokens, learning optimizer adam with beta2 = 0.998, learning rate: 2. All neural systems have the learning rate decay enabled and their training is distributed over 4 nVidia 1080Ti GPUs. The selected settings for the RNN systems are optimal according to (Britz et al., 2017); for the Transformer we use the setDublin, Aug. 19-23, 2019 |p. 224 Language pair SRC TRG EN–FR 113,132 131,104 EN–ES 113,692 168,195 Table 2: Training vocabularies for the English, French and Spanish data used for our models. tings suggested by the OpenNMT community2 as the optimal ones that lead to quality on par with the original Transformer work (Vaswani et al., 2017). For the SMT systems we use Moses (Koehn et al., 2007) with default settings and a 5-gram language model with pruning of bigrams. Each system is further tuned with MERT (Och and Ney, 2003) until convergence or for a maximum"
W19-6622,P11-2031,0,0.0479215,"but translating the same original English dataset with the neural REV systems (RNN and Transformer) results in a huge drop in vocabulary size; with the SMT REV systems the decrease is still significant, but not as profound as in the former cases. In Table 4 we present automatic evaluation scores – BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) – for the 12 analysed systems. For completeness we present BLEU and TER for the REV systems in Table 5, although we do not consider them in our analysis. For the test set we performed a statistical significance test using the multeval tool (Clark et al., 2011). For p &lt; 0.05 all results in Table 4 are statistically significant. In what follows we use the following denotations to indicate the system we refer to: {src}-{trg}-{system}-{dir}, where {src} indicates the source language ‘en’, that is English, {trg} indicates the target languag – ‘fr’ Proceedings of MT Summit XVII, volume 1 Dublin, Aug. 19-23, 2019 |p. 225 2 System Dev set Test set reference BLEU↑ TER↓ BLEU↑ TER↓ en-fr-rnn-ff 33.7 50.7 33.8 51.0 en-fr-smt-ff 35.9 50.4 35.7 50.7 en-fr-trans-ff 35.9 49.5 36.0 49.4 en-fr-rnn-back 32.8 52.1 33.0 52.1 en-fr-smt-back 35.2 51.0 35.0 51.3 en-fr-tra"
W19-6622,W13-3304,0,0.442166,"s, one of which he refers to as ‘quantitative impoverishment’, a loss of lexical richness and diversity. Although mitigated by a human translator, this loss is to some extent inevitable as it is hard to respect the multitude of signifiers and constructions when translating one language into another. While Berman (2000) studied the decrease of lexical richness of human translations (HTs) from a theoretical point of view, Kruger (2012) demonstrated using empirical methods that there is indeed a lexical loss when comparing translations to original texts. In the field of Machine Translation (MT), Klebanov and Flor (2013) showed that Statistical c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 MT (SMT) suffers considerably more from lexical loss than HTs in a study focused on lexical tightness and text cohesion. We are not aware of any other research in this direction. As generating accurate translations has been the main objective of current MT systems, maintaining lexical richness and creating diverse outputs has understandably not been a priority. Nevertheless, the issue of lexical loss in MT"
W19-6622,W04-3250,0,0.256938,"cs for the human and the machine translations of the training set, i.e. the seen data, and Table 7 shows the scores for the human (HT) and the machine translations of the test sets, i.e. the unseen data. Due to the large number of output words, e.g. the rnn-ff translation of the EN–FR test set contains 14 561 653 words, and the low vocabulary size relative to the total number of words, our TTR scores are quite low. For readability and for ease of comparison we present these scores multiplied by a factor of 1000. We tested pairwise statistical significance through bootstrap sampling following (Koehn, 2004). The scores for all MT variants are significantly different from the HT variant. Translation Yules I en-fr-HT en-fr-rnn-ff en-fr-smt-ff en-fr-trans-ff en-fr-rnn-back en-fr-smt-back en-fr-trans-back en-es-HT en-es-rnn-ff en-es-smt-ff en-es-trans-ff en-es-rnn-back en-es-smt-back en-es-trans-back 9.2793 0.7107 6.7492 1.1768 0.7587 7.8738 1.0325 12.3065 0.6298 7.3249 1.0022 0.7355 8.1325 0.9162 TTR * 1000 2.9277 0.8656 2.6442 1.0925 0.8776 2.7496 1.0172 3.7037 0.9394 3.1170 1.1581 0.9829 3.2166 1.1014 MTLD 127.1766 109.4506 118.1239 120.5179 116.8942 120.9909 121.5801 99.0850 89.3562 95.1146 96.2"
W19-6622,2005.mtsummit-papers.11,0,0.0497281,"erson), or the difficulties of MT systems to appropriately handle morphologically richer target languages in general. Because NMT handles translation and language modelling (or alignment) jointly (Bahdanau et al., 2015; Vaswani et al., 2017), which makes it harder to optimize compared to SMT, we further hypothesise that NMT is more susceptible to problems related to overgeneralisation. We present our experiments and analyses in Section 4 and Section 5. 4 Empirical evaluation To test our hypothesis we built three types of MT systems and analysed their output for two language pairs on Europarl (Koehn, 2005) data. The language pairs are English → French (EN-FR) and English → Spanish (EN-ES). We 1 Despite the fact that often phrase-based SMT is labeled as ‘statistical’ and contrasted to ‘neural’ MT or NMT, we ought to stress that both approaches are in fact statistical. Proceedings of MT Summit XVII, volume 1 Language pair EN–FR EN–ES Train 1,467,489 1,472,203 Test 499,487 459,633 Dev 7,723 5,734 Table 1: Number of parallel sentences in the train, test and development splits for the language pairs we used. trained attentional RNN (Bahdanau et al., 2015), Transformer (Vaswani et al., 2017) and Mose"
W19-6622,J03-1002,0,0.011878,"systems are optimal according to (Britz et al., 2017); for the Transformer we use the setDublin, Aug. 19-23, 2019 |p. 224 Language pair SRC TRG EN–FR 113,132 131,104 EN–ES 113,692 168,195 Table 2: Training vocabularies for the English, French and Spanish data used for our models. tings suggested by the OpenNMT community2 as the optimal ones that lead to quality on par with the original Transformer work (Vaswani et al., 2017). For the SMT systems we use Moses (Koehn et al., 2007) with default settings and a 5-gram language model with pruning of bigrams. Each system is further tuned with MERT (Och and Ney, 2003) until convergence or for a maximum of 25 iterations. For the neural systems, we opted not to use subword units as is typically done for NMT. This is because we focus on the word frequencies in the translations and do not want any algorithm for splitting into sub-word units to add extra variability in our data. To construct the dictionaries we use all words in our training data. Table 2 (first two columns) shows the training vocabularies for the source and target sides. To assess how MT amplifies bias and loss of lexical richness, along with the original-data systems, we trained MT with backtr"
W19-6622,P02-1040,0,0.114455,"e source-side vocabulary sizes for the RNN, SMT and Transformer systems. These are in practice the number of distinct words of the translations produced by the REV systems. Compared to Table 2, this table clearly shows how source and target vocabularies are comparable in the original datasets, but translating the same original English dataset with the neural REV systems (RNN and Transformer) results in a huge drop in vocabulary size; with the SMT REV systems the decrease is still significant, but not as profound as in the former cases. In Table 4 we present automatic evaluation scores – BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) – for the 12 analysed systems. For completeness we present BLEU and TER for the REV systems in Table 5, although we do not consider them in our analysis. For the test set we performed a statistical significance test using the multeval tool (Clark et al., 2011). For p &lt; 0.05 all results in Table 4 are statistically significant. In what follows we use the following denotations to indicate the system we refer to: {src}-{trg}-{system}-{dir}, where {src} indicates the source language ‘en’, that is English, {trg} indicates the target languag – ‘fr’ Proceedings of MT Su"
W19-6622,C18-1265,1,0.81318,"ike ‘I am intelligent’ or ‘See?’ where there is little context to decide on a particular target variant of the same source word, it essentially boils down to the same thing: picking the translation that maximizes the probability over the entire sentence. As such, the loss of richness and diversity and the exacerbation of already frequent patterns might not simply be limited to the loss of (near) synonyms or rare words, but could also be the underlying cause of, for example, the inability of statistical MT systems to handle morphologically richer language correctly (Vanmassenhove et al., 2016; Passban et al., 2018), the already observed issues with gender bias (Vanmassenhove et al., 2018) in MT output or the difficulties of dealing with agglutinative languages (Unanue et al., 2018). The inability of neural models to generate diverse output has already been observed for tasks involving language generation, where creating intrinsically diverse outputs is more of a necessity. Dublin, Aug. 19-23, 2019 |p. 222 uncountable innombrable incalculable ind´enombrable Figure 1: One-to-many relation between an English source word and some of its possible French translations see voir vois voyons voyez voient Figure 2"
W19-6622,2006.amta-papers.25,0,0.204211,"or the RNN, SMT and Transformer systems. These are in practice the number of distinct words of the translations produced by the REV systems. Compared to Table 2, this table clearly shows how source and target vocabularies are comparable in the original datasets, but translating the same original English dataset with the neural REV systems (RNN and Transformer) results in a huge drop in vocabulary size; with the SMT REV systems the decrease is still significant, but not as profound as in the former cases. In Table 4 we present automatic evaluation scores – BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) – for the 12 analysed systems. For completeness we present BLEU and TER for the REV systems in Table 5, although we do not consider them in our analysis. For the test set we performed a statistical significance test using the multeval tool (Clark et al., 2011). For p &lt; 0.05 all results in Table 4 are statistically significant. In what follows we use the following denotations to indicate the system we refer to: {src}-{trg}-{system}-{dir}, where {src} indicates the source language ‘en’, that is English, {trg} indicates the target languag – ‘fr’ Proceedings of MT Summit XVII, volume 1 Dublin, Au"
W19-6622,L18-1141,0,0.0233165,"Missing"
W19-6622,D18-1334,1,0.863307,"ide on a particular target variant of the same source word, it essentially boils down to the same thing: picking the translation that maximizes the probability over the entire sentence. As such, the loss of richness and diversity and the exacerbation of already frequent patterns might not simply be limited to the loss of (near) synonyms or rare words, but could also be the underlying cause of, for example, the inability of statistical MT systems to handle morphologically richer language correctly (Vanmassenhove et al., 2016; Passban et al., 2018), the already observed issues with gender bias (Vanmassenhove et al., 2018) in MT output or the difficulties of dealing with agglutinative languages (Unanue et al., 2018). The inability of neural models to generate diverse output has already been observed for tasks involving language generation, where creating intrinsically diverse outputs is more of a necessity. Dublin, Aug. 19-23, 2019 |p. 222 uncountable innombrable incalculable ind´enombrable Figure 1: One-to-many relation between an English source word and some of its possible French translations see voir vois voyons voyez voient Figure 2: One-to-many relation between English verb ‘see’ and its conjugations in F"
W19-6622,D12-1097,0,0.0257951,"Berman (2000) researched the so-called deforming tendencies that are inherent to the act of translation. Although these tendencies can be mitigated by the (human) translator, they are to a large extent inevitable. Quantitative impoverishment (or lexical loss), is one of the tendencies mentioned. Kruger (2012) compared human-translated to comparable nontranslated English texts and found the translations to be more simplified in terms of language use than the original writings. In the field of MT, the concept of lexical loss/diversity and its importance is indirectly related to the research of Wong and Kit (2012) on cohesion. They illustrated the relevance of the under-use of linguistic devices (super-ordinates, meronyms, synonyms and near-synonyms) for SMT in terms of cohesion. More closely related to our work is the work of Klebanov and Flor (2013) who presented findings regarding the loss of associative texture by comparing original and backtranslated texts, references and system translations and a set of different MT systems. Although the destruction of the underlying networks of signification might be, to some extent, unavoidable in any translation process, the work of Klebanov and Flor (2013) sh"
W19-6622,D17-1323,0,0.0200141,"learn and generalize well are –compared to previous MT systems– the biggest asset of NMT. We however, hypothesize that this type of generalization might as well have serious drawbacks and that diversity, although not deemed a priority, is of importance for the field of MT as well. Overgeneralization over a seen input and the exacerbation of dominant forms might not only lead to a loss of lexical choice, but could also be the underlying cause of gender bias exacerbation. Although, in the context of gender, some researchers have already alluded to the existence of so-called ‘algorithmic bias’ (Zhao et al., 2017; Prates et al., 2019), no empirical evidence has been provided so far. With our empirical approach, comparing the lexical diversity of different MT systems and further analyzing the frequencies of words, we aim to shed some light on the relation between the loss of diversity and the exacerbation or loss of certain words. Thus, the first objective of our work is to verify how NMT compares to SMT and HT in terms of lexical richness or the loss thereof. The second objective is to quantify to what extent the different MT architectures favour translations that are more frequently observed in the t"
W19-6622,P07-2045,0,\N,Missing
W19-6622,P16-1009,0,\N,Missing
W19-6718,W18-2202,1,0.804314,"//elrc-share.eu/ 3 http://lr-coordination.eu/ Dublin, Aug. 19-23, 2019 |p. 112 avail of their local contacts in each (relatively small) country to try to persuade key stakeholders of the benefit of releasing corpora in their possession, negotiating in each case the most permissive terms possible for distribution and further reuse. However, rather than just acting as data collectors, and passing data blindly to ELRC-SHARE, the ADAPT MT team at DCU, Iconic and the University of Zagreb all have ample experience of building MT engines, including for the low-resource language pairs of the project. Dowling et al. (2018) compare statistical MT and neural MT performance for English-Irish; Klubička et al. (2017) built Croatian-English neural MT systems with superior quality to Google Translate;4 and Gupta et al. (2019) addresses the issue of robustness in real commercial neural MT systems. Accordingly, PRINCIPLE will build in-house baseline MT engines for each language pair and domain, add incremental amounts of data gathered, retrain the MT engines, and only submit data to eTranslation if improvements in MT quality are clearly visible via both automatic metrics and human evaluation. Once the utility of the dat"
W19-6718,W19-6727,0,0.023682,"Missing"
W19-6722,Q17-1024,0,0.0494836,"Missing"
W19-6732,W05-0909,0,0.113502,"ted to the same amount as the initial training data, the backtranslated synthetic parallel corpora were added to the initial training data and final (domain-specific) systems were trained from scratch. For the remaining systems (English-Estonian and EnglishLithuanian), domain adaptation of the initial models was performed using continued training. 6 Evaluating iADAATPA’s MT Systems The evaluation of all iADAATPA’s MT systems was carried out following current MT assessment practices (see Castilho et al. (2018)) with a combination of automatic evaluation metrics (AEMs) – including BLEU, METEOR (Banerjee and Lavie, 2005), TER and chrF (Popovi´c, 2015) – and human evaluation, consisting of assessing fluency, adequacy and ranking against a baseline. The Adequacy rating was based on the statement “The translated sentence conveys the meaning of the original...”, which was to be completed with a 3point Likert scale (1-Poorly, 2-Fairly, 3-Well). The Fluency rating was based on the statement “The translated sentence is grammatically...”, which was to be completed with a 3-point Likert scale (1Incomprehensible, 2-Fair, 3-Flawless). The Ranking assessment was based on asking the translators to rate the translations fr"
W19-6732,W17-4712,0,0.0182245,"provided by the PA) and generic training data set. Engine customization The data was cleaned using the Bicleaner tool (S´anchez-Cartagena et al., 2018). Moreover, embeddings for case inforDublin, Aug. 19-23, 2019 |p. 180 Use-case Language pair Gazette Gazette R&D R&D Spanish→English Spanish→Basque English→Spanish Basque→Spanish # segments init train 0 34.2M 820k 820k 0 36.3M 0 4.6M Table 3: Data used to train Prompsit NMT systems mation and byte pair encoding tokenization were added. The models were trained with multidomain data and we improved performance following a domain-mixing approach (Britz et al., 2017). The domain information was indicated using special tokens for each target sequence. The domain prediction was based only on the source as the extra token was added at target-side and there was no need for a priori domain information. This approach allowed the model to improve the quality for each domain. 4 Prompsit Prompsit is a language technology (LT) provider with a strong focus on tailored MT services involving data curation, training and development of other multilingual applications. 4.1 Prompsit’s MT systems Language pairs and domains Prompsit partnered with SESIAD, the Spanish State"
W19-6732,W15-3049,0,0.0398645,"Missing"
W19-6732,P17-2061,0,0.0297506,"didates for new monolingual and bilingual dictionary entries from a word-aligned parallel corpus generated with ruLearn (S´anchezCartagena et al., 2016). For NMT systems, based on OpenNMT, automatic segmentation of long sentences and linguistically informed word segmentation for Basque (S´anchez-Cartagena, 2018) were added to the corpus pre-processing pipeline. Moreover, to ensure translation consistency, carefully designed terminology to restrict translation hypotheses and named entity recognition to control the translation of proper names, places, etc. was added. Finally, mixed fine-tuning (Chu et al., 2017) was applied to some systems to balance the weight of the different sources of training data. 5 Tilde Tilde is an LSP and LT developer offering customized MT system development, as well as a wide range of other cloud-based and stand-alone LT tools and services for terminology management, spelling and grammar checking, speech recognition and synthesis, personalised virtual assistants, and other applications. It provides onpremise and cloud-based LT solutions to public and private organisations as well as LT productivity tools to individual users. 5.1 Tilde’s MT systems Language pairs and domain"
W19-6732,W18-6488,1,0.890711,"Missing"
W19-6732,goldhahn-etal-2012-building,0,0.0372147,"Missing"
W19-6732,C14-1182,0,0.0531557,"Missing"
W19-6732,2005.mtsummit-papers.11,0,0.0191017,"ized: News and Events, President Office, School of Applied Languages and Intercultural Studies, and Fiontar – Irish Language Research. The language pairs consisted of English as the source for all the neural MT (NMT) engines into Bulgarian, Dutch, French, German, Irish, Italian, Polish, Portuguese, Romanian, and Spanish. 2.2 Data Acquisition The data used in the customization of KantanMT’s engines was selected from publicly available sources, such as the DGT (European Commission’s Directorate-General for Translation), EMEA (European Medicines Agency), ECB (European Central Bank) and EuroParl (Koehn, 2005).3 Table 1 shows the training data for KantanMT’s NMT systems. 2.3 Engine Customization All initial NMT engines were developed using the Torch implementation of the OpenNMT framework.4 The development test reference set, used to generate automated scores and to establish a performance baseline for each engine, consisted of 500 segments chosen at random from the live DCU website. Both recurrent and transformer neural models were trained. The model with the best overall automated scores was then selected as the final release candidate. (For the purposes of engine selection, F-Measure, TER (Snove"
W19-6732,P02-1040,0,0.123127,"aining data for KantanMT’s NMT systems. 2.3 Engine Customization All initial NMT engines were developed using the Torch implementation of the OpenNMT framework.4 The development test reference set, used to generate automated scores and to establish a performance baseline for each engine, consisted of 500 segments chosen at random from the live DCU website. Both recurrent and transformer neural models were trained. The model with the best overall automated scores was then selected as the final release candidate. (For the purposes of engine selection, F-Measure, TER (Snover et al., 2006), BLEU (Papineni et al., 2002), and Perplexity were used as automated scores.) 3 4 https://www.statmt.org/europarl/ http://www.opennmt.net/ Proceedings of MT Summit XVII, volume 2 Language pair Spanish→Catalan Spanish→English Spanish→French Spanish→German Spanish→Italian Spanish→Portuguese Spanish→Russian # segments init train 30k 13.6M 30k 14.6M 30k 14.6M 30k 14.5M 30k 14.5M 30k 14.6M 30k 13.8M Table 2: Data used to train Pangeanic NMT systems 3 Pangeanic Pangeanic (Yuste et al., 2010) is a Language Service Provider (LSP) specialised in Natural Language Processing and MT. It provides solutions to cognitive companies, inst"
W19-6732,W17-4737,1,0.889461,"Missing"
W19-6732,2006.amta-papers.25,0,0.17659,"2005).3 Table 1 shows the training data for KantanMT’s NMT systems. 2.3 Engine Customization All initial NMT engines were developed using the Torch implementation of the OpenNMT framework.4 The development test reference set, used to generate automated scores and to establish a performance baseline for each engine, consisted of 500 segments chosen at random from the live DCU website. Both recurrent and transformer neural models were trained. The model with the best overall automated scores was then selected as the final release candidate. (For the purposes of engine selection, F-Measure, TER (Snover et al., 2006), BLEU (Papineni et al., 2002), and Perplexity were used as automated scores.) 3 4 https://www.statmt.org/europarl/ http://www.opennmt.net/ Proceedings of MT Summit XVII, volume 2 Language pair Spanish→Catalan Spanish→English Spanish→French Spanish→German Spanish→Italian Spanish→Portuguese Spanish→Russian # segments init train 30k 13.6M 30k 14.6M 30k 14.6M 30k 14.5M 30k 14.5M 30k 14.6M 30k 13.8M Table 2: Data used to train Pangeanic NMT systems 3 Pangeanic Pangeanic (Yuste et al., 2010) is a Language Service Provider (LSP) specialised in Natural Language Processing and MT. It provides solution"
W19-6732,tiedemann-2012-parallel,0,0.0856107,"Missing"
W19-6732,2010.amta-commercial.4,0,0.0489296,"es was then selected as the final release candidate. (For the purposes of engine selection, F-Measure, TER (Snover et al., 2006), BLEU (Papineni et al., 2002), and Perplexity were used as automated scores.) 3 4 https://www.statmt.org/europarl/ http://www.opennmt.net/ Proceedings of MT Summit XVII, volume 2 Language pair Spanish→Catalan Spanish→English Spanish→French Spanish→German Spanish→Italian Spanish→Portuguese Spanish→Russian # segments init train 30k 13.6M 30k 14.6M 30k 14.6M 30k 14.5M 30k 14.5M 30k 14.6M 30k 13.8M Table 2: Data used to train Pangeanic NMT systems 3 Pangeanic Pangeanic (Yuste et al., 2010) is a Language Service Provider (LSP) specialised in Natural Language Processing and MT. It provides solutions to cognitive companies, institutions, translation professionals, and corporations. 3.1 Pangeanic’s MT systems Language pairs and domains Pangeanic’s usecases were for two Spanish PAs: (1) Generalitat Valenciana (regional administration) translating from Spanish into and out of English, French, Catalan/Valencian, German, Italian, Russian; and (2) Segittur (tourism administration) translating from Spanish into and out of English, French, German, Italian, Portuguese. For this purpose, NM"
W19-6738,W18-2100,0,0.23382,"Missing"
W19-6738,C18-1266,0,0.0413611,"Missing"
W19-6738,W17-4763,0,0.0552638,"both two-phase systems. QEBrain is an extension of the ‘Neural Bilingual Expert model’ (Fan et al., 2018) with extra features. The first phase extracts high latent semantic and alignment information between the source and the translation output. Based on Transformer (Vaswani et al., 2017), this network builds a conditional language model – the neural bilingual expert. It is complemented with an errorprediction model which identifies possible mismatches of words. In the second phase, the features of these two models are used in a bi-LSTM model to output the QE score. The POSTECH architecture (Kim et al., 2017) consists of a word predictor model and an estimator model. The predictor model is used to extract QE feature vectors (QEFVs) which are employed to train the estimator: a logistic regression model based on a summary representation of the QEFVs. deepQuest (Ive et al., 2018) implements two types of architectures: (i) BiRNN (a one-phase approach) and (ii) POSTECH (a two-phase approach). The BiRNN architecture employs two Proceedings of MT Summit XVII, volume 2 bidirectional RNNs (with GRU units) whose outputs are combined through an attention mechanism. The resulting vector representation is used"
W19-6738,D15-1166,0,0.0581789,"icient QE system, we implemented our SiameseQE with one LSTM-based RNN that encodes both source and MT sentences in so called left and right passes, respectively. The encoded representations – the RNN outputs – of both sentences are used to compute a distance score which is optimised through an MSELoss with respect to the expected TER score. We use Euclidean distance in our implementation. Given that we build on a single RNN, we use joint vocabulary so that we could train without mismatch of tokens. We also explored three types of networks: (i) with no attention; (ii) with Soft Dot Attention (Luong et al., 2015) and (iii) with word-by-word attention, as defined in Rockt¨aschel et al. (2015). Ueffing et al. (2018) presented a Siamese NN system for QE with two LSTM RNNs with tied weights, using cosine similarity. Their application identified quality levels of automatically generated product titles. We aim to further optimise the performance via a single RNN (with LSTM units) and by implementing attention mechanisms. 4 Use-case and data Our use-case is QE of the translations of software UI strings from Microsoft products. The domain is, therefore, technical/IT. To train our QE sysDublin, Aug. 19-23, 201"
W19-6738,P02-1040,0,0.103783,"hat while two-phase systems perform best in terms of the predicted QE scores, their computational costs suggest that alternatives should be considered for large-scale translation production. 1 Introduction Quality estimation (QE) (Specia et al., 2009) is the process of predicting the quality of a machine translation (MT) system without human intervention or reference translations. QE can be applied at word-, sentence-, or document-level. In the case of document- and sentence-level, the task is typically to predict a score that corresponds to a target evaluation criteria or metric (e.g., BLEU (Papineni et al., 2002), TER (Snover et al., 2006), etc.), i.e. it is a regression task. In this work, we investigate sentence-level QE, estimating TER scores. QE has been the focus of multiple WMT shared tasks. In such tasks the common evaluation criteria are metrics that score the quality of the estimates, such as Pearson’s r or Root Mean Square c 2018 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 2 Error (RMSE). However, in a commercial setting, it is important to set a balance between performance and ef"
W19-6738,2006.amta-papers.25,0,0.191388,"erform best in terms of the predicted QE scores, their computational costs suggest that alternatives should be considered for large-scale translation production. 1 Introduction Quality estimation (QE) (Specia et al., 2009) is the process of predicting the quality of a machine translation (MT) system without human intervention or reference translations. QE can be applied at word-, sentence-, or document-level. In the case of document- and sentence-level, the task is typically to predict a score that corresponds to a target evaluation criteria or metric (e.g., BLEU (Papineni et al., 2002), TER (Snover et al., 2006), etc.), i.e. it is a regression task. In this work, we investigate sentence-level QE, estimating TER scores. QE has been the focus of multiple WMT shared tasks. In such tasks the common evaluation criteria are metrics that score the quality of the estimates, such as Pearson’s r or Root Mean Square c 2018 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 2 Error (RMSE). However, in a commercial setting, it is important to set a balance between performance and efficiency. Furthermore, a QE"
W19-6738,2009.eamt-1.5,0,0.144965,"suitable in dynamic translation workflows, where translations need to be assessed continuously with no specific reference provided. In this paper, we investigate sentence-level neural QE and its applicability in an industry usecase. We assess six QE approaches, which we divide into two-phase and one-phase approaches, based on quality and cost. Our evaluation shows that while two-phase systems perform best in terms of the predicted QE scores, their computational costs suggest that alternatives should be considered for large-scale translation production. 1 Introduction Quality estimation (QE) (Specia et al., 2009) is the process of predicting the quality of a machine translation (MT) system without human intervention or reference translations. QE can be applied at word-, sentence-, or document-level. In the case of document- and sentence-level, the task is typically to predict a score that corresponds to a target evaluation criteria or metric (e.g., BLEU (Papineni et al., 2002), TER (Snover et al., 2006), etc.), i.e. it is a regression task. In this work, we investigate sentence-level QE, estimating TER scores. QE has been the focus of multiple WMT shared tasks. In such tasks the common evaluation crit"
W19-6738,P13-4014,0,0.0515709,"Missing"
W19-6738,2005.mtsummit-papers.11,0,0.134056,"application identified quality levels of automatically generated product titles. We aim to further optimise the performance via a single RNN (with LSTM units) and by implementing attention mechanisms. 4 Use-case and data Our use-case is QE of the translations of software UI strings from Microsoft products. The domain is, therefore, technical/IT. To train our QE sysDublin, Aug. 19-23, 2019 |p. 229 tems we used proprietary Microsoft data collected from post-edits scored using TER. The language pairs are English-German (EN-DE) and EnglishSpanish (EN-ES). We also used parallel data from Europarl (Koehn, 2005) and from Microsoft for two-phase systems, abbreviated as EU and MS respectively. In Table 1 we present details of the QE and the extra parallel training data. To train the one-phase systems, only the QE data was used. To train the two-phase systems (POSTECH systems and QEBrain) for EN-DE and EN-ES we used parallel data (EU or MS) for the feature-extraction part of the model, i.e. for the first phase, and the provided QE data for the QE score computation model, i.e. the second phase. We trained one POSTECH system per language pair on EU data, and another on the MS parallel data sets. The evalu"
W19-6738,N18-3007,0,0.0242167,"Missing"
W19-6738,W18-6465,0,0.154587,"two-phase approaches We classify QE in two groups: one-phase and twophase approaches. The former have a unified architecture and are trained to generate estimates in an end-to-end fashion, with no distinct intermediate stages. The latter employ two phases in training and in testing, typically involving two networks that are trained separately; the first one targets decomposing the input (a source sentence and its MT) into features, which are then used as input for the second network to compute a QE score. NQE Systems The top-scoring systems in the segment-level task at WMT 2018 were QEBrain (Wang et al., 2018) and UNQE (Li et al., 2018), both two-phase systems. QEBrain is an extension of the ‘Neural Bilingual Expert model’ (Fan et al., 2018) with extra features. The first phase extracts high latent semantic and alignment information between the source and the translation output. Based on Transformer (Vaswani et al., 2017), this network builds a conditional language model – the neural bilingual expert. It is complemented with an errorprediction model which identifies possible mismatches of words. In the second phase, the features of these two models are used in a bi-LSTM model to output the QE score"
W19-6738,W11-0329,0,0.0191976,"first-phase models of systems like QEBrain and POSTECH are typically trained on parallel data. One-phase systems, such as the deepQuest BiRNN, are trained only on QE data: source, MT output, and a score. 3 SiameseQE Siamese NNs were proposed initially for the problems of signature verification (Bromley et al., 1993) and fingerprint recognition (Baldi and Chauvin, 1993). The model consists of two (or more) identical networks, encoding different inputs. The two networks share the same configuration with mirrored weights. Siamese NNs have also been applied to address the task of text similarity (Yih et al., 2011; Mueller and Thyagarajan, 2016) and image recognition (Koch et al., 2015). With the aim of providing an efficient QE system, we implemented our SiameseQE with one LSTM-based RNN that encodes both source and MT sentences in so called left and right passes, respectively. The encoded representations – the RNN outputs – of both sentences are used to compute a distance score which is optimised through an MSELoss with respect to the expected TER score. We use Euclidean distance in our implementation. Given that we build on a single RNN, we use joint vocabulary so that we could train without mismatc"
W19-6908,W18-6315,0,0.154776,"nt quality could be taken from Escart´ın and Arcedillo (2015) who indicate that a BLEU score of 45+ can increase translator productivity for EN-ES. Proceedings of the Celtic Language Technology Workshop 2019 Dublin, 19–23 Aug., 2019 |p. 58 lieve that language technology resources are vital for the preservation and growth of every language and that it is necessary to develop methods of creating MT systems for languages without an extensive amount of language data available. Previous experiments have shown backtranslation to be a viable method of artificial data creation (Sennrich et al., 2015; Burlot and Yvon, 2018; Poncelas et al., 2018). One possible benefit of backtranslation is that it allows the use of more than one MT paradigm (e.g. rule-based, statistical, neural) to create a MT model. In this way, the resulting model could gain benefits from each paradigm used. Apertium (Forcada et al., 2011) is an open source machine translation platform which uses rule-based machine translation (RBMT) as the underlying MT technology. One of the benefits of RBMT is that it requires no parallel data, apart from a dictionary. There have been some efforts towards creating a RBMT system for GA↔GD. However, the GA↔G"
W19-6908,E06-1032,0,0.0536099,"ord order between English and both Irish and Gaelic Irish and Scottish Gaelic both display richer morphology than English. Example sentence 1 shows the inflection of the feminine nouns ‘creag’ (GD) and ‘carraig’ (GA), both meaning ‘rock’ or ‘cliff’4 . Inflection can have an impact on data sparsity (inflected words seen less frequently in training data) and also on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which considers inflected words as being wholly different from their uninflected counterparts, and can sometimes penalise translation output too harshly as a result (Callison-Burch et al., 2006). (1) creag a’ chreag creagan na creige 4 rock/a rock the rock rocks of the rock carraig an charraig carraigeacha na carraige Data SMT and NMT, currently the two most prominent MT paradigms, require large amounts of bilingual data. Therefore, the availability of data plays a huge part in the quality of MT output. In this section we describe the GD and GA language data resources used in our experiments. 4.1 Scottish Gaelic Wikipedia Scottish Gaelic language Wikipedia (Uicipeid5 ) contains 14,801 articles at the time of download 6 . Pre-processing including sentence tokenising, removal of wiki-t"
W19-6908,W18-2202,1,0.845912,"at are in fact 1-3 word phrases rich in technical jargon. Tatoeba, another OPUS source, is a corpus of short, simplified sentences for language learning purposes. While there was not a GD–GA Tatoeba corpus available, we downloaded the monolingual corpora for each language and manually aligned any matching sentences (referred to as Tatoeba-ga). OPUS also provides us with EN-GD parallel corpora from Tatoeba (Tatoeba-en), Ubuntu and GNOME. Figure 2: Simplified diagram of backtranslation method used to build SMT systems in these experiments 4.2 Irish In this work, we use the datasets described by Dowling et al. (2018). This consists of 108,000 parallel sentences from sources such as the Department of Culture, Heritage and the Gaeltacht and the Citizens Information website8 . Corpus Uicipeid Ubuntu GNOME Tatoeba-ga Tatoeba-en EN–GA TOTAL # GA words N/A 20,166 14,897 466 N/A 1,859,042 1,894,571 # GD words 1,449,636 25,125 19,956 489 2,556 N/A 1,497,762 # EN words N/A N/A N/A N/A 2,254 1,697,387 1,699,641 Table 1: Number of words in bilingual (GD-EN, GD-GA, GA-EN) and monolingual (GD only) corpora used 5 Method In these experiments we take an approach to building an MT system using backtranslation illustrated"
W19-6908,W17-3204,0,0.0114996,"sambiguation, syntactic transfer, lexical transfer and post-processing. There is also some literature surrounding the development of a SMT system for the GA–GD pair (Scannell, 2014). This approach involves training a word-based model, similar to the IBM model 1. Research has been carried out on GD-EN NMT (Chen, 2018), in which the author uses linguistic features such as glosses to improve the system. 3 Linguistic overview Translating between sentences with differing sentence structures can be a challenge for MT systems and can lead to poor quality MT output, particularly for longer sentences (Koehn and Knowles, 2017). Gaelic languages employ a verb-subjectobject (VSO) sentence structure, different to the sentence-verb-object (SVO) structure more commonly seen in Indo-European languages. Figure 1 illustrates the similar word order of Scottish Gaelic and Irish, and how it diverges with that of English. Figure 1: An example sentence highlighting the divergent word order between English and both Irish and Gaelic Irish and Scottish Gaelic both display richer morphology than English. Example sentence 1 shows the inflection of the feminine nouns ‘creag’ (GD) and ‘carraig’ (GA), both meaning ‘rock’ or ‘cliff’4 ."
W19-6908,P07-2045,0,0.0133249,"ments are presented as experiments 4A–C in Table 2. Proceedings of the Celtic Language Technology Workshop 2019 Dublin, 19–23 Aug., 2019 |p. 60 5.1 Building and adding to the baseline Each experiment contains three parts (referred to in Table 2). Part A involves creating a baseline by training a SMT system using only authentic data. Part B trains a SMT system using the artificial dataset created through backtranslation. This experiment most closely resembles Figure 2. Finally, in part C, the authentic and artificial datasets are combined to train a SMT system. Systems are trained using Moses (Koehn et al., 2007) with default parameters, with the exception of the GD↔EN systems which use a 6-gram language model and hierarchical reordering tables to partly address the divergent word order between the two languages. 6 the translation of the test corpus Tatoeba-ga. Despite the low BLEU score of the Apertium GA-GD module, SMT systems trained using solely artificial data also show an increase in BLEU over the baseline. This indicates that even if the quality of the MT system used to backtranslate is poor, it may still be possible to gain benefits from the backtranslated data. The highest automatic scores fr"
W19-6908,P02-1040,0,0.10861,"pean languages. Figure 1 illustrates the similar word order of Scottish Gaelic and Irish, and how it diverges with that of English. Figure 1: An example sentence highlighting the divergent word order between English and both Irish and Gaelic Irish and Scottish Gaelic both display richer morphology than English. Example sentence 1 shows the inflection of the feminine nouns ‘creag’ (GD) and ‘carraig’ (GA), both meaning ‘rock’ or ‘cliff’4 . Inflection can have an impact on data sparsity (inflected words seen less frequently in training data) and also on automatic evaluation metrics such as BLEU (Papineni et al., 2002), which considers inflected words as being wholly different from their uninflected counterparts, and can sometimes penalise translation output too harshly as a result (Callison-Burch et al., 2006). (1) creag a’ chreag creagan na creige 4 rock/a rock the rock rocks of the rock carraig an charraig carraigeacha na carraige Data SMT and NMT, currently the two most prominent MT paradigms, require large amounts of bilingual data. Therefore, the availability of data plays a huge part in the quality of MT output. In this section we describe the GD and GA language data resources used in our experiments"
W19-6908,W14-4605,0,0.0189606,"system for GA↔GD. However, the GA↔GD Apertium module is listed as being in the incubator stage, which indicates that more work is needed before the MT system can be classed as being reliable. There has been some previous work to create a GA→GD MT system with little or no data (Scannell, 2006). In this approach, the author builds a pipeline-style MT system which uses stages of standardisation, part-of-speech tagging, word sense disambiguation, syntactic transfer, lexical transfer and post-processing. There is also some literature surrounding the development of a SMT system for the GA–GD pair (Scannell, 2014). This approach involves training a word-based model, similar to the IBM model 1. Research has been carried out on GD-EN NMT (Chen, 2018), in which the author uses linguistic features such as glosses to improve the system. 3 Linguistic overview Translating between sentences with differing sentence structures can be a challenge for MT systems and can lead to poor quality MT output, particularly for longer sentences (Koehn and Knowles, 2017). Gaelic languages employ a verb-subjectobject (VSO) sentence structure, different to the sentence-verb-object (SVO) structure more commonly seen in Indo-Eur"
W19-6908,tiedemann-2012-parallel,0,0.0309198,"data. Therefore, the availability of data plays a huge part in the quality of MT output. In this section we describe the GD and GA language data resources used in our experiments. 4.1 Scottish Gaelic Wikipedia Scottish Gaelic language Wikipedia (Uicipeid5 ) contains 14,801 articles at the time of download 6 . Pre-processing including sentence tokenising, removal of wiki-text, tags and blank lines was performed, providing us with a resulting corpus of 87,788 sentences of monolingual Scottish Gaelic. This corpus can be described as being of mixed domain, with clear, formal sentences. OPUS OPUS (Tiedemann, 2012) is a repository of language resources available for download from 4 For clarity, the inflection markers (letters) in each example are displayed in bold 5 https://gd.wikipedia.org 6 04/04/2019 Proceedings of the Celtic Language Technology Workshop 2019 Dublin, 19–23 Aug., 2019 |p. 59 the web7 . OPUS provides us with bilingual GA– GD and EN-GD corpora from a number of sources. Two bilingual GA–GD corpora that OPUS provides us with are the Ubuntu (655 parallel sentences) and GNOME (5,317 sentences) manuals. These are strictly within the technical domain, and often contain ‘sentences’ that are in"
W19-7202,D11-1033,0,0.0246646,"f both as we use data selection methods (data centric) and fine-tuning (model centric). The technique of fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) consists of training an NMT model with a general domain data set until convergence, and then using an in-domain set for the last epochs. The work of van der Wees et al. (2017) showed that training an NMT model using less (but more Dublin, Aug. 20, 2019 |p. 14 in-domain) data each epoch achieves improvements over a model trained with all data. Their experiments include weighting the sentences using Cross Entropy Difference (Axelrod et al., 2011), and then, each epoch e the top-Ne sentences are used as training data where N1 ≥ Ne ≥ Nlast A proposal in which they use the test set to adapt the model is the work of Li et al. (2018). In particular, they fine-tune a pre-built NMT model for each sentence in the test set. They use three methods to retrieve the sentences that are the most similar to a sentence of the test set: (i) Levenshtein distance (Levenshtein, 1966); (ii) cosine similarity of the average of the word embeddings (Mikolov et al., 2013); and (iii) the cosine similarity between hidden states of the encoder in NMT. The main di"
W19-7202,W05-0909,0,0.0359678,"Pair Encoding (BPE) (Sennrich et al., 2016) is applied with 89500 merge operations (the number of operations used in the work of Sennrich et al. (2016)). The models have been built using OpenNMT-py (Klein et al., 2017). We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units, vocabulary size of 50000 words for each language. We use different evaluation metrics to evaluate the performance of the models built in the experiments. These models are evaluated on the test sets using several evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The scores assigned by this metrics indicate an estimation of the 1 https://tilde.com/ http://opus.nlpl.eu/EMEA.php 3 http://www.himl.eu/test-sets 2 Dublin, Aug. 20, 2019 |p. 16 quality of the translation (compared to a humantranslated reference). Higher scores of BLEU and METEOR indicate better translation quality. TER is an error metric, therefore lower scores indicate better performance. In each table, scores that are better than the baseline are shown in bold. Furthermore, scores that constitute a statistically significant improvement at level p=0.01 over the baseline are marked with an"
W19-7202,W11-2131,0,0.386484,"Missing"
W19-7202,W13-2206,0,0.451969,"Missing"
W19-7202,W17-4714,0,0.238134,"Missing"
W19-7202,C18-1111,0,0.0206941,"ropose to explore are the following three: 1. Can a model fine-tuned with a subset of data outperform the model trained with general domain data? The work of Poncelas et al. (2018b) showed that performing fine-tuning on a subset of data (used to build the model) yields small improvements (and not statistically significant at level p=0.01). A limitation in their experiments is that, as BPE is not applied, the vocabulary of the adapted model remains the Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 3 Related Work There are several adaptation techniques for NMT. Chu and Wang (2018) structure them into two main groups, data centric (techniques which involve augmenting or modifying the training data) and model centric (techniques which involve modifying the architecture or the procedure with which the model is trained). In this paper, we use a combination of both as we use data selection methods (data centric) and fine-tuning (model centric). The technique of fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) consists of training an NMT model with a general domain data set until convergence, and then using an in-domain set for the last epochs. The work of"
W19-7202,P17-2061,0,0.0203286,"ains and different selected-data sizes. Moreover, other configurations of data selection algorithms could be investigated. For example, using n-grams of higher order, executing INR with different values of t, in Equation (5), or FDA with different values of d and c, in Equation (6) (Poncelas et al., 2016; Poncelas et al., 2017). The techniques explored here can also be used in combination with other approaches aiming to adapt models towards a particular domain. The models presented in Section 7.3 can be further expanded by adding a tag in the source sentences indicating the domain explicitly (Chu et al., 2017; Poncelas et al., 2019b), using a target-side seed or using synthetic sentences (Chinea-Rios et al., 2017; Poncelas et al., 2019a). Acknowledgements This research has been supported by the ADAPT Centre for Digital Content Technology which is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund. This work has also received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 713567. Proceedings of The 8th Workshop on Patent and Scientific L"
W19-7202,P11-2031,0,0.142128,"ate an estimation of the 1 https://tilde.com/ http://opus.nlpl.eu/EMEA.php 3 http://www.himl.eu/test-sets 2 Dublin, Aug. 20, 2019 |p. 16 quality of the translation (compared to a humantranslated reference). Higher scores of BLEU and METEOR indicate better translation quality. TER is an error metric, therefore lower scores indicate better performance. In each table, scores that are better than the baseline are shown in bold. Furthermore, scores that constitute a statistically significant improvement at level p=0.01 over the baseline are marked with an asterisk. This was computed with multeval (Clark et al., 2011) using Bootstrap Resampling (Koehn, 2004). 6 BASE12 BLEU TER METEOR 26.16 54.41 30.00 BASE12 + rapid2016 24.05 55.86 28.74 Table 3: Results of the model BASE12 fine-tuned with the in-domain news set. BASE12 BLEU TER METEOR Baseline Results 33.29 46.11 34.62 BASE12 EMEA 34.69 44.43 34.99 + 6.1 Baseline Results with General-domain Data Table 4: Results of the model BASE12 fine-tuned with the in-domain health set. BASE12 26.16 54.41 30.00 news (BASE12 + rapid2016) and another one to the health domain (BASE12 + EMEA). We see, in Table 4, how using in-domain data for fine-tuning can increase the pe"
W19-7202,E12-1016,0,0.387121,"Missing"
W19-7202,2005.eamt-1.19,0,0.0307339,"to retrieve sentences. These methods select a subset of from the parallel set (S, T ) used as training data. In particular, they select sentences based on overlaps of n-grams between the test set Stest and the source side of the parallel data S. In this work, we explore the following three techniques: TF-IDF Distance Method: Distance methods measure how close two sentences are by using metrics as Levenshtein distance (which computes the minimum number of insertion, deletions or substitutions of characters that are necessary to transform one sentence into the other) to score the similarities. Hildebrand et al. (2005) propose TF-IDF distance i.e. to use cosine between TFIDF (Salton and Yang, 1973) vectors as distance Proceedings of The 8th Workshop on Patent and Scientific Literature Translation metric. In their work, for each stest ∈ Stest the top sentences from S are selected. Although they are aware that the resulting set contains duplicated sentences, in their experiments the models containing duplicated sentences achieve slightly better results. TF-IDF measures the importance of the terms in a set of documents. Each document D can be represented as a vector of terms wD = (w1 , w2 , . . . w|V |), where"
W19-7202,P17-4012,0,0.0497076,"dicines Agency (EMEA)2 (Tiedemann, 2009) (361K sentence pairs). For health domain test set we use the Cochrane 3 dataset provided in WMT 2017 biomedical translation shared task (Yepes et al., 2017). Note that the general-domain set contains sentences from a corpus such as Europarl (Koehn, 2005) which causes the domain to be closer to the news domain. All data sets are tokenized, truecased and Byte Pair Encoding (BPE) (Sennrich et al., 2016) is applied with 89500 merge operations (the number of operations used in the work of Sennrich et al. (2016)). The models have been built using OpenNMT-py (Klein et al., 2017). We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units, vocabulary size of 50000 words for each language. We use different evaluation metrics to evaluate the performance of the models built in the experiments. These models are evaluated on the test sets using several evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The scores assigned by this metrics indicate an estimation of the 1 https://tilde.com/ http://opus.nlpl.eu/EMEA.php 3 http://www.himl.eu/test-sets 2 Dublin, Aug. 20, 2019 |p. 16 quality of th"
W19-7202,W04-3250,0,0.398592,"tp://opus.nlpl.eu/EMEA.php 3 http://www.himl.eu/test-sets 2 Dublin, Aug. 20, 2019 |p. 16 quality of the translation (compared to a humantranslated reference). Higher scores of BLEU and METEOR indicate better translation quality. TER is an error metric, therefore lower scores indicate better performance. In each table, scores that are better than the baseline are shown in bold. Furthermore, scores that constitute a statistically significant improvement at level p=0.01 over the baseline are marked with an asterisk. This was computed with multeval (Clark et al., 2011) using Bootstrap Resampling (Koehn, 2004). 6 BASE12 BLEU TER METEOR 26.16 54.41 30.00 BASE12 + rapid2016 24.05 55.86 28.74 Table 3: Results of the model BASE12 fine-tuned with the in-domain news set. BASE12 BLEU TER METEOR Baseline Results 33.29 46.11 34.62 BASE12 EMEA 34.69 44.43 34.99 + 6.1 Baseline Results with General-domain Data Table 4: Results of the model BASE12 fine-tuned with the in-domain health set. BASE12 26.16 54.41 30.00 news (BASE12 + rapid2016) and another one to the health domain (BASE12 + EMEA). We see, in Table 4, how using in-domain data for fine-tuning can increase the performance with more than 2 BLEU points. H"
W19-7202,2005.mtsummit-papers.11,0,0.133506,"search question 2 and 3 explained in Section 2): • News Domain: We use the test set provided in WMT 2015 News Translation Task, and the in-domain rapid20161 data set (1.3M sentence pairs) provided in WMT 2017 News Translation (Bojar et al., 2017). • Health Domain: German-to-English parallel text from the European Medicines Agency (EMEA)2 (Tiedemann, 2009) (361K sentence pairs). For health domain test set we use the Cochrane 3 dataset provided in WMT 2017 biomedical translation shared task (Yepes et al., 2017). Note that the general-domain set contains sentences from a corpus such as Europarl (Koehn, 2005) which causes the domain to be closer to the news domain. All data sets are tokenized, truecased and Byte Pair Encoding (BPE) (Sennrich et al., 2016) is applied with 89500 merge operations (the number of operations used in the work of Sennrich et al. (2016)). The models have been built using OpenNMT-py (Klein et al., 2017). We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units, vocabulary size of 50000 words for each language. We use different evaluation metrics to evaluate the performance of the models built in the experiments. These models are evaluated on the test s"
W19-7202,L18-1146,0,0.194867,"ng an NMT model with a general domain data set until convergence, and then using an in-domain set for the last epochs. The work of van der Wees et al. (2017) showed that training an NMT model using less (but more Dublin, Aug. 20, 2019 |p. 14 in-domain) data each epoch achieves improvements over a model trained with all data. Their experiments include weighting the sentences using Cross Entropy Difference (Axelrod et al., 2011), and then, each epoch e the top-Ne sentences are used as training data where N1 ≥ Ne ≥ Nlast A proposal in which they use the test set to adapt the model is the work of Li et al. (2018). In particular, they fine-tune a pre-built NMT model for each sentence in the test set. They use three methods to retrieve the sentences that are the most similar to a sentence of the test set: (i) Levenshtein distance (Levenshtein, 1966); (ii) cosine similarity of the average of the word embeddings (Mikolov et al., 2013); and (iii) the cosine similarity between hidden states of the encoder in NMT. The main difference with our work is that they adapt the model sentence-wise (one model for each sentence) whereas the adaptations presented here are document-wise (one model for each test set). Al"
W19-7202,D12-1037,0,0.0251301,"ording to the criteria considered to select sentences (e.g. select sentences of a particular domain, good quality sentences, etc.). In this work, we use the transductive (Vapnik, 1998) data selection methods which use the document to be translated to select sentences that are the most relevant for translating such text. In some cases, the organizations in charge of translating a document are also the owner of the translation model and training data. Therefore, knowing the test set is an advantage that can be helpful for adapting the generic MT model towards the test set (Utiyama et al., 2009; Liu et al., 2012). The approaches presented here consist of building a single NMT model and delay part of the process of training data for adapting the model when the test set is available. Although this implies increasing the time involved in translating a document, it also has some benefits. First, using a single model causes storing multiple task-adapted models not to be necessary. Moreover, identifying the domain of the document (and so, the most appropriate model) before the Dublin, Aug. 20, 2019 |p. 13 same as the general model. As in these experiments we are processing the data using BPE, the limitation"
W19-7202,2015.iwslt-evaluation.11,0,0.810857,"contains some insights of other works that are related to this and Section 4 describes the data selection methods used in the experiments. In Section 5 we perform an analysis of fine-tuning and in Section 6 we build the models used as baselines in later experiments. The results of the main experiments are explained in Section 7 and finally, in Section 8, we conclude and indicate further research that can be carried out in the future. 2 2. Can a model fine-tuned with a subset of indomain data outperform the model fine-tuned with the complete data set? The general uses of fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) consist of using in-domain data set to adapt a model. However, we want to investigate whether applying data selection in smaller in-domain set can also lead to improvements. 3. Can a model fine-tuned with a dataset mixture of general-domain and in-domain data outperform the previous-mentioned models? By considering both datasets (general and in-domain data), the number of candidate sentences is increased. This also poses a challenge to the transductive algorithm as most of the candidate sentences are not indomain. We are interested in exploring whether these alg"
W19-7202,P02-1040,0,0.104441,"news domain. All data sets are tokenized, truecased and Byte Pair Encoding (BPE) (Sennrich et al., 2016) is applied with 89500 merge operations (the number of operations used in the work of Sennrich et al. (2016)). The models have been built using OpenNMT-py (Klein et al., 2017). We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units, vocabulary size of 50000 words for each language. We use different evaluation metrics to evaluate the performance of the models built in the experiments. These models are evaluated on the test sets using several evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The scores assigned by this metrics indicate an estimation of the 1 https://tilde.com/ http://opus.nlpl.eu/EMEA.php 3 http://www.himl.eu/test-sets 2 Dublin, Aug. 20, 2019 |p. 16 quality of the translation (compared to a humantranslated reference). Higher scores of BLEU and METEOR indicate better translation quality. TER is an error metric, therefore lower scores indicate better performance. In each table, scores that are better than the baseline are shown in bold. Furthermore, scores that constitute a statistically significant"
W19-7202,P16-1162,0,0.0545624,"domain rapid20161 data set (1.3M sentence pairs) provided in WMT 2017 News Translation (Bojar et al., 2017). • Health Domain: German-to-English parallel text from the European Medicines Agency (EMEA)2 (Tiedemann, 2009) (361K sentence pairs). For health domain test set we use the Cochrane 3 dataset provided in WMT 2017 biomedical translation shared task (Yepes et al., 2017). Note that the general-domain set contains sentences from a corpus such as Europarl (Koehn, 2005) which causes the domain to be closer to the news domain. All data sets are tokenized, truecased and Byte Pair Encoding (BPE) (Sennrich et al., 2016) is applied with 89500 merge operations (the number of operations used in the work of Sennrich et al. (2016)). The models have been built using OpenNMT-py (Klein et al., 2017). We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units, vocabulary size of 50000 words for each language. We use different evaluation metrics to evaluate the performance of the models built in the experiments. These models are evaluated on the test sets using several evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The scores assig"
W19-7202,W18-6323,1,0.885175,"Missing"
W19-7202,2006.amta-papers.25,0,0.0738165,"re tokenized, truecased and Byte Pair Encoding (BPE) (Sennrich et al., 2016) is applied with 89500 merge operations (the number of operations used in the work of Sennrich et al. (2016)). The models have been built using OpenNMT-py (Klein et al., 2017). We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units, vocabulary size of 50000 words for each language. We use different evaluation metrics to evaluate the performance of the models built in the experiments. These models are evaluated on the test sets using several evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005). The scores assigned by this metrics indicate an estimation of the 1 https://tilde.com/ http://opus.nlpl.eu/EMEA.php 3 http://www.himl.eu/test-sets 2 Dublin, Aug. 20, 2019 |p. 16 quality of the translation (compared to a humantranslated reference). Higher scores of BLEU and METEOR indicate better translation quality. TER is an error metric, therefore lower scores indicate better performance. In each table, scores that are better than the baseline are shown in bold. Furthermore, scores that constitute a statistically significant improvement at level p=0.01"
W19-7202,2009.iwslt-evaluation.12,0,0.0391567,"madi et al., 2015) according to the criteria considered to select sentences (e.g. select sentences of a particular domain, good quality sentences, etc.). In this work, we use the transductive (Vapnik, 1998) data selection methods which use the document to be translated to select sentences that are the most relevant for translating such text. In some cases, the organizations in charge of translating a document are also the owner of the translation model and training data. Therefore, knowing the test set is an advantage that can be helpful for adapting the generic MT model towards the test set (Utiyama et al., 2009; Liu et al., 2012). The approaches presented here consist of building a single NMT model and delay part of the process of training data for adapting the model when the test set is available. Although this implies increasing the time involved in translating a document, it also has some benefits. First, using a single model causes storing multiple task-adapted models not to be necessary. Moreover, identifying the domain of the document (and so, the most appropriate model) before the Dublin, Aug. 20, 2019 |p. 13 same as the general model. As in these experiments we are processing the data using"
W19-7202,D17-1147,0,0.0973666,"Missing"
W19-7202,W17-4719,0,\N,Missing
W19-7202,W17-4717,0,\N,Missing
W19-8629,W04-3250,0,0.278153,"5.97* 34.71 59.24 33.96* 45.64* 35.01* 59.56 33.75* 45.92* 34.92* 59.57 5.1 26.49 54.21 30.21* 51.80 26.55* 54.17* 30.24* 51.89 26.40* 54.47 30.10* 51.71 Results of Models Fine-tuned with Hybrid Data INR 200K lines 100K lines 500K lines 200K lines 100K lines BLEU TER MET. CHRF3 BLEU TER MET. CHRF3 BLEU TER MET. CHRF3 500K lines 200K lines 100K lines BASE13 In addition, we indicate in bold those scores that show an improvement over the baseline (in Table 1 we use BASE13 as the baseline) and add an asterisk if the improvements are statistically significant at p=0.01 (using Bootstrap Resampling (Koehn, 2004), computed with multeval (Clark et al., 2011)). In the table, we can see that using a small subset of data for training the 13th epoch can cause the performance of the model to improve. In the following experiments, we want to compare whether augmenting the candidate set with synthetic data can further boost these improvements. For this reason, we use INR and FDA FDA as baselines. 200K lines 100K lines 5 Table 1: Performance of the BASE13 model, and the models fine-tuned with subsets of the training data. First of all, we present in Table 1 the performance of the model trained with all data fo"
W19-8629,W11-2131,0,0.0619572,"Missing"
W19-8629,L18-1146,0,0.0485705,"Missing"
W19-8629,2015.iwslt-evaluation.11,0,0.0438212,"shold t, then the component max(0, t − CS (ngr)) is 0 and so the n-gram does not contribute to scoring the sentence. Feature Decay Algorithms (Bic¸ici and Yuret, 2011; Bic¸ici, 2013) also retrieve those sentences sharing the highest number of n-grams from the test set. However, in order to increase the variability and avoid selecting the same n-grams, those that have been selected are penalized is proportional to the number of occurrences in L. The score of a sentence is computed as in (2): Adaptation of NMT Models to the Test Set The improvement of NMT models can be performed by fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), i.e. train the models for additional epochs using a small set of indomain data. Alternatively, van der Wees et al. (2017) train models using smaller but more indomain sentences in each epoch of the training process. The use of the test set to retrieve relevant sentences for fine-tuning the model has been explored by Li et al. (2016), adapting a different model for each sentence in the test set, or Poncelas et al. (2018b, 2019b) where they adapt the model for the P score(s, L) = ngr∈Stest 0.5CL (ngr) length(s) (2) where length(s) indicates the number of words in"
W19-8629,W17-4714,0,0.0991687,"to the source side using an MT model. Other uses of monolingual target-side sentences include building the parallel set by using a NULL token in the source side (Sennrich et al., 2016a) or creating language models to improve the decoder (G¨ulc¸ehre et al., 2015). Hoang et al. (2018) improve the model used for back-translation by training this model with increasing amounts of artificial sentences. They iteratively improve the models creating artificial sentences of better quality. Similarly to this paper, the use of artificiallygenerated sentences to fine-tuned models has also been explored by Chinea-Rios et al. (2017) where they select monolingual authentic sentences in the source-side and translate them into the target language, or the work of Poncelas et al. (2019a) where they use back-translated sentences only to adapt the models. 2.2 Transductive Algorithms score(s) = X ngr∈{Stest max(0, t − CL (ngr)) T s} (1) where t is the threshold that indicates the number of occurrences of an n-gram to be considered infrequent. If the number of occurrences of ngr in the selected pool (CL (ngr)) is above the threshold t, then the component max(0, t − CS (ngr)) is 0 and so the n-gram does not contribute to scoring t"
W19-8629,P02-1040,0,0.104082,"ments. For this reason, we use INR and FDA FDA as baselines. 200K lines 100K lines 5 Table 1: Performance of the BASE13 model, and the models fine-tuned with subsets of the training data. First of all, we present in Table 1 the performance of the model trained with all data for 13 epochs (BASE13), as this is when the model converges. We also show the performance of the model when fine-tuning the 12th epoch with the subset of (authentic) data selected by INR (INR column) and FDA (FDA column). In order to evaluate the performance of the models, we present the following evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005), and CHRF (Popovic, 2015). These metrics provide an estimation of the translation quality when the output is compared to a human-translated reference. Note that in general, the higher the score, the better the translation quality is. The only exception is TER which is an error metric and so lower results indicate better quality. BIO 33.52 45.92 34.77 59.43 33.88 45.90 34.94 59.56 NEWS BLEU 26.49 TER 54.19 METEOR 30.21 CHRF3 51.78 BLEU 26.44 TER 54.35 METEOR 30.12 CHRF3 51.67 BLEU TER METEOR CHRF3 BLEU TER METEOR CHRF3 INR HYBR 33.8"
W19-8629,P11-2031,0,0.0402877,"59.56 33.75* 45.92* 34.92* 59.57 5.1 26.49 54.21 30.21* 51.80 26.55* 54.17* 30.24* 51.89 26.40* 54.47 30.10* 51.71 Results of Models Fine-tuned with Hybrid Data INR 200K lines 100K lines 500K lines 200K lines 100K lines BLEU TER MET. CHRF3 BLEU TER MET. CHRF3 BLEU TER MET. CHRF3 500K lines 200K lines 100K lines BASE13 In addition, we indicate in bold those scores that show an improvement over the baseline (in Table 1 we use BASE13 as the baseline) and add an asterisk if the improvements are statistically significant at p=0.01 (using Bootstrap Resampling (Koehn, 2004), computed with multeval (Clark et al., 2011)). In the table, we can see that using a small subset of data for training the 13th epoch can cause the performance of the model to improve. In the following experiments, we want to compare whether augmenting the candidate set with synthetic data can further boost these improvements. For this reason, we use INR and FDA FDA as baselines. 200K lines 100K lines 5 Table 1: Performance of the BASE13 model, and the models fine-tuned with subsets of the training data. First of all, we present in Table 1 the performance of the model trained with all data for 13 epochs (BASE13), as this is when the mod"
W19-8629,E12-1016,0,0.0625541,"Missing"
W19-8629,W18-2703,0,0.0208205,"general-domain terms). A candidate sentence s ∈ S is scored according to the number of infrequent n-grams shared with the set of sentences of the test set Stest , computed as in (1): The proposal of Sennrich et al. (2016a) showed that NMT models can be improved by backtranslating a set of (monolingual) sentences in the target side into the source side using an MT model. Other uses of monolingual target-side sentences include building the parallel set by using a NULL token in the source side (Sennrich et al., 2016a) or creating language models to improve the decoder (G¨ulc¸ehre et al., 2015). Hoang et al. (2018) improve the model used for back-translation by training this model with increasing amounts of artificial sentences. They iteratively improve the models creating artificial sentences of better quality. Similarly to this paper, the use of artificiallygenerated sentences to fine-tuned models has also been explored by Chinea-Rios et al. (2017) where they select monolingual authentic sentences in the source-side and translate them into the target language, or the work of Poncelas et al. (2019a) where they use back-translated sentences only to adapt the models. 2.2 Transductive Algorithms score(s)"
W19-8629,W19-7202,1,0.528743,"side (Sennrich et al., 2016a) or creating language models to improve the decoder (G¨ulc¸ehre et al., 2015). Hoang et al. (2018) improve the model used for back-translation by training this model with increasing amounts of artificial sentences. They iteratively improve the models creating artificial sentences of better quality. Similarly to this paper, the use of artificiallygenerated sentences to fine-tuned models has also been explored by Chinea-Rios et al. (2017) where they select monolingual authentic sentences in the source-side and translate them into the target language, or the work of Poncelas et al. (2019a) where they use back-translated sentences only to adapt the models. 2.2 Transductive Algorithms score(s) = X ngr∈{Stest max(0, t − CL (ngr)) T s} (1) where t is the threshold that indicates the number of occurrences of an n-gram to be considered infrequent. If the number of occurrences of ngr in the selected pool (CL (ngr)) is above the threshold t, then the component max(0, t − CS (ngr)) is 0 and so the n-gram does not contribute to scoring the sentence. Feature Decay Algorithms (Bic¸ici and Yuret, 2011; Bic¸ici, 2013) also retrieve those sentences sharing the highest number of n-grams from"
W19-8629,P17-4012,0,0.105132,"Missing"
W19-8629,R19-1107,1,0.83141,"side (Sennrich et al., 2016a) or creating language models to improve the decoder (G¨ulc¸ehre et al., 2015). Hoang et al. (2018) improve the model used for back-translation by training this model with increasing amounts of artificial sentences. They iteratively improve the models creating artificial sentences of better quality. Similarly to this paper, the use of artificiallygenerated sentences to fine-tuned models has also been explored by Chinea-Rios et al. (2017) where they select monolingual authentic sentences in the source-side and translate them into the target language, or the work of Poncelas et al. (2019a) where they use back-translated sentences only to adapt the models. 2.2 Transductive Algorithms score(s) = X ngr∈{Stest max(0, t − CL (ngr)) T s} (1) where t is the threshold that indicates the number of occurrences of an n-gram to be considered infrequent. If the number of occurrences of ngr in the selected pool (CL (ngr)) is above the threshold t, then the component max(0, t − CS (ngr)) is 0 and so the n-gram does not contribute to scoring the sentence. Feature Decay Algorithms (Bic¸ici and Yuret, 2011; Bic¸ici, 2013) also retrieve those sentences sharing the highest number of n-grams from"
W19-8629,J82-2005,0,0.703201,"Missing"
W19-8629,W15-3049,0,0.0484753,"rformance of the BASE13 model, and the models fine-tuned with subsets of the training data. First of all, we present in Table 1 the performance of the model trained with all data for 13 epochs (BASE13), as this is when the model converges. We also show the performance of the model when fine-tuning the 12th epoch with the subset of (authentic) data selected by INR (INR column) and FDA (FDA column). In order to evaluate the performance of the models, we present the following evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005), and CHRF (Popovic, 2015). These metrics provide an estimation of the translation quality when the output is compared to a human-translated reference. Note that in general, the higher the score, the better the translation quality is. The only exception is TER which is an error metric and so lower results indicate better quality. BIO 33.52 45.92 34.77 59.43 33.88 45.90 34.94 59.56 NEWS BLEU 26.49 TER 54.19 METEOR 30.21 CHRF3 51.78 BLEU 26.44 TER 54.35 METEOR 30.12 CHRF3 51.67 BLEU TER METEOR CHRF3 BLEU TER METEOR CHRF3 INR HYBR 33.87 46.17 35.01 59.53 33.70 46.33 35.23 60.03 26.76 54.36 30.48* 52.35* 26.80* 54.34 30.51"
W19-8629,P16-1009,0,0.35934,"sentences (a set of sentence-pairs in which each sentence is paired with its translation). As Neural Machine Translation (NMT) models typically achieve best performance when using large sets of parallel sentences, they can benefit from the sentences created by Natural Language Generation (NLG) systems. Although artificial data is expected to be of lower quality than authentic sentences, it still can help the model to learn how to better generalize over the training instances and produce better translations. A popular technique used to create artificial data is the back-translation technique (Sennrich et al., 2016a; Poncelas et al., 2018c). This consists of generating sentences in the source language by translating monolingual sentences in the target language. Then, these sentences in both languages are 219 Proceedings of The 12th International Conference on Natural Language Generation, pages 219–228, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics how they can be combined into a single hybrid set. Finally, we investigate whether the hybrid sets retrieved by TAs can be more useful than the authentic set of sentences to fine-tune NMT models. complete test set using t"
W19-8629,P16-1162,0,0.790055,"sentences (a set of sentence-pairs in which each sentence is paired with its translation). As Neural Machine Translation (NMT) models typically achieve best performance when using large sets of parallel sentences, they can benefit from the sentences created by Natural Language Generation (NLG) systems. Although artificial data is expected to be of lower quality than authentic sentences, it still can help the model to learn how to better generalize over the training instances and produce better translations. A popular technique used to create artificial data is the back-translation technique (Sennrich et al., 2016a; Poncelas et al., 2018c). This consists of generating sentences in the source language by translating monolingual sentences in the target language. Then, these sentences in both languages are 219 Proceedings of The 12th International Conference on Natural Language Generation, pages 219–228, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics how they can be combined into a single hybrid set. Finally, we investigate whether the hybrid sets retrieved by TAs can be more useful than the authentic set of sentences to fine-tune NMT models. complete test set using t"
W19-8629,2006.amta-papers.25,0,0.0421421,"e INR and FDA FDA as baselines. 200K lines 100K lines 5 Table 1: Performance of the BASE13 model, and the models fine-tuned with subsets of the training data. First of all, we present in Table 1 the performance of the model trained with all data for 13 epochs (BASE13), as this is when the model converges. We also show the performance of the model when fine-tuning the 12th epoch with the subset of (authentic) data selected by INR (INR column) and FDA (FDA column). In order to evaluate the performance of the models, we present the following evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005), and CHRF (Popovic, 2015). These metrics provide an estimation of the translation quality when the output is compared to a human-translated reference. Note that in general, the higher the score, the better the translation quality is. The only exception is TER which is an error metric and so lower results indicate better quality. BIO 33.52 45.92 34.77 59.43 33.88 45.90 34.94 59.56 NEWS BLEU 26.49 TER 54.19 METEOR 30.21 CHRF3 51.78 BLEU 26.44 TER 54.35 METEOR 30.12 CHRF3 51.67 BLEU TER METEOR CHRF3 BLEU TER METEOR CHRF3 INR HYBR 33.87 46.17 35.01 59.53 33.70 4"
W19-8629,D17-1147,0,0.0346298,"Missing"
W19-8629,W05-0909,0,\N,Missing
W19-8629,W17-4719,0,\N,Missing
W19-8629,W13-2206,0,\N,Missing
W99-1024,P98-1022,0,0.24862,"ning together with other trees to form a representation for a sentence is used to contribute to the overall probability of that representation given the corpus. 2.1 Opportunities for Hybridity—LFG DOP DOP-based approaches are necessarily limited to those contextual dependencies actually occurring in the corpus, which is a reflection of surface phenomena only. Given its facility to capture and provide representations of linguistic phenomena other than those occurring at surface structure, the functional structures of LFG have been allied to the techniques of DOP to create a new model, LFG-DOP ([3]), which adds a measure of robustness not available to models based solely on LFG. We suggest that this framework has the potential to be utilised for MT. As with DOP, LFG-DOP needs to be defined using four parameters. Its representations are simply lifted en bloc from LFG theory, so that each string is annotated with a c-structure, an f-structure, and a mapping <j>between them, with well-formedness conditions operating solely on f-structure, as usual. Since we are now deeding with (c,f) pairs of structure, the Root and Frontier decom position operations of DOP need to be adapted to stipulate"
W99-1024,E89-1037,0,0.0929919,"Missing"
W99-1024,C92-2103,0,0.0189761,"ture. One line of investigation which we now develop that can overcome this linear restriction is to use LFG-DOP ([3]) as the basis for an innovative MT system, using LFG’s r-equations to relate translation fragments between languages. Proceedings of NODALIDA 1999 250 4.1 M odel 1: {c,<p, f , r , f') Using separate language corpora, this simple, linear model builds a target f-structure / ' from a source c-structure c and f-structure /, the mapping between them 4>, and the tauequations r. Prom this target f-structure / ', a target string is generated via the standard LFG generation algorithms ([7]; [11]). The probability of the target f-structure Rt being the translation of the source string W, is: I W,) (9) = ‘ E P[Rs I W,).P{R^ I R„ W.) ‘ = R t.. Z p ( R ,  w ,). p {r ,  r ,) R t,. incorporating a Markov assumption that the target f-structure’s derivation from a source string (via 4>and r) is independent of the original words involved: it is dependent solely on the monolingual LFG-DOP representation assigned. This is an attempt to avoid as much as possible the sparse data problem, given that in all probability we will never have enough LFG-DOP fragments to model these numbers with"
W99-1024,C88-2150,0,0.0427909,"One line of investigation which we now develop that can overcome this linear restriction is to use LFG-DOP ([3]) as the basis for an innovative MT system, using LFG’s r-equations to relate translation fragments between languages. Proceedings of NODALIDA 1999 250 4.1 M odel 1: {c,<p, f , r , f') Using separate language corpora, this simple, linear model builds a target f-structure / ' from a source c-structure c and f-structure /, the mapping between them 4>, and the tauequations r. Prom this target f-structure / ', a target string is generated via the standard LFG generation algorithms ([7]; [11]). The probability of the target f-structure Rt being the translation of the source string W, is: I W,) (9) = ‘ E P[Rs I W,).P{R^ I R„ W.) ‘ = R t.. Z p ( R ,  w ,). p {r ,  r ,) R t,. incorporating a Markov assumption that the target f-structure’s derivation from a source string (via 4>and r) is independent of the original words involved: it is dependent solely on the monolingual LFG-DOP representation assigned. This is an attempt to avoid as much as possible the sparse data problem, given that in all probability we will never have enough LFG-DOP fragments to model these numbers with any re"
W99-1024,C00-2092,0,\N,Missing
W99-1024,C98-1022,0,\N,Missing
Y04-1016,P04-1041,1,0.861636,"Missing"
Y04-1016,C02-1126,0,0.0611897,"Missing"
Y04-1016,P02-1043,0,0.030143,"robust, state-of-art resources. However, (with few exceptions) the grammars induced are mostly &quot;shallow&quot;, i.e. without the deep syntactic (dependency) or semantic information captured by deep, constraint-based grammar formalisms such as LFG or HPSG. A recent body of research had extended the basic paradigm of automatic PCFG acquisition from treebanks to the extraction of deep, wide-coverage, constraint-based grammars and lexical resources such as LFG (Cahill et al., 2002; Cahill et al., 2003; Cahill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short descripti"
Y04-1016,W03-2401,0,0.0943368,"Missing"
Y04-1016,H94-1020,0,0.543341,"ch had extended the basic paradigm of automatic PCFG acquisition from treebanks to the extraction of deep, wide-coverage, constraint-based grammars and lexical resources such as LFG (Cahill et al., 2002; Cahill et al., 2003; Cahill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short description of the CTB (Xue et al., 2002). We present an automatic f-structure annotation algorithm for the CTB. The algorithm generates proto-f-structures (Cahill et al., 2002). Proto-fstructures capture basic but possibly incomplete predicate-argument-adjunct structure a"
Y04-1016,P04-1047,1,0.631032,"Missing"
Y04-1016,P02-1035,0,0.530094,"In order to assess the quality of the extracted grammars we carried out three types of parsing experiments: • In experiment 1 we evaluate the CFG tree output of our parsers against the original trees for strings length &lt;= 40 in articles 301-325 CTB, reporting f-scores for labelled and unlabelled bracketings using evalb. • In experiment 2 we evaluate the f-structures generated by our grammars against the manually annotated 50 gold-standard f-structures for randomly selected trees from articles 301-325 using the triple-based dependency encoding and evaluation software from (Crouch et al., 2002; Riezler et al., 2002). • In experiment 3 we evaluate the f-structures generated by our grammars against the fstructures for the full 318 test strings as generated by the automatic f-structure annotation algorithm for the original trees in articles 301-325 CTB using the triple-based dependency encoding and evaluation software from (Crouch et al., 2002; Riezler et al., 2002). - 168 - PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo 7.2.1 Experiment 1 (Tree-Based Evaluation) Table 5 describes the results obtained in experiment 1. In this experiment we evaluate the parse output generated by our grammars ag"
Y04-1016,C02-1145,0,0.217827,"ill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short description of the CTB (Xue et al., 2002). We present an automatic f-structure annotation algorithm for the CTB. The algorithm generates proto-f-structures (Cahill et al., 2002). Proto-fstructures capture basic but possibly incomplete predicate-argument-adjunct structure as they do not yet resolve long-distance dependencies. Section 4 outlines the architecture underlying the automatic fstructure annotation algorithm and how it was applied to the CTB. Section 5 provides an evaluation of the f-s"
Y04-1016,C04-1024,0,\N,Missing
Y04-1016,P03-1046,0,\N,Missing
Y04-1016,P03-1056,0,\N,Missing
Y09-1007,apidianaki-2008-translation,1,0.842285,"semantically distant ones. We explain how the semantic information acquired by this method can be exploited by METEOR for evaluation. Exploiting this kind of information permits the capturing of domain-relevant synonymy relations, overrides the need for predefined resources and permits the use of METEOR’s synonymy module for evaluation in languages other than English. The only requirement is that a parallel corpus, needed for training the sense induction method, be available in the concerned languages. 3 Data-driven sense induction The semantic analysis method used here is the one proposed in Apidianaki (2008). Her method reveals the senses of ambiguous words of one language by clustering their TEs in another language. The created sense-clusters group semantically similar equivalents, whose relations are discovered from a parallel aligned training corpus. The method is based on the distributional hypotheses of meaning (Harris, 1954) and of semantic similarity (Miller and Charles, 1991), and on the assumption of sense correspondence between words in translation relation in real texts. The analysis is thus performed by combining distributional and translation information from a parallel corpus. Being"
Y09-1007,E09-1010,1,0.816811,"in METEOR for two reasons. Firstly, as the WN module of METEOR is shown to improve the correlation with human judgment, it would be interesting to see if our method has the same effect. Secondly, METEOR is a well established and stable metric and improvement on such a metric would be of more practical use. A clear advantage of this automatically created inventory in comparison to WordNet is that the information is acquired directly from corpora. It is thus relative to the domains of the processed texts and may concern languages for which WordNet-type resources are not available. Additionally, Apidianaki (2009) showed that this data-driven sense induction method provides, as a by-product, information that can be exploited by an unsupervised Word Sense Disambiguation classifier. It consists of the SL distributional information that reveals the similarity of the TEs and can serve to disambiguate new instances of the ambiguous words. This is another advantage of this unsupervised method: it makes it possible to carry out a disambiguation step during evaluation, which could replace METEOR’s “poor-man’s synonymy detection algorithm” (Banerjee and Lavie, 2005). Nevertheless, this inventory is also charact"
Y09-1007,W05-0909,0,0.553531,"French and we analyze the obtained results. Then we show the advantages of integrating automatically acquired semantic information into MT evaluation. Finally we conclude, together with avenues for further work. 2 Lexical variation in existing evaluation metrics BLEU (Papineni et al., 2002) captures lexical variation by the use of multiple reference translations. However, this has been shown to be a rather problematic solution: even if numerous human translations of the same original text are available, which is rarely the case, their use poses additional problems during evaluation.2 METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) matches unigrams between the hypothesis and the reference in a flexible way, by using a stemming and a synonymy module. While the first matches different word forms, the second increases the number of pertinent translations by exploiting WordNet information: a translation is considered to be correct not only if it exactly corresponds to the reference, but also if it is semantically similar to it, i.e. found in the same WordNet synset. Nevertheless, predefined semantic resources like WordNet present some limitations. They cannot be easily updated and adapted to the do"
Y09-1007,P05-1048,0,0.0225795,"on scores often do not reflect the quality of translation, there is a growing tendency towards increasing the correlation of the metrics with human judgments of translation quality. An important factor determining this correlation is the identification of sense correspondences between the hypothesis and the reference, which may exist even if the words used in the translations differ. Capturing this type of correspondence would also allow a more conclusive estimation of the impact of WSD techniques on MT systems than is possible with the current evaluation metrics (Callison-Burch et al., 2006; Carpuat and Wu, 2005; Chan et al., 2007). In this paper, we show how variation at the unigram level can be captured during evaluation using information induced from parallel corpora by an unsupervised sense induction method. This method generates bilingual semantic inventories, where the senses of the words of one language are described by clusters of their semantically similar translation equivalents (TEs) in another language. These sense-clusters, which are similar to WordNet1 synsets, can serve to capture correspondences between synonymous words found in the compared translations. 1 Copyright 2009 by Marianna"
Y09-1007,W08-0309,0,0.0519018,"gments In order to calculate the correlation that METEOR has with human judgments during evaluation in French, we use the WMT08 evaluation shared task dataset.10 All English–French human rankings (307 in total), distributed during this shared evaluation task for estimating the correlation of automatic metrics to human judgments of translation quality, were used for our experiments. The rankings provided here are at the level of the segment. To measure the correlation of the automatic metrics with the human judgments of translation quality, we use Spearman’s rank order correlation coefficient (Callison-Burch et al., 2008). Spearman’s correlation is defined as in (1), where d is the difference between corresponding values in rankings and n is the length of the rankings. ρ=1−( P 6 d2 ) n(n2 − 1) (1) An automatic evaluation metric with a higher correlation value is considered to make predictions that are more similar to the human judgments than a metric with a lower value. For measuring the consistency of the automatic metrics with human judgments, we use the pairwise consistent percentage (Callison-Burch et al., 2008). For every pairwise comparison of two systems on a single sentence by a person, the automatic m"
Y09-1007,P07-1005,0,0.0176917,"reflect the quality of translation, there is a growing tendency towards increasing the correlation of the metrics with human judgments of translation quality. An important factor determining this correlation is the identification of sense correspondences between the hypothesis and the reference, which may exist even if the words used in the translations differ. Capturing this type of correspondence would also allow a more conclusive estimation of the impact of WSD techniques on MT systems than is possible with the current evaluation metrics (Callison-Burch et al., 2006; Carpuat and Wu, 2005; Chan et al., 2007). In this paper, we show how variation at the unigram level can be captured during evaluation using information induced from parallel corpora by an unsupervised sense induction method. This method generates bilingual semantic inventories, where the senses of the words of one language are described by clusters of their semantically similar translation equivalents (TEs) in another language. These sense-clusters, which are similar to WordNet1 synsets, can serve to capture correspondences between synonymous words found in the compared translations. 1 Copyright 2009 by Marianna Apidianaki, Yifan He"
Y09-1007,2009.eamt-1.7,1,0.813123,"the domains of the processed texts and, most importantly, they are not publicly available for languages other than English. This is an important issue concerning METEOR as, when it is used for evaluation in languages other than English, only the exact and stemming matching modules are used, while the synonymy module is not operational and is omitted. This explains why Lavie and Agarwal (2007) propose to develop new synonymy modules for languages other than English, that would be based on alternative methods and could be used in the place of WordNet. Some other metrics (Owczarzak et al., 2007; He and Way, 2009) go beyond pure string matching. They look for “deeper” correspondences and thus correlate better with human judgments of translation quality. The above-mentioned metrics both use syntactic structure and dependency information in order to capture variations between sentences. In Owczarzak et al. (2007), lexical variation is also accommodated by adding WordNet synonyms into the matching process. In previous work by Owczarzak et al. (2006), lexical and syntactic paraphrases were extracted from the bitext used for evaluation using word and phrase alignment. In their work, the target language (TL)"
Y09-1007,2005.mtsummit-papers.11,0,0.011162,"ed on the distributional hypotheses of meaning (Harris, 1954) and of semantic similarity (Miller and Charles, 1991), and on the assumption of sense correspondence between words in translation relation in real texts. The analysis is thus performed by combining distributional and translation information from a parallel corpus. Being totally data-driven this sense induction method is language-independent and permits the creation of sense inventories for different language pairs. 3.1 The training data The training corpus used here is the sentence-aligned English(EN) – French(FR) part of Europarl (Koehn, 2005), which has been lemmatized and tagged by part-of-speech (POS) (Schmid, 1994). As the semantic analysis method is rather sensible to spurious alignments, a number of filters have been applied prior to word alignment in order to ensure the results with the least possible noise. First, function words were deleted in order to keep only the lemmas of content words. Then sentences containing more than five content words (and their translations) were deleted, as well as the sentence pairs presenting a great difference in length (cases where one sentence was three times longer than the other). After"
Y09-1007,P07-2045,0,0.0106248,"Missing"
Y09-1007,W07-0734,0,0.0662391,"obtained results. Then we show the advantages of integrating automatically acquired semantic information into MT evaluation. Finally we conclude, together with avenues for further work. 2 Lexical variation in existing evaluation metrics BLEU (Papineni et al., 2002) captures lexical variation by the use of multiple reference translations. However, this has been shown to be a rather problematic solution: even if numerous human translations of the same original text are available, which is rarely the case, their use poses additional problems during evaluation.2 METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) matches unigrams between the hypothesis and the reference in a flexible way, by using a stemming and a synonymy module. While the first matches different word forms, the second increases the number of pertinent translations by exploiting WordNet information: a translation is considered to be correct not only if it exactly corresponds to the reference, but also if it is semantically similar to it, i.e. found in the same WordNet synset. Nevertheless, predefined semantic resources like WordNet present some limitations. They cannot be easily updated and adapted to the domains of the processed tex"
Y09-1007,J04-2003,0,0.0210714,"been applied at the level of the lexicons as well: first, the TEs of the ambiguous words were filtered on the basis of their score;5 then, an intersection filter was applied, which discards any translation correspondences not found in both lexicons. While eliminating many false TEs, this process eliminated some good ones as well. The reason why we opted for this filtering is that the negative effect of the elimination of good TEs on the semantic analysis is less important than the noise present in the lexicons.6 3 4 5 6 Aligning word types rather than tokens decreases data sparseness effects (Nießen and Ney, 2004). We aligned the corpus using two Giza++ configurations, with and without the mkcls component. As the lexicons generated from the two alignments contained some different entries (SL words), we kept their union in order to increase the coverage. The adopted threshold (0.03) was defined empirically. This could be described as an increase in precision – which is more important in lexicography applications (Och and Ney, 2003) – and a decrease in recall. 55 Table 1: Entries from the EN–FR and the FR–EN sense inventories. Language POS Nouns EN–FR Verbs Adjectives Nouns FR–EN Verbs Adjectives Source"
Y09-1007,J03-1002,0,0.0123803,"c analysis method is rather sensible to spurious alignments, a number of filters have been applied prior to word alignment in order to ensure the results with the least possible noise. First, function words were deleted in order to keep only the lemmas of content words. Then sentences containing more than five content words (and their translations) were deleted, as well as the sentence pairs presenting a great difference in length (cases where one sentence was three times longer than the other). After these filtering steps, word alignment was performed at the level of word types using Giza++ (Och and Ney, 2003).3 Two bilingual lexicons, one for each translation direction (EN–FR/FR–EN), were built from the alignment of word types. In these lexicons, each SL word (w) is associated with the set of TEs to which it was aligned.4 Given the sensibility of the sense induction method to noise, some filtering steps have been applied at the level of the lexicons as well: first, the TEs of the ambiguous words were filtered on the basis of their score;5 then, an intersection filter was applied, which discards any translation correspondences not found in both lexicons. While eliminating many false TEs, this proce"
Y09-1007,W07-0714,1,0.921275,"Missing"
Y09-1007,P09-1034,0,0.204466,"Missing"
Y09-1007,P02-1040,0,0.0878175,"we present how lexical variation is dealt with in existing MT evaluation metrics. In section 3, we describe the sense induction method and the training data used. In section 4, we explain how the automatically built sense inventory is integrated into METEOR. In section 5, we present the experiments carried out in English and in French and we analyze the obtained results. Then we show the advantages of integrating automatically acquired semantic information into MT evaluation. Finally we conclude, together with avenues for further work. 2 Lexical variation in existing evaluation metrics BLEU (Papineni et al., 2002) captures lexical variation by the use of multiple reference translations. However, this has been shown to be a rather problematic solution: even if numerous human translations of the same original text are available, which is rarely the case, their use poses additional problems during evaluation.2 METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) matches unigrams between the hypothesis and the reference in a flexible way, by using a stemming and a synonymy module. While the first matches different word forms, the second increases the number of pertinent translations by exploiting Wor"
Y09-1007,W09-0441,0,0.0457594,"of the text, which makes them more appropriate to the task at hand than synonyms extracted from an external resource like WordNet. However, an important issue concerning the equivalence sets is not addressed: the TL equivalents of a SL word/phrase are not always semantically related. SL words may be ambiguous, in which case their equivalents translate their different senses. There are metrics that use paraphrases or textual entailment features to facilitate automatic evaluation, which are related to our proposed metric. Paraphrase-based metrics, such as ParaEval (Zhou et al., 2006) and TERp (Snover et al., 2009), use paraphrases mined from the corpus as “synonyms”; while in the Textual Entailment-based approach (Pad´o et al., 2009), the metric tries to 2 BLEU puts very few constraints on how n-gram matches can be drawn from the multiple reference translations and so it allows a too high amount of translation variation. Apart from that, the notion of recall - an important parameter in the evaluation of translation quality - is difficult to formulate over multiple reference translations and is not thus taken into account by BLEU (Callison-Burch et al., 2006). 54 determine whether entailment can be infe"
Y09-1007,W06-1610,0,0.0922879,"hrases relevant to the domain of the text, which makes them more appropriate to the task at hand than synonyms extracted from an external resource like WordNet. However, an important issue concerning the equivalence sets is not addressed: the TL equivalents of a SL word/phrase are not always semantically related. SL words may be ambiguous, in which case their equivalents translate their different senses. There are metrics that use paraphrases or textual entailment features to facilitate automatic evaluation, which are related to our proposed metric. Paraphrase-based metrics, such as ParaEval (Zhou et al., 2006) and TERp (Snover et al., 2009), use paraphrases mined from the corpus as “synonyms”; while in the Textual Entailment-based approach (Pad´o et al., 2009), the metric tries to 2 BLEU puts very few constraints on how n-gram matches can be drawn from the multiple reference translations and so it allows a too high amount of translation variation. Apart from that, the notion of recall - an important parameter in the evaluation of translation quality - is difficult to formulate over multiple reference translations and is not thus taken into account by BLEU (Callison-Burch et al., 2006). 54 determine"
Y09-1007,E06-1032,0,\N,Missing
Y09-1007,W06-3112,1,\N,Missing
Y09-1019,W05-0909,0,0.059657,"Missing"
Y09-1019,P07-1020,0,0.209869,"Missing"
Y09-1019,J96-1002,0,0.0373506,"describe the features used in the experiments, and the pre-processing required. Section 7 presents the results obtained, and offers some analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work Brown et al. (1991) were the first to propose the use of dedicated WSD models in word-based SMT systems. Results were limited to the case of binary disambiguation, i.e., deciding between only two possible translation candidates, and to a reduced set of common words. A significant improvement in translation was reported according to manual evaluation. Berger et al. (1996) suggested context-sensitive modeling of word translations in order to integrate local contextual information into their IBM translation models using a Maximum Entropy (MaxEnt) model, but the work is not supported by any significant evaluation results. García Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER)."
Y09-1019,J93-2003,0,0.0107764,"ng. In the second group, predictions are allowed to interact with other models (e.g., language, distortion, additional translation models etc.) during decoding time. The present work falls into the second type of interaction methods. 3 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . e I of a source sentence f1J = f1 . . . f J is chosen to maximize (1): arg max P(e1I |f1J ) = arg max P( f 1J |e1I ) P(e1I ) I ,e1I (1) I ,e1I where P( f 1J |e1I ) and P(e1I ) denote respectively the translation model and the target-language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( f 1I |e1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P ( e1I |f 1 J ) = ∑λ I m h m ( f 1 J , e1I , s1K ) + λLM log P(e1 ) (2) m =1 where s1K = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ ,..., eˆ ) and ( fˆ ,..., fˆ ) such that (we set i0 = 0) (3): 1 k ∀1 ≤ k ≤ K , sk = (ik ; bk, jk), 1 k eˆk = eik−1 +1...eik , fˆk = f b ... f j k k ("
Y09-1019,H91-1025,0,0.219377,"79 170 The remainder of the paper is organized as follows. In Section 2 we discuss related work. Section 3 provides a brief overview of PB-SMT. In Section 4 we describe how we model dependency information as context-informed features in our baseline log-linear PB-SMT system. Section 5 describes the memory-based classification approach. In Section 6 we describe the features used in the experiments, and the pre-processing required. Section 7 presents the results obtained, and offers some analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work Brown et al. (1991) were the first to propose the use of dedicated WSD models in word-based SMT systems. Results were limited to the case of binary disambiguation, i.e., deciding between only two possible translation candidates, and to a reduced set of common words. A significant improvement in translation was reported according to manual evaluation. Berger et al. (1996) suggested context-sensitive modeling of word translations in order to integrate local contextual information into their IBM translation models using a Maximum Entropy (MaxEnt) model, but the work is not supported by any significant evaluation re"
Y09-1019,I05-2021,0,0.158454,"-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvement"
Y09-1019,2007.mtsummit-papers.11,0,0.220443,"ses. Approaches to include source context for proper selection of target phrases have been inspired by methods for word sense disambiguation (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 170–179 170 The remainder of the paper is organized"
Y09-1019,P07-1005,0,0.234639,"to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is"
Y09-1019,W06-1628,0,0.185399,"Missing"
Y09-1019,P01-1027,0,0.430501,"Missing"
Y09-1019,W07-0719,0,0.377407,"p a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and is most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008)"
Y09-1019,W08-0302,0,0.104114,"influence the weighting and selection of target phrases. Approaches to include source context for proper selection of target phrases have been inspired by methods for word sense disambiguation (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages"
Y09-1019,2009.eamt-1.32,1,0.872115,"n (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 170–179 170 The remainder of the paper is organized as follows. In Section 2 we discuss related work. Section 3 provides a brief overview of PB-SMT. In Section 4 we describe how we model d"
Y09-1019,D08-1039,0,0.0899696,"integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is evaluated in a re-ranking framework; slight improvements are observed over IBM model 1 in terms of BLEU and TER (Snover et al., 2006). Target-language models arguably play the most significant role in today’s PB-SMT systems. However, for some time now people have believed that some incorporation of source language information into SMT systems was bound to help. Stroppa et al. (2007) added source-side contextual features to a state-of-the-art log-linear PB"
Y09-1019,N03-1017,0,0.0255589,"Missing"
Y09-1019,P07-2045,0,0.00699313,"features from the head-words of the SMT phrases, identified from the dependency graph generated for the source sentence (as described earlier in Section 4). (ii) They filter out phrases from phrase table entries for which P( eˆk |fˆk ) < 0.0002. In contrast, we keep all phrase pairs for more discrimination. (iii) Their experimental data contains 95K English-to-French training pairs, while we trained our models on about three times as many (286K) Dutch-to-English translation pairs, a less explored direction. 6.2 Pre-processing As (Stroppa et al., 2007) point out, PB-SMT decoders such as Moses (Koehn et al., 2007) rely on a static phrase table, represented as a list of aligned phrases accompanied by several estimated metrics. Since these features do not express the context information in which those phrases occur, no dependency information is kept in the phrase table, and there is no way to recover this information from the phrase table. In order to take into account the dependency information features within such decoders, the test text to be translated is pre-processed. Each word appearing in the test set (and, during development, the development set) is assigned a unique identifier. First we prepare"
Y09-1019,P06-1096,0,0.0168677,"es learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier (Daelemans et al., 2005) that implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian— English and Chinese—English IWSLT tasks. Several proposals have recently been made to fully exploit the accuracy and the flexibility of discriminative learning (Cowan et al., 2006; Liang et al., 2006). Work of this type generally 171 requires a redefinition of the training procedure; in contrast, our approach introduces new features while retaining the strength of existing state-of-the-art systems. Like the work of (Max et al., 2008), the present work is directly motivated by and is an extension of the approach of (Stroppa et al., 2007). The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focuses on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for English-to-German, (Max et al., 2008) conduct experiments fro"
Y09-1019,2008.eamt-1.17,0,0.39666,"fier (Daelemans et al., 2005) that implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian— English and Chinese—English IWSLT tasks. Several proposals have recently been made to fully exploit the accuracy and the flexibility of discriminative learning (Cowan et al., 2006; Liang et al., 2006). Work of this type generally 171 requires a redefinition of the training procedure; in contrast, our approach introduces new features while retaining the strength of existing state-of-the-art systems. Like the work of (Max et al., 2008), the present work is directly motivated by and is an extension of the approach of (Stroppa et al., 2007). The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focuses on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for English-to-German, (Max et al., 2008) conduct experiments from English-to-French. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT base"
Y09-1019,P03-1021,0,0.0118444,"ized, these weights can be seen as the posterior probabilities of the target phrases eˆ k , which thus give access to P( eˆk |fˆk ,DI( fˆk )). Therefore, the expected feature is derived as in (7): hˆmbl = log P( eˆk |fˆk ,DI( fˆk )) (7) In addition to the above feature, we derived a simple binary feature hˆbest . The feature hˆbest is defined as in (8): hˆbest = 1 0 if eˆk maximizes P( eˆk |fˆk ,CI( fˆk )) otherwise, (8) We performed experiments by integrating these two features hˆmbl and hˆbest directly into the log-linear model. Their weights are optimized using minimum error-rate training (Och, 2003) on a held-out development set for each of the experiments. Our approach in terms of experimental set-up and classification of a source phrase along with contextual dependency features differs from Stroppa et al., (2009) and Haque et al., (2009) in the following respects: (i) Stroppa et al. (2007) and Haque et al., (2009) integrate local, position-specific contextual features into the log-linear framework. Here we integrate a feature encoding positionindependent dependency information; (ii) Haque et al. (2009) interpolate the context-dependent phrase translation probability with the forward ph"
Y09-1019,P02-1038,0,0.32297,"g time. The present work falls into the second type of interaction methods. 3 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . e I of a source sentence f1J = f1 . . . f J is chosen to maximize (1): arg max P(e1I |f1J ) = arg max P( f 1J |e1I ) P(e1I ) I ,e1I (1) I ,e1I where P( f 1J |e1I ) and P(e1I ) denote respectively the translation model and the target-language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( f 1I |e1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P ( e1I |f 1 J ) = ∑λ I m h m ( f 1 J , e1I , s1K ) + λLM log P(e1 ) (2) m =1 where s1K = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ ,..., eˆ ) and ( fˆ ,..., fˆ ) such that (we set i0 = 0) (3): 1 k ∀1 ≤ k ≤ K , sk = (ik ; bk, jk), 1 k eˆk = eik−1 +1...eik , fˆk = f b ... f j k k (3) The translational features depend only on pairs of source/target phrases and do not take into account any context of these phrases, i.e. each feature hm i"
Y09-1019,P02-1040,0,0.0773823,"Missing"
Y09-1019,2006.amta-papers.25,0,0.0152464,"astic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is evaluated in a re-ranking framework; slight improvements are observed over IBM model 1 in terms of BLEU and TER (Snover et al., 2006). Target-language models arguably play the most significant role in today’s PB-SMT systems. However, for some time now people have believed that some incorporation of source language information into SMT systems was bound to help. Stroppa et al. (2007) added source-side contextual features to a state-of-the-art log-linear PB-SMT system by incorporating contextdependent phrasal translation probabilities learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such"
Y09-1019,2007.tmi-papers.28,1,0.904233,"Missing"
Y09-1019,tiedemann-nygaard-2004-opus,0,0.0233867,"l. Thus we create a dynamic phrase table. A lexicalized reordering model is used for all the experiments undertaken on development and test texts. The source phrase in the reordering table is replaced by the sequence of unique identifiers when the new phrase table is created. After replacing all words by their unique identifyers, we perform MERT using our new phrase table to optimize the feature weights. 7 Results and Analysis The experiments were carried out on the Dutch-to-English Open Subtitles corpus,2 which is collected as part of the Opus collection of freely available parallel corpora (Tiedemann and Nygaard, 2004). The corpus contains user-contributed translations of movie subtitles. The training text contains 286,160 sentences; the development set and test set each contain 1,000 sentences. Dutch sentences were parsed using Tadpole 3 , a morphosyntactic analyzer and dependency parser (Van den Bosch et al., 2007). 2 3 http://urd.let.rug.nl/tiedeman/OPUS/OpenSubtitles.php http://ilk.uvt.nl/tadpole/ 175 Table 1: Experiments with words and part-of-speech. Experiments Baseline Word±2 POS±2 POS±2* Word±2+POS±2 BLEU 32.39 32.48 33.07 33.29 32.59 NIST 6.11 6.11 6.13 6.17 6.09 METEOR 55.39 55.72 56.17 55.72 55."
Y09-1019,H05-1097,0,0.0687212,"present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and is most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality h"
Y09-1019,N04-1033,0,0.0677221,"his feature is denoted as PW (parent word). Together we refer to these dependency features as the grammatical dependency information (DI) of the focus phrase fˆk , DI ( fˆk ). They are expressed as the conditional probability of the target phrase given the source phrase fˆk and its grammatical dependency information DI ( fˆk ), as in (6): (6) hˆm ( fˆk , DI ( fˆk ), eˆk , sk) = log P ( eˆk |fˆk , DI( fˆk )) 5 Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases. In the case of grammatical dependency-informed features, which include the identity of the parent word of the focus phrase, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers that are able to estimate P ( eˆk |fˆk , DI ( fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source—target phrase translations to a new source phrase to be translate"
Y09-2027,A94-1010,0,0.38398,"rder (used with emphasis and complex structures) language. Therefore, applying adaptation techniques on such a language pair could produce interesting findings. For adaptation purposes, previous research used similarity metrics to cluster heterogeneous corpus data into sub-corpora with homogeneous topics. In order to compute the distance Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Josef van Genabith, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 670–677 670 between a sentence and a cluster, different similarity metrics have been proposed. (Carter, 1994) introduced an entropy reduction based similarity metric to cluster a multi-domain monolingual corpus. A regular expression based similarity function has been defined to build class specific language models (Hasan and Ney, 2005). In our research, we explore a clustering technique based on an n-gram overlap metric to extract sentences similar to in-domain text from large outof-domain training data. We employ domain adaptation techniques to adapt an out-of-domain bilingual corpus to an in-domain SMT model using clustering to extract sentences similar to in-domain text from large out-of-domain tr"
Y09-2027,W07-0722,0,0.097568,"g the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-domain and out-of-domain language models as log-linear features in an SMT model. They also used multiple decoding paths (Birch et al. 2007) for combining multiple domain translation tables in the state-of-the-art PB-SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain"
Y09-2027,eck-etal-2004-language,0,0.312628,"mbination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work, we conduct experiments on the English—Hindi language pair. Like other Indian languages, Hindi is also a free phrase order (used with emphasis and complex structures) language. Therefore, applying adaptation techniques on such a language pair could produce interesting findings. For adaptation purposes, previous research used similarity metrics to cluster heterogeneous corpus data into sub-corpora w"
Y09-2027,W08-0334,0,0.0119442,"m on news stories. For the translation of each source text, a large monolingual data set in the target language is searched for documents that might be comparable to the source text. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results show substantial gains. Lim and Kirchhoff (2008) proposed a method for incorporating out-of-domain data through phrase generalization in order to improve the Italian-English translation quality. They showed a noticeable improvement in translation quality. Finch and Sumita (2008) employed probabilistic mixture weights to combine two models for questions and declarative sentences with a general model. Foster and Kuhn (2007) used distance based weights in a mixture model. In contrast to their work, Finch and Sumita (2008) used a probabilistic classifier to determine a vector of probability representing classmembership. They performed experiments on a number of language pairs and experimental results showed the usefulness of their method. Domain adaptation techniques can be broadly divided into two categories: (i) adaptation techniques to improve word alignment models; s"
Y09-2027,W07-0717,0,0.165775,"ectly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular expressions to build class specific language models. They proposed a method of interpolating class specific and global language models following the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-d"
Y09-2027,2005.eamt-1.17,0,0.514059,"etrics to cluster heterogeneous corpus data into sub-corpora with homogeneous topics. In order to compute the distance Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Josef van Genabith, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 670–677 670 between a sentence and a cluster, different similarity metrics have been proposed. (Carter, 1994) introduced an entropy reduction based similarity metric to cluster a multi-domain monolingual corpus. A regular expression based similarity function has been defined to build class specific language models (Hasan and Ney, 2005). In our research, we explore a clustering technique based on an n-gram overlap metric to extract sentences similar to in-domain text from large outof-domain training data. We employ domain adaptation techniques to adapt an out-of-domain bilingual corpus to an in-domain SMT model using clustering to extract sentences similar to in-domain text from large out-of-domain training data. We apply adaptation techniques to combine sub-corpora with indomain small-scale training data into a unified framework. The remainder of the paper is organized as follows. In section 2 we discuss related work. Secti"
Y09-2027,2005.eamt-1.19,0,0.0349785,"in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approach was further refined by Zhao et al. (2004). Hildebrand et al. (2005) adapt the translation model by selecting similar sentences from the available training data applying the approach of Eck et al. (2004). The adapted models significantly improve the translation performance compared to baseline systems. More recently, Bulyko et al., (2007) studied language model adaptation for SMT. They explored discriminative estimation of language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular e"
Y09-2027,N03-1017,0,0.00855717,"cally (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-domain and out-of-domain language models as log-linear features in an SMT model. They also used multiple decoding paths (Birch et al. 2007) for combining multiple domain translation tables in the state-of-the-art PB-SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain model (translation and reordering) into Moses (Koehn et al., 2007). They derived log-linear features to distinguish between phrases of multiple domains by applying the datasource indicator features and showed modest improvement in translation quality. Munteanu and Marcu (2006) automatically extract in-domain bilingual sentence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced trans"
Y09-2027,P07-2045,0,0.0154384,"o account the semantic context in which they appear. The semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve tran"
Y09-2027,W07-0733,0,0.509107,"e semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work"
Y09-2027,W02-1405,0,0.709646,"section 2 we discuss related work. Section 3 describes experimental results using our baseline SMT model. In section 4 we describe the domain adaptation techniques which are employed to combine multiple models. Section 5 presents the results obtained, together with some analysis. Section 6 concludes, and provides avenues for further work. 2 Related Work Topic-dependent modeling was effectively applied in speech recognition to improve the quality of models (Carter, 1994). Adaptation technology has been widely used in language modeling in the same filed over the last decade (Iyer et al., 1997). Langlais (2002) was the first to introduce domain adaptation in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approa"
Y09-2027,W08-0320,0,0.487175,"eneous topics. These topics usually define a set of terminological lexicons. Terminologies need to be translated taking into account the semantic context in which they appear. The semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Rece"
Y09-2027,P03-1021,0,0.0301251,"ated EILMT and TIDES language models and translation models using a log-linear combination. 4.1 Language Model Adaptation We used the language modeling toolkit SRILM (Stolke, 2002) to build two language models from the target side of the EILMT and TIDES training data. We performed log-linear interpolation of multi-domain translation models. This results in a straight-forward combination of in-domain and out-of-domain language models. Fortunately, the PB-SMT Moses decoder supports log-linear combinations of language models. Language model weights are optimized with minimum error rate training (Och, 2003). 4.2 Translation Model Adaptation In general, translation models are built separately for each of the domain specific corpora. These models are then combined using two techniques: (i) linear interpolation (ii) log-linear interpolation. We performed the log-linear interpolation of multi-domain translation models. There are two ways of performing log-linear interpolation: Multiple Decoding Paths: a recent feature of Moses is multiple decoding paths. This alternate decoding path model was developed by Birch et al. (2007). Here we use Moses’ capabilities to use different decoding paths for transl"
Y09-2027,D08-1090,0,0.0207945,"ted repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffing et al. 2007) has been used to adapt the in-domain monolingual corpus. Wu et al. (2008) also showed that loglinear interpolation performs better than linear interpolation to combine in-domain and out-ofdomain language models as well as translation models. Snover et al. (2008) describes a novel domain adaptation method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. For the translation of each source text, a large monolingual data set in the target language is searched for documents that might be comparable to the source text. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results show substantial gains. Lim and Kirchhoff (2008) proposed a method for incorporating out-of-domain data t"
Y09-2027,P07-1004,0,0.0249606,"SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain model (translation and reordering) into Moses (Koehn et al., 2007). They derived log-linear features to distinguish between phrases of multiple domains by applying the datasource indicator features and showed modest improvement in translation quality. Munteanu and Marcu (2006) automatically extract in-domain bilingual sentence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced transductive semi-supervised learning for SMT, where source language corpora are used to train the models. The transductive learning can be seen as a means to adapt the SMT system to a new domain. Sentences from the devset or testset are translated repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffin"
Y09-2027,P05-1058,0,0.27129,"prove translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work, we conduct experiments on the English—Hindi language pair. Like other Indian languages, Hindi is also a free phrase order (used with emphasis and complex structures) language. Therefore, a"
Y09-2027,C08-1125,0,0.0743408,"ence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced transductive semi-supervised learning for SMT, where source language corpora are used to train the models. The transductive learning can be seen as a means to adapt the SMT system to a new domain. Sentences from the devset or testset are translated repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffing et al. 2007) has been used to adapt the in-domain monolingual corpus. Wu et al. (2008) also showed that loglinear interpolation performs better than linear interpolation to combine in-domain and out-ofdomain language models as well as translation models. Snover et al. (2008) describes a novel domain adaptation method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. For the tran"
Y09-2027,D07-1054,0,0.353807,"f language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular expressions to build class specific language models. They proposed a method of interpolating class specific and global language models following the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adap"
Y09-2027,C04-1059,0,0.0553937,"e domain adaptation in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approach was further refined by Zhao et al. (2004). Hildebrand et al. (2005) adapt the translation model by selecting similar sentences from the available training data applying the approach of Eck et al. (2004). The adapted models significantly improve the translation performance compared to baseline systems. More recently, Bulyko et al., (2007) studied language model adaptation for SMT. They explored discriminative estimation of language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific"
Y09-2027,wu-wang-2004-improving-domain,0,\N,Missing
Y09-2027,J05-4003,0,\N,Missing
Y09-2027,P08-1000,0,\N,Missing
