2020.acl-demos.26,N19-1125,1,0.868376,"Missing"
2020.acl-demos.26,D19-1190,1,0.896272,"Missing"
2020.acl-demos.26,W18-2501,0,0.0582739,"Missing"
2020.acl-demos.26,P17-1141,0,0.177222,"s 224 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 224–231 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ing works focusing on the knowledge grounded text generation (Prabhumoye et al., 2019; Qin et al., 2019; Galley et al., 2019) usually assume the knowledge passage is given. However in practice this is not true. We provide the component to retrieve knowledge passage on-the-fly from web or customized document, to allow engineers or researchers test existing or new generation models. Keyphrase constrained generation (Hokamp and Liu, 2017) is another type of grounded generation, broadly speaking. Similarly the keyphrase needs to be provided to apply such constraints. We provide tools to extract constraints from knowledge passage or stylized corpus. Finally, friendly user interface is a component usually lacking in the implementation of neural models but it is necessary for a demo-centric framework. We provide scripts to build local terminal demo, webpage demo, and RESTful API demo. 2 2019), knowledge grounded generation (Qin et al., 2019; Prabhumoye et al., 2019) or span retrieval (Seo et al., 2016; Devlin et al., 2018), style"
2020.acl-demos.26,W18-2503,0,0.0434578,"Missing"
2020.acl-demos.26,D17-2014,0,0.032824,"le algorithms and components can benefit the community in several ways, as it provides (1) a shared codebase to reproduce and compare the state-of-the-art algorithms from different groups without time consuming trial and errors, (2) a platform to experiment the cross-model integration of these algorithms, and (3) a framework to build demo quickly upon these components. This framework can be built upon existing deep learning libraries (Paszke et al., 2019; Abadi et al., 2015) and neural NLP toolkits (HuggingFace, 2019; Gardner et al., 2018; Hu et al., 2018; Ott et al., 2019; Shiv et al., 2019; Miller et al., 2017)2 , as illustrated in Fig. 1. There are several challenges to do such integration. Firstly, engineering efforts are needed to unify the interface of different implementation. Secondly, a top-level manager needs to be designed to utilize different models together. Finally, different models are trained using different data with different performance. Cross-model integration, instead of calling each isolated model individually, can potentially improve the overall performance. In this work, we unified the models of different implementation in a single codebase, implemented demos as top-level manag"
2020.acl-demos.26,Q18-1027,0,0.0165915,"e 227 4 Cross-model integration Multiple models may be called for the same query and returns different responses. We propose the following ways to organically integrate multiple models, as illustrated in Fig. 4. User can apply these strategies with customized models. • Token probability interpolation refers prediction of the next token using a (weighted) average of the token probability distributions from two or more models given the same time step and given the same context and incomplete hypothesis. Previously, it has been proposed to bridge a conversation model and stylized language model (Niu and Bansal, 2018). This technique does not require the models to share the latent space but the vocabulary should be shared across different models. • Latent interpolation refers the technique introduced in Section 3.3. It provides a way to interpolate texts in the latent space. Unlike the token-level strategy introduced above, this technique focuses on the latent level and ingests information from the whole sentence. However if the two candidates are too dissimilar, the interpolation may result in undesired outputs. The soft constraint algorithm introFigure 4: An example flow chart showing the integration of"
2020.acl-demos.26,C16-2015,0,0.0313389,"kes multi-hot input of the keywords x identified from sentence y, as illustrated in Fig. 3. During training, we simply choose the top-k rare words (rareness measured by inverse document frequency) as the keywords, and k is randomly choose from a Uniform distribution k ∼ U (1, K). Constrained generation Besides the grounded generation, it is also useful to apply constraints at the decoding stage that encourage the generated hypotheses contain the desired phrases. We provide the following two ways to obtain constraints. • Key phrases extracted from the Knowledge passage. We use the PKE package (Boudin, 2016) to identify the keywords. • In some cases, users may want to use a stylized version of the topic phrases or phrase of a desired style as the constraints. We use the stylized synonym algorithm as introduced in Section 3.4 to provide such stylized constraints. With the constraints obtained above, we provide the following two ways to apply such constraints during decoding. • Hard constraint is applied via Grid Beam Search (GBS) (Hokamp and Liu, 2017), which is a lexically constrained decoding algorithm that can be applied to almost any models at the decoding stage and generate hypotheses that co"
2020.acl-demos.26,N19-4009,0,0.0266547,"ework to organically integrate multiple algorithms and components can benefit the community in several ways, as it provides (1) a shared codebase to reproduce and compare the state-of-the-art algorithms from different groups without time consuming trial and errors, (2) a platform to experiment the cross-model integration of these algorithms, and (3) a framework to build demo quickly upon these components. This framework can be built upon existing deep learning libraries (Paszke et al., 2019; Abadi et al., 2015) and neural NLP toolkits (HuggingFace, 2019; Gardner et al., 2018; Hu et al., 2018; Ott et al., 2019; Shiv et al., 2019; Miller et al., 2017)2 , as illustrated in Fig. 1. There are several challenges to do such integration. Firstly, engineering efforts are needed to unify the interface of different implementation. Secondly, a top-level manager needs to be designed to utilize different models together. Finally, different models are trained using different data with different performance. Cross-model integration, instead of calling each isolated model individually, can potentially improve the overall performance. In this work, we unified the models of different implementation in a single codeb"
2020.acl-demos.26,N19-1269,1,0.937575,", cross-mode scoring, latent interpolation, and unified hypothesis ranking. This work is also aimed to promote the development of grounded text generation. The exist2 1 Source code available at github.com/microsoft/ MixingBoard Although multiple libraries and toolkits are mentioned in Fig. 1, the current implementation is primarily based on PyTorch models 224 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 224–231 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ing works focusing on the knowledge grounded text generation (Prabhumoye et al., 2019; Qin et al., 2019; Galley et al., 2019) usually assume the knowledge passage is given. However in practice this is not true. We provide the component to retrieve knowledge passage on-the-fly from web or customized document, to allow engineers or researchers test existing or new generation models. Keyphrase constrained generation (Hokamp and Liu, 2017) is another type of grounded generation, broadly speaking. Similarly the keyphrase needs to be provided to apply such constraints. We provide tools to extract constraints from knowledge passage or stylized corpus. Finally, friendly user interface"
2020.acl-demos.26,P19-1539,1,0.939921,"ent interpolation, and unified hypothesis ranking. This work is also aimed to promote the development of grounded text generation. The exist2 1 Source code available at github.com/microsoft/ MixingBoard Although multiple libraries and toolkits are mentioned in Fig. 1, the current implementation is primarily based on PyTorch models 224 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 224–231 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics ing works focusing on the knowledge grounded text generation (Prabhumoye et al., 2019; Qin et al., 2019; Galley et al., 2019) usually assume the knowledge passage is given. However in practice this is not true. We provide the component to retrieve knowledge passage on-the-fly from web or customized document, to allow engineers or researchers test existing or new generation models. Keyphrase constrained generation (Hokamp and Liu, 2017) is another type of grounded generation, broadly speaking. Similarly the keyphrase needs to be provided to apply such constraints. We provide tools to extract constraints from knowledge passage or stylized corpus. Finally, friendly user interface is a component us"
2020.acl-demos.26,P19-3021,1,0.851364,"ly integrate multiple algorithms and components can benefit the community in several ways, as it provides (1) a shared codebase to reproduce and compare the state-of-the-art algorithms from different groups without time consuming trial and errors, (2) a platform to experiment the cross-model integration of these algorithms, and (3) a framework to build demo quickly upon these components. This framework can be built upon existing deep learning libraries (Paszke et al., 2019; Abadi et al., 2015) and neural NLP toolkits (HuggingFace, 2019; Gardner et al., 2018; Hu et al., 2018; Ott et al., 2019; Shiv et al., 2019; Miller et al., 2017)2 , as illustrated in Fig. 1. There are several challenges to do such integration. Firstly, engineering efforts are needed to unify the interface of different implementation. Secondly, a top-level manager needs to be designed to utilize different models together. Finally, different models are trained using different data with different performance. Cross-model integration, instead of calling each isolated model individually, can potentially improve the overall performance. In this work, we unified the models of different implementation in a single codebase, implemented de"
2020.acl-demos.26,P19-3027,0,\N,Missing
2020.acl-demos.26,2020.emnlp-main.378,1,\N,Missing
2020.acl-demos.30,D17-2014,0,0.0333427,"transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Dinan et al., 2019) has a focus on personalized conversations. ParlAI (Miller et al., 2017) is another library for developing task-oriented dialogue systems. It contains pre-trained models for knowledge-grounded chatbot trained with crowdsourced data. The Text-to-Text Transformer (Raffel et al., 2019) unifies multiple text modeling tasks, and achieves the state-of-the-art results in various natural language generation and understanding benchmarks. User Bot Table 5: An interactive example of multi-turn dialogue Role Response User Bot what is the meaning of life ? The meaning is to be with your family and friends . I’m going to guess : It means that your parents and friends have loved"
2020.acl-demos.30,P02-1040,0,0.106794,"of 50,257 entries, and was trained on 16 Nvidia V100 machines with 4 https://github.com/mgalley/ DSTC7-End-to-End-Conversation-Modeling/ tree/master/evaluation 3 https://github.com/huggingface/ pytorch-transformers 272 other filtering criteria such as turn length, this yields a 5-reference test set of size 2208. (For each instance, one of the 6 human responses is set aside to assess human performance on this task.) Note that our training data is collected from a different time span from the test set. We performed automatic evaluation using standard machine translation metrics, including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Doddington, 2002). NIST is a variant of BLEU that weights n-gram matches by their information gain, i.e., it indirectly penalizes uninformative n-grams. We also use Entropy (Zhang et al., 2018) and Dist-n (Li et al., 2016a) to evaluate lexical diversity. More details are provided in Galley et al. (2019). We compared D IALO GPT with our in-house competitive sequence-to-sequence model P ER SONALITY C HAT based on (Li et al., 2016a) and trained on Twitter data, which has been used in production as a Cognitive Service for Microsoft Azure.5 Table 2 summ"
2020.acl-demos.30,N18-1202,0,0.0181606,"rning repository (Wolf et al., 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Din"
2020.acl-demos.30,P19-1539,1,0.811156,"t thus poses a greater one-to-many problem than is typical in other text generation tasks such as neural machine translation, text summarization and paraphrasing. Human conversations are also generally more informal, noisy, and, when in the form of textual chat, often contain informal abbreviations or syntactic/lexical errors. Most open-domain neural response generation systems suffer from content or style inconsistency (Li et al., 2016b; Zhang et al., 2019; Gao et al., 2019c), lack of long-term contextual information (Serban et al., 2017), and blandness (Li et al., 2016a; Zhang et al., 2018; Qin et al., 2019). While these issues can be alleviated by modelling strategies specifically designed to boost information content, a transformer-based architecture like GPT-2 (Radford et al., 2018), which uses a multi-layer self-attentive mechanism to allow fully-connected cross-attention to the full context in a computationally efficient manner, seems like a natural choice for exploring a more general solution. Transformer models, for example, allow long-term dependency information to be better be preserved across time (Radford et al., 2018), thereby improving content consistency. They also have higher model"
2020.acl-demos.30,N19-1125,1,0.926516,"set aside to assess human performance on this task.) Note that our training data is collected from a different time span from the test set. We performed automatic evaluation using standard machine translation metrics, including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Doddington, 2002). NIST is a variant of BLEU that weights n-gram matches by their information gain, i.e., it indirectly penalizes uninformative n-grams. We also use Entropy (Zhang et al., 2018) and Dist-n (Li et al., 2016a) to evaluate lexical diversity. More details are provided in Galley et al. (2019). We compared D IALO GPT with our in-house competitive sequence-to-sequence model P ER SONALITY C HAT based on (Li et al., 2016a) and trained on Twitter data, which has been used in production as a Cognitive Service for Microsoft Azure.5 Table 2 summarizes the automatic evaluation results. D IALO GPT with 345M parameters and beam search achieved the highest automatic score across most metrics. Scores for D IALO GPT with 345M parameters are better across the board than with 117M parameters. Beam search (with beam width 10) dramatically improves BLEU and DIST scores, and marginally improves NIST"
2020.acl-demos.30,D19-1190,1,0.865771,"an Microsoft Corporation, Redmond, WA, USA ∗ {yizzhang,siqi.sun,mgalley,yenchen,chrisbkt,xiag,jfgao,jingjl,billdol}@microsoft.com Abstract tion. Neural response generation is a subcategory of text-generation that shares the objective of generating natural-looking text (distinct from any training instance) that is relevant to the prompt. Modelling conversations, however, presents distinct challenges in that human dialogue, which encapsulates the possibly competing goals of two participants, is intrinsically more diverse in the range of potential responses (Li et al., 2016a; Zhang et al., 2018; Gao et al., 2019a,b). It thus poses a greater one-to-many problem than is typical in other text generation tasks such as neural machine translation, text summarization and paraphrasing. Human conversations are also generally more informal, noisy, and, when in the form of textual chat, often contain informal abbreviations or syntactic/lexical errors. Most open-domain neural response generation systems suffer from content or style inconsistency (Li et al., 2016b; Zhang et al., 2019; Gao et al., 2019c), lack of long-term contextual information (Serban et al., 2017), and blandness (Li et al., 2016a; Zhang et al.,"
2020.acl-demos.30,W18-2501,0,0.0119024,"lionaire and happy . There is a reason the rich have a lot of money There are several open-sourced toolkits for largescale pre-trained transformer models. Huggingface Conv-AI transfer learning repository (Wolf et al., 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification."
2020.acl-demos.30,P16-1162,0,0.0541096,"of masked multi-head selfattention layers to train on massive web-text data. The text generated either from scratch or based on a user-specific prompt is realistic-looking. The success of GPT-2 demonstrates that a transformer language model is able to characterize human language data distributions at a fine-grained level, presumably due to large large model capacity and superior efficiency. Our model inherits from GPT-2 (Radford et al., 2018), a 12-to-48 layer transformer with layer normalization, a initialization scheme that accounts for model depth that we modified, and byte pair encodings (Sennrich et al., 2016) for the tokenizer. We follow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language modeling. We first concatenate all dialog turns within a dialogue session into a long text x1 , · · · , xN (N is the sequence length), ended by the end-of-text token. We denote the source sentence (dialogue history) as S = x1 , · · · , xm and target sentence (ground truth response) as T = xm+1 , · · · , xN , the conditional probability of P (T |S) can be written as the product of a series of conditional probabilities: Dataset The dataset is extracted fro"
2020.acl-demos.30,W19-5944,0,0.0533119,"the “ground truth” references that will be tested on, while R4 is the “heldout” human response that serves to compute a “human” score. In semantic space, a generated response Rg from a well-trained model will presumably tend to lie in the vicinity the geometric center R3: I will s end some one right away R4: I . s the perp e trato Rg: W r still hen w R1: W insid as th e? as an is bre ythin ak-in g sto ? len? R2: I s any one hurt or in jured ? Source: I would like to report a break-in. Figure 1: A generated response can surpass a human response in automatic metrics. Example responses are from Gupta et al. (2019) of all possible responses, because the training objective seeks to generate the most likely response. This may be close to the geometric mean of all training instances, thus “averaging out” these instances. Consequently, a generated response Rg might have a lower “semantic distance” (manifested in higher automatic scores like BLEU) from R1-R3 than the targeted human response R4. 4.3 A New Reddit Multi-reference Dataset We further evaluate D IALO GPT on a multireference test set with 6K examples. The results are shown in Table 3. We test our method on two settings: training from scratch and fi"
2020.acl-demos.30,W18-2503,0,0.0199459,", 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Dinan et al., 2019) has a fo"
2020.emnlp-main.28,2020.acl-demos.26,1,0.861152,"Missing"
2020.emnlp-main.28,N19-1125,1,0.881167,"Missing"
2020.emnlp-main.28,D19-1190,1,0.886305,"Missing"
2020.emnlp-main.28,W19-4101,0,0.0613694,"Missing"
2020.emnlp-main.28,2020.emnlp-main.378,1,0.843421,"ning dataset for response quality prediction. cannot reliably distinguish between human- and machine-generated responses. Though surprisingly effective, the training objective for these models is conceptually simple: minimizing the perplexity of a reference response for a given context. Introduction Conversing freely in natural language is one of the greatest challenges of artificial intelligence. Endto-end open-domain dialog systems have become increasingly powerful, with advanced model architectures and large-scale training (Zhang et al., 2019b; Adiwardana et al., 2020; Roller et al., 2020; Li et al., 2020). In some settings, human annotators However, a meaningful evaluation of response generation must take into account more than whether a generated turn is relevant in context, or whether it “sounds human.” Conventional neural conversation models often generate trivial or bland responses (Li et al., 2016; Zhao et al., 2017) that are relevant to context but are not engaging. Even human responses can vary dramatically in terms of tonal appropriateness and whether they are interesting enough to prompt a rich listener reaction. A successful dialog turn must be proactive, engaging, and consistent wit"
2020.emnlp-main.28,P18-1205,0,0.075349,"Large-scale training data is necessary because of the one-to-many nature of dialog and the scope and complexity of human conversation. However, labeling conversations at scale is too expensive and time-consuming for this purpose. Labeling the “engagingness” of a response is not something a single annotator can do; the task requires something more like a large-scale, collective vote. And yet there is no obvious automated substitute for this kind of human labeling. Conventional quality measurements such as reference-based similarity (Papineni et al., 2002) or lexical diversity (Li et al., 2016; Zhang et al., 2018b) capture only limited aspects of response quality, and are not strongly predictive of human reactions: simply because a response is different from others does not necesarily mean that it will be perceived as “bad”. Figure 2: The long-tailed distribution of the raw scores of feedback of Reddit.com. pairwise classification. Using a dataset of 133M pairs of human comments and their associated number of replies or up-/downvotes, we train a set of large-scale transformer-based feedback ranking models which outperform several baselines. In particular, dialog perplexity shows little predictive powe"
2020.emnlp-main.28,N16-1014,1,0.939356,"versing freely in natural language is one of the greatest challenges of artificial intelligence. Endto-end open-domain dialog systems have become increasingly powerful, with advanced model architectures and large-scale training (Zhang et al., 2019b; Adiwardana et al., 2020; Roller et al., 2020; Li et al., 2020). In some settings, human annotators However, a meaningful evaluation of response generation must take into account more than whether a generated turn is relevant in context, or whether it “sounds human.” Conventional neural conversation models often generate trivial or bland responses (Li et al., 2016; Zhao et al., 2017) that are relevant to context but are not engaging. Even human responses can vary dramatically in terms of tonal appropriateness and whether they are interesting enough to prompt a rich listener reaction. A successful dialog turn must be proactive, engaging, and consistent with social norms (Grice, 1975, 1989). 1 Dataset and models open-sourced on https:// github.com/golsun/DialogRPT In this work, we move beyond simple prediction of response relevance, augmenting this with a prediction of how likely a response is to elicit a 386 Proceedings of the 2020 Conference on Empiric"
2020.emnlp-main.28,I17-1099,0,0.0369889,"ationship between these models. 4.2 Human-like Classification Human-vs-Rand We first evaluate performance on the task of selecting the gold response from a set of random distractor responses. For each context, we randomly select n distractors. Performance is evaluated using Hits@k, which is the ratio of the number of gold responses in the top-k ranked hypotheses. Here, k is equal to the number of gold responses. Although D IALOG RPT is trained solely on Human-vs-Rand Reddit data, we show in Table 7 that it performs well even when compared to baseline models on other data sources: DailyDialog (Li et al., 2017) and Twitter5 PersonaChat6 (Zhang et al., 2018a). Such zero-shot performance indicate that the model generalize reasonably well on unseen datasets. For the Reddit dataset, which has multiple gold replies, we also compare our method with reference-based similarity measurements, 7 including BLEU (Papineni et al., 2002), BERTScore (Zhang et al., 2019a), and BLEURT (Sellam et al., 2020). These metrics are not applicable on-thefly, since references are not available, but they are commonly used as offline measures of dialog system quality. As shown in Table 7, although BLEU, BERTScore, and BLEURT ta"
2020.emnlp-main.28,P02-1040,0,0.113728,"world human preferences or feedback in an end-to-end fashion. Large-scale training data is necessary because of the one-to-many nature of dialog and the scope and complexity of human conversation. However, labeling conversations at scale is too expensive and time-consuming for this purpose. Labeling the “engagingness” of a response is not something a single annotator can do; the task requires something more like a large-scale, collective vote. And yet there is no obvious automated substitute for this kind of human labeling. Conventional quality measurements such as reference-based similarity (Papineni et al., 2002) or lexical diversity (Li et al., 2016; Zhang et al., 2018b) capture only limited aspects of response quality, and are not strongly predictive of human reactions: simply because a response is different from others does not necesarily mean that it will be perceived as “bad”. Figure 2: The long-tailed distribution of the raw scores of feedback of Reddit.com. pairwise classification. Using a dataset of 133M pairs of human comments and their associated number of replies or up-/downvotes, we train a set of large-scale transformer-based feedback ranking models which outperform several baselines. In"
2020.emnlp-main.28,2020.acl-main.704,0,0.270005,"representative negative modes: retrieval and generative dialog model generation. For the former we simply construct negative examples by randomly sampling from the training data. For the latter we use DialoGPT with top-k decoding. Since DialoGPT is able to produce human-like responses in certain evaluation settings, we select only 5.3 M highly-rated human response as positive examples, instead of using all human responses. Note that our method can be extended to include other negative modes such as perturbations and excessive repetition, similar to the synthetic example creation using BLEURT (Sellam et al., 2020). 3.4 Baselines (MMI) between the response and context. We use DialoGPT and its reverse model to compute ppl. BM25 This classic metric measures keywords similarity (Robertson and Zaragoza, 2009). We use the inner product of the context BM25 vector and candidate response BM25 vector to rank candidates, similar to (Henderson et al., 2019a). ConveRT (Henderson et al., 2019b) is a transformer-based model pretrained on Reddit data. It encodes context and candidate as vectors and compute their inner product as similarity used for ranking, achieved the existing state-of-the-art performance on several"
2020.emnlp-main.28,P17-1061,0,0.0303204,"natural language is one of the greatest challenges of artificial intelligence. Endto-end open-domain dialog systems have become increasingly powerful, with advanced model architectures and large-scale training (Zhang et al., 2019b; Adiwardana et al., 2020; Roller et al., 2020; Li et al., 2020). In some settings, human annotators However, a meaningful evaluation of response generation must take into account more than whether a generated turn is relevant in context, or whether it “sounds human.” Conventional neural conversation models often generate trivial or bland responses (Li et al., 2016; Zhao et al., 2017) that are relevant to context but are not engaging. Even human responses can vary dramatically in terms of tonal appropriateness and whether they are interesting enough to prompt a rich listener reaction. A successful dialog turn must be proactive, engaging, and consistent with social norms (Grice, 1975, 1989). 1 Dataset and models open-sourced on https:// github.com/golsun/DialogRPT In this work, we move beyond simple prediction of response relevance, augmenting this with a prediction of how likely a response is to elicit a 386 Proceedings of the 2020 Conference on Empirical Methods in Natura"
2021.findings-acl.185,W19-4412,0,0.149057,". While automated document-level generation seems tantalizingly within reach, a high branching factor presents significant challenges in tailoring generated documents to the specific requirements of users. Topic drift and “hallucination” of information are endemic to these models (Wiseman et al., 2017). These risks have ensured that end-user applications involving text generation (e.g., Smart Compose, Smart Reply, Grammarly) still require a human to remain in control of content and are restricted to individual sentences or even smaller segments of text (Chen et al., 2019; Kannan et al., 2016; Alikaniotis and Raheja, 2019; Prabhumoye et al., 2019; Faltings et al., 2021). Can large generative language models be used to assist user writing at the document level while the user still controls the factual content? A possible answer lies in the observation that a substantial portion of day-to-day writing involves some form of reuse. Similar documents (e.g., monthly reports, sales letters, job descriptions) are effectively recycled by changing those segments that need to be modified (Fig. 1).1 Moreover, documents containing analogous texts are often found collocated in repositories, a common practice for organization"
2021.findings-acl.185,alonso-etal-2004-multiple,0,0.262173,"Missing"
2021.findings-acl.185,N03-1003,0,0.487908,"et) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-en"
2021.findings-acl.185,D19-1195,0,0.0127106,"derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge"
2021.findings-acl.185,P18-1015,0,0.0196927,"structed from a set of similar documents. 3 Our experiments in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input"
2021.findings-acl.185,N18-1150,0,0.14002,"ent learning (RL) to further improve generation quality. For each training example with input X, we generate a sequence sˆ, which is sampled from the probability distribution at each time step, p(ˆ st |ˆ s1 ...ˆ st−1 , X). We observe that directly optimizing the evaluation function proposed in Eq. (2) at the sequence level, using a vanilla policy gradient (PG) or a self-critical sequence training (TD-SCST) algorithm (Rennie et al., 2017; 2105 Paulus et al., 2018; Pasunuru and Bansal, 2018), can lead to instability during training as the reward cannot be calculated until the end of generation (Celikyilmaz et al., 2018). Therefore, we instead use a token-level incremental reward that is based on the change to the original reward function r(ˆ s, Y ) = score(ˆ s, Y ) from each sampled token sˆt , given references Y : rt (ˆ st , Y ) = r(ˆ s1...t , Y ) − r(ˆ s1...t−1 , Y ). (3) The training objective can be written as: LRL = T X −rt (sˆt , Y )p(ˆ st |ˆ s1 ...ˆ st−1 , X) (4) t=1 where T = |ˆ s|. Since optimizing RL loss alone runs the risk of compromising the language model (Paulus et al., 2018; Pasunuru and Bansal, 2017), we use a mixed loss as follows: LMIX = λLRL + (1 − λ)LMLE (5) where λ is a hyperparameter t"
2021.findings-acl.185,W19-2401,1,0.921705,"rzindar, 2010), we define an automatic evaluation metric based on Word Error Rate (Snover et al., 2006; Tom´as et al., 2003). We compare against strong baseline models including a mixture of experts model and a reinforcement learning approach designed to handle multi-source inputs and a weak supervision setting. Finally, we provide experimental analysis of these models, using automated and human evaluation studies. 2 2.1 Related Work Document Generation Recent work leverages the success of large pretrained language models to generate long texts such as stories (Rashkin et al., 2020), reviews (Cho et al., 2019a,b) and fake news (Zellers et al., 2019). Most end-user applications for assisting user writing, however, are confined to sentence-level generation (Chen et al., 2019; Kannan et al., 2016; Alikaniotis and Raheja, 2019; Prabhumoye et al., 2019; Faltings et al., 2021). Our work focuses on document-level writing assistance in which a document sketch is constructed from a set of similar documents. 3 Our experiments in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for"
2021.findings-acl.185,2021.eacl-main.2,1,0.72692,"oteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-encoder model with a transformer-based coordinator. 3 Problem Definition We introduce the task of document sketching, which aims to facilitate the authoring process by generating a template-like document draft, based on a collection of sampled similar documents. Formally, the task can be defined as follow: given a set of n documents X = {x1 , x2 , ..., xn }, generate 2103 a text sequence s that can be used as the sketch to reduce the human effort involved in composing a target document y. Evaluation Metrics As in most text generatio"
2021.findings-acl.185,2020.acl-main.413,0,0.0245581,"sting work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to a"
2021.findings-acl.185,2021.naacl-main.414,1,0.885569,"lizingly within reach, a high branching factor presents significant challenges in tailoring generated documents to the specific requirements of users. Topic drift and “hallucination” of information are endemic to these models (Wiseman et al., 2017). These risks have ensured that end-user applications involving text generation (e.g., Smart Compose, Smart Reply, Grammarly) still require a human to remain in control of content and are restricted to individual sentences or even smaller segments of text (Chen et al., 2019; Kannan et al., 2016; Alikaniotis and Raheja, 2019; Prabhumoye et al., 2019; Faltings et al., 2021). Can large generative language models be used to assist user writing at the document level while the user still controls the factual content? A possible answer lies in the observation that a substantial portion of day-to-day writing involves some form of reuse. Similar documents (e.g., monthly reports, sales letters, job descriptions) are effectively recycled by changing those segments that need to be modified (Fig. 1).1 Moreover, documents containing analogous texts are often found collocated in repositories, a common practice for organizations that manage professional documents.2 The high b"
2021.findings-acl.185,2020.acl-main.186,0,0.0167638,"atent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstr"
2021.findings-acl.185,D19-1388,0,0.0143741,"nts in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based metho"
2021.findings-acl.185,C16-1133,0,0.0117745,"Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-encoder model with a transformer-based coordinator. 3 Problem Definition We introduce the task of document sketching, which aims to facilitate the authoring process by generating a template-like document draft, based on a collection of sampled similar documents. Formally, the task can be defined as follow: given a set of n documents X = {x1 , x2 , ..., xn }, generate 2103 a text sequence s that can be used as the sketc"
2021.findings-acl.185,Q18-1031,0,0.0221217,"ng. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in th"
2021.findings-acl.185,K15-1007,0,0.035958,"Missing"
2021.findings-acl.185,2020.emnlp-main.622,0,0.0221967,"Missing"
2021.findings-acl.185,2020.acl-main.703,0,0.0117555,"irs: (x1 , s), (x2 , s), ..., (xn , s) from a data instance. This is followed by training a complete MoE model as described in Section 5. We use greedy beam search as the decoding strategy for all generative models with a beam size of 4 (the default value in T5base). We observe that consecutive ellipses and uninformative tokens hurt readability. To improve the readability of output sketches, we apply minor post-processing to all models in our experiments: we merge consecutive ellipses if all tokens between 5 Note that T5 can be easily replaced by Other pre-trained generative models like BART (Lewis et al., 2020) in our model framework. them are punctuation or among the 30 most frequent tokens.6 Sentence-terminating periods are excepted. Training and Parameters T5 has about 220 million parameters and MoE has about 310 million parameters in total. The average training time is about 20 hours for T5 and about 30 hours for MoE on a single Tesla V100 node (32GB) with 3 epochs. It takes an additional 10 hours to train a single expert in MoE. For MoE+RL, we use 4 V100 nodes and it takes about 2 days for training. During training, we tune batch size, learning rate and warm-up steps for T5. For MoE and MoE+RL,"
2021.findings-acl.185,N18-1169,0,0.0303707,"Missing"
2021.findings-acl.185,P19-1500,0,0.0250568,"ific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-encoder model with a transformer-based coordinator. 3 Problem Definition We introduce the task of document sketching, which aims to facilitate the authoring process by generating a template-like document draft, based on a collection of sampled similar documents. Formally, the task can be defined as follow: given a"
2021.findings-acl.185,2020.acl-main.173,0,0.0345364,"Missing"
2021.findings-acl.185,W18-6322,0,0.0129935,"t documents. MoE based models are much more robust to low input similarity compared to the baselines. For the group with the most similar input documents, consensus-MSA gives the best score, while MoE based models yield much better performance in the other two groups. Human Evaluation In order to avoid bias in human judgments, we control possible confounding factors including sequence length and the number of ellipses in a sequence during decoding (Nakov et al., 2012; Guzm´an et al., 2015). We apply a tuneable penalty for each confounding factor (at inference time only) for each neural model (Murray and Chiang, 2018) to generate examples for human evaluation, such that the average output length and number of ellipses in each sequence from all neural systems compared in Tab. 4 are almost the 2108 same.8 We notice that such normalization does not affect the automatic evaluation score of each system much and the system ranks do not change. Human evaluation was conducted using crowdsourced workers. Judges were presented with paired randomized outputs and target documents, and were instructed to choose their preference for a starting point for writing the target document in order to save editing time.9 Judgmen"
2021.findings-acl.185,C12-1121,0,0.0203556,"ntain a lot of hallucinated content, an issue that is even more severe when there is little overlapping structure or factual content among the input documents. MoE based models are much more robust to low input similarity compared to the baselines. For the group with the most similar input documents, consensus-MSA gives the best score, while MoE based models yield much better performance in the other two groups. Human Evaluation In order to avoid bias in human judgments, we control possible confounding factors including sequence length and the number of ellipses in a sequence during decoding (Nakov et al., 2012; Guzm´an et al., 2015). We apply a tuneable penalty for each confounding factor (at inference time only) for each neural model (Murray and Chiang, 2018) to generate examples for human evaluation, such that the average output length and number of ellipses in each sequence from all neural systems compared in Tab. 4 are almost the 2108 same.8 We notice that such normalization does not affect the automatic evaluation score of each system much and the system ranks do not change. Human evaluation was conducted using crowdsourced workers. Judges were presented with paired randomized outputs and targ"
2021.findings-acl.185,W18-2711,0,0.0120534,"ssing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document summarization, which seeks to generate an abstractive text summary of multiple input documents (Liu and Lapata, 2019; Chu and Liu, 2019), and multi-source machine translation (Nishimura et al., 2018; Garmash and Monz, 2016) that encodes input texts in multiple source languages and translates them into a target language. Cho et al. (2021) generate a question from input documents by applying a multi-encoder model with a transformer-based coordinator. 3 Problem Definition We introduce the task of document sketching, which aims to facilitate the authoring process by generating a template-like document draft, based on a collection of sampled similar documents. Formally, the task can be defined as follow: given a set of n documents X = {x1 , x2 , ..., xn }, generate 2103 a text sequence s that"
2021.findings-acl.185,W14-4407,0,0.0165642,"etrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso"
2021.findings-acl.185,D17-1103,0,0.0139213,"o instability during training as the reward cannot be calculated until the end of generation (Celikyilmaz et al., 2018). Therefore, we instead use a token-level incremental reward that is based on the change to the original reward function r(ˆ s, Y ) = score(ˆ s, Y ) from each sampled token sˆt , given references Y : rt (ˆ st , Y ) = r(ˆ s1...t , Y ) − r(ˆ s1...t−1 , Y ). (3) The training objective can be written as: LRL = T X −rt (sˆt , Y )p(ˆ st |ˆ s1 ...ˆ st−1 , X) (4) t=1 where T = |ˆ s|. Since optimizing RL loss alone runs the risk of compromising the language model (Paulus et al., 2018; Pasunuru and Bansal, 2017), we use a mixed loss as follows: LMIX = λLRL + (1 − λ)LMLE (5) where λ is a hyperparameter to be tuned. 6 Experiments 6.1 Setup To leverage the recent success in such transformerbased generation models, neural generation models in our experiments are initialized with the base version of T5 (Raffel et al., 2019; Wolf et al., 2020), an encoder-decoder architecture pre-trained on a variety of text-to-text tasks.5 All hyperparameters are tuned on the validation set. For MoE, we first fine-tune T5-base to obtain a “single expert” model to initialize each individual component model of the MoE. The"
2021.findings-acl.185,N18-2102,0,0.0606828,"= ni=1 wit πit , where πt is the final distribution for generation at timestamp t. 5.2 Reinforcement Learning We leverage reinforcement learning (RL) to further improve generation quality. For each training example with input X, we generate a sequence sˆ, which is sampled from the probability distribution at each time step, p(ˆ st |ˆ s1 ...ˆ st−1 , X). We observe that directly optimizing the evaluation function proposed in Eq. (2) at the sequence level, using a vanilla policy gradient (PG) or a self-critical sequence training (TD-SCST) algorithm (Rennie et al., 2017; 2105 Paulus et al., 2018; Pasunuru and Bansal, 2018), can lead to instability during training as the reward cannot be calculated until the end of generation (Celikyilmaz et al., 2018). Therefore, we instead use a token-level incremental reward that is based on the change to the original reward function r(ˆ s, Y ) = score(ˆ s, Y ) from each sampled token sˆt , given references Y : rt (ˆ st , Y ) = r(ˆ s1...t , Y ) − r(ˆ s1...t−1 , Y ). (3) The training objective can be written as: LRL = T X −rt (sˆt , Y )p(ˆ st |ˆ s1 ...ˆ st−1 , X) (4) t=1 where T = |ˆ s|. Since optimizing RL loss alone runs the risk of compromising the language model (Paulus et"
2021.findings-acl.185,2020.acl-main.396,0,0.0471883,"Missing"
2021.findings-acl.185,N19-1263,0,0.0218549,"lso show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such"
2021.findings-acl.185,N19-1269,1,0.928824,"el generation seems tantalizingly within reach, a high branching factor presents significant challenges in tailoring generated documents to the specific requirements of users. Topic drift and “hallucination” of information are endemic to these models (Wiseman et al., 2017). These risks have ensured that end-user applications involving text generation (e.g., Smart Compose, Smart Reply, Grammarly) still require a human to remain in control of content and are restricted to individual sentences or even smaller segments of text (Chen et al., 2019; Kannan et al., 2016; Alikaniotis and Raheja, 2019; Prabhumoye et al., 2019; Faltings et al., 2021). Can large generative language models be used to assist user writing at the document level while the user still controls the factual content? A possible answer lies in the observation that a substantial portion of day-to-day writing involves some form of reuse. Similar documents (e.g., monthly reports, sales letters, job descriptions) are effectively recycled by changing those segments that need to be modified (Fig. 1).1 Moreover, documents containing analogous texts are often found collocated in repositories, a common practice for organizations that manage professiona"
2021.findings-acl.185,2020.emnlp-main.349,0,0.054737,"Missing"
2021.findings-acl.185,2006.amta-papers.25,0,0.0826491,"er portions of the document to reflect modifications introduced by the user. In this work, we propose a new task, called DOC UMENT SKETCHING, in which initial template-like prototype documents are generated from collections of analogous documents. To support this task, we collected a dataset consisting of approximately 20K Wikipedia documents with similar textual characteristics.4 For this new task, inspired by previous work in measuring machine translation post-editing productivity (Tatsumi, 2009; Specia and Farzindar, 2010), we define an automatic evaluation metric based on Word Error Rate (Snover et al., 2006; Tom´as et al., 2003). We compare against strong baseline models including a mixture of experts model and a reinforcement learning approach designed to handle multi-source inputs and a weak supervision setting. Finally, we provide experimental analysis of these models, using automated and human evaluation studies. 2 2.1 Related Work Document Generation Recent work leverages the success of large pretrained language models to generate long texts such as stories (Rashkin et al., 2020), reviews (Cho et al., 2019a,b) and fake news (Zellers et al., 2019). Most end-user applications for assisting us"
2021.findings-acl.185,2010.jec-1.5,0,0.116675,"what is entailed in writing such documents.3 A fully-implemented dynamic system might update other portions of the document to reflect modifications introduced by the user. In this work, we propose a new task, called DOC UMENT SKETCHING, in which initial template-like prototype documents are generated from collections of analogous documents. To support this task, we collected a dataset consisting of approximately 20K Wikipedia documents with similar textual characteristics.4 For this new task, inspired by previous work in measuring machine translation post-editing productivity (Tatsumi, 2009; Specia and Farzindar, 2010), we define an automatic evaluation metric based on Word Error Rate (Snover et al., 2006; Tom´as et al., 2003). We compare against strong baseline models including a mixture of experts model and a reinforcement learning approach designed to handle multi-source inputs and a weak supervision setting. Finally, we provide experimental analysis of these models, using automated and human evaluation studies. 2 2.1 Related Work Document Generation Recent work leverages the success of large pretrained language models to generate long texts such as stories (Rashkin et al., 2020), reviews (Cho et al., 20"
2021.findings-acl.185,2009.mtsummit-posters.20,0,0.47678,"ull picture of what is entailed in writing such documents.3 A fully-implemented dynamic system might update other portions of the document to reflect modifications introduced by the user. In this work, we propose a new task, called DOC UMENT SKETCHING, in which initial template-like prototype documents are generated from collections of analogous documents. To support this task, we collected a dataset consisting of approximately 20K Wikipedia documents with similar textual characteristics.4 For this new task, inspired by previous work in measuring machine translation post-editing productivity (Tatsumi, 2009; Specia and Farzindar, 2010), we define an automatic evaluation metric based on Word Error Rate (Snover et al., 2006; Tom´as et al., 2003). We compare against strong baseline models including a mixture of experts model and a reinforcement learning approach designed to handle multi-source inputs and a weak supervision setting. Finally, we provide experimental analysis of these models, using automated and human evaluation studies. 2 2.1 Related Work Document Generation Recent work leverages the success of large pretrained language models to generate long texts such as stories (Rashkin et al., 2"
2021.findings-acl.185,W03-2804,0,0.121231,"Missing"
2021.findings-acl.185,2020.acl-main.450,0,0.0566789,"Missing"
2021.findings-acl.185,P19-1207,0,0.0512804,"Missing"
2021.findings-acl.185,D18-1356,0,0.0235525,"d learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align input documents and create heuristic templates as weak supervision. Other tasks taking multiple text sequences as input include multi-document sum"
2021.findings-acl.185,2020.emnlp-demos.6,0,0.0270809,"Missing"
2021.findings-acl.185,2020.acl-main.531,0,0.0128,"ences from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a template with blanks using rule-based methods. Other work such as (Wiseman et al., 2018) and (Gangadharaiah and Narayanaswamy, 2020) relies on a knowledge base or a domain/task specific ontology to segment text sequences into templates. 2.3 Multi-Sequence Processing Multiple Sequence Alignment (MSA), widely used in the biological domain (Sauder et al., 2000) to align multiple biological sequences like proteins, has long been leveraged for text pattern matching (Barzilay and Lee, 2003; Alonso et al., 2004). We adopt this method to align i"
2021.findings-acl.185,D19-1197,0,0.0149075,"t of similar documents. 3 Our experiments in Section 6 also show that drafts derived from multiple analogous documents are more effective than those derived from one or two documents. 4 We release our data and source code for dataset construction and experiments at https://github.com/ ellenmellon/document_sketching. 2.2 Template-Based Generation Some existing work induces templates as an intermediate step for performing tasks like text summarization or response generation. Most use a retrieval-based method to extract similar references from the training corpus as prototypes (Cao et al., 2018; Yang et al., 2019; Wang et al., 2019; Gao et al., 2019; Peng et al., 2019), and learn to separate salient information and latent template structure. Cai et al. (2019) induce an intermediate template for response generation explicitly, but from a single retrieved relevant response. Similar prototype editing work (Guu et al., 2018; Hashimoto et al., 2018; Fabbri et al., 2020) focuses on short text (e.g., a question or a single sentence) or structured output (e.g., code snippet) editing. Oya et al. (2014); Magooda and Litman (2019); Yang et al. (2020); Li et al. (2018) convert each single input text into a templa"
2021.naacl-main.340,W05-0909,0,0.0734054,"Missing"
2021.naacl-main.340,D18-1241,0,0.0277876,"questions i.e., questions that ask about information present in a given text (Duan et al., 2017; Zhang and Bansal, 2019). Later, Rao and Daumé III (2018, 2019) introduced the task of clarification question generation in order to ask questions about missing information in a given context. ClarQ (Kumar and Black, 2020) entails clarification questions in a question answering setup. However, unlike our work, these works still suffer from estimating the most useful missing information. Recent works on conversational question answering also focused on the aspect of question generation or retrieval (Choi et al., 2018; Aliannejadi et al., 2019). Qi et al. (2020a) especially focused on generating information-seeking questions while Majumder et al. (2020) proposed a question generation task in free-form interview-style conversations. In this work, in addition to improving clarification question generation in a community-QA dataset, we are the first to explore a goal-oriented dialog scenario as well. Representing context and associated global information in a structure format has been shown to improve performance in generation task (Das et al., 2019; Subramanian et al., 2018; Khashabi et al., 2017) in general"
2021.naacl-main.340,N03-1007,0,0.0249285,"Missing"
2021.naacl-main.340,D17-1090,0,0.0240491,"reflecting them in subsequent generations without retraining from scratch. Dynamic expansion of global schema We anticipate that even if we build the global schema from the available offline dataset, it is possible that new entries may appear in a real application. We 6 Related Work investigate how our framework responds to the dynamic expansion of global schema. We simulate Most previous work on question generation foa scenario where we extend the “Laptop Acces- cused on generating reading comprehension style 4307 questions i.e., questions that ask about information present in a given text (Duan et al., 2017; Zhang and Bansal, 2019). Later, Rao and Daumé III (2018, 2019) introduced the task of clarification question generation in order to ask questions about missing information in a given context. ClarQ (Kumar and Black, 2020) entails clarification questions in a question answering setup. However, unlike our work, these works still suffer from estimating the most useful missing information. Recent works on conversational question answering also focused on the aspect of question generation or retrieval (Choi et al., 2018; Aliannejadi et al., 2019). Qi et al. (2020a) especially focused on generatin"
2021.naacl-main.340,D19-1428,0,0.0217649,", 2019). Qi et al. (2020a) especially focused on generating information-seeking questions while Majumder et al. (2020) proposed a question generation task in free-form interview-style conversations. In this work, in addition to improving clarification question generation in a community-QA dataset, we are the first to explore a goal-oriented dialog scenario as well. Representing context and associated global information in a structure format has been shown to improve performance in generation task (Das et al., 2019; Subramanian et al., 2018; Khashabi et al., 2017) in general and summarization (Fan et al., 2019) and story-generation (Yao et al., 2019) in particular. We also derive inspiration from recent works on information extraction from free-form text (Vedula et al., 2019; Stanovsky et al., 2016) and develop a novel framework to estimate missing information from available natural text contexts. Finally, for question generation, we use BART (Lewis et al., 2019), that is state-of-the-art for many generation tasks such as summarization, dialog generation etc. Furthermore, inspired from recent works that use controlled language generation during decoding (Ghazvininejad et al., 2017; Holtzman et al.,"
2021.naacl-main.340,P17-4008,0,0.0878899,"n context. GAN-Utility The state-of-the-art model for the task of clarification question generation (Rao and Daumé III, 2019) trained on (context, question, answer) triples. Transformer A transformer (Vaswani et al., 2017)5 model trained on (context, question) pairs. BART We finetune a BART model (Lewis et al., 2019) on (context, question) pairs. BART + missinfo We compare to a BART model fine-tuned on (missing schema, question) pairs. BART + missinfo + WD This is similar to the “BART + missinfo” baseline with the modification that, at test time only, we use a weighted-decoding (WD) strategy (Ghazvininejad et al., 2017) by redefining the probability of words in the vocabulary using usefulness criteria (more in appendix). BART + missinfo + PPLM This is our proposed model as described in §3 where we fine-tune the BART model on (missing schema, question) pairs and use a usefulness classifier based PPLM model for decoding at test time. 4.3.2 Human Judgment Similar to Rao and Daumé III (2019), we conduct a human evaluation on Amazon Mechanical Turk to evaluate model generation on the four criteria below. Each generated output is shown with the context and is evaluated by three annotators. Relevance We ask “Is the"
2021.naacl-main.340,P18-1152,0,0.0120698,"Fan et al., 2019) and story-generation (Yao et al., 2019) in particular. We also derive inspiration from recent works on information extraction from free-form text (Vedula et al., 2019; Stanovsky et al., 2016) and develop a novel framework to estimate missing information from available natural text contexts. Finally, for question generation, we use BART (Lewis et al., 2019), that is state-of-the-art for many generation tasks such as summarization, dialog generation etc. Furthermore, inspired from recent works that use controlled language generation during decoding (Ghazvininejad et al., 2017; Holtzman et al., 2018), we use Plug-and-Play-LanguageModel (Dathathri et al., 2019) to tune generations during decoding. While similar approaches for controllable generation (Keskar et al., 2019; See et al., 2019) have been proposed, we extend such efforts to enhance the usefulness of the generated clarification questions. 7 Conclusion We show how we can fine-tune a large-scale pretrained model such as BART on such differences to generate questions about missing information. Further, we show how we can tune these generations to make them more useful using PPLM with a usefulness classifier as its attribute model. Th"
2021.naacl-main.340,K17-1010,0,0.0202377,"n or retrieval (Choi et al., 2018; Aliannejadi et al., 2019). Qi et al. (2020a) especially focused on generating information-seeking questions while Majumder et al. (2020) proposed a question generation task in free-form interview-style conversations. In this work, in addition to improving clarification question generation in a community-QA dataset, we are the first to explore a goal-oriented dialog scenario as well. Representing context and associated global information in a structure format has been shown to improve performance in generation task (Das et al., 2019; Subramanian et al., 2018; Khashabi et al., 2017) in general and summarization (Fan et al., 2019) and story-generation (Yao et al., 2019) in particular. We also derive inspiration from recent works on information extraction from free-form text (Vedula et al., 2019; Stanovsky et al., 2016) and develop a novel framework to estimate missing information from available natural text contexts. Finally, for question generation, we use BART (Lewis et al., 2019), that is state-of-the-art for many generation tasks such as summarization, dialog generation etc. Furthermore, inspired from recent works that use controlled language generation during decodin"
2021.naacl-main.340,2020.acl-main.651,0,0.0179106,"at new entries may appear in a real application. We 6 Related Work investigate how our framework responds to the dynamic expansion of global schema. We simulate Most previous work on question generation foa scenario where we extend the “Laptop Acces- cused on generating reading comprehension style 4307 questions i.e., questions that ask about information present in a given text (Duan et al., 2017; Zhang and Bansal, 2019). Later, Rao and Daumé III (2018, 2019) introduced the task of clarification question generation in order to ask questions about missing information in a given context. ClarQ (Kumar and Black, 2020) entails clarification questions in a question answering setup. However, unlike our work, these works still suffer from estimating the most useful missing information. Recent works on conversational question answering also focused on the aspect of question generation or retrieval (Choi et al., 2018; Aliannejadi et al., 2019). Qi et al. (2020a) especially focused on generating information-seeking questions while Majumder et al. (2020) proposed a question generation task in free-form interview-style conversations. In this work, in addition to improving clarification question generation in a comm"
2021.naacl-main.340,2020.acl-main.703,0,0.0187723,"Missing"
2021.naacl-main.340,N16-1014,1,0.768571,"ation Metrics does not always yield better questions. BART, on the other hand, outperforms GAN-Utility suggest4.3.1 Automatic Metrics ing the benefit of large-scale pretraining (RQ2). BART+missinfo further outperforms BART showBLEU-4 (Papineni et al., 2002) evaluates 4-gram precision between model generation and references. ing the value in training on missing schemata inat the corpus level; METEOR (Banerjee and Lavie, stead of training directly on the context (RQ1). A variation of this model that uses weighted de2005) additionally uses stem and synonym matches for similarity; and Distinct-2 (Li et al., 2016) mea- coding performs marginally better on METEOR but slightly worse of BLEU-4. Our final proposed sures diversity by calculating the number of distinct model i.e., BART+missinfo+PPLM performs the bigrams in model generations scaled by the total best among all baselines across both BLEU-4 and number of generated tokens. METEOR. 5 We use original hyperparameters & tokenization scheme. Under diversity (Distinct-2), the retrieval model 4304 Model BLEU-4 METEOR Distinct-2 Retrieval 8.76 9.23 0.92 GAN-Utility 14.23 16.82 0.79 Transformer 12.89 14.56 0.60 BART 15.98 16.78 0.78 + missinfo 16.87 17.11"
2021.naacl-main.340,W15-4640,0,0.103753,"n in a text. In the first stage, we find what’s missing by taking a difference between the global knowledge’s schema and schema of the local context (§3.1). In the second stage we feed this missing schema to a fine-tuned BART (Lewis et al., 2019) model to generate a question which is further made more useful using PPLM (Dathathri et al., 2019) (§3.2).1 We test our proposed model on two scenarios (§2): community-QA, where the context is a product description from amazon.com (McAuley and Yang, 2016) (see e.g. Table 1); and dialog where the context is a dialog history from the Ubuntu Chat forum (Lowe et al., 2015). We compare our model to several baselines (§4.2) and evaluate outputs using both automatic metrics and human evaluation to show that our model significantly outperforms baselines in generating useful questions that identify missing information in a given context (§4.4). 1 The code is available at https://github.com/ microsoft/clarification-qgen-globalinfo 4300 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4300–4312 June 6–11, 2021. ©2021 Association for Computational Linguistics Title Targ"
2021.naacl-main.340,2020.emnlp-main.653,1,0.762985,"and Daumé III (2018, 2019) introduced the task of clarification question generation in order to ask questions about missing information in a given context. ClarQ (Kumar and Black, 2020) entails clarification questions in a question answering setup. However, unlike our work, these works still suffer from estimating the most useful missing information. Recent works on conversational question answering also focused on the aspect of question generation or retrieval (Choi et al., 2018; Aliannejadi et al., 2019). Qi et al. (2020a) especially focused on generating information-seeking questions while Majumder et al. (2020) proposed a question generation task in free-form interview-style conversations. In this work, in addition to improving clarification question generation in a community-QA dataset, we are the first to explore a goal-oriented dialog scenario as well. Representing context and associated global information in a structure format has been shown to improve performance in generation task (Das et al., 2019; Subramanian et al., 2018; Khashabi et al., 2017) in general and summarization (Fan et al., 2019) and story-generation (Yao et al., 2019) in particular. We also derive inspiration from recent works"
2021.naacl-main.340,P02-1040,0,0.110293,"ion that matches the most with the missing schema does not always yield a good question. This strengthens the need of the second stage of our proposed model i.e. BART + PPLM based learning. GAN-Utility, which is state-of-the-art on Amazon, outperforms the Transformer baseline suggesting that training a larger model (in terms of the number of parameters) 4.3 Evaluation Metrics does not always yield better questions. BART, on the other hand, outperforms GAN-Utility suggest4.3.1 Automatic Metrics ing the benefit of large-scale pretraining (RQ2). BART+missinfo further outperforms BART showBLEU-4 (Papineni et al., 2002) evaluates 4-gram precision between model generation and references. ing the value in training on missing schemata inat the corpus level; METEOR (Banerjee and Lavie, stead of training directly on the context (RQ1). A variation of this model that uses weighted de2005) additionally uses stem and synonym matches for similarity; and Distinct-2 (Li et al., 2016) mea- coding performs marginally better on METEOR but slightly worse of BLEU-4. Our final proposed sures diversity by calculating the number of distinct model i.e., BART+missinfo+PPLM performs the bigrams in model generations scaled by the t"
2021.naacl-main.340,2020.findings-emnlp.3,0,0.0881204,"hrases. schema s = { element }; where element ∈ {(key-phrase, verb, relation), (1) key-phrase} Schema Extraction Our goal is to extract a schema from a given context. We consider (keyphrase, action verb, relation) as the basic element of our schema. Such triples have been found to be representative of key information in previous work (Vedula et al., 2019). Given a sentence from the context, we first extract bigram and unigram key-phrases using YAKE (Yet-Another-KeywordExtractor) (Campos et al., 2020) and retain only those that contain at least a noun. We then obtain the dependency parse tree (Qi et al., 2020b) of the sentence and map the key-phrases to tree nodes.3 Now, to obtain the required triple, we need to associate a verb and a relation to each key-phrase. This procedure is described in Alg 1. At a high-level, we use the path between the key-phrase and the closest verb in the dependency tree to establish a relation between the key-phrase and the verb. In cases where there is no path, we use only the keyphrase as our schema element. Figure 2 shows an example dependency tree for a sentence. Figure 2: Dependency tree and paths showing how we obtain schema triples for a sentence: “Will this bag"
2021.naacl-main.340,2020.acl-demos.14,0,0.0367878,"hrases. schema s = { element }; where element ∈ {(key-phrase, verb, relation), (1) key-phrase} Schema Extraction Our goal is to extract a schema from a given context. We consider (keyphrase, action verb, relation) as the basic element of our schema. Such triples have been found to be representative of key information in previous work (Vedula et al., 2019). Given a sentence from the context, we first extract bigram and unigram key-phrases using YAKE (Yet-Another-KeywordExtractor) (Campos et al., 2020) and retain only those that contain at least a noun. We then obtain the dependency parse tree (Qi et al., 2020b) of the sentence and map the key-phrases to tree nodes.3 Now, to obtain the required triple, we need to associate a verb and a relation to each key-phrase. This procedure is described in Alg 1. At a high-level, we use the path between the key-phrase and the closest verb in the dependency tree to establish a relation between the key-phrase and the verb. In cases where there is no path, we use only the keyphrase as our schema element. Figure 2 shows an example dependency tree for a sentence. Figure 2: Dependency tree and paths showing how we obtain schema triples for a sentence: “Will this bag"
2021.naacl-main.340,P18-1255,1,0.90207,"Missing"
2021.naacl-main.340,N19-1013,1,0.815452,"Missing"
2021.naacl-main.340,N19-1170,0,0.0124172,"et al., 2016) and develop a novel framework to estimate missing information from available natural text contexts. Finally, for question generation, we use BART (Lewis et al., 2019), that is state-of-the-art for many generation tasks such as summarization, dialog generation etc. Furthermore, inspired from recent works that use controlled language generation during decoding (Ghazvininejad et al., 2017; Holtzman et al., 2018), we use Plug-and-Play-LanguageModel (Dathathri et al., 2019) to tune generations during decoding. While similar approaches for controllable generation (Keskar et al., 2019; See et al., 2019) have been proposed, we extend such efforts to enhance the usefulness of the generated clarification questions. 7 Conclusion We show how we can fine-tune a large-scale pretrained model such as BART on such differences to generate questions about missing information. Further, we show how we can tune these generations to make them more useful using PPLM with a usefulness classifier as its attribute model. Thorough analyses reveal that our framework works across domains, shows robustness towards information availability, and responds to the dynamic change in global knowledge. Although we experime"
2021.naacl-main.340,W18-2609,0,0.0246129,"pect of question generation or retrieval (Choi et al., 2018; Aliannejadi et al., 2019). Qi et al. (2020a) especially focused on generating information-seeking questions while Majumder et al. (2020) proposed a question generation task in free-form interview-style conversations. In this work, in addition to improving clarification question generation in a community-QA dataset, we are the first to explore a goal-oriented dialog scenario as well. Representing context and associated global information in a structure format has been shown to improve performance in generation task (Das et al., 2019; Subramanian et al., 2018; Khashabi et al., 2017) in general and summarization (Fan et al., 2019) and story-generation (Yao et al., 2019) in particular. We also derive inspiration from recent works on information extraction from free-form text (Vedula et al., 2019; Stanovsky et al., 2016) and develop a novel framework to estimate missing information from available natural text contexts. Finally, for question generation, we use BART (Lewis et al., 2019), that is state-of-the-art for many generation tasks such as summarization, dialog generation etc. Furthermore, inspired from recent works that use controlled language g"
2021.naacl-main.414,P17-1171,0,0.0244406,"as it involves making nuet al. (2019) control higher level attributes of text, anced changes to text according to natural lansuch as style, tone, or topic. Our task instead guage commands. We also believe this task has 5266 uses natural language commands, which can flexibly express different types of constraints, ranging from low-level lexical ones, to high-level topical ones. In this sense, we can also draw the parallel to dialog response generation (Ghazvininejad et al., 2018; Dinan et al., 2018), task-oriented dialog (Gao et al., 2018), or open domain question answering (Min et al., 2019; Chen et al., 2017), that also involve user responses or queries, although these tasks are not concerned with text generation in the context of document creation. senting them. Related to Wikipedia data, Pryzant et al. (2020) also used Wikipedia revision histories to learn to debias text, whereas we considered general edits. Iso et al. (2020) propose a factbased text editing task, but they do not consider control or other types of edits. Another related task to text editing is text paraphrasing (Gupta et al., 2018), however paraphrasing usually conserves the meaning of a sentence. While the edits we consider inc"
2021.naacl-main.414,P18-1082,0,0.163969,"the office. add years in office Barack Obama was the 44th President of the United States from 2009 to 2017 and the first African-American to hold the office. Figure 1: An illustration of our interactive text generation setting. This is an example generated by our model. The blue panels represent the text being edited, taken from the document shown on the right. The orange panels represent user edit commands. The model grounds edits in query results from a commercial search engine. A long-standing goal of natural language processing research has been to generate long-form text (Lebowitz, 1985; Fan et al., 2018; Rashkin et al., 2020). Recent large generative language models such as GPT-2 (Radford et al., 2019), and GPT3 (Brown et al., 2020), demonstrate an impressive ability to generate fluent text, but their outputs are difficult to control beyond a prompt, and they manifest a tendency to hallucinate facts (Wiseman et al., 2017). Much recent work has thus focused on making such models more controllable (Keskar et al., 2019; Hu et al., 2017; Zhang et al., 2020; Dathathri et al., 2019), and factually grounded (Guu et al., 2020; Liu et al., 2018b). Most such work only considers a one-shot generation s"
2021.naacl-main.414,D18-1028,0,0.0813115,"of our full model are broken down by edit intention labels in Table 6. The columns report the same metrics as Ablations The middle rows of Table 5 show the in our main table of results, with the exception of results for three ablations of our model. The first S-BLEU, which reports the BLEU score between ablation removes everything but the source senthe source sentence and target, and the last coltence s. This is similar to the paraphrase setumn, which reports the number of test edits that ting (Gupta et al., 2018), and the editing setting were classified into each category. With the caveat in Faruqui et al. (2018) and Yin et al. (2018). that intention labels come from an automatic clasWe can see that including the context, grounding, sifier and not human annotation, we can observe and command as additional inputs yields signifithat our model has varying performance across cant improvements over only using the source sendifferent types of edits. The model performs very tence. We can also see from the second ablation well on fluency edits, but worse on content edits. that the commands are a crucial element in the This comes at no surprise given that fluency edmodel’s performance. This is not surprising s"
2021.naacl-main.414,P18-5002,1,0.889002,"Missing"
2021.naacl-main.414,Q18-1031,0,0.0603054,"h pretrained language model weights, yields encouraging results on both automatic and human evaluations. Additionally, our ablation studies showed the crucial role played by the user command and grounding. Breaking down our results by types of edits, we saw that our model not only performs well on easier fluency edits, but also on much harder content edits. Finally, we discussed future research directions for interactive document generation, as well as possible extensions to other domains such as images or code. Acknowledgments Text Editing Several previous works have focused on text editing. Guu et al. (2018) generate The authors would like to thank Thomas Hofsentences by editing prototypes taken from their mann, as well as Sudha Rao, Matt Richardtraining corpus, although they use editing only as a son, Zhang Li, Kosh Narayanan, and Chandra means for language modeling. Wu et al. (2019) exChikkareddy for their helpful suggestions. pand upon Guu et al. (2018)’s setting, but for dialog. More related to our own setting, Faruqui et al. (2018) propose WikiAtomicEdits, a dataset of edReferences its crawled from Wikipedia. However, they consider a much narrower definition of edits than our Giusepppe Attar"
2021.naacl-main.414,P17-1141,0,0.0461026,"Missing"
2021.naacl-main.414,W19-2405,0,0.0163809,"n generating long-form narratives (Jain et al., 2017). While earlier work in Story Generation focused more on plan-based architectures (Lebowitz, 1985), more recent work moved towards end-to-end approaches (Fan et al., 2018) allowing generation to be unconstrained and creative. As narratives are often aimed at particular goals expressed in terms of outlines and plans, much of the literature in Story Generation is framed as a form of controllable generation, using storylines (Peng et al., 2018), events (Martin et al., 2017; Harrison et al., 2017), plot words or word skeletons (Xu et al., 2018; Ippolito et al., 2019), plans (Yao et al., 2019), story ending (Tambwekar et al., 2019), and outlines (Rashkin et al., 2020) as various forms of constraints. Our work takes a significantly different approach, as we treat document or story generation as an iterative process that allows a human to generate a full document from scratch, but also allows constraints to be more dynamic (e.g., add nationality in Table 9 only if the system missed that the first time). 8 Conclusion In this work we argued that text generation should be interactive, and, as a means towards that end, we proposed a general text editing task, wh"
2021.naacl-main.414,2020.acl-main.17,0,0.0328833,"om low-level lexical ones, to high-level topical ones. In this sense, we can also draw the parallel to dialog response generation (Ghazvininejad et al., 2018; Dinan et al., 2018), task-oriented dialog (Gao et al., 2018), or open domain question answering (Min et al., 2019; Chen et al., 2017), that also involve user responses or queries, although these tasks are not concerned with text generation in the context of document creation. senting them. Related to Wikipedia data, Pryzant et al. (2020) also used Wikipedia revision histories to learn to debias text, whereas we considered general edits. Iso et al. (2020) propose a factbased text editing task, but they do not consider control or other types of edits. Another related task to text editing is text paraphrasing (Gupta et al., 2018), however paraphrasing usually conserves the meaning of a sentence. While the edits we consider include meaning-preserving edits, we are mostly interested in edits that affect meaning. Story Generation The task of Document Generation considered in our work bears similarity with work on generating long-form narratives (Jain et al., 2017). While earlier work in Story Generation focused more on plan-based architectures (Leb"
2021.naacl-main.414,J06-4003,0,0.0584681,"Missing"
2021.naacl-main.414,N19-1238,0,0.0634756,"Missing"
D08-1084,W06-3123,0,0.0103284,"heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14 For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. 810 lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT performance. We would argue that previous work in MT phrase alignment is orthogonal to our work. In MANLI, the need for phrases arises when word-based representations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality. In MT phrase alignment, it is beneficial to account for arbitrarily large phr"
D08-1084,J93-2003,0,0.00979229,"ods in Natural Language Processing, pages 802–811, c Honolulu, October 2008. 2008 Association for Computational Linguistics 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clau"
D08-1084,W07-1427,1,0.825638,"Missing"
D08-1084,P08-2007,0,0.0210802,"daligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14 For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. 810 lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT performance. We would argue that previous work in MT phrase alignment is orthogonal to our work. In MANLI, the need for phrases arises when word-based representations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality. In MT phrase alignment, it is beneficial to account for arbitrarily large phrases, since the larger co"
D08-1084,W06-3105,0,0.0259711,"October 2008. 2008 Association for Computational Linguistics 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Indeed, one cannot assume"
D08-1084,J07-3002,0,0.0176829,"Missing"
D08-1084,W07-1428,0,0.0289183,"m et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al. (2006) and Hickl and Bensley (2007). However, each of these systems has pursued alignment in idiosyncratic and poorly-documented ways, often using proprietary data, making comparisons and further development difficult. In this paper we undertake the first systematic study of alignment for NLI. We propose a new NLI alignment system which uses a phrase-based representation of alignment, exploits external resources for knowledge of semantic relatedness, and capitalizes on the recent appearance of new supervised training data for NLI alignment. In addition, we examine the relation between NLI alignment and MT alignment, and investi"
D08-1084,O97-1002,0,0.00458023,"involved in the edit, and whether these phrases are non-constituents (in syntactic parses of the sentences involved). Lexical similarity feature. For SUB edits, a very important feature represents the lexical similarity of the substituends, as a real value in [0, 1]. This similarity score is computed as a max over a number of component scoring functions, some based on external lexical resources, including: • various string similarity functions, of which most are applied to word lemmas • measures of synonymy, hypernymy, antonymy, and semantic relatedness, including a widelyused measure due to Jiang and Conrath (1997), based on manually constructed lexical resources such as WordNet and NomBank • a function based on the well-known distributional similarity metric of Lin (1998), which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources— both manually and automatically constructed—is critical to the success of MANLI. Contextual features. Even when the lexical similarity for a SUB edit is high, it may not be a good match. If P or H contains multiple occurrences of the same word—which happens freq"
D08-1084,P07-2045,0,0.00293131,"performance of the LCC system, all achieve respectable results, and the Stanford and MANLI aligners outperform the average RTE2 entry. Thus, even if alignment quality does not determine inferential validity, many NLI systems could be improved by harnessing a well-designed NLI aligner. 7 Related work Given the extensive literature on phrase-based MT, it may be helpful further to situate our phrase-based alignment model in relation to past work. The standard approach to training a phrase-based MT system is to apply phrase extraction heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14 For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. 810 lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebase"
D08-1084,N06-1014,0,0.11006,"Missing"
D08-1084,P98-2127,0,0.0185433,"ant feature represents the lexical similarity of the substituends, as a real value in [0, 1]. This similarity score is computed as a max over a number of component scoring functions, some based on external lexical resources, including: • various string similarity functions, of which most are applied to word lemmas • measures of synonymy, hypernymy, antonymy, and semantic relatedness, including a widelyused measure due to Jiang and Conrath (1997), based on manually constructed lexical resources such as WordNet and NomBank • a function based on the well-known distributional similarity metric of Lin (1998), which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources— both manually and automatically constructed—is critical to the success of MANLI. Contextual features. Even when the lexical similarity for a SUB edit is high, it may not be a good match. If P or H contains multiple occurrences of the same word—which happens frequently with function words, and occasionally with content words—lexical similarity may not suffice to determine the right match. To remedy this, we introduce con"
D08-1084,N06-1006,1,0.811831,"Missing"
D08-1084,W02-1018,0,0.041262,"s 802–811, c Honolulu, October 2008. 2008 Association for Computational Linguistics 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Ind"
D08-1084,W05-1201,0,0.0773166,"n bags of words. While ignoring structure, such methods depend on matching each word in H to the word in P with which it is most similar—in effect, an alignment. At the other extreme, Tatu and Moldovan (2007) and Bar-Haim et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al. (2006) and Hickl and Bensley (2007). However, each of these systems has pursued alignment in idiosyncratic and poorly-documented ways, often using proprietary data, making comparisons and further development difficult. In this paper we undertake the first systematic study of alignment for NLI. We propose a new NLI alignment system which uses a phrase-based representation of alignment, exploits external resources"
D08-1084,J03-1002,0,0.0704504,"tly, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Indeed, one cannot assume even approximate semantic equivalence—usually a given in MT. Because NLI prob"
D08-1084,C96-2141,0,0.253093,"age Processing, pages 802–811, c Honolulu, October 2008. 2008 Association for Computational Linguistics 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no co"
D08-1084,C04-1051,0,\N,Missing
D08-1084,W02-1001,0,\N,Missing
D08-1084,W07-1404,0,\N,Missing
D08-1084,2006.amta-papers.2,0,\N,Missing
D08-1084,C98-2122,0,\N,Missing
D08-1089,koen-2004-pharaoh,0,\N,Missing
D08-1089,N04-4026,0,\N,Missing
D08-1089,P06-1067,0,\N,Missing
D08-1089,J09-4009,0,\N,Missing
D08-1089,P06-1066,0,\N,Missing
D08-1089,J04-4002,0,\N,Missing
D08-1089,P07-2045,0,\N,Missing
D08-1089,P06-1098,0,\N,Missing
D08-1089,P05-1033,0,\N,Missing
D08-1089,N03-1017,0,\N,Missing
D08-1089,W06-3108,0,\N,Missing
D08-1089,J97-3002,0,\N,Missing
D08-1089,W05-0908,0,\N,Missing
D08-1089,W04-3250,0,\N,Missing
D08-1089,N04-1021,0,\N,Missing
D08-1089,N03-1021,0,\N,Missing
D08-1089,D08-1076,0,\N,Missing
D11-1004,W08-0304,0,0.659722,"tly; prior work offers no algorithm with a good approximation guarantee. While much of the earlier work in MERT (Chou et al., 1993; Juang et al., 1997) relies on standard convex optimization techniques applied to non-convex problems, the Och algorithm (Och, 2003) represents a significant advance for MERT since it applies a series of special line minimizations that happen to be exhaustive and efficient. Since this algorithm remains inexact in the multidimensional case, much of the recent work on MERT has focused on extending Och’s algorithm to find better search directions and starting points (Cer et al., 2008; Moore and Quirk, 2008), and on experimenting with other derivative-free methods such as the Nelder-Mead simplex algorithm (Nelder and Mead, 1965; Zens et al., 2007; Zhao and Chen, 2009). In this paper, we present LP-MERT, an exact search algorithm for N -best optimization that exploits general assumptions commonly made with MERT, e.g., that the error metric is decomposable by sentence.1 While there is no known optimal algo1 Note that MERT makes two types of approximations. First, the set of all possible outputs is represented only approximately, by N -best lists, lattices, or hypergraphs. Se"
D11-1004,D10-1059,0,0.117181,"Missing"
D11-1004,D08-1024,0,0.709704,"e thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics"
D11-1004,J07-2003,0,0.0822759,"Quirk et al., 2005): it first applies a dependency parser to the source language data at both training and test time. Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations. Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted. At runtime, these mappings and templates are used to construct transduction rules to convert the source tree into a target string. The best transduction is sought using approximate search techniques (Chiang, 2007). Each hypothesis is scored by a relatively standard set of features. The mappings contain five features: maximum-likelihood estimates of source given target and vice versa, lexical weighting estimates of source given target and vice versa, and a constant value that, when summed across a whole hypothesis, indicates the number of mappings used. For each template, we include a maximum-likelihood estimate of the target reordering given the source structure. The system may fall back to templates that mimic the source word order; the count of such templates is a feature. Likewise we include a featu"
D11-1004,P08-2010,0,0.204434,"rge tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical"
D11-1004,W05-1506,0,0.053108,"combinations that are not extreme. For instance, if we find that (h1,1 ,h2,2 ) is interior for sentences s = 1, 2, the divide-and-conquer branch for s = 1 . . . 4 never actually receives this bad combination from its left child, thus avoiding the cost of enumerating combinations that are known to be interior, e.g., (h1,1 ,h2,2 , h3,1 ,h4,1 ). The LP-MERT algorithm for the general case is shown as Algorithm 2. It basically only calls a recursive divide-and-conquer function (G ET N EXT B EST) for sentence range 1 . . . S. The latter function uses binary lazy enumeration in a manner similar to (Huang and Chiang, 2005), and relies on two global variables: I and L. The first of these, I, is used to memoize the results of calls to G ET N EXT B EST; given a range of sentences and a rank n, it stores the nth best combination for that range of sentences. The global variable L stores hypotheses combination matrices, one matrix for each range of sentences (s, t) as shown in {h31, h41} {h32, h41} h21 h11 h12 h13 {h11, h23} 126.0 {h12, h21} 126.1 h22 h24 h23 69.1 69.2 69.2 69.9 h31 h32 69.3 69.4 70.0 h33 L[1,2] 126.5 h41 h42 h23 56.8 57.1 57.9 57.3 57.6 Combinations checked: {h11, h23, h31, h41} {h12, h21, h31, h41}"
D11-1004,P07-2045,0,0.0124069,"owards a promising region), but beam pruning also helps reduce LP optimization time and thus enables us to 44 explore a wider space. Since wbest often improves during search, it is useful to run multiple iterations of LP-MERT until wbest doesn’t change. Two or three iterations suffice in our experience. In our experiments, we use a beam size of 1000. 4 Experimental Setup Our experiments in this paper focus on only the application of machine translation, though we believe that the current approach is agnostic to the particular system used to generate hypotheses. Both phrasebased systems (e.g., Koehn et al. (2007)) and syntaxbased systems (e.g., Li et al. (2009), Quirk et al. (2005)) commonly use MERT to train free parameters. Our experiments use a syntax-directed translation approach (Quirk et al., 2005): it first applies a dependency parser to the source language data at both training and test time. Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations. Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted. At runtime, these mappin"
D11-1004,P09-1019,0,0.383186,"Missing"
D11-1004,D08-1088,0,0.0475917,"et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010). This particular refinement is orthogonal to our approach, though. We expect to extend LP-MERT 8 One interesting observation is that the performance of 1DMERT degrades as S grows from 2 to 8 (Fig. 5), which contrasts with the results shown in Tab. 2. This may have to do with the fact that N -best lists with S = 2 have much fewer local maxima than with S = 4, 8, in which case 20 restarts is generally enough. 46 to hypergraphs in future work. Exact search may be challenging due to the computational complexity of the search space (Leusch et al., 2008), but approximate search should be feasible. Other research has explored alternate methods of gradient-free optimization, such as the downhillsimplex algorithm (Nelder and Mead, 1965; Zens et al., 2007; Zhao and Chen, 2009). Although the search space is different than that of Och’s algorithm, it still relies on one-dimensional line searches to reflect, expand, or contract the simplex. Therefore, it suffers the same problems of one-dimensional MERT: feature sets with complex non-linear interactions are difficult to optimize. LP-MERT improves on these methods by searching over a larger subspace"
D11-1004,W09-0424,0,0.0133969,"lps reduce LP optimization time and thus enables us to 44 explore a wider space. Since wbest often improves during search, it is useful to run multiple iterations of LP-MERT until wbest doesn’t change. Two or three iterations suffice in our experience. In our experiments, we use a beam size of 1000. 4 Experimental Setup Our experiments in this paper focus on only the application of machine translation, though we believe that the current approach is agnostic to the particular system used to generate hypotheses. Both phrasebased systems (e.g., Koehn et al. (2007)) and syntaxbased systems (e.g., Li et al. (2009), Quirk et al. (2005)) commonly use MERT to train free parameters. Our experiments use a syntax-directed translation approach (Quirk et al., 2005): it first applies a dependency parser to the source language data at both training and test time. Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations. Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted. At runtime, these mappings and templates are used to construct transducti"
D11-1004,P06-1096,0,0.269228,"Missing"
D11-1004,C04-1072,0,0.616086,"error count along any line is a piecewise constant function. Furthermore, this function for a single sentence may be computed efficiently by first finding the hypotheses that form the upper envelope of the model score function, then gathering the error count for each hypothesis along the range for which it is optimal. Error counts for the whole corpus are simply the sums of these piecewise constant functions, leading to an 2 A metric such as TER is decomposable by sentence. BLEU is not, but its sufficient statistics are, and the literature offers several sentence-level approximations of BLEU (Lin and Och, 2004; Liang et al., 2006). efficient algorithm for finding the global optimum of the error count along any single direction. Such a hill-climbing algorithm in a non-convex space has no optimality guarantee: without a perfect direction finder, even a globally-exact line search may never encounter the global optimum. Coordinate ascent is often effective, though conjugate direction set finding algorithms, such as Powell’s method (Powell, 1964; Press et al., 2007), or even random directions may produce better results (Cer et al., 2008). Random restarts, based on either uniform sampling or a random wal"
D11-1004,D08-1076,0,0.241701,"Missing"
D11-1004,P05-1012,0,0.0976301,"only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequ"
D11-1004,E06-1038,0,0.0204582,"r space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate"
D11-1004,C08-1074,1,0.730402,"fers no algorithm with a good approximation guarantee. While much of the earlier work in MERT (Chou et al., 1993; Juang et al., 1997) relies on standard convex optimization techniques applied to non-convex problems, the Och algorithm (Och, 2003) represents a significant advance for MERT since it applies a series of special line minimizations that happen to be exhaustive and efficient. Since this algorithm remains inexact in the multidimensional case, much of the recent work on MERT has focused on extending Och’s algorithm to find better search directions and starting points (Cer et al., 2008; Moore and Quirk, 2008), and on experimenting with other derivative-free methods such as the Nelder-Mead simplex algorithm (Nelder and Mead, 1965; Zens et al., 2007; Zhao and Chen, 2009). In this paper, we present LP-MERT, an exact search algorithm for N -best optimization that exploits general assumptions commonly made with MERT, e.g., that the error metric is decomposable by sentence.1 While there is no known optimal algo1 Note that MERT makes two types of approximations. First, the set of all possible outputs is represented only approximately, by N -best lists, lattices, or hypergraphs. Second, error functions on"
D11-1004,P02-1038,0,0.921328,"ns such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss. While competitive in practice, MERT faces several challenges, the most significant of which is search. The unsmoothed error count is a highly non-convex objective function and therefore difficult to optimize directly; prior work offers no algorithm with a good approximation gua"
D11-1004,P03-1021,0,0.709167,"that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al."
D11-1004,2001.mtsummit-papers.68,0,0.0219041,"statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss. While competitive in practice, MERT faces several challenges, the most significant of which is search. The unsmoothed error count is a highly non-convex objective function and therefore difficult to optimize directly; prior work offers no algorithm with a good approximation guarantee. While much of the earlier work in MERT (Chou et al., 1993; Juang et al., 1997) relies on"
D11-1004,P05-1034,1,0.759044,"mization time and thus enables us to 44 explore a wider space. Since wbest often improves during search, it is useful to run multiple iterations of LP-MERT until wbest doesn’t change. Two or three iterations suffice in our experience. In our experiments, we use a beam size of 1000. 4 Experimental Setup Our experiments in this paper focus on only the application of machine translation, though we believe that the current approach is agnostic to the particular system used to generate hypotheses. Both phrasebased systems (e.g., Koehn et al. (2007)) and syntaxbased systems (e.g., Li et al. (2009), Quirk et al. (2005)) commonly use MERT to train free parameters. Our experiments use a syntax-directed translation approach (Quirk et al., 2005): it first applies a dependency parser to the source language data at both training and test time. Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations. Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted. At runtime, these mappings and templates are used to construct transduction rules to convert t"
D11-1004,P06-2101,0,0.138351,"s scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evalu"
D11-1004,2006.amta-papers.25,0,0.0285717,"on (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss. While competitive in practice, MERT faces several challenges, the most significant of which is search. The unsmoothed error count is a highly non-convex objective function and therefore difficult to optimize directly; prior work offers no algorithm with a good approximation guarantee. While much of the earlier work in MERT (Chou et al., 1993; Juang et al., 1997) relies on standard convex optimizatio"
D11-1004,D07-1080,0,0.29665,"e can also change the objective function in a number of ways to make it more amenable to optimization, leveraging knowledge from elsewhere in the machine learning community. Instance reweighting as in boosting may lead to better parameter inference (Duh and Kirchhoff, 2008). Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). Smith and Eisner (2006) use a smoothed objective along with deterministic annealing in hopes of finding good directions and climbing past locally optimal points. Other papers use margin methods such as MIRA (Watanabe et al., 2007; Chiang et al., 2008), updated somewhat to match the MT domain, to perform incremental training of potentially large numbers of features. However, in each of these cases the objective function used for training no longer matches the final evaluation metric. 7 Conclusions Our primary contribution is the first known exact search algorithm for direct loss minimization on N best lists in multiple dimensions. Additionally, we present approximations that consistently outperform standard one-dimensional MERT on a competitive machine translation system. While Och’s method of MERT is generally quite s"
D11-1004,D09-1006,0,0.0451561,"lgorithm for optimizing task loss on N -best lists in general dimensions. We also present an approximate version of LP-MERT that offers a natural means of trading speed for accuracy, as we are guaranteed to eventually find the global optimum as we gradually increase beam size. This trade-off may be beneficial in commercial settings and in large-scale evaluations like the NIST evaluation, i.e., when one has a stable system and is willing to let MERT run for days or weeks to get the best possible accuracy. We think this work would also be useful as we turn to more human involvement in training (Zaidan and Callison-Burch, 2009), as MERT in this case is intrinsically slow. 2 Unidimensional MERT Let f S1 = f1 . . . fS denote the S input sentences of our tuning set. For each sentence fs , let Cs = 39 es,1 . . . es,N denote a set of N candidate translations. For simplicity and without loss of generality, we assume that N is constant for each index s. Each input and output sentence pair (fs , es,n ) is weighted by a linear model that combines model parameters w = w1 . . . wD ∈ RD with D feature functions h1 (f , e, ∼) . . . hD (f , e, ∼), where ∼ is the hidden state associated with the derivation from f to e, such as phr"
D11-1004,D07-1055,0,0.365227,"Missing"
D11-1004,N09-2006,0,0.375842,"ization techniques applied to non-convex problems, the Och algorithm (Och, 2003) represents a significant advance for MERT since it applies a series of special line minimizations that happen to be exhaustive and efficient. Since this algorithm remains inexact in the multidimensional case, much of the recent work on MERT has focused on extending Och’s algorithm to find better search directions and starting points (Cer et al., 2008; Moore and Quirk, 2008), and on experimenting with other derivative-free methods such as the Nelder-Mead simplex algorithm (Nelder and Mead, 1965; Zens et al., 2007; Zhao and Chen, 2009). In this paper, we present LP-MERT, an exact search algorithm for N -best optimization that exploits general assumptions commonly made with MERT, e.g., that the error metric is decomposable by sentence.1 While there is no known optimal algo1 Note that MERT makes two types of approximations. First, the set of all possible outputs is represented only approximately, by N -best lists, lattices, or hypergraphs. Second, error functions on such representations are non-convex and previous work only offers approximate techniques to optimize them. Our work avoids the second approximation, while the fir"
D11-1004,P02-1040,0,\N,Missing
D13-1106,W12-2703,0,0.286096,"average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward networks as well as conventional n-gram models, both of which are limited to fixed-length contexts. Building on the success of recurrent architectures, we base our joint language and translation model on an extension of the recurrent neural network language model (Mikolov and Zweig, 2012) that introduces a layer of additional inputs (§2). Most previous work on"
D13-1106,J92-4003,0,0.206592,"s from 2010-2011 containing between 2034-3003 sentences. Log-linear weights are estimated on the 2009 data set comprising 2525 sentences. We rescore the lattices produced by the baseline systems with an aggressive but effective context beam of k = 1 that did not harm accuracy in preliminary experiments (§3). Neural Network Language Model. The vocabularies of the language models are comprised of the words in the training set after removing singletons. We obtain word-classes using a version of Brown-Clustering with an additional regularization term to optimize the runtime of the language model (Brown et al., 1992; Zweig and Makarychev, 2013). 1048 Direct connections use maximum entropy features over unigrams, bigrams and trigrams (Mikolov et al., 2011a). We use the standard settings for the model with the default learning rate α = 0.1 that decays exponentially if the validation set entropy does not increase after each epoch. Back propagation through time computes error gradients over the past twenty time steps. Training is stopped after 20 epochs or when the validation entropy does not decrease over two epochs. We experiment with varying training data sizes and randomly draw the data from the same cor"
D13-1106,D11-1103,0,0.0212121,"e representing an unbounded history of both source and target words, rather than a feedforward style network. Feed-forward networks and n-gram models have a finite history which makes predictions independent of anything but a small history of words. Furthermore, we only model the target-side which is different to previous work modeling both sides. We introduced a new algorithm to tackle lattice rescoring with an unbounded model. The automatic speech recognition community has previously addressed this issue by either approximating longspan language models via simpler but more tractable models (Deoras et al., 2011b), or by identifying confusable subsets of the lattice from which n-best lists are constructed and rescored (Deoras et al., 2011a). We extend their work by directly mapping a recurrent neural network model onto the structure of the lattice, rescoring all states instead of focusing only on subsets. 7 Conclusion and Future Work Joint language and translation modeling with recurrent neural networks leads to substantial gains over the 1-best decoder output, raising accuracy by up to 1.5 BLEU and by 1.1 BLEU on average across several test sets. The joint approach also improves over the gains of th"
D13-1106,N03-1017,0,0.0236734,"result in similar recurrent histories, which in turn reduces the effect of aggressive pruning. 4 Language Model Experiments Recurrent neural network language models have previously only been used in n-best rescoring settings and on small-scale tasks with baseline language models trained on only 17.5m words (Mikolov, 2012). We extend this work by experimenting on lattices using strong baselines with ngram models trained on over one billion words and by evaluating on a number of language pairs. 4.1 Experimental Setup Baseline. We experiment with an in-house phrasebased system similar to Moses (Koehn et al., 2003), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrasepenalties, a linear distortion feature and a lexicalized reordering feature. Log-linear weights are estimated with minimum error rate training (Och, 2003). Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English, German-English and EnglishGerman. Translation models are estimated on 102m words of parallel"
D13-1106,P07-2045,0,0.0219354,"ynamic programming point of view. Fortunately, we do not need to maintain entire translations as context in the states: the recurrent model compactly encodes the entire history of previous words in the hidden layer configuration hi . It is therefore sufficient to add hi as context, instead of the entire translation. The language model can then simply score any new words based on hi from the previous state when a new state is created. A much larger problem is that items, that were previously equivalent from a dynamic programming perspective, may now be different. Standard phrasebased decoders (Koehn et al., 2007) recombine decoder states with the same context into a single state because they are equivalent to the model features; usually recombination retains only the highest scoring candidate.3 However, if the context is large, then the amount of recombination will decrease significantly, leading to less variety in the decoder beam. This was confirmed in preliminary experiments where we simulated context sizes of up to 100 words but found that accuracy dropped by between 0.5-1.0 BLEU. Integrating a long-span language model na¨ıvely requires to keep context equivalent to the entire left prefix of the t"
D13-1106,W04-3250,0,0.111211,"d any significant improvements. Even this representation of sentences is composed of a large number of instances, and so we resorted to feature hashing by computing feature ids as the least significant 20 bits of each feature name. Our best transform achieved a cosine similarity of 0.816 on the training data, 0.757 on the validation data, and 0.749 on news2011. The results (Table 8) show that the transform improves over the recurrent neural network language model on all test sets and by 0.2 BLEU on average. We verified significance over the target-only model using paired bootstrap resampling (Koehn, 2004) over all test sets (7526 sentences) at the p < 0.001 level. Overall, we improve accuracy by up to 1.5 1052 BLEU and by 1.1 BLEU on average across all test sets over the decoder 1-best with our joint language and translation model. 6 Related Work Our approach of combining language and translation modeling is very much in line with recent work on n-gram-based translation models (Crego and Yvon, 2010), and more recently continuous space-based translation models (Le et al., 2012a; Gao et al., 2013). The joint model presented in this paper differs in a number of key aspects: we use a recurrent arc"
D13-1106,N12-1005,0,0.864704,"l builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward net"
D13-1106,D08-1076,0,0.0229444,"Missing"
D13-1106,P03-1021,0,0.174336,"models trained on over one billion words and by evaluating on a number of language pairs. 4.1 Experimental Setup Baseline. We experiment with an in-house phrasebased system similar to Moses (Koehn et al., 2003), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrasepenalties, a linear distortion feature and a lexicalized reordering feature. Log-linear weights are estimated with minimum error rate training (Och, 2003). Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English, German-English and EnglishGerman. Translation models are estimated on 102m words of parallel data for French-English, 91m words for German-English and English-German; between 3.5-5m words are newswire, depending on the language pair, and the remainder are parliamentary proceedings. The baseline systems use two 5-gram modified Kneser-Ney language models; the first is estimated on the target-side of the parallel data, while the second is based on a large newswire corpus released as part o"
D13-1106,W12-2702,0,0.0348831,"known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward networks as well as convent"
D13-1106,N13-1090,1,\N,Missing
D13-1106,W11-2135,0,\N,Missing
D13-1201,J93-2003,0,0.0480096,"g to compute w0 in the first place is a bit of a disadvantage compared to standard MERT, the need for good initializer is hardly surprising in the context of non-convex optimization. Other non-convex problems in machine learning, such as deep neural networks (DNN) and word alignment models, commonly require such initializers in order to obtain decent performance. In the case of DNN, extensive research is devoted to the problem of finding good initializers.10 In the case of word alignment, it is common practice to initialize search in non-convex optimization problems—such as IBM Model 3 and 4 (Brown et al., 1993)—with solutions of simpler models—such as IBM Model 1. 7 Related work the weights in the context of MERT, (Cer et al., 2008) achieves a related effect. Cer et al.’s goal is to achieve a more regular or smooth objective function, while ours is to obtain a more regular set of parameters. The two approaches may be complementary. More recently, new research has explored direction finding using a smooth surrogate loss function (Flanigan et al., 2013). Although this method is successful in helping MERT find better directions, it also exacerbates the tendency of MERT to overfit.11 As an indirect way"
D13-1201,W08-0304,0,0.721887,"ularized objective function along the line. Finally, we address the issue of searching in a high-dimensional space by using the gradient of expected BLEU (Smith and Eisner, 2006) to find better search directions for our line searches. This direction finder addresses one of the serious concerns raised by Hopkins and May (2011): MERT widely failed to reach the optimum of a synthetic linear objective function. In replicating Hopkins and May’s experiments, we confirm that existing search algorithms for MERT—including coordinate ascent, Powell’s algorithm (Powell, 1964), and random direction sets (Cer et al., 2008)—perform poorly in this experimental condition. However, when using our gradient-based direction finder, MERT has no problem finding the true optimum even in a 1000-dimensional space. Our results suggest that the combination of a regularized objective function and a gradient-informed line search algorithm enables MERT to scale well with a large number of features. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO (Hopkins and May, 2011), a parameter tuning method known to be effective with large feature sets. 2 Unregularized MERT Prior to i"
D13-1201,N12-1047,1,0.910733,"Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good as MERT on small feature se"
D13-1201,N13-1003,1,0.916487,"g to w∗ has a BLEU score of 1, and so that the translation with lowest model score for the sentence gets a BLEU of zero. This normalization has no impact on search, but makes results more interpretable. For our translation experiments, we use multistack phrase-based decoding (Koehn et al., 2007). We report results for two feature sets: non-linear features induced using Gradient Boosting Machines (Toutanova and Ahn, 2013) and sparse lexicalized 7 The objective function remains piecewise constant, and the plateau containing w∗ maps to the optimal value of the function. 1954 reordering features (Cherry, 2013). We exploit these feature sets (GBM and SparseHRM, respectively) in two distinct experimental conditions, which we detail in the two next paragraphs. Both GBM and SparseHRM augment baseline features similar to Moses’: relative frequency and lexicalized phrase translation scores for both translation directions; one or two language model features, depending on the language pair; distortion penalty; word and phrase count; six lexicalized reordering features. For both experimental conditions, phrase tables have maximum phrase length of 7 words on either side. In reference to Table 1, we used the"
D13-1201,D08-1024,0,0.392627,"Missing"
D13-1201,N09-1025,0,0.103202,"Microsoft Research Chris Quirk Colin Cherry Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method app"
D13-1201,J07-2003,0,0.09406,"tion can drive the regularization term down to zero by scaling down w. As special treatments for `2 , we evaluate three linear transforms of the weight vector, where the vector w of the regularization term ||w||22 /2σ 2 is replaced with either: 1. an affine transform: w − w0 2. a vector with only (D − 1) free parameters, e.g., 0 ) (1, w20 , · · · , wD 3. an `1 renormalization: w/||w||1 In (1), regularization is biased towards w0 , a weight vector previously optimized using a competitive yet much smaller feature set, such as core features of a phrase-based (Koehn et al., 2007) or hierarchical (Chiang, 2007) system. The requirement that this feature set be small is to prevent overfitting. Otherwise, any regularization toward an overfit parameter vector w0 would defeat the purpose of introducing a regularization term in the first place.3 In (2), the transformation is motivated by the observation that the D-parameter linear model of Equation 2 only needs (D − 1) degrees of freedom. Fixing one of the components of w to any non-zero constant and allowing the others to vary, the new linear model retains the same modeling power, but the (D − 1) free parameters are no longer scale invariant, i.e., scali"
D13-1201,N13-1025,0,0.320973,"Missing"
D13-1201,D11-1004,1,0.672028,"search towards the greatest increase of expected BLEU score. While our best results are comparable to PRO and not significantly better, we think that this paper provides a deeper understanding of why standard MERT can fail when handling an increasingly larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents"
D13-1201,N12-1023,0,0.787448,"one of the components of w to any non-zero constant and allowing the others to vary, the new linear model retains the same modeling power, but the (D − 1) free parameters are no longer scale invariant, i.e., scaling the (D − 1)-dimensional vector now has an effect on linear model predictions. In (3), the weight vector is normalized as to have an `1 -norm equal to 1. In contrast, the `0 norm is scale insensitive, thus not affected by this problem. 3.1 Exact line search with regularization Optimizing with a regularized error surface requires a change in the line search algorithm presented in 3 (Gimpel and Smith, 2012, footnote 6) briefly mentions the use of such a regularizer with its ramp loss objective function. 1951 Section 2, but the other aspects of MERT remain the same, and we can still use global search algorithms such as coordinate ascent, Powell, and random directions exactly the same way as with unregularized MERT. Line search with a regularization term is still as efficient as in (Och, 2003), and it is still guaranteed to find the optimum of the (now regularized) objective function along the line. Considering again a given point wt and a given direction dt at line search iteration t, finding th"
D13-1201,P12-1031,0,0.190707,"Missing"
D13-1201,D11-1125,0,0.578272,"ris Quirk Colin Cherry Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good a"
D13-1201,P07-2045,0,0.0817011,"constant intervals of the corpus-level error function, and by selecting the one that has the lowest error count (or, correspondingly, highest BLEU score). Assuming the optimum is found in the interval [γk−1 , γk ], we define γopt = (γk−1 + γk )/2 and change the parameters using the update wt+1 = wt + γopt · dt . Finally, this method is turned into a global Ddimensional search using algorithms that repeatedly use the aforementioned exact line search algorithm. Och (2003) first advocated the use of Powell’s method (Powell, 1964; Press et al., 2007). Pharaoh (Koehn, 2004) and subsequently Moses (Koehn et al., 2007) instead use coordinate ascent, and more recent work often uses random search directions (Cer et al., 2008; Macherey et al., 2008). In Section 4, we will present a novel direction finder for maximum-BLEU optimization, which uses the gradient of expected BLEU to find directions where the BLEU score is most likely to increase. 3 Regularization for MERT Because MERT is prone to overfitting when a large number of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regularize the objective function with a penalty ba"
D13-1201,koen-2004-pharaoh,0,0.526758,"omputed by enumerating all piecewise constant intervals of the corpus-level error function, and by selecting the one that has the lowest error count (or, correspondingly, highest BLEU score). Assuming the optimum is found in the interval [γk−1 , γk ], we define γopt = (γk−1 + γk )/2 and change the parameters using the update wt+1 = wt + γopt · dt . Finally, this method is turned into a global Ddimensional search using algorithms that repeatedly use the aforementioned exact line search algorithm. Och (2003) first advocated the use of Powell’s method (Powell, 1964; Press et al., 2007). Pharaoh (Koehn, 2004) and subsequently Moses (Koehn et al., 2007) instead use coordinate ascent, and more recent work often uses random search directions (Cer et al., 2008; Macherey et al., 2008). In Section 4, we will present a novel direction finder for maximum-BLEU optimization, which uses the gradient of expected BLEU to find directions where the BLEU score is most likely to increase. 3 Regularization for MERT Because MERT is prone to overfitting when a large number of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regular"
D13-1201,P09-1019,0,0.704285,"rches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of adding a regularization term to the MERT objective function can be perplexing at first, because the most common regularizers, such as `1 and `2 , are not directly applicable to MERT. Indeed, these regul"
D13-1201,P06-1096,0,0.129495,"Missing"
D13-1201,C04-1072,0,0.0497267,"our optimizer featuring both regularization and the gradient-based direction finder. All variants of MERT are initialized with a single starting point, which is either uniform weight or w0 . Instead of providing MERT with additional random starting points as in Moses, we use random walks as in (Moore and Quirk, 2008) to attempt to move out of local optima.8 Since PRO and our optimizer have hyperparameters, we use a held-out set (Dev) for adjusting them. For PRO, we adjust three parameters: a regularization penalty for `2 , the parameter α in the add-α smoothed sentence-level version of BLEU (Lin and Och, 2004), and a parameter for scaling the corpus-level length of the references. The latter scaling parameter is discussed in (He and 8 In the case of the gradient-based direction finder, we also use the following strategy whenever optimization converges to a (possibly local) optimum. We run one round of coordinate ascent, and continue with the gradient direction finder as soon as the optimum improves. If the none of the coordinate directions helped, we stop the search. Method MERT MERT MERT PRO `2 MERT (v1: ||w − w0 ||) `2 MERT (v2: D − 1 dimensions) `2 MERT (v3: `1 -renormalized) `0 MERT Starting pt"
D13-1201,D08-1076,0,0.707037,"ed BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of adding a regularization term to the MERT objective function can be perplexing at first, because the most common regularizers, such as `1 and `2 , are not directly app"
D13-1201,C08-1074,1,0.906037,"Finally, Figure 3 shows our rate of convergence compared to coordinate ascent. Our experimental results with the GBM feature set data are shown in Table 2. Each table is divided into three sections corresponding respectively to MERT (Och, 2003) with Koehn-style coordinate ascent (Koehn, 2004), PRO, and our optimizer featuring both regularization and the gradient-based direction finder. All variants of MERT are initialized with a single starting point, which is either uniform weight or w0 . Instead of providing MERT with additional random starting points as in Moses, we use random walks as in (Moore and Quirk, 2008) to attempt to move out of local optima.8 Since PRO and our optimizer have hyperparameters, we use a held-out set (Dev) for adjusting them. For PRO, we adjust three parameters: a regularization penalty for `2 , the parameter α in the add-α smoothed sentence-level version of BLEU (Lin and Och, 2004), and a parameter for scaling the corpus-level length of the references. The latter scaling parameter is discussed in (He and 8 In the case of the gradient-based direction finder, we also use the following strategy whenever optimization converges to a (possibly local) optimum. We run one round of coo"
D13-1201,C12-1121,0,0.0629013,"on the Tune set. For PRO and regularized MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental condition the model that worked best on Dev. The table shows the performance of these retained models. 52.6 52.4 BLEU 52.2 52 51.8 51.6 51.4 51.2 1e-05 expected BLEU gradient coordinate ascent 0.0001 0.001 0.01 0.1 regularization weight 1 10 Figure 4: BLEU score on the Finnish Dev set (GBM) with different values for the 1/2σ 2 regularization weight. To enable comparable results, the other hyperparameter (length) is kept fixed. Deng, 2012; Nakov et al., 2012) and addresses the problem that systems tuned with PRO tend to produce sentences that are too short. On the other hand, regularized MERT only requires one hyperparameter to tune: a regularization penalty for `2 or `0 . However, since PRO optimizes translation length on the Dev dataset and MERT does so using the Tune set, a comparison of the two systems would yield a discrepancy in length that would be undesirable. Therefore, we add another hyperparameter to regularized MERT to tune length in the same manner using the Dev set. Table 2 offers several findings. First, unregularized MERT can achie"
D13-1201,P02-1038,0,0.192281,"integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of add"
D13-1201,P03-1021,0,0.145765,"s `2 are inapplicable to MERT due to the scale invariance of its objective function, we turn to two regularizers—`0 and a modification of `2 — and present methods for efficiently integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g"
D13-1201,2001.mtsummit-papers.68,0,0.129733,"umber of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regularize the objective function with a penalty based on the qP 2 Euclidean norm ||w||2 = i wi , also known as `2 regularization. In the case of MERT, this yields the following objective function:2 ˆ = arg min w w X S s=1 ||w||22 E(rs , ˆ e(fs ; w)) + 2σ 2  (4) 1 This assumes that the sufficient statistics of the metric under consideration are additively decomposable by sentence, which is the case with most popular evaluation metrics such as BLEU (Papineni et al., 2001). 2 The `2 regularizer is often used in conjunction with loglikelihood objectives. The regularization term of Equation 4 could similarly be added to the log of an objective—e.g., log(BLEU) instead of BLEU—but we found that the distinction doesn’t have much of an impact in practice. 1950 1.4 1.2 1 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 MERT Max at 0.225 × × 0 0.1 0.2 0.3 0.4 1.4 MERT − `2 1.2 Max at -0.018 × 1 −`2 0.8 0.6 × 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 × 1.4 MERT − `0 1.2 Max at 0 × 1 `0 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 γ, the step size in the"
D13-1201,W11-2119,0,0.0513815,"larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents a pre-trained DNN that outperforms a shallow network, but the performance of the DNN becomes much worse relative to the shallow network once pre-training is turned off. 11 Indeed, in their Table 3, a comparison between HILS and HOLS suggests tuning"
D13-1201,P06-2101,0,0.503336,"l Language Processing, pages 1948–1959, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics regularization, where we apply `2 regularization to scale-senstive linear transforms of the original linear model. In addition, we introduce efficient methods of incorporating regularization in Och (2003)’s exact line searches. For all of these regularizers, our methods let us find the true optimum of the regularized objective function along the line. Finally, we address the issue of searching in a high-dimensional space by using the gradient of expected BLEU (Smith and Eisner, 2006) to find better search directions for our line searches. This direction finder addresses one of the serious concerns raised by Hopkins and May (2011): MERT widely failed to reach the optimum of a synthetic linear objective function. In replicating Hopkins and May’s experiments, we confirm that existing search algorithms for MERT—including coordinate ascent, Powell’s algorithm (Powell, 1964), and random direction sets (Cer et al., 2008)—perform poorly in this experimental condition. However, when using our gradient-based direction finder, MERT has no problem finding the true optimum even in a 1"
D13-1201,P13-2072,1,0.846348,"ector hs,m . By this linear construction, w∗ is guaranteed to be a global optimum.7 The pseudo-BLEU score is normalized for each M -best list, so that the translation with highest model score according to w∗ has a BLEU score of 1, and so that the translation with lowest model score for the sentence gets a BLEU of zero. This normalization has no impact on search, but makes results more interpretable. For our translation experiments, we use multistack phrase-based decoding (Koehn et al., 2007). We report results for two feature sets: non-linear features induced using Gradient Boosting Machines (Toutanova and Ahn, 2013) and sparse lexicalized 7 The objective function remains piecewise constant, and the plateau containing w∗ maps to the optimal value of the function. 1954 reordering features (Cherry, 2013). We exploit these feature sets (GBM and SparseHRM, respectively) in two distinct experimental conditions, which we detail in the two next paragraphs. Both GBM and SparseHRM augment baseline features similar to Moses’: relative frequency and lexicalized phrase translation scores for both translation directions; one or two language model features, depending on the language pair; distortion penalty; word and p"
D13-1201,D07-1080,0,0.35769,"tanding of why standard MERT can fail when handling an increasingly larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents a pre-trained DNN that outperforms a shallow network, but the performance of the DNN becomes much worse relative to the shallow network once pre-training is turned off. 11 Indeed, in t"
D13-1201,P02-1040,0,\N,Missing
D14-1132,J92-4003,0,0.0612884,"+j , we have: model. We experiment with these variants and extensions: ∂γθT h(f, e) ∂hk (e, f ) = γλk ∂sφ (o, pp) ∂sφ (o, pp) • SparseHRMLocal: This feature set is exclu= γλk N (o, pp, e, f ) sively based on the local phrase-pair and By using the following definition: 1254 consists of features over the first and last word of both the source and target phrase.5 We use four different word representations: The word identity itself, but only for the 80 most common source and target language words. The three other word representations are based on Brown clustering with either 20, 50 or 80 classes (Brown et al., 1992). There is one feature for every orientation type. • SparseHRM: The main feature set of Cherry (2013). This is an extension of SparseHRMLocal adding features based on the first and last word of both the source and the target of the hierarchical block at the top of the stack. There are also features based on the source words in-between the current phrase and the hierarchical block at the top of the stack. • SparseHRM+UncommonWords: This set is identical to SparseHRM, except that wordidentity features are not restricted to the 80 most frequent words, but can be instantiated for all words, regard"
D14-1132,N13-1003,0,0.0676183,"red interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to"
D14-1132,N09-1025,0,0.0205233,"extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to fit millions of parameters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with"
D14-1132,J07-2003,0,0.116682,"ering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in p"
D14-1132,P13-4034,0,0.0438564,"Missing"
D14-1132,D08-1089,1,0.946153,"ingle phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum like"
D14-1132,N04-1035,1,0.812734,"train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work"
D14-1132,P06-1121,1,0.750834,"discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer fea"
D14-1132,N13-1048,1,0.922125,"dels to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c Octobe"
D14-1132,P14-1066,1,0.937605,"mensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c October 25-29, 2014, Doh"
D14-1132,W14-3360,0,0.0343077,"t attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c October 25-29, 2014, Doha, Qatar. 2014 Associ"
D14-1132,P12-1031,0,0.116388,"parse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1"
D14-1132,D11-1125,0,0.0430797,"of features to 3M (SparseHRM+BiPhrases) results in a slightly better average gain of 0.3 BLEU for CLL but but expected BLEU still achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11 MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets (Eidelman et al., 2013). set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set comprising of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baselin"
D14-1132,N03-1017,0,0.233513,"features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering mode"
D14-1132,P07-2045,0,0.0295763,"ient beam search (Och and Ney, 2004). Early phrase-based models simply relied on a linear distortion feature, which measures the distance between the first word of the current source phrase and the last word of the previous source phrase (Koehn et al., 2003; Och and Ney, 2004). Unfortunately, this approach is agnostic to the actual phrases being reordered, and does not take into account that certain phrases are more likely to be reordered than others. This shortcoming led to a range of lexicalized reordering models that capture exactly those preferences for individual phrases (Tillmann, 2003; Koehn et al., 2007). Reordering models generally assume a sequence of English phrases e = {¯ e1 , . . . , e¯n } currently hypothesized by the decoder, a phrase alignment a = {a1 , . . . , an } that defines a foreign phrase f¯ai for each English phrase e¯i , and an orientation oi which describes how a phrase pair should be reordered with respect to the previous phrases. There are typically three orientation types and the exact definition depends on the specific models which we describe below. Orientations can be determined during decoding and from wordaligned training corpora. Most models estimate a probability d"
D14-1132,P06-1096,0,0.0617731,"Missing"
D14-1132,C12-1121,0,0.0133419,"to be useful. To prevent overfitting, we experimented with `2 regularization, but found that it did not improve test accuracy. We also tuned the probability scaling parameter γ (Eq. 6) but found γ = 1 to be very good among other settings. We evaluate the performance on a held-out validation set during training and stop whenever the objective changes less than a factor of 0.0003. For our PRO experiments, we tuned three hyper-parameters controlling `2 regularization, sentence-level BLEU smoothing, and length. The latter is important to eliminate PRO’s tendency to produce too short translations (Nakov et al., 2012). 7.1 Scaling the Feature Set We first compare our baseline, a likelihood trained hierarchical reordering model (HRM; Galley & Manning, 2008), to various expected BLEU trained models, starting with SparseHRMLocal, inspired by Cherry (2013) and compare it to SparseHRM+BiPhrases, a set that is three orders of magnitudes larger. Our results on French-English translation (Table 1) and German-English translation (Table 2) show that the expected BLEU trained models scale to millions of features and that we outperform the baseline by up to 2.0 BLEU on newstest2012 for French-English and by up to 1.1"
D14-1132,2009.mtsummit-papers.10,0,0.125252,"Missing"
D14-1132,J04-4002,0,0.749151,"s of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maxi"
D14-1132,P03-1021,0,0.208745,"ights, we generate n-best lists for the foreign sentences in the training data using the setup described in the experimental section (§7). The n-best lists serve as an approximation to E(f ), the set of possible translations of f , used in the next step for expected BLEU training of the reordering model (§5). 3. Next, we fix θ, set θm+1 = 1, . . . θm+j = 1 and optimize φ with respect to the loss function on the training data using stochastic gradient descent.2 4. Finally, we fix φ and re-optimize θ in the presence of the discriminative reordering model using Minimum Error Rate Training (MERT; Och 2003; §7). l(φ) = − xBLEU(φ) X pθ,φ (e|f ) sBLEU(e, e(i) ) =− e∈E(f ) pθ,φ (e|f ) = P (6) ∂l(φ) X ∂l(φ) ∂sφ (o, pp) = ∂φ ∂sφ (o, pp) ∂φ o,pp X = −δo,pp u(o, pp) o,pp Expected BLEU Objective Function The expected BLEU objective (Gao and He, 2013; Gao et al., 2014) allows us to efficiently optimize a large scale discriminative reordering model towards the desired task-specific metric, which in our setting is BLEU. 2 exp{γθT h(f, e)} T 0 e0 ∈E(f ) exp{γθ h(f, e )} where θT h(f, e) includes the discriminative reordering model hm+1 (e, f ), . . . , hm+j (e, f ) parameterized by φ, and γ ∈ [0, inf) is a"
D14-1132,P02-1040,0,0.0906386,"llow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to fit millions of parameters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with lit"
D14-1132,W10-1748,0,0.0531366,"eters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natur"
D14-1132,W11-2119,0,0.0180257,"ould like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processi"
D14-1132,N03-2036,0,0.0831567,"ering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering mode"
D14-1132,D08-1065,0,0.0274346,") X ∂l(φ) ∂sφ (o, pp) = ∂φ ∂sφ (o, pp) ∂φ o,pp X = −δo,pp u(o, pp) o,pp Expected BLEU Objective Function The expected BLEU objective (Gao and He, 2013; Gao et al., 2014) allows us to efficiently optimize a large scale discriminative reordering model towards the desired task-specific metric, which in our setting is BLEU. 2 exp{γθT h(f, e)} T 0 e0 ∈E(f ) exp{γθ h(f, e )} where θT h(f, e) includes the discriminative reordering model hm+1 (e, f ), . . . , hm+j (e, f ) parameterized by φ, and γ ∈ [0, inf) is a tuned scaling factor that flattens the distribution for γ < 1 and sharpens it for γ > 1 (Tromble et al., 2008).4 Next, we define the gradient of the expected BLEU loss function l(φ). To simplify our notation we omit the local context c in sφ (o, pp, c) (Eq. 3) from now on and assume it to be part of pp. Using the observation that the loss does not explicitly depend on φ, we get: We found that re-optimizing θ after a few iterations of stochastic gradient descent in step 3 did not improve accuracy. 5 (5) We tuned θm+1 , . . . θm+j on the development set but found that setting them uniformly to one resulted in faster training and equal accuracy. where δo,pp is the error term for orientation o of phrase p"
D14-1132,J97-3002,0,0.339127,"ows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation,"
D14-1132,P06-1066,0,0.283179,"t is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Mann"
D14-1132,D13-1112,0,0.029053,"ill achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11 MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets (Eidelman et al., 2013). set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set comprising of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baseline features (§7) as well as the 4.4K indicator features corresponding to the sparse reordering model. For expected BLEU t"
D14-1132,N04-4026,0,\N,Missing
D14-1132,W12-3102,0,\N,Missing
D14-1132,2005.iwslt-1.8,0,\N,Missing
D15-1021,P84-1044,0,0.207614,"Missing"
D15-1021,D14-1086,0,0.0746329,"Missing"
D15-1021,P11-1020,0,0.00577849,"s also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations. outdoor environments), showing multiple simultaneous events between a subset of four objects: a person, a backpack, a chair, and a trash-can. Each video was manually annotated (with very restricted grammar and lexicon) with several sentences describing what occurs in the video. • Microsoft Research Video Description Corpus (MS VDC) (Chen and Dolan, 2011) contains parallel descriptions (85,550 English ones) of 2,089 short video snippets (10-25 seconds long). The descriptions are one sentence summaries about the actions or events in the video as described by Amazon Turkers. In this dataset, both paraphrase and bilingual alternatives are captured, hence, the dataset can be useful translation, paraphrasing, and video description purposes. 3.3 Beyond Visual Description Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically"
D15-1021,N15-1053,0,0.0853302,"iled analysis of reporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. User-generated Captions • SBU Captioned Photo Dataset (Ordonez et al., 2011) contains 1 million images with original user generated captions, collected in the wild by systematic querying of Flickr. This dataset is collected by querying Flickr for specific terms such as objects and actions and then filtered images with descriptions longer than certain mean length. • D´ej`a Images Dataset (Chen et al., 2015) consists of 180K unique user-generated captions associated with 4M Flickr images, where one caption is aligned with multiple images. This dataset was collected by querying Flickr for 693 high frequency nouns, then further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Captions of Densely Labeled Images Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Ima"
D15-1021,N15-1015,0,0.0118245,"is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3 This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4 We did not use an external synonym"
D15-1021,P14-5010,0,0.0019222,"Missing"
D15-1021,N15-3006,1,0.0451788,"d Frazier measurements (Frazier, 1985); each provides a different counting on the number of nodes in the phrase markers of syntactic trees. • Part of Speech Distribution measures the distribution of nouns, verbs, adjectives, and other parts of speech. • Abstract:Concrete Ratio (#Conc, #Abs, %Abs) indicates the range of visual and non-visual concepts the dataset covers. Abstract terms are ideas or concepts, such as ‘love’ or ‘think’ and concrete terms are all the objects or events that are mainly available to the senses. For this purpose, we use a list of most common abstract terms in English (Vanderwende et al., 2015), and define concrete terms as all other words except for a small set of function words. • Average Sentence Length (Sent Len.) shows how rich and descriptive the sentences are. • Perplexity provides a measure of data skew by measuring how expected sentences are from one corpus according to a model trained on another corpus. We analyze perplexity (Ppl) for each dataset against a 5-gram language model learned on a generic 30B words English dataset. We further analyze pair-wise perplexity of datasets against each other in Section 4. Quality Criteria for Language & Vision Datasets The quality of a"
D15-1021,N15-1173,0,0.0673315,"Missing"
D15-1021,N15-1017,0,0.00967414,"ning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3 This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4 We did not"
D15-1021,H89-1033,0,0.673771,"ailable corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision & language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision & language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Proc"
D15-1021,S14-1015,1,0.879416,"n size and contain more contextual descriptions. 3.1.1 3.1.3 Existing caption datasets provide images paired with captions, but such brief image descriptions capture only a subset of the content in each image. Measuring the magnitude of the reporting bias inherent in such descriptions helps us to understand the discrepancy between what we can learn for the specific task of image captioning versus what we can learn more generally from the photographs people take. One dataset useful to this end provides image annotation for content selection: • Microsoft Research Dense Visual Annotation Corpus (Yatskar et al., 2014) provides a set of 500 images from the Flickr 8K dataset (Rashtchian et al., 2010) that are densely labeled with 100,000 textual labels, with bounding boxes and facets annotated for each object. This approximates “gold standard” visual recognition. To get a rough estimate of the reporting bias in image captioning, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A mor"
D15-1021,W10-0721,0,0.81083,"atasets provide images paired with captions, but such brief image descriptions capture only a subset of the content in each image. Measuring the magnitude of the reporting bias inherent in such descriptions helps us to understand the discrepancy between what we can learn for the specific task of image captioning versus what we can learn more generally from the photographs people take. One dataset useful to this end provides image annotation for content selection: • Microsoft Research Dense Visual Annotation Corpus (Yatskar et al., 2014) provides a set of 500 images from the Flickr 8K dataset (Rashtchian et al., 2010) that are densely labeled with 100,000 textual labels, with bounding boxes and facets annotated for each object. This approximates “gold standard” visual recognition. To get a rough estimate of the reporting bias in image captioning, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we fo"
D15-1021,Q13-1003,0,0.034408,"CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3 This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attribu"
D15-1021,Q14-1006,0,0.177701,"ists of 180K unique user-generated captions associated with 4M Flickr images, where one caption is aligned with multiple images. This dataset was collected by querying Flickr for 693 high frequency nouns, then further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Captions of Densely Labeled Images Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets (Rashtchian et al., 2010), and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events. • Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning chall"
D15-1021,P13-1006,0,0.0201542,"ntics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3 This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4 We did not use an external synonym or paraphrasing resource to perform the matching between labels and captions, as the dataset itself provides paraphrases for each object: each object is labeled by multiple Turkers, who labeled Isa relations (e.g., “eagle” is a “bird”). 209 Size"
D15-1021,W03-0610,0,0.0240278,"ize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision & language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision & language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗ 1 F.F. and N.M. contributed equally"
D15-1021,H05-2005,1,\N,Missing
D15-1021,W10-0707,0,\N,Missing
D16-1127,D11-1054,1,\N,Missing
D16-1127,W00-0306,0,\N,Missing
D16-1127,P02-1040,0,\N,Missing
D16-1127,P10-1083,0,\N,Missing
D16-1127,P15-1152,0,\N,Missing
D16-1127,P16-1094,1,\N,Missing
D16-1127,D16-1230,0,\N,Missing
D16-1127,P11-1028,0,\N,Missing
D16-1127,P16-1153,1,\N,Missing
D19-1190,I17-2069,0,0.0689783,"different styles into a shared latent space where the ”content” information is preserved and ”style” information is discarded. An adversarial discriminator is used to align the latent spaces of two different styles. However, Yang et al. (2018) point out the difficulty of training an adversarial discriminator and proposed instead the use of language models as discriminator. Like Shen et al. (2017); Yang et al. (2018), we align latent spaces for different styles. However we also align latent spaces encoded by different models (S2S and AE). Stylized response generation is a relatively new task. Akama et al. (2017) use a stylized conversation corpus to fine-tune a conversation model pretrained on a background conversation dataset. However, stylized texts are usually in non-conversational format, as in the present setting. Niu and Bansal (2018) proposed a method that takes the weighted average of the token probability distribution predicted by a S2S trained on background conversational dataset and that predicted by a LM trained on style dataset as the token probability. They observed reduced relevance and attributed this to the fact that the LM was not trained to attend to conversation context and S2S wa"
D19-1190,W14-4012,0,0.0260137,"Missing"
D19-1190,N19-1125,1,0.816761,"oder to promote generation of response for that target speaker. However non-conversational data cannot be used. Luan et al. (2017) applied a multi-task learning approach to utilize non-conversational data. A S2S model, taking in conversational data, and an autoencoder (AE), taking in nonconversational data, share the decoder and are trained alternately. However, Gao et al. (2019b) observed that sharing the decoder may not truly allow S2S and AE to share the latent space, and thus S2S may not fully utilize what is learned by AE. Unlike Li et al. (2016b) using labelled persona IDs, Zhang et al. (2019) have proposed using a self-supervised method to extract persona features from conversation history. This allows modeling persona dynamically, which agrees with the fact that even the same person can speak in different style in different scenarios. Multi-task learning McCann et al. (2018); Liu et al. (2019); Luan et al. (2017); Gao et al. (2019b); Zhang et al. (2017) aggregates the strengths of each specific task, and induces regularization effects (Liu et al., 2019) as the model is trained to learn a more universal representation. However a simple multi-task approach (Luan et al., 2017) may l"
D19-1190,N19-1320,0,0.0371334,"ecific responses from conversational data and non-parallel non-conversation style data. 2) We generalize the S PACE F USION model of (Gao et al., 2019b) to non-parallel data by a new 2 Integrated into Microsoft Icecaps toolkit (Shiv et al., 2019) https://github.com/microsoft/ icecaps. regularization method. 3) We present a visualization analysis that provides intuitive insights into the drawbacks of alternative approaches. 2 Related Work Text style transfer is a related but distinct task. It usually preserves the content (Yang et al., 2018; Hu et al., 2017; Fu et al., 2018; Shen et al., 2017; Gong et al., 2019). In contrast, content of conversational responses in a given context can be semantically diverse. Various approaches have been proposed for non-parallel data setup. Fu et al. (2018) proposed to use separate decoders for different styles and a classifier to measure style strength. Shen et al. (2017) proposed to map texts of two different styles into a shared latent space where the ”content” information is preserved and ”style” information is discarded. An adversarial discriminator is used to align the latent spaces of two different styles. However, Yang et al. (2018) point out the difficulty o"
D19-1190,N16-1014,1,0.955002,"and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets"
D19-1190,P16-1094,1,0.952451,"and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets"
D19-1190,P19-1441,1,0.789342,"ional data, share the decoder and are trained alternately. However, Gao et al. (2019b) observed that sharing the decoder may not truly allow S2S and AE to share the latent space, and thus S2S may not fully utilize what is learned by AE. Unlike Li et al. (2016b) using labelled persona IDs, Zhang et al. (2019) have proposed using a self-supervised method to extract persona features from conversation history. This allows modeling persona dynamically, which agrees with the fact that even the same person can speak in different style in different scenarios. Multi-task learning McCann et al. (2018); Liu et al. (2019); Luan et al. (2017); Gao et al. (2019b); Zhang et al. (2017) aggregates the strengths of each specific task, and induces regularization effects (Liu et al., 2019) as the model is trained to learn a more universal representation. However a simple multi-task approach (Luan et al., 2017) may learn separate representations for each dataset (Gao et al., 2019b). To address this, in previous work (Gao et al., 2019b), we proposed the S PACE F USION model featuring a regularization technique that explicitly encourages alignment of latent spaces for a universal representation. S PACE F USION, however,"
D19-1190,I17-1061,1,0.723732,"ghly correspond to contents and style intensity, respectively, illustrated by examples taken from Table 2. linguistic style of each other, sometime even regardless of their intentions. Achieving this level of performance, however, is challenging. Lacking parallel data in different conversational styles, researchers often resort to what we will term style datasets that are in non-conversational format (e.g. news, novels, blogs). Since the contents and formats of these are quite different from conversation data, existing approaches tend to generate responses that are either less style-specific (Luan et al., 2017) or less context-relevant (Niu and Bansal, 2018). We suggest that this trade-off between appropriateness and style stems from profound differences 1814 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1814–1823, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics between conversation and style datasets in format, style and contents that impede joint learning. One approach has been to combine these only during decoding: Niu and Bansal (2018) t"
D19-1190,P16-2020,0,0.0288575,"control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets are available at htt"
D19-1190,Q18-1027,0,0.414647,"ty, respectively, illustrated by examples taken from Table 2. linguistic style of each other, sometime even regardless of their intentions. Achieving this level of performance, however, is challenging. Lacking parallel data in different conversational styles, researchers often resort to what we will term style datasets that are in non-conversational format (e.g. news, novels, blogs). Since the contents and formats of these are quite different from conversation data, existing approaches tend to generate responses that are either less style-specific (Luan et al., 2017) or less context-relevant (Niu and Bansal, 2018). We suggest that this trade-off between appropriateness and style stems from profound differences 1814 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1814–1823, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics between conversation and style datasets in format, style and contents that impede joint learning. One approach has been to combine these only during decoding: Niu and Bansal (2018) trained two models separately, a Sequence-to-Sequ"
D19-1190,P02-1040,0,0.104322,"Missing"
D19-1190,P19-1539,1,0.883551,"Missing"
D19-1190,P19-3021,1,0.834508,"m Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, our system can generate responses in a targeted style and outperforms competitive baselines. Our contribution can be summarized thus: 1) We introduce an end-to-end approach that generates style-specific responses from conversational data and non-parallel non-conversation style data. 2) We generalize the S PACE F USION model of (Gao et al., 2019b) to non-parallel data by a new 2 Integrated into Microsoft Icecaps toolkit (Shiv et al., 2019) https://github.com/microsoft/ icecaps. regularization method. 3) We present a visualization analysis that provides intuitive insights into the drawbacks of alternative approaches. 2 Related Work Text style transfer is a related but distinct task. It usually preserves the content (Yang et al., 2018; Hu et al., 2017; Fu et al., 2018; Shen et al., 2017; Gong et al., 2019). In contrast, content of conversational responses in a given context can be semantically diverse. Various approaches have been proposed for non-parallel data setup. Fu et al. (2018) proposed to use separate decoders for differe"
I17-1047,N15-1053,0,0.036937,"Missing"
I17-1047,P04-1077,0,0.0230767,"ext C. The function V counts the number of verbs in the hypothesis and |h |denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N|D| best list. We define idf(h, D) = log |{d∈D:h∈d}| , where we set N =10 to cut short each N-best list. These parameters were selected following reranking experiments on the validation set. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011). 5.2 Retrieval Models In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016b; Devlin et al., 2015; 3 An example generic question is where is this? and a generic response is I don’t know. 467 Do you think this happened"
I17-1047,P15-2017,0,0.200084,"des answers. Figure 2 contrasts an example ICG conversation with the VisDial dataset. As this example shows, IGC involves natural conversations with the image as the grounding, where the literal objects (e.g., the pumpkins) may not even be mentioned in the conversation at all, whereas VisDial targets explicit image understanding. More recently, Das et al. (2017b) have explored the VisDial dataset with richer models that incorporate deep-reinforcement learning. Related Work Vision and Language Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al."
I17-1047,D16-1230,0,0.0151424,"Example question and response generations on IGCCrowd test set. All the generation models use beam search with reranking. In the textual context, <UTT> separates different utterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual Context Sensitive Model (V-Ret). This model uses only the provided image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the f c7 vision feature vectors. Then we"
I17-1047,W15-4640,0,0.0312834,"ce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logue corpus (Lowe et al., 2015) is the largest corpus of dialogues (almost 1 million mainly 3-turn dialogues) for the specific topic of troubleshooting Ubuntu problems. On the other hand, for openended conversation modeling (chitchat), now a high demand application in AI, shared datasets with which to track progress are severely lacking. The ICG task presented here lies nicely in the continuum between the two, where the visual grounding of event-centric images constrains the topic of conversation to contentful utterances. To enable benchmarking of progress in the IGC task, we constructed the IGCCrowd dataset for validation"
I17-1047,P15-2073,1,0.665818,"tterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual Context Sensitive Model (V-Ret). This model uses only the provided image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the f c7 vision feature vectors. Then we retrieve those K annotations as our pool of K candidates. Finally, we compute the textual similarity among the questions in the pool according to a Smoothed-BLEU (Lin and Och,"
I17-1047,W16-1007,1,0.460669,"Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logu"
I17-1047,D11-1125,0,0.00844008,"|h |denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N|D| best list. We define idf(h, D) = log |{d∈D:h∈d}| , where we set N =10 to cut short each N-best list. These parameters were selected following reranking experiments on the validation set. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011). 5.2 Retrieval Models In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016b; Devlin et al., 2015; 3 An example generic question is where is this? and a generic response is I don’t know. 467 Do you think this happened on highway <EOS> <GO> day this not . . decoder This was not the way I imag"
I17-1047,P16-1170,1,0.732217,"Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logu"
I17-1047,N16-1147,1,0.891951,"Missing"
I17-1047,P02-1040,0,0.122669,"at this baseball game. <UTT> Nice, which team won? My team won this game. 10 for me and 28 for my dad. ding ding ding! No it wasn’t too bad of a bang up. Yes. Nah, I’m at home now. lords cricket ground . beautiful. He’s not mine! Table 4: Example question and response generations on IGCCrowd test set. All the generation models use beam search with reranking. In the textual context, <UTT> separates different utterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual C"
I17-1047,N16-1014,1,0.87514,"tistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following two consecutive conversational steps: • Question Generation: Gi"
I17-1047,P16-1094,1,0.153491,"tistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following two consecutive conversational steps: • Question Generation: Gi"
I17-1047,D16-1090,0,0.027098,"sations with the image as the grounding, where the literal objects (e.g., the pumpkins) may not even be mentioned in the conversation at all, whereas VisDial targets explicit image understanding. More recently, Das et al. (2017b) have explored the VisDial dataset with richer models that incorporate deep-reinforcement learning. Related Work Vision and Language Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Qu"
I17-1047,D11-1054,0,0.0139427,"ional questions and responses for the IGCCrowd contexts and initial questions. Table 1 shows three full conversations found in the IGCCrowd dataset. These examples show show that eventful images lead to conversations that are semantically rich and appear to involve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models t"
I17-1047,P15-1152,0,0.0151711,"olve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following tw"
I17-1047,N15-1020,1,0.516832,"rich and appear to involve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC"
I17-1047,N15-1173,0,0.0116461,"Missing"
I17-1047,H89-1033,0,0.533497,"Missing"
I17-1047,N10-1020,1,\N,Missing
I17-1047,P98-1013,0,\N,Missing
I17-1047,C98-1013,0,\N,Missing
I17-1061,P16-2020,1,0.793373,"Missing"
I17-1061,D17-1279,1,0.830225,"Missing"
I17-1061,P15-2073,1,0.174602,"Missing"
I17-1061,P03-1021,0,0.0260752,"maximum length of the generated candidates was set at 20 tokens. At each time step, we first examine all B × B possible next-word candidates, and add all hypotheses ending with an EOS token to the N-best list. We then preserve the top-B unfinished hypotheses and move to the next word position. We then use LSTM-MMI to rerank the N-best list and use the 1-best result of the re-ranked list in all evaluation. where p(R|M, v) is the probability of the generated response given message M and the respondents user ID. |R |is the length of the target and γ is the associated penalty weight. We use MERT (Och, 2003) to optimize γ and λ on BLEU using N-best lists of response candidates generated from the development set. To compute p(M |R), we train an inverse S EQ 2S EQ model by swapping messages and responses. The reverse S EQ 2S EQ models p(M |R) is trained with no user information considered. 6.4 MTASK -M Table 2: Performance on the Twitter dataset of 2-layer S EQ 2S EQ models and MMI models. Distinct-1 and distinct-2 are respectively the number of distinct unigrams and bigrams divided by total number of generated words. Baseline log p(R|M, v) + λ log p(M |R) + γ|R| MTASK -S Table 1: Perplexity for st"
I17-1061,N15-1124,0,0.0209474,"Missing"
I17-1061,P02-1040,0,0.120451,"wapping messages and responses. The reverse S EQ 2S EQ models p(M |R) is trained with no user information considered. 6.4 MTASK -M Table 2: Performance on the Twitter dataset of 2-layer S EQ 2S EQ models and MMI models. Distinct-1 and distinct-2 are respectively the number of distinct unigrams and bigrams divided by total number of generated words. Baseline log p(R|M, v) + λ log p(M |R) + γ|R| MTASK -S Table 1: Perplexity for standard S EQ 2S EQ and the user model on the Twitter Persona dev set. As in previous work (Sordoni et al., 2015), we use BLEU and human evaluation for evaluation. BLEU (Papineni et al., 2002) has been shown to correlate fairly well with human judgment at a document- and corpus-level, including on the response generation task.2 We also report perplexity as an indicator of model capability. We additionally report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses. The value is scaled by total number of generated tokens to avoid favoring long sentences (shown as distinct-1 and distinct-2). Finally, we present a human evaluation that validates our main findings. 6.3 Baseline 7 Training and Decoding Experimental Results The perplexity"
I17-1061,N16-1014,1,0.777511,"ovements over baseline model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems"
I17-1061,D11-1054,0,0.0941847,"to exploit directly, since, not being in conversational format, it does not mesh easily with existing source-target conversational models. Introduction Conversational engines are key components of intelligent “personal assistants” such as Apple’s Siri and Amazon’s Alexa. These assistants can perform simple tasks, answer questions, provide recommendations, and even engage in chitchats (De Mori et al., 2008; Chen et al., 2015, 2016). The emergence of these agents has been paralleled by burgeoning interest in training natural-sounding dialog systems from conversational exchanges between humans (Ritter et al., 2011; Sordoni et al., 2015; Luan et al., 2014, 2015; Vinyals and Le, 2015). A major challenge for data-driven systems is how to generate output that corresponds to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al."
I17-1061,P16-1094,1,0.933706,"ovements over baseline model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems"
I17-1061,P16-1009,0,0.0899948,"to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a). This is a technique that has seen success in machine translation, where large monolingual data sets have been used to improve translation models (Sennrich et al., 2016). The intuition is that if two tasks are related, then joint training and parameter sharing can enable one task to benefit the other. In our case, this sharing is between two models: On one hand, a standard Sequence-to-Sequence conversational models is trained to predict the current response given the previous context. On the other hand, using the non-conversational data, we introduce an autoencoder multi-task learning strategy that predicts the response given the same sequence, but with the target parameters tied with the general conversational model. Our experiments with 4M conversation trip"
I17-1061,D17-1235,0,0.028605,"line model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems, then, is that an ag"
I17-1061,D16-1230,0,0.0284166,"Missing"
I17-1061,N15-1020,1,0.282437,"since, not being in conversational format, it does not mesh easily with existing source-target conversational models. Introduction Conversational engines are key components of intelligent “personal assistants” such as Apple’s Siri and Amazon’s Alexa. These assistants can perform simple tasks, answer questions, provide recommendations, and even engage in chitchats (De Mori et al., 2008; Chen et al., 2015, 2016). The emergence of these agents has been paralleled by burgeoning interest in training natural-sounding dialog systems from conversational exchanges between humans (Ritter et al., 2011; Sordoni et al., 2015; Luan et al., 2014, 2015; Vinyals and Le, 2015). A major challenge for data-driven systems is how to generate output that corresponds to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a). This is a te"
I17-1061,N15-1092,1,0.125072,"Missing"
I17-1061,D15-1199,0,0.0128467,"Missing"
N04-1035,2003.mtsummit-papers.6,1,0.303093,"tistical MT. We take this approach in our paper. Of course, the broad statistical MT program is aimed at a wider goal than the conventional rule-based program – it seeks to understand and explain human translation data, and automatically learn from it. For this reason, we think it is important to learn from the model/data explainability studies of Fox (2002) and to extend her results. In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models (Charniak et al., 2003) and other potential benefits described by Eisner (2003). Our basic idea is to create transformation rules that condition on larger fragments of tree structure. It is certainly possible to build such rules by hand, and we have done this to formally explain a number of humantranslation examples. But our main interest is in collecting a large set of such rules automatically through corpus analysis. The search for these rules is driven exactly by the problems raised by Fox (2002) – cases of crossing and divergence motivate the algorithms to come up with better explanations of the data and better"
N04-1035,P03-2041,0,0.926149,"road statistical MT program is aimed at a wider goal than the conventional rule-based program – it seeks to understand and explain human translation data, and automatically learn from it. For this reason, we think it is important to learn from the model/data explainability studies of Fox (2002) and to extend her results. In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models (Charniak et al., 2003) and other potential benefits described by Eisner (2003). Our basic idea is to create transformation rules that condition on larger fragments of tree structure. It is certainly possible to build such rules by hand, and we have done this to formally explain a number of humantranslation examples. But our main interest is in collecting a large set of such rules automatically through corpus analysis. The search for these rules is driven exactly by the problems raised by Fox (2002) – cases of crossing and divergence motivate the algorithms to come up with better explanations of the data and better rules. Section 2 of this paper describes algorithms for"
N04-1035,W02-1039,0,0.537809,"Dept. of Computer Science Information Sciences Institute University of California University of Southern California Los Angeles, CA 90024 Marina Del Rey, CA 90292 mhopkins@cs.ucla.edu Abstract We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. 1 Introduction In a very interesting study of syntax in statistical machine translation, Fox (2002) looks at how well proposed translation models fit actual translation data. One such model embodies a restricted, linguistically-motivated notion of word re-ordering. Given an English parse tree, children at any node may be reordered prior to translation. Nodes are processed independently. Previous to Fox (2002), it had been observed that this model would prohibit certain re-orderings in certain language pairs (such as subjectVP(verb-object) into verb-subject-object), but Fox carried out the first careful empirical study, showing that many other common translation patterns fall outside the sco"
N04-1035,P03-1011,0,0.0428361,"e focused on providing a well-founded mathematical theory and efficient, linear algorithms for learning syntactically motivated transformation rules from parallel corpora. One can easily imagine a range of techniques for defining probability distributions over the rules that we learn. We suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems. 5 Conclusion The fundamental assumption underlying much recent work in statistical machine translation (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003) is that local transformations (primarily child-node re-orderings) of one-level parent-children substructures are an adequate model for parallel corpora. Our empirical results suggest that this may be too strong of an assumption. To explain the data in two parallel corpora, one English-French, and one English-Chinese, we are often forced to learn rules involving much larger tree fragments. The theory, algorithms, and transformation rules we learn automatically from data have several interesting aspects. 1. Our rules provide a good, realistic indicator of the complexities inherent in translatio"
N04-1035,N03-1017,1,0.077612,"parsing errors also cause trouble, as a normally well-behaved re-ordering environment can be disrupted by wrong phrase attachment. For other language pairs, the divergence is expected to be greater. In the face of these problems, we may choose among several alternatives. The first is to abandon syntax in statistical machine translation, on the grounds that syntactic models are a poor fit for the data. On this view, adding syntax yields no improvement over robust phrasesubstitution models, and the only question is how much {knight,marcu}@isi.edu does syntax hurt performance. Along this line, (Koehn et al., 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance – the ability to translate nonconstituent phrases (such as “there are”, “note that”, and “according to”) turns out to be critical and pervasive. Another direction is to abandon conventional English syntax and move to more robust grammars that adapt to the parallel training corpus. One approach here is that of Wu (1997), in which word-movement is modeled by rotations at unlabeled, binary-branching nodes. At each sentence pair, the parse adapts to explain the translatio"
N04-1035,P00-1056,0,0.251997,"to explain the data well. 3.2 Data We performed experiments with two corpora, the FBIS English-Chinese Parallel Text and the Hansard FrenchEnglish corpus.We parsed the English sentences with a state-of-the-art statistical parser (Collins, 1999). For the FBIS corpus (representing eight million English words), we automatically generated word-alignments using GIZA++ (Och and Ney, 2003), which we trained on a much larger data set (150 million words). Cases other than one-to-one sentence mappings were eliminated. For the Hansard corpus, we took the human annotation of word alignment described in (Och and Ney, 2000). The corpus contains two kinds of alignments: S (sure) for unambiguous cases and P (possible) for unclear cases, e.g. idiomatic expressions and missing function words (S ⊆ P ). In order to be able to make legitimate comparisons between the two language pairs, we also used GIZA++ to obtain machine-generated word alignments for Hansard: we trained it with the 500 sentences and additional data representing 13.7 million English words (taken from the Hansard and European parliament corpora). 3.3 Results From a theoretical point of view, we have shown that our model can fully explain the transforma"
N04-1035,J03-1002,0,0.0261537,"study with that of Fox (2002). The additional language pair provides a good means of evaluating how our transformation rule extraction method scales to more problematic language pairs for which child-reordering models are shown not to explain the data well. 3.2 Data We performed experiments with two corpora, the FBIS English-Chinese Parallel Text and the Hansard FrenchEnglish corpus.We parsed the English sentences with a state-of-the-art statistical parser (Collins, 1999). For the FBIS corpus (representing eight million English words), we automatically generated word-alignments using GIZA++ (Och and Ney, 2003), which we trained on a much larger data set (150 million words). Cases other than one-to-one sentence mappings were eliminated. For the Hansard corpus, we took the human annotation of word alignment described in (Och and Ney, 2000). The corpus contains two kinds of alignments: S (sure) for unambiguous cases and P (possible) for unclear cases, e.g. idiomatic expressions and missing function words (S ⊆ P ). In order to be able to make legitimate comparisons between the two language pairs, we also used GIZA++ to obtain machine-generated word alignments for Hansard: we trained it with the 500 sen"
N04-1035,J97-3002,0,0.857464,"er robust phrasesubstitution models, and the only question is how much {knight,marcu}@isi.edu does syntax hurt performance. Along this line, (Koehn et al., 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance – the ability to translate nonconstituent phrases (such as “there are”, “note that”, and “according to”) turns out to be critical and pervasive. Another direction is to abandon conventional English syntax and move to more robust grammars that adapt to the parallel training corpus. One approach here is that of Wu (1997), in which word-movement is modeled by rotations at unlabeled, binary-branching nodes. At each sentence pair, the parse adapts to explain the translation pattern. If the same unambiguous English sentence were to appear twice in the corpus, with different Chinese translations, then it could have different learned parses. A third direction is to maintain English syntax and investigate alternate transformation models. After all, many conventional translation systems are indeed based on syntactic transformations far more expressive than what has been proposed in syntax-based statistical MT. We tak"
N04-1035,P01-1067,1,0.719731,"ossing due to a modal. In this paper, we focused on providing a well-founded mathematical theory and efficient, linear algorithms for learning syntactically motivated transformation rules from parallel corpora. One can easily imagine a range of techniques for defining probability distributions over the rules that we learn. We suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems. 5 Conclusion The fundamental assumption underlying much recent work in statistical machine translation (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003) is that local transformations (primarily child-node re-orderings) of one-level parent-children substructures are an adequate model for parallel corpora. Our empirical results suggest that this may be too strong of an assumption. To explain the data in two parallel corpora, one English-French, and one English-Chinese, we are often forced to learn rules involving much larger tree fragments. The theory, algorithms, and transformation rules we learn automatically from data have several interesting aspects. 1. Our rules provide a good, realistic indicator of the comple"
N04-1035,J03-4003,0,\N,Missing
N07-1023,P05-1022,0,0.0126392,"ction. We feel it is important to use a relatively large development corpus, since we will provide in Section 5 detailed analyses of model selection on the development set (e.g., by evaluating different Markov structures), and we want these findings to be as significant as possible. Finally, we used the same test data as K&M for human evaluation purposes (32 sentence pairs). 4 Tree Alignment and Synchronous Grammar Inference We now describe methods to train SCFG models from sentence pairs. Given a tree pair (f , c), whose respective parses (πf , πc ) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. Clearly, the two trees may sometimes be structurally quite different (e.g., a given PP may attach to an NP in πf , while attaching to VP in πc ), and it is not always possible to build an SCFG derivation given the constraints in (πf , πc ). The approach taken by K&M is to analyze both trees and count an SCFG rule whenever two nodes are “deemed to correspond”, i.e., roots are the same, and αc is a sub-sequence of αf . This leads"
N07-1023,W03-0501,0,0.0520647,"Missing"
N07-1023,A00-1043,0,0.891657,"Missing"
N07-1023,J98-4004,0,0.0282372,"from DT the . PP JJ NN year-ago period IN because of NP VBG NN NN slowing microchip demand Figure 1: Penn Treebank tree with adjuncts in italic. occurrences are sparsely seen (e.g., “fell”-“from”). At a lower level, lexicalization is clearly desirable for pre-terminals. Indeed, current SCFG models such as K&M have no direct way of preventing highly improbable single word removals, such as deletions of adverbs “never” or “nowhere”, which may turn a negative statement into a positive one.4 A second type of annotation that can be added to syntactic categories is the so-called parent annotation (Johnson, 1998), which was effectively used in syntactic parsing to break unreasonable context-free assumptions. For instance, a PP with a VP parent is marked as PPˆVP. It is reasonable to assume that, e.g., that constituents deep inside a PP have more chances to be removed than otherwise expected, and one may seek to increase the amount of vertical context that is available for conditioning each constituent deletion. To achieve the above desiderata for better SCFG probability estimates—i.e., reduce the amount of sister annotation within each SCFG production, by conditioning deletions on a context smaller th"
N07-1023,P03-1054,0,0.205977,"data, which greatly benefited the lexical probabilities incorporated into our Markovized SCFGs. Our work provides three main contributions: 180 Proceedings of NAACL HLT 2007, pages 180–187, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics (1) Our lexicalized head-driven Markovization yields more robust probability estimates, and our compressions outperform (Knight and Marcu, 2000) according to automatic and human evaluation. (2) We provide a comprehensive analysis of the impact of different Markov orders for sentence compression, similarly to a study done for PCFGs (Klein and Manning, 2003). (3) We provide a framework for exploiting document-abstract sentence pairs that are not purely compressive, and augment the available training resources for syntax-directed sentence compression systems. 2 Synchronous Grammars for Sentence Compression One successful syntax-driven approach (Knight and Marcu, 2000, henceforth K&M) relies on synchronous context-free grammars (SCFG) (Lewis and Stearns, 1968; Aho and Ullman, 1969). SCFGs can be informally defined as context-free grammars (CFGs) whose productions have two right-hand side strings instead of one, namely source and target right-hand s"
N07-1023,E06-1038,0,0.626032,"Missing"
N07-1023,P05-1036,0,0.571044,"ion to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work. 1 Introduction Sentence compression addresses the problem of removing words or phrases that are not necessary in the generated output of, for instance, summarization and question answering systems. Given the need to ensure grammatical sentences, a number of researchers have used syntax-directed approaches that perform transformations on the output of syntactic parsers (Jing, 2000; Dorr et al., 2003). Some of them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) ∗ This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA. In this paper, we present a head-driven Markovization of SCFG compression rules, an"
N07-1023,J93-2004,0,\N,Missing
N07-1023,J03-4003,0,\N,Missing
N10-1129,P06-1067,0,0.47311,"mpare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexicalized, thus penalizing all re-orderings equally (Khalilov et al., 2009). When extended to phrases as in Equation (1), linear distortion is also agnostic to internal phrase alignments. To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. Maximum likelihood estimates for inbound, outbound, and pairwise transitions are computed from automatic word alignments. But no estimate of future cost is included, and their model cannot easily accommodate features defined over the entire translation sequence. As for experimental results, they use a distortion limit that is half of what we report, and compare against a baseline that lacks a distortion model entirely. Neither their model nor ours requires generation of lattices prior to search (Zhang et al., 2007; Niehues and Kolss, 2009)."
N10-1129,W09-0437,0,0.0109281,"SBJ            ! Followers of all of the Christian and Islamic sects engaged in waiting for them Figure 1: The oracle translation for this Arabic VOS sentence would be pruned during search using typical distortion parameters. The Arabic phrases read right-to-left, but we have ordered the sentence from left-to-right in order to clearly illustrate the re-ordering problem. ders in which significant re-ordering is required, the distortion limit can eliminate the oracle, or “best,” translation prior to search, placing an artificial limit on translation performance (Auli et al., 2009). To illustrate this problem, consider the ArabicEnglish example in Figure 1. Assuming that the English translation is constructed left-to-right, the verb CAJ shaaraka must be translated after the noun phrase (NP) subject. If P phrases are used to translate the Arabic source s to the English target t, then the (unsigned) linear distortion is given by Introduction D(s, t) = p1f irst + It is well-known that translation performance in Moses-style (Koehn et al., 2007) machine translation (MT) systems deteriorates when high distortion is allowed. The linear distortion cost model used in these syst"
N10-1129,P08-1087,0,0.00964604,"tion is required. By not modeling source side features like agreement— which, in Arabic, appears between both verb and  step k 0 1 2 3 4     Fk 3 5 7 0 0 ∆cost 3 2 2 −7 0 dlimit-4 D(s, t) 1 0 0 4 3 8 D(s, t) + ∆cost 4 2 2 −3 3 8 Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the first skipped word. Future cost estimation penalizes the two monotone steps, yet total distortion cost remains unchanged. subject, and adjective and noun—baseline phrasebased systems rely on the language model to specify an appropriate target word order (Avramidis and Koehn, 2008). Returning to Figure 1, we could have an alternate hypothesis They waited for the followers of the Christian and Islamic sects, which is acceptable English and has low distortion, but is semantically inconsistent with the Arabic. 3 The Cost Model In this section we describe the new distortion cost model, which has four independent components. 3.1 Future Cost Estimation Despite its lack of sophistication, linear distortion is a surprisingly effective baseline cost model for phrase-based MT systems. It can be computed in constant time, gives non-decreasing values that are good for search, and d"
N10-1129,N10-2003,1,0.167101,"Missing"
N10-1129,J07-2003,0,0.0151208,"probabilities for the outermost classes. Noise in the alignments along with the few cases of longdistance movement are penalized heavily. For Arabic, this property works in our favor as we do not want extreme movement (as we might with Chinese or German). But C OUNTS applies a uniform penalty for all movement that exceeds the outermost class boundaries, making it more prone to search errors than even linear distortion despite its favorable performance when tested in isolation. Finally, we note that previous attempts to improve re-ordering during search (particularly long-distance re-ordering (Chiang, 2007)) have delivered remarkable gains for languages like Chinese, but improvements for Arabic have been less exceptional. By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). 6 Prior Work There is an expansive literature on re-ordering in statistical MT. We first review the development of re-ordering constraints, then describe previous cost models for those constraints in beam search decoders. Because we allow re-o"
N10-1129,P05-1066,0,0.0967593,"ituents must often be identified and precisely re-ordered. The VOS configuration is especially challenging for Arabic-English MT. It usually appears when the direct object is short—e.g., pronominal—and the subject is long. For example, translation of the VOS sentence in Figure 1 requires both a high distortion limit to accommodate the subject movement and tight restrictions on the movement of the PP. The particularity of these requirements in Arabic and other languages, and the difficulty of modeling them in phrase-based systems, has inspired significant work in source language preprocessing (Collins et al., 2005; Habash and Sadat, 2006; Habash, 2007). Finally, we observe that target language models cannot always select appropriate translations when basic word order transformation is required. By not modeling source side features like agreement— which, in Arabic, appears between both verb and  step k 0 1 2 3 4     Fk 3 5 7 0 0 ∆cost 3 2 2 −7 0 dlimit-4 D(s, t) 1 0 0 4 3 8 D(s, t) + ∆cost 4 2 2 −3 3 8 Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the first skipped word. Future cost estimation penalizes the two monotone steps, yet total d"
N10-1129,D08-1089,1,0.917659,"score. When we say linear distortion, we refer to the “simple distortion model” of Koehn et al. (2003) that is shown in Equation (1) and is converted to a cost by multiplying by −1. When extended to phrases,  868 the key property of this model is that monotone decoding gives the least costly translation path. Reorderings internal to extracted phrases are not penalized. In practice, we commonly see n-best lists of hypotheses with linear distortion costs equal to zero. More sophisticated local phrase re-ordering models have been proposed (Tillmann, 2004; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008), but these are typically used in addition to linear distortion. 2.2 Arabic Linguistic Essentials In this paper we use Arabic-English as a case study since we possess a strong experimental baseline. But we expect that the technique presented could be even more effective for high distortion language pairs such as Chinese-English and Hindi-English. Since the analysis that follows is framed in terms of Arabic, we point out several linguistic features that motivate our approach. From the perspective of the three criteria used to specify basic word order typology (Greenberg, 1966), Arabic is somewh"
N10-1129,N04-1039,0,0.0490678,"mit (Figure 5). As expected, future cost estimation alone does not increase performance at the lower distortion limit. We also observe that the effect of conditioning on evidence is significant: the C OUNTS model is categorically worse than all other models. To understand why, we randomly sampled 500 sentences from the excluded UN data and computed the log-likelihoods of the alignments according to the different models.7 In this test, C OUNTS is clearly better with a score of 7 We approximated linear distortion using a Laplacian distribution with estimated parameters µ ˆ = 0.51 and ˆb = 1.76 (Goodman, 2004). 873 −23388 versus, for example, the inbound model at −38244. The explanation is due in part to optimization. The two discriminative models often give very low probabilities for the outermost classes. Noise in the alignments along with the few cases of longdistance movement are penalized heavily. For Arabic, this property works in our favor as we do not want extreme movement (as we might with Chinese or German). But C OUNTS applies a uniform penalty for all movement that exceeds the outermost class boundaries, making it more prone to search errors than even linear distortion despite its favor"
N10-1129,N06-2013,0,0.00623196,"identified and precisely re-ordered. The VOS configuration is especially challenging for Arabic-English MT. It usually appears when the direct object is short—e.g., pronominal—and the subject is long. For example, translation of the VOS sentence in Figure 1 requires both a high distortion limit to accommodate the subject movement and tight restrictions on the movement of the PP. The particularity of these requirements in Arabic and other languages, and the difficulty of modeling them in phrase-based systems, has inspired significant work in source language preprocessing (Collins et al., 2005; Habash and Sadat, 2006; Habash, 2007). Finally, we observe that target language models cannot always select appropriate translations when basic word order transformation is required. By not modeling source side features like agreement— which, in Arabic, appears between both verb and  step k 0 1 2 3 4     Fk 3 5 7 0 0 ∆cost 3 2 2 −7 0 dlimit-4 D(s, t) 1 0 0 4 3 8 D(s, t) + ∆cost 4 2 2 −3 3 8 Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the first skipped word. Future cost estimation penalizes the two monotone steps, yet total distortion cost remains u"
N10-1129,2007.mtsummit-papers.29,0,0.0935294,"re-ordered. The VOS configuration is especially challenging for Arabic-English MT. It usually appears when the direct object is short—e.g., pronominal—and the subject is long. For example, translation of the VOS sentence in Figure 1 requires both a high distortion limit to accommodate the subject movement and tight restrictions on the movement of the PP. The particularity of these requirements in Arabic and other languages, and the difficulty of modeling them in phrase-based systems, has inspired significant work in source language preprocessing (Collins et al., 2005; Habash and Sadat, 2006; Habash, 2007). Finally, we observe that target language models cannot always select appropriate translations when basic word order transformation is required. By not modeling source side features like agreement— which, in Arabic, appears between both verb and  step k 0 1 2 3 4     Fk 3 5 7 0 0 ∆cost 3 2 2 −7 0 dlimit-4 D(s, t) 1 0 0 4 3 8 D(s, t) + ∆cost 4 2 2 −3 3 8 Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the first skipped word. Future cost estimation penalizes the two monotone steps, yet total distortion cost remains unchanged. subje"
N10-1129,W05-0831,0,0.0236372,"were first introduced by Berger et al. (1996) in the context of the IBM translation models. The IBM constraints treat the source word sequence as a coverage set C that is processed sequentially. A source token is “covered” when it is aligned with a new target token. For a fixed value of k, we may leave up to k − 1 positions uncovered and return to them later. We can alter the constraint slightly such that for the first uncovered position u ∈ / C we can cover position j when j−u<k j∈ /C which is the definition of the distortion limit used in Moses. Variations of the IBM constraints also exist (Kanthak et al., 2005), as do entirely different regimes like the hierarchical ITG constraints, which represent the source as a sequence of blocks that can be iteratively merged and inverted (Wu, 1996). Zens and Ney (2003) exhaustively compare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexi"
N10-1129,W09-2310,0,0.011333,"hical ITG constraints, which represent the source as a sequence of blocks that can be iteratively merged and inverted (Wu, 1996). Zens and Ney (2003) exhaustively compare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexicalized, thus penalizing all re-orderings equally (Khalilov et al., 2009). When extended to phrases as in Equation (1), linear distortion is also agnostic to internal phrase alignments. To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. Maximum likelihood estimates for inbound, outbound, and pairwise transitions are computed from automatic word alignments. But no estimate of future cost is included, and their model cannot easily accommodate features defined over the entire translation sequence. As for experimental results, they use a distortion limit that is half of what we report, and compare against a"
N10-1129,N03-1017,0,0.00905336,"eterization. 2 2.1 Background Search in Phrase-based MT  Given a J token source input string f = fiJ , we  I seek the most probable I token translation e = ei . The Moses phrase-baseddecoder models the posterior probability pλ eI1 |f1J directly according to a log-linear model (Och and Ney, 2004), which gives the decision rule ( M ) X  I J eˆ = arg max λm hm e1 , f1 I,eI1 where hm eI1 , f1J m=1 are M arbitrary feature functions over sentence pairs, and λm are feature weights set using a discriminative training method like MERT (Och, 2003). This search is made tractable by the use of beams (Koehn et al., 2003). Hypotheses are pruned from the beams according the sum of the current model score and a future cost estimate for the uncovered source words. Since the number of reordering possibilities for those words is very large— in theory it is exponential—an inadmissible heuristic is typically used to estimate future cost. The baseline distortion cost model is a weighted feature in this framework and affects beam pruning only through the current model score. When we say linear distortion, we refer to the “simple distortion model” of Koehn et al. (2003) that is shown in Equation (1) and is converted to"
N10-1129,P07-2045,0,0.0321216,"ion limit can eliminate the oracle, or “best,” translation prior to search, placing an artificial limit on translation performance (Auli et al., 2009). To illustrate this problem, consider the ArabicEnglish example in Figure 1. Assuming that the English translation is constructed left-to-right, the verb CAJ shaaraka must be translated after the noun phrase (NP) subject. If P phrases are used to translate the Arabic source s to the English target t, then the (unsigned) linear distortion is given by Introduction D(s, t) = p1f irst + It is well-known that translation performance in Moses-style (Koehn et al., 2007) machine translation (MT) systems deteriorates when high distortion is allowed. The linear distortion cost model used in these systems is partly at fault. It includes no estimate of future distortion cost, thereby increasing the risk of search errors. Linear distortion also penalizes all re-orderings equally, even when appropriate re-orderings are performed. Because linear distortion, which is a soft constraint, does not effectively constrain search, a distortion limit is imposed on the translation model. But hard constraints are ultimately undesirable since they prune the search space. For la"
N10-1129,W04-3250,0,0.0170905,"e Arabic source is written right-to-left. bilities, and the alignment penalty. The main objective of this paper is to improve performance at very high distortion limits. Table 4 shows performance at a distortion limit of 15. To the set of baselines we add L EX, which is the lexicalized re-ordering model of Galley and Manning (2008). This model was shown to outperform other lexicalized re-ordering models in common use. Statistical significance was computed with the approximate randomization test of Riezler and Maxwell (2005), which is less sensitive to Type I errors than bootstrap re-sampling (Koehn, 2004). 5 Discussion The new distortion cost model allows us to triple the distortion limit while maintaining a statistically significant improvement over the M OSES L INEAR baseline at the lower distortion limit for three of the four test sets. More importantly, we can raise the distortion limit in the D ISCRIM +F UTURE configuration at minimal cost: a statistically insignificant −0.2 BLEU performance decrease on average. We also see a considerable improvement over both the M OSES L INEAR and L EX baselines at the high distortion limit (Figure 5). As expected, future cost estimation alone does not"
N10-1129,N06-1014,0,0.367963,"ighted probabilities), word penalty, phrase penalty, linear distortion, and language model score. We disable baseline linear distortion when evaluating the other distortion cost models. To tune parameters, we run MERT with the Downhill Simplex algorithm on the MT04 dataset. For all models, we use 20 random starting points and generate 300-best lists. We use the NIST MT09 constrained track training data, but remove the UN and comparable data.6 The reduced training bitext has 181k aligned sentences with 6.20M English and 5.73M Arabic tokens. We create word alignments using the Berkeley Aligner (Liang et al., 2006) and take the intersection of the alignments in both directions. Phrase pairs with a maximum target or source length of 7 tokens are extracted using the method of Och and Ney (2004). We build a 5-gram language model from the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40), in addition to all of the target side training data permissible in the NIST MT09 constrained competition. We manually remove Giga6 Removal of the UN data does not affect the baseline at a distortion limit of 5, and lowers the higher distortion baseline by −1.40 BLEU. The NIST MT09 data is available at http://www."
N10-1129,P08-1114,0,0.0476741,"boundaries, making it more prone to search errors than even linear distortion despite its favorable performance when tested in isolation. Finally, we note that previous attempts to improve re-ordering during search (particularly long-distance re-ordering (Chiang, 2007)) have delivered remarkable gains for languages like Chinese, but improvements for Arabic have been less exceptional. By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). 6 Prior Work There is an expansive literature on re-ordering in statistical MT. We first review the development of re-ordering constraints, then describe previous cost models for those constraints in beam search decoders. Because we allow re-ordering during search, we omit discussion of the many different methods for preprocessing the source input prior to monotonic translation. Likewise, we do not recite prior work in re-ranking translations. Re-ordering constraints were first introduced by Berger et al. (1996) in the context of the IBM translation models. The IBM constraints treat the sour"
N10-1129,2007.mtsummit-papers.43,0,0.174814,"el.2 By definition, the model has a least cost translation path: monotone. Therefore, we can add to the baseline calculation D(s, t) the cost of skipping back to the first uncovered source word and then translating the remaining positions monotonically. It can be verified by induction on |C |that this is an admissible heuristic. Formally, let j represent the first uncovered index in the source coverage set C. Let C j represent the subset of C starting from position j. Finally, let j 0 represent the leftmost position in phrase p applied at translation step k. Then the future cost estimate Fk 2 Moore and Quirk (2007) propose an alternate future cost formulation. However, their model seems prone to the same deterioration in performance shown in Table 1. They observed decreased translation quality above a distortion limit of 5. is Fk = |C j |+ (j 0 BASELINE F UTURE C OST + |p |+ 1 − j) if > j 0 otherwise j0 For k > 0, we add the difference between the current future cost estimate and the previous cost estimate ∆cost = Fk − Fk−1 to the linear penalty D(s, t).3 Table 2 shows that, as expected, the difference between the baseline and augmented models is statistically insignificant at a low distortion limit. Ho"
N10-1129,W09-0435,0,0.0105672,"aizan and Papineni (2006) proposed a lexicalized, generative distortion model. Maximum likelihood estimates for inbound, outbound, and pairwise transitions are computed from automatic word alignments. But no estimate of future cost is included, and their model cannot easily accommodate features defined over the entire translation sequence. As for experimental results, they use a distortion limit that is half of what we report, and compare against a baseline that lacks a distortion model entirely. Neither their model nor ours requires generation of lattices prior to search (Zhang et al., 2007; Niehues and Kolss, 2009). Lexicalized re-ordering models are the other significant approach to re-ordering. These models make local predictions about the next phrase to be translated during decoding, typically assigning costs to one of three categories: monotone, swap, or discontinuous. Both generative (Tillmann, 2004; Och and Ney, 2004; Koehn et al., 2007) and discriminative training (Tillmann and Zhang, 2005; Zens and 874 Ney, 2006; Liang et al., 2006) algorithms have been proposed. Recently, Galley and Manning (2008) introduced a hierarchical model capable of analyzing alignments beyond adjacent phrases. Our discr"
N10-1129,J04-4002,0,0.873905,"s. Together these two extensions allow us to triple the distortion limit in our NIST MT09 Arabic-English system while maintaining a statistically significant improvement over the low distortion baseline. At the high distortion limit, we also show a +2.32 BLEU average gain over Moses with an equivalent distortion parameterization. 2 2.1 Background Search in Phrase-based MT  Given a J token source input string f = fiJ , we  I seek the most probable I token translation e = ei . The Moses phrase-baseddecoder models the posterior probability pλ eI1 |f1J directly according to a log-linear model (Och and Ney, 2004), which gives the decision rule ( M ) X  I J eˆ = arg max λm hm e1 , f1 I,eI1 where hm eI1 , f1J m=1 are M arbitrary feature functions over sentence pairs, and λm are feature weights set using a discriminative training method like MERT (Och, 2003). This search is made tractable by the use of beams (Koehn et al., 2003). Hypotheses are pruned from the beams according the sum of the current model score and a future cost estimate for the uncovered source words. Since the number of reordering possibilities for those words is very large— in theory it is exponential—an inadmissible heuristic is typi"
N10-1129,P03-1021,0,0.0282777,"LEU average gain over Moses with an equivalent distortion parameterization. 2 2.1 Background Search in Phrase-based MT  Given a J token source input string f = fiJ , we  I seek the most probable I token translation e = ei . The Moses phrase-baseddecoder models the posterior probability pλ eI1 |f1J directly according to a log-linear model (Och and Ney, 2004), which gives the decision rule ( M ) X  I J eˆ = arg max λm hm e1 , f1 I,eI1 where hm eI1 , f1J m=1 are M arbitrary feature functions over sentence pairs, and λm are feature weights set using a discriminative training method like MERT (Och, 2003). This search is made tractable by the use of beams (Koehn et al., 2003). Hypotheses are pruned from the beams according the sum of the current model score and a future cost estimate for the uncovered source words. Since the number of reordering possibilities for those words is very large— in theory it is exponential—an inadmissible heuristic is typically used to estimate future cost. The baseline distortion cost model is a weighted feature in this framework and affects beam pruning only through the current model score. When we say linear distortion, we refer to the “simple distortion model” o"
N10-1129,2001.mtsummit-papers.68,0,0.0335555,"iods that overlapped with the development and test sets. The language model is smoothed with the modified Kneser-Ney algorithm, retaining only trigrams, 4grams, and 5-grams that occurred two, three, and three times, respectively, in the training data. We remove from the test sets source tokens not present in the phrase tables. For the discriminative distortion models, we tag the pre-processed input using the log-linear POS tagger of Toutanova et al. (2003). After decoding, we strip any punctuation that appears at the beginning of a translation. 4.2 Results In Table 3 we report uncased BLEU-4 (Papineni et al., 2001) scores at the distortion limit (5) of our most competitive baseline Arabic-English system. M OSES L INEAR uses the linear distortion model present in Moses. C OUNTS is a separate baseline with a discrete cost model that uses unlexicalized maximum likelihood estimates for the same classes present in the discriminative model. To show the effect of the components in our combined distortion model, we give separate results for linear distortion with future cost estimation (F UTURE) and for the combined discriminative distortion model (D IS CRIM +F UTURE) with all four features: linear distortion w"
N10-1129,W05-0908,0,0.164027,"e translation (M OSES L INEAR-d15). D ISCRIM +F UTURE (dlimit=15) correctly guides the search. The Arabic source is written right-to-left. bilities, and the alignment penalty. The main objective of this paper is to improve performance at very high distortion limits. Table 4 shows performance at a distortion limit of 15. To the set of baselines we add L EX, which is the lexicalized re-ordering model of Galley and Manning (2008). This model was shown to outperform other lexicalized re-ordering models in common use. Statistical significance was computed with the approximate randomization test of Riezler and Maxwell (2005), which is less sensitive to Type I errors than bootstrap re-sampling (Koehn, 2004). 5 Discussion The new distortion cost model allows us to triple the distortion limit while maintaining a statistically significant improvement over the M OSES L INEAR baseline at the lower distortion limit for three of the four test sets. More importantly, we can raise the distortion limit in the D ISCRIM +F UTURE configuration at minimal cost: a statistically insignificant −0.2 BLEU performance decrease on average. We also see a considerable improvement over both the M OSES L INEAR and L EX baselines at the hi"
N10-1129,P05-1069,0,0.0201684,"ion limit that is half of what we report, and compare against a baseline that lacks a distortion model entirely. Neither their model nor ours requires generation of lattices prior to search (Zhang et al., 2007; Niehues and Kolss, 2009). Lexicalized re-ordering models are the other significant approach to re-ordering. These models make local predictions about the next phrase to be translated during decoding, typically assigning costs to one of three categories: monotone, swap, or discontinuous. Both generative (Tillmann, 2004; Och and Ney, 2004; Koehn et al., 2007) and discriminative training (Tillmann and Zhang, 2005; Zens and 874 Ney, 2006; Liang et al., 2006) algorithms have been proposed. Recently, Galley and Manning (2008) introduced a hierarchical model capable of analyzing alignments beyond adjacent phrases. Our discriminative distortion framework is not designed as a replacement for lexicalized re-ordering models, but as a substitute for linear distortion. Finally, we comment on differences between our Arabic-English results and the well-known high distortion system of Zollmann et al. (2008), who find optimal baseline performance at a distortion limit of 9. First, they use approximately two orders"
N10-1129,N04-4026,0,0.721121,"and affects beam pruning only through the current model score. When we say linear distortion, we refer to the “simple distortion model” of Koehn et al. (2003) that is shown in Equation (1) and is converted to a cost by multiplying by −1. When extended to phrases,  868 the key property of this model is that monotone decoding gives the least costly translation path. Reorderings internal to extracted phrases are not penalized. In practice, we commonly see n-best lists of hypotheses with linear distortion costs equal to zero. More sophisticated local phrase re-ordering models have been proposed (Tillmann, 2004; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008), but these are typically used in addition to linear distortion. 2.2 Arabic Linguistic Essentials In this paper we use Arabic-English as a case study since we possess a strong experimental baseline. But we expect that the technique presented could be even more effective for high distortion language pairs such as Chinese-English and Hindi-English. Since the analysis that follows is framed in terms of Arabic, we point out several linguistic features that motivate our approach. From the perspective of the three criteria used to sp"
N10-1129,N03-1033,1,0.0224686,"tortion baseline by −1.40 BLEU. The NIST MT09 data is available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/. 872 word documents that were released during periods that overlapped with the development and test sets. The language model is smoothed with the modified Kneser-Ney algorithm, retaining only trigrams, 4grams, and 5-grams that occurred two, three, and three times, respectively, in the training data. We remove from the test sets source tokens not present in the phrase tables. For the discriminative distortion models, we tag the pre-processed input using the log-linear POS tagger of Toutanova et al. (2003). After decoding, we strip any punctuation that appears at the beginning of a translation. 4.2 Results In Table 3 we report uncased BLEU-4 (Papineni et al., 2001) scores at the distortion limit (5) of our most competitive baseline Arabic-English system. M OSES L INEAR uses the linear distortion model present in Moses. C OUNTS is a separate baseline with a discrete cost model that uses unlexicalized maximum likelihood estimates for the same classes present in the discriminative model. To show the effect of the components in our combined distortion model, we give separate results for linear dist"
N10-1129,P96-1021,0,0.116875,"ly. A source token is “covered” when it is aligned with a new target token. For a fixed value of k, we may leave up to k − 1 positions uncovered and return to them later. We can alter the constraint slightly such that for the first uncovered position u ∈ / C we can cover position j when j−u<k j∈ /C which is the definition of the distortion limit used in Moses. Variations of the IBM constraints also exist (Kanthak et al., 2005), as do entirely different regimes like the hierarchical ITG constraints, which represent the source as a sequence of blocks that can be iteratively merged and inverted (Wu, 1996). Zens and Ney (2003) exhaustively compare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexicalized, thus penalizing all re-orderings equally (Khalilov et al., 2009). When extended to phrases as in Equation (1), linear distortion is also agnostic to internal phrase align"
N10-1129,P03-1019,0,0.0627552,"e token is “covered” when it is aligned with a new target token. For a fixed value of k, we may leave up to k − 1 positions uncovered and return to them later. We can alter the constraint slightly such that for the first uncovered position u ∈ / C we can cover position j when j−u<k j∈ /C which is the definition of the distortion limit used in Moses. Variations of the IBM constraints also exist (Kanthak et al., 2005), as do entirely different regimes like the hierarchical ITG constraints, which represent the source as a sequence of blocks that can be iteratively merged and inverted (Wu, 1996). Zens and Ney (2003) exhaustively compare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexicalized, thus penalizing all re-orderings equally (Khalilov et al., 2009). When extended to phrases as in Equation (1), linear distortion is also agnostic to internal phrase alignments. To remedy thes"
N10-1129,W06-3108,0,0.620891,"pruning only through the current model score. When we say linear distortion, we refer to the “simple distortion model” of Koehn et al. (2003) that is shown in Equation (1) and is converted to a cost by multiplying by −1. When extended to phrases,  868 the key property of this model is that monotone decoding gives the least costly translation path. Reorderings internal to extracted phrases are not penalized. In practice, we commonly see n-best lists of hypotheses with linear distortion costs equal to zero. More sophisticated local phrase re-ordering models have been proposed (Tillmann, 2004; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008), but these are typically used in addition to linear distortion. 2.2 Arabic Linguistic Essentials In this paper we use Arabic-English as a case study since we possess a strong experimental baseline. But we expect that the technique presented could be even more effective for high distortion language pairs such as Chinese-English and Hindi-English. Since the analysis that follows is framed in terms of Arabic, we point out several linguistic features that motivate our approach. From the perspective of the three criteria used to specify basic word ord"
N10-1129,I08-1068,0,0.118544,"Missing"
N10-1129,W07-0401,0,0.0358094,"deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. Maximum likelihood estimates for inbound, outbound, and pairwise transitions are computed from automatic word alignments. But no estimate of future cost is included, and their model cannot easily accommodate features defined over the entire translation sequence. As for experimental results, they use a distortion limit that is half of what we report, and compare against a baseline that lacks a distortion model entirely. Neither their model nor ours requires generation of lattices prior to search (Zhang et al., 2007; Niehues and Kolss, 2009). Lexicalized re-ordering models are the other significant approach to re-ordering. These models make local predictions about the next phrase to be translated during decoding, typically assigning costs to one of three categories: monotone, swap, or discontinuous. Both generative (Tillmann, 2004; Och and Ney, 2004; Koehn et al., 2007) and discriminative training (Tillmann and Zhang, 2005; Zens and 874 Ney, 2006; Liang et al., 2006) algorithms have been proposed. Recently, Galley and Manning (2008) introduced a hierarchical model capable of analyzing alignments beyond a"
N10-1129,C08-1144,0,0.0566492,"uous. Both generative (Tillmann, 2004; Och and Ney, 2004; Koehn et al., 2007) and discriminative training (Tillmann and Zhang, 2005; Zens and 874 Ney, 2006; Liang et al., 2006) algorithms have been proposed. Recently, Galley and Manning (2008) introduced a hierarchical model capable of analyzing alignments beyond adjacent phrases. Our discriminative distortion framework is not designed as a replacement for lexicalized re-ordering models, but as a substitute for linear distortion. Finally, we comment on differences between our Arabic-English results and the well-known high distortion system of Zollmann et al. (2008), who find optimal baseline performance at a distortion limit of 9. First, they use approximately two orders of magnitude more training data, which allows them to extract much longer phrases (12 tokens v. our maximum of 7). In this setting, many Arabic-English reorderings can be captured in the phrase table. Second, their “Full” system uses three language models each trained with significantly more data than our single model. Finally, although they use a lexicalized re-ordering model, no details are given about the baseline distortion cost model. 7 Conclusion We have presented a discriminative"
N10-1129,P02-1040,0,\N,Missing
N10-1129,D08-1076,0,\N,Missing
N10-1140,P05-1032,0,0.0481617,"Missing"
N10-1140,N10-2003,1,0.273755,"ating to not . . . anymore in Fig. 2). Beam search algorithm. 1 create initial hypothesis H∅ ; add it to S0g 2 for j = 0 to J 3 if j > 0 then 4 for n = 1 to N c 5 for each Hnew in consolidate(Hjn ) 6 add Hnew to Sjg 7 if j < J then 8 for n = 1 to N g 9 Hold := Hjn 10 u := first uncovered source word of Hold 11 for m = u to u + distortionLimit 12 for each (sk , tk ) in translation options(m) 13 if source sk does not overlap Hold then 14 Hnew :=combine(Hold , sk , tk ) c 15 add Hnew to Sj+l , where l = |sk | g 16 return arg max(SJ ) 3 Decoder The core engine of our phrase-based system, Phrasal (Cer et al., 2010), is a multi-stack decoder similar to Moses (Koehn, 2004), which we extended to support variable-size gaps in the source and the target. In Moses, partial translation hypotheses are arranged into different stacks according to the total number of input words they cover. At every translation step, stacks are pruned using partial translation cost and a lower bound on the estimated future cost. Pruning is implemented using both threshold and histogram pruning, and Moses allows for hypothesis recombination between hypotheses that are indistinguishable according to the underlying models. The key dif"
N10-1140,N09-1025,0,0.0185371,"Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that this turns out to also be the case with the hierarchical phrases of Joshua. In future work, we plan to extend the parameterization of phrasebased lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (Marton and Resnik, 2008; Chiang et al., 2009). The discontinuous phrase-based MT system described in this work is part of Phrasal, an opensource phrase-based system available for download at http://nlp.stanford.edu/software/phrasal . Acknowledgements The authors thank three anonymous reviewers, Dan Jurafsky, Spence Green, Steven Bethard, Daniel Cer, Chris Callison-Burch, and Pi-Chuan Chang for their helpful comments. This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. Refe"
N10-1140,J07-2003,0,0.200458,"y find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (i) source: ai bj ck dl target: bm dn ap ct (ii) ai bj ak bl am bn (iii) ai bj ak bl am bn Figure 1:"
N10-1140,2009.eamt-1.10,0,0.177847,"evious attempt to incorporate gaps is de973 scribed in (Simard et al., 2005). Simard et al. presents an extension to Moses that allows gaps in both source and target phrases, though each of their gap symbols must span exactly one word. This fact makes decoding simpler, since the position of all target words in a translation hypothesis is known as soon as the hypothesis is laid down, but fixed-size discontinuous phrases are less general and increase sparsity. By comparison, our gaps may span any number of words, so we have an increased ability to flexibly match the input sentence effectively. (Crego and Yvon, 2009) also handles gaps, though this work is applicable to an n-gram-based SMT framework (Mari`oo et al., 2006), which is fairly different from the phrase-based framework. 8 Conclusions In this paper, we presented a generalization of conventional phrase-based decoding to handle discontinuities in both source and target phrases. Our system significantly outperforms Moses and Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that thi"
N10-1140,W05-1507,0,0.0542351,"Missing"
N10-1140,N03-1017,0,0.18783,"commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne . . . pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of"
N10-1140,P07-2045,0,0.025929,"tly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to bin"
N10-1140,koen-2004-pharaoh,0,0.584336,"(Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne . . . pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the cur"
N10-1140,W09-0424,0,0.0386882,"Missing"
N10-1140,N06-1014,0,0.0442448,"Missing"
N10-1140,D07-1104,0,0.351235,"eyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems s"
N10-1140,C08-1064,0,0.0961671,"case of Joshua, we used the growdiag-final heuristic since this gave better results. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections 4 We use Moses’ default orientations: monotone, swap, and discontinuous. As far as this reordering model is concerned, we treat discontinuous phrases as continuous, i.e., we simply ignore what lies within gaps to determine phrase orientation. 5 (Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. As noted in (Lopez, 2008), Hiero can represent the same information with hierarchical rules of the form uX, Xu, and XuX. Hiero actually models lexicalized reordering patterns that (Tillmann, 2004) does not account for, e.g., a transformation from X1 uX2 v to X2 u′ v ′ X1 . 970 of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algori"
N10-1140,J06-4004,0,0.0337724,"Missing"
N10-1140,P08-1114,0,0.0273213,"ly outperforms Moses and Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that this turns out to also be the case with the hierarchical phrases of Joshua. In future work, we plan to extend the parameterization of phrasebased lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (Marton and Resnik, 2008; Chiang et al., 2009). The discontinuous phrase-based MT system described in this work is part of Phrasal, an opensource phrase-based system available for download at http://nlp.stanford.edu/software/phrasal . Acknowledgements The authors thank three anonymous reviewers, Dan Jurafsky, Spence Green, Steven Bethard, Daniel Cer, Chris Callison-Burch, and Pi-Chuan Chang for their helpful comments. This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement sh"
N10-1140,J04-4002,0,0.938227,"3 BLEU on average on five ChineseEnglish NIST test sets), even though both Joshua and our system support discontinuous phrases. Since the key difference between these two systems is that ours is not hierarchical—i.e., our system uses a string-based decoder instead of CKY, and it imposes no hard hierarchical reordering constraints during training and decoding—this paper sets out to challenge the commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test ti"
N10-1140,P03-1021,0,0.0409179,"ods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in SRILM (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data. For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translation"
N10-1140,2001.mtsummit-papers.68,0,0.034792,"test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in SRILM (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data. For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translations of our development set MT06, which were selected because our"
N10-1140,W05-0908,0,0.0181823,"Missing"
N10-1140,2006.amta-papers.25,0,0.0154965,"nese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translations of our development set MT06, which were selected because our system makes a crucial use of discontinuous phrases. In the first example, the Chi... , which typically transnese input contains lates as when. Lacking an entry for the input phrase in its phrase table, Moses is unable to translate this segment appropriately, and must instead split this ph"
N10-1140,W09-2303,0,0.410798,"partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (i) source: ai bj ck dl target: bm dn ap ct (ii) ai bj ak bl am bn (iii) ai bj ak bl am bn Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009); (iii) “bonbon” (Simard et al., 2005). Standard phrasebased decoders c"
N10-1140,W09-3805,0,0.158911,"Missing"
N10-1140,N04-4026,0,0.0205417,"alignment from cross-EM Viterbi alignment using the Moses grow-diag heuristic in the case Moses and our system. In the case of Joshua, we used the growdiag-final heuristic since this gave better results. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections 4 We use Moses’ default orientations: monotone, swap, and discontinuous. As far as this reordering model is concerned, we treat discontinuous phrases as continuous, i.e., we simply ignore what lies within gaps to determine phrase orientation. 5 (Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. As noted in (Lopez, 2008), Hiero can represent the same information with hierarchical rules of the form uX, Xu, and XuX. Hiero actually models lexicalized reordering patterns that (Tillmann, 2004) does not account for, e.g., a transformation from X1 uX2 v to X2 u′ v ′ X1 . 970 of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that ove"
N10-1140,D07-1077,0,0.048909,"Missing"
N10-1140,P06-1098,0,0.152278,"Missing"
N10-1140,P06-1123,0,0.0310399,"at may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (i) source: ai bj ck dl target: bm dn ap ct (ii) ai bj ak bl am bn (iii) ai bj ak bl am bn Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009); (iii) “bonbon” (Simard et al., 2005). Standar"
N10-1140,J97-3002,0,0.0265619,"Missing"
N10-1140,2005.eamt-1.39,0,0.031452,"inst Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne . . . pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow v"
N10-1140,P02-1040,0,\N,Missing
N10-1140,H05-1095,0,\N,Missing
N10-1140,D08-1076,0,\N,Missing
N10-2003,W09-2307,1,0.558961,"ensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Ra"
N10-2003,D08-1089,1,0.894263,"construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/sof"
N10-2003,P09-1087,1,0.142109,"rget language do a poor job of modeling long distance syntactic relationships. For example, if there are a number of intervening words between a verb and its subject, n-gram language models will often not be of much help in selecting the verb form that agrees with the subject. The target side dependency language model feature captures these long distance relationships by providing a dependency score for the target translations produced by the decoder. This is done using an efficient quadratic time algorithm that operates within the main decoding loop rather than in a separate reranking stage (Galley and Manning, 2009). Discriminative Distortion The standard distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically sign"
N10-2003,N10-1129,1,0.124195,"d distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models"
N10-2003,N03-1017,0,0.00907943,"ases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license"
N10-2003,P07-2045,0,0.0488706,"Missing"
N10-2003,koen-2004-pharaoh,0,0.121277,"f) (model_name) Running this command will first create word level alignments for the sentences in source.txt and target.txt using the Berkeley cross-EM aligner 1 http://www.itl.nist.gov/iad/mig/tests /mt/2009/ResultsRelease/currentArabic.html 9 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9–12, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 3 Decoder Decoding Engines The package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Manning, 2010). Both engines use features written to a common but extensible feature API, which allows features to be written once and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading"
N10-2003,N06-1014,0,0.141897,"ematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by"
N10-2003,niessen-etal-2000-evaluation,0,0.0200338,"ature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assist in the development of new"
N10-2003,J03-1002,0,0.00159764,"e and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to m"
N10-2003,P03-1021,0,0.0139183,"and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. tranlations per minute Figure 1: Chinese-to-English translation using discontinuous phrases. 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared betwe"
N10-2003,P02-1040,0,0.102389,"ord-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. tranlations per minute Figure 1: Chinese-to-English translation using discontinuous phrases. 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared between the threads is starting to overwhelm the value provided b"
N10-2003,W09-0441,0,0.0154617,"ese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assi"
N15-1020,D13-1106,1,0.323212,"bels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses c"
N15-1020,W05-0909,0,0.0736125,"uced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3 . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation. 5.2 Automatic Evaluation We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using m"
N15-1020,D14-1179,0,0.0151747,"Missing"
N15-1020,P14-1129,0,0.0340037,"when generating long responses. 4.3 Dynamic-Context Generative Model II Because DCGM-I does not distinguish between c and m, that model has the propensity to underestimate the strong dependency that holds between m and r. Our third model (DCGM-II) addresses this issue by concatenating the two linear mappings of the bag-ofwords representations bc and bm in the input layer of the feed-forward network representing c and m (see Figure 3 right). Concatenating continuous representations prior to deep architectures is a common strategy to obtain order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014). The forward equations for the context encoder are: 1 > 1 k1 = [b> c Wf , bm Wf ], > ` k` = σ(k`−1 Wf ) for ` = 2, · · · , L (8) where [x, y] denotes the concatenation of x and y vectors. In DCGM-II, the bias on the recurrent hidden state and the probability distribution over the next token are computed as described in Eq. 7. 5 Experimental Setting 5.1 Dataset Construction For computational efficiency and to alleviate the burden of human evaluators, we restrict the context sequence c to a single sentence. Hence, our dataset is composed of “triples” τ ≡ (cτ , mτ , rτ ) consisting of three sent"
N15-1020,P14-1066,1,0.560619,"stems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Mode"
N15-1020,D14-1002,1,0.788594,"stems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Mode"
N15-1020,D13-1176,0,0.046151,"tates. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probabilit"
N15-1020,P07-2045,0,0.0499714,"3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively. 5.3 Feature Sets The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets: MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very large phrase table of 160.7 million entries by first filtering out Twitterisms (e.g., long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher’s exact test (Ritter et al., 2011). We also included MT decoder features specifical"
N15-1020,J04-4002,0,0.0179799,"a set of candidate triples {˜ τ }, human evaluators are asked to rate the quality of the response within the new triples {(cτ , mτ , rτ˜ )}. After human evaluation, we retain the references for which the score is 4 or better on a 5 point scale, resulting in 3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively. 5.3 Feature Sets The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets: MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very lar"
N15-1020,P03-1021,0,0.041731,"r generalization. The last layer embeds the context vector into the hidden space of the decoder RLM. 5.5 Rescoring Setup We evaluate the proposed models by rescoring the n-best candidate responses obtained using the MT phrase-based decoder and the IR system. In contrast to MT, the candidate responses provided by IR have been created by humans and are less affected by fluency issues. The different n-best lists will provide a comprehensive testbed for our experiments. First, we augment the n-best list of the tuning set with the scores of the model of interest. Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features. At test time, we rescore the test n-best list with the new weights. 6 Results 6.1 Lower and Upper Bounds Table 2 shows the expected upper and lower bounds for this task as suggested by BLEU scores for human responses and a random response baseline. The RAN DOM system comprises responses randomly extracted from the triples corpus. HUMAN is computed by choosing one reference amongst the multi-reference set for each context-status pair.4 Although the scores are lower than those usually reported in SMT tasks, the ranking of the three systems"
N15-1020,P02-1040,0,0.121444,"an 3 times in the corpus. This produced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3 . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation. 5.2 Automatic Evaluation We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by o"
N15-1020,D11-1054,0,0.803346,"on-context-sensitive Machine Translation and Information Retrieval baselines. 1 message response yeah i’m on my way now ok good luck ! Figure 1: Example of three consecutive utterances occurring between two Twitter users A and B. Introduction Until recently, the goal of training open-domain conversational systems that emulate human conversation has seemed elusive. However, the vast quantities of conversational exchanges now available on social media websites such as Twitter and Reddit raise the prospect of building data-driven models that can begin to communicate conversationally. The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is “translated” into a plausible looking response. However, an approach such as that presented in Ritter et al. (2011) does not address the challenge of *The entirety of this work was conducted while at Microsoft Research. † Corresponding authors: Alessandro Sordoni (sordonia@iro.umontreal.ca) and Michel Galley (mgalley@microsoft.com). generating responses that are sensitive to the context of the conv"
N16-1014,P12-3007,0,0.0207071,"ilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood. 2 3 Related work The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as ik , fk and ok . We distinguish e and h wh"
N16-1014,P15-2073,1,0.256939,"ecutive message-response pairs spoken by different characters. We randomly selected two subsets as development and test datasets, each containing 2k pairs, with source and target length restricted to the range of [6,18]. 5.2 7 IMSDB (http://www.imsdb.com/) is a relatively small database of around 0.4 million sentences and thus not suitable for open domain dialogue training. 115 Model S EQ 2S EQ MMI-antiLM Evaluation For parameter tuning and final evaluation, we used B LEU (Papineni et al., 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al., 2015). In the case of the Twitter models, we used multireference B LEU. As the IMSDB data is too limited to support extraction of multiple references, only single reference B LEU was used in training and evaluating the OSDb models. We did not follow Vinyals et al. (2015) in using perplexity as evaluation metric. Perplexity is unlikely to be a useful metric in our scenario, since our proposed model is designed to steer away from the standard S EQ 2S EQ model in order to diversify the outputs. We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated respon"
N16-1014,P14-1066,1,0.170591,"systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as ik , fk and ok . We distinguish e and h where ek denotes the vector for an individual text unit (for example, a word or sentence) at time step k while hk denotes the vector computed by LSTM model at time k by combining ek and hk−1 . ck is the cell state vector at time k, and σ denotes the sigmoid function. Then, the vector representation hk for each time step 2 Augmenting our technique"
N16-1014,D13-1111,0,0.0100372,"Missing"
N16-1014,P07-2045,0,0.0627244,"et We first report performance on Twitter datasets in Table 2, along with results for different models (i.e., Machine Translation and MT+neural reranking) reprinted from Sordoni et al. (2015) on the same dataset. The baseline is the S EQ 2S EQ model with its standard likelihood objective and a beam size of 200. We compare this baseline against greedy-search S EQ 2S EQ (Vinyals and Le, 2015), which can help achieve higher diversity by increasing search errors.8 Machine Translation is the phrase-based MT system described in (Ritter et al., 2011). MT features include commonly used ones in Moses (Koehn et al., 2007), e.g., forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, etc. For more details, refer to Sordoni et al. (2015). MT+neural reranking is the phrase-based MT system, reranked using neural models. N-best lists are first generated from the MT system. Recurrent neural models generate scores for N-best list candidates given the input messages. These generated scores are re-incorporated to rerank all the candidates. Additional features to score [1, 2, 3, 4]-gram matches between context and response and between message and context (conte"
N16-1014,P15-1002,0,0.00622466,"200. The top examples are the responses with the highest average probability loglikelihoods in the N-best list. Lower-ranked, less-generic responses were manually chosen. speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs. Below, we present practical strategies for neural generation models that use MMI as an objective function. We show that use of MMI results in a clear decrease in the proportion of generic response sequences, generating correspondingly more varied and interesting outputs. al., 2015; Luong et al., 2015) has inspired attempts to extend these neural techniques to response generation. Sordoni et al. (2015) improved upon Ritter et al. (2011) by rescoring the output of a phrasal SMT-based conversation system with a S EQ 2S EQ model that incorporates prior context. (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015) apply direct end-to-end S EQ 2S EQ models These S EQ 2S EQ models are Long Short-Term Memory (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) that can implicitly capture compositionality and long-span dependencies. (Wen et al., 2015) attempt to le"
N16-1014,P03-1021,0,0.0536546,"sponses (T ) interchanged. 4.5 4.5.1 Decoding MMI-antiLM As described in Section 4.3.1, decoding using log p(T |S) − λU (T ) can be readily implemented by predicting tokens at each time-step. In addition, we found in our experiments that it is also important to take into account the length of responses in decod114 ing. We thus linearly combine the loss function with length penalization, leading to an ultimate score for a given target T as follows: Score(T ) = p(T |S) − λU (T ) + γNt (15) where Nt denotes the length of the target and γ denotes associated weight. We optimize γ and λ using MERT (Och, 2003) on N-best lists of response candidates. The N-best lists are generated using the decoder with beam size B = 200. We set a maximum length of 20 for generated candidates. At each time step of decoding, we are presented with B × B candidates. We first add all hypotheses with an EOS token being generated at current time step to the N-best list. Next we preserve the top B unfinished hypotheses and move to next time step. We therefore maintain beam size of 200 constant when some hypotheses are completed and taken down by adding in more unfinished hypotheses. This will lead the size of final N-best"
N16-1014,W00-0306,0,0.0299638,"oes not require identifying lexical overlap to foster diversity.2 On a somewhat different task, Mao et al. (2015, Section 6) utilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood. 2 3 Related work The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time"
N16-1014,P02-1040,0,0.120743,"Missing"
N16-1014,D11-1054,0,0.770418,"tantive gains in B LEU scores on two conversational datasets and in human evaluations. 1 Introduction Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response"
N16-1014,P15-1152,0,0.523144,"h interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, however, neural conversation models tend to ge"
N16-1014,N15-1020,1,0.849031,"of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, howe"
N16-1014,D15-1199,0,0.506634,"Missing"
N16-1147,N15-1053,0,0.0541033,"31) visit (321) market (311) outdoor activity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between"
N16-1147,P15-2017,1,0.822682,"Baseline Experiments We report baseline experiments on the storytelling task in Table 7, training on the SIS tier and testing on half the SIS validation set (valtest). Example output from each system is presented in Table 5. To highlight some differences between story and caption generation, we also train on the DII tier in isolation, and produce captions per-image, rather than in sequence. These results are shown in Table 7. To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al. (2014) to multiple images. Here, we encode an image sequence by running an RNN over the fc7 vectors of each image, in reverse order. This is used as the initial hidden state to the story decoder model, which learns to produce the story one word at a time using softmax loss over the training data vocabulary. We use Gated Recurrent Units (GRUs) (Cho et al., 2014) for both the image encoder and story decoder. In the baseline system, we generate the story using a simple beam search (size=10), which has been successful in image captioning previously (Devlin et al., 2015). Howeve"
N16-1147,D13-1128,0,0.0674328,"tivity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between remarking that a visual scene shows “sitting i"
N16-1147,D15-1021,1,0.889893,"Missing"
N16-1147,N16-1014,1,0.758927,"The dog was happy to be in the water. The family gathered together for a meal. The food was delicious. The dog was excited to be there. The kids were playing in the water. The boat was a little too much to drink. The family got together for a cookout. They had a lot of delicious food. The dog was happy to be there. They had a great time on the beach. They even had a swim in the water. Table 5: Example stories generated by baselines. This is a predictable result given the label bias problem inherent in maximum likelihood training; recent work has looked at ways to address this issue directly (Li et al., 2016). To establish a stronger baseline, we explore several decode-time heuristics to improve the quality of the generated story. The first heuristic is to lower the decoder beam size substantially. We find that using a beam size of 1 (greedy search) significantly increases the story quality, resulting in a 4.6 gain in METEOR score. However, the same effect is not seen for caption generation, with the greedy caption model obtaining worse quality than the beam search model. This highlights a key difference in generating stories versus generating captions. Although the stories produced using a greedy"
N16-1147,P04-1077,0,0.227891,"tter understand which metric could serve as a proxy for human evaluation, we compute pairwise correlation coefficients between automatic metrics and human judgments on 3,000 stories sampled from the SIS training set. For the human judgements, we again use crowdsourcing on MTurk, asking five judges per story to rate how strongly they agreed with the statement “If these were my photos, I would like using a story like this to share my experience with my friends”.7 We take the average of the five judgments as the final score for the story. For the automatic metrics, we use METEOR,8 smoothed-BLEU (Lin and Och, 2004), and Skip-Thoughts (Kiros et al., 2015) to compute similarity between each story for a given sequence. Skip-thoughts provide a Sentence2Vec embedding which models the semantic space of novels. As Table 4 shows, METEOR correlates best with human judgment according to all the correlation coefficients. This signals that a metric such as METEOR which incorporates paraphrasing correlates best with human judgement on this task. A more 7 Scale presented ranged from “Strongly disagree” to “Strongly agree”, which we convert to a scale of 1 to 5. 8 We use METEOR version 1.5 with hter weights. r ρ τ MET"
N16-1147,P14-5010,0,0.00748671,"instill morals, and share advice; focusing AI research towards this task therefore has the potential to bring about more humanlike intelligence and understanding. Re-telling Story 3 Story 1 2 easter (259) church (243) graduation ceremony (236) office (226) father’s day (221) Story 2 Story 5 Description for Images in Isolation & in Sequences Figure 2: Dataset crowdsourcing workflow. Caption in Sequence session, e.g., “John’s birthday party,” or “Shabnam’s visit.” Using the Flickr data release (Thomee et al., 2015), we aggregate 5-grams of photo titles and descriptions, using Stanford CoreNLP (Manning et al., 2014) to extract possessive dependency patterns. We keep the heads of possessive phrases if they can be classified as an EVENT in WordNet3.0, relying on manual winnowing to target our collection efforts.2 These terms are then used to collect albums using the Flickr API.3 We only include albums with 10 to 50 photos where all album photos are taken within a 48-hour span and CC-licensed. See Table 1 for the query terms with the most albums returned. The photos returned from this stage are then presented to crowd workers using Amazon’s Mechanical Turk to collect the corresponding stories and descriptio"
N16-1147,Q14-1006,0,0.0517771,"ket (311) outdoor activity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between remarking that a vi"
N19-1125,K16-1002,0,0.544301,"versity and relevance of the responses, compared to strong baselines on two datasets with one-tomany context-response mapping. 2 Related Work Grounded conversation models utilize extra context inputs besides conversation history, such as persona (Li et al., 2016b), textual knowledge (Ghazvininejad et al., 2017; Galley et al., 2019), dialog act (Zhao et al., 2017) and emotion (Huber et al., 2018). Our approach does not depend on such extra input and thus is complementary to this line of studies. Variational autoencoder (VAE) models explicitly model the uncertainty of responses in latent space. Bowman et al. (2016) used VAE with Long-Short Term Memory (LSTM) cells to generate sentences. The basic idea of VAE is to encode the input x into a probability distribution (e.g. Gaussian) z instead of a point encoding. However, it suffers from the vanishing latent variable problem (Bowman et al., 2016; Zhao et al., 2017) when applied to text generation tasks. Bowman et al. (2016); Fu et al. (2019) proposed to tackle this problem with word dropping and specific KL annealing methods. Zhao et al. (2017) proposed to add a bag-of-word loss, complementary to KL annealing. Applying this to a CVAE conversation model, th"
N19-1125,W14-4012,0,0.196356,"Missing"
N19-1125,N19-1021,1,0.840516,"et al., 2018). Our approach does not depend on such extra input and thus is complementary to this line of studies. Variational autoencoder (VAE) models explicitly model the uncertainty of responses in latent space. Bowman et al. (2016) used VAE with Long-Short Term Memory (LSTM) cells to generate sentences. The basic idea of VAE is to encode the input x into a probability distribution (e.g. Gaussian) z instead of a point encoding. However, it suffers from the vanishing latent variable problem (Bowman et al., 2016; Zhao et al., 2017) when applied to text generation tasks. Bowman et al. (2016); Fu et al. (2019) proposed to tackle this problem with word dropping and specific KL annealing methods. Zhao et al. (2017) proposed to add a bag-of-word loss, complementary to KL annealing. Applying this to a CVAE conversation model, they showed that even greedy decoding can generate diverse responses. However, as VAE/CVAE conversation models can be limited to a simple latent representations such as standard Gaussian distribution, Gu et al. (2018) proposed to enrich the latent space by leveraging a Gaussian mixture prior. Our work takes a geometrical approach that is fundamentally different from probabilistic"
N19-1125,P16-1094,1,0.881173,"Yes I do. When? Figure 1: Illustration of one context and its multiple responses in the latent space induced by our model. Distance and direction from the predicted response vector given the context roughly match the relevance and diversity, respectively. Based on the example in Table 2.2 Introduction The field of neural response generation is advancing rapidly both in terms of research and commercial applications (Gao et al., 2019; Zhou et al., 2018; Yoshino et al., 2019; Zhang et al., 2019). Nevertheless, vanilla sequence-to-sequence (S2S) models often generate bland and generic responses (Li et al., 2016a). Li et al. (2016a) encourage diversity by re-ranking the beam search results according to their mutual information with the conversation context. However, as beam search itself often produces lists of nearly identical sequences, this method can require a large beam width (e.g. 200). As a result, re-ranking can be extremely 1 An implementation of our model is available at https: //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications"
N19-1125,I17-1061,1,0.843614,"ted in Figure 1. To induce such a latent space, we leverage two different models: 1) a S2S model, producing the predicted response vector (the black dot at the center in Figure 1), and 2) an autoencoder (AE) model, yielding the vectors for potential responses (the colored dots). In order to make the S2S and AE share the same latent space (the cloud), we use the same decoder for both and train them jointly end-to-end with novel regularization terms. As this fuses the two latent spaces, we refer to our model as S PACE F USION. Regularization is necessary because only sharing the decoder, as in (Luan et al., 2017), does not necessarily align the latent spaces obtained by S2S and AE respectively or impose a disentangled structure onto the space. We introduce two regularization terms to tackle this issue. 1) interpolation term: we encourage a smooth semantic transition along the path between the predicted response vector and each target response vector (arrowed lines in Figure 1). This term effectively prevents semantically different responses from aligning in the same direction, essentially scattering them over different directions. 2) fusion term: we want the vectors from the two models to be distribut"
N19-1125,P02-1040,0,0.103428,"Missing"
N19-1125,P16-1009,0,0.0250463,"ibutions in representation and difficulties in training. Decoding and ranking encourage diversity during the decoding stage. As “vanilla” beam search often produces lists of nearly identical sequences, Vijayakumar et al. (2016) propose to include a dissimilarity term in the objective of beam search decoding. Li et al. (2016a) re-ranked the results 1230 obtained by beam search based on mutual information with the context using a separately trained response-to-context S2S model. ? S2S encoder +? ℒ fuse Multi-task learning is another line of studies related to the present work (see Section 3.2). Sennrich et al. (2016) use multi-task learning to improve neural machine translation by utilizing monolingual data, which usually far exceeds the amount of parallel data. A similar idea is applied by Luan et al. (2017) to conversational modeling, involving two tasks: 1) a S2S model that learns a context-to-response mapping using conversation data, and 2) an AE model that utilizes speakerspecific non-conversational data. The decoders of S2S and AE were shared, and the two tasks were trained alternately. 3 3.1 The S PACE F USION Model Problem statement Let D = [(x0 , y0 ), (x1 , y1 ), · · · , (xn , yn )] denote a con"
N19-1125,1983.tc-1.13,0,0.435323,"Missing"
N19-1125,N16-1014,1,0.836698,"Yes I do. When? Figure 1: Illustration of one context and its multiple responses in the latent space induced by our model. Distance and direction from the predicted response vector given the context roughly match the relevance and diversity, respectively. Based on the example in Table 2.2 Introduction The field of neural response generation is advancing rapidly both in terms of research and commercial applications (Gao et al., 2019; Zhou et al., 2018; Yoshino et al., 2019; Zhang et al., 2019). Nevertheless, vanilla sequence-to-sequence (S2S) models often generate bland and generic responses (Li et al., 2016a). Li et al. (2016a) encourage diversity by re-ranking the beam search results according to their mutual information with the conversation context. However, as beam search itself often produces lists of nearly identical sequences, this method can require a large beam width (e.g. 200). As a result, re-ranking can be extremely 1 An implementation of our model is available at https: //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications"
N19-1125,P17-1061,0,0.528488,": //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications. This highlights the need to improve the diversity of candidates before re-ranking, and the need to optimize for diversity during training rather than just at the decoding stage. While various approaches have been explored to diversify the output of conversation models, the improvement often comes at the cost of decreased response relevance along other dimensions. For instance, Zhao et al. (2017) present an approach to enhancing diversity by mapping diverse responses to a probability distribution using a conditional variational autoencoder (CVAE). Despite the improved response diversity, this approach reduces response relevance as measured against the baseline. One possible reason for this diversityrelevance trade-off is that such probabilistic approaches are not explicitly encouraged to induce a disentangled representation in latent space for 1229 Proceedings of NAACL-HLT 2019, pages 1229–1238 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguis"
N19-1269,P17-1059,0,0.0299531,"s its context and is grounded in a content-rich external textual source such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in"
N19-1269,P17-1070,0,0.0137682,"ant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1, which illustrates a situation where an author edits a document (here a Wikip"
N19-1269,D16-1140,0,0.0198381,"titive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1, which illustrates a situation where an author edits a document (here a Wikipedia article), 1 Historically, NLG"
N19-1269,D16-1128,0,0.0461276,"Missing"
N19-1269,N15-1145,0,0.0246576,"9, 2014, manning signed a one-year contract with the cincinnati bengals. on feb 9, 2013, barrett signed with the memphis grizzlies. some people think elvis, but most of us think he’s dead and gone.” it’s always the goal of the foreign- entry film award executive to be as possible.” Table 5: Example generations from the CIG system, paired with the human generated updates. is a substantial difference. Also our datasets are substantially larger, enabling generative models to be used in this space, where prior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015). For any generation task, it is important to address both the content (‘what’ is being said) as well its style (‘how’ it is being said). Recently, a great deal of research has focused on the ‘how’ (Li et al., 2018; Shen et al., 2017), including efforts to collect a parallel dataset that differs in politeness (Rao and Tetreault, 2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’"
N19-1269,N16-1014,1,0.950328,"ource such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1,"
N19-1269,P16-1094,1,0.943366,"ource such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1,"
N19-1269,N18-1169,0,0.0582512,"Missing"
N19-1269,D15-1166,0,0.0540266,"t salient to the topic or focus of the text, then generate a single sentence that represents this information. 3.1 Generative models A natural though difficult means of generating this additional update sentence x is to use a generative model conditioned on the information in the curated text s and the new document d. Recent methods inspired by successful neural machine translation systems have produced impressive results in abstractive summarization (Nallapati et al., 2016). Hence, our first step is to use the sequence-to-sequence encoder-decoder model (Bahdanau et al., 2015) with attention (Luong et al., 2015) for our task. This kind of model assumes that the output sentence can be generated word-by-word. Each output word xti generated is conditioned on all prior words x&lt;t i and an encoded representation of the context z: Y p(ˆ xti |ˆ x&lt;t (1) i , z) of the curated text. Hence, the model may capture some generalizations about the kinds of information and locations in d that are most likely to contribute novel information to s. Context Only Generative (COG) Model: This algorithm is trained to generate the most likely upˆ = arg max p(x|s). This model date sentence x is similar to CAG except that we co"
N19-1269,N16-1086,0,0.0570062,"2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’. Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei et al., 2016), selecting and describing slot-value pairs for task-specific dialog response generation (Wen et al., 2015), and even generating Wikipedia biography abstracts given Infobox information (Lebret et al., 2016). Our task, while grounded in external content, is different in that it leverages linguistic grounding as well as prior text context when generating text. This challenging setting enables a huge range of grounded generation tasks: there are vast amounts of unstructured textual data. formation. We demonstrate how multiple models can address this challenging problem on a novel dataset derived"
N19-1269,N16-1098,0,0.0319046,"hor edits a document (here a Wikipedia article), 1 Historically, NLG has focused on generation from structured content such as a database or semantic representation, but this paper is interested in generation from free-form text. Figure 1: Example of content transfer: Given existing curated text (yellow) and a document with additional relevant information (green), the task is to update the curated text (orange) to reflect the most salient updates. and the goal is to generate or suggest a next sentence (shown in orange) to the author. This type of unconstrained, long-form text generation task (Mostafazadeh et al., 2016; Fan et al., 2018) is of course extremely difficult. Free-form generation can easily go astray due to two opposing factors. On one hand, ensuring that the generated output is of relatively good quality often comes at the cost of making it bland and devoid of factual content (Li et al., 2016a). On the other hand, existing techniques can help steer neural models away 2622 Proceedings of NAACL-HLT 2019, pages 2622–2632 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics from blandness in order to produce more contentful outputs (using temperature sampl"
N19-1269,K16-1028,0,0.464293,"to incorporate the additional information from document di . The goal would be to identify new information (in particular, di  si ) that is most salient to the topic or focus of the text, then generate a single sentence that represents this information. 3.1 Generative models A natural though difficult means of generating this additional update sentence x is to use a generative model conditioned on the information in the curated text s and the new document d. Recent methods inspired by successful neural machine translation systems have produced impressive results in abstractive summarization (Nallapati et al., 2016). Hence, our first step is to use the sequence-to-sequence encoder-decoder model (Bahdanau et al., 2015) with attention (Luong et al., 2015) for our task. This kind of model assumes that the output sentence can be generated word-by-word. Each output word xti generated is conditioned on all prior words x&lt;t i and an encoded representation of the context z: Y p(ˆ xti |ˆ x&lt;t (1) i , z) of the curated text. Hence, the model may capture some generalizations about the kinds of information and locations in d that are most likely to contribute novel information to s. Context Only Generative (COG) Model"
N19-1269,W16-4620,0,0.012626,"y. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1, which illustrates a situation whe"
N19-1269,P02-1040,0,0.103679,"ntencepiece toolkit6 to use byte-pairencoding (BPE) with a vocabulary size of 32k. We used stochastic gradient descent optimizer and the stopping criterion was perplexity on the validation set. We filtered our dataset to contain instances which have length of the document between 50 and 2000 tokens, length of the curated text between 20 and 500 tokens and the length of the update sentence between 5 and 200 tokens. 5.1 Automated Evaluation Our primary automated evaluation metric for system-generated update sentences is ROUGE-L F1 against reference update sentence,7 though we also include BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011) as additional indicators. ROUGE is a standard family of metrics for summarization tasks; ROUGE-L measures the longest common subsequence between the system and the reference, capturing both lexical selection and word order. Table 2 illustrates that this task is quite difficult for extractive techniques. Furthermore, the results emphasize the importance of having curated text as context when generating the update. In all experimental conditions, models aware of context perform much better than models agnostic of it. In contrast to Liu et al. (2018), gener"
N19-1269,P18-1205,0,0.0223743,"rior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015). For any generation task, it is important to address both the content (‘what’ is being said) as well its style (‘how’ it is being said). Recently, a great deal of research has focused on the ‘how’ (Li et al., 2018; Shen et al., 2017), including efforts to collect a parallel dataset that differs in politeness (Rao and Tetreault, 2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’. Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei et al., 2016), selecting and describing slot-value pairs for task-specific dialog response generation (Wen et al., 2015), and even generating Wikipedia biography abstracts giv"
N19-1269,P18-1080,1,0.837215,"sets are substantially larger, enabling generative models to be used in this space, where prior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015). For any generation task, it is important to address both the content (‘what’ is being said) as well its style (‘how’ it is being said). Recently, a great deal of research has focused on the ‘how’ (Li et al., 2018; Shen et al., 2017), including efforts to collect a parallel dataset that differs in politeness (Rao and Tetreault, 2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’. Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei et al., 2016), selecting and describing slot-value pairs for task-specific dialog respons"
N19-1269,N18-1012,0,0.0318856,"tem, paired with the human generated updates. is a substantial difference. Also our datasets are substantially larger, enabling generative models to be used in this space, where prior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015). For any generation task, it is important to address both the content (‘what’ is being said) as well its style (‘how’ it is being said). Recently, a great deal of research has focused on the ‘how’ (Li et al., 2018; Shen et al., 2017), including efforts to collect a parallel dataset that differs in politeness (Rao and Tetreault, 2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’. Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei e"
N19-1269,D15-1044,0,0.0955856,"nerated update is close to the reference. well as the reference update. As we can see in examples 3 and 4, the CIG model misplaces the date but correctly generates the remaining content. In examples 1 and 2, the CIG model appears to successfully select the correct pronouns for coreference resolution, though it gets confused as to when to use the pronoun or the named entity. Examples 5 and 6 represent failure cases due to missing words. 6 Related Work The proposed content transfer task is clearly related to a long series of papers in summarization, including recent work with neural techniques (Rush et al., 2015; Nallapati et al., 2016). In particular, one recent paper casts the the task of generating an entire Wikipedia article as a multidocument summarization problem (Liu et al., 2018). Their best-performing configuration was a two-stage extractive-abstractive framework; a multi-stage approach helped circumvent the diffiDocument (News Article) anne kirkbride, who portrayed bespectacled, gravelly-voiced deirdre barlow in coronation street for more that four decades, has died. the 60-year-old, whose first appearance in the soap opera was in 1972, died in a manchester hospital after a short illness..."
N19-1269,N16-1005,0,0.0327879,"a content-rich external textual source such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Con"
N19-1269,D15-1199,0,0.141923,"Missing"
P03-1071,A00-2004,0,0.864903,"the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001)"
P03-1071,J86-3001,0,0.614029,"mber of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25 meetings to be segmented, each by at least three subjects. We opted for a linear representation of discourse, since finer-grained discourse structures (e.g. (Grosz and Sidner, 1986)) are generally considered to be difficult to mark reliably. Subjects were asked to mark each speaker change (potential boundary) as either boundary or non-boundary. In the resulting annotation, the agreed segmentation based on majority 1 While it would be desirable to have a broader variety of meetings, we hope that experiments on this corpus will still carry some generality. opinion contained 7.5 segments per meeting on average, while the average number of potential boundaries is 770. We used Cochran’s Q (1950) to evaluate the agreement among annotators. Cochran’s test evaluates the null hyp"
P03-1071,P98-2145,0,0.0135692,"are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corpus (Janin et al., 2003). This corpus is one of a growing number of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25"
P03-1071,P94-1002,0,0.983157,"ing approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg a"
P03-1071,P96-1038,0,0.663903,"91; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corp"
P03-1071,W98-1123,1,0.766858,"ule. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses,"
P03-1071,P93-1041,0,0.823497,"r features like cue phrases. We present a machine learning approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Na"
P03-1071,P95-1015,0,0.0924914,"e counted the number of its occurrences near any topic boundary, and its number of appearances overall. Then, we performed χ2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists. We selected terms whose χ2 value rejected the hypothesis under a 0.01-level confidence (the rejection criterion is χ2 ≥ 6.635). Finally, induced cue phrases whose usage has never been described in other work were removed (marked with ∗ in Table 3). Indeed, there is a risk that the automatically derived list of cue phrases could be too specific to the word usage in 9 As in (Litman and Passonneau, 1995), we restrict ourselves to the first lexical item of any utterance, plus the second one if the first item is also a cue word. okay Other Near boundary 64 657 Distant 740 25896 Table 2: okay (χ2 = 89.11, df = 1, p &lt; 0.01). okay shall ∗ anyway we’re ∗ alright let’s ∗ 93.05 27.34 23.95 17.67 16.09 14.54 but so and should ∗ good ∗ 13.57 11.65 10.99 10.21 7.70 to the discussion can greatly change from one discourse unit to the next. We try to capture significant changes in speakership by measuring the dissimilarity between two analysis windows. For each potential boundary, we count for each speaker"
P03-1071,J91-1002,0,0.963738,"speaker change, and other features like cue phrases. We present a machine learning approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hi"
P03-1071,P93-1020,0,0.0314282,"wo analysis windows. For each potential boundary, we count for each speaker i the number of words that are uttered before (Li ) and after (Ri ) the potential boundary (we limit our analysis to a window of fixed size). The two distributions are normalized to form two probability distributions l and r, and significant changes of speakership are detected by computing their Jensen-Shannon divergence: JS(l, r) = 12 [D(l||avgl,r ) + D(r||avgl,r )] Table 3: Automatically selected cue phrases. these meetings. Silences: previous work has found that major shifts in topic typically show longer silences (Passonneau and Litman, 1993; Hirschberg and Nakatani, 1996). We investigated the presence of silences in meetings and their correlation with topic boundaries, and found it necessary to make a distinction between pauses and gaps (Levinson, 1983). A pause is a silence that is attributable to a given party, for example in the middle of an adjacency pair, or when a speaker pauses in the middle of her speech. Gaps are silences not attributable to any party, and last until a speaker takes the initiative of continuing the discussion. As an approximation of this distinction, we classified a silence that follows a question or in"
P03-1071,J97-1005,0,0.950475,"roadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997;"
P03-1071,J02-1002,0,0.625567,"ng the average (µ) and standard deviation (σ) of the p(mi ) values, and each potential boundary mi above the threshold µ−α·σ is hypothesized as a real boundary. 4.2 Evaluation We evaluate LCseg against two state-of-the-art segmentation algorithms based on lexical cohesion (Choi, 2000; Utiyama and Isahara, 2001). We use the error metric Pk proposed by Beeferman et al. (1999) to evaluate segmentation accuracy. It computes the probability that sentences k units (e.g. sentences) apart are incorrectly determined as being either in different segments or in the same one. Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). A test corpus of concatenated6 texts extracted from the Brown corpus was built by Choi (2000) to evaluate several domain-independent segmentation algorithms. We reuse the same test corpus for our evaluation, in addition to two other test corpora we constructed to test how segmenters scale across genres and how they perform with texts with various 6 Concatenated documents correspond to reference segments. number of segments.7 We designed two test corpora, each of"
P03-1071,P94-1050,0,0.200681,"features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur"
P03-1071,P99-1046,0,0.0706811,"provement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range"
P03-1071,J01-1002,0,0.457388,"Missing"
P03-1071,P01-1064,0,0.942001,"es lexical chains, which are thought to mirror the discourse structure of the underlying text (Morris and Hirst, 1991). We ignore synonymy and other semantic relations, building a restricted model of lexical chains consisting of simple term repetitions, hypothesizing that major topic shifts are likely to occur where strong term repetitions start and end. While other relations between lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001). The preprocessing steps of LCseg are common to many segmentation algorithms. The input document is first tokenized, non-content words are removed, 2 Four other meetings failed short the significance test, while there was little agreement on the two last ones (p > 0.1). and remaining words are stemmed using an extension of Porter’s stemming algorithm (Xu and Croft, 1998) that conflates stems using corpus statistics. Stemming will allow our algorithm to more accurately relate terms that are semantically close. The core algorithm of LCseg has two main parts: a method to identify and weight stro"
P03-1071,J93-3003,0,\N,Missing
P03-1071,C98-2140,0,\N,Missing
P04-1085,J96-1002,0,0.00482603,"Missing"
P04-1085,P97-1023,1,0.16656,"rk of (Hillard et al., 2003). We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. Table 3 lists the features that were found most helpful at identifying agreements and disagreements. Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g. “yes” and “right”, as listed in (Cohen, 2002), general cue phrases, e.g. “but” and “alright” (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997). We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002). Duration is also a good predictor of backchannels, since they tend to be quite short. Finally, a fair amount of silence and filled pauses is sometimes an indicator of disagreement, since it is a dispreferred response in most social contexts and can be associated wi"
P04-1085,N03-2012,1,0.55458,"sus decision is reached. Our ultimate goal is automated summarization of multi-participant meetings and we hypothesize that the ability to automatically identify agreement and disagreement between participants will help us in the summarization task. For example, a summary might resemble minutes of meetings with major decisions reached (consensus) along with highlighted points of the pros and cons for each decision. In this paper, we present a method to automatically classify utterances as agreement, disagreement, or neither. Previous work in automatic identification of agreement/disagreement (Hillard et al., 2003) demonstrates that this is a feasible task when various textual, durational, and acoustic features are available. We build on their approach and show that we can get an improvement in accuracy when contextual information is taken into account. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational and structural features that look both forward and backward in the discourse. This allows us to acquire, and subsequently process, knowledge about who speaks to whom. We hypothesize that pragmatic features that center around previous agreemen"
P04-1085,W02-1002,0,0.0100279,"beling. Their drawback is that, as most generative models, they are generally computed to maximize the joint likelihood of the training data. In order to define a probability distribution over the sequences of observation and labels, it is necessary to enumerate all possible sequences of observations. Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)). Conditional models address these concerns. Conditional Markov models (CMM) (Ratnaparkhi, 1996; Klein and Manning, 2002) have been successfully used in sequence labeling tasks incorporating rich feature sets. In a left-to-right CMM as shown in Figure 1(a), the probability of a sequence of L tags + !      is decomposed as: 1        E4            !   . is the vector of observations and each is the index of a spurt. The probability distribution  @  / !- associated with each state of the Markov chain only depends on the preceding tag F  and the local observation "" . However, in order to incorporate more than one label dependency and, in"
P04-1085,W03-1209,0,0.011538,"Missing"
P04-1085,W04-2319,1,0.693642,"Missing"
P04-1085,J00-3003,1,0.795799,"Missing"
P04-1085,J93-3003,1,\N,Missing
P06-1121,N04-1014,1,\N,Missing
P06-1121,N04-1035,1,\N,Missing
P06-1121,C00-2092,0,\N,Missing
P06-1121,P01-1067,1,\N,Missing
P06-1121,J04-4002,0,\N,Missing
P06-1121,P05-1033,0,\N,Missing
P06-1121,J97-3002,0,\N,Missing
P06-1121,W02-1039,0,\N,Missing
P06-1121,J08-3004,1,\N,Missing
P06-1121,N06-1033,1,\N,Missing
P09-1034,P06-2003,0,0.0627553,"Missing"
P09-1034,W05-0909,0,0.0605981,"e system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references a"
P09-1034,2005.mtsummit-papers.11,0,0.00383163,"Missing"
P09-1034,C04-1072,0,0.0380796,"y 5–10 points.1 4.3 Baseline Metrics We consider four baselines. They are small regression models as described in Section 2 over component scores of four widely used MT metrics. To alleviate possible nonlinearity, we add all features in linear and log space. Each baselines carries the name of the underlying metric plus the suffix -R.2 B LEU R includes the following 18 sentence-level scores: BLEU-n and n-gram precision scores (1 ≤ n ≤ 4); BLEU brevity penalty (BP); BLEU score divided by BP. To counteract BLEU’s brittleness at the sentence level, we also smooth BLEU-n and n-gram precision as in Lin and Och (2004). N IST R consists of 16 features. NIST-n scores (1 ≤ n ≤ 10) and information-weighted n-gram precision scores (1 ≤ n ≤ 4); NIST brevity penalty (BP); and NIST score divided by BP. 1 Due to space constraints, we only show results for “tieaware” predictions. See Pad´o et al. (2009) for a discussion. 2 The regression models can simulate the behaviour of each component by setting the weights appropriately, but are strictly more powerful. A possible danger is that the parameters overfit on the training set. We therefore verified that the three non-trivial “baseline” regression models indeed confer"
P09-1034,W05-0904,0,0.324729,"translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses. We operationalize meaning equivalence by bidirectional textual entailment (RTE, D"
P09-1034,E06-1032,0,0.104031,"tnesses are terming it terrorism. Introduction Constant evaluation is vital to the progress of machine translation (MT). Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations. The resulting scores are cheap and objective. However, studies such as Callison-Burch et al. (2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be “gamed” by permuting word order; (3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The conte"
P09-1034,C08-1066,1,0.800593,"ile the original RTE task is asymmetric, MT evaluation needs to determine meaning equivalence, which is a symmetric relation. We do this by checking for entailment in both directions (see Figure 1). Operationally, this ensures we detect translations which either delete or insert material. Clearly, there are also differences between the two tasks. An important one is that RTE assumes the well-formedness of the two sentences. This is not generally true in MT, and could lead to degraded linguistic analyses. However, entailment relations are more sensitive to the contribution of individual words (MacCartney and Manning, 2008). In Example 2, the modal modifiers break the entailment between two otherwise identical sentences: (2) HYP: Peter is certainly from Lincolnshire. REF: Peter is possibly from Lincolnshire. This means that the prediction of TE hinges on correct semantic analysis and is sensitive to misanalyses. In contrast, human MT judgments behave robustly. Translations that involve individual errors, like (2), are judged lower than perfect ones, but usually not crucially so, since most aspects are still rendered correctly. We thus expect even noisy RTE features to be predictive for translation quality. This"
P09-1034,W08-0309,0,0.0715095,"Gibbs sampling (see de Marneffe et al. (2007)). Entailment features. In the third stage, the system produces roughly 100 features for each aligned premise-hypothesis pair. A small number of them are real-valued (mostly quality scores), but most are binary implementations of small linguistic theories whose activation indicates syntactic and se4 4.1 Experimental Evaluation Experiments Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five- or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al., 2008). An alternative that has been adopted by the yearly WMT evaluation shared tasks since 2008 is the collection of pairwise preference judgments between pairs of MT hypotheses which can be elicited (somewhat) more reliably. We demonstrate that our approach works well for both types of annotation and different corpora. Experiment 1 models absolute scores on Asian newswire, and Experiment 2 pairwise preferences on European speech and news data. 4.2 Evaluation We evaluate the output of our models both on the sentence and on the system level. At the sentence level, we can correlate predictions in Ex"
P09-1034,N06-1006,1,0.583147,"Missing"
P09-1034,P08-1007,0,0.286549,"ed towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses. We operati"
P09-1034,I08-1042,0,0.0473301,"Missing"
P09-1034,P06-1114,0,0.0466837,". 3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE). TE was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning patterns than classical, strict logical entailment. Textual entailment is defined informally as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if “a human reading P would infer that H is most likely true”. Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). The relation between textual entailment and MT evaluation is shown in Figure 1. Perfect MT output and the reference translation entail each other (top). Translation problems that impact semantic equivalence, e.g., deletion or addition of material, can break entailment in one or both directions (bottom). On the modelling level, there is common ground between RTE and MT evaluation: Both have to distinguish between valid and invalid variation to determine whether two texts convey the same information or not. For example, to recognize the bidirectional entailment in Ex. (1), RTE must account for"
P09-1034,hovy-etal-2006-automated,0,0.0208131,"a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar ideas have been applied by Owczarzak et al. (2008) to LFG parses, and by Liu and Gildea (2005) to features derived from phrase-structure tress. This approach has also been successful for the related task of summarization evaluation (Hovy et al., 2006). The most comparable work to ours is Gim´enez and M´arquez (2008). Our results agree on the crucial point that the use of a wide range of linguistic knowledge in MT evaluation is desirable and important. However, Gim´enez and M´arquez advocate the use of a bottom-up development process that builds on a set of “heterogeneous”, independent metrics each of which measures overlap with respect to one linguistic level. In contrast, our aim is to provide a “top-down”, integrated motivation for the features we integrate through the textual entailment recognition paradigm. 8 Conclusion and Outlook In"
P09-1034,N06-1058,0,0.0523827,"ependents of predicates is unreliable on the WMT data. Other differences do reflect more fundamental differences between the two tasks (cf. Section 3). For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation. 7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al. (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar ideas have been applied by Owcza"
P09-1034,P03-1021,0,0.00335176,"Missing"
P09-1034,W09-0404,1,0.763215,"Missing"
P09-1034,P02-1040,0,0.104893,"fferent settings. The combination metric outperforms the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements. 1 (1) HYP: However, this was declared terrorism by observers and witnesses. REF: Nevertheless, commentators as well as eyewitnesses are terming it terrorism. Introduction Constant evaluation is vital to the progress of machine translation (MT). Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations. The resulting scores are cheap and objective. However, studies such as Callison-Burch et al. (2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be “gamed” by permuting word order; (3) for some corpora and languages,"
P09-1034,2007.tmi-papers.19,0,0.082174,"ment REF: Three aid workers were kidnapped by pirates. Figure 1: Entailment status between an MT system hypothesis and a reference translation for equivalent (top) and non-equivalent (bottom) translations. 2 Regression-based MT Quality Prediction Current MT metrics tend to focus on a single dimension of linguistic information. Since the importance of these dimensions tends not to be stable across language pairs, genres, and systems, performance of these metrics varies substantially. A simple strategy to overcome this problem could be to combine the judgments of different metrics. For example, Paul et al. (2007) train binary classifiers on a feature set formed by a number of MT metrics. We follow a similar idea, but use a regularized linear regression to directly predict human ratings. Feature combination via regression is a supervised approach that requires labeled data. As we show in Section 5, this data is available, and the resulting model generalizes well from relatively small amounts of training data. 3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE). TE was introduced by Dagan et al. (2005) as a"
P09-1034,2006.amta-papers.25,0,0.0532152,"(3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quali"
P09-1034,W09-0441,0,0.0147185,"tness and improved correlations for the regression models. An exception is BLEU-1 and NIST-4 on Expt. 1 (Ar, Ch), which perform 0.5–1 point better at the sentence level. T ER R includes 50 features. We start with the standard TER score and the number of each of the four edit operations. Since the default uniform cost does not always correlate well with human judgment, we duplicate these features for 9 non-uniform edit costs. We find it effective to set insertion cost close to 0, as a way of enabling surface variation, and indeed the new TERp metric uses a similarly low default insertion cost (Snover et al., 2009). M ETEOR R 4.4 consists of METEOR v0.7. Combination Metrics The following three regression models implement the methods discussed in Sections 2 and 3. M T R combines the 85 features of the four baseline models. It uses no entailment features. RTE R uses the 70 entailment features described in Section 3.1, but no M T R features. M T +RTE R uses all M T R and RTE R features, combining matching and entailment evidence.3 5 Expt. 1: Predicting Absolute Scores Data. Our first experiment evaluates the models we have proposed on a corpus with traditional annotation on a seven-point scale, namely the"
P09-1034,W06-1610,0,0.214402,"or mismatches between dependents of predicates is unreliable on the WMT data. Other differences do reflect more fundamental differences between the two tasks (cf. Section 3). For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation. 7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al. (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar"
P09-1034,W07-1401,0,\N,Missing
P09-1087,W03-3017,0,0.182869,"Missing"
P09-1087,J04-4002,0,0.010408,"Missing"
P09-1087,N04-1021,0,0.0913612,"Missing"
P09-1087,P03-1021,0,0.00500391,"Missing"
P09-1087,2001.mtsummit-papers.68,0,0.103583,"Missing"
P09-1087,D08-1012,0,0.0322584,"Missing"
P09-1087,P05-1034,0,0.184492,"Missing"
P09-1087,W97-0301,0,0.0828007,"Missing"
P09-1087,W05-0908,0,0.11709,"Missing"
P09-1087,P08-1066,0,0.0730585,"ntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of McDonald et al. (2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p ≤ .01). 2 &lt;root> who do you think they hired &lt;root> WP VB PRP VB PRP VBD . 0 1 2 3 4 5 6 7 ? Figure 1: A dependency tree with directed edges going from heads to modifiers. The edge between who and h"
P09-1087,2006.amta-papers.25,0,0.0438387,"Missing"
P09-1087,N03-1033,1,0.092602,"Missing"
P09-1087,P07-1031,0,0.0117656,"Missing"
P09-1087,D07-1077,0,0.0409968,"Missing"
P09-1087,P96-1021,0,0.117053,"Missing"
P09-1087,P08-1025,0,0.0382139,"Missing"
P09-1087,D07-1090,0,0.0117605,"Missing"
P09-1087,W08-0336,1,0.906959,"Missing"
P09-1087,P05-1033,0,0.211608,"Missing"
P09-1087,P07-1033,0,0.0263438,"Missing"
P09-1087,P05-1067,0,0.072379,"Missing"
P09-1087,P99-1059,0,0.101932,"systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m ) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1 The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1) ), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1) ), i.e., O(n3m ). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). 2 Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English"
P09-1087,C96-1058,0,0.118578,"Missing"
P09-1087,W02-1039,0,0.107882,"Missing"
P09-1087,P07-1019,0,0.0420279,"Missing"
P09-1087,W05-1507,0,0.020767,"arcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m ) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1 The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1) ), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1) ), i.e., O(n3m ). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). 2 Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_"
P09-1087,J99-4005,0,0.0703363,"parsing with m-grams runs in O(n3m ) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1 The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1) ), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1) ), i.e., O(n3m ). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). 2 Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks incorporate synchronous CFG models, score differences with the best phrase-based system were insignificantly small. 773 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773–781, c Suntec, Singapore, 2-7 August 2009."
P09-1087,N03-1017,0,0.00937669,"Missing"
P09-1087,P07-2045,0,0.00641963,"without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of McDonald et al. (2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p ≤ .01). 2 &lt;root> who do you think they hired &lt;root> WP VB PRP VB PRP VBD . 0 1 2 3 4 5 6 7 ? Figure 1: A dependency tree with directed edges going from heads to modifiers. The edge between who and hired causes this tree to be non-projective. Such a head-modifier relationship is difficult to represent with a CFG, sin"
P09-1087,W06-1606,0,0.0614139,"Missing"
P09-1087,P05-1012,0,0.782472,"/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks incorporate synchronous CFG models, score differences with the best phrase-based system were insignificantly small. 773 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773–781, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of McDonald et al. (2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides imp"
P09-1087,P02-1040,0,\N,Missing
P09-1087,H05-1066,0,\N,Missing
P09-1087,W07-0702,0,\N,Missing
P09-1087,D08-1076,0,\N,Missing
P11-2081,N10-1083,0,0.0618786,"Missing"
P11-2081,J93-2003,0,0.0914619,"Missing"
P11-2081,P00-1056,0,0.144713,", but it catches up and surpasses the random models at data sizes greater than 100 sentence pairs. To further evaluate the impact of initialization for IBM Model 1, we report on a set of experiments looking at alignment error rate achieved by different models. We report the performance of Model 1, as well as the performance of the more competitive HMM alignment model (Vogel et al., 1996), initialized from IBM-1 parameters. The dataset for these experiments is English-French parallel data from Hansards. The manually aligned data for evaluation consists of 137 sentences (a development set from (Och and Ney, 2000)). We look at two different training set sizes, a small set consisting of 1000 sentence pairs, and a reasonably-sized dataset containing 100,000 sentence pairs. In each data size condition, we report on the performance achieved by IBM-1, and the performance achieved by HMM initialized from the IBM1 parameters. For IBM Model 1 training, we either perform only 5 EM iterations (the standard setting in GIZA++), or run it to convergence. For each of these two settings, we either start training from uniform t(f |e) parameters, or random parameters. Table 2 details the results of these experiments. E"
P11-2081,C96-2141,0,0.944716,"as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate. 1 Introduction Statistical alignment models have become widely used in machine translation, question answering, textual entailment, and non-NLP application areas such as information retrieval (Berger and Lafferty, 1999) and object recognition (Duygulu et al., 2002). The complexity of the probabilistic models needed to explain the hidden correspondence among words has necessitated the development of highly non-convex and difficult to optimize models, such as HMMs (Vogel et al., 1996) and IBM Models 3 and higher (Brown et al., 1993). To reduce the impact of getting stuck in bad local optima the original IBM paper (Brown et al., 1993) proposed the idea of training a sequence of models from simpler to complex, and using the simpler models to initialize the more complex ones. IBM Model 1 was the first model in this sequence and was considered a reliable initializer due to its convexity. In this paper we show that although IBM Model 1 is convex, it is not strictly convex, and there is a large 461 Michel Galley Microsoft Research Redmond, WA 98005, USA mgalley@microsoft.com spa"
P11-2081,steinberger-etal-2006-jrc,0,\N,Missing
P15-1085,W99-0604,0,\N,Missing
P15-1085,N10-1083,0,\N,Missing
P15-1085,N07-1022,1,\N,Missing
P15-1085,N06-1056,1,\N,Missing
P15-1085,P07-1121,1,\N,Missing
P15-1085,P13-2009,0,\N,Missing
P15-1085,P07-2045,0,\N,Missing
P15-1085,P06-1115,1,\N,Missing
P15-1085,P05-1033,0,\N,Missing
P15-1085,N03-1017,0,\N,Missing
P15-1085,J97-3002,0,\N,Missing
P15-1085,P06-1121,1,\N,Missing
P15-1085,D13-1160,0,\N,Missing
P15-1085,P09-1010,0,\N,Missing
P15-2073,W05-0909,0,0.0517039,"irable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the"
P15-2073,E06-1032,0,0.0476642,"on, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propos"
P15-2073,2003.mtsummit-papers.9,0,0.031569,"run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Ve"
P15-2073,N12-1017,0,0.0244703,"ri,j )  PP i g ∈ n-grams(hi ) maxj wi,j ·#g (hi ) n 1 e(1−ρ/η) if η &gt; ρ otherwise Discriminative B LEU (2) where ρ and η are respectively hypothesis and reference lengths.2 Then corpus-level n-gram precision is defined as:  P P i g ∈ n-grams(hi ) maxj #g (hi , ri,j ) P P pn = i g ∈ n-grams(hi ) #g (hi ) where #g (·) is the number of occurrences of n-gram g in a given  sentence, and #g (u, v) is a shorthand for min #g (u), #g (v) . It has been demonstrated that metrics such as B LEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012). Unfortunately, gathering multiple references is difficult in the case of conversations. Data gathered from naturally occurring conversations offer only one response per message. One could search (c, m) pairs that occur multiple times in conversational data with the hope of finding distinct responses, but this solution is not feasible. Indeed, the larger 1 Unless mentioned otherwise, B LEU refers to the original IBM B LEU as first described in (Papineni et al., 2002). 2 In the case of multiple references, B LEU selects the reference whose length is closest to that of the hypothesis. In a nuts"
P15-2073,D14-1020,0,0.0691491,"evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), emp"
P15-2073,N15-1124,0,0.0162952,"w model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU"
P15-2073,W07-0734,0,0.0243517,"ool”, “well then! Why were the biscuits needed?”); others are a little more plausible, but irrelevant or possibly topic changing (“ohh I love that song”). Higher-valued positive-weighted mined responses are typically reasonably appropriate and relevant (even though 447 3 For this work, we sought 2 additional annotations of the seed responses for consistency with the mined responses. As a result, scores for some seed responses slipped below our initial threshold of 4. Nonetheless, these responses were retained. test set.4 While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwi"
P15-2073,C12-1121,0,0.0718467,"corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by averaging human ratings on the M sentences, and mi by computing metric scores on the same set of sentences.7 We compare three different metrics: B LEU, ∆B LEU, and sentence-level B LEU (sB LEU). The last computes sentence-level B LEU scores (Nakov et al., 2012) and averages them on the M sentences (akin to macro-averaging). Finally, unless otherwise noted, all versions of B LEU use n-gram order up to 2 (B LEU-2), as this achieves better correlation for all metrics on this data. extracted from a completely unrelated conversation), and in some cases can outscore the original response, as can be seen in the third set of examples. 4.2 Human Evaluation of System Outputs Responses generated by the 7 systems used in this study on the 2114-triple test set were hand evaluated by 5 crowdsourced raters each on a 5-point Likert-type scale. From these 7 systems,"
P15-2073,P03-1021,0,0.0436342,"rced raters each on a 5-point Likert-type scale. From these 7 systems, 12 system pairs were evaluated, for a total of about pairwise 126K ratings (12 · 5 · 2114). Here too, raters were asked to evaluate responses in terms of their relevance to both context and message. Outputs from different systems were randomly interleaved for presentation to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While ∆B LEU relies on human qualitative judgments, it is important to note that human judgments on multi-references (§ 4.1) and those on system outputs were collected completely independently. We also note that the"
P15-2073,P02-1040,0,0.116946,"f outputs are acceptable or even desirable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result"
P15-2073,D11-1054,0,0.592121,"f a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for C"
P15-2073,N15-1020,1,0.452044,"to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic"
P15-2073,P12-2008,0,0.0788347,"llison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU in which n-grams are weighted by tf ·idf. This assumes the availability of a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU"
P16-1094,P12-3007,0,0.0527842,"s the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et"
P16-1094,P15-2073,1,0.826361,"Missing"
P16-1094,P14-1066,1,0.260835,", 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a S EQ 2S EQ model conditioned on conversation history. Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conv"
P16-1094,N16-1014,1,0.261728,"? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized avatar-like agents, or game characters.1 For present purposes, we"
P16-1094,P15-1002,0,0.023784,"2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a S EQ 2S EQ model conditioned on conversation history. Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conversation history. Recent work by Li et al. ("
P16-1094,P03-1021,0,0.0474555,"rate generic and commonplace responses such as I don’t know, we follow Li et al. (2016) by reranking the generated N-best list using 997 a scoring function that linearly combines a length penalty and the log likelihood of the source given the target: log p(R|M, v) + λ log p(M |R) + γ|R| (11) where p(R|M, v) denotes the probability of the generated response given the message M and the respondent’s speaker ID. |R |denotes the length of the target and γ denotes the associated penalty weight. We optimize γ and λ on N-best lists of response candidates generated from the development set using MERT (Och, 2003) by optimizing B LEU. To compute p(M |R), we train an inverse S EQ 2S EQ model by swapping messages and responses. We trained standard S EQ 2S EQ models for p(M |R) with no speaker information considered. 5 Datasets 5.1 • Learning rate is set to 1.0. • Parameters are initialized by sampling from the uniform distribution [−0.1, 0.1]. • Gradients are clipped to avoid gradient explosion with a threshold of 5. • Vocabulary size is limited to 50,000. • Dropout rate is set to 0.2. Twitter Persona Dataset Data Collection Training data for the Speaker Model was extracted from the Twitter FireHose for"
P16-1094,W00-0306,0,0.39345,"nnotators. 2 Related Work This work follows the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural languag"
P16-1094,P02-1040,0,0.111268,"Missing"
P16-1094,D11-1054,0,0.655954,"ou? Where were you born? I was born in Canada. Where are you from? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized ava"
P16-1094,walker-etal-2012-annotated,0,0.0254272,"to reduce the proportion of generic responses typical of S EQ 2S EQ systems. Yao et al. (2015) employ an intention network to maintain the relevance of responses. Modeling of users and speakers has been extensively studied within the standard dialog modeling framework (e.g., (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)). Since generating meaningful responses in an open-domain scenario is intrinsically difficult in conventional dialog systems, existing models often focus on generalizing character style on the basis of qualitative statistical analysis (Walker et al., 2012; Walker et al., 2011). The present work, by contrast, is in the vein of the S EQ 2S EQ models of Vinyals and Le (2015) and Li et al. (2016), enriching these models by training persona vectors directly from conversational data and relevant side-information, and incorporating these directly into the decoder. 995 3 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xnX }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as it , ft and ot . We distinguish e and h where et denotes the vector for an individual text"
P16-1094,D15-1199,0,0.168657,"Missing"
P16-1094,P15-1152,0,0.72735,"Missing"
P16-1094,N15-1020,1,0.919869,"rn? I was born in Canada. Where are you from? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized avatar-like agents, or ga"
P18-5002,P17-5004,0,0.0614004,"Missing"
P18-5002,N16-1014,1,0.6779,"ng for Dialogue. tion (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016). Such end-to-end models have been particularly successful with social bot scenarios, as they require little interaction with the user’s environment (no need for API calls) and such models cope well with free-form and open domain texts. However, neural responses are often too general to carry meaningful information, e.g., with the common response “I don’t know” which can serve as a reply to most user questions. A mutual information model is proposed by (Li et al., 2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al.(Li et al., 2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots are originally developed for different purposes, there is a trend of combining both as a step towards building an open-domain dialogue agent. For example, on the one hand, (Ghazvininejad et al., 2018) presented a fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentfu"
P18-5002,P16-1094,1,0.803013,"ng for Dialogue. tion (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016). Such end-to-end models have been particularly successful with social bot scenarios, as they require little interaction with the user’s environment (no need for API calls) and such models cope well with free-form and open domain texts. However, neural responses are often too general to carry meaningful information, e.g., with the common response “I don’t know” which can serve as a reply to most user questions. A mutual information model is proposed by (Li et al., 2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al.(Li et al., 2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots are originally developed for different purposes, there is a trend of combining both as a step towards building an open-domain dialogue agent. For example, on the one hand, (Ghazvininejad et al., 2018) presented a fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentfu"
P18-5002,D16-1127,1,0.858377,"ng for Dialogue. tion (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016). Such end-to-end models have been particularly successful with social bot scenarios, as they require little interaction with the user’s environment (no need for API calls) and such models cope well with free-form and open domain texts. However, neural responses are often too general to carry meaningful information, e.g., with the common response “I don’t know” which can serve as a reply to most user questions. A mutual information model is proposed by (Li et al., 2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al.(Li et al., 2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots are originally developed for different purposes, there is a trend of combining both as a step towards building an open-domain dialogue agent. For example, on the one hand, (Ghazvininejad et al., 2018) presented a fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentfu"
P18-5002,P18-1203,1,0.724112,"l networks, so that they can be jointly optimized from user feedback signals using backpropagation and RL. The second is the use of advanced RL techniques to optimize dialogue policies in more complex scenarios. Examples include improved efficiency of exploration for faster learning, and hierarchical problem solving for composite-task dialogues where the reward signal is particularly sparse. We review several recent proposals, including the ones based on Bayesian models, curiosity-driven strategy, hierarchical reinforcement learning, adversarial learning, and the Dyna framework (Sutton, 1990; Peng et al., 2018) to integrate planning and learning, etc. We end this section by presenting a few example task-oriented systems from some of the leading players in the industry, including Microsoft’s Cortana, Amazon’s Alexa and Google’s Assistant. 2.4 Fully Data-Driven Conversation Models and Social Bots Social bots (also known as chatbots) are of growing importance in facilitating smooth interaction between humans and their electronic devices. Recently, researchers have begun to explore fully data-driven generation of conversational responses within the framework of neural machine transla3 Contributions and"
P18-5002,N15-1020,1,0.847016,"l task-oriented dialogue system. It consists of (1) a natural language understanding (NLU) 3 dialogue QA task-oriented chatbot top-level bot state understanding of user query intent understanding of user goal conversation history and user intent understanding of user top-level intent action clarification questions or answers dialogue-act and slot/value reward relevance of answer # of dialogue turns task success rate # of dialogue turns response user engagement options daily/monthly usage Table 2: Reinforcement Learning for Dialogue. tion (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016). Such end-to-end models have been particularly successful with social bot scenarios, as they require little interaction with the user’s environment (no need for API calls) and such models cope well with free-form and open domain texts. However, neural responses are often too general to carry meaningful information, e.g., with the common response “I don’t know” which can serve as a reply to most user questions. A mutual information model is proposed by (Li et al., 2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c)."
P18-5002,N15-4004,1,0.881835,"Missing"
P18-5002,W17-5505,0,0.0375882,"Missing"
P19-1539,W18-5709,0,0.13562,"round responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse abo"
P19-1539,D18-1241,1,0.822774,"ent for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The functional combination of MRC"
P19-1539,D18-1045,0,0.0185929,"model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric i"
P19-1539,P18-1082,0,0.0333813,"used the pretrained GloVe8 for initialization. We set hidden dimensions to 512 and dropout rate to 0.4. GRU cells are used for S EQ 2S EQ and M EM N ET (we also tested LSTM cells and obtained similar results). We used the Adam optimizer for model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history b"
P19-1539,N19-1125,1,0.868164,"Missing"
P19-1539,W07-0734,0,0.0141127,"vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric is a variant of B LEU that weights n-gram matches by their information gain by effectively penalizing uninformative n-grams (such as “I don’t know”), which makes it a relevant metric for evaluating systems aiming diverse and informative responses. MT metrics may not be particularly adequate for our task (Liu et al., 2016), given its focus on the informativeness of responses, and for that reason we also use two other types of metrics to measure the level of grounding and diversity. As a diversity metric, we count all n-grams in the system output"
P19-1539,N16-1014,1,0.92711,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,P16-1094,1,0.958274,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,D16-1230,0,0.0886793,"Missing"
P19-1539,P18-1138,0,0.391323,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,P18-1157,1,0.938959,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,D15-1166,0,0.0725271,"n layer is applied to further ingest and capture the most salient information. The output memory, M ∈ Rd×n , is obtained by applying another BiLSTM layer for final information rearrangement. Note that d is the hidden size of the memory and n is the length of the document. 3.2 Response Generation Having read and processed both the conversation history and the extra knowledge in the document, the model then produces a free-form response y = (y1 , . . . , yT ) instead of generating a span or performing answer classification as in MRC tasks. We use an attentional recurrent neural network decoder (Luong et al., 2015) to generate response tokens while attending to the memory. At the beginning, the initial hidden state h0 is the weighted sum of the representation of the history X. For each decoding step t with a hidden state ht , we generate a token yt based on the distribution: p(yt ) = softmax((W1 ht + b)/τ ), (1) where τ > 0 is the softmax temperature. The hidden state ht is defined as follows: ht = W2 [zt ++fattention (zt , M )]. (2) Here, [· ++·] indicates a concatenation of two vectors; fattention is a dot-product attention (Vaswani et al., 2017); and zt is a state generated by GRU(et−1 , ht−1 ) with"
P19-1539,D18-1255,0,0.363735,"d-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to"
P19-1539,I17-1047,1,0.933114,"Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to continue a meaningful and informative conversation. Figure 1 i"
P19-1539,P02-1040,0,0.106889,"e decoding to draw yt from the above distribution p(yt ). Section 5 provides more details about the experimental configuration. 3.3 Data Weighting Scheme We further propose a simple data weighting scheme to encourage the generation of grounded responses. The idea is to bias the model training to fit better to those training instances where the ground-truth response is more closely relevant to the document. More specifically, given a training instance (X, D, y), we measure the closeness score c ∈ R between the document D and the gold response y (e.g., with the NIST (Doddington, 2002) or B LEU (Papineni et al., 2002) metrics). In each training data batch, we normalize the closeness scores of all the instances to have a sum of 1, and weight each of the instances with its corresponding normalized score when evaluating the 5429 # dialogues # utterances # documents # document sentences Train Valid Test 28.4k 2.36M 28.4k 15.18M 1.2k 0.12M 1.2k 0.58M 3.1k 0.34M 3.1k 1.68M 18.84 14.17 18.48 14.15 Average length (# words): utterances 18.74 document sentences 13.72 Table 1: Our grounded conversational dataset. training loss. This training regime promotes instances with grounded responses and thus encourages the mo"
P19-1539,D16-1264,0,0.230968,"ble at https://github.com/qkaren/ converse_reading_cmr. of turns X = (x1 , . . . , xM ) and a web document D = (s1 , . . . , sN ) as the knowledge source, where si is the ith sentence in the document. With the pair (X, D), the system needs to generate a natural language response y that is both conversationally appropriate and reflective of the contents of the web document. 3 Approach Our approach integrates conversation generation with on-demand MRC. Specifically, we use an MRC model to effectively encode the conversation history by treating it as a question in a typical QA task (e.g., SQuAD (Rajpurkar et al., 2016)), and encode the web document as the context. We then replace the output component of the MRC model (which is usually an answer classification module) with an attentional sequence generator that generates a free-form response. We refer to our approach as CMR (Conversation with on-demand Machine Reading). In general, any off-the-shelf MRC model could be applied here for knowledge comprehension. We use Stochastic Answer Networks (SAN)2 (Liu et al., 2018b), a performant machine reading model that until very recently held state-of-the-art performance on the SQuAD benchmark. We also employ a simpl"
P19-1539,Q19-1016,0,0.0224858,"ng meaning. from a given document for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The"
P19-1539,P15-1152,0,0.0338416,"fall without a parachute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017;"
P19-1539,P18-1205,0,0.0465991,"compared to machine translation, it is common for the generator to retain focus on the key information in the external document to produce semantically relevant responses. 7 Related Work Dialogue: Traditional dialogue systems (see (Jurafsky and Martin, 2009) for an historical perspective) are typically grounded, enabling these systems to be reflective of the user’s environment. The lack of grounding has been a stumbling block for the earliest end-to-end dialogue systems, as various researchers have noted that their outputs tend to be bland (Li et al., 2016a; Gao et al., 2019b), inconsistent (Zhang et al., 2018a; Li et al., Figure 3: Attention weights between words of the documents and words of the response. Dark (blue) cells represent probabilities closer to 1. 2016b; Zhang et al., 2019), and lacking in factual content (Ghazvininejad et al., 2018; Agarwal et al., 2018). Recently there has been growing interest in exploring different forms of grounding, including images, knowledge bases, and plain texts (Das et al., 2017; Mostafazadeh et al., 2017; Agarwal et al., 2018; Yang et al., 2019). A recent survey is included in Gao et al. (2019a). Prior work, e.g, (Ghazvininejad et al., 2018; Zhang et al.,"
P19-1539,N15-1020,1,0.77954,"hute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al.,"
P19-1539,N19-1423,0,\N,Missing
P19-3021,P19-1441,1,0.838229,"nt. Figure from (Gao et al., 2019b). Figure 1: An example of a basic multi-task configuration. Two encoder-decoder chains share a common decoder, alternately trained on separate datasets. coders. This architecture consists of additional personality embeddings, which are provided to the decoder alongside token embeddings at each timestep of decoding. Grounding generated responses helps condition outputs based on a given personality embedding: for the same query, the system learns to generate responses in different styles, all while preserving the underlying context. tations (Gao et al., 2019b; Liu et al., 2019). By unifying a conversational sequence-to-sequence model and an autoencoder with a shared decoder, multi-task learning can personalize the conversational model (Luan et al., 2017). Multi-task learning has potentially many other powerful applications for inducing biases in conversational systems. I CECAPS allows users to build arrays of models with arbitrary sharing of components, and place them in a multi-task learning environment. Users can construct arbitrary multi-task training schedules, assigning different tasks or balances among tasks per training step. 3 3.2 SpaceFusion (Gao et al., 20"
P19-3021,P18-1157,1,0.844079,"general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable during response generation; probabilities associated with these tokens are clamped to zero. The start-token censor-list is similar, but only masks the response’s first token. We also support infrequency filters; users may restrict the decoder from generating responses with rare words. provide informed responses with context about the real world, without needing comprehensive paired conversational data to embody that information. We provide an extension of stochastic answer networks (Liu et al., 2018), a machine reading comprehension system, that acts as a full knowledgegrounded conversation model (Qin et al., 2019), hybridizing machine reading comprehension with a response generation model. At a high level, this model consists of two deep biLSTMs in parallel that encode conversational context and knowledge, respectively. The information from these encoders is then combined using cross-attention, the output of which forms the basis of a memory cell that powers a response generator. 4 4.3 The standard beam-search implementation in TensorFlow works by iteratively generating tokens, generatin"
P19-3021,P18-4021,0,0.146101,"onversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can be chained together to form a single, end-to-end functional model. This chaining paradigm allows users"
P19-3021,I17-1061,1,0.82046,"atasets. coders. This architecture consists of additional personality embeddings, which are provided to the decoder alongside token embeddings at each timestep of decoding. Grounding generated responses helps condition outputs based on a given personality embedding: for the same query, the system learns to generate responses in different styles, all while preserving the underlying context. tations (Gao et al., 2019b; Liu et al., 2019). By unifying a conversational sequence-to-sequence model and an autoencoder with a shared decoder, multi-task learning can personalize the conversational model (Luan et al., 2017). Multi-task learning has potentially many other powerful applications for inducing biases in conversational systems. I CECAPS allows users to build arrays of models with arbitrary sharing of components, and place them in a multi-task learning environment. Users can construct arbitrary multi-task training schedules, assigning different tasks or balances among tasks per training step. 3 3.2 SpaceFusion (Gao et al., 2019b) is a learning paradigm that aligns latent spaces learned by different models trained over different datasets. Of particular interest is its application to neural conversation"
P19-3021,D15-1166,0,0.0461808,"me context to be placed nearby in latent space and aligning semantically related responses along straight lines in latent space. This induces a structure in the latent space such that distance and direction from a predicted response vector roughly correspond to relevance and diversity, respectively, as in Figure 2. Built-in modules and configurations I CECAPS provides several built-in modules and configurations. Most standard NLP architectures are available, including transformers (Vaswani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A cri"
P19-3021,D17-2014,0,0.0618856,"Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can be chained together to form a single, end-to-end functional model. This chainin"
P19-3021,N18-1202,0,0.0294649,"ons with their agents and directly observe their responses. Response generation is powered by the custom decoder described in Section 4. While the commandline session is useful for quick testing, for conveTraining configurations I CECAPS training configurations follow a basic five-phase pattern. We include example training scripts that users may use as templates. 126 learning models. It places a strong emphasis on sequence modeling baselines. AllenNLP (Gardner et al., 2018) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo (Peters et al., 2018). OpenNMT (Klein et al., 2017) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow. MarianNMT (Junczys-Dowmunt et al., 2018) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh. It is built in C++ and designed for fast training in multi-GPU systems. Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to I CE CAPS . It includes reinforc"
P19-3021,N19-1125,1,0.84795,"sed directly or loaded for fine-tuning or bootstrapping other models; these models power an online demo of our framework. 1 2 Architecture I CECAPS is designed for modularity, flexibility, and ease of use. Modules are built on top of TensorFlow Estimators, making them easy for developers to use and extend flexibly. I CECAPS supports arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-tu"
P19-3021,P19-1539,1,0.727317,"g response generation; probabilities associated with these tokens are clamped to zero. The start-token censor-list is similar, but only masks the response’s first token. We also support infrequency filters; users may restrict the decoder from generating responses with rare words. provide informed responses with context about the real world, without needing comprehensive paired conversational data to embody that information. We provide an extension of stochastic answer networks (Liu et al., 2018), a machine reading comprehension system, that acts as a full knowledgegrounded conversation model (Qin et al., 2019), hybridizing machine reading comprehension with a response generation model. At a high level, this model consists of two deep biLSTMs in parallel that encode conversational context and knowledge, respectively. The information from these encoders is then combined using cross-attention, the output of which forms the basis of a memory cell that powers a response generator. 4 4.3 The standard beam-search implementation in TensorFlow works by iteratively generating tokens, generating a constant number of hypotheses at the end of the decoding phase. I CECAPS implements a modified beam search decode"
P19-3021,W18-2501,0,0.128523,"rts arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and"
P19-3021,P16-1162,0,0.198835,"ke building complex dialogue systems intuitive for the end user. 5.1 Text data processing TensorFlow estimators expect to read data from TFRecord binary files for efficient processing. We provide a script TEXT DATA PROCESSING . PY for converting text data into TFRecords, equipped with several useful preprocessing transformations. Our script can sort data within local windows so that batches fed during training have minimal padding inefficiency. These batches can be shuffled amongst each other to mitigate any biases induced by sorting. We provide token preprocessing through byte pair encoding (Sennrich et al., 2016), which builds a token set at a level of abstraction between characters and words. This often allows for faster training and improved generalization. Another feature focused on conversational scenarios is fixed-length context extraction. Conversational data often contains large, potentially unwieldy multi-turn contexts; we can limit our data samples to a desired context length. We also provide an option for annotating datasets with topic grounding information, by analyzing the data for unique tokens to use as topic markers. 5.2 Training the system. The system is now ready to train: a single ca"
P19-3021,W18-2503,0,0.111568,"NLP (Gardner et al., 2018) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo (Peters et al., 2018). OpenNMT (Klein et al., 2017) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow. MarianNMT (Junczys-Dowmunt et al., 2018) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh. It is built in C++ and designed for fast training in multi-GPU systems. Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to I CE CAPS . It includes reinforcement learning capabilities alongside its sequence modelling tools. A few other toolkits have a dialog emphasis. DeepPavlov (Burtsev et al., 2018) is a deep learning library with a focus on task-oriented dialogue. It provides demos and pre-trained models for tasks such as question answering and sentiment classification. Affiliated with DeepPavlov is the ConvAI2 challenge (Dinan et al., 2019), a general dialogue competition featuring a synthetic"
P19-3021,P18-4020,0,0.0368469,"Missing"
P19-3021,W18-1819,0,0.0298992,"exibly. I CECAPS supports arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements"
P19-3021,P17-4012,0,0.176011,"tures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can b"
P19-3021,N16-1014,1,0.717728,"ani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A critical task in building intelligent conversational agents is grounding their responses in an external knowledge base. This allows agents to 124 agent to avoid profanities or other offensive language. Likewise, the system should avoid obvious ungrammatical outputs, such as broken abbreviations or nonsensical punctuation marks. I CECAPS supports several filters, including a general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable d"
P19-3021,P16-1094,1,0.920476,"ani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A critical task in building intelligent conversational agents is grounding their responses in an external knowledge base. This allows agents to 124 agent to avoid profanities or other offensive language. Likewise, the system should avoid obvious ungrammatical outputs, such as broken abbreviations or nonsensical punctuation marks. I CECAPS supports several filters, including a general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable d"
S14-1015,W13-1302,0,0.00932687,"n. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. Attributes are a good intermedia"
S14-1015,D10-1049,0,0.040539,"been widely used in object recognition (Felzenszwalb et al., 2010). Yet, no work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations,"
S14-1015,D09-1030,0,0.020787,"bias from annotators’ perception about which objects are important, since one of the problems we would like to solve is content selection. This dataset will be available for future experiments. We built on the dataset from (Rashtchian et al., 2010) which contained 8,000 Flickr images and associated descriptions gathered using Amazon Mechanical Turk (MTurk). Restricting ourselves to Creative Commons images, we sampled 500 images for annotation. We collected annotations of images in three stages using MTurk, and assigned each annotation task to 3-5 workers to improve quality through redundancy (Callison-Burch, 2009). Below we describe the process for annotating a single image. Stage 1: We prompted five turkers to list all objects in an image, ignoring objects that are parts of larger objects (e.g., the arms of a person), which we collected later in Stage 3. This list also included groups, such as crowds of people. Stage 2: For each unique object label from Stage 1, we asked two turkers to draw a polygon around the object identified.3 In cases where the object is a group, we also asked for the number of objects present (1-6 or many). Finally, we created a list of all references to the object from the firs"
S14-1015,P12-1038,0,0.0632436,"halving the difference from human performance to two baselines on 4-gram BLEU. In ablations, we measure the importance of different annotations and visual cues, showing that annotation of activities and relative bounding box information between objects are crucial to generating human-like description. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triple"
S14-1015,de-marneffe-etal-2006-generating,0,0.104217,"Missing"
S14-1015,D13-1128,0,0.219833,"Missing"
S14-1015,E12-1076,0,0.27079,"t surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. Attributes are a good intermediate representation for categorization (Farhadi"
S14-1015,N13-1137,0,0.0823812,"Missing"
S14-1015,W99-0604,0,0.0902418,"ring something true in the image; and salience, which sentence is capturing important things in the image while still being concise. Two annotators annotated all test pairs for all criteria for a given pair of systems. Six annotators were used (none authors) and agreement was high (Cohen’s kappa = 0.963, 0.823 and 0.703 for grammar, truth and salience). Machine Translation Baseline The first baseline is designed to see if it is possible to generate good sentences from the facet string labels alone, with no visual information. We use an extension of phrase-based machine translation techniques (Och et al., 1999). We created a virtual bitext by pairing each image description (the target sentence) with a sequence10 of visual identifiers (the source “sentence”) listing strings from the facet labels. Since phrases produced by turkers lack many of the functions words needed to create fluent sentences, we added one of 47 function words either at the start or the end of each output phrase. The translation model included standard features such as language model score (using our caption language model described previously), word count, phrase count, linear distortion, and the count of deleted source words. We"
S14-1015,P10-1126,0,0.0977587,"Missing"
S14-1015,P03-1021,0,0.0393045,"s computation is intractable because we need to consider all possible sentences, so we use beam search for strings up to a fixed length. Reranking Generating directly from the process in Figure 3 results in sentences that may be short and repetitive because the model score is a product of locally normalized distributions. The reranker takes as input a candidate list c, for an image I, as decoded from the generative model. The candidate list includes the top-k scoring hypotheses for each sentence length up to a fixed maximum. A linear scoring function is used for reranking optimized with MERT (Och, 2003) to maximize BLEU-2. 5 Features We construct indicator features to capture variation in usage in different parts of the sentence, types of objects that are mentioned, visual salience, and semantic and visual coordination between objects. The features are included in the maximum entropy models used to parameterize the distributions described in Figure 3. Whenever possible, we use WordNet Synsets (Miller, 1995) instead of lexical features to limit over-fitting. Features in the generative model use tests for local properties, such as the identity of a synset of a word in WordNet, conjoined with a"
S14-1015,D13-1197,1,0.58205,"adi et al., 2009). Activity recognition is an emerging area in images (Li and Fei-Fei, 2007; Yao et al., 2011; Sharma et al., 2013) and video (Weinland et al., 2011), although less studied than object recognition. Also, parts have been widely used in object recognition (Felzenszwalb et al., 2010). Yet, no work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have b"
S14-1015,2001.mtsummit-papers.68,0,0.0257801,"Missing"
S14-1015,P12-1039,0,0.0297731,"work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations, including the merged Object labels, as facets. These labels provide feature norms"
S14-1015,W10-0721,0,0.428906,"richly annotated images to approximate gold standard visual recognition. In collecting the data, we sought a visual annotation with sufficient coverage to support the generation of as many of the words in the original image descriptions as possible. We also aimed to make it as visually exhaustive as possible—giving equal treatment to all visible objects. This ensures less bias from annotators’ perception about which objects are important, since one of the problems we would like to solve is content selection. This dataset will be available for future experiments. We built on the dataset from (Rashtchian et al., 2010) which contained 8,000 Flickr images and associated descriptions gathered using Amazon Mechanical Turk (MTurk). Restricting ourselves to Creative Commons images, we sampled 500 images for annotation. We collected annotations of images in three stages using MTurk, and assigned each annotation task to 3-5 workers to improve quality through redundancy (Callison-Burch, 2009). Below we describe the process for annotating a single image. Stage 1: We prompted five turkers to list all objects in an image, ignoring objects that are parts of larger objects (e.g., the arms of a person), which we collecte"
S14-1015,J12-1006,0,0.0506618,"Missing"
S14-1015,D12-1130,0,0.153821,"ndaries and descriptive text, here including the facts that the children are “riding” and “walking” and that they are “young.” Our goal is to be as exhaustive as possible, giving equal treatment to all objects. For example, the annotations in Figure 1 contain enough information to generate the first three sentences and most of the content in the remaining two. Labels gathered in this way are a type of feature norms (McRae et al., 2005), which have been used in the cognitive science literature to approximate human perception and were recently used as a visual proxy in distributional semantics (Silberer and Lapata, 2012). We present the first effort, that we are aware of, for using feature norms to study image description generation. Such rich data allows us to develop significantly more comprehensive generation models. We divide generation into choices about which visual content to select and how to realize a sentence that describes that content. Our approach is grammarbased, feature-rich, and jointly models both decisions. The content selection model includes latent variables that align phrases to visual objects and features that, for example, measure how visual salience and spatial relationships influence"
S14-1015,P13-1056,0,0.0159995,"a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations, including the merged Object labels, as facets. These labels provide feature norms (McRae et al., 2005), which have recently used as a visual proxy in distributional semantics (Silberer and Lapata, 2012; Silberer et al., 2013) but have not been previous studied for generation. This annotation of 500 images (2500 sentences) yielded over 4000 object instances and 100,000 textual labels. Dataset We collected a dataset of richly annotated images to approximate gold standard visual recognition. In collecting the data, we sought a visual annotation with sufficient coverage to support the generation of as many of the words in the original image descriptions as possible. We also aimed to make it as visually exhaustive as possible—giving equal treatment to all visible objects. This ensures less bias from annotators’ percept"
S14-1015,P13-1006,0,0.0292359,"human-like description. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. At"
S14-1015,W04-0216,0,0.0134982,"Missing"
S14-1015,D11-1041,0,\N,Missing
S14-1015,P02-1040,0,\N,Missing
S14-1015,W10-0707,0,\N,Missing
W06-1643,W04-1013,0,0.0637504,"Missing"
W06-1643,W05-0905,0,0.455979,"which the locally-normalized model estimates the cost of predicting yˆt = 1 given a label history y ˆ1:t−1 . This ensures that we have a well-formed probability distribution at each time slice, while capitalizing on the good performance of CRF models. Table 3: Features for extractive summarization. Unless otherwise mentioned, we refer to features of utterance t whose label yt we are trying to predict. 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al., 2004; Maskey and Hirschberg, 2005; Murray et al., 2005) or text summarization (Mani and Maybury, 1999). Our goal is to build a very competitive feature set that capitalizes on recent advances in summarization of both genres. Table 3 lists some important features. There is strong evidence that lexical cues such as “significant” and “great” are strong predictors in many summarization tasks (Edmundson, 1968). Such cues are admittedly quite genre specific, so we did not want to commit ourselves to any specific list, which may not carry over well to our specific speech domain, and we automatically selected a list of n-grams (n ≤ 3) using crossvalidatio"
W06-1643,N04-1019,0,0.584687,"t compensation.1-13 Did you have a look at meeting digits if they have a them?14-26 I didn&apos;t. No.27-29 Hmm.30 No. The DC component is negligible. All mikes have DC removal.31-41 Yeah.42 Because there&apos;s a sample and hold in the A-to-D.43-51 And I also, um, did some experiments about normalizing the phase.52-62 And came up with a web page people can take a look at.63-75 Peer (len=22): 1-13 43-51 Optimal (len=22): 31-41 52-62 Figure 2: Model, peer, and “optimal” summaries are all extracts taken from the same transcription. quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). Pyramid and ROUGE are techniques looking for content units repeated in different model summaries, i.e., summary content units (SCUs) such as clauses and noun phrases for the Pyramid method, and ngrams for ROUGE. The underlying hypothesis is that different model sentences, clauses, or phrases may convey the same meaning, which is a reasonable assumption when dealing with reference summaries produced by different authors, since it is quite unlikely that any two abstractors would use the exact same words to convey the same idea. gram with the class variable, and selected"
W06-1643,N04-4027,0,0.166045,"Missing"
W06-1643,N03-1028,0,0.0595615,"Missing"
W06-1643,C04-1128,0,0.0928243,"Missing"
W06-1643,W04-2319,0,0.0507974,"Missing"
W06-1643,J02-4003,0,0.149753,"Missing"
W06-1643,P05-1037,0,0.0813897,"Missing"
W06-1643,J96-1002,0,0.00730404,"Missing"
W06-1643,P05-1045,0,0.0397247,"Missing"
W06-1643,P04-1085,1,0.742673,"ntiguous labels yt−1 and yt seem to seldom influence each other, the correlation between AP elements ys and yd is particularly strong, and they have a tendency to be either both included or both excluded. Note that the second table is not symmetric, because the data allows an A part to be linked to multiple B parts, but not vice-versa. While counts in Table 2 reflect human labels, we only use automatically predicted (s, d) pairs in the experiments of the remaining part of this paper. To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al., 2004). 4 y1 y2 y3 y4 y5 x1 x2 x3 x4 x5 Statement BackChannel Statement Statement Statement Figure 1: A skip-chain CRF with pragmatic-level links. Linear-chain edges yt−1 = 1 yt−1 = −1 Skip-chain edges ys = 1 ys = −1 yt = 1 529 7742 yd = 1 6792 1479 yt = −1 7742 116040 yd = −1 2191 121591 Table 2: Contingency tables: while the correlation between adjacent labels yt−1 and yt is not significant (χ2 = 2.3, p &gt; .05), empirical evidence clearly shows that ys and yd influence each other (χ2 = 78948, p &lt; .001). summarization predictors (see Section 6), and the binary sequence y = y1:T = (y1 , . . . , yT )"
W06-1643,P94-1002,0,0.0540124,"the turn · speech rate · min/max/mean/median/stddev/onset/outset f0 of utterance t, and of first and last word · min/max/mean/stddev energy · .05, .25, .5, .75, .95 quantiles of f0 and energy · pitch range · f0 mean absolute slope Durational and structural features: · duration of the previous/current/next utterance · relative position within meeting (i.e., index t) · relative position within speaker turn · large number of structural predicates, i.e. “is the previous utterance of the same speaker?” · number of APs initiated in yt Discourse features: · lexical cohesion score (for topic shifts) (Hearst, 1994) · first and second word of utterance, if in cue word list · number of pronouns · number of fillers and fluency devices (e.g., “uh”, “um”) · number of backchannel and acknowledgment tokens (e.g., “uh-huh”, “ok”, “right”) BNs assign probability distributions over entire sequences by estimating the probability of each individual instance yt in the sequence (Equation 1), and seem thus particularly suited for ranking utterances. A first approach is then to rank utterances according to the cost of predicting yt = 1 at each time step on the Viterbi path. While these costs are well-formed (negative l"
W08-0336,N06-2013,0,0.0191538,"the role of a phrase table is to build domain and application appropriate larger chunks that are semantically coherent in the translation process. For example, even if the word for smallpox is treated as two one-character words, they can still appear in a phrase like “ →smallpox”, so that smallpox will still be a candidate translation when the system translates “ ” “ ”. Nevertheless, Xu et al. (2004) show that an MT system with a word segmenter outperforms a system working with individual characters in an alignment template approach. On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. U s U s Beyond this, there has been no finer-grained analysis of what style and size of word segmentation is optimal for MT. Moreover, most discussion of segmentation for other tasks relates to the size units to identify in the segmentation standard: whether to join or split noun compounds, for instance. People 224 Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics ge"
W08-0336,E03-1076,0,0.0686921,"d be optimal. This is because the role of a phrase table is to build domain and application appropriate larger chunks that are semantically coherent in the translation process. For example, even if the word for smallpox is treated as two one-character words, they can still appear in a phrase like “ →smallpox”, so that smallpox will still be a candidate translation when the system translates “ ” “ ”. Nevertheless, Xu et al. (2004) show that an MT system with a word segmenter outperforms a system working with individual characters in an alignment template approach. On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. U s U s Beyond this, there has been no finer-grained analysis of what style and size of word segmentation is optimal for MT. Moreover, most discussion of segmentation for other tasks relates to the size units to identify in the segmentation standard: whether to join or split noun compounds, for instance. People 224 Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, c Columbus, Ohio, USA, June 2008. 2008 Association for"
W08-0336,N03-1017,0,0.0587278,"Missing"
W08-0336,W06-0115,0,0.0202511,"Missing"
W08-0336,N06-2024,0,0.0124782,"was neither specifically designed nor optimized for MT, it seems reasonable to investigate whether any segmentation granularity in continuum between character-level and CTB-style segmentation is more effective for MT. In this section, we present a technique for directly optimizing a segmentation property—characters per token average— for translation quality, which yields significant improvements in MT performance. In order to calibrate the average word length produced by our CRF segmenter—i.e., to adjust the rate of word boundary predictions (yt = +1), we apply a relatively simple technique (Minkov et al., 2006) originally devised for adjusting the precision/recall tradeoff of any sequential classifier. Specifically, the weight vector w and feature vector of a trained linear sequence classifier are augmented at test time to include new class-conditional feature functions to bias the classifier towards particular class labels. In our case, since we wish to increase the frequency of word boundaries, we add a feature function:  1 if yt = +1 f0 (x, yt−1 , yt ,t) = 0 otherwise Its weight λ0 controls the extent of which the classifier will make positive predictions, with very large positive λ0 values caus"
W08-0336,W04-3236,0,0.00953984,"over-generates a big MT training lexicon and OOV words in MT test data, and thus causes a problem for MT. To improve a feature-based sequence model for MT, we propose 4 different approaches to deal with named entities, optimal length of word for MT and joint search for segmentation and MT decoding. 5.1 Making Use of External Lexicons One way to improve the consistency of the CRF model is to make use of external lexicons (which are not part of the segmentation training data) to add lexicon-based features. All the features we use are listed in Table 6. Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005). There are three categories of features: Lexicon-based Features (1.1) LBegin (Cn ), n ∈ [−2, 1] (1.2) LMid (Cn ), n ∈ [−2, 1] (1.3) LEnd (Cn ), n ∈ [−2, 1] (1.4) LEnd (C−1 ) + LEnd (C0 ) +LEnd (C1 ) (1.5) LEnd (C−2 ) + LEnd (C−1 ) +LBegin (C0 ) + LMid (C0 ) (1.6) LEnd (C−2 ) + LEnd (C−1 ) +LBegin (C−1 ) +LBegin (C0 ) + LMid (C0 ) Segmentation Performance F measure OOV Recall CRF-Lex-NR 0.943 0.753 CRF-Lex 0.940 0.729 MT Performance Segmenter MT03 (dev) MT05 (test) CRF-Lex-NR 32.96 31.27 CRF-Lex 32.70 30.95 Linguistic Features (2.1) Cn , n ∈ [−2, 1] (2.2) Cn−1Cn , n ∈"
W08-0336,J03-1002,0,0.0118059,"Missing"
W08-0336,P03-1021,0,0.0842894,") alignments, and using Moses’ grow-diag alignment symmetrization heuristic.1 We set the maximum phrase length to a large value (10), because some segmenters described later in this paper will result in shorter 1 In our experiments, this heuristic consistently performed better than the default, grow-diag-final. words, therefore it is more comparable if we increase the maximum phrase length. During decoding, we incorporate the standard eight feature functions of Moses as well as the lexicalized reordering model. We tuned the parameters of these features with Minimum Error Rate Training (MERT) (Och, 2003) on the NIST MT03 Evaluation data set (919 sentences), and then test the MT performance on NIST MT03 and MT05 Evaluation data (878 and 1082 sentences, respectively). We report the MT performance using the original BLEU metric (Papineni et al., 2001). All BLEU scores in this paper are uncased. The MT training data was subsampled from GALE Year 2 training data using a collection of character 5-grams and smaller n-grams drawn from all segmentations of the test data. Since the MT training data is subsampled with character n-grams, it is not biased towards any particular word segmentation. The MT t"
W08-0336,2001.mtsummit-papers.68,0,0.0232942,"heuristic consistently performed better than the default, grow-diag-final. words, therefore it is more comparable if we increase the maximum phrase length. During decoding, we incorporate the standard eight feature functions of Moses as well as the lexicalized reordering model. We tuned the parameters of these features with Minimum Error Rate Training (MERT) (Och, 2003) on the NIST MT03 Evaluation data set (919 sentences), and then test the MT performance on NIST MT03 and MT05 Evaluation data (878 and 1082 sentences, respectively). We report the MT performance using the original BLEU metric (Papineni et al., 2001). All BLEU scores in this paper are uncased. The MT training data was subsampled from GALE Year 2 training data using a collection of character 5-grams and smaller n-grams drawn from all segmentations of the test data. Since the MT training data is subsampled with character n-grams, it is not biased towards any particular word segmentation. The MT training data contains 1,140,693 sentence pairs; on the Chinese side there are 60,573,223 non-whitespace characters, and the English sentences have 40,629,997 words. Our main source for training our five-gram language model was the English Gigaword c"
W08-0336,C02-1148,0,0.0871509,"flower). Without a standardized notion of a word, traditionally, the task of Chinese word segmentation starts from designing a segmentation standard based on linguistic and task intuitions, and then aiming to building segmenters that output words that conform to the standard. One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (Xue et al., 2005). It has been recognized that different NLP applications have different needs for segmentation. s U Us Chinese information retrieval (IR) systems benefit from a segmentation that breaks compound words into shorter “words” (Peng et al., 2002), paralleling the IR gains from compound splitting in languages like German (Hollink et al., 2004), whereas automatic speech recognition (ASR) systems prefer having longer words in the speech lexicon (Gao et al., 2005). However, despite a decade of very intense work on Chinese to English machine translation (MT), the way in which Chinese word segmentation affects MT performance is very poorly understood. With current statistical phrase-based MT systems, one might hypothesize that segmenting into small chunks, including perhaps even working with individual characters would be optimal. This is b"
W08-0336,C04-1081,0,0.0385509,"Missing"
W08-0336,I05-3027,1,0.133632,"T training lexicon and OOV words in MT test data, and thus causes a problem for MT. To improve a feature-based sequence model for MT, we propose 4 different approaches to deal with named entities, optimal length of word for MT and joint search for segmentation and MT decoding. 5.1 Making Use of External Lexicons One way to improve the consistency of the CRF model is to make use of external lexicons (which are not part of the segmentation training data) to add lexicon-based features. All the features we use are listed in Table 6. Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005). There are three categories of features: Lexicon-based Features (1.1) LBegin (Cn ), n ∈ [−2, 1] (1.2) LMid (Cn ), n ∈ [−2, 1] (1.3) LEnd (Cn ), n ∈ [−2, 1] (1.4) LEnd (C−1 ) + LEnd (C0 ) +LEnd (C1 ) (1.5) LEnd (C−2 ) + LEnd (C−1 ) +LBegin (C0 ) + LMid (C0 ) (1.6) LEnd (C−2 ) + LEnd (C−1 ) +LBegin (C−1 ) +LBegin (C0 ) + LMid (C0 ) Segmentation Performance F measure OOV Recall CRF-Lex-NR 0.943 0.753 CRF-Lex 0.940 0.729 MT Performance Segmenter MT03 (dev) MT05 (test) CRF-Lex-NR 32.96 31.27 CRF-Lex 32.70 30.95 Linguistic Features (2.1) Cn , n ∈ [−2, 1] (2.2) Cn−1Cn , n ∈ [−1, 1] (2.3) Cn−2Cn , n"
W08-0336,W04-1118,0,0.206696,"y understood. With current statistical phrase-based MT systems, one might hypothesize that segmenting into small chunks, including perhaps even working with individual characters would be optimal. This is because the role of a phrase table is to build domain and application appropriate larger chunks that are semantically coherent in the translation process. For example, even if the word for smallpox is treated as two one-character words, they can still appear in a phrase like “ →smallpox”, so that smallpox will still be a candidate translation when the system translates “ ” “ ”. Nevertheless, Xu et al. (2004) show that an MT system with a word segmenter outperforms a system working with individual characters in an alignment template approach. On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. U s U s Beyond this, there has been no finer-grained analysis of what style and size of word segmentation is optimal for MT. Moreover, most discussion of segmentation for other tasks relates to the size units to identify in the segmentation standard: whether to join"
W08-0336,P02-1040,0,\N,Missing
W08-0336,J05-4005,0,\N,Missing
W08-0336,D08-1076,0,\N,Missing
W09-0404,W05-0909,0,0.112089,"tion. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which 2 2.1 Textual Entailment for MT Evaluation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between tw"
W09-0404,P03-1021,0,0.00708576,"del. We plan to assess the additional benefit of the full entailment feature set against the T RAD M T feature set extended by a proper lexical similarity metric, such as METEOR. The computation of entailment features is more heavyweight than traditional MT evaluation metrics. We found the speed (about 6 s per hypothesis on a current PC) to be sufficient for easily judging the quality of datasets of the size conventionally used for MT evaluation. However, this may still be too expensive as part of an MT model that directly optimizes some performance measure, e.g., minimum error rate training (Och, 2003). Feature Weights. Finally, we assessed the importance of the different entailment feature groups in the RTE model.1 Since the presence of correlated features makes the weights difficult to interpret, we restrict ourselves to two general observations. First, we find high weights not only for the score of the alignment between hypothesis and reference, but also for a number of syntacto-semantic match and mismatch features. This means that we do get an additional benefit from the presence of these features. For example, features with a negative effect include dropping adjuncts, unaligned root no"
W09-0404,W07-0411,0,0.0306635,"Missing"
W09-0404,E06-1032,0,0.0370947,"quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an M"
W09-0404,P02-1040,0,0.0809369,"combination of lexical and structural features that model the matches and mismatches between system output and reference translation. We use supervised regression models to combine these features and analyze feature weights to obtain further insights into the usefulness of different feature types. Introduction Automatic metrics to assess the quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement"
W09-0404,W08-0309,0,0.108416,"Missing"
W09-0404,2006.amta-papers.25,0,0.157306,"Missing"
W09-0404,P08-1007,0,0.0429762,"of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which 2 2.1 Textual Entailment for MT Evaluation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences ("
W09-0404,P06-1057,0,0.0216052,"valuation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over ∗ This paper is based on work funded by the Defense Advanced Research Pro"
W09-0404,W08-0332,0,0.0303371,"Missing"
W09-0404,P06-1114,0,0.0608647,"tailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. Th"
W09-0404,C08-1066,1,0.823965,"re are also substantial differences between TE and MT evaluation. Crucially, TE assumes the premise and hypothesis to be well-formed sentences, which is not true in MT evaluation. Thus, a possible criticism to the use of TE methods is that the features could become unreliable for ill-formed MT output. However, there is a second difference between the tasks that works to our advantage. Due to its strict compositional nature, TE requires an accurate semantic analysis of all sentence parts, since, for example, one misanalysed negation or counterfactual embedding can invert the entailment status (MacCartney and Manning, 2008). In contrast, human MT judgments behave more additively: failure of a translation with respect to a single semantic dimension (e.g., polarity or tense) degrades its quality, but usually not crucially so. We therefore expect that even noisy entailment features can be predictive in MT evaluation. 2.2 Table 1: Entailment feature groups provided by the Stanford RTE system, with number of features quadratic in the number of systems. On the other hand, it can be trained on more reliable pairwise preference judgments. In a second step, we combine the individual decisions to compute the highest-likel"
W09-0404,N06-1006,1,\N,Missing
W12-3159,2004.iwslt-evaluation.13,0,0.0191764,"h space. Under a constrained decoding setting, it appears that a large beam size seldom affects translation speed, but this is misleading and largely due to constraints created by the lattice. We thus evaluate the latticeconstrained case without tuning ‘search’ features, and find that direct search is significantly faster using lattice-constrained, with only a slight degrada477 tion of translation quality. Lattice constraints are augmented 2-5 times before it converges. 7 Related work The use of derivative-free optimization methods to tune machine translation parameters has been tried before. Bender et al. (2004) used the Nelder-Mead method to tune model parameters for a phrase-based translation system. However, their way of making direct search fast and practical is to set distortion limit to zero, which results in poor translation quality for many language pairs. Zens et al. (2007) also use the Nelder-Mead method to tune parameters in a log-linear model to maximize expected BLEU. Zhao and Chen (2009) proposes changes to Nelder-Mead method to better fit parameter tuning in their machine translation setting. They show the modification brings better search of parameters over the regular Nelder-Mead met"
W12-3159,D11-1003,0,0.0153475,"commonly used to combat the fact the search space is highly non-convex, often with multiple minima. Several problems still remain with MERT, three of which are addressed by this work. First, the N best error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N -best error surface is not guaranteed to be any close to an optimum of the true error surface. Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011). MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors. Third, MERT is only used to tune linear model parameters, yet SMT systems have many free decoder parameters—such as distortion limit and beam size—that are not handled by MERT. MERT does not provide a principled way to set these parameters. In order to overcome these issues, we explore the application of direct search methods (Wright, 1995) to SMT. To do this, we integrate the decoder and the eva"
W12-3159,P05-1033,0,0.0261495,"o the significance test makes direct search reasonably fast, since Racing is effective during the initial steps of search (when steps tend to be relatively big, and when differences in error rate are pretty significant), and our modification to randomization tests helps while search converges towards an optimum using increasingly smaller steps. 5.2 Lattice-based decoding We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al., 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005). The successive expansion of translation options in order to construct the search graph is generally done from scratch, but this can be wasteful when the same sentences are translated multiple times, as it is the case with direct search. Even when the parameters of the decoder change across function evaluations, some partial translation are likely to be constructed multiple times, and this is more likely to happen when changes in parameters are relatively small. To overcome this inefficiency, we memoize hypotheses expansions made in all function evaluations, which then allows us to reuse some"
W12-3159,1993.eamt-1.1,0,0.199687,"ions. Since functions in downhill simplex are evaluated in sequence and not in parallel, our solution is to race the current model against our current best model.4 When the evaluation of a model M is interrupted because it is deemed significantly worse ˆ , the error rate of M than the current best model M on the entire development set is extrapolated from its relative performance on the decoded subset.5 The second main difference with the LOOCV case is that we do not use confidence intervals to determine which of two or models are best. In SMT, it is common to use either bootstrap resampling (Efron and Tibshirani, 1993; Och, 2003) or randomization tests (Noreen, 1989). In this paper, we use the randomization test for discarding unpromising models, since this statistical test was shown to be less likely to cause type-I errors6 than bootstrap methods (Riezler and Maxwell, 2005). Since both kinds of statistical tests involve a time-consuming sampling step, it 4 Since Racing only discards suboptimal models, the current best model M ∗ is one for which we have decoded the entire development set. Once a new model M is evaluated, we perform at step j a significance test to determine whether M ’s translation of sent"
W12-3159,D08-1089,1,0.879201,"ds unless language model supports the reordering. Since having a large distortion limit leads to slower decoding, having the smallest possible distortion limit that still facilitates correct reordering would be ideal. Not only this speeds up translation, but this also leads to better translation quality by minimizing search errors. Since a larger distortion limit means there are more possible re-orderings of translations, it is prone to more search errors. In fact, there are evidences that tuning the distortion limit is beneficial in improving quality of translation by limiting search errors. Galley and Manning (2008) conduct a line search along increments of distortion limit and separately tune the translation model parameters for each increment of distortion limit. The result shows significant difference in translation quality when distortion limit is tuned along with the model parameters. Separately tuning model parameters for different distortion limit is necessary because model parameters are coupled with distortion limit. A representative example: when distortion limit is zero, the distortion penalty feature can have any weight and not affect BLEU scores, but this is not the case when distortion limi"
W12-3159,D11-1004,1,0.868369,"rch errors. Third, MERT is only used to tune linear model parameters, yet SMT systems have many free decoder parameters—such as distortion limit and beam size—that are not handled by MERT. MERT does not provide a principled way to set these parameters. In order to overcome these issues, we explore the application of direct search methods (Wright, 1995) to SMT. To do this, we integrate the decoder and the evaluation metric inside the objective function, 1 The optimum found by MERT (Och, 2003) is generally not globally optimal. An alternative that optimizes N -best lists exactly is presented by Galley and Quirk (2011), and we do not discuss it further here. 468 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 468–479, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences. Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Nelder and Mead, 1965) and Powell’s method (Powell, 1964), which generally handle such diffi"
W12-3159,N10-1129,1,0.923067,"istortion limit is needed to accommodate the correct reordering due to typological difference between two languages. jump. The total distortion penalty is calculated as follows: X D(e, f ) = λd |dj |pd j where λd is the weight for distortion penalty feature, and dj is the size of the jump needed to translate the j-th phrase pair. For example, in Figure 2, the total distortion penalty feature value is 11, which is multiplied with λd to get the total distortion cost of translating the example sentence. Although pd is typically set to one (linear), one may consider polynomial distortion penalty (Green et al., 2010). Green et al. (2010) show that setting pd to a higher value than one improves the translation quality, but uses a predetermined value for pd . Instead of manually setting the value of pd , it can be given a value tuned with direct search. Although we only discussed distortion penalty here, it is straightforward to tune pi for each feature hi (e, f )pi using direct error rate minimization, where hi (e, f ) is any linear model feature of the decoder. 4.3 Future cost estimates Since beam search involves pruning, it is crucial to have good future cost estimation in order to minimize the number of"
W12-3159,N03-1017,0,0.0160119,"2010) show that setting pd to a higher value than one improves the translation quality, but uses a predetermined value for pd . Instead of manually setting the value of pd , it can be given a value tuned with direct search. Although we only discussed distortion penalty here, it is straightforward to tune pi for each feature hi (e, f )pi using direct error rate minimization, where hi (e, f ) is any linear model feature of the decoder. 4.3 Future cost estimates Since beam search involves pruning, it is crucial to have good future cost estimation in order to minimize the number of search errors (Koehn et al., 2003). The concept of future cost estimation is related to heuristic functions in the A* search algorithm. The total cost f (x) of a partial translation hypothesis is estimated by combining g(x), which is the actual current cost from the beginning of a sentence to point x and h(x), which is the future cost 5 estimate from point x to the end of the sentence: f (x) = g(x) + h(x) 4 In SMT decoding, the same feature weight vector is generally used when computing g(x) and h(x). However, this may not be ideal since future cost estimators use different heuristics depending on the features. For example, th"
W12-3159,P07-2045,0,0.0173828,"se of the previous ones, so constraining lattices grow over time. The two stopping criteria are similar to MERT: if the norm of the difference between the previous parameter vector—including λ and θ—and the current vector falls below a predefined tolerance value, we do not continue to the next iteration. Alternatively, if a new pass of unconstrained decoding generates lattices that are subsumed by lattices constructed at previous iteration, we stop and do not run the next optimization step. 6 Experiments 6.1 Setup For our experiments, we use a phrase-based translation system similar to Moses (Koehn et al., 2007). Our decoder uses many of the same features as Moses, including four phrasal and lexicalized translation scores, phrase penalty, word penalty, a language model score, linear distortion, and six lexicalized reordering scores. Unless specified otherwise, the decoder’s stack size is 50, and the number of translation options per input phrase is 25. Table 1 summarizes the amount of training data used to train translation systems from Korean, Arabic, and Farsi into English. These data sets are drawn from various sources, which include news, web, and technical data, as well as United Nations data in"
W12-3159,P09-1019,0,0.038156,"is an important part of building good machine translation systems. MERT minimizes an arbitrary loss function, usually an evaluation metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) from a surrogate representation of the search space, such as the N -best candidate translations of a development set. Much of the recent work ∗ This research was conducted during the author’s internship at Microsoft Research. on minimum error rate training focused on improving the method by Och (2003). Recent efforts extended MERT to work on lattices (Macherey et al., 2008) and hypergraphs (Kumar et al., 2009). Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple minima. Several problems still remain with MERT, three of which are addressed by this work. First, the N best error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N -best error surface is not guaranteed to be any close to an optimum of the true error surface. Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an"
W12-3159,D08-1076,0,0.204526,"ptimizing linear model parameters, which is an important part of building good machine translation systems. MERT minimizes an arbitrary loss function, usually an evaluation metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) from a surrogate representation of the search space, such as the N -best candidate translations of a development set. Much of the recent work ∗ This research was conducted during the author’s internship at Microsoft Research. on minimum error rate training focused on improving the method by Och (2003). Recent efforts extended MERT to work on lattices (Macherey et al., 2008) and hypergraphs (Kumar et al., 2009). Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple minima. Several problems still remain with MERT, three of which are addressed by this work. First, the N best error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N -best error surface is not guaranteed to be any close to an optimum of the true error surface. Second, most SMT decoders make search errors, yet MERT ignores"
W12-3159,C08-1074,0,0.0548856,"ion systems. MERT minimizes an arbitrary loss function, usually an evaluation metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) from a surrogate representation of the search space, such as the N -best candidate translations of a development set. Much of the recent work ∗ This research was conducted during the author’s internship at Microsoft Research. on minimum error rate training focused on improving the method by Och (2003). Recent efforts extended MERT to work on lattices (Macherey et al., 2008) and hypergraphs (Kumar et al., 2009). Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple minima. Several problems still remain with MERT, three of which are addressed by this work. First, the N best error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N -best error surface is not guaranteed to be any close to an optimum of the true error surface. Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact deco"
W12-3159,W99-0604,0,0.0836923,"pre-defined threshold, we only keep the best model.10 This adjustment to the significance test makes direct search reasonably fast, since Racing is effective during the initial steps of search (when steps tend to be relatively big, and when differences in error rate are pretty significant), and our modification to randomization tests helps while search converges towards an optimum using increasingly smaller steps. 5.2 Lattice-based decoding We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al., 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005). The successive expansion of translation options in order to construct the search graph is generally done from scratch, but this can be wasteful when the same sentences are translated multiple times, as it is the case with direct search. Even when the parameters of the decoder change across function evaluations, some partial translation are likely to be constructed multiple times, and this is more likely to happen when changes in parameters are relatively small. To overcome this inefficiency, we memoize hypotheses expansions"
W12-3159,P03-1021,0,0.644914,"function being optimized is the true error rate. Second, it lets us optimize parameters of translations systems other than standard linear model features, such as distortion limit. Since integrating the decoder into the minimizer is often too slow to be practical, we also exploit statistical significance tests to accelerate the search by quickly discarding unpromising models. Experiments with a phrasebased system show that our approach is scalable, and that optimizing the parameters that MERT cannot handle brings improvements to translation results. 1 Introduction Minimum error rate training (Och, 2003) is a common method for optimizing linear model parameters, which is an important part of building good machine translation systems. MERT minimizes an arbitrary loss function, usually an evaluation metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) from a surrogate representation of the search space, such as the N -best candidate translations of a development set. Much of the recent work ∗ This research was conducted during the author’s internship at Microsoft Research. on minimum error rate training focused on improving the method by Och (2003). Recent efforts extended M"
W12-3159,P02-1040,0,0.0956003,"is often too slow to be practical, we also exploit statistical significance tests to accelerate the search by quickly discarding unpromising models. Experiments with a phrasebased system show that our approach is scalable, and that optimizing the parameters that MERT cannot handle brings improvements to translation results. 1 Introduction Minimum error rate training (Och, 2003) is a common method for optimizing linear model parameters, which is an important part of building good machine translation systems. MERT minimizes an arbitrary loss function, usually an evaluation metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) from a surrogate representation of the search space, such as the N -best candidate translations of a development set. Much of the recent work ∗ This research was conducted during the author’s internship at Microsoft Research. on minimum error rate training focused on improving the method by Och (2003). Recent efforts extended MERT to work on lattices (Macherey et al., 2008) and hypergraphs (Kumar et al., 2009). Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple mi"
W12-3159,W05-0908,0,0.114121,"de the objective function forces the optimizer to account for possible search errors. Third, contrary to MERT, our approach does not require input parameters to be those of a linear model, so our approach can tune a broader range of features, including non-linear and hidden-state parameters (e.g., distortion limit, beam size, and weight vector applied to future cost estimates). In this paper, we make direct search reasonably fast thanks to two speedup techniques. First, we use a model selection acceleration technique called racing (Moore and Lee, 1994) in conjunction with randomization tests (Riezler and Maxwell, 2005) to avoid decoding the entire development set at each function evaluation. This approach discards the current model whenever performance on the translated subset of the development data is deemed significantly worse in comparison to the current best model. Second, we store and re-use search graphs across function evaluations, which eliminates some of the redundancy of regenerating the same translations in different optimization steps. Our experiments with a strong phrase-based translation system show that the direct search approach is an effective alternative to MERT. The speed of direct searc"
W12-3159,2006.amta-papers.25,0,0.0265889,"cal, we also exploit statistical significance tests to accelerate the search by quickly discarding unpromising models. Experiments with a phrasebased system show that our approach is scalable, and that optimizing the parameters that MERT cannot handle brings improvements to translation results. 1 Introduction Minimum error rate training (Och, 2003) is a common method for optimizing linear model parameters, which is an important part of building good machine translation systems. MERT minimizes an arbitrary loss function, usually an evaluation metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) from a surrogate representation of the search space, such as the N -best candidate translations of a development set. Much of the recent work ∗ This research was conducted during the author’s internship at Microsoft Research. on minimum error rate training focused on improving the method by Och (2003). Recent efforts extended MERT to work on lattices (Macherey et al., 2008) and hypergraphs (Kumar et al., 2009). Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple minima. Several problems still"
W12-3159,D07-1055,0,0.0946091,"nd that direct search is significantly faster using lattice-constrained, with only a slight degrada477 tion of translation quality. Lattice constraints are augmented 2-5 times before it converges. 7 Related work The use of derivative-free optimization methods to tune machine translation parameters has been tried before. Bender et al. (2004) used the Nelder-Mead method to tune model parameters for a phrase-based translation system. However, their way of making direct search fast and practical is to set distortion limit to zero, which results in poor translation quality for many language pairs. Zens et al. (2007) also use the Nelder-Mead method to tune parameters in a log-linear model to maximize expected BLEU. Zhao and Chen (2009) proposes changes to Nelder-Mead method to better fit parameter tuning in their machine translation setting. They show the modification brings better search of parameters over the regular Nelder-Mead method. Our work is related to the search-based structured prediction (SEARN) model of Daum´e (2006), in the sense that direct search also accounts for what happens during search (including search errors) to try to find parameters that are not only good for prediction, but for s"
W12-3159,N09-2006,0,0.0765432,"ion quality. Lattice constraints are augmented 2-5 times before it converges. 7 Related work The use of derivative-free optimization methods to tune machine translation parameters has been tried before. Bender et al. (2004) used the Nelder-Mead method to tune model parameters for a phrase-based translation system. However, their way of making direct search fast and practical is to set distortion limit to zero, which results in poor translation quality for many language pairs. Zens et al. (2007) also use the Nelder-Mead method to tune parameters in a log-linear model to maximize expected BLEU. Zhao and Chen (2009) proposes changes to Nelder-Mead method to better fit parameter tuning in their machine translation setting. They show the modification brings better search of parameters over the regular Nelder-Mead method. Our work is related to the search-based structured prediction (SEARN) model of Daum´e (2006), in the sense that direct search also accounts for what happens during search (including search errors) to try to find parameters that are not only good for prediction, but for search as well. 8 Conclusion This paper addressed the problem of minimizing error rate at a corpus level. We show that a t"
W15-3504,J93-2003,0,0.0364347,"Instead, we train a global model – similarly to Subotin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-to-String Translation We develop models for semantic-graph-to-string translation. These models are essentially discriminative translation models, relying on a decomposition structure similar to both maximum entropy language models and IBM Models 1, 2 (Brown et al., 1993), and the HMM translation model (Vogel et al., 1996). In particular, we see translation as a process of selecting target words in order conditioned on source language representation as well as prior target words. Similar to the IBM Models, we see each target word as being generated based on source concepts, though in our case the concepts are semantic graph nodes rather than surface words. That is, we assume the existence of an alignment, though it aligns the target words to Semantic Representation Our representation of sentence semantics is based on Logical Form (Vanderwende, 2015). LFs are l"
W15-3504,D07-1007,0,0.019894,"such as (Galley et al., 2004) operate on trees and have also been applied to semantic parsing problems (Li et al., 2013), Jones et al. (2012) generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon model (Mauser et al., 2009) and phrase-sense disambiguation (Carpuat and Wu, 2007) are perhaps the best known methods. Similarly to Carpuat and Wu (2007), we use the classifier to rescore phrasal translations, however we do not train a separate classifier for each source phrase. Instead, we train a global model – similarly to Subotin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-to-String Translation We develop models for semantic-gr"
W15-3504,N04-1035,1,0.675715,"by the free variable X. The logical form can be converted using a sequence of rules to a representation which conforms to the AMR specification (Vanderwende et al., 2015). We do not use the full conversion pipeline in our work, so our semantic graphs are somewhere between the LF and AMR. Notably, we keep the bits which serve as important features for the discriminative modeling of translation. mary. (Jones et al., 2012) presents an MT approach that can exploit semantic graphs such as AMR, in a continuation of earlier work that abstracted translation away from strings (Yamada and Knight, 2001; Galley et al., 2004). While rule extraction algorithms such as (Galley et al., 2004) operate on trees and have also been applied to semantic parsing problems (Li et al., 2013), Jones et al. (2012) generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon model (Mauser et al., 2009"
W15-3504,C12-1083,0,0.191367,"Missing"
W15-3504,N15-1114,0,0.0404792,". Finally text generation was applied. The level of analysis is somewhat arguable – sometimes it was purely syntactic, but in other cases it reached into the semantic domain. One of the earliest architectures was described in 1957 (Yngve, 1957). More contemporary examples of such systems include KANT (Nyberg and Mitamura, 1992), which used a very deep representation close to an interlingua, early versions of SysTran and Microsoft Translator, or more reˇ cently TectoMT (Popel and Zabokrtsk´ y, 2010) for English→Czech translation. AMR itself has recently been used for abstractive summarization (Liu et al., 2015). In this work, sentences in the document to be summarized are parsed to AMRs, then a decoding algorithm is run to produce a summary graph. The surface realization of this graph then constitutes the final sum30 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 30–36, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics Figure 1: Logical Form (computed tree) for the sentence: I would like to give you a sandwich taken from the fridge. Additional linguistic information, such as verb subcategorization frames, definiteness, tense e"
W15-3504,D09-1022,0,0.024975,"alley et al., 2004). While rule extraction algorithms such as (Galley et al., 2004) operate on trees and have also been applied to semantic parsing problems (Li et al., 2013), Jones et al. (2012) generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon model (Mauser et al., 2009) and phrase-sense disambiguation (Carpuat and Wu, 2007) are perhaps the best known methods. Similarly to Carpuat and Wu (2007), we use the classifier to rescore phrasal translations, however we do not train a separate classifier for each source phrase. Instead, we train a global model – similarly to Subotin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-"
W15-3504,C92-3168,0,0.396558,"ge-specific phenomena, effectively bringing different languages closer together. A number of machine translation systems starting as early as the 1950s therefore used a form of transfer: the source sentences were parsed, and those parsed representations were translated into target representations. Finally text generation was applied. The level of analysis is somewhat arguable – sometimes it was purely syntactic, but in other cases it reached into the semantic domain. One of the earliest architectures was described in 1957 (Yngve, 1957). More contemporary examples of such systems include KANT (Nyberg and Mitamura, 1992), which used a very deep representation close to an interlingua, early versions of SysTran and Microsoft Translator, or more reˇ cently TectoMT (Popel and Zabokrtsk´ y, 2010) for English→Czech translation. AMR itself has recently been used for abstractive summarization (Liu et al., 2015). In this work, sentences in the document to be summarized are parsed to AMRs, then a decoding algorithm is run to produce a summary graph. The surface realization of this graph then constitutes the final sum30 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 30–36, c B"
W15-3504,P00-1056,0,0.266378,"ribes the probability of the edge coming into the node ni , the token emission and finally the outgoing edge. We evaluate this probability for each node ni in the graph and re-align the token according to the random sample from this distribution. α and β are hyper-parameters specifying the concentration parameters of symmetric Dirichlet priors over the transition and emission distributions. Specifying values less than 1 for these hyper-parameters pushes the model toward sparse solutions. They are tuned by a grid search which evaluates model perplexity on a held-out set. Direct GIZA++. GIZA++ (Och and Ney, 2000) is a commonly used toolkit for word alignment which implements the IBM models. In this setting, we linearized the semantic graph nodes using a simple heuristic based on the surface word order and aligned them directly to the target-side sentences. We experimented with different symmetrizations and found that grow-diag-final-and gives the best results. Composed alignments. We divided the alignment problem into two stages: aligning semantic graph nodes to source-side words and aligning the source- and target-side words (i.e., standard MT word alignment). We then simply compose the two alignment"
W15-3504,P03-1021,0,0.0276172,"graph node (nai−1 ), from the parent node. (If there are multiple parents in the graph, we break ties in 33 three test sets provided for the Workshop on Statistical Machine Translation (Bojar et al., 2013) – WMT 2009, 2010 and 2013. This system had a set of 13 commonly used features: four channel model scores (forward and backward MLE and lexical weighting scores), a 5-gram language model, five lexicalized reordering model scores (corresponding to different ordering outcomes), linear distortion penalty, word count, and phrase count. The system was optimized using minimum error rate training (Och, 2003) on WMT 2009. a consistent but heuristic manner, picking the leftmost parent node according to its position in the source sentence) We also gather all the bits of the parent and the parent relation. These features may capture agreement phenomena. We also look at the shortest path in the semantic graph from the previous node to the current one and we extract features which describe it: • path length, • relations (edges) along the path. Dataset WMT 2009 = devset WMT 2010 WMT 2013 We use the lemmas of all nodes in the semantic graph as bag-of-word features, as well as all the surface words in the"
W15-3504,W13-2322,0,0.0617336,"s of words and phrases are learned directly from natural data, as are other syntactic operations such as reordering. However, commonly used methods have a very simple view of the linguistic data. Each word is generally modeled independently, for instance, and the relations between words are generally captured only in fixed phrases or as syntactic relationships. Recently there has been a resurgence of interest in unified semantic representations: deep analyses with heavy normalization of morphology, syntax, and even semantic representations. In particular, Abstract Meaning Representation (AMR, Banarescu et al. (2013)) is a novel representation of (sentential) semantics. Such representations could influence a number of natural language understanding and generation tasks, particularly machine translation. Deeper models can be used for multiple aspects of the translation modeling problem. Building translation models that rely on a deeper representation of the input allows for a more parsimonious translation model: morphologically related words can be handled in a unified manner; semantically related concepts are immediately adjacent 2 Related Work There is a large body of related work on utilizing deep langu"
W15-3504,W06-1608,1,0.75418,"French on 1 million parallel sentence pairs and produced 1000-best lists for 34 fort on feature engineering and decoder integration could lead to more substantial gains. Our approach is gated by the accuracy and consistency of the semantic parser. We have used a broad coverage parser with accuracy competitive to the current state-of-the-art, but even the stateof-the-art is rather low. It would be interesting to explore more robust features spanning multiple analyses, or to combine the outputs of multiple parsers. Even syntax-based machine translation systems are dependent on accurate parsers (Quirk and Corston-Oliver, 2006); deeper analyses are likely to be more dependent on parse quality. In a similar vein, it would be interesting to evaluate the impact of morphological, syntactic, and semantic features separately. A careful feature ablation and exploration would help identify promising areas for future research. We have only scratched the surface of possible integrations. Even this model could be applied to MT systems in multiple ways. For instance, rather than applying from source to target, we might evaluate in a noisy channel sense. That is, we could predict the source language surface forms given the targe"
W15-3504,P11-1024,0,0.0201005,"st to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon model (Mauser et al., 2009) and phrase-sense disambiguation (Carpuat and Wu, 2007) are perhaps the best known methods. Similarly to Carpuat and Wu (2007), we use the classifier to rescore phrasal translations, however we do not train a separate classifier for each source phrase. Instead, we train a global model – similarly to Subotin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-to-String Translation We develop models for semantic-graph-to-string translation. These models are essentially discriminative translation models, relying on a decomposition structure similar to both maximum entropy language models and IBM Models 1, 2 (Brown et al., 1993), and the HMM translation model (Vogel et al"
W15-3504,P09-1054,0,0.021084,"sociated with node n. We collect all translations observed in the training data and keep the 30 most frequent ones for each lemma. Our model thus assigns zero probability to unseen translations. Because of the size of our training data, we used online learning. We implemented a parallelized (multi-threaded) version of the standard stochastic gradient descent algorithm (SGD). Our learning rate was fixed – using line search, we found the optimal rate to be 0.05. Our batch size was set to one; different batch sizes made almost no difference in model performance. We used online L1 regularization (Tsuruoka et al., 2009) with weight 1. We implemented feature hashing to further improve performance and set the hash length to 22 bits. We shuffled our data and split it into five parts which were processed independently and their final weights were averaged. 4.3 Feature Set Our semantic representation enables us to use a very rich set of features, including information commonly used by both translation models and language models. We extract a significant amount of information from the graph node nai aligned to the generated word: Model • lemma, For our discriminative model, the alignment is assumed to be given. At"
W15-3504,N15-3006,1,0.822073,"´amˇest´ı 25 Prague, Czech Republic Chris Quirk and Michel Galley Mircosoft Research One Microsoft Way Redmond, WA 98052 tamchyna@ufal.mff.cuni.cz {chrisq,mgalley}@microsoft.com Abstract and available for modeling, etc. Language models using deep representations might help us model which interpretations are more plausible. We present an initial discriminative method for modeling the likelihood of a target language surface string given source language deep semantics. This approach relies on an automatic parser for source language semantics. We use a system that parses into AMR-like structures (Vanderwende et al., 2015), and apply the resulting model as an additional feature in a translation system. We present a feature-rich discriminative model for machine translation which uses an abstract semantic representation on the source side. We include our model as an additional feature in a phrase-based decoder and we show modest gains in BLEU score in an n-best re-ranking experiment. 1 Introduction The goal of machine translation is to take source language utterances and convert them into fluent target language utterances with the same meaning. Most recent approaches learn transformations using statistical techni"
W15-3504,C96-2141,0,0.497832,"otin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-to-String Translation We develop models for semantic-graph-to-string translation. These models are essentially discriminative translation models, relying on a decomposition structure similar to both maximum entropy language models and IBM Models 1, 2 (Brown et al., 1993), and the HMM translation model (Vogel et al., 1996). In particular, we see translation as a process of selecting target words in order conditioned on source language representation as well as prior target words. Similar to the IBM Models, we see each target word as being generated based on source concepts, though in our case the concepts are semantic graph nodes rather than surface words. That is, we assume the existence of an alignment, though it aligns the target words to Semantic Representation Our representation of sentence semantics is based on Logical Form (Vanderwende, 2015). LFs are labeled directed graphs whose nodes roughly correspon"
W15-3504,P01-1067,0,0.188419,"ubject which is replaced by the free variable X. The logical form can be converted using a sequence of rules to a representation which conforms to the AMR specification (Vanderwende et al., 2015). We do not use the full conversion pipeline in our work, so our semantic graphs are somewhere between the LF and AMR. Notably, we keep the bits which serve as important features for the discriminative modeling of translation. mary. (Jones et al., 2012) presents an MT approach that can exploit semantic graphs such as AMR, in a continuation of earlier work that abstracted translation away from strings (Yamada and Knight, 2001; Galley et al., 2004). While rule extraction algorithms such as (Galley et al., 2004) operate on trees and have also been applied to semantic parsing problems (Li et al., 2013), Jones et al. (2012) generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon mode"
W19-2401,P18-5002,1,0.877282,"Missing"
W19-2401,D14-1002,1,0.771046,"at local connections between any two neighboring sentences can be overlooked. One can easily distinguish a generated sentence from a real one by judging whether it is semantically cohesive with its neighboring sentences. We strive to embody these two different yet important concepts by developing coherence and cohesion discriminators, operating on the sentence level and word level, respectively. Our design of these two discriminators is inspired by the Deep Structured Semantic Model (DSSM) which was originally developed to measure the semantic similarity between two texts (Huang et al., 2013; Gao et al., 2014; Palangi et al., 2016; Xu et al., 2017). In this study, we extend ‘semantic similarity’ to coherence and cohesion in a long-form text. 3.1 Figure 1: Illustration of coherence and cohesion discriminators. Dcoherence takes in bag-of-words sentence embeddings as inputs, and Dcohesion takes in the raw word embeddings of consecutive sentences as inputs. The source encoder f (or u) is different from the target encoder g (or v). (CNN)1 or RNN2 , denoted as f , takes as input the BOW vectors of the source text chunk S and encodes it into a single vector f (S). Similarly, g encodes the target text chu"
W19-2401,J08-1001,0,0.0220925,"ohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an action to be chosen by the policy from a large discrete space, or vocabulary, conditioned on state st−1 = w≤t−1 . Let rt be the reward"
W19-2401,P85-1007,0,0.24846,"nerations. Furthermore, we model cohesion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an a"
W19-2401,P18-1152,0,0.020626,"anguage model that generates more coherent and cohesive long-form texts, and empirically validate its effectiveness using the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablation study on the relative contribution of these reward models individually. It is not clear that these reward models can be generalized to other tasks, in particular, long-form text generation tasks. The most relevant to our work is Bosselut et al. (2018), which promotes text generation in the correct order, and discour"
W19-2401,P98-1044,0,0.0287386,"ion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an action to be chosen by the policy f"
W19-2401,P88-1020,0,0.329802,"the relative quality of generations. Furthermore, we model cohesion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated w"
W19-2401,D17-1153,0,0.0304854,"Missing"
W19-2401,D14-1181,0,0.00294367,"l encoder. Given source text chunk S and target text chunk T , the coherence discriminator Dcoherence computes the coherence score in three steps, as illustrated in Figure 1 (upper). First, each sentence is encoded by the bag-of-words (BOW) embedding, i.e., the average of its word vectors from a pre-trained word embedding (Pennington et al., 2014). Secondly, an encoder which can be implemented using a convolutional neural network 1 We explored with deeper networks. However, the performance difference was marginal. For simplicity, we decided to use a 1-layer convolutional network architecture (Kim, 2014; Collobert et al., 2011). 2 For clarity in our model description, we omit RNN hereafter. We present results using both CNN and RNN encoders in Table 2. 3 ing data, negative (incoherent) pairs need to be artificially constructed. The next section describes the way these negative pairs are generated. form acohesive pairof consecutive sentences. Let sk := s1k , s2k , ..., snk be the k th sentence that con- sists of n words, sk+1 := s1k+1 , s2k+1 , ..., sm k+1 be the real next sentence of m  1 that2 consistsm e words, and sek+1 := sek+1 , sek+1 , ..., sek+1 be the artificially constructed i"
W19-2401,D17-1259,0,0.0347691,"Missing"
W19-2401,P02-1040,0,0.10418,"hat uses negative samples to estimate its reward baseline and therefore eliminates the need for a sepa1 Proceedings of the First Workshop on Narrative Understanding, pages 1–11 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics rate critic function; and (4) we develop a new neural language model that generates more coherent and cohesive long-form texts, and empirically validate its effectiveness using the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablatio"
W19-2401,P16-1094,1,0.872387,"Missing"
W19-2401,D14-1162,0,0.0821075,"that consists of m e sentences. Dcoherence is designed to distinguish a positive (coherent) pair (S, T ) from a negative (incoherent) pair (S, Te) by assigning different scores, i.e., Dcoherence (S, T ) &gt; Dcoherence (S, Te). Model architecture. The model takes a form of dual encoder. Given source text chunk S and target text chunk T , the coherence discriminator Dcoherence computes the coherence score in three steps, as illustrated in Figure 1 (upper). First, each sentence is encoded by the bag-of-words (BOW) embedding, i.e., the average of its word vectors from a pre-trained word embedding (Pennington et al., 2014). Secondly, an encoder which can be implemented using a convolutional neural network 1 We explored with deeper networks. However, the performance difference was marginal. For simplicity, we decided to use a 1-layer convolutional network architecture (Kim, 2014; Collobert et al., 2011). 2 For clarity in our model description, we omit RNN hereafter. We present results using both CNN and RNN encoders in Table 2. 3 ing data, negative (incoherent) pairs need to be artificially constructed. The next section describes the way these negative pairs are generated. form acohesive pairof consecutive sen"
W19-2401,D16-1127,1,0.823404,"the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablation study on the relative contribution of these reward models individually. It is not clear that these reward models can be generalized to other tasks, in particular, long-form text generation tasks. The most relevant to our work is Bosselut et al. (2018), which promotes text generation in the correct order, and discourages in its reverse order using rewards. However, this may not be sufficient in capturing coherence since there ar"
W19-2401,D17-1230,0,0.0520846,"Missing"
W19-2401,P15-1152,0,0.0590307,"Missing"
W19-2401,N03-1020,0,0.21645,"Missing"
W19-2401,N15-1020,1,0.830371,"al approaches to natural language generation rely on a large amount of human-generated text to train language models (Cho et al., 2014; Graves, 2013; Sutskever et al., 2014). Although these models can generate sentences that, if judged individually, are similar to human-generated ones, they often fail to capture the local and global dependencies among sentences, resulting in a text that is neither coherent nor cohesive. For example, neural language models based on Recurrent Neural Networks (RNNs) are widely applied to response generation for dialogue (Vinyals and Le, 2015; Shang et al., 2015; Sordoni et al., 2015; Li et al., 2015). Although the responses by themselves look reasonable, they are detached from the whole dialogue session. See Gao et al. (2018) for a comprehensive survey. In this paper, we address the challenge in a principled manner, employing a pair of discriminators to score whether and to what extent a text is coherent or cohesive. The coherence discriminator measures the compatibility among all sentences in a paragraph. The cohesion discriminator measures the compatibility of each pair of consecutive sentences. These models, given a conditional input text and multiple candidate output"
W19-2401,1983.tc-1.13,0,0.528181,"Missing"
W19-2401,C98-1044,0,\N,Missing
W19-2401,N18-1016,1,\N,Missing
W19-2401,N16-1014,1,\N,Missing
