1999.mtsummit-1.32,C94-1015,0,0.0694964,"Missing"
1999.mtsummit-1.32,C96-1070,0,0.0362002,"Missing"
1999.mtsummit-1.32,P91-1024,0,0.0462899,"Missing"
1999.mtsummit-1.34,C96-1070,0,0.132529,"Missing"
1999.mtsummit-1.34,C96-1075,0,0.0608382,"Missing"
1999.mtsummit-1.34,P89-1013,0,0.0183586,"Missing"
1999.mtsummit-1.34,C88-2118,0,0.0291956,"Missing"
1999.mtsummit-1.34,P91-1024,1,0.749748,"Missing"
1999.mtsummit-1.34,P98-2233,1,0.883543,"Missing"
1999.mtsummit-1.34,W97-0404,0,0.260944,"Missing"
1999.mtsummit-1.34,W99-0207,1,\N,Missing
1999.mtsummit-1.34,C98-2228,1,\N,Missing
1999.mtsummit-1.43,1999.mtsummit-1.43,1,0.0530913,"Missing"
2003.mtsummit-papers.29,takezawa-etal-2002-toward,0,0.0725326,"Missing"
2003.mtsummit-papers.29,maekawa-etal-2000-spontaneous,0,0.078719,"Missing"
2005.mtsummit-papers.29,2004.iwslt-evaluation.1,0,0.0123601,"ure. ᶛ VTCPUNCVKQPGZCORNG UQWTEG U K 6&apos; K K VCTIGV U 6&apos; U QWVRWV V V V V V V Figure 5: Output Sentence Generation. 4 Experiments 2. The relation between translation examples is equal to the relation between their corresponding input phrases. For example, TE2 has a corresponding input phrase i2 , and it has a relation to i3 in the input structure. In this case, TE2 has a relation to an example phrase t( 3), which corresponds to i3 ( as shown in a dotted line). 4.1 Experimental setting For evaluation, we used corpora (trainingset and test-set) which are provided in the IWSLT04(Akiba et al., 2004). The training-set consists of 20K English-Japanese sentence pairs in a travel conversation domain. We built translation examples from the training-set by using the proposed alignment method mentioned in Section 3. The test-set consists of 500 Japanese sentences and their English references (500 × 16). The experiments are conducted using the following ﬁve systems: Finally, the output word-order is decided based on the n-gram language model (n = 3). proposed: The system which selects translation examples based on the proposed 223 method . Table 1: Evaluation Metrics. basic: The system (Aramaki"
2005.mtsummit-papers.29,2004.iwslt-evaluation.15,1,0.799506,"., 2004). The training-set consists of 20K English-Japanese sentence pairs in a travel conversation domain. We built translation examples from the training-set by using the proposed alignment method mentioned in Section 3. The test-set consists of 500 Japanese sentences and their English references (500 × 16). The experiments are conducted using the following ﬁve systems: Finally, the output word-order is decided based on the n-gram language model (n = 3). proposed: The system which selects translation examples based on the proposed 223 method . Table 1: Evaluation Metrics. basic: The system (Aramaki and Kurohashi, 2004) which selects translation examples based on the heuristic criterion. This system submitted translation evaluation workshop on IWSLT04(Akiba et al., 2004), and showed its basic feasibility. Note that basic uses the same alignment result as proposed. BLEU NIST baseline: EBMT baseline, which searches the most similar translation examples by using a character-based DP matching method, and outputs its target parts as is. WER C1, C2: Commercial machine translation systems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve eval"
2005.mtsummit-papers.29,2001.mtsummit-papers.5,1,0.784603,"rs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDICT, and EIJIRO. These dictionaries have about two million entries in total. Step 3: Building Translation Example Database The system generate possible combinations of correspondences from aligned sentence pairs as shown in Figure 4. We regard these combinations of correspondences as translation examples, and store them in the database. In this operation, the system stores also their surrounding phrases, which are used for calculating context sim. Translation module Step 1: Input sentence analysis First, an input sentence is analyzed by the Japanes"
2005.mtsummit-papers.29,W03-0312,1,0.781548,"ith larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally, we investigated the relation between the corpus size (the number of training sentence pairs) and its performance (BLE"
2005.mtsummit-papers.29,J93-2003,0,0.00677215,"odel, the system searches the translation example combination which has the highest probability. The proposed model clearly formalizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental results demonstrate that the proposed model has a slightly better translation quality than stateof-the-art EBMT systems. 1 Introduction Nowadays, much attention has been given to data-driven (or corpus-based) machine translation, such as example-based machine translation or EBMT(Nagao, 1984) and statistical machine translation or SMT (Brown et al., 1993). This paper focuses on EBMT approach. The idea of EBMT is that translation examples similar to a part of an input sentence are retrieved and combined to produce a translation. EBMT basically prefers larger translation examples, because the larger the translation is, the wider context is taken into account. So, most EBMT systems retrieve large examples as possible as they can, and the retrieving is based on some heuristic criterion/measures which prefer larger examples. On the other hand, SMT approach basically breaks down translation examples into small word/phrases in order to calculate tran"
2005.mtsummit-papers.29,A00-2018,0,0.013935,"able, i.e., P (play |kakeru) = 23 , P (set |kakeru) = 1 3. Thus, a translation example with the similar context can naturally get a higher translation probability. 3 Algorithm The Algorithm of proposed method consists of the following two modules: (1) an alignment module, which builds translation example from corpus, and (2) a translation module, which generates a translation. Alignment module Step 1: Conversion into phrasal dependency structures saurus(Ikehara et al., 1997). 222 First, sentence pairs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDICT, and EIJIRO. These dictionaries have abo"
2005.mtsummit-papers.29,C94-1015,0,0.0476541,"HERS the fact that, as mentioned before, the system will achieve a higher performance with larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally, we investigated the relation be"
2005.mtsummit-papers.29,2002.tmi-papers.9,0,0.0180469,"D-ORDER SELECTION-ERR OTHERS the fact that, as mentioned before, the system will achieve a higher performance with larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally,"
2005.mtsummit-papers.29,J94-4001,1,0.691537,"only three, but its target translation becomes more stable, i.e., P (play |kakeru) = 23 , P (set |kakeru) = 1 3. Thus, a translation example with the similar context can naturally get a higher translation probability. 3 Algorithm The Algorithm of proposed method consists of the following two modules: (1) an alignment module, which builds translation example from corpus, and (2) a translation module, which generates a translation. Alignment module Step 1: Conversion into phrasal dependency structures saurus(Ikehara et al., 1997). 222 First, sentence pairs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDI"
2005.mtsummit-papers.29,niessen-etal-2000-evaluation,0,0.0324007,"ems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We inv"
2005.mtsummit-papers.29,W99-0604,0,0.105445,"input sentence are retrieved and combined to produce a translation. EBMT basically prefers larger translation examples, because the larger the translation is, the wider context is taken into account. So, most EBMT systems retrieve large examples as possible as they can, and the retrieving is based on some heuristic criterion/measures which prefer larger examples. On the other hand, SMT approach basically breaks down translation examples into small word/phrases in order to calculate translation probability reliably. Of course, recent SMT studies incorporate larger phrase unit, for example, Och(Och et al., 1999) used alignment template to handle phrase chunks. However, SMT translation unit is smaller than EBMT, which has no limitation in its unit size. 219 Simply speaking, EBMT and SMT have two diﬀerences: 1. EBMT pays more attention to the size; SMT to the frequency. 2. EBMT relies rion/measures; formalized. on heuristic criteSMT is statistically For the formalization of EBMT, this paper proposes a probabilistic translation model, which deals not only with the example size but with the context similarity. In the experiments, the proposed model has a slightly better translation quality than stateof-t"
2005.mtsummit-papers.29,W01-1408,0,0.0234891,"nditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We investigated the contribution of context similarity to the translation performance. This is condu"
2005.mtsummit-papers.29,P02-1040,0,0.0788812,"ed. BLEU NIST baseline: EBMT baseline, which searches the most similar translation examples by using a character-based DP matching method, and outputs its target parts as is. WER C1, C2: Commercial machine translation systems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Ta"
2005.mtsummit-papers.29,W01-1402,0,0.174752,"Missing"
2005.mtsummit-papers.29,2003.mtsummit-papers.51,0,0.0610725,"yphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We investigated the contribution of context similarity to the translation performance. This is conducted by performance comparison between the proposed system and without sim, which does not use the thesaurus (Table 2). As shown in Table 2, propose"
2005.mtsummit-posters.15,C00-1004,0,0.027986,"hree lines for each sentence pair: the ﬁrst line is a label, the second line is the target sentence and the third line is the source sentence. Alignment information is represented in the source sentence. Each token is followed by a set of numbers that indicate the positions of the target words to which the source word is aligned. The “NULL” token is added to the source sentence. This token is a special one that illustrates the set of target words that have no connection to the source words. The word segmentation of Japanese sentences is produced by the Japanese morphological analyzer “ChaSen”(Asahara and Matsumoto, 2000). 3.2 Implementation of the viewer Our developed viewer1 has three modes. Two modes are for displaying the alignment information and one other one is for showing structural 1 This viewer was implemented via JAVA on Windows. 428 information on each sentence. The ﬁrst mode is shown in Fig.1. This mode has three main areas. The upper-right area in this mode displays source and target sentences, and the alignment information of each segment is shown in the same color for both source and target sentences. When the mouse cursor placed over a word, more detailed information can be shown by highlighti"
2005.mtsummit-posters.15,A00-2018,0,0.00914426,"Also in this mode, the expression of English (Y-axis) is not by just one word: if the Y-axis were just one word, it would be too long and we would need an enormous area to display the grid. These English expressions are deﬁned by a parse tree2 . When the words represented by leaf nodes in the parse tree have a relationship with brother nodes, they form an expression3 . In addition, some glid lines are thick. In Japanese, these thick lines (bars) indicate borders between clauses. Clause boundaries are annotated in preprocessing. The crossed thick lines indicate 2 Now we use the Charniak Parser(Charniak, 2000) Japanese (X-axis) do not process with the concatenation. 3 Figure 1: Screenshot of the word alignment viewer with full sentences the border of a constituent with its corresponding Japanese clause. There are several ways to calculate the borders of the corresponding Japanese clauses. The processing will be discussed in detail in the next section. These thick lines help to determine whether the segmentations are suitable for the translation model. The third mode is shown in Fig.3. This mode mainly illustrates the analysis result for each sentence. The left-hand area in this mode is the same as"
2005.mtsummit-posters.15,2003.mtsummit-papers.29,1,0.716014,"ith its corresponding Japanese clause. There are several ways to calculate the borders of the corresponding Japanese clauses. The processing will be discussed in detail in the next section. These thick lines help to determine whether the segmentations are suitable for the translation model. The third mode is shown in Fig.3. This mode mainly illustrates the analysis result for each sentence. The left-hand area in this mode is the same as in the other modes. The lower-middle area in this mode shows morphological analysis and clause-boundary annotation results for the Japanese by ChaSen and CBAP(Kashioka et al., 2003). The lower-right area shows the parse result with English4 , while the upper-right area shows the English constituent list that was segmented with corresponding Japanese clauses. 4 Currently we use the Charniak Parser to get these results. 429 4 Discussion In this section, we would like to discuss alignment information. The patent parallel corpus includes long sentences and these are diﬃcult to align automatically. In the previous section, we used the alignment information automatically calculated by GIZA++. This viewer can display pseudo-units as Japanese clauses for translation. We would li"
2006.iwslt-papers.8,takezawa-kikui-2004-comparative,0,0.146526,"Missing"
2006.iwslt-papers.8,2006.iwslt-evaluation.12,1,\N,Missing
2011.iwslt-evaluation.2,2011.iwslt-evaluation.1,0,0.0265638,"Missing"
2011.iwslt-evaluation.22,W08-0510,0,0.0280144,"M training text consisted of 85k sentences and had a vocabulary of 59k words. We built a modified KneserNey smoothed tri-gram using the MITLM toolkit[14]. Prior to training the LM the Japanese text was segmented using the same propriety tool as used to segment the IWSLT SMT training text. This was done to make coupling easier and remove the need for a possible re-segmentation after recognition. Our in-house decoder SprinTra is a general one-pass Viterbi decoder. To parallel decode the gender dependent 3.2. SMT Training The SMT system was based around the Moses training and decoding components [16]. The SMT systems were all trained on the standard IWSLT 2007 training data which consisted of 20k training sentence pairs. The phrase tables were all built using the standard Moses training scripts. In all cases the English text was lower-cased and tokenized using the standard Moses scripts prior to training. After SMT decoding the casing was recovered prior to evaluation. To build the target LM we used the mono-lingual component of the IWSLT training text. Again a modified KneserNey trigram was built using the MITLM toolkit. We observed no improvement in translation performance when using hi"
2011.iwslt-evaluation.22,P11-2001,0,0.0408993,"in ASR performance with or without the tuple form of the vocabulary entries. Although, for a large vocabulary Japanese spontaneous speech task we have observed significantly larger search networks and slower decoding speeds by removing the POS tags. A problem with stripping POS tags from the lattice and applying optimizations is it may increase the chances that the lattice cannot be determinized. For future work in this area we plan to investigate the use of POS tags and pronunciation data in a factored SMT based system, or to extend the Lexicographic semiring idea as proposed by Roark et al [17]. 4.1.3. Punctuation Recovery In the first experiments we looked a several simple heuristics to add punctuation. The silences in the recognition output are either removed or mapped to periods or commas. We found that removing all silences and simply adding a final period to the recognition output performed best. On inspection of the Japanese test-set we found that the only types of punctuation present were final periods. 4.2. Decoder Tuning and Translation Performance In this section we considered how the SMT performed as we changed the speech decoder parameters. We were not only interested in"
2011.iwslt-evaluation.22,P03-1006,0,0.0355375,"ence scores problematic. • The determinization operation will push the output labels forward towards the initial state of the WFST. Furthermore, the context-dependency transducer will introduce a delay between the input and output labels [20]. This means that we cannot interpret the output labels as corresponding to the actual word endings. • The determinization and weight pushing algorithms will move the weights closer to the initial state of the WFST. • Repeated lower order language model paths are introduced by the back-off approximation used in the WFST representation of the language model[21]. The lattices generated from the WFST decoder will have paths replicated that correspond to each of the lower order n-grams sequences. We initially looked at converting the WFST phone lattices to word lattices or confusion networks and using them as the input to the Moses decoder. However, we found that the system did not perform well, possibly due to the previously mentioned issues. Given these issues, we decided to investigate the use of a WFST based translation decoder. When direct coupling WFST implementations the total path scores not individual label scores are important, and this will"
2011.iwslt-evaluation.22,J10-3008,0,0.088634,"cribe our approach to optimizing the log-linear weights λn in the WFST framework. 5.2. Optimization of WFST Feature Weights The area we address in this section is the problem of tuning the log-linear weights within the WFST framework. The optimization of the feature weights requires that during decoding we maintain the individual feature contributions. However in the WFST framework after composition and optimization the contributions from each of the underlying knowledge sources is lost. One way to obtain the feature contributions is perform a re-alignment phase after decoding as described in [23], the drawback with this approach is the 171 need for a second decoding pass. An alternative approach is to access the internals of the composition process and try to recover the component state sequences from the state pairs the composition algorithm maintains internally . This approach would fail if during or after the decoding process we apply operations such as determinization or epsilon removal. The method we describe uses a tuple semiring which can track individual score contributions from a cascade of WFSTs. This allows for the preservation of scores after any composition or even optimi"
2011.iwslt-evaluation.22,2007.iwslt-1.1,0,\N,Missing
2012.iwslt-evaluation.2,P10-2041,0,0.0239202,"Corpus TED Talks News Commentary English Gigaword #sentences 142K 212K 123M #words 2,402K 4,566K 2,722M 3.2. Domain adaptation The large out-of-domain corpora likely includes sentences that are so unlike the domain of the TED talks. LM trained on these unlike sentences is probably harmful. Therefore, we adopt domain adaptation by selecting only a portion of the out-of-domain corpus instead of using the whole. We employ cross-entropy difference metric for domain adaptation, which biases towards sentences that are both like the in-domain corpus and unlike the average of the out-ofdomain corpus [6]. Each sentence s of the out-of-domain corpus is scored as follows, HI (s) − HO (s), (1) where HI (s) and HO (s) represent cross-entropy scores according to LMI trained on the in-domain corpus, and LMO trained on a subset sentences randomly selected from the outof-domain corpus. Here, LMI and LMO are similar size. Then the lowest-scoring sentences are selected as a subset of out-of-domain corpus. 3.3. N -gram LM For the in-domain and the selected out-of-domain corpora, modiﬁed Kneser-Ney smoothed n-gram LMs (n=3,4) are constructed using SRILM [7]. They are interpolated to form a baseline of n-"
2012.iwslt-evaluation.2,C12-1173,1,0.872404,"Missing"
2012.iwslt-evaluation.2,federico-etal-2012-iwslt,0,\N,Missing
2012.iwslt-evaluation.2,2012.iwslt-papers.11,1,\N,Missing
2012.iwslt-papers.11,N06-2001,0,0.295778,"Missing"
2012.iwslt-papers.11,W12-2703,0,0.359363,"te word error rate reductions over RNNLM and n-gram LM are 0.4∼0.8 points. 1. Introduction Language models (LM) are a critical component of many application systems such as automatic speech recognition (ASR), machine translation (MT) and optical character recognition (OCR). In the past, statistical back-off n-gram language models with sophisticated smoothing techniques have gained great popularity because of their simplicity and good performance. Recently, neural network based language models (NNLMs), such as the feed-forward NNLM [3, 19], the recurrent NNLM (RNNLM) [15, 16] and the deep NNLM [2], have been continuously reported to perform well amongst other language modeling techniques. Among them, RNNLMs are state-of-the art [2, 14], which embed words in a continuous space in which probability estimation is performed using artiﬁcial neural networks consisting of input layer, hidden layer, and output layer. Due to consistent improvement in terms of perplexity and word error rate and their inherently strong generalization, they have become an increasingly popular choice for LVCSR and statistical MT tasks. Many of these RNNLMs only use one single feature stream, i.e., surface words, wh"
2012.iwslt-papers.11,N03-2002,0,0.282095,"preceding time step, as shown in Eq. (2). 1 2 K , fi−1 , ..., fi−1 ] F (wi−1 ) = [fi−1 (1) 2 http://www.cis.upenn.edu/ ˜treebank/ countries country countri NNS and and and CC developing developing develop VBG countries country countri NNS xi = [F (wi−1 ), si−1 ] (2) Using the concatenation operation, our factored RNNLM can simultaneously integrate all factors and the entire history in stead of backing-off to fewer factors and a shorter context. The weight of each factor is represented in connection weight matrix U . Therefore, it can address the optimization problem well in factored n-gram LM [4, 7]. In the special case that 1 k fi−1 is a surface word factor vector and fi−1 (k = 2, ..., K) are dropped, our proposed factored RNNLM goes back to the RNNLM. The hidden layer employs a sigmoid activation function:  j (xi × umj )) ∀m ∈ [1, H] sm i = f( j 1 f (z) = 1 + e−z (3) where H is the number of hidden neurons in the hidden layer and umj is an element in matrix U denoting the corresponding connection weight. Like [10, 16], we assume that each word belongs to exactly one class and divide the output layer into two parts: the ﬁrst estimates the posterior probability distribution over all cla"
2012.iwslt-papers.11,P98-1035,0,0.147078,"Missing"
2012.iwslt-papers.11,2011.iwslt-evaluation.1,0,0.0434545,"Missing"
2012.iwslt-papers.11,P02-1025,0,0.0613464,"Missing"
2012.iwslt-papers.11,P10-2041,0,0.0567379,"ergence. Table 5: Elapsed time during training and test. #1 and #2 denote time of an iteration and time of all iterations during training, m=minute, s=second. RNNLM fRNNLMwsp #1 120m 141m #2 1923m 1843m time on testing tst2010 35.7s 43.4s 245 ppl 225 RNNLM RNNLMwp RNNLMwsp 205 185 165 145 iteration 125 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Figure 3: Convergence curve on the dev2010 set. 3.3. Training corpus size This subsection analyzes the inﬂuence of training corpus size to RNNLM and fRNNLMwsp . The training corpus is gradually increased by selecting sentences from the generaldomain corpus [17, 20]. Note that, we change the order of the training data as follows, the training starts with the sentences selected from the general-domain data, and ends with the in-domain data. The selected sentences are also sorted in descending order of perplexities. The results are shown in Table 6. This experiment indicates that the impacts of morphological and syntactic information become smaller with increasing of training data. The largest improvement of fRNNLMwsp trained on the indomain data (2.4M words) reaches 0.8 points. However, this improvement reduces to 0.2 points when the model is trained on t"
2012.iwslt-papers.11,W12-2702,0,0.022562,"ergence. Table 5: Elapsed time during training and test. #1 and #2 denote time of an iteration and time of all iterations during training, m=minute, s=second. RNNLM fRNNLMwsp #1 120m 141m #2 1923m 1843m time on testing tst2010 35.7s 43.4s 245 ppl 225 RNNLM RNNLMwp RNNLMwsp 205 185 165 145 iteration 125 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Figure 3: Convergence curve on the dev2010 set. 3.3. Training corpus size This subsection analyzes the inﬂuence of training corpus size to RNNLM and fRNNLMwsp . The training corpus is gradually increased by selecting sentences from the generaldomain corpus [17, 20]. Note that, we change the order of the training data as follows, the training starts with the sentences selected from the general-domain data, and ends with the in-domain data. The selected sentences are also sorted in descending order of perplexities. The results are shown in Table 6. This experiment indicates that the impacts of morphological and syntactic information become smaller with increasing of training data. The largest improvement of fRNNLMwsp trained on the indomain data (2.4M words) reaches 0.8 points. However, this improvement reduces to 0.2 points when the model is trained on t"
2012.iwslt-papers.11,W04-3242,0,\N,Missing
2012.iwslt-papers.11,C98-1035,0,\N,Missing
2012.iwslt-papers.11,C04-1022,0,\N,Missing
A00-1006,1997.tmi-1.21,0,0.057569,"Missing"
A00-1006,P98-2185,0,0.0676193,"Missing"
A00-1006,1999.mtsummit-1.34,1,0.898011,"Missing"
A00-1006,W97-0400,0,0.313823,"Missing"
A00-1006,C96-1070,0,0.0632717,"Missing"
A00-1006,W97-0403,0,0.0435074,"Missing"
A00-1006,C98-2180,0,\N,Missing
A00-1006,C98-2126,0,\N,Missing
A00-1006,1993.mtsummit-1.8,0,\N,Missing
C12-1173,N06-2001,0,0.0603319,", and then the second most distant word etc. until the unigram is reached. However, the factors in FLM occur simultaneously, i.e., without forming a temporal sequence, so the order in which they should be dropped is not immediately obvious. In this case, FLM creates a large space of back-off graphs that cannot be exhaustively searched. Duh and Kirchhoff (2004) employed a genetic algorithm (GA) that, however, provides no guarantee of finding the optimal back-off graph. Our factored RNNLM addresses this optimization problem well, as described in Section 3. In addition, Emami and Jelinek (2004); Alexandrescu and Kirchhoff (2006); Kuo et al. (2009); Collins et al. (2005) introduced various syntactic features into their feed-forward NNLMs and discriminative language models. Table 1 summarizes FLM, RNNLM, and our approach from three points of view. Conditioning variables Word and its linguistic features History Pros and Cons n-1 preceding history RNNLM Word Entire history factored RNNLM Word and its linguistic features Entire history Better than n-gram LM due to linguistic features; Creating a large space of models that cannot be searched exhaustively. Further enhancing FLM due to RNN architecture; Conditioning variable"
C12-1173,W12-2703,0,0.182133,"smoothing techniques (Chen and Goodman, 1996), class n-gram language models (Brown et al., 1992), topic language models (Gildea and Hofmann, 1999; Hsu and Glass, 2006), structured language models (Chelba and Jelinek, 2000), maximum entropy language models (Rosenfeld, 1996) and random forests language models (Xu and Jelinek, 2004). Among these techniques, one of the most successful schemes is the neural network language model (NNLM), such as the feed-forward NNLM (Bengio et al., 2003; Schwenk, 2007; Kuo et al., 2012), the recurrent NNLM (RNNLM) (Mikolov et al., 2010, 2011b) and the deep NNLM (Arisoy et al., 2012). Compared to other LMs, recurrent NNLMs, which are state-of-the art (Mikolov et al., 2011a; Arisoy et al., 2012), embed words in a continuous space in which probability estimation is performed using artificial neural networks consisting of input layer, single or multiple hidden layers, and output layer. Due to consistent improvement in terms of perplexity and word error rate and their inherently strong generalization, they have become an increasingly popular choice for LVCSR and statistical MT tasks. Many of these RNNLMs only use one single feature stream, i.e., surface words, which are limit"
C12-1173,J92-4003,0,0.0471111,"e vocabulary size is 64K , a 4-gram language model needs to estimate 64K 2 bigrams, 64K 3 trigrams and 64K 4 4-grams. Due to data sparseness, many are not observed during the training phase. This means that n-gram LMs have poor generalization to low-frequency and unseen n-grams. This problem becomes more severe as the vocabulary size increases. Many interesting approaches have been proposed to overcome it in large vocabulary continuous speech recognition (LVCSR) and statistical machine translation systems, especially smoothing techniques (Chen and Goodman, 1996), class n-gram language models (Brown et al., 1992), topic language models (Gildea and Hofmann, 1999; Hsu and Glass, 2006), structured language models (Chelba and Jelinek, 2000), maximum entropy language models (Rosenfeld, 1996) and random forests language models (Xu and Jelinek, 2004). Among these techniques, one of the most successful schemes is the neural network language model (NNLM), such as the feed-forward NNLM (Bengio et al., 2003; Schwenk, 2007; Kuo et al., 2012), the recurrent NNLM (RNNLM) (Mikolov et al., 2010, 2011b) and the deep NNLM (Arisoy et al., 2012). Compared to other LMs, recurrent NNLMs, which are state-of-the art (Mikolov"
C12-1173,P98-1035,0,0.0213357,"the way here are four doctors in your part of the united states who * OFFERED and their phone numbers * by the way here are four doctors in your part of the united states who offer it and their phone numbers Table 11: Re-scoring results sampled from RNNLM and fRNNLMwp . * denotes deletion errors, capitalized words denote substitution errors, and underlined words show their differences. 1.5 1.4 1 1 0.9 0.9 1 0.8 0.6 0.5 0.5 0.5 0.5 0.3 0.3 0.3 0.3 0.2 0.1 -0.2 -0.4 0 -0.5 tst2011 tst2010 Figure 5: Absolute improvement on each talk. Recently, syntactic parse trees are used in many advanced LMs (Chelba and Jelinek, 1998; Khudanpur and Wu, 2000; Xu et al., 2002; Collins et al., 2005; Rastrow et al., 2012). For future work, we intend to investigate topic information (Shi et al., 2012) and richer syntactic structure features into factored RNNLM, such as context-free rule productions, constituent/head features, and head-to-head dependencies that can be extracted using parser tools. Second, neural networks are notorious for being time consuming during training, future studies will also focus on speeding up the training of factored RNNLM using graphical processing units (Schwenk et al., 2012). Furthermore, factore"
C12-1173,P96-1041,0,0.395481,"in a back-off n-gram LM is still enormous. Assuming the vocabulary size is 64K , a 4-gram language model needs to estimate 64K 2 bigrams, 64K 3 trigrams and 64K 4 4-grams. Due to data sparseness, many are not observed during the training phase. This means that n-gram LMs have poor generalization to low-frequency and unseen n-grams. This problem becomes more severe as the vocabulary size increases. Many interesting approaches have been proposed to overcome it in large vocabulary continuous speech recognition (LVCSR) and statistical machine translation systems, especially smoothing techniques (Chen and Goodman, 1996), class n-gram language models (Brown et al., 1992), topic language models (Gildea and Hofmann, 1999; Hsu and Glass, 2006), structured language models (Chelba and Jelinek, 2000), maximum entropy language models (Rosenfeld, 1996) and random forests language models (Xu and Jelinek, 2004). Among these techniques, one of the most successful schemes is the neural network language model (NNLM), such as the feed-forward NNLM (Bengio et al., 2003; Schwenk, 2007; Kuo et al., 2012), the recurrent NNLM (RNNLM) (Mikolov et al., 2010, 2011b) and the deep NNLM (Arisoy et al., 2012). Compared to other LMs, r"
C12-1173,P05-1063,0,0.0260422,"unigram is reached. However, the factors in FLM occur simultaneously, i.e., without forming a temporal sequence, so the order in which they should be dropped is not immediately obvious. In this case, FLM creates a large space of back-off graphs that cannot be exhaustively searched. Duh and Kirchhoff (2004) employed a genetic algorithm (GA) that, however, provides no guarantee of finding the optimal back-off graph. Our factored RNNLM addresses this optimization problem well, as described in Section 3. In addition, Emami and Jelinek (2004); Alexandrescu and Kirchhoff (2006); Kuo et al. (2009); Collins et al. (2005) introduced various syntactic features into their feed-forward NNLMs and discriminative language models. Table 1 summarizes FLM, RNNLM, and our approach from three points of view. Conditioning variables Word and its linguistic features History Pros and Cons n-1 preceding history RNNLM Word Entire history factored RNNLM Word and its linguistic features Entire history Better than n-gram LM due to linguistic features; Creating a large space of models that cannot be searched exhaustively. Further enhancing FLM due to RNN architecture; Conditioning variables are only words, no morphological or synt"
C12-1173,C04-1022,0,0.487382,"ent NNLMs reduce computational complexity and have relatively fast training due to the factorization of the output layer. Other experiments (Mikolov et al., 2011a; Arisoy et al., 2012; Kuo et al., 2012) demonstrated that RNNLM significantly outperforms feed-forward NNLM. Therefore, this paper uses RNNLM as a baseline and improves it by incorporating additional information other than surface words, such as morphological or syntactic features. Although few studies incorporate morphological and syntactic features into RNNLM, using multiple features in language modeling is not novel. For example, Duh and Kirchhoff (2004) presented 2837 a factored back-off n-gram LM (FLM) that assumes each word is equivalent to a fixed number (K ) 1:K of factors, i.e., W ≡ f 1:K , and produces a statistic model of the following form: p( f i1:K |f i−n+1:i−1 ). The standard back-off in an n-gram LM first drops the most distant word (w i−n+1 in the case of Eq. (1)), and then the second most distant word etc. until the unigram is reached. However, the factors in FLM occur simultaneously, i.e., without forming a temporal sequence, so the order in which they should be dropped is not immediately obvious. In this case, FLM creates a l"
C12-1173,2011.iwslt-evaluation.1,0,0.0258234,"Missing"
C12-1173,W06-1644,0,0.0182052,"4K 2 bigrams, 64K 3 trigrams and 64K 4 4-grams. Due to data sparseness, many are not observed during the training phase. This means that n-gram LMs have poor generalization to low-frequency and unseen n-grams. This problem becomes more severe as the vocabulary size increases. Many interesting approaches have been proposed to overcome it in large vocabulary continuous speech recognition (LVCSR) and statistical machine translation systems, especially smoothing techniques (Chen and Goodman, 1996), class n-gram language models (Brown et al., 1992), topic language models (Gildea and Hofmann, 1999; Hsu and Glass, 2006), structured language models (Chelba and Jelinek, 2000), maximum entropy language models (Rosenfeld, 1996) and random forests language models (Xu and Jelinek, 2004). Among these techniques, one of the most successful schemes is the neural network language model (NNLM), such as the feed-forward NNLM (Bengio et al., 2003; Schwenk, 2007; Kuo et al., 2012), the recurrent NNLM (RNNLM) (Mikolov et al., 2010, 2011b) and the deep NNLM (Arisoy et al., 2012). Compared to other LMs, recurrent NNLMs, which are state-of-the art (Mikolov et al., 2011a; Arisoy et al., 2012), embed words in a continuous space"
C12-1173,D07-1091,0,0.0252911,"tory Pros and Cons n-1 preceding history RNNLM Word Entire history factored RNNLM Word and its linguistic features Entire history Better than n-gram LM due to linguistic features; Creating a large space of models that cannot be searched exhaustively. Further enhancing FLM due to RNN architecture; Conditioning variables are only words, no morphological or syntactic linguistic features are used. Combining the above merits, but more parameters and computation complexity, which actually does not cause problems, as described in Section 4.4. FLM Table 1: Comparison of FLM, RNNLM, and factored RNNLM Koehn and Hoang (2007) introduced various features from linguistic tools or word classes into phrase-based MT models for better translation performance. 3 Factored RNNLM The architecture of our factored RNNLM is illustrated in Fig. 1. It consists of input layer x , hidden layer s (state layer), and output layer y . The connection weights among layers are denoted by matrixes U and W . Unlike RNNLM, which predicts probability P(w i |w i−1 , si−1 ), our factored RNNLM predicts probability P(w i |F (w i−1 ), si−1 ) of generating following word w i and is explicitly conditioned on a collection or bundle of K factors of"
C12-1173,P12-1019,0,0.0186022,"hone numbers * by the way here are four doctors in your part of the united states who offer it and their phone numbers Table 11: Re-scoring results sampled from RNNLM and fRNNLMwp . * denotes deletion errors, capitalized words denote substitution errors, and underlined words show their differences. 1.5 1.4 1 1 0.9 0.9 1 0.8 0.6 0.5 0.5 0.5 0.5 0.3 0.3 0.3 0.3 0.2 0.1 -0.2 -0.4 0 -0.5 tst2011 tst2010 Figure 5: Absolute improvement on each talk. Recently, syntactic parse trees are used in many advanced LMs (Chelba and Jelinek, 1998; Khudanpur and Wu, 2000; Xu et al., 2002; Collins et al., 2005; Rastrow et al., 2012). For future work, we intend to investigate topic information (Shi et al., 2012) and richer syntactic structure features into factored RNNLM, such as context-free rule productions, constituent/head features, and head-to-head dependencies that can be extracted using parser tools. Second, neural networks are notorious for being time consuming during training, future studies will also focus on speeding up the training of factored RNNLM using graphical processing units (Schwenk et al., 2012). Furthermore, factored RNNLMs need to be evaluated on other tasks like MT and with other languages such as"
C12-1173,W12-2702,0,0.0675618,"Missing"
C12-1173,P02-1025,0,0.0330347,"united states who * OFFERED and their phone numbers * by the way here are four doctors in your part of the united states who offer it and their phone numbers Table 11: Re-scoring results sampled from RNNLM and fRNNLMwp . * denotes deletion errors, capitalized words denote substitution errors, and underlined words show their differences. 1.5 1.4 1 1 0.9 0.9 1 0.8 0.6 0.5 0.5 0.5 0.5 0.3 0.3 0.3 0.3 0.2 0.1 -0.2 -0.4 0 -0.5 tst2011 tst2010 Figure 5: Absolute improvement on each talk. Recently, syntactic parse trees are used in many advanced LMs (Chelba and Jelinek, 1998; Khudanpur and Wu, 2000; Xu et al., 2002; Collins et al., 2005; Rastrow et al., 2012). For future work, we intend to investigate topic information (Shi et al., 2012) and richer syntactic structure features into factored RNNLM, such as context-free rule productions, constituent/head features, and head-to-head dependencies that can be extracted using parser tools. Second, neural networks are notorious for being time consuming during training, future studies will also focus on speeding up the training of factored RNNLM using graphical processing units (Schwenk et al., 2012). Furthermore, factored RNNLMs need to be evaluated on other ta"
C12-1173,W04-3242,0,\N,Missing
C12-1173,C98-1035,0,\N,Missing
C96-1020,E91-1004,0,0.0283742,"T , U K rgg©comp, l a n e s , a c . uk {black,eubank,kashioka}@atr.itl.co.jp G.LeechOcentl.lancs.ac.uk 1 Introduction A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis. Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers (Black et al., 1992; Brill, 1994; Merialdo, 1994; Weischedel et al., 1993) and for statistical parsers (Black et al., 1993; Brill, 1993; aelinek et al., 1994; Magerman, 1995; Magerman and Marcus, 1991). In this article, we present the AT&apos;R/Lancaster 7&apos;reebauk of American English, a new resource tbr natural-language-, processing research, which has been prepared by Lancaster University (UK)&apos;s Unit for Computer Research on the English Language, according to specifications provided by ATR (Japan)&apos;s Statistical Parsing Group. First we provide a ""static"" description, with (a) a discussion of the mode of selection and initial processing of text for inclusion in the treebank, and (b) an explanation of the scheme of grammatical annotation we then apply to the text. Sec.ond, we supply a ""process"" de"
C96-1020,P95-1037,1,0.85353,"s t e r LA1 4 Y T , U K rgg©comp, l a n e s , a c . uk {black,eubank,kashioka}@atr.itl.co.jp G.LeechOcentl.lancs.ac.uk 1 Introduction A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis. Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers (Black et al., 1992; Brill, 1994; Merialdo, 1994; Weischedel et al., 1993) and for statistical parsers (Black et al., 1993; Brill, 1993; aelinek et al., 1994; Magerman, 1995; Magerman and Marcus, 1991). In this article, we present the AT&apos;R/Lancaster 7&apos;reebauk of American English, a new resource tbr natural-language-, processing research, which has been prepared by Lancaster University (UK)&apos;s Unit for Computer Research on the English Language, according to specifications provided by ATR (Japan)&apos;s Statistical Parsing Group. First we provide a ""static"" description, with (a) a discussion of the mode of selection and initial processing of text for inclusion in the treebank, and (b) an explanation of the scheme of grammatical annotation we then apply to the text. Sec.o"
C96-1020,J93-2004,0,0.0252211,"Missing"
C96-1020,H90-1054,0,0.114612,"Missing"
C96-1020,J93-2006,0,\N,Missing
C96-1020,H94-1052,0,\N,Missing
C96-1020,H94-1020,0,\N,Missing
C96-1020,P93-1035,0,\N,Missing
C96-1020,H93-1047,0,\N,Missing
C96-1020,J94-2001,0,\N,Missing
C96-1020,H92-1023,1,\N,Missing
C96-1020,1991.iwpt-1.22,1,\N,Missing
C98-1020,P97-1048,0,0.0352884,"Missing"
C98-1020,H92-1020,0,0.0223503,"us of around a million words. The subsequent experiments investigate the 1See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et ah, 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et ah, 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes My) of a random variable Y be observed and used to predict a random variable X (with alp"
C98-1020,H93-1016,0,0.0218224,"Missing"
C98-1020,H94-1052,0,0.0537282,"Missing"
C98-1020,H89-1054,0,0.0385685,"n words. The subsequent experiments investigate the 1See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et ah, 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et ah, 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes My) of a random variable Y be observed and used to predict a random variable X (with alphabet A x ) ."
C98-1020,P95-1037,0,0.0758488,"Missing"
C98-1020,W97-0301,0,0.0234852,"Missing"
C98-1020,C96-1020,1,0.670292,"Missing"
C98-1020,P96-1025,0,\N,Missing
C98-1020,W97-0105,1,\N,Missing
C98-1104,J92-4003,0,0.026876,"Missing"
C98-1104,A92-1018,0,0.0611342,"Missing"
C98-1104,A88-1019,0,0.0086381,"al systems to tackle this problem, uncontrolled heuristics are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on .Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate th"
C98-1104,C94-1032,0,0.148178,"ly used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on .Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunat"
C98-1104,W96-0113,0,0.489536,"use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on .Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunately, such a task"
D07-1004,P03-1003,0,0.262988,"Missing"
D07-1004,E06-1050,0,0.0671299,"Missing"
D07-1004,P02-1006,0,0.417849,"Missing"
D07-1004,W01-0509,0,0.0377519,"Missing"
D07-1004,C02-1119,0,0.0212896,"f questions. The deep NLP-based model [Moldovan et al. 2002; Hovy et al. 2001; and Pasca et al. 2001] usually parses the user question and an answer-bearing sentence into a semantic representation, and then semantically matches them to find the answer. This model has performed very well at TREC workshops, but it heavily depends on highperformance NLP tools, which are time consuming and labor intensive for many languages. Finally, the machine learning-based model has also been investigated. current models of this type are based on supervised approaches [Ittycheriah et al. 2001; Ng et al. 2001; Suzuki et al. 2002; and Sasaki et al. 2005] that are heavily dependent on hand-tagged questionanswer training pairs, which not readily available. In response to this situation, this paper presents the U-SVM for answer selection in open-domain web question answering system. Our U-SVM has the following advantages over supervised machine learning techniques. First, the U-SVM classifies questions into a question-dependent set of clusters, and the answer is the name of a question cluster. In contrast, most previous models have classified candidates into positive and negative. Second, the U-SVM automatically learns t"
D07-1004,P05-1027,0,0.735632,"Missing"
D07-1004,E03-1070,0,\N,Missing
D07-1004,P02-1054,0,\N,Missing
I05-4003,C04-1001,0,0.0243233,"inese Grammar Hideki Kashioka Yan Zhang ATR Spoken language ATR Spoken language Communication Research Communication Research Laboratories Laboratories 2-2-2 Keihanna Science City, 2-2-2 Keihanna Science City, Kyoto, 619-0288 Kyoto, 619-0288 Hideki.kashioka@atr.jp yan.zhang@atr.jp mantic and common sense. Grammar development has to depend on linguistic knowledge and the characteristics of the corpus to explicate a system of linguistic entities. However, it is expensive and time-consuming to maintain a robust grammar system by manual writing. Recently some researchers (H. Meng et al., 2002; S. Dipper, 2004 and Y. Ding, 2004) have presented a methodology to semi-automatically capture different grammar inductions from annotated corpora within restricted domains. A corpus-oriented approach (Y. Miyao, 2004) provides a way to extract grammars automatically from an annotated corpus. The specific language knowledge and knowledge relations need to be constructed and oriented to different corpora and tasks (K. Chen, 2004). The Chinese treebank is a useful resource for acquiring grammar rules and the context relations. Currently there are several Chinese treebanks on a scale of size. In the Penn Chinese"
I05-4003,xia-etal-2000-developing,0,0.0781252,"Missing"
I05-4003,W04-0212,0,0.0659328,"Missing"
I05-4003,P03-1057,0,\N,Missing
I05-4003,W04-0910,0,\N,Missing
I11-1020,P03-1003,0,0.0431498,"Missing"
I11-1020,W09-1125,0,0.0436241,"Missing"
I11-1020,C02-1130,0,0.0885058,"Missing"
I11-1020,P02-1006,0,0.0376049,"t entities are extracted. The main reason lies in: the NER tool is trained on newspapers, but we use it to tag web data. Therefore, it is necessary to filter out or negatively reward entities that do not match the fine-grained entity types if specified in queries. For example, this step can hopefully remove or negatively reward the extracted entities that are not airlines for the TREC 2009 test query shown in Figure 1. Many semi-supervised methods have been proposed to recognize fine-grained types of entities. For example, Hearst (1992) used lexical patterns such as “X, such as Y”. Fleischman (2002) employed a supervised learning method that considered the local context surrounding the entity as well as global semantic information. Etzioni (2005) started with a set of “predicates” and bootstrapped the extraction process from highprecision generic patterns. Oh (2009) exploited p(ei , hei |Q) = p(ei |Q) × p(hei |Q) (3) where p(ei |Q) stands for the probability of entity ei being an answer given query Q, and can be calculated using Equation (1) and (2), p(hei |Q) stands for the probability of homepage hei being an answer given query Q. By applying the Bayes’ Theorem and assuming that p(hei"
I11-1020,E03-1070,0,0.0481322,"Missing"
I11-1020,P09-1049,0,0.0362224,"Missing"
I11-1020,W09-1119,0,0.108602,"Missing"
I11-1020,P05-1027,0,0.0952587,"a particular web site (http://www.ijcnlp2011.org/). The TREC expert search task (2005-2008) (Nick, 2005) focused on creating an ordered list of experts who have skills and experiments on a specific topic with enterprise data. Most of the proposed approaches generally fall into two categories: generative language models and discriminative models. For example, Balog (2006) proposed profile-centric (directly models the knowledge of an expert from associated documents) and document-centric (locates documents on the topic and then finds the associated experts) generative language models (LMs). Cao (2005) proposed a two-stage language model consisting of a document relevance and co-occurrence model. There are many other generative probabilistic models such as (Fang, 2007; Serdyukov, 2008). Fang (2010) proposed a principled relevance-based discriminative model that integrates a variety of document evidence and document candidate association features for improving expert searching. The INEX entity ranking task (2007-2010) (Vries, 2007) studies ranking of Wikipedia entities to a query topic. Apart from estimating similarities between Wikipedia pages and the given query topic, many systems (Pehcev"
I11-1020,C92-2082,0,0.0205838,"ies such as airlines, universities, or actresses. Second, many incorrect entities are extracted. The main reason lies in: the NER tool is trained on newspapers, but we use it to tag web data. Therefore, it is necessary to filter out or negatively reward entities that do not match the fine-grained entity types if specified in queries. For example, this step can hopefully remove or negatively reward the extracted entities that are not airlines for the TREC 2009 test query shown in Figure 1. Many semi-supervised methods have been proposed to recognize fine-grained types of entities. For example, Hearst (1992) used lexical patterns such as “X, such as Y”. Fleischman (2002) employed a supervised learning method that considered the local context surrounding the entity as well as global semantic information. Etzioni (2005) started with a set of “predicates” and bootstrapped the extraction process from highprecision generic patterns. Oh (2009) exploited p(ei , hei |Q) = p(ei |Q) × p(hei |Q) (3) where p(ei |Q) stands for the probability of entity ei being an answer given query Q, and can be calculated using Equation (1) and (2), p(hei |Q) stands for the probability of homepage hei being an answer given"
I11-1020,D07-1004,1,\N,Missing
I11-1107,P09-1082,0,0.0345849,"Missing"
I11-1107,P03-1003,0,0.0687993,"Missing"
I11-1107,P08-1092,0,0.0366134,"Missing"
I11-1107,J03-1002,0,0.00714684,"Missing"
I11-1107,H05-1116,0,0.0880368,"Missing"
I11-1107,H05-1040,0,0.0603646,"Missing"
I11-1107,P08-1019,0,0.0589551,"Missing"
I11-1107,N07-1027,0,0.0545018,"Missing"
I11-1107,D09-1060,0,0.0599779,"Missing"
I11-1107,N06-1049,0,0.0606212,"Missing"
I11-1107,C02-1150,0,0.156452,"Missing"
I11-1107,P06-1136,0,0.0281749,"re selected to form an answer profile Ap. (3) Answer candidates are re-ranked according to the similarity formula sim(ai ) = γsim(Q, ai ) + (1−γ)sim(ai , Ap), where sim(Q, ai ) denotes the similarity between question Q and candidates ai , sim(ai , Ap) means the similarity between candidates and the answer profile Ap, γ is the weight. Both sim(Q, ai ) and sim(ai , Ap) are estimated using cosine similarity in this paper. (4) Finally, the top N candidates are selected as answers to Q. QSM is also widely used in answering definitional questions and TREC QA “other” questions (Kaisser et al., 2006; Chen, et al., 2006), which, however, learn answer words from the most relevant snippets returned by a Web search engine. Section 6 compares QSM based on 50 most relevant social Q&A pairs and that based on 50 most relevant snippets returned by Yahoo!. 5.2 MTM Figure 1: Example of dependency patterns The MTM learns word-to-word translation probability from all social Q&A pairs without consideration of the question and question type to improve complex QA system. The monolingual translation-based method treats Q&A pairs as the parallel corpus, with questions corresponding to the “source” language and answers to the"
I11-1107,P08-1082,0,0.0475295,"Missing"
I11-1107,C08-1063,0,0.0691116,"Missing"
I11-1107,I08-1055,0,0.0741218,"Missing"
I11-1107,P05-1027,0,0.0481438,"Missing"
I11-1107,P07-1059,0,\N,Missing
kashioka-2004-grouping,P02-1040,0,\N,Missing
kashioka-2004-grouping,P01-1008,0,\N,Missing
kashioka-2004-grouping,takezawa-etal-2002-toward,0,\N,Missing
kashioka-2004-grouping,shimohata-sumita-2002-automatic,0,\N,Missing
ohno-etal-2006-syntactically,J93-2004,0,\N,Missing
ohno-etal-2006-syntactically,P92-1041,0,\N,Missing
ohno-etal-2006-syntactically,maekawa-etal-2000-spontaneous,0,\N,Missing
ohtake-etal-2010-dialogue,W04-2319,0,\N,Missing
ohtake-etal-2010-dialogue,W07-1524,0,\N,Missing
ohtake-etal-2010-dialogue,P01-1066,0,\N,Missing
ohtake-etal-2010-dialogue,P06-1026,0,\N,Missing
ohtake-etal-2010-dialogue,J02-3001,0,\N,Missing
ohtake-etal-2010-dialogue,W02-0708,0,\N,Missing
ohtake-etal-2010-dialogue,2007.sigdial-1.45,0,\N,Missing
ohtake-etal-2010-dialogue,maekawa-etal-2000-spontaneous,0,\N,Missing
P06-1022,W02-2016,0,0.031121,"of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attributes, the conditional rel probability P (bik → bil |Bi ) is calculated as follows: rel P (bik →"
P06-1022,J94-4001,0,0.0440763,"` our method ``` correct conv. method ```` correct 1037 103 incorrect total 1,140 incorrect total 63 534 597 1,100 637 1,737 Since monologue sentences tend to be long and have complex structures, it is important to consider the features. Although there have been very few studies on parsing monologue sentences, some studies on parsing written language have dealt with long-sentence parsing. To resolve the syntactic ambiguity of a long sentence, some of them have focused attention on the “clause.” First, there are the studies that focused attention on compound clauses (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994). These tried to improve the parsing accuracy of long sentences by identifying the boundaries of coordinate structures. Next, other research efforts utilized the three categories into which various types of subordinate clauses are hierarchically classified based on the “scope-embedding preference” of Japanese subordinate clauses (Shirai et al., 1995; Utsuro et al., 2000). Furthermore, Kim et al. (Kim and Lee, 2004) divided a sentence into “S(ubject)-clauses,” which were defined as a group of words containing several predicates and their common subject. The above studies have attempted to reduc"
P06-1022,maekawa-etal-2000-spontaneous,0,0.0946367,"Missing"
P06-1022,P92-1003,0,0.0521559,"a clause boundary unit) ``` our method ``` correct conv. method ```` correct 1037 103 incorrect total 1,140 incorrect total 63 534 597 1,100 637 1,737 Since monologue sentences tend to be long and have complex structures, it is important to consider the features. Although there have been very few studies on parsing monologue sentences, some studies on parsing written language have dealt with long-sentence parsing. To resolve the syntactic ambiguity of a long sentence, some of them have focused attention on the “clause.” First, there are the studies that focused attention on compound clauses (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994). These tried to improve the parsing accuracy of long sentences by identifying the boundaries of coordinate structures. Next, other research efforts utilized the three categories into which various types of subordinate clauses are hierarchically classified based on the “scope-embedding preference” of Japanese subordinate clauses (Shirai et al., 1995; Utsuro et al., 2000). Furthermore, Kim et al. (Kim and Lee, 2004) divided a sentence into “S(ubject)-clauses,” which were defined as a group of words containing several predicates and their common subject. The above stu"
P06-1022,W97-0301,0,0.0218029,"unsetsu has one or more ancillary words, the type of dependency is the lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attribute"
P06-1022,A00-2018,0,0.0125219,"e lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attributes, the conditional rel probability P (bik → bil |Bi ) is calculate"
P06-1022,P96-1025,0,0.0724292,"f a dependent bunsetsu has one or more ancillary words, the type of dependency is the lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using"
P06-1022,P99-1053,0,0.0325274,", Nagoya University, Japan ‡Information Technology Center, Nagoya University, Japan §ATR Spoken Language Communication Research Laboratories, Japan ]The National Institute for Japanese Language, Japan Faculty of Information Science and Technology, Aichi Prefectural University, Japan a)ohno@el.itc.nagoya-u.ac.jp Abstract logues. Spontaneously spoken monologues include a lot of grammatically ill-formed linguistic phenomena such as fillers, hesitations and selfrepairs. In order to robustly deal with their extragrammaticality, some techniques for parsing of dialogue sentences have been proposed (Core and Schubert, 1999; Delmonte, 2003; Ohno et al., 2005b). On the other hand, monologues also have the characteristic feature that a sentence is generally longer and structurally more complicated than a sentence in dialogues which have been dealt with by the previous researches. Therefore, for a monologue sentence the parsing time would increase and the parsing accuracy would decrease. It is thought that more effective, high-performance spoken monologue parsing could be achieved by dividing a sentence into suitable language units for simplicity. This paper proposes a method for dependency parsing of monologue sen"
P06-1022,C04-1159,0,0.0355551,"od, the transcribed sentence on which morphological analysis, clause boundary detection, and bunsetsu segmentation are performed is considered the input 4 . The dependency 3 Asu-Wo-Yomu is a collection of transcriptions of a TV commentary program of the Japan Broadcasting Corporation (NHK). The commentator speaks on some current social issue for 10 minutes. 4 It is difficult to preliminarily divide a monologue into sentences because there are no clear sentence breaks in monologues. However, since some methods for detecting sentence boundaries have already been proposed (Huang and Zweig, 2002; Shitaoka et al., 2004), we assume that they can be detected automatically before dependency parsing. P (Si |Bi ) = nY i −1 k=1 171 rel P (bik → bil |Bi ), (1) rel Note that F is a co-occurrence frequency function. In order to resolve the sparse data problems rel caused by estimating P (bik → bil |Bi ) with formula (2), we adopted the smoothing method described by Fujio and Matsumoto (Fujio and Matsumoto, i 1998): if F (hik , hil , tik , til , rki , dii kl , el ) in formula (2) where P (bik → bil |Bi ) is the probability that a bunsetsu bik depends on a bunsetsu bil when the sequence of bunsetsus Bi is provided. Unl"
P06-1022,W98-1511,0,0.0175422,"more ancillary words, the type of dependency is the lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attributes, the conditional rel prob"
P06-1022,E99-1026,0,0.0206787,"ype of dependency is the lexicon, part-of-speech and conjugated form of the rightmost ancillary word, and if not so, it is the part-of-speech and conjugated form of the rightmost morpheme. The type of dependency rki is the same attribute used in our stochastic method proposed for robust dependency parsing of spoken language dialogue (Ohno et al., 2005b). Then dii kl takes 1 or more than 1, that is, a binary value. Incidentally, the above attributes are the same as those used by the conventional stochastic dependency parsing methods (Collins, 1996; Ratnaparkhi, 1997; Fujio and Matsumoto, 1998; Uchimoto et al., 1999; Charniak, 2000; Kudo and Matsumoto, 2002). Additionally, we prepared the attribute eil to indicate whether bil is the final bunsetsu of a clause boundary unit. Since we can consider a clause boundary unit as a unit corresponding to a simple sentence, we can treat the final bunsetsu of a clause boundary unit as a sentence-end bunsetsu. The attribute that indicates whether a head bunsetsu is a sentence-end bunsetsu has often been used in conventional sentence-by-sentence parsing methods (e.g. Uchimoto et al., 1999). By using the above attributes, the conditional rel probability P (bik → bil |B"
P06-1022,A00-2015,0,0.0164835,"nce parsing. To resolve the syntactic ambiguity of a long sentence, some of them have focused attention on the “clause.” First, there are the studies that focused attention on compound clauses (Agarwal and Boggess, 1992; Kurohashi and Nagao, 1994). These tried to improve the parsing accuracy of long sentences by identifying the boundaries of coordinate structures. Next, other research efforts utilized the three categories into which various types of subordinate clauses are hierarchically classified based on the “scope-embedding preference” of Japanese subordinate clauses (Shirai et al., 1995; Utsuro et al., 2000). Furthermore, Kim et al. (Kim and Lee, 2004) divided a sentence into “S(ubject)-clauses,” which were defined as a group of words containing several predicates and their common subject. The above studies have attempted to reduce the parsing ambiguity between specific types of clauses in order to improve the parsing accuracy of an entire sentence. On the other hand, our method utilizes all types of clauses without limiting them to specific types of clauses. To improve the accuracy of longsentence parsing, we thought that it would be more effective to cyclopaedically divide a sentence into all t"
P06-1022,C02-1136,1,\N,Missing
P98-1020,P97-1048,0,0.19381,"s and tags over our full corpus of around a million words. The subsequent experiments investigate the ]See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes A y ) of a random variable Y be observed and used to predict"
P98-1020,W97-0105,1,0.859821,"Missing"
P98-1020,C96-1020,1,0.692474,"; Collins, 1996; Jelinek et al., 1994; Magerman, 1995; Ratnaparkhi, 1997). All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results 7 The window was not allowed to cross a document boundary. The perplexity of the task before taking the trigger-pair information into account for tags was 224.0 and for rules was 57.0. The characteristics of the training corpus we employ are given in Table 1. The corpus, a subset s of the ATR/Lancaster General-English Treebank (Black et al., 1996), consists of a sequence of sentences which have been tagged and parsed by human experts in terms of the ATR English Grammar; a broad-coverage grammar of English with a high level of analytic detail (Black et al., 1996; Black et al., 1997). For instance, the tagset is both seman¢By rule assignment, we mean the task of assigning a rule-name to a node in a parse tree, given that the constituent boundaries have already been defined. 7This is not completely possible, since correlations, even if slight, will exist between intra- and extrasentential information Sspecifically, a roughly-900,000-word"
P98-1020,P96-1025,0,0.0863972,"Missing"
P98-1020,H92-1020,0,0.0228416,"s of around a million words. The subsequent experiments investigate the ]See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes A y ) of a random variable Y be observed and used to predict a random variable X (with"
P98-1020,H93-1016,0,0.0241345,"Missing"
P98-1020,H94-1052,0,0.0545148,"Missing"
P98-1020,H89-1054,0,0.0367426,"words. The subsequent experiments investigate the ]See Section 2. additional information gains accruing from triggerpair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger-pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information-theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes A y ) of a random variable Y be observed and used to predict a random variable X (with alphabet .Ax)."
P98-1020,P95-1037,0,0.0780142,"Missing"
P98-1020,W97-0301,0,0.0237869,"Missing"
P98-1108,J92-4003,0,0.0285758,"Missing"
P98-1108,A92-1018,0,0.0646998,"Missing"
P98-1108,A88-1019,0,0.00839802,"al systems to tackle this problem, uncontrolled heuristics are primarily used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the"
P98-1108,C94-1032,0,0.143546,"ly used. The use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunate"
P98-1108,W96-0113,0,0.494336,"use of information on character sorts, however, mitigates this difficulty. This paper presents our method of incorporating character clustering based on mutual information into DecisionTree Dictionary-less morphological analysis. By using natural classes, we have confirmed that our morphological analyzer has been significantly improved in both tokenizing and tagging Japanese text. 1 Introduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place word boundaries inconsistently. Consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications. For a practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to coordinate the dictionary used, tag sets, and numerous other parameters. Unfortunately, such a task"
W03-0312,A00-2018,0,\N,Missing
W03-0312,S01-1009,1,\N,Missing
W03-0312,2002.tmi-papers.9,0,\N,Missing
W03-0312,J94-4001,1,\N,Missing
W03-0312,W01-1402,0,\N,Missing
W03-0312,C94-1015,0,\N,Missing
W03-0312,C90-3044,0,\N,Missing
W03-0312,W01-1406,0,\N,Missing
W03-0312,2001.mtsummit-ebmt.4,0,\N,Missing
W03-0312,C90-3101,0,\N,Missing
W03-0312,2001.mtsummit-papers.5,1,\N,Missing
W03-1503,P96-1018,0,0.0160564,"m bilingual corpora, using literally translated parallel corpora, such as oﬃcial documents written in several languages makes it easier to get the desired information. However, not many of such corpora contain the latest NEs. There are few Japanese-English corpora which are translated literally. Therefore, we decided to extract NE translation pairs from content-aligned corpora, such as multilingual broadcast news articles including new NEs daily, which are not literally translated. Sentential alignment (Brown et al., 1991; Gale and Church, 1993; Kay and R¨oscheisen, 1993; Utsuro et al., 1994; Haruno and Yamazaki, 1996) is commonly used as a starting point for finding the translations of words or expressions from bilingual corpora. However, it is not always possible to correspond non-parallel corpora in sentences. Past statistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. These methods have a problem in covering NEs because there are many NEs that appear only once in a corpus. So we need a specialized method for extracting NE translation pairs. Transliteration is used for finding the translations of NE in the sourc"
W03-1503,M98-1002,0,0.0201997,"that if NE occurrence information, such as classes, number of occurrence and occurrence order, is given for each language, it may provide a good clue for corresponding NEs across languages. 1 Introduction Studies on named entity (NE) extraction are making progress for various languages, such as English and Japanese. A number of evaluation workshops have been held, including the Message Understanding Conference (MUC)1 for English and other languages, and the Information Retrieval and Extraction Exercise (IREX)2 for Japanese. Extraction accuracy for English has reached a nearly practical level (Marsh and Perzanowski, 1998). As for Japanese, it is more diﬃcult to find NE bound1 http://www.itl.nist.gov/iaui/894.02/ related_projects/muc/ 2 http://nlp.cs.nyu.edu/irex/ aries, however, NE extraction is relatively accurate (Sekine and Isahara, 2000). Most of the past research on NE extraction used monolingual corpora, but the application of NE extraction techniques to bilingual (or multilingual) corpora is expected to obtain NE translation pairs. We are developing a Japanese-English machine translation system for documents including many NEs, such as news articles or documents about current topics. Translating NE corr"
W03-1503,P02-1051,0,0.0158846,"from bilingual corpora. However, it is not always possible to correspond non-parallel corpora in sentences. Past statistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. These methods have a problem in covering NEs because there are many NEs that appear only once in a corpus. So we need a specialized method for extracting NE translation pairs. Transliteration is used for finding the translations of NE in the source language from texts in the target language (Stalls and Knight, 1998; Goto et al., 2001; Al-Onazian and Knight, 2002). Transliteration is useful for the names of persons and places; however, it is not applicable to all sorts of NEs. Content-aligned documents, such as a bilingual news corpus, are made to convey the same topics. Since NEs are the essential element of document contents, content-aligned documents are likely to share NEs pointing to the same objects. Consequently, when extracting all NEs with NE class information from each of a pair of bilingual documents separately by applying monolingual NE extraction techniques, the distribution of the NEs in each document may be similar enough to recognize co"
W03-1503,P91-1022,0,0.0297391,"on knowledge from the latest bilingual documents. When extracting translation knowledge from bilingual corpora, using literally translated parallel corpora, such as oﬃcial documents written in several languages makes it easier to get the desired information. However, not many of such corpora contain the latest NEs. There are few Japanese-English corpora which are translated literally. Therefore, we decided to extract NE translation pairs from content-aligned corpora, such as multilingual broadcast news articles including new NEs daily, which are not literally translated. Sentential alignment (Brown et al., 1991; Gale and Church, 1993; Kay and R¨oscheisen, 1993; Utsuro et al., 1994; Haruno and Yamazaki, 1996) is commonly used as a starting point for finding the translations of words or expressions from bilingual corpora. However, it is not always possible to correspond non-parallel corpora in sentences. Past statistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. These methods have a problem in covering NEs because there are many NEs that appear only once in a corpus. So we need a specialized method for extra"
W03-1503,P98-1069,0,0.0177718,"ranslated literally. Therefore, we decided to extract NE translation pairs from content-aligned corpora, such as multilingual broadcast news articles including new NEs daily, which are not literally translated. Sentential alignment (Brown et al., 1991; Gale and Church, 1993; Kay and R¨oscheisen, 1993; Utsuro et al., 1994; Haruno and Yamazaki, 1996) is commonly used as a starting point for finding the translations of words or expressions from bilingual corpora. However, it is not always possible to correspond non-parallel corpora in sentences. Past statistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. These methods have a problem in covering NEs because there are many NEs that appear only once in a corpus. So we need a specialized method for extracting NE translation pairs. Transliteration is used for finding the translations of NE in the source language from texts in the target language (Stalls and Knight, 1998; Goto et al., 2001; Al-Onazian and Knight, 2002). Transliteration is useful for the names of persons and places; however, it is not applicable to all sorts of NEs. Content-aligned documents, such as"
W03-1503,J93-1004,0,0.012648,"e latest bilingual documents. When extracting translation knowledge from bilingual corpora, using literally translated parallel corpora, such as oﬃcial documents written in several languages makes it easier to get the desired information. However, not many of such corpora contain the latest NEs. There are few Japanese-English corpora which are translated literally. Therefore, we decided to extract NE translation pairs from content-aligned corpora, such as multilingual broadcast news articles including new NEs daily, which are not literally translated. Sentential alignment (Brown et al., 1991; Gale and Church, 1993; Kay and R¨oscheisen, 1993; Utsuro et al., 1994; Haruno and Yamazaki, 1996) is commonly used as a starting point for finding the translations of words or expressions from bilingual corpora. However, it is not always possible to correspond non-parallel corpora in sentences. Past statistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. These methods have a problem in covering NEs because there are many NEs that appear only once in a corpus. So we need a specialized method for extracting NE translation pa"
W03-1503,sekine-isahara-2000-irex,0,0.0239299,"y (NE) extraction are making progress for various languages, such as English and Japanese. A number of evaluation workshops have been held, including the Message Understanding Conference (MUC)1 for English and other languages, and the Information Retrieval and Extraction Exercise (IREX)2 for Japanese. Extraction accuracy for English has reached a nearly practical level (Marsh and Perzanowski, 1998). As for Japanese, it is more diﬃcult to find NE bound1 http://www.itl.nist.gov/iaui/894.02/ related_projects/muc/ 2 http://nlp.cs.nyu.edu/irex/ aries, however, NE extraction is relatively accurate (Sekine and Isahara, 2000). Most of the past research on NE extraction used monolingual corpora, but the application of NE extraction techniques to bilingual (or multilingual) corpora is expected to obtain NE translation pairs. We are developing a Japanese-English machine translation system for documents including many NEs, such as news articles or documents about current topics. Translating NE correctly is indispensable for conveying information correctly. NE translations, however, are not listed in conventional dictionaries. It is necessary to retrieve NE translation knowledge from the latest bilingual documents. Whe"
W03-1503,W98-1005,0,0.0115897,"ing the translations of words or expressions from bilingual corpora. However, it is not always possible to correspond non-parallel corpora in sentences. Past statistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. These methods have a problem in covering NEs because there are many NEs that appear only once in a corpus. So we need a specialized method for extracting NE translation pairs. Transliteration is used for finding the translations of NE in the source language from texts in the target language (Stalls and Knight, 1998; Goto et al., 2001; Al-Onazian and Knight, 2002). Transliteration is useful for the names of persons and places; however, it is not applicable to all sorts of NEs. Content-aligned documents, such as a bilingual news corpus, are made to convey the same topics. Since NEs are the essential element of document contents, content-aligned documents are likely to share NEs pointing to the same objects. Consequently, when extracting all NEs with NE class information from each of a pair of bilingual documents separately by applying monolingual NE extraction techniques, the distribution of the NEs in ea"
W03-1503,C94-2175,0,0.0169735,"slation knowledge from bilingual corpora, using literally translated parallel corpora, such as oﬃcial documents written in several languages makes it easier to get the desired information. However, not many of such corpora contain the latest NEs. There are few Japanese-English corpora which are translated literally. Therefore, we decided to extract NE translation pairs from content-aligned corpora, such as multilingual broadcast news articles including new NEs daily, which are not literally translated. Sentential alignment (Brown et al., 1991; Gale and Church, 1993; Kay and R¨oscheisen, 1993; Utsuro et al., 1994; Haruno and Yamazaki, 1996) is commonly used as a starting point for finding the translations of words or expressions from bilingual corpora. However, it is not always possible to correspond non-parallel corpora in sentences. Past statistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. These methods have a problem in covering NEs because there are many NEs that appear only once in a corpus. So we need a specialized method for extracting NE translation pairs. Transliteration is used for finding the tra"
W03-1503,C98-1066,0,\N,Missing
W03-1503,J93-1006,0,\N,Missing
W05-1204,E03-1029,0,0.0163223,"only roughly process sentence pairs, though the process of making groups is very simple. Sometimes a group may include sentences or words that have slightly different meanings, such as. fukuro (bag), kamibukuro (paper bag), shoppingu baggu (shopping bag), tesagebukuro (tote bag), and biniiru bukuro (plastic bag). In this case if we select tesagebukuro from the Japanese side and “paper bag” from the English side, we have an incorrect word pair in the translation model. To handle such a problem, we would have to arrange a method to select the sentences from a group. This problem is discussed in Imamura et al. (2003). As one solution to this problem, we borrowed the measures of literalness, context freedom, and word translation stability in the sentence-selection process. In some cases, the group includes sentences with different meanings, and this problem was mentioned in Kashioka (2004). In an attempt to solve the problem, he performed a secondary decomposition step to produce a synonymous group. However, in the current training corpus, each synonymous group before the decomposition step is small, so there would not be enough difference for modifications after the decomposition step. The replacement of"
W05-1204,kashioka-2004-grouping,1,0.926639,"e a negative effect on training in SMT because when several sentences of input-side language are translated into the exactly equivalent output-side sentences, the probability of correct translation decreases due to the large number of possible pairs of expressions. Therefore, if we can restrain or modify the training corpus, the SMT system might achieve high accuracy. As an example of modification, different output-side sentences paired with the exactly equivalent input-side sentences are replaced with one target sentence. These sentence replacements are required for synonymous sentence sets. Kashioka (2004) discussed synonymous sets of sentences. Here, we employ a method to group them as a way of modifying the training corpus for use with SMT. This paper focuses on how to control the corpus while giving consideration to synonymous sentence groups. 19 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 19–24, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics 2 Target Corpus In this paper, we use a multilingual parallel corpus called BTEC (Takezawa et al., 2002) for our experiments. BTEC was used in IWSLT (Akiba et al., 2004). Th"
W05-1204,niessen-etal-2000-evaluation,0,0.0113607,"ide. For the evaluation, we use BLEU, NIST, WER, and PER as follows: BLEU: A weighted geometric mean of the ngram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences. Figure 4. Modification example of replacing the output side’s sentence S1⇔T1 S2⇔T1 S1⇔T2 S3⇔T1 ⇒ S1⇔T1 S1⇔T1 S1⇔T2 S1⇔T1 NIST: An arithmetic mean of the n-gram matches between test and reference sentences multiplied by a length factor, which again penalizes short translation sentences. Figure 5. Modification example of replacing the input side’s sentence 4 mWER (Niessen et al., 2000): Multiple reference word-error rate, which computes the edit distance (minimum number of insertions, deletions, and substitutions) between test and reference sentences. SMT System and Evaluation method In this section, we describe the SMT systems used in these experiments. The SMT systems’ decoder is a graph-based decoder (Ueffing et al., 2002; Zhang et al., 2004). The first pass of the decoder generates a word-graph, a compact representation of alternative translation candidates, using a beam search based on the scores of the lexicon and language models. In the second pass, an A* search trav"
W05-1204,J03-1002,0,0.00274822,"the graph. The edges of the word-graph, or the phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model. The phrase translations extracted from the Viterbi alignments of the training corpus also constitute the edges. Similarly, the edges are also created from dynamically extracted phrase translations from the bilingual sentences (Watanabe and Sumita, 2003). The decoder used the IBM Model 4 with a trigram language model and a five-gram part-of-speech language model. Training of the IBM model 4 was implemented by the GIZA++ package (Och and Ney, 2003). All parameters in training and decoding were the same for all experiments. Most systems with this training can be expected to achieve better accuracy when we run the parameter tuning processes. However, our purpose is to compare the difference in results caused by modifying the training corpus. We performed experiments for JE/EJ and JC/CJ systems and four types of training corpora: 1) Original BTEC corpus; 2) Compressed BTEC corpus (see 3.2.1); 3) Replace both languages (see 3.2.2); 22 mPER: Multiple reference position-independent word-error rate, which computes the edit distance without con"
W05-1204,P02-1040,0,0.0729299,"curacy is achieved. Usually, the quantity problem of the training corpus is discussed in relation to the size of the training corpus and system performance; therefore, researchers study line graphs that indicate the relationship between accuracy and training corpus size. On the other hand, needless to say, a single sentence in the source language can be used to translate several sentences in the target language. Such various possibilities for translation make MT system development and evaluation very difficult. Consequently, here we employ multiple references to evaluate MT systems like BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). Moreover, such variations in translation have a negative effect on training in SMT because when several sentences of input-side language are translated into the exactly equivalent output-side sentences, the probability of correct translation decreases due to the large number of possible pairs of expressions. Therefore, if we can restrain or modify the training corpus, the SMT system might achieve high accuracy. As an example of modification, different output-side sentences paired with the exactly equivalent input-side sentences are replaced with one target sentenc"
W05-1204,shimohata-etal-2004-building,0,0.0237166,"rowed the measures of literalness, context freedom, and word translation stability in the sentence-selection process. In some cases, the group includes sentences with different meanings, and this problem was mentioned in Kashioka (2004). In an attempt to solve the problem, he performed a secondary decomposition step to produce a synonymous group. However, in the current training corpus, each synonymous group before the decomposition step is small, so there would not be enough difference for modifications after the decomposition step. The replacement of a sentence could be called paraphrasing. Shimohata et al. (2004) reported a paraphrasing effect in MT systems, where if each group would have the same meaning, the variation in the phrases that appeared in the other groups would reduce the probability. Therefore, considering our results in light of their discussion, if the training corpus could be modified with the module for paraphrasing in order to control phrases, we could achieve better performance. 7 Conclusion This paper described the modification of a training set based on a synonymous sentence group for a statistical machine translation system in order to attain better performance. In an EJ/JE syst"
W05-1204,takezawa-etal-2002-toward,0,0.0623804,"Missing"
W05-1204,W02-1021,0,0.0134503,"T2 S1⇔T1 NIST: An arithmetic mean of the n-gram matches between test and reference sentences multiplied by a length factor, which again penalizes short translation sentences. Figure 5. Modification example of replacing the input side’s sentence 4 mWER (Niessen et al., 2000): Multiple reference word-error rate, which computes the edit distance (minimum number of insertions, deletions, and substitutions) between test and reference sentences. SMT System and Evaluation method In this section, we describe the SMT systems used in these experiments. The SMT systems’ decoder is a graph-based decoder (Ueffing et al., 2002; Zhang et al., 2004). The first pass of the decoder generates a word-graph, a compact representation of alternative translation candidates, using a beam search based on the scores of the lexicon and language models. In the second pass, an A* search traverses the graph. The edges of the word-graph, or the phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model. The phrase translations extracted from the Viterbi alignments of the training corpus also constitute the edges. Similarly, the edges are also created from dynamically extrac"
W05-1204,2003.mtsummit-papers.54,0,0.0163074,"enerates a word-graph, a compact representation of alternative translation candidates, using a beam search based on the scores of the lexicon and language models. In the second pass, an A* search traverses the graph. The edges of the word-graph, or the phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model. The phrase translations extracted from the Viterbi alignments of the training corpus also constitute the edges. Similarly, the edges are also created from dynamically extracted phrase translations from the bilingual sentences (Watanabe and Sumita, 2003). The decoder used the IBM Model 4 with a trigram language model and a five-gram part-of-speech language model. Training of the IBM model 4 was implemented by the GIZA++ package (Och and Ney, 2003). All parameters in training and decoding were the same for all experiments. Most systems with this training can be expected to achieve better accuracy when we run the parameter tuning processes. However, our purpose is to compare the difference in results caused by modifying the training corpus. We performed experiments for JE/EJ and JC/CJ systems and four types of training corpora: 1) Original BTEC"
W05-1204,C04-1168,0,0.23287,"ive effect on training in SMT because when several sentences of input-side language are translated into the exactly equivalent output-side sentences, the probability of correct translation decreases due to the large number of possible pairs of expressions. Therefore, if we can restrain or modify the training corpus, the SMT system might achieve high accuracy. As an example of modification, different output-side sentences paired with the exactly equivalent input-side sentences are replaced with one target sentence. These sentence replacements are required for synonymous sentence sets. Kashioka (2004) discussed synonymous sets of sentences. Here, we employ a method to group them as a way of modifying the training corpus for use with SMT. This paper focuses on how to control the corpus while giving consideration to synonymous sentence groups. 19 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 19–24, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics 2 Target Corpus In this paper, we use a multilingual parallel corpus called BTEC (Takezawa et al., 2002) for our experiments. BTEC was used in IWSLT (Akiba et al., 2004). Th"
W05-1204,2004.iwslt-evaluation.1,0,\N,Missing
W09-3405,2007.sigdial-1.45,0,0.0254459,"o express the dialogue act of each utterance. Many studies have focused on developing spoken dialogue systems. Their typical task domains included the retrieval of information from databases or making reservations, such as airline information e.g., DARPA Communicator (Walker et al., 2001) and train information e.g., ARISE (Bouwman et al., 1999) and MASK (Lamel et al., 2002). Most studies assumed a definite and consistent user objective, and the dialogue strategy was usually designed to minimize the cost of information access. Other target tasks include tutoring and trouble-shooting dialogues (Boye, 2007). 32 Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32–39, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP Table 2: Overview of Kyoto corpus dialogue type F2F # of dialogues 114 3 # of guides avg. # of utterance 365.4 / dialogue (guide) avg. # of utterance 301.7 / dialogue (tourist) sional tour guide and a tourist. Three guides, one male and two females, were employed to collect the dialogues. All three guides were involved in almost the same number of dialogues. The guides used maps, guidebooks, and a PC connected to the internet. tour guide dialo"
W09-3405,J02-3001,0,0.0350253,"e t y pe ……. o b j ect Figure 3: A part of the semantic category hierarchy Kinkakuji temple.” is annotated as shown in Figure 2. In this figure, the semantic content tag preference.action indicates that the predicate portion expresses the speaker’s preference for the speaker’s action, while the semantic content tag preference.spot.name indicates the name of the spot as the object of the speaker’s preference. Although we do not define semantic the role (e.g., object (Kinakuji temple) and subject (I)) of each argument item in this case, we can use conventional semantic role labeling techniques (Gildea and Jurafsky, 2002) to estimate them. Therefore, we do not annotate such semantic role labels in the corpus. 5.1 Annotation of semantic contents tags The annotation of semantic contents tags is performed by the following four steps. First, an utterance is analyzed by a morphological analyzer, ChaSen3 . Second, the morphemes are chunked into dependency unit (bunsetsu). Third, dependency analysis is performed using a Japanese dependency parser, CaboCha4 . Finally, we annotate the semantic content tags for each bunsetsu unit by using our annotation tool. An example of an annotation is shown in Table 1. Each row in"
W09-3405,W02-0708,0,0.13753,"collaboratively operate the WOZ system to serve a user (tourist). structure of the transducers and the weight for each state transitions from an annotated corpus. Thus, the corpus must be sufficiently rich in information to describe the consulting dialogue to construct the statistical dialogue manager via such techniques. In addition, a detailed description would be preferable when developing modules that focus on spoken language understanding and generation modules. In this study, we adopt dialogue acts (DAs) (Bunt, 2000; Shriberg et al., 2004; Bangalore et al., 2006; Rodriguez et al., 2007; Levin et al., 2002) for this information and annotate DAs in the corpus. In this paper, we describe the design of the Kyoto tour guide dialogue corpus in Section 2. Our design of the DA annotation is described in Section 3. Sections 4 and 5 respectively describe two types of the tag sets, namely, the speech act tag and the semantic content tag. In the telephone dialogues, two female guides who are the same for the WOZ dialogues were employed. In these dialogues, we used the WOZ system, but we did not need the speech synthesis program. The guide and a tourist shared the same interface in different rooms, and they"
W09-3405,maekawa-etal-2000-spontaneous,0,0.0469089,"ideki Kashioka Satoshi Nakamura MASTAR Project, National Institute of Information and Communications Technology Hikaridai, Keihanna Science City, JAPAN kiyonori.ohtake (at) nict.go.jp Abstract In such tasks, dialogue scenarios or agendas are usually described using a (dynamic) tree structure, and the objective is to satisfy all requirements. In this paper, we introduce our corpus, which is being developed as part of a project to construct consulting dialogue systems, that helps the user in making a decision. So far, several projects have been organized to construct speech corpora such as CSJ (Maekawa et al., 2000) for Japanese. The size of CSJ is very large, and a great part of the corpus consists of monologues. Although, CSJ includes some dialogues, the size of dialogues is not enough to construct a dialogue system via recent statistical techniques. In addition, relatively to consulting dialogues, the existing large dialogue corpora covered very clear tasks in limited domains. However, consulting is a frequently used and very natural form of human interaction. We often consult with a sales clerk while shopping or with staff at a concierge desk in a hotel. Such dialogues usually form part of a series o"
W09-3405,W07-1524,0,0.0612339,"Missing"
W09-3405,W04-2319,0,0.243262,"e guide and operators used own computer connected each other, and they collaboratively operate the WOZ system to serve a user (tourist). structure of the transducers and the weight for each state transitions from an annotated corpus. Thus, the corpus must be sufficiently rich in information to describe the consulting dialogue to construct the statistical dialogue manager via such techniques. In addition, a detailed description would be preferable when developing modules that focus on spoken language understanding and generation modules. In this study, we adopt dialogue acts (DAs) (Bunt, 2000; Shriberg et al., 2004; Bangalore et al., 2006; Rodriguez et al., 2007; Levin et al., 2002) for this information and annotate DAs in the corpus. In this paper, we describe the design of the Kyoto tour guide dialogue corpus in Section 2. Our design of the DA annotation is described in Section 3. Sections 4 and 5 respectively describe two types of the tag sets, namely, the speech act tag and the semantic content tag. In the telephone dialogues, two female guides who are the same for the WOZ dialogues were employed. In these dialogues, we used the WOZ system, but we did not need the speech synthesis program. The guide"
W09-3405,P01-1066,0,0.0305516,"Missing"
W09-3405,P06-1026,0,\N,Missing
W10-4339,W09-3405,1,0.921607,"ed on the state model through reinforcement learning with a natural policy gradient approach using a user simulator trained on the collected dialogue corpus. 1 p1 Criteria 1. Cherry Blossoms v11 … Alternatives (choices) KinkakujiTemple p2 2. Japanese Garden v12 … RyoanjiTemple p3 3. Easy Access ・・・・・ v13 … NanzenjiTemple ・・・・・ Figure 1: Hierarchy structure for sightseeing guidance dialogue by a car navigation system. In this work, we deal with a sightseeing planning task where the user determines the sightseeing spot to visit, with little prior knowledge about the target domain. The study of (Ohtake et al., 2009), which investigated human-human dialogue in such a task, reported that such consulting usually consists of a sequence of information requests from the user, presentation and elaboration of information about certain spots by the guide followed by the user’s evaluation. We thus focus on these interactions. Several studies have featured decision support systems in the operations research field, and the typical method that has been employed is the Analytic Hierarchy Process (Saaty, 1980) (AHP). In AHP, the problem is modeled as a hierarchy that consists of the decision goal, the alternatives for"
W10-4339,P08-1055,0,0.0295842,"The information about the spot in terms of the criteria is not known to the users, but is obtained only via navigating through the system’s information. In addition, spoken dialogue systems usually handle several candidates and criteria, making pairwise comparison a costly affair. We thus consider a spoken dialogue framework that estimates the weights for the user’s preference (potential preferences) as well as the user’s knowledge Introduction In many situations where spoken dialogue interfaces are used, information access by the user is not a goal in itself, but a means for decision making (Polifroni and Walker, 2008). For example, in a restaurant retrieval system, the user’s goal may not be the extraction of price information but to make a decision on candidate restaurants based on the retrieved information. This work focuses on how to assist a user who is using the system for his/her decision making, when he/she does not have enough knowledge about the target domain. In such a situation, users are often unaware of not only what kind of information the system can provide but also their own preference or factors that they should emphasize. The system, too, has little knowledge about the user, or where his/"
W10-4339,N07-2038,0,0.0283932,"parameter Kuser = (k1 , k2 , . . . , kM ) that shows if the user has the perception that the system is able to handle or he/she is interested in the determinants. km is set to “1” if the user knows (or is listed by system’s recommendations) that the system can handle determinant m and “0” when he/she does not. For example, the state that the determinant m is the potential preference of a user (but he/she is unaware of that) is represented by (km = 0, pm = 1). This idea is in contrast to previous research which assumes some fixed goal observable by the user from the beginning of the dialogue (Schatzmann et al., 2007). A user’s local weight vnm for spot n in terms of determinant m is set to “1”, when the system lets the user know that the evaluation of spots is “1” through recommendation Methods 1, 2 and 6. We constructed a user simulator that is based on the statistics calculated through an experiment with the trial system (Misu et al., 2010) as well as the knowledge and preference of the user. That is, the user’s communicative act catuser and the semantic content sctuser for the system’s recommendation atsys are generated based on the following equation: Knowledge base Our back-end DB consists of 15 sigh"
W97-0105,C96-1020,1,0.831002,"of the grammar and of the lexical generalizations created by our grammarian. Section 3 shows; from a formal standpoint, how prediction is carried out, and more generally how the parser operates. Section 4 presents experimental results. Finally, Section 5 details our efforts to radically expand the size of our training corpus by employing techniques of treebank conversion. 2. H O W T H E GB.AiVIMAB. A N D L E X I C A L G E N E R A L I Z A T I O N S 2.1. HELP How the Grammar Helps Figure 1 shows a sampling of parsed sentences from the one-million-word ATR/Lancaster 'IYeebauk of General English (Black et al., 1996), which we employ for training; smoothing and testing our parser. The Treebank consists of a correct parse for each sentence it contains; with respect to the ATR English Grammar. 1 Every non-terminal node is labelled with the n~rae of the ATR English Grammar rule2 that generates the node; and each word is labelled with one of the 2843 tags in the Gramm,r's tagset. 3 Together, the bracket locations, rule names, and lexical tags of a Treebank parse specify a unique parse within the Gr~rnra~r. In the Grammar parse, rule names and lexical tags are replaced by bundles of feature/value pairs. Each n"
W97-0105,P93-1005,1,0.935578,"995), and as in these references, training is supervised, and in particular is treebank-based. In all other respects, our work departs from previous research on broad--coverage 16 i ! I i I I I I I I I I I I I i I 1, . I I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict gr~rarn~tical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al., 1994; Magerman, 1995; S~kine and Orishman, 1995; Sharman et al., 1990), or employs a grammar and sometimes a dictionary to capture linguistic expertise directly (Black et al., 1993a; GrinBerg et al., 1995; Schabes; 1992), but arguably at a less detailed and informative level than in the research reported here. In what follows, Section 2 explains the contribution to the prediction process of the grammar and of the lexical generalizations created by our grammarian. Section 3 shows; from a formal standpoint, how prediction is carried out, and more generally how the parser operates. Section 4 presents experimental results. Finally, Section 5 details our efforts to radically expand the size of our training corpus by employing techniques of treebank conversion. 2. H O W T H E"
W97-0105,P93-1035,0,0.0161115,"for the parser as trained and tested on a large, highly varied treebank of unrestricted English text. Probabilistic decision trees are utilized as a mea.ns of prediction, roughly as in (Jelinek et al., 1994; Magermau, 1995), and as in these references, training is supervised, and in particular is treebank-based. In all other respects, our work departs from previous research on broad--coverage 16 i ! I i I I I I I I I I I I I i I 1, . I I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict gr~rarn~tical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al., 1994; Magerman, 1995; S~kine and Orishman, 1995; Sharman et al., 1990), or employs a grammar and sometimes a dictionary to capture linguistic expertise directly (Black et al., 1993a; GrinBerg et al., 1995; Schabes; 1992), but arguably at a less detailed and informative level than in the research reported here. In what follows, Section 2 explains the contribution to the prediction process of the grammar and of the lexical generalizations created by our grammarian. Section 3 shows; from a formal standpoint, how prediction is carried out, and more ge"
W97-0105,J92-4003,0,0.0221075,"Missing"
W97-0105,P96-1025,0,0.0935504,"er as trained and tested on a large, highly varied treebank of unrestricted English text. Probabilistic decision trees are utilized as a mea.ns of prediction, roughly as in (Jelinek et al., 1994; Magermau, 1995), and as in these references, training is supervised, and in particular is treebank-based. In all other respects, our work departs from previous research on broad--coverage 16 i ! I i I I I I I I I I I I I i I 1, . I I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict gr~rarn~tical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al., 1994; Magerman, 1995; S~kine and Orishman, 1995; Sharman et al., 1990), or employs a grammar and sometimes a dictionary to capture linguistic expertise directly (Black et al., 1993a; GrinBerg et al., 1995; Schabes; 1992), but arguably at a less detailed and informative level than in the research reported here. In what follows, Section 2 explains the contribution to the prediction process of the grammar and of the lexical generalizations created by our grammarian. Section 3 shows; from a formal standpoint, how prediction is carried out, and more generally how the"
W97-0105,C96-1058,0,0.0148066,"nd tested on a large, highly varied treebank of unrestricted English text. Probabilistic decision trees are utilized as a mea.ns of prediction, roughly as in (Jelinek et al., 1994; Magermau, 1995), and as in these references, training is supervised, and in particular is treebank-based. In all other respects, our work departs from previous research on broad--coverage 16 i ! I i I I I I I I I I I I I i I 1, . I I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict gr~rarn~tical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al., 1994; Magerman, 1995; S~kine and Orishman, 1995; Sharman et al., 1990), or employs a grammar and sometimes a dictionary to capture linguistic expertise directly (Black et al., 1993a; GrinBerg et al., 1995; Schabes; 1992), but arguably at a less detailed and informative level than in the research reported here. In what follows, Section 2 explains the contribution to the prediction process of the grammar and of the lexical generalizations created by our grammarian. Section 3 shows; from a formal standpoint, how prediction is carried out, and more generally how the parser operat"
W97-0105,1995.iwpt-1.15,0,0.0208885,"Missing"
W97-0105,H94-1052,0,0.52696,"y, a statistical procedure is described for converting less-detailed into more--detailed treebank, for use in increasing parser accuracy via much larger training treeb~.nlc~. S u b j e c t A r e a s : statistical parsing; automatic treebank conversion; semantic and syntactic analysis of text 1. t I I I I INTRODUCTION This article describes a grammar-based probabilistic parser, and presents experimental results for the parser as trained and tested on a large, highly varied treebank of unrestricted English text. Probabilistic decision trees are utilized as a mea.ns of prediction, roughly as in (Jelinek et al., 1994; Magermau, 1995), and as in these references, training is supervised, and in particular is treebank-based. In all other respects, our work departs from previous research on broad--coverage 16 i ! I i I I I I I I I I I I I i I 1, . I I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict gr~rarn~tical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al., 1994; Magerman, 1995; S~kine and Orishman, 1995; Sharman et al., 1990), or employs a grammar and sometimes a dictionary to capture linguistic experti"
W97-0105,P95-1037,1,0.859072,"treebank of unrestricted English text. Probabilistic decision trees are utilized as a mea.ns of prediction, roughly as in (Jelinek et al., 1994; Magermau, 1995), and as in these references, training is supervised, and in particular is treebank-based. In all other respects, our work departs from previous research on broad--coverage 16 i ! I i I I I I I I I I I I I i I 1, . I I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict gr~rarn~tical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al., 1994; Magerman, 1995; S~kine and Orishman, 1995; Sharman et al., 1990), or employs a grammar and sometimes a dictionary to capture linguistic expertise directly (Black et al., 1993a; GrinBerg et al., 1995; Schabes; 1992), but arguably at a less detailed and informative level than in the research reported here. In what follows, Section 2 explains the contribution to the prediction process of the grammar and of the lexical generalizations created by our grammarian. Section 3 shows; from a formal standpoint, how prediction is carried out, and more generally how the parser operates. Section 4 presents experimental re"
W97-0105,J93-2004,0,0.0233754,"ature, on average. Prediction in our parser is conditioned partially on questions about feature values of words and non-terminal nodes. For instance, when we predict whether a constituent has ended, we ask how many words until the next finite verb; the next comma; the next noun; etc. In tagging, we ask if the same word has already occurred in the sentence; and ff so, what its value is for various features. By labelling Treeb~n~ nodes with Gr~ramar rule names, and not with phrasal and clausal n~raes, as in other (non-gr~rarnar-based) treebanks' (Eyes and Leech, 1993; Garside and McEnery, 1993; Marcus et al., 1993), we gain access to all information provided by the Grammar regarding each ~reebank node. It would be difficult to attempt to induce this information from the Treebank alone. The parent of a rule in the Grammar often contains feature values that are not derived from any of its children. Further~ the parent inherits some feature values from one child, and some from another. Each rule in the Grammar is associated with a primary and secondary head, and head information is passed up the parse tree. Finally, extensive Boolean conditions are imposed on the application of each individual rule. These"
W97-0105,H92-1027,0,0.0446467,"Missing"
W97-0105,1995.iwpt-1.26,0,0.0325234,"Missing"
W97-0105,H90-1054,0,0.0175666,"abilistic decision trees are utilized as a mea.ns of prediction, roughly as in (Jelinek et al., 1994; Magermau, 1995), and as in these references, training is supervised, and in particular is treebank-based. In all other respects, our work departs from previous research on broad--coverage 16 i ! I i I I I I I I I I I I I i I 1, . I I I I I i I 1 I I I I probabilistic parsing, which either attempts to learn to predict gr~rarn~tical structure of test data directly from a training treebank (Brill, 1993; Collins, 1996; Eisner, 1996; Jelinek et al., 1994; Magerman, 1995; S~kine and Orishman, 1995; Sharman et al., 1990), or employs a grammar and sometimes a dictionary to capture linguistic expertise directly (Black et al., 1993a; GrinBerg et al., 1995; Schabes; 1992), but arguably at a less detailed and informative level than in the research reported here. In what follows, Section 2 explains the contribution to the prediction process of the grammar and of the lexical generalizations created by our grammarian. Section 3 shows; from a formal standpoint, how prediction is carried out, and more generally how the parser operates. Section 4 presents experimental results. Finally, Section 5 details our efforts to r"
W97-0105,W96-0103,0,0.0121662,"the number of children, span, constituent boundaries, etc. is available. 19 Answers to the questions are of various types: Boolean, categorical,integer, sets of integers. But we transform all these types of answers into binary strings. Some transformations are obvious. Boolean values, for example, are mapped to a single bit. Other transformations are based on clustering, either expert or automatic. For example, the sets of tags and rule labels have been clustered by our team gr~:mm~trian,while a vocabulary of about 60,000 words has been clustered by machine (Brown et al.,1992; Ushioda~ 1996a; Ushioda, 1996b). 3. 3.1. . t HOW PREDICTION IS CARRIED OUT System Design The ATR parser is a probabilistic parser which uses decision-tree models. A parse is built up from a succession of parse states, each of which represents a partial parse tree. Transition between states is accomplished by one of the following steps: (1) assigning syntax to a word; (2) assigning semantics to a word; (3) deciding whether the current parse tree node is the last node of a constituent; (4) assigning a (rule) label to an internal node of the parse tree. Note that the first two steps together determine the tag for a word, and"
