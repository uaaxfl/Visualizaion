2006.amta-papers.8,J93-2003,0,0.017409,"Missing"
2006.amta-papers.8,N03-1017,0,0.0147799,"bei r4 ⇓ [jingfang]2 [jibi]1 Figure 2: A synatx-directed translation process for Example (1). VP VBD (r3 ) was VP-C x1 :VBN IN PP → bei x2 x1 x2 :NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Kni"
2006.amta-papers.8,koen-2004-pharaoh,0,0.0531818,"end up with 140 English sentences to translate in both dev and test sets. Note that this arrangement makes sure the test set is blind. 6.2 Systems We implemented our system as follows: for each input sentence, we first run Algorithm 1, which returns the 1-best translation and also builds the derivation forest of all translations for this sentence. Then we extract the top-k non-duplicate translated strings from this forest using the algorithm in Section 5.2 and rescore them with the trigram model and the length penalty. We compared our system with a state-of-theart phrase-based system Pharaoh (Koehn, 2004) on the evaluation data. Since the target language is Chinese, we will report character-based BLEU scores instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based). Feature weights of both systems are tuned for BLEU-4 (up to 4-grams) on the dev set. For Pharaoh, we use the standard minimum error-rate training (Och, 2003) (David Chiang’s implementation); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the language-model weight ax"
2006.amta-papers.8,N03-1019,0,0.00473659,"ranslations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts t"
2006.amta-papers.8,A00-2018,0,0.040742,"-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model also enjoys a speed-up by this decoupling, with each of the two stages having a smaller search space. In fact, the recursive transfer step can be done by a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Our model, being linguistically motivated, is also more expressive than the formally syntaxbased models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3 . In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be I was [aslee"
2006.amta-papers.8,P05-1033,0,0.810076,"2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model also enjoys a speed-"
2006.amta-papers.8,C04-1090,0,0.561123,"a corresponding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within indi"
2006.amta-papers.8,P05-1066,0,0.184638,"Missing"
2006.amta-papers.8,N06-1045,1,0.682453,"hs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, . . . , k th derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations, while the rescoring approach prefers diversity within the k-best list. To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree automata by May and Knight (2006). These methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form. However, this transformation often leads to a blow-up in forest size, which is exponential in the original size in the worst-case. So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings: • keep a hash-table of unique strings at each vertex in the hypergraph • when asking for the next-best derivation of a vertex, keep asking until we get a new string,"
2006.amta-papers.8,P02-1038,0,0.170149,"τ ∗ ) = {d |E(d) = τ ∗ } that translates English tree τ into some Chinese string and apply the Viterbi approximation again to search for the best derivation d∗ : c∗ = C(d∗ ) = C(argmax Pr(d)) (6) d∈D(τ ∗ ) Assuming different rules in a derivation are applied independently, we approximate Pr(d) as Y Pr(r) (7) Pr(d) = r∈d where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol ρ(t(r)): Pr(r) = Pr(t(r), s(r) |ρ(t(r))) c(r) = P 0 r 0 :ρ(t(r 0 ))=ρ(t(r)) c(r ) (8) where c(r) is the count (or frequency) of rule r in the training data. 4.2 Log-Linear Model Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: c∗ = argmax Pr(c |e)α · Pr(c)β · e−λ|c| (9) c where Pr(c) is the language model and e−λ|c |is the length penalty term based on |c|, the length of the translation. Parameters α, β, and λ are the weights of relevant features. Note that positive λ prefers longer translations, thus we call λ the length-bonus parameter. We use a standard trigram model for Pr(c). 5 Search Algorithms We first present a linear-time algorithm for searching the best derivation under the direct model, and then extend"
2006.amta-papers.8,P05-1067,0,0.754859,"nding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules."
2006.amta-papers.8,J04-4002,0,0.149326,"2 [jibi]1 Figure 2: A synatx-directed translation process for Example (1). VP VBD (r3 ) was VP-C x1 :VBN IN PP → bei x2 x1 x2 :NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r4 and r5 which perform phrasal translations for the two remaining sub-trees in (d), respectively, and get the completed Chinese string in (e). 67 ◦ 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chian"
2006.amta-papers.8,P03-2041,0,0.850735,"rget-language string with the highest probability. However, the structural divergence across languages often results in non-isomorphic parse-trees that is beyond the power of SCFGs. For example, the S(VO) structure in English is translated into a VSO wordorder in Arabic, an instance of complex reordering not captured by any SCFG (Fig. 1). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-"
2006.amta-papers.8,P03-1021,0,0.0783409,"ted strings from this forest using the algorithm in Section 5.2 and rescore them with the trigram model and the length penalty. We compared our system with a state-of-theart phrase-based system Pharaoh (Koehn, 2004) on the evaluation data. Since the target language is Chinese, we will report character-based BLEU scores instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based). Feature weights of both systems are tuned for BLEU-4 (up to 4-grams) on the dev set. For Pharaoh, we use the standard minimum error-rate training (Och, 2003) (David Chiang’s implementation); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the language-model weight axis. For a given language-model weight β, we use binary search to find the best length bonus parameter λ that leads to a length-ratio closest to 1 against the reference. dev set BLEU-4 25.96 ±2.8 22.10 ±2.6 test set (140 sentences) BLEU-4 BLEU-8 23.54 ±1.9 6.739 ±1.2 24.53 ±2.2 7.309 ±1.9 26.01 ±2.7 26.95 ±2.8 25.74 ±2.3 26.69 ±2.4 8.489 ±2.1 9.323 ±2.2 6.3 Results and Statistical Significance"
2006.amta-papers.8,W02-1039,0,0.210419,"nchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treeb"
2006.amta-papers.8,N04-1035,1,0.81719,"reeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treebank parser but focus o"
2006.amta-papers.8,N04-1014,1,0.849397,"-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since syntax-directed translation models sep66 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 66-73, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas arate the source-language analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFG-based Treebank parser but focus on the extended domain in the recursive converter. Following Galley et al."
2006.amta-papers.8,W05-1506,1,0.28971,"caching the best solution for future use 16: return cache[η] . returns the best string with its prob. However, integrating the n-gram model Pr(C(d)) with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty. Like other instances of dynamic programming, Algorithm 1 can be viewed as a hypergraph search problem. To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, . . . , k th derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations, while the rescoring approach prefers diversity within the k-best list. To alleviate this problem, determinization techniques have been propos"
2006.amta-papers.8,W06-1608,0,0.0648092,"Missing"
2006.amta-papers.8,P05-1034,0,0.570793,"as [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). Recent works on dependency-based MT (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) are closest to this work in the sense that their translations are also based on source-language parse trees. The difference is that they use dependency trees instead of constituent trees. Although they share with this work the basic motivations and similar speed-up, it is difficult to specify reordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works). Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules. In addition, it is mo"
2006.amta-papers.8,C90-3045,0,0.426197,"vation (a sequence of translation steps) that converts the whole tree into some target-language string with the highest probability. However, the structural divergence across languages often results in non-isomorphic parse-trees that is beyond the power of SCFGs. For example, the S(VO) structure in English is translated into a VSO wordorder in Arabic, an instance of complex reordering not captured by any SCFG (Fig. 1). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous treeadjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the nonisomorphic tree pair in Fig. 1. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of one-level contextfree productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often"
2006.amta-papers.8,J97-3002,0,0.123979,"by a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Our model, being linguistically motivated, is also more expressive than the formally syntaxbased models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3 . In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common"
2006.amta-papers.8,P01-1067,1,0.929016,"n et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model and do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their methods are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level context-free rule, while our approach decouples the source-language analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our model al"
2006.amta-papers.8,J03-4003,0,\N,Missing
2006.amta-papers.8,J08-3004,1,\N,Missing
2006.amta-papers.8,W90-0102,0,\N,Missing
2017.lilt-15.1,W13-5413,1,0.660566,"rated material to a verb can license the presence of an argument or adjunct in the syntax. For example, the incorporated factors of butter, leg, and dance, respectively, are realized syntactically, in the sentences below. (53) a. Mary buttered her bread with margarine. b. John kicked the ball with his left leg. c. Mary and John danced a polka. This follows from the shadow argument principle in (44), where, in each case, the expressed argument is more informative than the lexically encoded factor.9 8 There could be a range of modifiers but adverbs lend themselves readily to this analysis. 9 In Batiukova and Pustejovsky (2013), informativeness is used as a criterion to Lexical Factorization and Syntactic Behavior / 19 Examining the PMI data in Table 2, we notice the same phenomenon: namely, the time factor encoded in the verb glance is actually acting to select specializations of temporal adverbials, rather than block them. This is the behavior we observed in (53). In other words, the verb glance can be viewed as having promoted the incorporated temporal factor as a shadow argument, as illustrated below. (54) glance =df zs ∶time moment y∶phys x∶human[glance(x, y, z)] On this analysis, the temporal adverbials presen"
2017.lilt-15.1,J90-1003,0,0.223568,"properties of how factors encode a word’s meaning and how this impacts the subsequent syntactic behavior of the word in a sentential context; that is, there is a correlation between the information encoded in a verb v, and the expression of factors for a particular situational frame in which it is used. It has long been recognized that correlations between lexical items and the properties of selection can be linked to patterns of linguistic behavior in data (Harris (1957), Firth (1961)), and this discussion relates directly to work done in corpus and distributional linguistics, starting with Church and Hanks (1990), Resnik (1993), Hindle and Rooth (1993), up to the present. Recall from previous discussion that, within a situational frame, a semantic factor, fi , can be either lexically encoded, syntactically realized, or not expressed at all. In fact, it might be expected that lexical and syntactic factorization is inversely correlated in an utterance: that is, if a verb already incorporates a semantic factor, fi , then it is less likely to be expressed syntactically. Likewise, if the verb does not incorporate Lexical Factorization and Syntactic Behavior / 15 such a factor, it might be more common to se"
2017.lilt-15.1,J93-1005,0,0.29397,"’s meaning and how this impacts the subsequent syntactic behavior of the word in a sentential context; that is, there is a correlation between the information encoded in a verb v, and the expression of factors for a particular situational frame in which it is used. It has long been recognized that correlations between lexical items and the properties of selection can be linked to patterns of linguistic behavior in data (Harris (1957), Firth (1961)), and this discussion relates directly to work done in corpus and distributional linguistics, starting with Church and Hanks (1990), Resnik (1993), Hindle and Rooth (1993), up to the present. Recall from previous discussion that, within a situational frame, a semantic factor, fi , can be either lexically encoded, syntactically realized, or not expressed at all. In fact, it might be expected that lexical and syntactic factorization is inversely correlated in an utterance: that is, if a verb already incorporates a semantic factor, fi , then it is less likely to be expressed syntactically. Likewise, if the verb does not incorporate Lexical Factorization and Syntactic Behavior / 15 such a factor, it might be more common to see it appear in the syntax. We refer to t"
2017.lilt-15.1,2017.lilt-15.2,1,0.873866,"paper, we examine the correlation between lexical semantics and the syntactic realization of the di↵erent components of a word’s meaning in natural language. More specifically, we will explore the effect that lexical factorization in verb semantics has on the suppression or expression of semantic features within the sentence. Factorization was a common analytic tool employed in early generative linguistic approaches to lexical decomposition, and continues to play a role in contemporary semantics, in various guises and modified forms. Building on the unpublished analysis of verbs of seeing in Joshi (1972), we argue here that the significance of lexical factorization is twofold: first, current models of verb meaning owe much of their insight to factor-based theories of meaning; secondly, the factorization properties of a lexical item appear to influence, both directly and indirectly, the possible syntactic expressibility of arguments and adjuncts in sentence composition. We argue that this information can be used to compute what we call the factor expression likelihood (FEL) associated with a verb in a sentence. This is the likelihood that the overt syntactic expression of a factor will cooccur"
2017.lilt-15.1,J01-3003,0,0.0219966,"tentionally accidentally PMI results The pointwise mutual information (PMI) compares the probability of a specific factor, fi , and the verb, v, showing up together (jointly), with the probabilities of seeing fi and v independently. As in Church and Hanks (1990), where it is argued “word association norms” can be measured in terms of PMI given enough linguistic data, the association between factor expression and lexical choice is also potentially captured with such a metric. While this is a relatively simple measure, and there have been many suggested improvements and alternatives to it (cf. (Merlo and Stevenson, 2001, Rumshisky, 2008, Pustejovsky et al., 2004), it will suffice to demonstrate our thesis for the remainder of this section. Let us return to the example of the verbs kill and murder first mentioned above. We measure their relative PMI scores for two specific lexicalizations of the intentionality factor, as calculated over a large corpus of English.7 If there is a real correlation between a verb and an adverb of intention, then the PMI score will be significantly larger than 0 (� 0). If, on the other hand, there is no relationship of interest between them, then PMI ≈ 0. What we find in Table 1 i"
2017.lilt-15.1,W04-1908,1,0.480776,"ntwise mutual information (PMI) compares the probability of a specific factor, fi , and the verb, v, showing up together (jointly), with the probabilities of seeing fi and v independently. As in Church and Hanks (1990), where it is argued “word association norms” can be measured in terms of PMI given enough linguistic data, the association between factor expression and lexical choice is also potentially captured with such a metric. While this is a relatively simple measure, and there have been many suggested improvements and alternatives to it (cf. (Merlo and Stevenson, 2001, Rumshisky, 2008, Pustejovsky et al., 2004), it will suffice to demonstrate our thesis for the remainder of this section. Let us return to the example of the verbs kill and murder first mentioned above. We measure their relative PMI scores for two specific lexicalizations of the intentionality factor, as calculated over a large corpus of English.7 If there is a real correlation between a verb and an adverb of intention, then the PMI score will be significantly larger than 0 (� 0). If, on the other hand, there is no relationship of interest between them, then PMI ≈ 0. What we find in Table 1 is that, as intuitively predicted, there appe"
2017.lilt-15.2,C69-0201,0,0.284893,"Missing"
A92-1030,W90-0203,0,0.0424511,"Missing"
A92-1030,C88-2121,1,0.931373,"Missing"
A92-1030,1991.iwpt-1.4,1,0.808348,"Missing"
A92-1030,W90-0102,1,0.901398,"Missing"
A92-1030,C90-3045,1,0.900667,"Missing"
A92-1030,C88-2147,0,0.0976216,"Missing"
A92-1030,C90-3001,1,\N,Missing
A92-1030,C92-3145,1,\N,Missing
C08-2022,prasad-etal-2008-penn,1,0.783332,"Missing"
C08-2022,W06-1317,0,0.358334,"d, the overall accuracy of identifying contingency and expansion relations is lower, 5 N-gram discourse relation models We have shown above that some relations, such as comparison, can be easily identified because they are often explicit and are expressed by an unambiguous connective. However, one must build a more subtle automatic classifier to find the implicit relations. We now look at the frequencies in which various relations are adjacent in the PDTB. Results from previous studies of discourse relations suggest that the context of a relation can be helpful in disambiguating the relation (Wellner et al., 2006). Here we identify specific dependencies that exist between sequences of relations. We computed χ2 statistics to test the independence of each pair of relations. The question is: do relations A and B occur adjacent to each other more than they would simply due to chance? The 89 First Relation E. Comparison E. Comparison E. Comparison I. Temporal I. Contingency I. Expansion E. Expansion I. Contingency Second Relation I. Contingency E. Comparison I. Expansion E. Temporal E. Contingency E. Expansion I. Expansion E. Comparison χ2 20.1 17.4 9.91 9.42 9.29 6.34 5.50 4.95 p-value .000007 .000030 .001"
C08-2022,N07-1054,0,\N,Missing
C08-2022,J93-2004,0,\N,Missing
C08-2022,J08-1001,0,\N,Missing
C08-2022,P02-1047,0,\N,Missing
C08-2022,N04-1020,0,\N,Missing
C10-2118,D09-1036,0,0.0226129,"Missing"
C10-2118,P02-1047,0,0.106414,"Missing"
C10-2118,J93-2004,0,0.03453,"tation of 624 tokens of AltLex in the PDTB. We turn to our analysis of these expressions in the next section. 3 What is found in AltLex? Several questions arise when considering the AltLex annotations. What kind of expressions are they? What can we learn from their syntax? Do they project discourse relations of a different sort than connectives? How can they be identified, both during manual annotation and automatically? To address these questions, we examined the AltLex annotation for annotated senses, and for common lexico-syntactic patterns extracted using alignment with the Penn Treebank (Marcus et al., 1993).4 3.1 Lexico-syntactic Characterization We found that we could partition AltLex annotation into three groups by (a) whether or not they belonged to one of the syntactic classes admitted as explicit connectives in the PDTB, and (b) whether the expression was frozen (ie, blocking free substitution, modification or deletion of any of its parts) or open-ended. The three groups are shown in Table 1 and discussed below. 4 The source texts of the PDTB come from the Penn Treebank (PTB) portion of the Wall Street Journal corpus. The PDTB corpus provides PTB tree alignments of all its text span annotat"
C10-2118,W09-3029,1,0.821637,"Missing"
C10-2118,P09-2004,0,0.00784831,"verbs, and prepositional phrases. Thus the literature presents lists of DRMs, which researchers try to make as complete as possible for their chosen language. In annotating lexicalized discourse relations of the Penn Discourse Treebank (Prasad et al., 2008), this same assumption drove the initial phase of annotation. A list of “explicit connectives” was collected from various sources and provided to annotators, who then searched for these expressions in the text and annotated them, along with their arguments and senses. The same assumption underlies methods for automatically identifying DRMs (Pitler and Nenkova, 2009). Since expressions functioning as DRMs can also have non-DRM functions, the task is framed as one of classifying given individual tokens as DRM or not DRM. In this paper, we argue that placing such syntactic and lexical restrictions on DRMs limits a proper understanding of discourse relations, which can be realized in other ways as well. For example, one should recognize that the instantiation (or exemplification) relation between the two sentences in Ex. (3) is explicitly signalled in the second sentence by the phrase Probably the most egregious example is, which is sufficient to express the"
C10-2118,P09-1077,0,0.0325156,"Missing"
C10-2118,W07-2314,0,0.011281,"ntra-clausal, he does not observe that verbalized discourse relations can hold across sentences as well, where a verb and one of its arguments function similarly to a discourse adverbial, and in the end, he does not provide a proposal for how to systematically identify these alternative realizations. Le Huong et al. (2003), in developing an algorithm for recognizing discourse relations, consider non-verbal realizations (called NP cues) in addition to verbal realizations (called VP cues). However, they provide only one example of such a cue (“the result”). Like Kibble (1999), Danlos (2006) and Power (2007) also focus only on identifying verbalizations of discourse relations, although they do consider cases where such relations hold across sentences. What has not been investigated in prior work is the basis for the alternation between connectives and AltLex’s, although there are several accounts of why a language may provide more than one connective that conveys the same relation. For example, the alternation in Dutch between dus (“so”), daardoor (“as a result”), and daarom (“that’s why”) is explained by Pander Maat and Sanders (2000) as having its basis in “subjectivity”. class expressions is p"
C10-2118,prasad-etal-2008-penn,1,0.419254,"tify explicit signals of discourse relations, exemplified in Ex. (1). To refer to all such signals, we use the term “discourse relation markers” (DRMs). Past research (e.g., (Halliday and Hasan, 1976; Martin, 1992; Knott, 1996), among others) has assumed that DRMs are frozen or fixed expressions from a few welldefined syntactic classes, such as conjunctions, adverbs, and prepositional phrases. Thus the literature presents lists of DRMs, which researchers try to make as complete as possible for their chosen language. In annotating lexicalized discourse relations of the Penn Discourse Treebank (Prasad et al., 2008), this same assumption drove the initial phase of annotation. A list of “explicit connectives” was collected from various sources and provided to annotators, who then searched for these expressions in the text and annotated them, along with their arguments and senses. The same assumption underlies methods for automatically identifying DRMs (Pitler and Nenkova, 2009). Since expressions functioning as DRMs can also have non-DRM functions, the task is framed as one of classifying given individual tokens as DRM or not DRM. In this paper, we argue that placing such syntactic and lexical restriction"
C10-2118,miltsakaki-etal-2004-penn,1,\N,Missing
C10-2118,D08-1021,0,\N,Missing
C16-2026,al-saif-markert-2010-leeds,0,0.0966919,"overview of the PDTB Framework and discusses the tool’s features, setup requirements and how it can also be used for adjudication. 1 Introduction In recent years, discourse relations have become a topic of some interest and there has in effect been a rise in the number of corpora annotated for discourse relations. Following the release of the Penn Discourse TreeBank (PDTB) in 2008 (Prasad et al., 2008), a number of comparable corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Hindi Discourse Relation Bank (Oza et al., 2009), the Leeds Arabic Discourse TreeBank (Al-Saif and Markert, 2010), the Biomedical Discourse Relation Bank (Prasad et al., 2011), the Chinese Discourse TreeBank (Zhou and Xue, 2012), the Turkish Discourse Bank (Zeyrek et al., 2013), the discourse layer of the Prague Dependency Treebank 3.0 (Bejˇcek et al, 2013) and the TED-Multilingual Discourse Bank (TED-MDB) (Zeyrek et al., 2016). Groups starting new discourse annotation projects have sought an openly available resource to support their work. To address this for annotation in the PDTB framework, we have packaged an updated version of our annotation tool - the PDTB Annotator - for use by the research commun"
C16-2026,C14-2008,0,0.0367071,"Missing"
C16-2026,J05-1004,0,0.0819207,"Missing"
C16-2026,W16-1704,1,0.881334,"72) This work has been supported by the National Science Foundation under grants RI 1422186 and RI 1421067. It is licensed under a Creative Commons Attribution 4.0 International Licence. License details: http://creativecommons.org/ licenses/by/4.0/ 121 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 121–125, Osaka, Japan, December 11-17 2016. All relations are taken to have two arguments - Arg1 (shown in italics) and Arg2 (in bold). As per the revised argument-naming conventions in recent ongoing work on PDTB enrichment (Webber et al., 2016), the Arg2 in syntactically coordinated relations follows (i.e. is to the right of) Arg1, while the Arg2 in syntactically subordinated relations is (syntactically) subordinate to Arg1, regardless of textual order. Discourse relations are not always realized as Explicit connectives. In such cases, a connective is left to be inferred by the annotator, who lexically encodes this inferred relation. This is shown in (2), where a Reason relation between the two adjacent sentences is annotated with because as the Implicit connective: (2) Also unlike Mr. Ruder, Mr. Breeden appears to be in a position"
C16-2026,P12-1008,0,0.0196254,"ication. 1 Introduction In recent years, discourse relations have become a topic of some interest and there has in effect been a rise in the number of corpora annotated for discourse relations. Following the release of the Penn Discourse TreeBank (PDTB) in 2008 (Prasad et al., 2008), a number of comparable corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Hindi Discourse Relation Bank (Oza et al., 2009), the Leeds Arabic Discourse TreeBank (Al-Saif and Markert, 2010), the Biomedical Discourse Relation Bank (Prasad et al., 2011), the Chinese Discourse TreeBank (Zhou and Xue, 2012), the Turkish Discourse Bank (Zeyrek et al., 2013), the discourse layer of the Prague Dependency Treebank 3.0 (Bejˇcek et al, 2013) and the TED-Multilingual Discourse Bank (TED-MDB) (Zeyrek et al., 2016). Groups starting new discourse annotation projects have sought an openly available resource to support their work. To address this for annotation in the PDTB framework, we have packaged an updated version of our annotation tool - the PDTB Annotator - for use by the research community. Some of the potential benefits of using the PDTB Annotator include the following: i) the tool is Java-based an"
C16-2026,prasad-etal-2008-penn,1,\N,Missing
C16-2026,W03-2120,0,\N,Missing
C69-4701,C67-1007,1,\N,Missing
C82-1066,P81-1016,0,0.0649695,", respond to a query) are not necessarily the best ones to use in Justifying a result. What one wants rather is the ability to use the system&apos;s reasoning to suggest and instantiate conceptually more accessible strategles fnr organlz~ng and presenting justifications. Both claims will be discussed in this section. &quot; 416 B. WEBBER ~nd A. JOSH1 Many researchers have already observed that explanations have a tree-llke structure. This observation reflects a view of each supported assertion as a non-terminal node in a tree, with the sub-tree under it corresponding to the reasons given in its support [2,11]. Since a statement acting as a &quot;reason&quot; may in turn be supported by other statements/reasons, explanations have a recursive structure. While the above is true, it masks what we see as a more significant recursive organization - one that reflects the inherently recursive strategies that people use in reasoning (i.e., in supporting or denying propositions). These strategies are recurslve because they contain subtasks that call in turn for other propositions to be supported or denied. One way to accomplish this is to chose and invoke another strategy. The kinds of strategies we have in mind are"
C82-1066,C82-1066,1,0.0528322,"and Information Science University of Pennsylvania Philadelphia PA 19104 I. Introduction In answering a factual database query, one often has the option of providing more than just the answer explicitly requested. As part of our research on Natural Language interactions with databases~ we have been looking at three ways in which the system could so &quot;take the initiative&quot; in constructing a response: (i) pointing out incorrect presuppositions reflected in the user&apos;s query [4,5]; (2) offering to &quot;monitor&quot; for the requested information or additional relevant information as the system learns of it [6,7]; and (3) providing grounds for the system&apos;s response i.e., &quot;justifying why&quot;. The following responses illustrate &quot;presupposition correctlon&quot;~ &quot;monitor offers&quot; and &quot;justification&quot;, respectively. This paper describes our research on producing justifications. (&quot;U&quot; refers to the user, &quot;S&quot; to the system.) U: SI: $2: $3: Is John taking four courses? No. John can&apos;t take any courses: he&apos;s not a student. NoD three. Shall I let you know if he registers for a fourth? No, three - CIS531, CIS679 and Linguistics 650. Database systems are growing more complex in both their domain models and reasoning capabil"
C86-1048,P85-1011,1,0.821545,"atural languages, were developed independently. TAG&apos;s deal with a set of elementary trees which are composed by means of an operation called a d j o i n i n g . HG&apos;s are like Context-free Grammars, except for the fact t h a t besides concatenation of strings, s t r i n g w r a p p i n g operations are permitted. TAG&apos;s were first introduced in 1975 by Joshi, Levy and Wakahashi [3]. Joshi [2] investigated some formal and linguistic properties of TAG&apos;s with local constraints. The formulation of local constraints was then modified and formal properties were investigated by Vijay-Shanker and Joshi [9]. The linguistic properties were studied in detail by Kroeh and Joshi [5]. HG&apos;s were first introduced by Pollard [6] in 1983 and their formal properties were investigated by Roach [7]. It was observed t h a t the two systems seemed to possess similar generative power and since they also appear to have the same closure properties [7,9] as well as similar parsing algorithn~ [6,9] a significant amount of indirect evidence existed to suggest t h a t they were formally equivalent. In the present paper, we will attempt to provide a characterization of the formal relationship between HG&apos;s and TAG&apos;s."
C86-1048,P86-1011,1,0.376533,"Missing"
C88-2121,C88-1002,1,0.829428,"Missing"
C88-2121,P84-1058,0,0.272853,"5-K0018, NSF grants MCS-82-191169 and DGR-84-10413. The second author is also partially supported by J.W. Zellldja grant. The authors would llke to thank Mitch Marcus for his helpful conunents about this work. Thanks are also due to Ellen Hays. **Visiting from University of Paris VII. 57,~ 1 'Lexicalization' of g r a m m a r formalisms Most of the current linguistics theories tend to give lexical accounts of several phenomena that used to be considered purely syntactic. The information put in the lexicon is therefore increased and complexified (e.g. lexical rules in LFG, used also by HPSG, or Gross 1984's lexicongrammar). But the question of what it means to 'lexicalize' a grammar is seldom addressed. The possible consequences of this question for parsing are not fully investigated. We present how to 'lexicalize' grammars such as CFGs in a radical way, while possibly keeping the rules in their full generality. If one assumes that the input sentence is finite and that it cannot be syntactically infinitely ambiguous, the 'lexicalization' simplifies the task of a parser. We say that a grammar formalism is 'lexicalized' if it consists of: • a finite set of structures to be associated with lexica"
C88-2121,P88-1032,1,0.908713,"s Earley-type a sentence to be parsed. It places no restrictions on the grammar. The algorithm is a bottom-up parser that uses top-down filtering. It is able to parse constraints on adjunction, substitution and feature structures for TAGs as defined by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). It is able to parse directly CFGs and TAGs. Thus it embeds the essential aspects of PATR-II as defined by Shieber (1984 and 1986). Its correctness was proven in Sehabes and Joshi (1988b). The concepts of dotted rule and states have been extended to TAG trees. The algorithm as described by Schabes and Joshi (1988a) manipulates states of the form: s = [a, dot, side, pos, l, fl, fi, star, t[, b[, snbst?] where a is a tree, dot is the address of the dot in the tree, side is the side of the symbol the dot is on (left or right), pos is the position of the dot (above or below), star is an address in a and l, f~, fr, star, t~, b~ are indices of positions in the input string. The variable subst? is a boolean that indicates whether the tree has been predicted for substitution. The algorithm uses nine processes: • The S c a n n e r allows lexical items to be recognized. • M o v e d o t d o w n and M o v e d o t"
C88-2121,P84-1075,0,0.00573075,"----+ S} Also, one can state a necessary condition on the correctness of a sentence similar to the category count theorem of van Benthem (1985 and 1986). 5 Extending the parser for TAGs Earley-type a sentence to be parsed. It places no restrictions on the grammar. The algorithm is a bottom-up parser that uses top-down filtering. It is able to parse constraints on adjunction, substitution and feature structures for TAGs as defined by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). It is able to parse directly CFGs and TAGs. Thus it embeds the essential aspects of PATR-II as defined by Shieber (1984 and 1986). Its correctness was proven in Sehabes and Joshi (1988b). The concepts of dotted rule and states have been extended to TAG trees. The algorithm as described by Schabes and Joshi (1988a) manipulates states of the form: s = [a, dot, side, pos, l, fl, fi, star, t[, b[, snbst?] where a is a tree, dot is the address of the dot in the tree, side is the side of the symbol the dot is on (left or right), pos is the position of the dot (above or below), star is an address in a and l, f~, fr, star, t~, b~ are indices of positions in the input string. The variable subst? is a boolean that indic"
C88-2121,P85-1018,0,0.0162873,"lied to execute the second step, since the number of structures produced js finite and since each of them corresponds to a token in the input string, the search space is finite and termination is guaranteed. In principle, one can proceed inside out, left to right or in any other way. Of course, standard parsing algorithm can be used too. In particular, we can use the top-down parsing strategy without encountering the usual problems due to recursion. Problems in the prediction step of the Earley parser used for unification-based formalisms no longer exist. The use of restrictors as proposed by Shieber (1985) is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem. By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous. Since the trees are produced by the input string, the parser can use information that might be non-local to guide the search. For example, consider the language generated by the following CFG (example due to Mitch Marcus):"
C88-2121,C88-2147,0,\N,Missing
C88-2121,C69-4701,1,\N,Missing
C88-2121,P88-1033,0,\N,Missing
C90-3001,C88-1007,0,0.0563856,"Missing"
C90-3001,E89-1037,0,0.0632686,"Missing"
C90-3001,C88-2121,1,0.910376,"Missing"
C90-3001,C88-1002,1,0.877286,"Missing"
C90-3001,P90-1037,1,0.779933,"Missing"
C90-3001,C90-3045,1,0.538116,"Missing"
C90-3001,E89-1001,1,\N,Missing
C90-3001,C86-1071,0,\N,Missing
C90-3001,W90-0102,1,\N,Missing
C94-1024,C92-4171,0,\N,Missing
C94-1024,H93-1048,0,\N,Missing
C94-1024,A88-1019,0,\N,Missing
C94-1024,C88-2121,1,\N,Missing
C96-2103,P88-1032,1,\N,Missing
C96-2103,H91-1035,1,\N,Missing
D07-1039,W99-0901,0,0.0786912,"references of verbs. We will use the preferences to find atypical verb-object combinations as we anticipate that such combinations are more likely to be non-compositional. 1 We use object to refer to direct objects. 369 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the e"
D07-1039,W03-1812,0,0.847965,"selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-object 1 combinations on"
D07-1039,W03-1809,0,0.49145,"use these ‘typebased’ selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-obj"
D07-1039,J98-2002,0,0.086539,"e of selectional preferences of verbs. We will use the preferences to find atypical verb-object combinations as we anticipate that such combinations are more likely to be non-compositional. 1 We use object to refer to direct objects. 369 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet a"
D07-1039,W04-3224,0,0.0402697,"Missing"
D07-1039,P98-2127,0,0.342873,"data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following (Lin, 1998). The first method is due to Li and Abe (1998). The classes over which the probability distribution is calculated are selected according to the minimum description length principle (MDL) which uses the argument head tokens for finding the best classes for representation. This method has previously been tried for modelling compositionality of verb-particle constructions (Bannard, 2002). The other two methods (we refer to them as ‘typebased’) also calculate a probability distribution using argument head tokens but they select the classes over which the distribution is calculated using the number"
D07-1039,briscoe-carroll-2002-robust,0,0.0186821,"predicate in question, for example eat hat. In contrast to Bannard, our experiments are with verb-object combinations rather than verb particle constructions. We compare Li and Abe models with WordNet models which use the number of argument types to obtain the classes for representation of the selectional preferences. In addition to experiments with these WordNet models, we propose models using entries in distributional thesauruses for representing preferences. 3 Three Methods for Acquiring Selectional Preferences All models were acquired from verb-object data extracted using the RASP parser (Briscoe and Carroll, 2002) from the 90 million words of written English from the BNC (Leech, 1992). We extracted verb and common noun tuples where the noun is the argument head of the object relation. The parser was also used to extract the grammatical relation data used for acquisition of the thesaurus described below in section 3.3. 3.1 TCMs This approach is a reimplementation of Li and Abe (1998). Each selectional preference model (referred to as a tree cut model, or TCM) comprises a set of disjunctive noun classes selected from all the possibilities in the WordNet hyponym hierarchy 3 using MDL (Rissanen, 1978). The"
D07-1039,J90-1003,0,0.0331396,"nd Evert, 2001). Fazly and Stevenson (2006) use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be a idiom. Although they are not specifically detecting compositionality, there is a strong correlation between syntactic rigidity and semantic idiosyncrasy. Venkatapathy and Joshi (2005) combine different statistical and distributional methods using support vector machines (SVMs) for identifying noncompositional verb-object combinations. They explored seven features as measures of compositionality: 1. frequency 2. pointwise mutual information (Church and Hanks, 1990), 3. least mutual information difference with similar collocations, based on (Lin, 1999) and using Lin’s thesaurus (Lin, 1998) for obtaining the similar collocations. 4. The distributed frequency of an object, which takes an average of the frequency of occurrence with an object over all verbs occurring with the object above a threshold. 5. distributed frequency of an object, using the verb, which considers the similarity between the target verb and the verbs occurring with the target object above the specified threshold. 6. a latent semantic approach (LSA) based on (Sch¨utze, 1998; Baldwin et"
D07-1039,J02-2003,0,0.318475,"will use the preferences to find atypical verb-object combinations as we anticipate that such combinations are more likely to be non-compositional. 1 We use object to refer to direct objects. 369 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus o"
D07-1039,E06-1043,0,0.136054,"class words output from his parser and filtered these by the loglikelihood statistic. Lin proposed that if there is a phrase obtained by substitution of either the head or modifier in the phrase with a ‘nearest neighbour’ from the thesaurus then the mutual information of this and the original phrase must be significantly different for the original phrase to be considered noncompositional. He evaluated the output manually. As well as distributional similarity, researchers have used a variety of statistics as indicators of non-compositionality (Blaheta and Johnson, 2001; Krenn and Evert, 2001). Fazly and Stevenson (2006) use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be a idiom. Although they are not specifically detecting compositionality, there is a strong correlation between syntactic rigidity and semantic idiosyncrasy. Venkatapathy and Joshi (2005) combine different statistical and distributional methods using support vector machines (SVMs) for identifying noncompositional verb-object combinations. They explored seven features as measures of compositionality: 1. frequency 2. pointwise mutual information (Church and Hanks, 1990), 3. least mutual in"
D07-1039,C94-2119,0,0.0403517,"369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following (Lin, 1998). The first method is due to Li and Abe (1998). The classes over which the probability distribution is calculated are selected according to the minimum description length principle (MDL) which uses the argument head tokens for finding the best classes for representation."
D07-1039,J03-3005,0,0.209835,"Missing"
D07-1039,P99-1041,0,0.868583,"ighly significant correlation between measures which use these ‘typebased’ selectional preferences and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investig"
D07-1039,W03-1810,1,0.953292,"s and compositionality judgements from a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-object 1 combinations on a continuum following p"
D07-1039,A00-2034,1,0.825144,"e for a noun may be indicative of peculiar semantics, this may not always be the case, for example chew the fat. Certainly it would be worth combining the preferences with other measures, such as syntactic fixedness (Fazly and Stevenson, 2006). We also believe it is worth targeting features to specific types of constructions, for example light verb constructions undoubtedly warrant special treatment (Stevenson et al., 2003) The selectional preference models we have proposed here might also be applied to other tasks. We hope to use these models in tasks such as diathesis alternation detection (McCarthy, 2000; Tsang and Stevenson, 2004) and contrast with WordNet models previously used for this purpose. 6 Acknowledgements We have demonstrated that the selectional preferences of a verbal predicate can be used to indicate if a specific combination with an object is noncompositional. We have shown that selectional preference models which represent prototypical arguments and focus on argument types (rather than tokens) do well at the task. Models produced from distributional thesauruses are the most promising which is encouraging as the technique could be applied to a language without a man-made thesau"
D07-1039,P93-1024,0,0.381518,"Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 369–379, Prague, June 2007. 2007 Association for Computational Linguistics Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002). There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together. The cluster output can then be used as classes for selectional preferences (Pereira et al., 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated. Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following (Lin, 1998). The first method is due to Li and Abe (1998). The classes over which the probability distribution is calculated are selected according to the"
D07-1039,W01-0513,0,0.0977826,"Missing"
D07-1039,J98-1004,0,0.150997,"Missing"
D07-1039,W04-2605,0,0.0165565,"be indicative of peculiar semantics, this may not always be the case, for example chew the fat. Certainly it would be worth combining the preferences with other measures, such as syntactic fixedness (Fazly and Stevenson, 2006). We also believe it is worth targeting features to specific types of constructions, for example light verb constructions undoubtedly warrant special treatment (Stevenson et al., 2003) The selectional preference models we have proposed here might also be applied to other tasks. We hope to use these models in tasks such as diathesis alternation detection (McCarthy, 2000; Tsang and Stevenson, 2004) and contrast with WordNet models previously used for this purpose. 6 Acknowledgements We have demonstrated that the selectional preferences of a verbal predicate can be used to indicate if a specific combination with an object is noncompositional. We have shown that selectional preference models which represent prototypical arguments and focus on argument types (rather than tokens) do well at the task. Models produced from distributional thesauruses are the most promising which is encouraging as the technique could be applied to a language without a man-made thesaurus. We find that the probab"
D07-1039,H05-1113,1,0.900041,"a data set used in previous research. The type-based models perform better than the models which use tokens for selecting the classes. Furthermore, the models which use the automatically acquired thesaurus entries produced the best results. The correlation for the thesaurus models is stronger than any of the individ1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al., 2003; Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005; Venkatapathy and Joshi, 2005). Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such “words with spaces” (Sag et al., 2002). For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words. In this work we are investigate methods of determining semantic compositionality of verb-object 1 combinations on a continuum following previous research in this direction (McCarthy e"
D07-1039,W04-0401,0,\N,Missing
D07-1039,C98-2122,0,\N,Missing
D08-1052,W01-1804,0,0.0250561,"w processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper is that selection of path and decisions about action are trained simultaneously with discriminative learning. In this way, we can employ context information more effectively. 7 Conclusion In this paper, we introduced bidirectional incremental parsing, a new architecture of parsing. We proposed a novel algorithm for graph-based incremental construction, and applied this algorithm to LTAG dependency parsing, revealing deep relations, which are unavailable in other approaches and difficult to learn. We evaluated the parser on an LTAG Treebank. Exp"
D08-1052,P04-1015,0,0.538724,"mming, one can only use a fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). However, the need for tractability does not allow much internal information to be used to represent a hypothesis. The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing. Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem. In these incremental parsers, tree structures are used to represent the left context. In this way, one can access the whole tree to collect rich context information at the expense of being limited to beam search, which only maintains k-best results at each 495 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504, c Honolulu, October 2008. 2008 Association for Computational Linguistics step. Compared to chart parsing, incremental parsing searches for the analyses for only 2n − 1 c"
D08-1052,W02-1001,0,0.925661,"decide the order of construction as well as the type of operation r. For example, in the very first step of dependency parsing, we need to decide which two words are to be combined as well as the dependency label to be used. This problem is solved statistically, based on the features defined on the substructures involved in the operation and their context. Suppose we are given the weights of these features, we will show in the next section how these parameters guide us to build a set of hypothesized hidden structures with beam search. In Section 3, we will present a Perceptron like algorithm (Collins, 2002; Daum´e III and Marcu, 2005) to obtain the parameters. Now we introduce the data structure to be used in our algorithms. A fragment is a connected sub-graph of G(V, E). Each fragment x is associated with a set of hypothesized hidden structures, or fragment hypotheses for short: Y x = {y1x , ..., ykx }. Each y x is a possible fragment hypothesis of x. It is easy to see that an operation to combine two fragments may depend on the fragments in the context, i.e. fragments directly connected to one of the operands. So we introduce the dependency relation over fragments. Suppose there is a dependen"
D08-1052,J98-4004,0,0.0499791,"wi wi+1 ...wj , a substring of the sentence. As far as CFG parsing is concerned, a chart parser computes the possible structures over all possible cells [i, j], where 1 ≤ i ≤ j ≤ n. The order of computing on these n(n + 1)/2 cells is based on some partial order , such that [p1 , p2 ]  [q1 , q2 ] if q1 ≤ p1 ≤ p2 ≤ q2 . In order to employ dynamic programming, one can only use a fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). However, the need for tractability does not allow much internal information to be used to represent a hypothesis. The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing. Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem. In these incremental parsers, tree structures are used to represent the left context. In this way, one can access the whole tree to collect rich context information at t"
D08-1052,W89-0206,0,0.0812184,"nd Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper is that selection of path and decisions about action are trained simultaneously with discriminative learning. In this way, we can employ context information more effectively. 7 Conclusion In this paper, we introduced bidirectional incremental parsing, a new architecture of parsing. We proposed a novel algorithm for graph-based incremental construction, and applied this algorithm to LTAG dependency parsing, revealing deep relations, which are unavailable in other approaches and difficult to learn. We evaluated the parser on an LTAG Treebank. Experimental re"
D08-1052,P05-1012,0,0.111307,"e cut T , hypotheses H T and candidate queue Q by calling initH and initQ as in Algorithm 1. Then we use the gold standard Hr to guide the search. We select candidate (x′ , y ′ ) which has the highest operation score in Q. If y ′ is compatible with Hr , we update H and Q by calling updateH and 499 We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons. First, the LTAG architecture itself reveals deeper dependency. Furthermore, the PTB was reconciled with the Propbank in the LTAG Treebank extraction (Shen et al., 2008). We are especially interested in the two types of structures in the LTAG Treebank, predicate adjunction and predicate coordination. They are used to encode dependency relations which are unav"
D08-1052,H05-1011,0,0.0139271,"003). McAllester et al. (2004) introduced Case-Factor Diagram (CFD) to transform a graph based construction problem to a labeling problem. However, adjunction, prediction coordination, and long distance dependencies in LTAG dependency parsing make it difficult to implement. Our approach provides a novel alternative to CFD. Our learning algorithm stems from Perceptron training in (Collins, 2002). Variants of this method have been successfully used in many NLP tasks, like shallow processing (Daum´e III and Marcu, 2005), parsing (Collins and Roark, 2004; Shen and Joshi, 2005) and word alignment (Moore, 2005). Theoretical justification for those algorithms can be applied to our training algorithm in a similar way. In our algorithm, dependency is defined on complicated hidden structures instead of on a graph. Thus long distance dependency in a graph becomes local in hidden structures, which is desirable from linguistic considerations. The search strategy of our bidirectional dependency parser is similar to that of the bidirectional CFG parser in (Satta and Stock, 1994; Ageno and Rodrguez, 2001; Kay, 1989). A unique contribution of this paper is that selection of path and decisions about action are"
D08-1052,J05-1004,0,0.0140307,"raining data. For each given training sample (Gr , Hr ), where Hr is the gold standard hidden structure of graph Gr , we first initiate cut T , hypotheses H T and candidate queue Q by calling initH and initQ as in Algorithm 1. Then we use the gold standard Hr to guide the search. We select candidate (x′ , y ′ ) which has the highest operation score in Q. If y ′ is compatible with Hr , we update H and Q by calling updateH and 499 We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons. First, the LTAG architecture itself reveals deeper dependency. Furthermore, the PTB was reconciled with the Propbank in the LTAG Treebank extraction (Shen et al., 2008). We are especially interested in the two types of struc"
D08-1052,H05-1102,1,0.961355,"fragment of a hypothesis to represent the whole hypothesis, which is assumed to satisfy conditional independence assumption. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). However, the need for tractability does not allow much internal information to be used to represent a hypothesis. The designs of hypotheses in (Collins, 1999; Charniak, 2000) show a delicate balance between expressiveness and tractability, which play an important role in natural language parsing. Some recent work on incremental parsing (Collins and Roark, 2004; Shen and Joshi, 2005) showed another way to handle this problem. In these incremental parsers, tree structures are used to represent the left context. In this way, one can access the whole tree to collect rich context information at the expense of being limited to beam search, which only maintains k-best results at each 495 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504, c Honolulu, October 2008. 2008 Association for Computational Linguistics step. Compared to chart parsing, incremental parsing searches for the analyses for only 2n − 1 cells, [1, 1], [2, 2], ["
D08-1052,P07-1096,1,0.956545,"ich can be applied to many structure learning problems in NLP. We apply this algorithm to LTAG dependency parsing, and achieve significant improvement on accuracy over the previous best result on the same data set. 1.1 1 Introduction The phrase “Bidirectional Incremental” may appear self-contradictory at first sight, since incremental parsing usually means left-to-right parsing in the context of conventional parsing. In this paper, we will extend the meaning of incremental parsing. The idea of bidirectional parsing is related to the bidirectional sequential classification method described in (Shen et al., 2007). In that paper, a tagger assigns labels to words of highest confidence first, and then these labels in turn serve as the context of later labelling operations. The bidirectional tagger obtained the best results in literature on POS tagging on the standard PTB dataset. We extend this method from labelling to structure learning, The search space of structure learning is much larger, so that it is appropriate to exploit confidence scores in search. In this paper, we are interested in LTAG dependency parsing because TAG parsing is a well known problem of high computational complexity in regular p"
D08-1052,W03-3023,0,0.031843,"graph Gr , we first initiate cut T , hypotheses H T and candidate queue Q by calling initH and initQ as in Algorithm 1. Then we use the gold standard Hr to guide the search. We select candidate (x′ , y ′ ) which has the highest operation score in Q. If y ′ is compatible with Hr , we update H and Q by calling updateH and 499 We apply the new algorithm to LTAG dependency parsing on an LTAG Treebank (Shen et al., 2008) extracted from Penn Treebank (Marcus et al., 1994) and Proposition Bank (Palmer et al., 2005). Penn Treebank was previously used to train and evaluate various dependency parsers (Yamada and Matsumoto, 2003; McDonald et al., 2005). In these works, Magerman’s rules are used to pick the head at each level according to the syntactic labels in a local context. The dependency relation encoded in the LTAG Treebank reveals deeper information for the following two reasons. First, the LTAG architecture itself reveals deeper dependency. Furthermore, the PTB was reconciled with the Propbank in the LTAG Treebank extraction (Shen et al., 2008). We are especially interested in the two types of structures in the LTAG Treebank, predicate adjunction and predicate coordination. They are used to encode dependency"
D08-1052,A00-2018,0,\N,Missing
D08-1052,J93-2004,0,\N,Missing
D08-1052,J03-4003,0,\N,Missing
D11-1111,C04-1180,0,0.0146915,"subjects’ interaction with a database of flight information, using spoken natural language. The utterances have be transcribed, and the average sentence length is 10 words (Berant et al., 2007). Algorithms, which achieve good accuracy, have been developed to compute the logical translation for these queries (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009). The annotated sentences in the FDA CFR Section 610.40 are longer (about 30 words on average), and contain modalities which are not present in these corpora. At the other end of the spectrum, Bos et al. (Bos et al., 2004) have developed a broad-coverage parser to translate sentences to a logic based on discourse representation theory. Here, there is no direct method to evaluate the correctness of the translation. However, indirect evaluations are possible, for example, by studying improvement in textual entailment tasks. To summarize, there are techniques that either produce an accurate translation for sentences in a limited domain, or produce some translation for sentences in a broader range of texts. ASTs offer a middle ground in two ways. First, we focus on regulatory texts which are less restricted than th"
D11-1111,H94-1010,0,0.0614542,"s. The algorithm that handles embedded operators (ALL+) usually raises them from a single operator node (as in Figure 3) to a multi-operator node (as in Figure 5). If it makes an incorrect decision to raise an operator it takes a precision hit, at the multi-operator node (because 1210 ASTs can be seen as a middle ground between two lines of research in translating sentences to logic. At one end of the spectrum, we have methods that achieve good accuracy on restricted texts. The two main corpora that have been considered are the G EO Q UERY corpus (Thompson et al., 1997) and the ATIS-3 corpus (Dahl et al., 1994). The G EO Q UERY corpus consists of queries to a geographical database. The queries were collected from students participating in a study and the average sentence length is 8 words. The ATIS corpus is collected from subjects’ interaction with a database of flight information, using spoken natural language. The utterances have be transcribed, and the average sentence length is 10 words (Berant et al., 2007). Algorithms, which achieve good accuracy, have been developed to compute the logical translation for these queries (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Col"
D11-1111,D09-1152,0,0.158936,"Introduction May (1985) argued for a level of logical form as a prelude to translating sentences to logic. Just as a parse tree determines the constituent structure of a sentence, a logical form of a sentence represents one way of resolving scope ambiguities. The level of logical form is an appealing layer of modularity; it allows us to take a step beyond parsing in studying scope phenomenon, and yet, avoid the open problem of fully translating sentences to logic. Data-driven analyses of scope have been of interest in psycholinguistics (Kurtzman and MacDonald, 1993) and more recently in NLP (Srinivasan and Yates, 2009). The focus has typically been ∗ This research was supported in part by ONR MURI N00014-07-1-0907, NSF CNS-1035715, NSF IIS 07-05671, and SRI International. on predicting the preferred scopal ordering of sentences with two quantifying determiners, for example, in the sentence “every kid climbed a tree”. In the related problem of translating database queries to logic, Zettlemoyer and Collins (2009) and Wong and Mooney (2007) consider the scope of adjectives in addition to determiners, for example the scope of “cheapest” in the noun phrase “the cheapest flights from Boston to New York”. To our k"
D11-1111,P07-1121,0,0.228196,"fully translating sentences to logic. Data-driven analyses of scope have been of interest in psycholinguistics (Kurtzman and MacDonald, 1993) and more recently in NLP (Srinivasan and Yates, 2009). The focus has typically been ∗ This research was supported in part by ONR MURI N00014-07-1-0907, NSF CNS-1035715, NSF IIS 07-05671, and SRI International. on predicting the preferred scopal ordering of sentences with two quantifying determiners, for example, in the sentence “every kid climbed a tree”. In the related problem of translating database queries to logic, Zettlemoyer and Collins (2009) and Wong and Mooney (2007) consider the scope of adjectives in addition to determiners, for example the scope of “cheapest” in the noun phrase “the cheapest flights from Boston to New York”. To our knowledge, empirical studies of scope have been restricted to phenomenon between and within noun phrases. In this paper, we describe experiments on a novel annotation of scope phenomenon in regulatory texts – Section 610 of the Food and Drug Administration’s Code of Federal Regulations1 (FDA CFR). Determiners, modals, negation, and verb phrase modifiers are the main scope-taking operators. We have annotated 195 sentences wit"
D11-1111,P09-1110,0,0.0737809,"and yet, avoid the open problem of fully translating sentences to logic. Data-driven analyses of scope have been of interest in psycholinguistics (Kurtzman and MacDonald, 1993) and more recently in NLP (Srinivasan and Yates, 2009). The focus has typically been ∗ This research was supported in part by ONR MURI N00014-07-1-0907, NSF CNS-1035715, NSF IIS 07-05671, and SRI International. on predicting the preferred scopal ordering of sentences with two quantifying determiners, for example, in the sentence “every kid climbed a tree”. In the related problem of translating database queries to logic, Zettlemoyer and Collins (2009) and Wong and Mooney (2007) consider the scope of adjectives in addition to determiners, for example the scope of “cheapest” in the noun phrase “the cheapest flights from Boston to New York”. To our knowledge, empirical studies of scope have been restricted to phenomenon between and within noun phrases. In this paper, we describe experiments on a novel annotation of scope phenomenon in regulatory texts – Section 610 of the Food and Drug Administration’s Code of Federal Regulations1 (FDA CFR). Determiners, modals, negation, and verb phrase modifiers are the main scope-taking operators. We have"
E91-1005,W90-0214,1,0.751211,"Missing"
E91-1005,C88-2121,1,0.794063,"Missing"
H05-1102,A00-2018,0,\N,Missing
H05-1102,J95-4002,0,\N,Missing
H05-1102,J93-2004,0,\N,Missing
H05-1102,W03-3023,0,\N,Missing
H05-1102,H86-1020,1,\N,Missing
H05-1102,C94-1024,1,\N,Missing
H05-1102,W02-1001,0,\N,Missing
H05-1102,E91-1006,0,\N,Missing
H05-1102,E91-1005,1,\N,Missing
H05-1102,C96-2103,1,\N,Missing
H05-1102,J03-4003,0,\N,Missing
H05-1102,W02-1507,0,\N,Missing
H05-1102,P04-1015,0,\N,Missing
H05-1102,P00-1058,0,\N,Missing
H05-1102,J01-2004,0,\N,Missing
H05-1102,P98-1033,0,\N,Missing
H05-1102,C98-1033,0,\N,Missing
H05-1102,P02-1035,0,\N,Missing
H05-1102,J05-1004,0,\N,Missing
H05-1102,W03-3011,0,\N,Missing
H05-1102,1997.iwpt-1.11,0,\N,Missing
H05-1102,P85-1011,1,\N,Missing
H05-1113,W03-1812,0,0.900328,"onal and noncompositional MWEs. They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes (Bannard et al., 2003). So, we rate the MWEs (V-N collocations in this paper) on a scale from 1 to 6 where 6 denotes a completely compositional expression, while 1 denotes a completely opaque expression. Various statistical measures have been suggested for ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Baldwin et al., 2003) (Schutze, 1998). In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). Integrating these statistical measures should provide better evidence for ranking the expressions. We use a SVM based ranking function to integrate the features and rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features an"
H05-1113,W03-1809,0,0.652965,"support-verb constructions (Abeille, 1988), (Akimoto, 1989), among others. The expression ‘take place’ is a MWE whereas ‘take a gift’ is not a MWE. 899 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 899–906, Vancouver, October 2005. 2005 Association for Computational Linguistics It is well known that one cannot really make a binary distinction between compositional and noncompositional MWEs. They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes (Bannard et al., 2003). So, we rate the MWEs (V-N collocations in this paper) on a scale from 1 to 6 where 6 denotes a completely compositional expression, while 1 denotes a completely opaque expression. Various statistical measures have been suggested for ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Baldwin et al., 2003) (Schutze, 1998). In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositi"
H05-1113,T75-2013,0,0.147595,"ations that are highly non-compositional can be handled in a special way (Schuler and Joshi, 2004) (Hwang and Sasaki, 2005). Multi-word expressions (MWEs) are those whose structure and meaning cannot be derived from their component words, as they occur independently. Examples include conjunctions like ‘as well as’ (meaning ‘including’), idioms like ‘kick the bucket’ (meaning ‘die’), phrasal verbs like ‘find out’ (meaning ‘search’) and compounds like ‘village community’. A typical natural language system assumes each word to be a lexical unit, but this assumption does not hold in case of MWEs (Becker, 1975) (Fillmore, 2003). They have idiosyncratic interpretations which cross word boundaries and hence are a ‘pain in the neck’ (Sag et al., 2002). They account for a large portion of the language used in day-today interactions (Schuler and Joshi, 2004) and so, handling them becomes an important task. A large number of MWEs have a standard syntactic structure but are non-compositional semantically. An example of such a subset is the class of non-compositional verb-noun collocations (V-N collocations). The class of non-compositional V-N collocations is important because they are used very frequently."
H05-1113,W04-3224,0,0.0260458,"ation between the statistical features and the human ranking. We have done similar experiments in this paper where we compare the correlation value of the ranks provided by the SVM based ranking function with the ranks of the individual features for the V-N collocations. We show that the ranks given by the SVM based ranking function which integrates all the features provides a significantly better correlation than the individual features. 4 Data used for the experiments The data used for the experiments is British National Corpus of 81 million words. The corpus is parsed using Bikel’s parser (Bikel, 2004) and the Verb-Object Collocations are extracted. There are 4,775,697 V-N collocations of which 1.2 million are unique. All the V-N collocations above the frequency of 100 (n=4405) are taken to conduct the experiments so that the evaluation of the system is feasible. These 4405 V-N collocations were searched in Wordnet, American Heritage Dictionary and SAID dictionary (LDC,2003). Around 400 were found in at least one of the dictionaries. Another 400 were extracted from the rest so that the evaluation set has roughly equal number of compositional and noncompositional expressions. These 800 expre"
H05-1113,P89-1010,0,0.675966,"nal Linguistics It is well known that one cannot really make a binary distinction between compositional and noncompositional MWEs. They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes (Bannard et al., 2003). So, we rate the MWEs (V-N collocations in this paper) on a scale from 1 to 6 where 6 denotes a completely compositional expression, while 1 denotes a completely opaque expression. Various statistical measures have been suggested for ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Baldwin et al., 2003) (Schutze, 1998). In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). Integrating these statistical measures should provide better evidence for ranking the expressions. We use a SVM based ranking function to integrate the features and rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A s"
H05-1113,J93-1003,0,0.0119487,"e V-N collocation ‘raise an eyebrow’ can be represented as Frequency = 271, Mutual Information = 8.43, Distributed frequency of object = 1456.29, etc. . A SVM based ranking function uses these features to rank the V-N collocations based on their relative compositionality. These ranks are then compared with the human ranking. 3 Related Work (Breidt, 1995) has evaluated the usefulness of the Point-wise Mutual Information measure (as suggested by (Church and Hanks, 1989)) for the extraction of V-N collocations from German text corpora. Several other measures like Log-Likelihood (Church et al., (Dunning, 1993), Pearson’s  1991), Z-Score (Church et al., 1991) , Cubic Association Ratio (MI3), etc., have been also proposed. These measures try to quantify the association of two words but do not talk about quantifying the non-compositionality of MWEs. Dekang Lin proposes a way to automatically identify the noncompositionality of MWEs (Lin, 1999). He suggests that a possible way to separate compositional phrases from non-compositional ones is to check the existence and mutual-information values of phrases obtained by replacing one of the words with a similar word. According to Lin, a phrase is probabl"
H05-1113,P05-1068,0,0.01783,"ania, Philadelphia, PA 19104, USA, when he was visiting IRCS as a Visiting Scholar, February to December, 2004. Aravind K. Joshi Department of Computer and Information Science and Institute for Research in Cognitive Science, University of Pennsylvania, Philadelphia, PA, USA. joshi@linc.cis.upenn.edu locations of V-N type using a SVM based ranking function. Measuring the relative compositionality of V-N collocations is extremely helpful in applications such as machine translation where the collocations that are highly non-compositional can be handled in a special way (Schuler and Joshi, 2004) (Hwang and Sasaki, 2005). Multi-word expressions (MWEs) are those whose structure and meaning cannot be derived from their component words, as they occur independently. Examples include conjunctions like ‘as well as’ (meaning ‘including’), idioms like ‘kick the bucket’ (meaning ‘die’), phrasal verbs like ‘find out’ (meaning ‘search’) and compounds like ‘village community’. A typical natural language system assumes each word to be a lexical unit, but this assumption does not hold in case of MWEs (Becker, 1975) (Fillmore, 2003). They have idiosyncratic interpretations which cross word boundaries and hence are a ‘pain i"
H05-1113,P98-2127,0,0.100893,"Missing"
H05-1113,P99-1041,0,0.672501,"Breidt, 1995) has evaluated the usefulness of the Point-wise Mutual Information measure (as suggested by (Church and Hanks, 1989)) for the extraction of V-N collocations from German text corpora. Several other measures like Log-Likelihood (Church et al., (Dunning, 1993), Pearson’s  1991), Z-Score (Church et al., 1991) , Cubic Association Ratio (MI3), etc., have been also proposed. These measures try to quantify the association of two words but do not talk about quantifying the non-compositionality of MWEs. Dekang Lin proposes a way to automatically identify the noncompositionality of MWEs (Lin, 1999). He suggests that a possible way to separate compositional phrases from non-compositional ones is to check the existence and mutual-information values of phrases obtained by replacing one of the words with a similar word. According to Lin, a phrase is probably non-compositional if such substitutions are not found in the collocations database or their mutual information values are significantly different from that of the phrase. Another way of determining the non-compositionality of V-N collocations is by using ‘distributed frequency of object’ (DFO) in V-N collocations (Tapanainen et al., 199"
H05-1113,W03-1810,0,0.716498,"n based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). Integrating these statistical measures should provide better evidence for ranking the expressions. We use a SVM based ranking function to integrate the features and rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features and the ranks of human judges has been made by McCarthy, Keller and Caroll (McCarthy et al., 2003) for verb-particle constructions. (See Section 3 for more details). Some preliminary work on recognition of V-N collocations was presented in (Venkatapathy and Joshi, 2004). We show that the measures which we have defined contribute greatly to measuring the relative compositionality of V-N collocations when compared to the traditional features. We also show that the ranks assigned by the SVM based ranking function correlated much better with the human judgement that the ranks assigned by individual statistical measures. This paper is organized in the following sections (1) Basic Architecture,"
H05-1113,W01-0513,0,0.0467506,"with a similar word. According to Lin, a phrase is probably non-compositional if such substitutions are not found in the collocations database or their mutual information values are significantly different from that of the phrase. Another way of determining the non-compositionality of V-N collocations is by using ‘distributed frequency of object’ (DFO) in V-N collocations (Tapanainen et al., 1998). The basic idea in there is that “if an object appears only with one verb (or few verbs) in a large corpus we expect that it has an idiomatic nature” (Tapanainen et al., 1998). Schone and Jurafsky (Schone and Jurafsky, 2001) applied Latent-Semantic Analysis (LSA) to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from the corpus. An interesting way of quantifying the relative compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows (Baldwin et al., 2003). They use LSA to determine the similarity between an MWE and its constituent words, and claim that higher similarity indicates great decomposability. In terms of compositionality, an expression is likely to be relatively more compositional if it is decomposable. They evaluate their model on English NN com"
H05-1113,J98-1004,0,0.209814,"al MWEs. They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes (Bannard et al., 2003). So, we rate the MWEs (V-N collocations in this paper) on a scale from 1 to 6 where 6 denotes a completely compositional expression, while 1 denotes a completely opaque expression. Various statistical measures have been suggested for ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Baldwin et al., 2003) (Schutze, 1998). In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). Integrating these statistical measures should provide better evidence for ranking the expressions. We use a SVM based ranking function to integrate the features and rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features and the ranks of h"
H05-1113,P98-2210,0,0.646898,"e a binary distinction between compositional and noncompositional MWEs. They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes (Bannard et al., 2003). So, we rate the MWEs (V-N collocations in this paper) on a scale from 1 to 6 where 6 denotes a completely compositional expression, while 1 denotes a completely opaque expression. Various statistical measures have been suggested for ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Baldwin et al., 2003) (Schutze, 1998). In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). Integrating these statistical measures should provide better evidence for ranking the expressions. We use a SVM based ranking function to integrate the features and rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Sema"
H05-1113,I05-1049,1,0.270034,"Missing"
H05-1113,J90-1003,0,\N,Missing
H05-1113,W89-0240,0,\N,Missing
H05-1113,H89-2012,0,\N,Missing
H05-1113,C98-2205,0,\N,Missing
H05-1113,C98-2122,0,\N,Missing
H86-1017,P84-1030,1,0.89698,"e should be ""'You can't drop 577; Pi isn~ true."" Alternatively, the language generator might paraphrase the whole response as, ""if Pi were true, you could drop."" Of course there are potentially many ways to try to achieve a goal: by a single action, by a single event, or by an event and an action .... In fact, the search for a sequence of events or actions that would achieve the goal may consider many alternatives. If all fail, it is far from obvious which blocked condition to notify Q of, and knowledge is needed to guide the choice. Some heuristics for dealing with that problem ~ .. given in [12]. 3.2. A n n o n p r o d u c t i v e a c t Suppose the proposed action does not achieve Q's l-goal, cL [6]. For example, dropping the course may still mean that failing status would be recorded as a WF (withdrawal while failing). R may initially plan to answer ""You can drop 577 by ...'. However, Q would expect to be told that his proposed action does not achieve his l-goal. Formula [7] states R's belief about this expectation. [6] RB(-holds(-fail(Q,C), drop(Q,C](Sc)) & admissible(drop(Q,C}(Sc)) ) [7] RBQB(RB[ want(Q,-,fail(Q,c)) & -,holds(-fail(Q,C),drop(Q,C](Sc)) I~ admissible( drop (Q, C]( S"
H86-1017,P84-1029,1,\N,Missing
H89-1036,P84-1058,0,0.0311519,"ed above. 1 L e x i c a l i z e d Tree A d j o i n i n g G r a m m a r Most current linguistic theories give lexical accounts of several phenomena that used to be considered purely syntactic. The information put in the lexicon is thereby increased both in amount and complexity: for example, lexical rules in LFG (Kaplan and Bresnan, 1983), GPSG (Gazdar, Klein, Pullum and Sag, 1985), HPSG (Pollard and Sag, 1987), Comhinatory Categoriai Grammars (Steedman 1988), Karttunen&apos;s version of Categorial G r a m m a r (Karttunen 1986, 1988), some versions of GB theory (Chomsky 1981), and LexiconGrammars (Gross 1984). We say that a grammar is &apos;lexicalized&apos; if it consists of: 1 • a finite set of structures associated with each lexical item, which is intended to be the head of these structures; • an operation or operations for composing the structures. The finite set of structures define the domain of locality over which constraints are specified, and these are local with respect to their lexical heads. Context free grammars cannot be in general be lexicalized. However TAGs are &apos;naturally&apos; lexicalized because they use an extended domain of locality (Schabes, Abeilld and Joshi, 1988). TAGs were first introdu"
H89-1036,C73-1005,0,0.221334,"Missing"
H89-1036,P88-1032,1,0.671987,"finite and termination is guaranteed. In principle, one can proceed inside out, left to right or in any other way. Of course, standard parsing algorithms can be used, too. In particular, we can use the top-down parsing strategy without encountering the usual problems due to recursion. By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous. An Earley-type parser for TAGs has been investigated by Schabes and Joshi (1988). The algorithm has a linear best time behavior and an O(n 9) worst time behavior. This is the first practical parser for TAGs because as is well known for CFGs, the average behavior of Earley-type parsers is superior to its worst time behavior. We extended it to deal with substitution and feature structures for TAGs. By doing this, we have built a system that parses unification formalisms that have a CFG skeleton and also those that have a TAG skeleton. The Earley-type parser for TAGs can be extended to take advantage of lexicalized TAGs. Once the first pass has been performed, a subset of th"
H89-1036,C88-2121,1,0.86168,"Missing"
H89-1036,C69-4701,1,\N,Missing
H89-2053,C88-1002,0,0.0224637,"Missing"
H89-2053,P84-1058,0,0.0607668,"Missing"
H89-2053,C73-1005,0,0.139804,"Missing"
H89-2053,J86-2006,0,0.0323485,"Missing"
H89-2053,P88-1032,1,0.9276,"stics. First, the amount of filtering on the entire grammar is evaluated: once the first pass is performed, the parser uses only a subset of the grammar. Second, we evaluate the use of non-local information: the structures selected during the first pass encode the morphological value (and therefore the position in the string) of their anchor; this enables the parser to use non-local information to guide its search. We take Lexicalized Tree Adjoining Grammars as an instance of lexicallzed grammar. We illustrate the organization of the grammar. Then we show how a general Earley-type TAG parser (Schabes and Joshi, 1988) can take advantage of lexicalization. Empirical d a t a show that the filtering of the grammar and the non-local information provided by the two-pass strategy improve the performance of the parser. 1 LEXICALIZED GRAMMARS M o s t c u r r e n t linguistic theories give lexical a c c o u n t s o f several p h e n o m e n a t h a t used t o be c o n s i d e r e d p u r e l y s y n t a c t i c . T h e i n f o r m a t i o n p u t in t h e lexicon is t h e r e b y i n c r e a s e d in b o t h a m o u n t a n d c o m p l e x i t y : see, for e x a m p l e , lexical rules in L F G ( K a p l a n a n d"
H89-2053,W89-0235,1,0.354792,"Missing"
H89-2053,C88-2121,1,0.835073,"Missing"
H89-2053,E89-1001,1,\N,Missing
H89-2053,C69-4701,1,\N,Missing
H90-1010,P90-1035,1,0.784394,"need to use a more powerful configuration then a finite state automaton driving a push down stack. The design of deterministic left to right bottom up parsers for TAGs in which a finite state control drives the moves of a Bottom-up Embedded Push Down Stack has been investigated. The class of corresponding non-deterministic automata recognizes exactly the set of TALs. Due to the lack of space, we focus our attention on the bottom-up embedded pushdown automaton. The moves of the parser are sequences of moves of the automaton. The complete construction of LR-style parser for TAGs can be found in Schabes and Vijay-Shanker (1990). A u t o m a t a M o d e l s of Tags By virtue of their extended domain of locality, Tree Adjoining Grammars allow regular correspondences between larger structures to be stated without a mediating interlingual representation. The mapping of derivation trees from source to target languages, using the formalism of synchronous TAGs, makes possible to state such direct correspondences. By doing so, we are able to match linguistic units with quite different internal structures. Furthermore, the fact that the grammars are lexicalized enables capturing some idiosyncrasies of each language. Before w"
H90-1010,C88-2121,1,0.884344,"Missing"
H90-1010,C90-3045,1,0.866342,"Missing"
H90-1010,W90-0102,1,0.873911,"Missing"
H90-1010,J87-1004,0,0.166907,"t Free Grammars (Knuth, 1965) consist of a finite state control (constructed given a CFG) that drives deterministically with k lookahead symbols a push down stack, while scanning the input from left to right. It has been shown that they recognize exactly the set of languages recognized by deterministic push down automata. LR(k) parsers for CFGs have been proven useful for compilers as well as recently for natural language processing. For natural language processing, although LR(k) parsers are not powerful enough, conflicts between multiple choices are solved by pseudo-parallelism (Lang, 1974, Tomita, 1987). This gives rise to a class of powerful yet efficient parsers for natural languages. It is in this context that deterministic (LR(k)-style) parsing of TAGs is studied (this work has been done in collaboration with Vijay-Shanker). The set of Tree Adjoining Languages is a strict superset of the set of Context Free Languages (CFLs). For example, the cross serial dependency construction in Dutch can be generated by a TAG. Walters (1970), R6v6sz (1971), Turnbull and Lee (1979) investigated deterministic parsing of the class of context-sensitive languages. However they used Turing machines which re"
H90-1010,C90-3001,1,\N,Missing
H90-1010,E89-1001,1,\N,Missing
H91-1035,C90-3045,1,\N,Missing
H91-1035,C88-2121,1,\N,Missing
H91-1035,H89-1038,0,\N,Missing
H91-1035,W90-0102,1,\N,Missing
I05-1049,W03-1812,0,0.0559254,"Another way of determining the non-compositionality of V-N collocations is by using ‘distributed frequency of object’(DFO) in V-N collocations [27]. The basic idea in there is that “if an object appears only with one verb (or few verbs) in a large corpus we expect that it has an idiomatic nature” [27]. Schone and Jurafsky [24] applied Latent-Semantic Analysis (LSA) to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from the corpus. An interesting way of quantifying the relative compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows [3]. They use latent semantic analysis (LSA) to determine the similarity between 556 S. Venkatapathy and A.K. Joshi an MWE and its constituent words, and claim that higher similarity indicates great decomposability. In terms of compositionality, an expression is likely to be relatively more compositional if it is decomposable. They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Wordnet based decomposibility theory [3]. Evert and Krenn [11] compare some of the existing statistical features for the recognition of MWEs o"
I05-1049,W03-1809,0,0.0330743,"emantically. An example of such a subset is the class of noncompositional verb-noun collocations (V-N collocations). The class of V-N collocations which are non-compositional is important because they are used very frequently. These include verbal idioms [22], support-verb constructions [1] [2] etc. The expression ‘take place’ is a MWE whereas ‘take a gift’ is not a MWE. It is well known that one cannot really make a binary distinction between compositional and non-compositional MWEs. They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes [4]. So, we rate the MWEs (V-N collocations in this paper) on a scale from 1 to 6 where 6 denotes a completely compositional expression, while 1 denotes a completely opaque expression. But, to address the problem of identiﬁcation, we still need to do an approximate binary distinction. We call the expressions with a rating of 4 to 6 compositional and the expressions with rating of 1 to 3 as non-compositional. (See Section 4 for further details). Various statistical measures have been suggested for identiﬁcation of MWEs and ranking expressions based on their compositionality. Some of these are Freq"
I05-1049,W04-3224,0,0.0458536,"n ranking was better than the correlation between the statistical features and the human ranking. We have done similar experiments in this paper where we compare the correlation value of the ranks provided by the classiﬁer with the ranks of the individual features for the V-N collocations. We show that the ranks given by the classiﬁer which integrates all the features provides a signiﬁcantly better correlation than the individual features. 4 Data Used for the Experiments The data used for the experiments is British National Corpus of 81 million words. The corpus is parsed using Bikel’s parser [5] and the Verb-Object Collocations are extracted. There are 4,775,697 V-N of which 1.2 million were unique. All the V-N collocations above the frequency of 100 (n=4405) are taken to conduct the experiments so that the evaluation of the system is feasible. These 4405 V-N collocations were searched in Wordnet, American Heritage Dictionary and SAID dictionary (LDC,2003). Around 400 were found in at least one of the dictionaries. Another 400 were extracted from the rest so that the evaluation set has roughly equal number of compositional and non-compositional expressions. These 800 expressions were"
I05-1049,J90-1003,0,0.760786,"N collocations in this paper) on a scale from 1 to 6 where 6 denotes a completely compositional expression, while 1 denotes a completely opaque expression. But, to address the problem of identiﬁcation, we still need to do an approximate binary distinction. We call the expressions with a rating of 4 to 6 compositional and the expressions with rating of 1 to 3 as non-compositional. (See Section 4 for further details). Various statistical measures have been suggested for identiﬁcation of MWEs and ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information [9], Log-Likelihood [10] and Pearson’s χ2 [8]. Integrating all the statistical measures should provide better evidence for recognizing MWEs and ranking the expressions. We use various Machine Learning Techniques (classiﬁers) to integrate these statistical features and classify the VN collocations as MWEs or Non-MWEs. We also use a classiﬁer to rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features and the ranks of human j"
I05-1049,J93-1003,0,0.323002,"is paper) on a scale from 1 to 6 where 6 denotes a completely compositional expression, while 1 denotes a completely opaque expression. But, to address the problem of identiﬁcation, we still need to do an approximate binary distinction. We call the expressions with a rating of 4 to 6 compositional and the expressions with rating of 1 to 3 as non-compositional. (See Section 4 for further details). Various statistical measures have been suggested for identiﬁcation of MWEs and ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information [9], Log-Likelihood [10] and Pearson’s χ2 [8]. Integrating all the statistical measures should provide better evidence for recognizing MWEs and ranking the expressions. We use various Machine Learning Techniques (classiﬁers) to integrate these statistical features and classify the VN collocations as MWEs or Non-MWEs. We also use a classiﬁer to rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features and the ranks of human judges has been done b"
I05-1049,P01-1025,0,0.0199547,"ve compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows [3]. They use latent semantic analysis (LSA) to determine the similarity between 556 S. Venkatapathy and A.K. Joshi an MWE and its constituent words, and claim that higher similarity indicates great decomposability. In terms of compositionality, an expression is likely to be relatively more compositional if it is decomposable. They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Wordnet based decomposibility theory [3]. Evert and Krenn [11] compare some of the existing statistical features for the recognition of MWEs of adjective-noun and preposition-noun-verb types. Galiano, Valdivia, Santiago and Lopez [14] use ﬁve statistical measures to classify generic MWEs using the LVQ (Learning Vector Quantization) algorithm. In contrast, we do a more detailed and focussed study of V-N collocations and the ability of various classiﬁers in recognizing MWEs. We also compare the roles of various features in this task. McCarthy, Keller and Caroll [19] judge compositionality according to the degree of overlap in the set of most similar words"
I05-1049,W01-0513,0,0.0893442,"phrases obtained by replacing one of the words with a similar word. According to Lin, a phrase is probably non-compositional if such substitutions are not found in the collocations database or their mutual information values are signiﬁcantly diﬀerent from that of the phrase. Another way of determining the non-compositionality of V-N collocations is by using ‘distributed frequency of object’(DFO) in V-N collocations [27]. The basic idea in there is that “if an object appears only with one verb (or few verbs) in a large corpus we expect that it has an idiomatic nature” [27]. Schone and Jurafsky [24] applied Latent-Semantic Analysis (LSA) to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from the corpus. An interesting way of quantifying the relative compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows [3]. They use latent semantic analysis (LSA) to determine the similarity between 556 S. Venkatapathy and A.K. Joshi an MWE and its constituent words, and claim that higher similarity indicates great decomposability. In terms of compositionality, an expression is likely to be relatively more compositional if it is decomposable."
I05-1049,J93-1007,0,0.318724,"Missing"
I05-1049,P98-2210,0,0.74263,"hniques have been proposed to recognize MWEs. In this paper, we integrate all the existing statistical features and investigate a range of classiﬁers for their suitability for recognizing the non-compositional Verb-Noun (V-N) collocations. In the task of ranking the V-N collocations based on their relative compositionality, we show that the correlation between the ranks computed by the classiﬁer and human ranking is signiﬁcantly better than the correlation between ranking of individual features and human ranking. We also show that the properties ‘Distributed frequency of object’ (as deﬁned in [27]) and ‘Nearest Mutual Information’ (as adapted from [18]) contribute greatly to the recognition of the non-compositional MWEs of the V-N type and to the ranking of the V-N collocations based on their relative compositionality. 1 Introduction The main goals of the work presented in this paper are (1) To investigate a range of classiﬁers for their suitability in recognizing the non-compositional V-N collocations, and (2) To examine the relative compositionality of collocations of V-N type. Measuring the relative compositionality of V-N collocations is extremely helpful in applications such as ma"
I05-1049,T75-2013,0,\N,Missing
I05-1049,W89-0240,0,\N,Missing
I05-1049,H89-2012,0,\N,Missing
I05-1049,W03-1810,0,\N,Missing
I05-1049,C88-1002,0,\N,Missing
I05-1049,C98-2205,0,\N,Missing
I05-1049,P99-1041,0,\N,Missing
I08-7010,I08-2099,1,0.820437,"Missing"
I08-7010,W05-0312,0,0.26675,"e connectives. 1 (1) The federal government suspended sales of U.S. savings bonds because Congress hasn’t lifted the ceiling on government debt. One of the questions that arises is how the PDTB style annotation can be carried over to languages other than English. It may prove to be a challenge cross-linguistically, as the guidelines and methodology appropriate for English may not apply as well or directly to other languages, especially when they differ greatly in syntax and morphology. To date, cross-linguistic investigations of connectives in this direction have been carried out for Chinese (Xue, 2005) and Turkish (Deniz and Webber, 2008). This paper explores discourse relation annotation in Hindi, a language with rich morphology and free word order. We describe our study of “explicit connectives” in a small corpus of Hindi texts, discussing them from two perspectives. First, we consider the type and distribution of Hindi connectives, proposing to annotate a wider range Introduction An increasing interest in human language technologies such as textual summarization, question answering, natural language generation has recently led to the development of several discourse annotation projects a"
I08-7010,I08-7009,0,0.404376,"Missing"
I08-7010,J03-4002,1,\N,Missing
J03-4002,P01-1009,0,0.146503,"phone and picked up the receiver. Here the receiver denotes the receiver associated with (by virtue of being part of) the already-mentioned phone Myra darted to. Coreference and indirect anaphora can be uniformly modeled by saying that the discourse referent eα denoted by an anaphoric expression α is either equal to or associated with an existing discourse referent er , that is, eα =er or eα ∈assoc(er ). But coreference and associative anaphora do not exhaust the space of constructs that derive all or part of their sense from the discourse context and are thus anaphoric. Consider “other NPs” (Bierner 2001a; Bierner and Webber 2000; Modjeska 2001, 2002), as in: (26) Sue grabbed one phone, as Tom darted to the other phone. Although “other NPs” are clearly anaphoric, should the referent of the other phone (eα )—the phone other than the one Sue grabbed (er )—simply be considered a case of eα ∈ assoc(er )? Here are two reasons why they should not. First, in all cases of associative anaphora discussed in the literature, possible associations have depended only on the antecedent er and not on the anaphor. For example, only antecedents that have parts participate in whole-part associations (e.g., phon"
J03-4002,P02-1011,0,0.254166,"situation with anaphors so far, we have coreference when eα =er , indirect anaphora when eα ∈assoc(er ), and lexically specified anaphora when eα =fα (ei ) where ei = er or ei ∈assoc(er ). 3.2 Discourse Adverbials as Lexical Anaphors There is nothing in this generalized approach to discourse anaphora that requires that the source of er be an NP, or that the anaphor be a pronoun or NP. For example, the antecedent er of a singular demonstrative pronoun (in English, this or that) is often an eventuality that derives from a clause, a sentence, or a larger unit in the recent discourse (Asher 1993; Byron 2002; Eckert and Strube 2000; Webber 1991). We will show that this is the case with discourse adverbials as well. The extension we make to the general framework presented above in order to include discourse adverbials as discourse anaphors is to allow more general functions fα to be associated with lexically specified anaphors. In particular, for the discourse adverbials considered in this article, the function associated with an adverbial maps its anaphoric argument—an eventuality derived from the current discourse context—to a function that applies to the interpretation of the adverbial’s matrix"
J03-4002,W02-0204,1,0.645077,"plied to either an existing discourse referent or an entity associated with it through a bridging inference. In the case of the premodifier other, fα applied to its argument produces contextually 11 With respect to how many discourse adverbials there are, Quirk et al. (1972) discuss 60 conjunctions and discourse adverbials under the overall heading time relations and 123 under the overall heading conjuncts. Some entries appear under several headings, so that the total number of conjunctions and discourse adverbials they present is closer to 160. In another enumeration of discourse adverbials, Forbes and Webber (2002) start with all annotations of sentence-level adverbials in the Penn Treebank, then filter them systematically to determine which draw part of their meaning from the preceding discourse and how they do so. What we understand from both of these studies is that there are fewer than 200 adverbials to be considered, many of which are minor variations of one another (in contrast, by contrast, by way of contrast, in comparison, by comparison, by way of comparison that are unlikely to differ in their anaphoric properties, and some of which, such as contrariwise, hitherto, and to cap it all, will occu"
J03-4002,C92-1048,0,0.0487587,"n. So the variables must be the discourse variables usually used to translate other kinds of discourse anaphors.6 These arguments have been directed at the behavioral similarity between discourse adverbials and what we normally take to be discourse anaphors. But this isn’t the only reason to recognize discourse adverbials as anaphors: In the next section, we suggest a framework for anaphora that is broad enough to include discourse adverbials as well as definite and demonstrative pronouns and NPs, along with other discourse phenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992, 1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frank and Kamp 1997; Stone and Hardt 1999). 3. A Framework for Anaphora Here we show how only a single extension to a general framework for discourse anaphora is needed to cover discourse adverbials. The general framework is presented in Section 3.1, and the extension in Section 3.2. 3.1 Discourse Referents and Anaphor Interpretation The simplest discourse anaphors are coreferential: definite pronouns and definite NPs that denote one (or more) discourse referents in focus within the current discourse 6 Although r"
J03-4002,P85-1008,0,0.251834,"ing or deriving an eventuality from the current discourse context that meets the constraints of the adverbial with respect to the eventuality interpretation of the matrix clause. (Examples of this are given throughout the rest of the article.) 3.3 A Logical Form for Eventualities Before using this generalized view of anaphora to show what discourse adverbials contribute to discourse and how they interact with discourse relations that arise from adjacency or explicit discourse connectives, we briefly describe how we represent clausal interpretations in logical form (LF). Essentially, we follow Hobbs (1985) in using a rich ontology and a representation scheme that makes explicit all the individuals and abstract objects (i.e., propositions, facts/beliefs, and eventualities) (Asher 1993) involved in the LF interpretation of an utterance. We do so because we want to make intuitions about individuals, eventualities, lexical meaning, and anaphora as clear as possible. But certainly, other forms of representation are possible. In this LF representation scheme, each clause and each relation between clauses is indexed by the label of its associated abstract object. So, for example, the LF interpretation"
J03-4002,P02-1003,0,0.0133857,"eg SPunct |SPunct | on the one hand Seg on the other hand Seg | not only Seg but also Seg SPunct := S Punctuation Punctuation := . |; |: |? |! S := S Coord S |S Subord S |Subord S S |Sadv S | NP Sadv VP |S Sadv |. . . Coord := and |or |but |so Subord := although |after |because |before |... Sadv := DAdv |SimpleAdv DAdv := instead |otherwise |for example |meanwhile |... SimpleAdv := yesterday |today |surprisingly |hopefully |... Figure 6 PS rules for a discourse grammar. when semantics underspecifies syntactic dependency (as discourse semantics must, on our account) is known to be intractable (Koller and Striegnitz 2002). An effective solution is to generate semantics and syntax simultaneously, which is straightforward with a lexicalized grammar (Stone et al. 2001). Given the importance of various types of inference in discourse understanding, there is a second argument for using a lexicalized discourse grammar that derives from the role of implicature in discourse. Gricean reasoning about implicatures requires a hearer be able to infer the meaningful alternatives that a speaker had in composing a sentence. With lexicalization, these alternatives can be given by a grammar, allowing the hearer, for example, to"
J03-4002,P92-1004,0,0.0506838,"ure context. (Under coreference we include split reference, in which a plural anaphor such as the companies denotes all the separately mentioned companies in focus within the discourse context.) Much has been written about the factors affecting what discourse referents are taken to be in focus. For a recent review by Andrew Kehler, see chapter 18 of Jurafsky and Martin (2000). For the effect of different types of quantifiers on discourse referents and focus, see Kibble (1995). Somewhat more complex than coreference is indirect anaphora (Hellman and Fraurud 1996) (also called partial anaphora [Luperfoy 1992], textual ellipsis [Hahn, Markert, and Strube 1996], associative anaphora [Cosse 1996] bridging anaphora [Clark 1975; Clark and Marshall 1981; Not, Tovena, and Zancanaro 1999], and inferrables [Prince 1992]), in which the anaphor (usually a definite NP) denotes a discourse referent associated with one (or more) discourse referents in the current discourse context; for example, (25) Myra darted to a phone and picked up the receiver. Here the receiver denotes the receiver associated with (by virtue of being part of) the already-mentioned phone Myra darted to. Coreference and indirect anaphora c"
J03-4002,P02-1047,0,0.0341497,"genuine, or they may be eliminated by a lexical specification. Multicomponent TAG tree sets are used to provide an appropriate compositional treatment for quantifiers, which we borrow for interpreting for example (examples (66c–d)). In showing how DLTAG and an interpretative process on its derivations operate, we must, of necessity, gloss over how inference triggered by adjacency or associated with a structural connective provides the intended relation between adjacent discourse 577 Computational Linguistics Volume 29, Number 4 units: It may be a matter simply of statistical inference, as in Marcu and Echihabi (2002), or of more complex inference, as in Hobbs et al. (1993). As we noted, our view is that there are three mechanisms at work in discourse semantics, just as there are in clause-level semantics: Inference isn’t the only process involved. Thus the focus of our presentation here is on how compositional rules and anaphor resolution (which itself often appears to require inference) operate together with inference to yield discourse semantics. We start with previous examples (44) (here (66c)) and (47) (here (66d)) and two somewhat simpler variants (66a–b): (66) a. You shouldn’t trust John because he"
J03-4002,J88-2003,0,0.226457,"ian accomplishment. In example (37), though, there is no culminated eventuality in the discourse context for then), to take as its first argument. (37) a. Go west on Lancaster Avenue. b. Then turn right on County Line. How does (37b) get its interpretation? As with (36d), the relevant elements of (37b) can be represented as α = then Rα = after S = turn right on County Line σ = e3 :turn-right(you, county line) and the unresolved interpretation of (37b) is thus [λ x . after(x, EV)]e3 ≡ after(e3 , EV) 559 Computational Linguistics Volume 29, Number 4 As for resolving EV, in a well-known article, Moens and Steedman (1988) discuss several ways in which an eventuality of one type (e.g., a process) can be coerced into an eventuality of another type (e.g., an accomplishment, which Moens and Steedman call a culminated process). In this case, the matrix argument of then (the eventuality of turning right on County Line) can be used to coerce the process eventuality in (37b) into a culminated process of going west on Lancaster Avenue until County Line. We treat this coercion as a type of associative or bridging inference, as in the examples discussed in section 3.1. That is, e2 = culmination(e1 )∈assoc(e1 ), where e1"
J03-4002,J92-4007,0,0.165676,"discourse structure and discourse semantics, we will continue to assume for as long as possible that an LF representation will suffice. Now it may appear as if there is no difference between treating adverbials as anaphors and treating them as structural connectives, especially in cases like (37) in which the antecedent comes from the immediately left-adjacent context, and in which the only obvious semantic relation between the adjacent sentences appears to be the one expressed by the discourse adverbial. (Of course, there may also be a separate intentional relation between the two sentences [Moore and Pollack 1992], independent of the relation conveyed by the discourse adverbial.) One must distinguish, however, between whether a theory allows a distinction to be made and whether that distinction needs to be made in a particular case. It is clear that there are many examples in which the two approaches (i.e., a purely structural treatment of all connectives, versus one that treats adverbials as linking into the discourse context anaphorically) appear to make the same prediction. We have already, however, demonstrated cases in which a purely structural account makes the wrong prediction, and in the next"
J03-4002,P95-1018,0,0.052621,"get their interpretations in different ways. Consider the two texts in example (69): 580 Webber et al. Anaphora and Discourse Structure α: so β: but α: because_mid β:then then τ1 T1 T3 τ3 * so but * T2 because α: so τ2 1 3 τ1 0 τ2 β: but τ4 T4 3 α: because_mid 1 3 τ4 τ3 0 β: then but because so T2 T1 T3 then T4 Figure 16 Derivation of example (68). (69) a. You should eliminate part 2 before part 3 because part 2 is more susceptible to damage. b. You should eliminate part 2 before part 3. This is because part 2 is more susceptible to damage. Example (69b) is a simpler version of an example in Moser and Moore (1995), in which This is because is treated as an unanalyzed cue phrase, no different from because in (69a). We show here that this isn’t necessary: One can analyze (69b) using compositional semantics and anaphor resolution and achieve the same results. First consider (69a). Given the interpretations of its two component clauses, its overall interpretation follows in the same way as (66a), shown in Figure 12. Now consider (69b) and the derivation shown in Figure 17. Here the initial tree α:because mid T1 α:because_mid τ1 T2 . 0 because TB β: punct1 β: punct1 T1 3 1 τ2 * because α:because_mid 3 T2 TB"
J03-4002,J96-3006,0,0.0132699,"Missing"
J03-4002,W99-0105,0,0.0441425,"Missing"
J03-4002,C88-2120,0,0.594403,"for resolving them. This is explored in section 3. 3. Any theory of discourse must still provide an account of how a sequence of adjacent discourse units (clauses, sentences, and the larger units that they can comprise) means more than just the sum of its component 547 Computational Linguistics Volume 29, Number 4 units. This is a goal that researchers have been pursuing for some time, using both compositional rules and defeasible inference to determine these additional aspects of meaning (Asher and Lascarides 1999; Gardent 1997; Hobbs et al. 1993; Kehler 2002; Polanyi and van den Berg 1996; Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996) If that portion of discourse semantics that can be handled by mechanisms already needed for resolving other forms of anaphora and deixis is factored out, there is less need to stretch and possibly distort compositional rules and defeasible inference to handle everything.2 Moreover, recognizing the possibility of two separate relations (one derived anaphorically and one associated with adjacency and/or a structural connective) admits additional richness to discourse semantics. Both points are discussed further in section 4. 4. Understanding discourse"
J03-4002,P98-2204,0,0.0570362,"(iii) Figure 5 Discourse structures for examples (11)–(13). Structural dependencies are indicated by solid lines and dependencies associate with discourse adverbials are indicated by broken lines. (explanation is the inverse of explanation—i.e., with its arguments in reverse order. Such relations are used to maintain the given linear order of clauses.) 551 Computational Linguistics Volume 29, Number 4 (14) Every mani tells every womanj hei meets that shej reminds himi of hisi mother. (15) Suei drives an Alfa Romeo. Shei drives too fast. Maryj races heri on weekends. Shej often beats heri . (Strube 1998) This suggests that in examples (11)–(13), the relationship between the discourse adverbial and its (initial) argument from the previous discourse might usefully be taken to be anaphoric as well.4 2.2 Discourse Adverbials Do Behave like Anaphors There is additional evidence to suggest that otherwise, then, and other discourse adverbials are anaphors. First, anaphors in the form of definite and demonstrative NPs can take implicit material as their referents. For example, in (16) Stack five blocks on top of one another. Now close your eyes and try knocking {the tower, this tower} over with your"
J03-4002,J88-2006,1,0.660385,"usually used to translate other kinds of discourse anaphors.6 These arguments have been directed at the behavioral similarity between discourse adverbials and what we normally take to be discourse anaphors. But this isn’t the only reason to recognize discourse adverbials as anaphors: In the next section, we suggest a framework for anaphora that is broad enough to include discourse adverbials as well as definite and demonstrative pronouns and NPs, along with other discourse phenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992, 1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frank and Kamp 1997; Stone and Hardt 1999). 3. A Framework for Anaphora Here we show how only a single extension to a general framework for discourse anaphora is needed to cover discourse adverbials. The general framework is presented in Section 3.1, and the extension in Section 3.2. 3.1 Discourse Referents and Anaphor Interpretation The simplest discourse anaphors are coreferential: definite pronouns and definite NPs that denote one (or more) discourse referents in focus within the current discourse 6 Although rhetorical structure theory (RST) (Mann and Thompson 19"
J03-4002,P92-1013,1,0.612638,"Missing"
J03-4002,W98-0315,1,0.480953,"scourse clause (Dc ): a clause or a structure composed of discourse clauses. One reason for taking something to be an initial tree is that its local dependencies can be stretched long distance. At the sentence level, the dependency between apples and likes in Apples John likes is localized in all the trees for likes. This dependency can be stretched long distance, as in Apples, Bill thinks John may like. In discourse, as we noted in section 2, local dependencies can be stretched long distance as well, as in (59) a. Although John is generous, he’s hard to find. 16 Although in an earlier paper (Webber and Joshi 1998), we discuss reasons for taking the lexical anchors of the initial trees in Figures 7 and 8 to be feature structures, following the analysis in Knott (1996) and Knott and Mellish (1996), here we just take them to be specific lexical items. 574 Webber et al. Anaphora and Discourse Structure α:contrast Dc Dc On the one hand Dc On the other Figure 8 An initial tree for parallel constructions. This particular tree is for a contrastive construction anchored by on the one hand and on the other hand. b. Although John is generous—for example, he gives money to anyone who asks him for it—he’s hard to f"
J03-4002,P99-1006,1,0.654145,"Missing"
J03-4002,W93-0239,0,0.295266,"me family of discourse relations. But what if the relational meaning conveyed by cue phrases could in fact interact with discourse meaning in multiple ways? Then Knott’s substitution patterns among cue phrases may have reflected these complex interactions, as well as the meanings of individual cue phrases themselves. This article argues that cue phrases do depend on another mechanism for conveying extrasentential meaning—specifically, anaphora. One early hint that adverbial cue phrases (called here discourse connectives) might be anaphoric can be found in an ACL workshop paper in which Janyce Wiebe (1993) used the following example to question the adequacy of tree structures for discourse: (1) a. The car was finally coming toward him. b. He [Chee] finished his diagnostic tests, c. feeling relief. d. But then the car started to turn right. The problem Wiebe noted was that the discourse connectives but and then appear to link clause (1d) to two different things: then to clause (1b) in a sequence relation (i.e., the car’s starting to turn right being the next relevant event after Chee’s finishing his tests) and but to a grouping of clauses (1a) and (1c) (i.e., reporting a contrast between, on the"
J03-4002,J00-4006,0,\N,Missing
J03-4002,C98-2199,0,\N,Missing
J14-4007,C12-1163,0,0.0216544,"s for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used th"
J14-4007,W12-4703,0,0.0192018,"s for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used th"
J14-4007,W13-2610,0,0.105535,"Missing"
J14-4007,W01-1605,0,0.526226,"Missing"
J14-4007,F12-2042,0,0.10177,"nd Xue (in press)), the Turkish Discourse Bank or TDB (Zeyrek et al. 2008, 2009; Aktas¸, Bozs¸ahin, and Zeyrek 2010; Zeyrek et al. 2010; Demirsahin et al. 2013; Zeyrek et al. 2013), the Hindi Discourse Relation Bank (Oza et al. 2009; Kolachina et al. 2012; Sharma et al. 2013), and the Prague Discourse TreeBank, or PDiT (Mladov´a, Zik´anov´a, and Hajiˇcov´a 2008; J´ınov´a, M´ırovsky, ´ and Pol´akov´a 2012; Rysov´a 2012; Pol´akov´a et al. 2013), now part of the Prague Dependency TreeBank, version 3.0, PDT 3.0 (Bejˇcek et al. 2013). (A comparable discourse treebank is being developed for French (Danlos et al. 2012), but it has not yet been released and the information needed to compare it to the other corpora in Table 4 is not available.) Although these comparable corpora differ in ways to be discussed subsequently, they all adhere to the key ideas of PDTB annotation (Section 2) in being neutral to any discourse structure beyond the argument structure of individual discourse relations and in grounding discourse relations in lexical expressions. Where they annotate implicit discourse relations (Table 4), these comparable corpora follow the PDTB in annotating an inferred lexical grounding. All of the corp"
J14-4007,W13-2315,0,0.0301067,"Missing"
J14-4007,W05-0305,1,0.280638,"Missing"
J14-4007,I11-1120,0,0.0122955,"e outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the ai"
J14-4007,ghosh-etal-2012-improving,0,0.0782143,"Missing"
J14-4007,J93-3003,0,0.567299,"er, and Joshi 2006). This set was then enlarged as new connectives were found in the WSJ corpus itself. Also identified during this phase were productive modifiers of explicit connectives such as apparently, at least partly, in large part, even, only, and so on, which were then annotated as connective modifiers.2 What were not taken to be discourse connectives were adverbial cue phrases, including sentence-initial Now (Example (3)), Well (Example (4)), So (Example (5)), and OK (Example (6)), because they signal topic changes such as the beginning of a subtopic or a return to a previous topic (Hirschberg and Litman 1993), rather than relating particular discourse elements. (3) Now why, you have to ask yourself, would intelligent beings haul a bunch of rocks around the universe? [wsj 0550] (4) Well, mankind can rest easier for now. [wsj 1272] (5) So, OK kids, everybody on stage for “Carry On Trading.” [wsj 2402] (6) When Mr. Jacobson walked into the office at 7:30 a.m. EDT, he announced: “OK, buckle up.” [wsj 1171] We did not intend to annotate as discourse connectives pragmatic markers such as actually and in fact, which serve to signal the conversational role of the speaker’s matrix utterance—specifically, t"
J14-4007,W12-4704,0,0.0298796,"Missing"
J14-4007,kolachina-etal-2012-evaluation,1,0.889811,"Missing"
J14-4007,J93-2004,0,0.0567604,"Missing"
J14-4007,P11-3009,0,0.0119687,"n both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDT"
J14-4007,W12-0117,0,0.0254165,"age technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents wha"
J14-4007,W13-3303,1,0.724081,"istics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since"
J14-4007,W04-2703,1,0.292051,"Missing"
J14-4007,mladova-etal-2008-sentence,0,0.117353,"Missing"
J14-4007,J88-2003,0,0.713677,"Missing"
J14-4007,J05-1004,0,0.126043,"Missing"
J14-4007,pareti-2012-database,0,0.0241257,"cessary for the discourse relation in Example (20).5 (19) Defense contractors “cannot continue to get contracts on that basis,” said Howard Rubel, an analyst with C.J. Lawrence, Morgan Grenfell Inc. in New York. (implicit=because) “The pain is too great.” [wsj 0673] 5 The PDTB also annotates attribution relations, capturing their textual signal and semantic features over each discourse relation and each of its arguments. For a full description of attribution and its annotation, the reader is referred to Prasad et al. (2007). Attribution is now being annotated as a separate layer over the WSJ (Pareti 2012), building on the PDTB attribution scheme, but aiming to capture the phenomena more comprehensively than in the PDTB. 929 Computational Linguistics (20) Volume 40, Number 4 Mr. Asman is also annoyed that Mr. Castro has resisted collaboration with U.S. officials, even though by his own account that collaboration has been devised essentially as a mechanism for acts directly hostile to the Cuban regime, such as facilitating defections. [wsj 1416] Attribution differs from supplementary information in that, when its polarity is negative, it can interact with discourse relations. (Sup has no such in"
J14-4007,D13-1094,0,0.452089,"ns (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used the PDTB, Section 3 aims to point out significant featur"
J14-4007,P09-2004,0,0.0946201,"it=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create compar"
J14-4007,C08-2022,1,0.898974,"of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of an"
J14-4007,I13-1011,0,0.176202,"Missing"
J14-4007,prasad-etal-2008-penn,1,0.9379,"on that may have weakened previous results or the performance of decision procedures induced from the data; (3) to explain variations seen in the annotation of comparable resources in other languages and genres, which should allow developers of future comparable resources to recognize whether the variations are relevant to them; and (4) to enumerate and explain relationships between PDTB annotation and complementary annotation of other linguistic phenomena. The paper draws on work done by ourselves and others since the corpus was released. 1. Introduction The Penn Discourse TreeBank, or PDTB (Prasad et al. 2008; PDTB-Group 2008) is the largest manually annotated resource of discourse relations. This annotation has been added to the million-word Wall Street Journal portion of the Penn Treebank (PTB) corpus ∗ Department of Health Informatics and Administration, University of Wisconsin-Milwaukee, 2025 E. Newport Ave (NWQB), Milwaukee WI 53211. E-mail: prasadr@uwm.edu. ∗∗ School of Informatics, University of Edinburgh, 10 Crichton Street (IF4.29), Edinburgh UK EH8 9AB. E-mail: bonnie.webber@ed.ac.uk. † Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street (Suite 400"
J14-4007,prasad-etal-2010-exploiting,1,0.857193,"aining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages"
J14-4007,C10-2118,1,0.673052,"aining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages"
J14-4007,W05-0302,0,0.0584237,"Missing"
J14-4007,rysova-2012-alternative,0,0.0860411,"Missing"
J14-4007,W13-0124,1,0.948187,"ve begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since"
J14-4007,D07-1010,0,0.00884648,"ction Agency imposed a gradual ban on virtually all uses of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Othe"
J14-4007,W05-0312,0,0.038729,"Missing"
J14-4007,W09-3006,0,0.0495351,"Missing"
J14-4007,W10-1844,0,0.0503983,"Missing"
J14-4007,P12-1008,0,0.0294487,"nature and the sources of this variation, so that people contemplating development of comparable resources in additional languages and/or genres will recognize variation that is appropriate to their situation, while avoiding unnecessary variation that prevents inter-operability of these comparable corpora (Bunt, Prasad, and Joshi 2012). Table 4 identifies the corpora we will discuss and the extent of their current annotation: the BioDRB (Prasad et al. 2011), the Leeds Arabic Discourse TreeBank, or LADTB (Al-Saif and Markert 2010, 2011; Al-Saif 2012), the Chinese Discourse TreeBank (Xue 2005; Zhou and Xue 2012; Zhou and Xue (in press)), the Turkish Discourse Bank or TDB (Zeyrek et al. 2008, 2009; Aktas¸, Bozs¸ahin, and Zeyrek 2010; Zeyrek et al. 2010; Demirsahin et al. 2013; Zeyrek et al. 2013), the Hindi Discourse Relation Bank (Oza et al. 2009; Kolachina et al. 2012; Sharma et al. 2013), and the Prague Discourse TreeBank, or PDiT (Mladov´a, Zik´anov´a, and Hajiˇcov´a 2008; J´ınov´a, M´ırovsky, ´ and Pol´akov´a 2012; Rysov´a 2012; Pol´akov´a et al. 2013), now part of the Prague Dependency TreeBank, version 3.0, PDT 3.0 (Bejˇcek et al. 2013). (A comparable discourse treebank is being developed for"
J14-4007,C10-2172,0,0.133404,"ng asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). W"
J14-4007,W10-1832,0,\N,Missing
J14-4007,D11-1068,0,\N,Missing
J14-4007,W09-3029,1,\N,Missing
J14-4007,al-saif-markert-2010-leeds,0,\N,Missing
J14-4007,W04-0211,0,\N,Missing
J14-4007,W11-1401,0,\N,Missing
J15-4009,C65-1020,0,0.363139,"n grammar might help her teach better. Although its subject matter did not match her expectations, the talk marked a turning point in her career. In the late 1950s, Jane became a consultant to the RAND Corporation group working on machine translation under Dave Hays (ACL president, 1964). From the beginning, Jane was concerned with identifying connections between different traditions in formal grammars and their corresponding detailed linguistic realizations. Her 1965 International Conference on Computational Linguistics (COLING) paper, “Endocentric constructions and the Cocke parsing logic” (Robinson 1965), is a beautiful example of connecting specific linguistic phenomena to parsing strategies in a way that preserves the nature of the linguistic phenomena, endocentric constructions. While at RAND, Jane became colleague and friend to many in the machine translation and emerging computational linguistics world, including Susumo Kuno (ACL president 1967), Martin Kay (ACL president, 1969), Joyce Friedman (ACL president, 1971), and Karen Sparck Jones (ACL president, 1994). In the late 1960s, Jane moved to the Automata Theory and Computability Group at the IBM Thomas J. Watson Research Center in Yor"
J15-4009,C67-1015,0,0.407095,"mas J. Watson Research Center in Yorktown Heights, NY. She used her knowledge of formal work on grammars and parsing to draw correspondences between Dependency Grammars and Phrase Structure Grammars. Although Jane came ∗ E-mail: grosz@eecs.harvard.edu. doi:10.1162/COLI a 00235 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 4 from the Dependency Grammar tradition, her balanced, careful analysis of tradeoffs enabled others to bridge the approaches. Her 1967 COLING paper, “Methods for obtaining corresponding phrase structure and dependency grammars” (Robinson 1967), is a wonderful example of her understanding of the seminal issues underlying these different systems. She subsequently published the classic paper connecting dependency structure and transformational rules, “Dependency structures and transformational rules” (Robinson 1970b). This paper exemplifies Jane’s scholarship and her deftness in dealing with the formal and computational issues of language processing in a very fair and informative manner. Her 1970 paper, “Case, category and configuration” (Robinson 1970a), demonstrated in a very convincing way the possibility of formally interpreting F"
J77-1008,T75-1005,0,0.0364248,"Missing"
J77-1008,T75-2023,0,0.0663207,"Missing"
J86-1015,J86-2003,0,0.0302802,"Missing"
J95-2003,P87-1022,0,0.241864,"Missing"
J95-2003,C90-2047,0,0.0984511,"Missing"
J95-2003,P83-1007,1,0.0962755,"Missing"
J95-2003,P86-1031,0,0.0731744,"Missing"
J95-2003,P93-1009,0,0.109852,"Missing"
J95-2003,P93-1000,0,0.123217,"he French Lieutenant's Woman. b. The book, which is Fowles's best, was a bestseller last year. The second case concerns the use of a pronoun to realize an entity not in the CdUn); such uses are strongly constrained. The particular cases that have been identified involve instances where attention is shifted globally back to a previously centered entity (e.g. Grosz [1977], Reichman [1985]). In such cases additional inferences are 17 Empirical investigations of these claims of Grosz, Joshi, and Weinstein (1986) suggest they are too strong. In particular, the results of Gordon, Grosz, and Gilliom (1993) suggest that (16d) without the intervening (c) utterance is not as bad as (15c). 18 Sequence (17) is an adaptation of one of Sidner's examples (Sidner 1979). 216 Barbara J. Grosz et al. Centering required to d e t e r m i n e that the p r o n o u n does not refer to a m e m b e r of the current f o r w a r d - l o o k i n g centers and to identify the context back to which attention is shifting. Further investigation is required to d e t e r m i n e the linguistic cues (e.g., intonation or cue phrases [Grosz and Hirschberg 1992]) and intentional information that are required to enable such sh"
J95-2003,P91-1009,0,0.107468,"Missing"
J95-2003,P89-1031,0,0.0589947,"Missing"
J95-2003,P90-1010,0,0.0659645,"Missing"
J95-2003,J94-2003,0,0.0912978,"Missing"
J95-2003,J86-3001,1,\N,Missing
J99-2004,J94-4005,0,0.0123151,"es are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This allows the system to assign the most-likely structure to each input. The output of these systems consists of constituent analysis, the degree of detail of which is dependent on the detail of annotation present in the treebank that is used to train the system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Tempe"
J99-2004,P93-1005,0,0.00471353,"language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded in the rules. This allows the system to assign the most-likely structure to each input. The output of these systems consists of constituent analysis, the degree of detail of which is dependent on the detail of annotation present in the treebank that is used to train the system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Jos"
J99-2004,P98-1022,0,0.0434909,"Missing"
J99-2004,P93-1035,0,0.00518333,"ees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity. The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requiring that all and only the dependent ele"
J99-2004,C96-2183,0,0.00538926,"Missing"
J99-2004,A88-1019,0,0.0171294,"ted by auxiliary trees. Elementary trees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity. The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requirin"
J99-2004,P96-1025,0,0.0103291,"tems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information that is encoded i"
J99-2004,C94-2149,0,0.0474949,"agging. 6.3 N-gram Models with Smoothing We have improved the performance of the trigram model by incorporating smoothing techniques into the model and training the model on a larger training corpus. We have also proposed some new models for supertag disambiguation. In this section, we discuss these developments in detail. Two sets of data are used for training and testing the models for supertag disambiguation. The first set has been collected by parsing the Wall Street Journal 7, IBM Manual, and ATIS corpora using the wide-coverage English grammar being developed as part of the XTAG system (Doran et al. 1994). The correct derivation from all the derivations produced by the XTAG system was picked for each sentence from these corpora. The second and larger data set was collected by converting the Penn Treebank parses of the Wall Street Journal sentences. The objective was to associate each lexical item of a sentence with a supertag, given the phrase structure parse of the sentence. This process involved a number of heuristics based on local tree contexts. The heuristics made use of information about the labels of a word's dominating nodes (parent, grandparent, and great-grandparent), labels of its s"
J99-2004,W89-0209,0,0.0704691,"e also Joshi (1998). 238 Bangalore and Joshi Supertagging matches the input string at a given position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparen"
J99-2004,P84-1058,0,0.107157,", for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system. 3. Lexicalized Grammars Lexicalized grammars are particularly well-suited for the specification of natural language grammars. The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992). Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalization. Lexicalization provides a clean interface for combining the syntactic and semantic information in the lexicon. We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars. Feature-based Lexicalized"
J99-2004,H94-1052,0,0.00730541,"n position. At present none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probabil"
J99-2004,C94-1024,1,0.249145,"g can be seen as specifying dependency requirements of the supertag. The probability with which a supertag depends on another supertag is collected from a corpus of sentences annotated with derivation structures. Given a set of supertags for each word and the dependency information between pairs of supertags, the objective of the dependency model is to compute the most likely dependency linkage that spans the entire string. The result of producing the dependency linkage is a sequence of supertags, one for each word of the sentence along with the dependency information. Since first reported in Joshi and Srinivas (1994), we have not continued experiments using this model of supertagging, primarily for two reasons. We are restrained by the lack of a large corpus of LTAG parsed derivation structures that is needed to reliably estimate the various parameters of this model. We are currently in the process of collecting a large LTAG parsed WSJ corpus, with each sentence annotated with the correct derivation. A second reason for the disuse of the dependency model for supertagging is that the objective of supertagging is to see how far local techniques can be used to disambiguate supertags even before parsing begin"
J99-2004,P95-1013,0,0.013813,"Missing"
J99-2004,C94-1025,0,0.0324563,"Missing"
J99-2004,P95-1037,0,0.0275782,"none of these systems use any statistical information to resolve ambiguity. The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions. This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995). 2.2 Statistical Parsers Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al. (1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it. These systems attempt to assign some structure to every input string. The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language. The resultant set of rules are not linguistically transparent and are not easily modifiable. Lexical and structural ambiguity is resolved using probability information t"
J99-2004,J93-2004,0,0.0366461,"Missing"
J99-2004,C88-2121,1,0.411404,"Missing"
J99-2004,E93-1040,0,0.0187113,"Missing"
J99-2004,1997.iwpt-1.22,0,0.167114,"Missing"
J99-2004,1995.iwpt-1.27,0,0.0261713,"Missing"
J99-2004,C94-1104,0,0.0214218,"Missing"
J99-2004,J93-2006,0,0.0134038,"ary trees. Elementary trees are combined by substitution and adjunction operations. The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree. The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence. A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A. 4. Supertags Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity. The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word. As is well known, these taggers are quite successful. In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree). The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requiring that all and only the"
J99-2004,H92-1026,0,\N,Missing
J99-2004,H93-1047,0,\N,Missing
J99-2004,C98-1022,0,\N,Missing
kolachina-etal-2012-evaluation,tonelli-etal-2010-annotation,1,\N,Missing
kolachina-etal-2012-evaluation,W04-2703,1,\N,Missing
kolachina-etal-2012-evaluation,mladova-etal-2008-sentence,0,\N,Missing
kolachina-etal-2012-evaluation,W09-3029,1,\N,Missing
kolachina-etal-2012-evaluation,al-saif-markert-2010-leeds,0,\N,Missing
kolachina-etal-2012-evaluation,W09-3036,1,\N,Missing
kolachina-etal-2012-evaluation,prasad-etal-2008-penn,1,\N,Missing
kolachina-etal-2012-evaluation,W11-0414,1,\N,Missing
kolachina-etal-2012-evaluation,I08-7009,0,\N,Missing
kolachina-etal-2012-evaluation,P12-1008,0,\N,Missing
kolachina-etal-2012-evaluation,I08-2099,1,\N,Missing
miltsakaki-etal-2004-penn,kingsbury-palmer-2002-treebank,0,\N,Missing
miltsakaki-etal-2004-penn,J93-2004,0,\N,Missing
miltsakaki-etal-2004-penn,W04-2703,1,\N,Missing
miltsakaki-etal-2004-penn,J03-4002,1,\N,Missing
miltsakaki-etal-2004-penn,P02-1045,0,\N,Missing
P03-1064,J99-2004,1,\N,Missing
P03-1064,W99-0621,0,\N,Missing
P03-1064,J93-2004,0,\N,Missing
P03-1064,W96-0213,0,\N,Missing
P03-1064,C94-1024,1,\N,Missing
P03-1064,W02-1001,0,\N,Missing
P03-1064,E99-1025,0,\N,Missing
P03-1064,N01-1006,0,\N,Missing
P03-1064,P01-1069,0,\N,Missing
P03-1064,J95-4004,0,\N,Missing
P03-1064,P98-1081,0,\N,Missing
P03-1064,C98-1078,0,\N,Missing
P03-1064,N03-1028,0,\N,Missing
P03-1064,N01-1025,0,\N,Missing
P07-1096,P04-1015,0,0.042089,"at our system does not need a large beam. As shown in Section 4.2, even deterministic inference shows rather good results. Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daum´e III and Marcu, 2005). However, as far as we know, 765 Data Set Training Develop Test Sections 0-18 19-21 22-24 Sentences 38,219 5,527 5,462 Tokens 912,344 131,768 129,654 Table 1: Data set splits the mechanism of bidirectional search with an online learning algorithm has not been investigated before. In (Daum´e III and Marcu, 2005), as well as other similar works (Collins, 2002; Collins and Roark, 2004; Shen and Joshi, 2005), only left-toright search was employed. Our guided learning algorithm provides more flexibility in search with an automatically learned order. In addition, our treatment of the score of action and the score of hypothesis is unique (see discussion in Section 2.3). Furthermore, compared to the above works, our guided learning algorithm is more aggressive on learning. In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates. In (Daum´e III and Marcu, 2005), the search is resume"
P07-1096,W02-1001,0,0.365173,"re, the order of inference and the local classification are dynamically incorporated in the learning phase. Guided learning is not as hard as reinforcement learning. At each local step in learning, we always know the undesirable labeling actions according to the gold standard, although we do not know which is the most desirable. In this approach, we can easily collect the automatically generated negative samples, and use them in learning. These negative samples are exactly those we will face during inference with the current weight vector. In our experiments, we have used Averaged Perceptron (Collins, 2002; Freund and Schapire, 1999) and Perceptron with margin (Krauth and M´ezard, 1987) to improve performance. 3 Related Works Tsuruoka and Tsujii (2005) proposed a bidirectional POS tagger, in which the order of inference is handled with the easiest-first heuristic. Gim´enez and M`arquez (2004) combined the results of a left-toright scan and a right-to-left scan. In our model, the order of inference is dynamically incorporated into the training of the local classifier. Toutanova et al. (2003) reported a POS tagger based on cyclic dependency network. In their work, the order of inference is fixed"
P07-1096,gimenez-marquez-2004-svmtool,0,0.162643,"Missing"
P07-1096,H05-1102,1,0.792033,"ed a large beam. As shown in Section 4.2, even deterministic inference shows rather good results. Our guided learning can be modeled as a search algorithm with Perceptron like learning (Daum´e III and Marcu, 2005). However, as far as we know, 765 Data Set Training Develop Test Sections 0-18 19-21 22-24 Sentences 38,219 5,527 5,462 Tokens 912,344 131,768 129,654 Table 1: Data set splits the mechanism of bidirectional search with an online learning algorithm has not been investigated before. In (Daum´e III and Marcu, 2005), as well as other similar works (Collins, 2002; Collins and Roark, 2004; Shen and Joshi, 2005), only left-toright search was employed. Our guided learning algorithm provides more flexibility in search with an automatically learned order. In addition, our treatment of the score of action and the score of hypothesis is unique (see discussion in Section 2.3). Furthermore, compared to the above works, our guided learning algorithm is more aggressive on learning. In (Collins and Roark, 2004; Shen and Joshi, 2005), a search stops if there is no hypothesis compatible with the gold standard in the queue of candidates. In (Daum´e III and Marcu, 2005), the search is resumed after some gold stand"
P07-1096,N03-1033,0,0.607831,"Missing"
P07-1096,H05-1059,0,0.403316,"m being used in large scale NLP tasks. Collins (2002) proposed a Perceptron like learning algorithm to solve sequence classification in the traditional left-to-right order. This solution does not suffer from the label bias problem. Compared to the undirected methods, the Perceptron like algorithm is faster in training. In this paper, we will improve upon Collins’ algorithm by introducing a bidirectional searching strategy, so as to effectively utilize more context information at little extra cost. When a bidirectional strategy is used, the main problem is how to select the order of inference. Tsuruoka and Tsujii (2005) proposed the easiest-first approach which greatly reduced the computation complexity of inference while maintaining the accuracy on labeling. However, the easiest-first approach only serves as a heuristic rule. The order of inference is not incorporated into the training of the MaxEnt classifier for individual labeling. Here, we will propose a novel learning framework, namely guided learning, to integrate classification of individual tokens and inference order selection into a single learning task. We proposed a Perceptron like learning algorithm (Collins and Roark, 2004; Daum´e III and Marcu"
P07-1096,J93-2004,0,\N,Missing
P83-1002,J82-1001,1,\N,Missing
P83-1007,P83-1005,0,0.0214199,"Missing"
P83-1007,P82-1028,0,0.0149252,"Missing"
P83-1007,J81-4001,0,0.0746498,"and coherence with the other subconstituents. T h a t is, discourses have been shown to have two levels of coherence. Global coherence refers to the ways in which the larger segments of discourse relate to one another. It depends on such things as the function of a discourse, its subject matter, and rhetorical schema [Grosz, 1977, 1981; Reichman, 1981 I. Local coherence refers to the ways in which individual sentences bind together to form larger discourse segments. It depends on such things as the syntactic structure of an utterance, ellipsis, and the use of pronominal referring expressions [Sidner, 1981 I. The two levels of discourse coherence correspond to two levels of focusing--global focusing and centering. Participants are said to be globally focused on a set of entitie.~ relevant to the overall discourse. These entities may either have been explicitly introduced into the discourse or be sufficiently closely related to such entities to be considered implicitly in focus [Grosz, 19811. In contrast, centering refers to a more local focusing process, one relates to identifying the single entity that an individual utterance most centrally concerns [Sidner, 1979; Joshi and Weinstein, 1981]. T"
P84-1029,P83-1007,1,\N,Missing
P87-1015,J84-3005,0,0.15258,"tions whose path sets have nested dependencies. Introduction Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical forma~sm- Little attention, however, has been paid to the structuraldescriptions that these formalisms can assign to strings, i.e. their strong generative capacity. This aspect of the formalism i s beth linguistically and computationally important. For example, Gazdar (1985) discusses the applicability of Indexed Grammars (IG&apos;s) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar CLFG) and Government and Bindings grammars (GB). The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG&apos;s and IG&apos;s. We consider properties of the tree sets generated by CFG&apos;s, Tree Adjoining Grammars (TAG&apos;s), Head GrammarS (HG&apos;s), Categorial Grammars (CG&apos;s), and IG&apos;s. We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths. These two properties of the tree sets are not only"
P87-1015,J88-4001,0,\N,Missing
P88-1032,P81-1022,0,0.065742,"uxiliary tree whose root node is also labeled by X. Then the adjunction of fl to a at node n will be the tree 7 shown in Figure 2. The resulting tree, 7, is built as follows: * The sub-tree of a dominated by n, call it t, is excised, leaving a copy of n behind. • The auxiliary tree fl is attached at n and its root node is identified with n. • The sub-tree t is attached to the foot node of # and the root node n of t is identified with the foot node of ft. $ runs much better than its worst time complexity, we decided to try to adapt Earley's parser for CFGs to TAGs. Earley's algorithm for CFGs (Earley, 1970, Aho and Ullman, 1973) is a bottomup parser which uses top-down information. It manipulates states of the form A -* a.fl[i] while using three processors: the predictor, the completot and the scanner. The algorithm for CFGs runs in O(IGl2n s) time and in O(IGI n2) space in all cases, and parses unambiguous grammars in O(n 2) time (n being the length of the input, IGI the size of the grammar). Given a context-free grammar in any form and an input string al &quot; ' a n , Earley's parser for CFGs maintains the following invariant: The state A --* a./3[i] is in states set Skiff $ S ::b 6A'r, 6 :bal &quot;"
P88-1032,P84-1075,0,0.0184666,"a(dot) is a non-terminal on the frontier of ~ .hieh is marked for subst itut ion: It adds the states {[fl, O, left, above, i, - , - , - , - , - , true] 4.3 ]/~ i s an L n i t i a l tree s . t . # ( O ) -- or(dot)} to Si. P a r s i n g f e a t u r e s t r u c t u r e s for TAGs The definition of feature structures for TAGs and their semantics was proposed by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). We first explain briefly how they work in TAGs and show how we have implemented them. We introduce in a TAG framework a language similar to PATR-II which was investigated by Shieber (Shieber, 1984 and 1986). We then show how one can embed the essential aspects of PATR-II in this system. Substitution Completor Suppose that the initial tree that we predicted for substitution has been recognized (see Figure 18). Then the algorithm should try to recognize the rest of the tree in which we predicted a substitution. This operation is performed by the s u b s t i tution completor. 267 t br A tUu&quot; m NP br f I PRO tf ..- I, Ubr (a) Vp / V / PP to go to the movies S.top::gtsnsed> = + S,bottom::<tensed> = V.boRom::<tensed> V.bottom::<tensed> = - Figure 19: Updating of features $ F e a t u r e s"
P88-1032,P85-1011,1,0.639221,"nals, S is a distinguished nonterminal, I is a finite set of trees called initial t r e e s and A is a finite set of trees called a u x i l i a r y t r e e s . The trees in I U A are called e l e m e n t a r y trees. I n i t i a l t r e e s (see left tree in Figure 1) are characterized as follows: internal nodes are labeled by non-terminals; leaf nodes are labeled by either terminal symbols or the empty string. Introduction S Although formal properties of Tree Adjoining Grammars (TAGs) have been investigated (VijayShanker, 1987)--for example, there is an O(ns)time CKY-like algorithm for TAGs (Vijay-Shanker and Joshi, 1985)--so far there has been no attempt to develop an Earley-type parser for TAGs. This paper presents an Earley parser for TAGs and discusses modifications to the parsing algorithm that make it possible to handle extensions of TAGs such as constraints on adjunction, subx /x Li~minill$ tofnflnldJ$ Ltef rntnll|$ Figure h Schematic initial and auxiliary trees A u x i l i a r y t r e e s (see right tree in Figure 1) are characterized as follows: internal nodes are labeled by non-terminals; leaf nodes are labeled by a terminal or by the empty string except for exactly one node (called the f o o t n o"
P88-1032,C88-2147,0,0.0897224,"trees rooted by A and tries to recognize the initial tree. This operation is performed by the s u b s t i t u t i o n predictor. It applies t o s[~, dot, left, above, l, f l, fr , star, t~ i b~ , subst?] such that a(dot) is a non-terminal on the frontier of ~ .hieh is marked for subst itut ion: It adds the states {[fl, O, left, above, i, - , - , - , - , - , true] 4.3 ]/~ i s an L n i t i a l tree s . t . # ( O ) -- or(dot)} to Si. P a r s i n g f e a t u r e s t r u c t u r e s for TAGs The definition of feature structures for TAGs and their semantics was proposed by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988). We first explain briefly how they work in TAGs and show how we have implemented them. We introduce in a TAG framework a language similar to PATR-II which was investigated by Shieber (Shieber, 1984 and 1986). We then show how one can embed the essential aspects of PATR-II in this system. Substitution Completor Suppose that the initial tree that we predicted for substitution has been recognized (see Figure 18). Then the algorithm should try to recognize the rest of the tree in which we predicted a substitution. This operation is performed by the s u b s t i tution completor. 267 t br A tUu&quot; m"
P88-1032,C88-1002,0,\N,Missing
P88-1034,T75-2001,0,0.462793,"hat they produced. Several formalisms that had previously been descn2~d as mildly contextsensitive were found to share a number of properties. In particular, the derivations of a grammar could be represenled with trees that always formed the tree set of a context-free grammar. Formalisms that share these properties were called Linear Context-Free Rewriting Systems Introduction There have been a number of results concerning the relationship between the weak generative capacity (family of string languages) associated with different grammar formalisms; for example, the thecxem of Oaifman, et al. [3] that Classical Categorial Grammars are weakly equivalent to Context-Free Grammars (CFG&apos;s). Mote recently it has been found that there is a class of languages slightly larger than the class of Context-Free languages that is generated by several different formalisms. In pardodar, Tree Adjoining Grammars (TAG&apos;s) and Head Grammars (HG&apos;s) have been shown to be weakly equivalent [15], and these formalism are also equivalent to a reslriction of Indexed Grammars considered by Gazdar [6] called Linear Indexed Grammars (LIG&apos;s) [13]. In this paper, we examine Combinatory Categorial Grammars (CCG&apos;s), an"
P88-1034,C86-1047,0,0.148532,"S&apos;s. This does not, however, nile out the possibility that there may be alternative ways of representing the derivation of CCG&apos;s that will allow for their classification as LCP&apos;RS&apos;s. Extensions to CCG&apos;s have been considered that enable them to compare two unbounded sU&apos;uctures (for example, in [12]). It has been argued that this may be needed in the analysis of certain coordination phenomena in Dutch. In Section 5 we discuss how these additional features increase the power of the formalism. In so doing, we also give an example demonstrating that the Parenthesisfree Categorial Grammar formalism [5,4] is moze powerful that CCG&apos;s as defined here. Extensions to TAG&apos;s (Multicomponent TAG) have been considered for similar *This work was partially mpportedby NSF gnmts MCS-82-19116CER. MCS-82-07294, DCR-84-10413, ARO grant DAA29-84-9-0027. and DARPA gnmt N0014-85-K0018. We are very grateful to Mark Steedmm, ]C Vijay-Shanker and Remo Pare~:hi for helpful disctmiem. 278 Restrictions can be associated with the use of the combinatory rule in R. These restrictions take the form of conswaints on the instantiations of variables in the rules. These can be constrained in two ways. reasons. However, in th"
P88-1034,P86-1012,0,0.0666507,"S&apos;s. This does not, however, nile out the possibility that there may be alternative ways of representing the derivation of CCG&apos;s that will allow for their classification as LCP&apos;RS&apos;s. Extensions to CCG&apos;s have been considered that enable them to compare two unbounded sU&apos;uctures (for example, in [12]). It has been argued that this may be needed in the analysis of certain coordination phenomena in Dutch. In Section 5 we discuss how these additional features increase the power of the formalism. In so doing, we also give an example demonstrating that the Parenthesisfree Categorial Grammar formalism [5,4] is moze powerful that CCG&apos;s as defined here. Extensions to TAG&apos;s (Multicomponent TAG) have been considered for similar *This work was partially mpportedby NSF gnmts MCS-82-19116CER. MCS-82-07294, DCR-84-10413, ARO grant DAA29-84-9-0027. and DARPA gnmt N0014-85-K0018. We are very grateful to Mark Steedmm, ]C Vijay-Shanker and Remo Pare~:hi for helpful disctmiem. 278 Restrictions can be associated with the use of the combinatory rule in R. These restrictions take the form of conswaints on the instantiations of variables in the rules. These can be constrained in two ways. reasons. However, in th"
P88-1034,C86-1048,1,0.67775,"There have been a number of results concerning the relationship between the weak generative capacity (family of string languages) associated with different grammar formalisms; for example, the thecxem of Oaifman, et al. [3] that Classical Categorial Grammars are weakly equivalent to Context-Free Grammars (CFG&apos;s). Mote recently it has been found that there is a class of languages slightly larger than the class of Context-Free languages that is generated by several different formalisms. In pardodar, Tree Adjoining Grammars (TAG&apos;s) and Head Grammars (HG&apos;s) have been shown to be weakly equivalent [15], and these formalism are also equivalent to a reslriction of Indexed Grammars considered by Gazdar [6] called Linear Indexed Grammars (LIG&apos;s) [13]. In this paper, we examine Combinatory Categorial Grammars (CCG&apos;s), an extension of Classical Categorial Grammars developed by Steedman and his collaborators [1,12,9,10,11]. The main result in this paper is (&apos;LCFRS&apos;s) [14]. On the basis of their weak generative capacity, it appears that CCG&apos;s should be classified as mildly contextsensitive. In Section 4 we consider whether CCG&apos;s should be included in the class of LCFRS&apos;s. The derivation tree sets t"
P88-1034,P87-1015,1,\N,Missing
P89-1027,C88-2147,1,\N,Missing
P89-1027,C88-1060,0,\N,Missing
P89-1027,P88-1032,1,\N,Missing
P89-1027,P87-1015,1,\N,Missing
P89-1027,P88-1034,1,\N,Missing
P89-1027,P86-1038,0,\N,Missing
P95-1036,C94-2149,1,0.821184,"erivation tree can also be interpreted as a dependency tree 2 with unlabeled arcs between words of the sentence as shown in Figure 2(c). Elementary trees of LTAG are the domains for specifying dependencies. Recursive structures are specified via the auxiliary trees. The three aspects of LTAG - (a) lexicalization, (b)-extended domain of locality and (c) factoring of recursion, provide a natural means for generalization during the EBL proce88. 3 O v e r v i e w of our approach to using EBL We are pursuing the EBL approach in the context of a wide-coverage grammar development system called XTAG (Doran et al., 1994). The XTAG system consists of a morphological analyzer, a part-ofspeech tagger, a wide-coverage LTAG English grammar, a predictive left-to-right Early-style parser for LTAG (Schabes, 1990) and an X-windows interface for grammar development (Paroubek et al., 1992). Figure 3 shows a flowchart of the XTAG system. The input sentence is subjected to morphological analysis and is parts-of-speech tagged before being sent to the parser. The parser retrieves the elementary trees that the words of the sentence anchor and combines them by adjunction and substitution operations to derive a parse of the se"
P95-1036,C94-1024,1,0.848007,"Missing"
P95-1036,A92-1030,1,0.815438,"trees. The three aspects of LTAG - (a) lexicalization, (b)-extended domain of locality and (c) factoring of recursion, provide a natural means for generalization during the EBL proce88. 3 O v e r v i e w of our approach to using EBL We are pursuing the EBL approach in the context of a wide-coverage grammar development system called XTAG (Doran et al., 1994). The XTAG system consists of a morphological analyzer, a part-ofspeech tagger, a wide-coverage LTAG English grammar, a predictive left-to-right Early-style parser for LTAG (Schabes, 1990) and an X-windows interface for grammar development (Paroubek et al., 1992). Figure 3 shows a flowchart of the XTAG system. The input sentence is subjected to morphological analysis and is parts-of-speech tagged before being sent to the parser. The parser retrieves the elementary trees that the words of the sentence anchor and combines them by adjunction and substitution operations to derive a parse of the sentence. Given this context, the training phase of the EBL process involves generalizing the derivation trees generated by XTAG for a training sentence and storing these generalized parses in the generalized parse 2There axe some differences between derivation tre"
P95-1036,C92-2065,0,0.0223421,"Missing"
P95-1036,P94-1026,0,0.185219,"ly supported by ARC) grant DAAL03-89-0031, ARPA grant N00014-90-J-1863, NSF STC grsmt DIR-8920230, and Ben Franklin Partnership Program (PA) gremt 93S.3078C-6 Rayner (1991) specialize a grammar for the ATIS domain by storing chunks of the parse trees present in a treebank of parsed examples. The idea is to reparse the training examples by letting the parse tree drive the rule expansion process and halting the expansion of a specialized rule if the current node meets a 'tree-cutting' criteria. However, the problem of specifying an optimal 'tree-cutting' criteria was not addressed in this work. Samuelsson (1994) used the information-theoretic measure of entropy to derive the appropriate sized tree chunks automatically. Neumann (1994) also attempts to specialize a grammar given a training corpus of parsed exampies by generalizing the parse for each sentence and storing the generalized phrasal derivations under a suitable index. Although our work can be considered to be in this general direction, it is distinct in that it exploits some of the key properties of LTAG to (a) achieve an immediate generalization of parses in the training set of sentences, (b) achieve an additional level of generalization of"
P95-1036,C88-2121,1,0.844065,"Missing"
P95-1036,C92-2066,0,0.0383095,"Missing"
P99-1006,P97-1012,1,0.841173,"Because y ~5&quot;, they cannot together be realised as &quot;Although ~ because y [3 &&quot; with the same meaning as &quot;Although o¢ [3. Because y 8&quot;. The same is true of certain relations whose realisation spans multiple sentences, such as ones realisable as &quot;On the one hand oz. On the other hand 13.&quot; and &quot;Not only T- But also &&quot; Together, they cannot be realised as &quot;On the one hand o¢. Not only T. On the other hand 13. But also &&quot; with the same meaning as in strict sequence. Thus we take such constructions to be structural as well (Webber and Joshi, 1998; Webber et al., 1999). Framework In previous papers (Cristea and Webber, 1997; Webber and Joshi, 1998; Webber et al., 1999), we have argued for using the more complex structures (elementary trees) of a Lexicalized Tree-Adjoining Grammar (LTAG) and its operations (adjoining and substitution) to associate structure and semantics with a sequence of discourse clauses. 2 Here we briefly review how it works. In a lexicalized TAG, each elementary tree has at least one anchor. In the case of discourse, the anchor for an elementary tree may be a lexical item, punctuation or a feature structure that is lexically null. The semantic contribution of a lexical anchor includes both w"
P99-1006,W98-0304,0,0.0231865,"modifier (e.g. &quot;He&apos;s an otherwise happy boy.&quot;) or a clausal modifier (e.g., &quot;The physical layer is different, but otherwise it&apos;s identical to metropolitan networks.&quot;). What is presupposed here are one or more actual properties of the situation under discussion. If the light had been red, John would have stopped. Otherwise, he would have carded straight on. But as it turned out, he never got to the light. 46 (9) You should take a coat with you because otherwise you&apos;ll get cold. Clearly, more remains to be done. First, the approach demands a precise semantics for connectives, as in the work of Grote (1998), Grote et al. (1997), Jayez and Rossari (1998) and Lagerwerf (1998). Secondly, the approach demands an understanding of the attentional characteristics of presuppositions. In particular, preliminary study seems to suggest that p-bearing elements differ in what source can license them, where this source can be located, and what can act as distractors for this source. In fact, these differences seem to resemble the range of differences in the information status (Prince, 1981; Prince, 1992) or familiarity (Gundel et al., 1993) of referential NPs. Consider, for example: and earlier examples. (Not"
P99-1006,J92-4007,0,0.1424,"ventions, etc., can then make defeasible contributions to discourse interpretation that elaborate the nondefeasible propositions contributed by compositional semantics. Introduction Research on discourse structure has, by and large, attempted to associate all meaningful relations between propositions with structural connections between discourse clauses (syntactic clauses or structures composed of them). Recognising that this could mean multiple structural connections between clauses, Rhetorical Structure Theory (Mann and Thompson, 1988) simply stipulates that only a single relation may hold. Moore and Pollack (1992) argue that both informational (semantic) and intentional relations can hold between clauses simultaneously and independently. This suggests that factoring the two kinds of relations might lead to a pair of structures, each still with no more than a single structural connection between any two clauses. But examples of multiple semantic relations are easy to find (Webber et al., 1999). Having structure account for all of them leads to the complexities shown in Figure 1, including the crossing dependencies shown in Fig. l c. These structures are no longer trees, making it difficult to define a c"
P99-1006,P97-1026,1,0.0647756,"1999), we have argued for using the more complex structures (elementary trees) of a Lexicalized Tree-Adjoining Grammar (LTAG) and its operations (adjoining and substitution) to associate structure and semantics with a sequence of discourse clauses. 2 Here we briefly review how it works. In a lexicalized TAG, each elementary tree has at least one anchor. In the case of discourse, the anchor for an elementary tree may be a lexical item, punctuation or a feature structure that is lexically null. The semantic contribution of a lexical anchor includes both what it presupposes and what it asserts (Stone and Doran, 1997; Stone, 1998; Stone and Webber, 1998). A feature structure anchor will either unify with a lexical item with compatible features (Knott and Mellish, 1996), yielding the previous case, or have an empty realisation, though one On the other hand, the p-bearing adverb &quot;then&quot;, which asserts that one eventuality starts after the culmination of another, has only one of its arguments coming structurally. The other argument is presupposed and thus able to come from across a structural boundary, as in (1) a. b. c. d. 1One may still need to admit structures having both a link back and a link forward to"
P99-1006,W98-1419,1,0.852452,"more complex structures (elementary trees) of a Lexicalized Tree-Adjoining Grammar (LTAG) and its operations (adjoining and substitution) to associate structure and semantics with a sequence of discourse clauses. 2 Here we briefly review how it works. In a lexicalized TAG, each elementary tree has at least one anchor. In the case of discourse, the anchor for an elementary tree may be a lexical item, punctuation or a feature structure that is lexically null. The semantic contribution of a lexical anchor includes both what it presupposes and what it asserts (Stone and Doran, 1997; Stone, 1998; Stone and Webber, 1998). A feature structure anchor will either unify with a lexical item with compatible features (Knott and Mellish, 1996), yielding the previous case, or have an empty realisation, though one On the other hand, the p-bearing adverb &quot;then&quot;, which asserts that one eventuality starts after the culmination of another, has only one of its arguments coming structurally. The other argument is presupposed and thus able to come from across a structural boundary, as in (1) a. b. c. d. 1One may still need to admit structures having both a link back and a link forward to different clauses (Gardent, 1997). But"
P99-1006,W98-0315,1,0.532764,"two relations, one realisable as &quot;Although o¢ [3, the other realisable as &quot;Because y ~5&quot;, they cannot together be realised as &quot;Although ~ because y [3 &&quot; with the same meaning as &quot;Although o¢ [3. Because y 8&quot;. The same is true of certain relations whose realisation spans multiple sentences, such as ones realisable as &quot;On the one hand oz. On the other hand 13.&quot; and &quot;Not only T- But also &&quot; Together, they cannot be realised as &quot;On the one hand o¢. Not only T. On the other hand 13. But also &&quot; with the same meaning as in strict sequence. Thus we take such constructions to be structural as well (Webber and Joshi, 1998; Webber et al., 1999). Framework In previous papers (Cristea and Webber, 1997; Webber and Joshi, 1998; Webber et al., 1999), we have argued for using the more complex structures (elementary trees) of a Lexicalized Tree-Adjoining Grammar (LTAG) and its operations (adjoining and substitution) to associate structure and semantics with a sequence of discourse clauses. 2 Here we briefly review how it works. In a lexicalized TAG, each elementary tree has at least one anchor. In the case of discourse, the anchor for an elementary tree may be a lexical item, punctuation or a feature structure that is"
P99-1006,J88-2006,1,0.218389,"at source. However, as with pronominal and definite NP anaphora, while attentional constraints on their interpretation may be influenced by structure, the links themselves are not structural. * Our thanks to Mark Steedman, Katja Markert, Gann Bierner and three ACL&apos;99 reviewers for all their useful comments. 41 The idea of combining compositional semantics with defeasible inference is not new. Neither is the idea of taking certain lexical items as anaphorically presupposing an eventuality or a set of eventualities: It is implicit in all work on the anaphoric nature of tense (cf. Partee (1984), Webber (1988), inter alia) and modality (Stone, 1999). What is new is the way we enable anaphoric presupposition to contribute to semantic relations and modal operators, in a way R1 Ci Ci (a) CI Ci Ck Ci (b) C i R2 Ck Cm (c) Figure 1: Multiple semantic links ( R j ) between discourse clauses ( C i ) : (a) back to the same discourse clause; (b) back to different discourse clauses; (c) back to different discourse clauses, with crossing dependencies. that maintains its semantic features. that does not lead to the violations of tree structure mentioned earlier.t We discuss these differences in more detail in S"
prasad-etal-2008-penn,W98-0315,1,\N,Missing
prasad-etal-2008-penn,J93-2004,0,\N,Missing
prasad-etal-2008-penn,W04-2703,1,\N,Missing
prasad-etal-2008-penn,W06-0305,1,\N,Missing
prasad-etal-2008-penn,W05-0305,1,\N,Missing
prasad-etal-2008-penn,W01-1605,0,\N,Missing
prasad-etal-2008-penn,J03-4002,1,\N,Missing
prasad-etal-2008-penn,J05-1004,0,\N,Missing
prasad-etal-2010-exploiting,poesio-artstein-2008-anaphoric,0,\N,Missing
prasad-etal-2010-exploiting,J97-1003,0,\N,Missing
prasad-etal-2010-exploiting,W05-0305,1,\N,Missing
prasad-etal-2010-exploiting,C00-1031,0,\N,Missing
prasad-etal-2010-exploiting,J95-2003,1,\N,Missing
prasad-etal-2010-exploiting,P09-1076,1,\N,Missing
prasad-etal-2010-exploiting,prasad-etal-2008-penn,1,\N,Missing
prasad-etal-2010-exploiting,W04-2322,0,\N,Missing
T87-1011,P84-1027,0,0.0314388,"erested in the efficiency of computation, the primary motivation for the formalism has to do with the second meaning of processing considerations. The standard CFG based formalisms (augmented in a variety of ways) can do all the computations that a unification based, formalism can do and vice-versa, however, the semantics of the formalism (not of the language described by the grammar) is not always well understood. The same is, of course, true of the ATN formalism. The unification formalism does give an opportunity to provide a well-def&apos;med semantics because of its algebraic characterization (Pereira and Schieber, 1984). How this understanding can be cashed into efficient algorithms for processing is still very much an open question. Good engineering is based on good theory therein lies the hope. Are we converging to some class of formalisms that are relevant to processing and, if so, how can this class be characterized in a theoretical manner? Most of the grammatical formalisms, especially those of the so-called nontransformational flavor, have been motivated, at least in part, by processing considerations, for example, parsing complexity. We could say that these formalisms are converging if convergence is"
T87-1011,P85-1018,0,0.0128861,"uble for parsing. Clearly, the standard parsing algorithms for parsing CFG&apos;s cannot be extended to unification formalism because of the exponential blowup of 45 computational complexity, including the possibility of nontermination. One could focus only on parts of feature structures, not necessarily the same parts for different feature structures, and thereby, have a flexible notion of nonterminal on the one hand and, perhaps, control the computational complexity on the other hand. This aspect of unification formalism has not received much attention yet, except in the very interesting work of Schieber (1985). To what extent has the unification formalism been motivated by processing considerations? First of all, we should distinguish at least two meanings of processing considerations. One has to do with the efficiency of computation and the other has to do with computational formalisms, which are well-defined and whose semantics (i.e., the semantics of the formalism) also can be well-defined. Although the unification formalism has been developed largely by researchers who are, no doubt, interested in the efficiency of computation, the primary motivation for the formalism has to do with the second"
T87-1011,C86-1048,1,0.890663,"Missing"
tonelli-etal-2010-annotation,pareti-prodanof-2010-annotating,0,\N,Missing
tonelli-etal-2010-annotation,J93-2004,0,\N,Missing
tonelli-etal-2010-annotation,mladova-etal-2008-sentence,0,\N,Missing
tonelli-etal-2010-annotation,W05-0312,0,\N,Missing
tonelli-etal-2010-annotation,W09-3029,1,\N,Missing
tonelli-etal-2010-annotation,W09-0505,1,\N,Missing
tonelli-etal-2010-annotation,prasad-etal-2008-penn,1,\N,Missing
tonelli-etal-2010-annotation,I08-7009,0,\N,Missing
W00-1208,J94-4004,0,0.0516326,"Missing"
W00-1208,W00-1307,1,0.931318,"Chinese in the sense that argument NPs are freely permutable (subject to certain discourse constraints). Third, Korean and Chinese freely allow subject and object deletion, but English does not. Fourth, Korean has richer inflectional morphology t h a n English, whereas Chinese has little, if any, inflectional morphology. 2.2 (NP (NP (iNNfountain)(NNS pens)) (CO and) (NP (VBGblotting)(NN papers)))))))) Figure 1: An example from Penn English Treebank 3.1 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II (Marcus et al., 1993), the Chinese P e n n Treebank (Xia et al., 2000b), and the Korean Penn Treebank (Chung-hye Han, 2000). The main parameters of these Treebanks are summarized in Table 1.1 The tags in each tagset can be classified into one of four types: (1) syntactic tags for phrase-level annotation, (2) PartOf-Speech (POS) tags for head-level annotation, (3) function tags for grammatical function annotation, and (4) empty category tags for dropped arguments, traces, and so on. We chose these Treebanks because they all use phrase structure annotation and their annotation schemata are similar, which facilitates the comparison between the extracted Treebank g"
W00-1208,xia-etal-2000-developing,1,0.818684,"Chinese in the sense that argument NPs are freely permutable (subject to certain discourse constraints). Third, Korean and Chinese freely allow subject and object deletion, but English does not. Fourth, Korean has richer inflectional morphology t h a n English, whereas Chinese has little, if any, inflectional morphology. 2.2 (NP (NP (iNNfountain)(NNS pens)) (CO and) (NP (VBGblotting)(NN papers)))))))) Figure 1: An example from Penn English Treebank 3.1 Three Treebanks The Treebanks that we used in this paper are the English Penn Treebank II (Marcus et al., 1993), the Chinese P e n n Treebank (Xia et al., 2000b), and the Korean Penn Treebank (Chung-hye Han, 2000). The main parameters of these Treebanks are summarized in Table 1.1 The tags in each tagset can be classified into one of four types: (1) syntactic tags for phrase-level annotation, (2) PartOf-Speech (POS) tags for head-level annotation, (3) function tags for grammatical function annotation, and (4) empty category tags for dropped arguments, traces, and so on. We chose these Treebanks because they all use phrase structure annotation and their annotation schemata are similar, which facilitates the comparison between the extracted Treebank g"
W00-1208,palmer-etal-1998-rapid,1,\N,Missing
W00-1208,J93-2004,0,\N,Missing
W00-1307,2000.iwpt-1.9,0,0.179946,"Missing"
W00-1307,E99-1025,0,0.033721,"Missing"
W00-1307,P97-1003,0,0.241549,"ls. Figure 7 shows the fully bracketed ttree. The nodes inserted by LexTract are in bold face. Treebank-specific information The phrase structures in the Treebank (ttrees for short) are partially bracketed in the sense that arguments and modifiers are not structurally distinguished. In order to construct the etrees, which make such distinction, LexTract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table. A Head Percolation Table has previously been used in several statistical parsers (Magerman, 1995; Collins, 1997) to find heads of phrases. Our strategy for choosing heads is similar to the one in (Collins, 1997). An Argument Table informs LexTract what types of arguments a head can take. The Tagset Table specifies what function tags always mark arguments (adjuncts, heads, respectively). LexTract marks each sibling of a head as an argument if the sibling can be an argument of the head according to the Argument Table and none of the function tags of the sister indicates that it is an adjunct. For example, in Figure 5, the head of the root S is the verb draft, and the verb has two siblings: the noun phrase"
W00-1307,C94-2149,0,0.0207537,"types of annotation errors. We have used LexTract for the final cleanup of the Penn Chinese Treebank. Due to space limitation, in this paper we will only discuss the first two tasks. 4.1 Evaluating the coverage of hand-crafted grammars The XTAG grammar (XTAG-Group, 1998) is a hand-crafted large-scale grammar for English, which has been developed at University of Pennsylvania in the last decade. It has been used in many NLP applications such as generation (Stone and Doran, 1997). Evaluating the coverage of such a grammar is important for both its developers and its users. Previous evaluations (Doran et al., 1994; Srinivas et al., 1998) of the XTAG grammar use raw data (i.e., a set of sentences without syntactic bracketing). The data are first parsed by an LTAG parser and the coverage of the g r a m m a r is measured as the percentage of sentences in the data that get at least one parse, which is not necessarily the correct parse. For more discussion on this approach, see (Prasad and Sarkar, 2000). We propose a new evaluation m e t h o d that takes advantage of Treebanks and LexTract. The idea is as follows: given a Treebank T and a hand-crafted grammar Gh, the coverage of Gh on T can be measured by t"
W00-1307,C94-1024,1,0.738348,"Missing"
W00-1307,P98-1115,0,0.111927,"ects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG formalism The primitive elements of an LTAG are elementary trees (etrees). Each etree is associated with a lexical item (called the anchor of the tree) on its frontier. We choose LTAGs as our target grammars (i.e., the grammars to be extracted) be"
W00-1307,P95-1037,0,0.0945816,"o different levels. Figure 7 shows the fully bracketed ttree. The nodes inserted by LexTract are in bold face. Treebank-specific information The phrase structures in the Treebank (ttrees for short) are partially bracketed in the sense that arguments and modifiers are not structurally distinguished. In order to construct the etrees, which make such distinction, LexTract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table. A Head Percolation Table has previously been used in several statistical parsers (Magerman, 1995; Collins, 1997) to find heads of phrases. Our strategy for choosing heads is similar to the one in (Collins, 1997). An Argument Table informs LexTract what types of arguments a head can take. The Tagset Table specifies what function tags always mark arguments (adjuncts, heads, respectively). LexTract marks each sibling of a head as an argument if the sibling can be an argument of the head according to the Argument Table and none of the function tags of the sister indicates that it is an adjunct. For example, in Figure 5, the head of the root S is the verb draft, and the verb has two siblings:"
W00-1307,P92-1007,0,0.0295146,"lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG formalism The primitiv"
W00-1307,C96-2103,1,0.838615,"Missing"
W00-1307,W00-2027,0,0.0272379,"It has been used in many NLP applications such as generation (Stone and Doran, 1997). Evaluating the coverage of such a grammar is important for both its developers and its users. Previous evaluations (Doran et al., 1994; Srinivas et al., 1998) of the XTAG grammar use raw data (i.e., a set of sentences without syntactic bracketing). The data are first parsed by an LTAG parser and the coverage of the g r a m m a r is measured as the percentage of sentences in the data that get at least one parse, which is not necessarily the correct parse. For more discussion on this approach, see (Prasad and Sarkar, 2000). We propose a new evaluation m e t h o d that takes advantage of Treebanks and LexTract. The idea is as follows: given a Treebank T and a hand-crafted grammar Gh, the coverage of Gh on T can be measured by the overlap of Gh and a Treebank grammar Gt that is produced by LexTract from T. In this case, we will estimate the coverage of the XTAG grammar on the English Penn Treebank (PTB) using the Treebank grammar G2. There are obvious differences between these two grammars. For example, feature structures and multi-anchor etrees are present only in the XTAG grammar, whereas frequency information"
W00-1307,P92-1022,0,0.0701148,"Missing"
W00-1307,P97-1026,0,0.120564,"entative of a class of lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalized tree structures (called elementary trees), not context-free rules (which can be seen. as trees with depth one). Therefore, an LTAG extraction algorithm needs to examine 2 LTAG f"
W00-1307,W98-0315,1,0.566444,"e our approaches with related work. 1 Introduction There are various grammar frameworks proposed for natural languages. We take Lexicalized Tree-adjoining Grammars (LTAGs) as representative of a class of lexicalized grammars. LTAGs (Joshi et al., 1975) are appealing for representing various phenomena in natural languages due to its linguistic and computational properties. In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., parsing (Schabes, 1990; Srinivas, 1997), semantics (Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 1999), and discourse (Webber and Joshi, 1998)) and a number of NLP applications (e.g., machine translation (Palmer et al., 1998), information retrieval (Chandrasekar and Srinivas, 1997), and generation (Stone and Doran, 1997; McCoy et al., 1992). This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks). There has been much work done on extracting Context-Free grammars (CFGs) (Shirai et al., 1995; Charniak, 1996; Krotov et al., 1998). However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs. First, the primitive elements of an LTAG are lexicalize"
W00-1307,W00-2030,1,0.863403,"Missing"
W00-1307,W00-1208,1,0.84805,"et al., 1993) and got two Treebank grammars. The first one, G1, uses the Treebank&apos;s tagset. The second Treebank grammar, G2, uses a reduced tagset, where some tags in the Treebank tagset are merged into a single tag. For example, the tags for verbs, MD/VB/VBP/VBZ/VBN/VBD/VBG, are merged into a single tag V. The reduced tagset is basically the same as the tagset used in the XTAG g r a m m a r ( X T A G - G r o u p , 1998). G2 is built so that we can compare it with the XTAG grammar, as will be discussed in the next section. We also ran the system on the 100-thousand-word Chinese Penn Treebank (Xia et al., 2000b) and on a 30-thousand-word Korean Penn Treebank. The sizes of extracted grammars are shown in Table 1. (For more discussion on the Chinese and the Korean Treebanks and the comparison between these Treebank grammars, see (Xia et al., 2000a)). The second column of the table lists the numbers of unique templates in each grammar, where templates are etrees with the lexical items removed, s The third column shows the numbers of unique S NP VP I &lt;1 N V I t John left Ja,. (E l) (T*) [ &~m t (E) lea (E,) I I John left (E2) lc~hn [ (Es) The Experiments Icft (E6) Figure 11: Tree sets for a fully brack"
W00-1307,xia-etal-2000-developing,1,0.69136,"et al., 1993) and got two Treebank grammars. The first one, G1, uses the Treebank&apos;s tagset. The second Treebank grammar, G2, uses a reduced tagset, where some tags in the Treebank tagset are merged into a single tag. For example, the tags for verbs, MD/VB/VBP/VBZ/VBN/VBD/VBG, are merged into a single tag V. The reduced tagset is basically the same as the tagset used in the XTAG g r a m m a r ( X T A G - G r o u p , 1998). G2 is built so that we can compare it with the XTAG grammar, as will be discussed in the next section. We also ran the system on the 100-thousand-word Chinese Penn Treebank (Xia et al., 2000b) and on a 30-thousand-word Korean Penn Treebank. The sizes of extracted grammars are shown in Table 1. (For more discussion on the Chinese and the Korean Treebanks and the comparison between these Treebank grammars, see (Xia et al., 2000a)). The second column of the table lists the numbers of unique templates in each grammar, where templates are etrees with the lexical items removed, s The third column shows the numbers of unique S NP VP I &lt;1 N V I t John left Ja,. (E l) (T*) [ &~m t (E) lea (E,) I I John left (E2) lc~hn [ (Es) The Experiments Icft (E6) Figure 11: Tree sets for a fully brack"
W00-1307,palmer-etal-1998-rapid,1,\N,Missing
W00-1307,J93-2004,0,\N,Missing
W00-1307,C98-1111,0,\N,Missing
W00-1605,E99-1025,0,0.298469,"Missing"
W00-1605,J93-2004,0,0.0251099,"Missing"
W00-1605,W00-2027,1,0.634896,"ation forest for each sentence which stores, in compact form, all derivations for each sentence. 2 1 0 2 4 6 8 10 12 Sentence length 14 16 18 20 Figure 3: Parse times plotted against sentence length. Coefficient of determination: . (x-axis: Sentence length; y-axis: log(time in seconds)) Since we can easily determine the number of trees selected by a sentence before we start parsing, we can use this number to predict the number of edges that will be proposed by a parser when parsing this sentence, allowing us to better handle difficult cases before parsing. 1 2 Some of these results appear in (Sarkar, 2000). In this section we present some additional data on the previous results and also the results of some new experiments that do not appear in the earlier work. Note that the precise number of edges proposed by the parser and other common indicators of complexity can be obtained only while or after parsing. We are interested in predicting parsing complexity. 39 10 the parser was reduced: 926 sentences (out of the 2250) did not get any parse. This was because some crucial tree was missing in the -best output. The results are graphed in Figure 6. The total number of derivations for all sentences w"
W00-1605,1997.iwpt-1.22,0,0.0335395,"ture for each word in the sentence. This eliminates lexical syntactic ambiguity but does not eliminate attachment ambiguity for the parser. The graph comparing the parsing times is shown in Figure 5. As the comparison shows, the elimination of lexical ambiguity leads to a drastic increase in parsing efficiency. The total time taken to parse all sentences went from 548K seconds to 31.2 seconds. Figure 5 shows us that a model which disambiguates syntactic lexical ambiguity can potentially be extremely useful in terms of parsing efficiency. Thus disambiguation of tree assignment or SuperTagging (Srinivas, 1997) of a sentence before parsing it might be a way of improving parsing efficiency. This gives us a way to reduce the parsing complexity for precisely the sentences which were problematic: the ones which selected too many trees. To test whether parsing times are reduced after SuperTagging we conducted an experiment in which the output of an -best SuperTagger was taken as input to the parser. In our experiment we set to be .3 The time taken to parse the same set of sentences was again dramatically reduced (the total time taken was 21K seconds). However, the disadvantage of this method was that the"
W00-1605,A00-2022,0,\N,Missing
W00-2015,W98-0107,0,0.103648,"Missing"
W00-2015,J94-1004,0,0.137471,"Missing"
W00-2015,P94-1022,0,\N,Missing
W03-0402,P93-1005,0,0.0187894,"y of using discriminative machine learning algorithms in sequential models is to rerank the n-best outputs of a generative system. Reranking uses global features as well as local features, and does not make local normalization. If the output set is large enough, the reranking approach may help to alleviate the impact of the label bias problem, because the victim parses (i.e. those parses which get penalized due to the label bias problem) will have a chance to take part in the reranking. In recent years, reranking techniques have been successfully applied to the so-called history-based models (Black et al., 1993), especially to parsing (Collins, 2000; Collins and Duffy, 2002). In a history-based model, the current decision depends on the decisions made previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been describe"
W03-0402,N01-1025,0,0.0371817,"es. Preference kernel PK in our paper is the same as the preference kernel in (Herbrich et al., 2000) in format. However, the purpose of our model is different from that of the ordinal regression algorithm. Ordinal regression searches for a regression function for ordinal values, while our algorithm is designed to solve a voting problem. As a result, the two algorithms differ on the definition of the margin. In ordinal regression, the margin is min |f (ri ) − f (ri−1 )|, where f is the regression function for ordinal values. In our algorithm, the margin is min |score(xi1 ) − score(xij )|. In (Kudo and Matsumoto, 2001), SVMs have been employed in the NP chunking task, a typical labeling problem. However, they have used a deterministic algorithm for decoding. In (Collins, 2000), two reranking algorithms were proposed. In both of these two models, the loss functions are computed directly on the feature space. All the features are manually defined. In (Collins and Duffy, 2002), the Voted Perceptron algorithm was used to in parse reranking. It was shown in (Freund and Schapire, 1999; Graepel et al., 2001) that error bound of (voted) Perceptron is related to margins existing in the training data, but these algor"
W03-0402,P02-1034,0,0.701777,"quential models is to rerank the n-best outputs of a generative system. Reranking uses global features as well as local features, and does not make local normalization. If the output set is large enough, the reranking approach may help to alleviate the impact of the label bias problem, because the victim parses (i.e. those parses which get penalized due to the label bias problem) will have a chance to take part in the reranking. In recent years, reranking techniques have been successfully applied to the so-called history-based models (Black et al., 1993), especially to parsing (Collins, 2000; Collins and Duffy, 2002). In a history-based model, the current decision depends on the decisions made previously. Therefore, we may regard parsing as a special form of sequential model without losing generality. Collins (2000) has proposed two reranking algorithms to rerank the output of an existing parser (Collins, 1999, Model 2). One is based on Markov Random Fields, and the other is based on a boosting approach. In (Collins and Duffy, 2002), the use of Voted Perceptron (VP) (Freund and Schapire, 1999) for the parse reranking problem has been described. In that paper, the tree kernel (Collins and Duffy, 2001) has"
W03-0402,A00-2018,0,\N,Missing
W03-0402,J93-2004,0,\N,Missing
W03-0402,J03-4003,0,\N,Missing
W03-1012,P93-1005,0,0.0143757,"s, thus making these constraints strictly local. For example, in the derivation tree of Examples 1, α1 (join) and α2 (V inken) are directly connected whether there is an auxiliary tree β 2 (will) or not. We will show how this property affects our redefined tree kernel later in this paper. In our experiments in this paper, we only use LTAG grammars where each elementary tree is lexicalized by exactly one word (terminal symbol) on the frontier. 3 Parse Reranking In recent years, reranking techniques have been successfully used in statistical parsers to rerank the output of history-based models (Black et al., 1993). In this paper, we will use the LTAG based features to improve the performance of reranking. Our motivations for using LTAG based features for reranking are the following: • Unlike the generative model, it is trivial to incorporate features of various kinds in a reranking setting. Furthermore the nature of reranking makes it possible to use global features, VP α1 (join)hi α2 (Vinken)h00i β2 (will)h01i α3 (board)h011i β1 (Pierre)h0i β3 (the)h0i will β4 (as)h01i .. . α4 (director)h011i • Several hand-crafted and arbitrary features have been exploited in the statistical parsing task, especially"
W03-1012,E03-1005,0,0.00617033,"nel over sequences. However, to use all possible n-gram features typically introduces too many noisy features, which can result in lower accuracy. One way to solve this problem is to use a kernel function that is tailored for particular NLP applications, such as the tree kernel (Collins and Duffy, 2001) for statistical parsing. In addition to n-gram features, more complex high-level features are often exploited to obtain higher accuracy, especially when discriminative models are used for statistical parsing. For example, all possible sub-trees can be used as features (Collins and Duffy, 2002; Bod, 2003). However, most of the sub-trees are linguistically meaningless, and are a source of noisy features thus limiting efficiency and accuracy. An alternative to the use of arbitrary sets of sub-trees is to use the set of elementary trees as defined in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1997). LTAG based features not only allow a more limited and a linguistically more valid set of features over sub-trees, they also provide the use of features that use discontinuous sub-trees which are outside the scope of previous tree kernel definitions using arbitrary sub-trees. In this"
W03-1012,2000.iwpt-1.9,0,0.0585427,"s are excised and recorded into the derivation tree as cases of sister adjunction. Each sub-tree excised is recursively analyzed with this method, split up into elementary trees and then recorded into the derivation tree. The output of our algorithm for the input parse tree in Fig. 6 is shown in Fig. 2 and Fig. 3. Our algorithm is similar to the derivation tree extraction explained in (Chiang, 2000), except we extract our LTAG from n-best sets of parse trees, while in (Chiang, 2000) the LTAG is extracted from the Penn Treebank.3 For other techniques for LTAG grammar extraction see (Xia, 2001; Chen and Vijay-Shanker, 2000). 4.3 Using Derivation Trees In this paper, we have described two models to employ derivation trees. Model 1 uses tree kernels on derivation trees. In order to make the tree kernel more lexicalized, we extend the original definition of the tree kernel, which we will describe below. Model 2 abstracts features from derivation trees and uses them with a linear kernel. In Model 1, we combine the SVM results of the tree kernel on derivation trees with the SVM results given by a linear kernel based on features on the derived trees. 3 Also note that the path from the root node to the foot node in aux"
W03-1012,P00-1058,0,0.168692,"alized Tree Adjoining Grammar (more details can be found in (Joshi and Schabes, 1997)). In LTAG, each word is associated with a set of elementary trees. Each elementary tree represents a possible tree structure for the word. There are two kinds of elementary trees, initial trees and auxiliary trees. Elementary trees can be combined through two operations, substitution and adjunction. Substitution is used to attach an initial tree, and adjunction is used to attach an auxiliary tree. In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). 1 The tree resulting from the combination of elementary trees is is called a derived tree. The tree that records the history of how a derived tree is built from the elementary trees is called a derivation tree. 2 We illustrate the LTAG formalism using an example. Example 1: Pierre Vinken will join the board as a non-executive director. The derived tree for Example 1 is shown in Fig. 1 (we omit the POS tags associated with each word to save space), and Fig. 2 shows the elementary trees for each word in the sentence. Fig. 3 is the derivation tree (the history of tree combinations). One of 1 Ad"
W03-1012,P02-1034,0,0.611474,"e use of a polynomial kernel over sequences. However, to use all possible n-gram features typically introduces too many noisy features, which can result in lower accuracy. One way to solve this problem is to use a kernel function that is tailored for particular NLP applications, such as the tree kernel (Collins and Duffy, 2001) for statistical parsing. In addition to n-gram features, more complex high-level features are often exploited to obtain higher accuracy, especially when discriminative models are used for statistical parsing. For example, all possible sub-trees can be used as features (Collins and Duffy, 2002; Bod, 2003). However, most of the sub-trees are linguistically meaningless, and are a source of noisy features thus limiting efficiency and accuracy. An alternative to the use of arbitrary sets of sub-trees is to use the set of elementary trees as defined in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1997). LTAG based features not only allow a more limited and a linguistically more valid set of features over sub-trees, they also provide the use of features that use discontinuous sub-trees which are outside the scope of previous tree kernel definitions using arbitrary sub-tr"
W03-1012,W01-1802,0,0.0833267,"Missing"
W03-1012,W03-0402,1,0.84097,"is to use the set of elementary trees as defined in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1997). LTAG based features not only allow a more limited and a linguistically more valid set of features over sub-trees, they also provide the use of features that use discontinuous sub-trees which are outside the scope of previous tree kernel definitions using arbitrary sub-trees. In this paper, we use the LTAG based features in the parse reranking problem (Collins, 2000; Collins and Duffy, 2002). We use the Support Vector Machine (SVM) (Vapnik, 1999) based algorithm proposed in (Shen and Joshi, 2003) as the reranker in this paper. We apply the tree kernel to derivation trees of LTAG, and extract features from derivation trees. Both the tree kernel and the linear kernel on the richer feature set are used. Our experiments show that the use of tree kernel on derivation trees makes the notion of a tree kernel more powerful and more applicable. S NP VP Pierre Vinken will VP VP join PP NP as the board a non-executive director Figure 1: Derived tree (parse tree) for Example 1. β1 : NP Pierre α2 : NP Vinken α1 : S β2 : VP will VP∗ NP↓ 2 Lexicalized Tree Adjoining Grammar In this section, we give"
W03-1012,J03-4003,0,\N,Missing
W03-2608,W98-0315,1,\N,Missing
W03-2608,J03-4002,1,\N,Missing
W03-2608,P02-1011,0,\N,Missing
W04-0212,P97-1011,0,0.0334976,"Missing"
W04-0212,kingsbury-palmer-2002-treebank,0,0.0252442,"Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora. 1 Introduction Large scale annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have played a central role in speech and natural language research. However, with the demand for more powerful NLP applications comes a need for greater richness in annotation – hence, the development of PropBank (Kingsbury and Palmer, 2002), which adds basic semantics to the PTB in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexi"
W04-0212,J93-2004,0,0.0345992,"nk (PDTB) is a new resource built on top of the Penn Wall Street Journal corpus, in which discourse connectives are annotated along with their arguments. Its use of standoff annotation allows integration with a stand-off version of the Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora. 1 Introduction Large scale annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have played a central role in speech and natural language research. However, with the demand for more powerful NLP applications comes a need for greater richness in annotation – hence, the development of PropBank (Kingsbury and Palmer, 2002), which adds basic semantics to the PTB in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their ar"
W04-0212,W03-2608,1,0.888723,"or a state, and discourse deictics that denote an abstract object. What we describe to annotators as arguments to discourse connectives are actually the textual span from which the argument is derived (Webber et al., 1999a; Webber et al., 2003). This is especially clear in the case of the first argument of instead in (3), which does not actually include the negation, although it is part of the selected text.3 2 For a more detailed discussion of how discourse adverbials can be distinguished from clausal adverbials, see Forbes (2003). 3 For a corpus-based study of the arguments of instead, see (Miltsakaki et al., 2003). (3) [No price for the new shares has been set]. Instead, [the companies will leave it up to the marketplace to decide]. How far does an argument extend? One particularly significant addition to the guidelines came as a result of differences among annotators as to how large a span constituted the argument of a connective. During pilot annotations, annotators used three annotation tags: CONN for the connective and ARG1 and ARG2 for the two arguments. To this set, we have added two optional tags, SUP1 and SUP2 (supplementary), for cases when the annotator wants to mark textual spans s/he consid"
W04-0212,W04-2703,1,0.796962,"and restrict the profits businessmen could make]. As a result, [industry operated out of small, expensive, highly inefficient industrial units]. (2) Strangely, conventional wisdom inside the Beltway regards these transfer payments as “uncontrollable” or “nondiscretionary.” Implicit connectives are taken to occur between adjacent sentences not related by any explicit connective. They are annotated with whatever explicit connective the annotator feels could be inserted, with the original meaning retained. Assessment of inter-annotator agreement groups these annotations into five coarse classes (Miltsakaki et al., 2004). Currently, we are not annotating implicit connectives intra-sententially (such as between a main clause and a free adjunct) or across paragraphs. What counts as a legal argument? The simplest argument to a connective is what we take to be the minimum unit of discourse. Because we take discourse relations to hold between abstract objects, we require that an argument contain at least one clause-level predication (usually a verb – tensed or untensed), though it may span as much as a sequence of clauses or sentences. The two exceptions are nominal phrases that express an event or a state, and di"
W04-0212,J88-2003,0,0.332427,"Missing"
W04-0212,P95-1018,0,0.0380731,"inconsistencies in how the lexical items are analyzed. We believe that the PDTB annotation can contribute to a range of linguistic discovery and language modeling tasks, such as      providing empirical evidence for the DLTAG claim that discourse adverbials get one argument anaphorically, while structural connectives such as conjunctions establish relations between adjacent units of text (Creswell et al., 2002). acquiring common usage patterns of connectives and identifying their dependencies, in order to support “natural” choices in Natural Language Generation (di Eugenio et al., 1997; Moser and Moore, 1995; Williams and Reiter, 2003). developing decision procedures for resolving and interpreting discourse adverbials (Miltsakaki et al., 2003) which can be built on top of discourse parsing systems (Forbes et al., 2003). developing “word sense disambiguation” procedures for distinguishing among different senses of a connective and hence interpreting connectives correctly (e.g., distinguishing between temporal and explanatory since, between hypothetical and counterfactual if, between epistemic and semantic because, etc.) providing empirical evidence for theories of anaphoric phenomena such as verb"
W04-0212,P04-1011,1,0.743438,"Missing"
W04-0212,W98-0315,1,0.941827,"in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexicalized approach to discourse – DLTAG (Webber and Joshi, 1998; Webber et al., 1999a; Webber et al., 2000; Webber et al., 2003). Here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicateargument relations whose predicates come mainly from discourse connectives1 and whose arguments 1 Despite this, we have deliberately adopted a policy of havcome from units of discourse – clausal, sentential or multi-sentential units. The PDTB therefore differs from the RST-annotated corpus (Carlson et al., 2003) which starts with (abstract) rhetorical relations (Mann and Thompson, 1988) and annotates a subset of th"
W04-0212,P99-1006,1,0.939969,"dicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexicalized approach to discourse – DLTAG (Webber and Joshi, 1998; Webber et al., 1999a; Webber et al., 2000; Webber et al., 2003). Here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicateargument relations whose predicates come mainly from discourse connectives1 and whose arguments 1 Despite this, we have deliberately adopted a policy of havcome from units of discourse – clausal, sentential or multi-sentential units. The PDTB therefore differs from the RST-annotated corpus (Carlson et al., 2003) which starts with (abstract) rhetorical relations (Mann and Thompson, 1988) and annotates a subset of the Penn WSJ corpus wit"
W04-0212,W01-1605,0,\N,Missing
W04-0212,J03-4002,1,\N,Missing
W04-2703,J93-2004,0,0.0274317,"being built directly on top of the Penn TreeBank and Propbank, thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms. We provide a detailed preliminary analysis of inter-annotator agreement – both the level of agreement and the types of inter-annotator variation. 1 Introduction Large scale annotated corpora have played a critical role in speech and natural language research. The Penn TreeBank (PTB) is an example of such a resource with worldwide impact on natural language processing (Marcus et al., 1993). However, the PTB deals with text only at the sentence level: with the demand for more powerful NLP applications comes a need for greater richness in annotation. At the sentence level, Penn Propbank is adding predicate-argument annotation to sentences in PTB (Kingsbury and Palmer, 2002). At the discourselevel are efforts to produce corpora annotated with rhetorical relations (Carlson et al., 2003). This paper describes a more basic discourse-level annotation project – the Penn Discourse TreeBank (PDTB) – that aims to produce a large-scale corpus in which discourse connectives are annotated, a"
W04-2703,W03-2608,1,0.843833,"reement between them and by correcting for chance expected agreement. However, the statistic requires the data tokens to be classified into discrete categories, and as a result, we could not apply it to our data since the PDTB annotation tokens cannot be classified as such. Rather, annotation in the PDTB constitutes either selection of a span of text for the arguments of connectives which can be of indeterminate length or providing explicit expressions for implicit connectives from an open-ended class of expressions. 8 For a preliminary corpus-based analysis of the arguments of ‘instead’, see Miltsakaki et al. (2003). Instead, we have assessed inter-annotator agreement in terms of agreement/disagreement on span or named expression identity for each token as a percentage of the pairs of spans or expressions that actually matched versus those that should have. For the argument annotations, we use a most conservative measure - the exact match criterion. In addition, we also used different diagnostics for the argument annotations for the explicit connectives, reporting percentage agreement on different classes of tokens, such as those in which the first argument (ARG1) annotations and second argument (ARG2) a"
W04-2703,P99-1006,1,0.897467,"nyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the sentence derives from discourse relations. Specification of the discourse relations for a discourse thus constitutes a description of a certain level of discourse structure. Rather than starting from (abstract) discourse relations, we describe an approach to annotating a largescale corpus in terms of a more basic characterisation of discourse structure in terms of discourse connectives and their arguments. The motivation for such an approach stems from work by Webber and Joshi (1998), Webber et al. (1999a), Webber et al. (2000) which integrates sentence level structures with discourse level structure (using tree-adjoining grammars for both cases, LTAG and DLTAG, respectively). 1 This allows structural composition and its associated semantic composition at the sentence level to be smoothly carried over to the discourse level, a goal also shared by Gardent (1997), Schilder (1997) and Polanyi and van den Berg (1996), among others. 2 Discourse connectives and their arguments can be successfully annotated with high reliability (cf. Section 4). This is not surprising, given that the task resembles"
W04-2703,kingsbury-palmer-2002-treebank,0,0.269727,"annotator agreement – both the level of agreement and the types of inter-annotator variation. 1 Introduction Large scale annotated corpora have played a critical role in speech and natural language research. The Penn TreeBank (PTB) is an example of such a resource with worldwide impact on natural language processing (Marcus et al., 1993). However, the PTB deals with text only at the sentence level: with the demand for more powerful NLP applications comes a need for greater richness in annotation. At the sentence level, Penn Propbank is adding predicate-argument annotation to sentences in PTB (Kingsbury and Palmer, 2002). At the discourselevel are efforts to produce corpora annotated with rhetorical relations (Carlson et al., 2003). This paper describes a more basic discourse-level annotation project – the Penn Discourse TreeBank (PDTB) – that aims to produce a large-scale corpus in which discourse connectives are annotated, along with their arguments. There have been several approaches to describing discourse in terms of discourse relations (Mann and Thompson, 1988; Asher and Lascarides, 1998; Polanyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the se"
W04-2703,W98-0315,1,0.725528,"nd Lascarides, 1998; Polanyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the sentence derives from discourse relations. Specification of the discourse relations for a discourse thus constitutes a description of a certain level of discourse structure. Rather than starting from (abstract) discourse relations, we describe an approach to annotating a largescale corpus in terms of a more basic characterisation of discourse structure in terms of discourse connectives and their arguments. The motivation for such an approach stems from work by Webber and Joshi (1998), Webber et al. (1999a), Webber et al. (2000) which integrates sentence level structures with discourse level structure (using tree-adjoining grammars for both cases, LTAG and DLTAG, respectively). 1 This allows structural composition and its associated semantic composition at the sentence level to be smoothly carried over to the discourse level, a goal also shared by Gardent (1997), Schilder (1997) and Polanyi and van den Berg (1996), among others. 2 Discourse connectives and their arguments can be successfully annotated with high reliability (cf. Section 4). This is not surprising, given tha"
W04-2703,J03-4002,1,\N,Missing
W04-2704,S01-1001,0,0.0687376,"Missing"
W04-2704,W98-0315,1,0.884365,"Missing"
W04-2704,kingsbury-palmer-2002-treebank,1,0.773736,"ng or annotation. The Proposition Bank is designed as a broad-coverage resource to facilitate the development of more general systems. It focuses on the argument structure of verbs, and provides a complete corpus annotated with semantic roles, including participants traditionally viewed as arguments and adjuncts. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming a component of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. 1 PropBank (Kingsbury & Palmer, 2002) is an annotation of the Wall Street Journal portion of the Penn Treebank II (Marcus, 1994) with `predicate-argument&apos; structures, using sense tags for highly polysemous words and semantic role labels for each argument. An important goal is to provide consistent semantic role labels across different syntactic realizations of the same verb, as in the window in [ARG0 John] broke [ARG1 the window] and [ARG1 The window] broke. PropBank can provide frequency counts for (statistical) analysis or generation components in a machine translation system, but provides only a shallow semantic analysis in th"
W04-2704,miltsakaki-etal-2004-penn,1,0.886331,"Missing"
W04-2704,W04-2807,1,\N,Missing
W04-2704,J00-4005,0,\N,Missing
W04-2704,W01-1605,0,\N,Missing
W04-2704,W03-1707,1,\N,Missing
W04-2704,kipper-etal-2004-extending,1,\N,Missing
W05-0305,P97-1003,0,0.0334303,"Missing"
W05-0305,J00-3005,0,0.0156263,"t. Consider example (12), where the PTB requires annotators to include the verb of attribution said and its subject Delmed in the complement of although. But although as a discourse connective denies the expectation that the supply of dialysis products will be discontinued when the distribution arrangement ends. It does not convey the expectation that Delmed will not say such things. On the other hand, in (13), the contrast established by while is between the opinions of two entities i.e., advocates and their opponents.4 4 This distinction is hard to capture in an RST-based parsing framework (Marcu, 2000). According to the RST-based annotation scheme (Carlson et al., 2003) ‘although Delmed said’ and ‘while opponents argued’ are elementary discourse units 32 (12) The current distribution arrangement ends in March 1990, although Delmed said it will continue to provide some supplies of the peritoneal dialysis products to National Medical, the spokeswoman said. (13) Advocates said the 90-cent-an-hour rise, to $4.25 an hour by April 1991, is too small for the working poor, while opponents argued that the increase will still hurt small business and cost many thousands of jobs. In Section 5, we will"
W05-0305,W04-2703,1,0.741875,"iscourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has expressed the discourse relation itself. These ascriptions need not be the same. Of particular interest is the fact that attribution may or may not play a role in the relation established by a connective. This may lead to a lack of congruence between arguments at the syntactic"
W05-0305,miltsakaki-etal-2004-penn,1,0.916062,"iscourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has expressed the discourse relation itself. These ascriptions need not be the same. Of particular interest is the fact that attribution may or may not play a role in the relation established by a connective. This may lead to a lack of congruence between arguments at the syntactic"
W05-0305,W04-0212,1,0.860298,"the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary) set of discourse relations whose identification may confound multiple sources of discourse meaning, we start with the annotation of discourse connectives and their arguments, thus exposing a clearly defined level of discourse representation. The PDTB annotates as explicit discourse connectives all subordinating conjunctions, coordinating conjunctions and discourse adverbials. These predicates establish relations between two abstract objects s"
W05-0305,W03-1014,0,0.0063381,"re looking to raise in the year ending March 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3 The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/ pdtb.  arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case"
W05-0305,W98-0315,1,0.823932,"the attribution of the arguments of a connective and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an"
W05-0305,P99-1006,1,0.857391,"rguments of a connective and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitr"
W05-0305,J04-3002,0,0.0189811,"he year ending March 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3 The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/ pdtb.  arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case 1: The relation and"
W05-0305,J93-2004,0,\N,Missing
W05-0305,J03-4002,1,\N,Missing
W06-0305,J93-2004,0,\N,Missing
W06-0305,W04-2703,1,\N,Missing
W06-0305,W05-0308,0,\N,Missing
W06-0305,W04-0212,1,\N,Missing
W06-0305,H05-1116,0,\N,Missing
W06-0305,W05-0305,1,\N,Missing
W06-0305,W03-1017,0,\N,Missing
W06-0305,J03-4002,1,\N,Missing
W06-0305,J04-3002,0,\N,Missing
W06-0305,P02-1053,0,\N,Missing
W06-0305,W02-1011,0,\N,Missing
W06-1204,P89-1010,0,0.426577,"interpretations that cross word boundaries (Sag et al., 2002). A large number of MWEs have standard syntactic structure but are semantically noncompositional. Here, we consider the class of verb based expressions (verb is the head of the phrase), which occur very frequently. This class of verb based multi-word expressions include verbal idioms, support-verb constructions, among others. The example ‘take place’ is a MWE but ‘take a gift’ is not. In the past, various measures have been suggested for measuring the compositionality of multi-word expressions. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al., 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al., 2003). Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet. In this paper, we explore this possibility of using the information about compositionality of MWEs (verb based) for the word alignment task. In this preliminary work, we use simple measures (such as point-wise mutual information) to measure compositionality. The paper is organized as follows. In section 2, we discuss"
W06-1204,E06-1043,0,0.0669961,"0.45 0.50 0.5045 0.52 0.40 0.45 0.5518 0.53 0.30 0.38 0.6185 0.82 0.23 0.36 0.6446 Table 6: Results using the compositionality based features Table 2: Results of GIZA++ - lemmatized set 7.3 pressions of various types. Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al., 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verbobject pair using the LSA model (Baldwin et al., 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific application yet. In this paper, we have suggested a framework for using the compositionality of multi-word expressions for the word alignment task. State-of-art systems for doing word ali"
W06-1204,H05-1066,0,0.0263923,"s away/RP He/N a compositionality based feature. The part of speech tag of a dependent is essential to determine the likelihood of the dependent to align with the same word in the target language sentence as the word to which its verb is aligned. vaha bhaaga gayaa Figure 6: Example of MergeMI feature 6 Online large margin training This binary feature is active when the alignment links of a dependent and its verb merge. For example, in Figure 5., the feature ‘merge RP’ will be active (that is, merge RP = 1). For parameter optimization, we have used an online large margin algorithm called MIRA (McDonald et al., 2005) (Crammer and Singer, 2003). We describe the training algorithm that we used very briefly. Our training set is a set of EnglishHindi word aligned parallel corpus. We get the verb based expressions in English by running a dependency parser (Shen, 2006). Let the number of sentence pairs in the training data be m. We have  MergeMI: This is a compositionality based feature which associates point-wise mutual information (apart from the POS information) with the cases where the dependents which have the same alignment in the target 24 fS ; T ; a^ g for training where q &lt;= m is the index number of t"
W06-1204,H05-1011,0,0.0253155,"judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific application yet. In this paper, we have suggested a framework for using the compositionality of multi-word expressions for the word alignment task. State-of-art systems for doing word alignment use generative models like GIZA++ (Och and Ney, 2003; Brown et al., 1993). Discriminative models have been tried recently for word-alignment (Taskar et al., 2005; Moore, 2005) as these models give the ability to harness variety of complex features which cannot be provided in the generative models. In our work, we have used the compositionality of multi-word expressions to predict how they align with the words in the target language sentence. For parameter optimization for the wordalignment task, Taskar, Simon and Klein (Taskar et al., 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. We cannot do such a factorization because the scores of alignment links in our case are not computed in"
W06-1204,J03-1002,0,0.0222093,"redictions in by a margin which is equal to the number of mistakes in the predictions when compared to gold alignment. While computing the number of mistakes, the mistakes due to the mis-alignment of head verb could be given greater weight, thus prompting the optimization algorithm to give greater importance to verb related mistakes and thereby improving overall performance. Step 4 in the algorithm mentioned above can be substituted by the following optimization problem, 7.2 Experiments with Giza We evaluated our discriminative approach by comparing it with the state-of-art Giza++ alignments (Och and Ney, 2003). The metric that we have used to do the comparison is the Alignment Error Rate (AER). The results shown below also contain Precision, Recall and F-measure. Giza was trained using an English-Hindi aligned corpus of 50000 sentence pairs. In Table 1., we report the results of the GIZA++ alignments run from both the directions (English to Hindi and Hindi to English). We also show the results of the intersected model. See Table 1. for the results of the GIZA++ alignments. minimize k(W i+1 W i )k s.t. 8k , s ore(^ aq ; Sq ; Tq ) s ore(aq;k ; Sq ; Tq ) >= Mistakes(ak ; a^q ; Sq ; Tq ) ! Hin ! Eng Pr"
W06-1204,P98-2210,0,0.205683,"et al., 2002). A large number of MWEs have standard syntactic structure but are semantically noncompositional. Here, we consider the class of verb based expressions (verb is the head of the phrase), which occur very frequently. This class of verb based multi-word expressions include verbal idioms, support-verb constructions, among others. The example ‘take place’ is a MWE but ‘take a gift’ is not. In the past, various measures have been suggested for measuring the compositionality of multi-word expressions. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al., 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al., 2003). Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet. In this paper, we explore this possibility of using the information about compositionality of MWEs (verb based) for the word alignment task. In this preliminary work, we use simple measures (such as point-wise mutual information) to measure compositionality. The paper is organized as follows. In section 2, we discuss the word-alignment task with respect to the class"
W06-1204,W03-1812,0,0.377671,"cture but are semantically noncompositional. Here, we consider the class of verb based expressions (verb is the head of the phrase), which occur very frequently. This class of verb based multi-word expressions include verbal idioms, support-verb constructions, among others. The example ‘take place’ is a MWE but ‘take a gift’ is not. In the past, various measures have been suggested for measuring the compositionality of multi-word expressions. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al., 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al., 2003). Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet. In this paper, we explore this possibility of using the information about compositionality of MWEs (verb based) for the word alignment task. In this preliminary work, we use simple measures (such as point-wise mutual information) to measure compositionality. The paper is organized as follows. In section 2, we discuss the word-alignment task with respect to the class of multi-word expressions of interest in this paper. In section"
W06-1204,H05-1113,1,0.842906,"za++ prob. Prec. Recall F-meas. AER 0.54 0.44 0.49 0.5155 Table 5: Results using the Giza++ probabilities Prec. Recall F-meas. AER AER + MergePos 0.54 0.45 0.49 0.5101 + MergeMI 0.55 0.45 0.50 0.5045 0.52 0.40 0.45 0.5518 0.53 0.30 0.38 0.6185 0.82 0.23 0.36 0.6446 Table 6: Results using the compositionality based features Table 2: Results of GIZA++ - lemmatized set 7.3 pressions of various types. Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al., 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verbobject pair using the LSA model (Baldwin et al., 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific applica"
W06-1204,J90-1003,0,\N,Missing
W06-1204,J93-2003,0,\N,Missing
W06-1204,C98-2205,0,\N,Missing
W06-1204,H05-1010,0,\N,Missing
W06-1503,P03-1024,1,0.901833,"Missing"
W06-1503,W05-1522,0,0.110278,"Missing"
W06-1503,W06-1520,0,\N,Missing
W06-1503,C96-1034,0,\N,Missing
W06-1503,P98-1033,0,\N,Missing
W06-1503,C98-1033,0,\N,Missing
W06-1635,J98-2004,0,\N,Missing
W06-1635,N03-1016,0,\N,Missing
W06-3601,N03-1017,0,0.00595915,"Missing"
W06-3601,J00-1004,0,0.0735197,"Missing"
W06-3601,koen-2004-pharaoh,0,0.0407508,"Missing"
W06-3601,J93-2003,0,0.0121903,"Missing"
W06-3601,A00-2018,0,0.0653872,"Missing"
W06-3601,P05-1033,0,0.0367601,"Missing"
W06-3601,N03-1019,0,0.0196045,"Missing"
W06-3601,C04-1090,0,0.0934174,"Missing"
W06-3601,N06-1045,1,0.849681,"Missing"
W06-3601,P05-1067,0,0.185749,"Missing"
W06-3601,P02-1038,0,0.0734509,"Missing"
W06-3601,P03-2041,0,0.0809973,"Missing"
W06-3601,J04-4002,0,0.0561881,"Missing"
W06-3601,W02-1039,0,0.136926,"Missing"
W06-3601,N04-1035,1,0.402231,"Missing"
W06-3601,P03-1021,0,0.00859701,"Missing"
W06-3601,P05-1034,0,0.293239,"Missing"
W06-3601,C90-3045,0,0.289084,"Missing"
W06-3601,N04-1014,1,0.880617,"Missing"
W06-3601,W05-1506,1,0.399291,"Missing"
W06-3601,J97-3002,0,0.0826936,"Missing"
W06-3601,P01-1067,1,0.837173,"Missing"
W06-3601,J03-4003,0,\N,Missing
W06-3601,J08-3004,1,\N,Missing
W06-3601,W90-0102,0,\N,Missing
W06-3902,P99-1058,0,0.0751223,"ine-assisted. While the design of more expressive logics makes the composition of specifications easier, using them for model checking needs the creation of more expressive models (which requires more effort). As a result, there is a trade-off between amount of effort spent in obtaining models, and that in obtaining the specifications. Our decision to work with less expressive models is motivated by the extensive tool support available for creating and extracting such models [5,11]. Further, subsets of NL for which automatic translation is guaranteed, such as the one derived by Holt and Klein [10], assume (among other things) that references are resolved and hence cannot be directly applied to regulatory documents. We are thus left with the choice of making the procedure machine-assisted. There have been two kinds of machine-assisted approaches to extracting temporal logic specifications: (a) composing the semantics in a general semantic framework which is then mapped to temporal logic [7], and (b) attempting to compose the semantics in the temporal logic directly [6]. In the latter approach, a human specifies denotations for a portion of the sentence, and the rest of the composition h"
W07-0407,P06-1009,0,0.0150346,"then by using structural features to re-rank the list. Also, by using all the k-best alignments for updating the parameters through MIRA, it is possible to model the entire inference algorithm but in Moore’s work, only the best alignment is used to update the weights of parameters. (Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. This model still relies on the generative story and achieves only a limited freedom in choosing the features. (Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. Even though their approach allows one to include overlapping features while training a discriminative model, it still does not allow us to use features that capture information of the entire alignment structure. In Section 2, we describe the alignment search in detail. Section 3 describes the features that we have considered in our paper. Section 4 talks about the Parameter optimization. In Section 5, we present the results of our experiments. Section 6 contains the conclusion and our proposed future work. 2 Alignment Se"
W07-0407,P06-1097,0,0.0263546,"ional-Link Probability) based model. LLR and CLP are the word association statistics used in Moore’s work (Moore, 2005). In contrast to the above approach, our search technique is more 50 general. It achieves this by breaking the search into two steps, first by using local features to get the k-best alignments and then by using structural features to re-rank the list. Also, by using all the k-best alignments for updating the parameters through MIRA, it is possible to model the entire inference algorithm but in Moore’s work, only the best alignment is used to update the weights of parameters. (Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. This model still relies on the generative story and achieves only a limited freedom in choosing the features. (Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. Even though their approach allows one to include overlapping features while training a discriminative model, it still does not allow us to use features that capture information of the entire alignment structure"
W07-0407,N06-1015,0,0.0182889,"a score (score(ep ,hq )) reflecting the desirability of the existence of the link. The matching problem is solved by formulating it as a linear programming problem. The parameter estimation is done within the framework of large margin estimation by reducing the problem to a quadratic program (QP). The main limitation of this work is that the features considered are local to the alignment links joining pairs of words. The score of an alignment is the sum of scores of individual alignment links measured independently i.e., it is assumed that there is no dependence between the alignment links. (Lacoste-Julien et al., 2006) extend the above approach to include features for fertility and first-order correlation between alignment links of consecutive words in the source sentence. They solve this by formulating the problem as a quadratic assignment problem (QAP). But, even this algorithm cannot include more general features over the entire alignment. In contrast to the above two approaches, our approach does not impose any constraints on the feature space except for fertility (≤1) of words in the source language. In our approach, we model the one-to-one and many-to-one links between the source sentence and target s"
W07-0407,H05-1066,0,0.157039,"ish and a Hindi sentence 1 Introduction In this paper, we propose a discriminative reranking approach for word alignment which allows us to make use of structural features effectively. The alignment algorithm first generates 1 1Part of the work was done at Institute for Research in Cognitive Science (IRCS), University of Pennsylvania, Philadelphia, PA 19104, USA, when he was visiting IRCS as a Visiting Scholar, February to December, 2006. To learn the weights associated with the parameters used in our model, we have used a learning framework called MIRA (The Margin Infused Relaxed Algorithm) (McDonald et al., 2005; Crammer and Singer, 2003). This is an online learning algorithm which looks at one sentence pair at a time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative (Och and Ney, 2003; Vogel et al., 1996). In the past couple of years, the discriminative models for doing word alignment have gained popularity because of 49 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–"
W07-0407,H05-1011,0,0.0970045,"s over the entire alignment. In contrast to the above two approaches, our approach does not impose any constraints on the feature space except for fertility (≤1) of words in the source language. In our approach, we model the one-to-one and many-to-one links between the source sentence and target sentence. The many-to-many alignment links are inferred in the post-processing stage using simple generic rules. Another positive aspect of our approach is the application of MIRA. It, being an online approach, converges fast and still retains the generalizing capability of the large margin approach. (Moore, 2005) has proposed an approach which does not impose any restrictions on the form of model features. But, the search technique has certain heuristic procedures dependent on the types of features used. For example, there is little variation in the alignment search between the LLR (Log-likelihood ratio) based model and the CLP (Conditional-Link Probability) based model. LLR and CLP are the word association statistics used in Moore’s work (Moore, 2005). In contrast to the above approach, our search technique is more 50 general. It achieves this by breaking the search into two steps, first by using loc"
W07-0407,J03-1002,0,0.0199535,"PA 19104, USA, when he was visiting IRCS as a Visiting Scholar, February to December, 2006. To learn the weights associated with the parameters used in our model, we have used a learning framework called MIRA (The Margin Infused Relaxed Algorithm) (McDonald et al., 2005; Crammer and Singer, 2003). This is an online learning algorithm which looks at one sentence pair at a time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative (Och and Ney, 2003; Vogel et al., 1996). In the past couple of years, the discriminative models for doing word alignment have gained popularity because of 49 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–56, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics the flexibility they offer in using a large variety of features and in combining information from various sources. (Taskar et al., 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two"
W07-0407,C96-2141,0,0.0684069,"he was visiting IRCS as a Visiting Scholar, February to December, 2006. To learn the weights associated with the parameters used in our model, we have used a learning framework called MIRA (The Margin Infused Relaxed Algorithm) (McDonald et al., 2005; Crammer and Singer, 2003). This is an online learning algorithm which looks at one sentence pair at a time and compares the k-best predictions of the alignment algorithm with the gold alignment to update the parameter weights appropriately. In the past, popular approaches for doing word alignment have largely been generative (Och and Ney, 2003; Vogel et al., 1996). In the past couple of years, the discriminative models for doing word alignment have gained popularity because of 49 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49–56, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics the flexibility they offer in using a large variety of features and in combining information from various sources. (Taskar et al., 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences. The link"
W07-0407,H05-1010,0,\N,Missing
W07-1201,W06-1517,0,0.507338,"subj-aux inversion in combination with raising verbs (Frank 1992), anaphoric binding (Ryant and Scheffler 2006), quantifier scope ambiguity (Joshi et al. 2003), clitic climbing in Romance (Bleam 1994), and Japanese causatives (Heycock 1986). The primary concern of this paper is the reconciliation of the observation noted above, that MCTAG appears to be on the right track for a good generative characterization of natural language, with a second observation: The graph drawings that correspond to MC-TAG derivations, are not guaranteed to retain the properties of basic-TAG induced graph drawings. Kuhlmann and Möhl (2006) report that if an entire MC set is anchored by a single lexical element (the natural extension of “lexicalization” of TAGs to MC-TAGs), then the class of dependency structures is expanded with respect to both conditions that characterized the TAG-induced graph drawings: MC-TAG induced graph drawings include structures that are not well-nested, have gap degree > 1, or both. As Kuhlmann and Möhl point out, the gap degree increases with the number of components, which we will elaborate in section 6. This is true even if we require that all components of a set combine with a single elementary tre"
W07-1201,P07-1021,0,0.0218347,"saw) de kinderen (the children) zwemmen zag de kinderen Jan (4) Jan de kinderen zag zwemmen Figure 1. Derivation for Jan de kinderen zag zwemmen and corresponding graph drawing 3 3.1 Properties of Dependency Graphs Gap-Degree It will be useful to first define the term projection. Definition: The projection of a node x is the set of nodes dominated by x (including x). (E.g. in (4), the projection of zag = {Jan, zag}.) t1, in a TAG-induced dependency graph, adjoining t2 to t1 corresponds to the reverse dependency. 7 This result refers to single graph drawings and particular LTAG derivation. See Kuhlmann and Möhl (2007) on the relationship between sets of graph drawings and LTAGs. Recall that the nodes of a graph drawing are in a precedence relation, and that this precedence relation is total. Definition: A gap is a discontinuity with respect to precedence in the projection of a node in the drawing. (E.g. in (4), de kinderen is the gap preventing Jan and zag from forming a contiguous interval.) Definition: The gap degree of a node is the number of gaps in its projection. (E.g. the gap degree of node zag = 1.) Definition: The gap degree of a drawing is the maximum among the gap degrees of its nodes. (E.g. in"
W07-1201,P06-2066,0,0.431128,"er from standard dependency structures), that are equivalent to lexicalized Tree Adjoining Grammar (LTAG) derivations (Joshi and Schabes 1997). Whereas TAG is a generative framework in which each well-formed expression corresponds with a legitimate derivation in that system, the graph drawing approach provides a set of structures and a set of constraints on wellformedness. Bodirsky et al. offer the class of graph drawings that satisfy these constraints as a model-based perspective on TAG. Section 2 summarizes this relationship between TAG derivations and these graph drawings. In related work, Kuhlmann and Nivre (2006) evaluate a number of constraints that have been proposed to restrict the class of dependency structures characterizing natural language with respect to two dependency treebanks: the Prague Dependency Treebank (PDT) (Hajič et al., 2001) and the Danish Dependency Treebank (DDT) (Kromann, 2003). The results indicate that two properties provide good coverage of the structures in both 1 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 1–8, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics treebanks. 3 The first is a binary well-nestedness"
W07-1201,W06-1509,0,0.113605,"Missing"
W08-0614,J93-2004,0,\N,Missing
W08-0614,W04-2703,1,\N,Missing
W08-0614,prasad-etal-2008-penn,1,\N,Missing
W08-2302,E91-1005,1,0.547632,"djoined structures may be on top (i.e. one derivation tree may correspond to more than one phrase structure), the order of the elementary trees in the final derived tree is determined by the order of adjoining: if tree A adjoins into a node X before tree B adjoins into the same node X, then tree A will be below tree B in the derived tree. 6 Additionally, we require that trees that target the same node belong to different MC-sets. 4 Non-binarized Phrase Structure The grammar we first explore is shown in Figure 2. These tree sets are based on the tree-sets for a verb with two arguments given in Becker et al, (1991) which have been assumed for subsequent TAG approaches to German scrambling. A point of departure, however, is that these trees have more than one VP node. While we assume that the VP nodes belonging to the noun components do not carry the indexing information for the verb it is associated with, we do assume that both the root VP node and internal VP nodes, if any, of a predicative elementary tree carry the indexing information associated with the verb. This means that there is an additional potential “host” node for adjoining, and hence, each scrambled sequence may have more than one structur"
W08-2302,J05-2003,0,0.533906,"e. In contrast, we also assume that the noun components in Grammar 1 do not have host nodes for predicates. This has the effect of banning adjoining into the noun components in general: an NPi component cannot combine into an NPj component without leaving the predicate Vi tiple noun components, or b) any number of noun components and one verb component. Since we have a different notion of modifier and predicate, we diverge from Schabes and Shieber (1994) by assuming predicative trees appear below modifier trees. 6 Multiple-adjoining is related to tree-local MCTAG with shared nodes (SN-MCTAG) (Kallmeyer, 2005) in that a node which hosts adjoining is not seen as having disappeared in the tree-rewriting process. Rather, the host node and the root node of the tree being adjoined are identified, and the node is considered to belong to both trees. Thus, the targeted node is still available as a host for additional adjoining. SN-MCTAG also considers the foot node to have identified with the host node and to be available as a host for additional adjoining, unlike Schabes and Shieber (1994). Proceedings of The Ninth International Workshop on Tree Adjoining Grammars and Related Formalisms Tübingen, Germany."
W08-2302,W04-3305,0,0.0209743,"ng MC-TAG with this perspective of adjoining, some derivational steps which appear to permit components from the same MC-set to combine into different trees can be recast as abiding by tree-locality. Tree-local MC-TAGs with flexible composition have been investigated from the point of view of understanding the range of structures they can generate. Some of the phenomena where flexible composition has been useful include scope ambiguity and available readings in nested quantifications (Joshi et al. 2003, Kallmeyer and Joshi 2003), complex noun phrases in pied-piping and stranding of whphrases (Kallmeyer and Scheffler 2004), and binding (Ryant and Scheffler 2006). The full range of flexibility that can be allowed without going outside the weak generative capacity of standard LTAG is not known yet. In this paper, the flexible composition we explore is limited to reverse adjoining at the root. Our investigation also includes a look at the effects of enforcing binary branching. The TAG composition operations, substitution and adjoining are binary, in the sense that each operation involves composing two trees into one, two structures into one. However, there is another dimension for this issue of binarization in TAG"
W08-2302,W06-1509,0,0.0261776,", some derivational steps which appear to permit components from the same MC-set to combine into different trees can be recast as abiding by tree-locality. Tree-local MC-TAGs with flexible composition have been investigated from the point of view of understanding the range of structures they can generate. Some of the phenomena where flexible composition has been useful include scope ambiguity and available readings in nested quantifications (Joshi et al. 2003, Kallmeyer and Joshi 2003), complex noun phrases in pied-piping and stranding of whphrases (Kallmeyer and Scheffler 2004), and binding (Ryant and Scheffler 2006). The full range of flexibility that can be allowed without going outside the weak generative capacity of standard LTAG is not known yet. In this paper, the flexible composition we explore is limited to reverse adjoining at the root. Our investigation also includes a look at the effects of enforcing binary branching. The TAG composition operations, substitution and adjoining are binary, in the sense that each operation involves composing two trees into one, two structures into one. However, there is another dimension for this issue of binarization in TAG which does not arise in other systems,"
W08-2302,J94-1004,0,\N,Missing
W08-2302,W08-2303,0,\N,Missing
W09-3029,I08-2099,1,0.828553,"in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse connectives towards the creation of HDRB is presented in Prasad et al. (2008). present our characterization of discourse connectives and their arguments in Hindi (Section 2), our proposals for modifying the sense classification scheme (Section 3), and present some crosslinguistics comparisons based on annotations done so far (Section 4). Section 5 concludes with a summary and future work. 2 Discourse"
W09-3029,mladova-etal-2008-sentence,0,0.309198,"carried out so far. 1 Introduction To enable NLP research and applications beyond the sentence-level, corpora annotated with discourse level information have been developed. The recently developed Penn Discourse Treebank (PDTB) (Prasad et al., 2008), for example, provides annotations of discourse relations (e.g., causal, contrastive, temporal, and elaboration relations) in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse connectives towards the creation"
W09-3029,W05-0312,0,0.242009,"nitial annotations carried out so far. 1 Introduction To enable NLP research and applications beyond the sentence-level, corpora annotated with discourse level information have been developed. The recently developed Penn Discourse Treebank (PDTB) (Prasad et al., 2008), for example, provides annotations of discourse relations (e.g., causal, contrastive, temporal, and elaboration relations) in the Penn Treebank Corpus. Recent interest in cross-linguistic studies of discourse relations has led to the initiation of similar discourse annotation projects in other languages as well, such as Chinese (Xue, 2005), Czech (Mladová et al., 2008), and Turkish (Deniz and Webber, 2008). In this paper, we describe our ongoing work on the creation of a Hindi Discourse Relation Bank (HDRB), broadly following the approach of the PDTB.1 The size of the HDRB corpus is 200K words and it is drawn from a 400K word corpus on which Hindi syntactic dependency annotation is being independently conducted (Begum et al., 2008). Source corpus texts are taken from the Hindi newspaper Amar Ujala, and comprise news articles from several domains, such as politics, sports, films, etc. We 1 An earlier study of Hindi discourse con"
W09-3029,I08-7009,0,0.484908,"Missing"
W09-3029,prasad-etal-2008-penn,1,\N,Missing
W09-3029,I08-7010,1,\N,Missing
W10-3809,P02-1040,0,0.0815744,"fficient based local and contextual features have a positive impact on the selection of correct transformations. A feature that uses a syntactic language model to compute the perplexity per word has a negative weight of -1.115. Table 6 presents the top-5 entries of contextual features that describe the translation of source argument ’nsubj’ using contextual information (‘tense’ of its parent). roleTenseVib:nsubj+NULL roleTenseVib:nsubj+has VBN roleTenseVib:nsubj+VBD 1. Poor Quality of the dataset Weight NULL ne Decoding We computed the translation accuracies using two metrics, (1) BLEU score (Papineni et al., 2002), and (2) Lexical Accuracy (or F-Score) on a test set of 30 sentences. We compared the accuracy of the experimental system (Vaanee) presented in this paper, with Moses (state-of-the-art translation system) and Shakti (rule-based translation system 7 ) under similar conditions (with using a development set to tune the models). The rule-based system considered is a general domain system tuned to the tourism domain. The best BLEU score for Moses on the test set is 0.118, and the best lexical accuracy is 0.512. The best BLEU score for Shakti is 0.054, and the best lexical accuracy is 0.369. In com"
W10-3809,N09-3004,1,0.834855,"such as prep Tense where, the case-marker in the target is linked to the tense of parent verb. 4 Decoding The goal is to compute the most probable target sentence given a source sentence. First, the source sentence is analyzed using a morphological analyzer5 , local word grouper (see section 2) and a dependency parser. Given the source structure, the task of the decoding algorithm is to choose the transformation that has the maximum score. Apart from the above feature functions, we can also have features that compute the score of a particular order of children using syntactic language models (Gali and Venkatapathy, 2009; Guo et al., 2008). Different features can be defined that use different levels of information pertaining to the atomic treelet and its children. 5 69 http://www.cis.upenn.edu/∼xtag/ lead to the target gold prediction for the source node. In such cases where the gold transformation is unreachable, the weights are not updated at all for the source node as it might cause erroneous weight updates. We conducted our experiments by considering both the cases, (1) Identifying source nodes with unreachable transformations, and (2) Updating weights for all the source nodes (till a maximum iteration li"
W10-3809,P05-1034,0,0.0300018,"propose a dependency based statistical system that uses discriminative techniques to train its parameters. We conducted experiments on an EnglishHindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper. 1 Some of the limitations with the syntax based approaches such as (Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005) are, (1) They do not offer flexibility for adding linguistically motivated features, and (2) It is not possible to use morphological factors in the syntax based approaches. In a recent work (Shen et al., 2009), linguistic and contextual information was effectively used in the framework of a hierarchical machine translation system. In their work, four linguistic and contextual features are used for accurate selection of translation rules. In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of various elements in the t"
W10-3809,C08-1038,0,0.03379,"Missing"
W10-3809,P09-1090,0,0.334974,"Missing"
W10-3809,D09-1008,0,0.018208,"the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper. 1 Some of the limitations with the syntax based approaches such as (Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005) are, (1) They do not offer flexibility for adding linguistically motivated features, and (2) It is not possible to use morphological factors in the syntax based approaches. In a recent work (Shen et al., 2009), linguistic and contextual information was effectively used in the framework of a hierarchical machine translation system. In their work, four linguistic and contextual features are used for accurate selection of translation rules. In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of various elements in the target during the translation process. This features use syntactic labels and collocation statistics in order to allow effective training of the model. Introduction Syntax based approaches for Machine Translation (MT) have gai"
W10-3809,P03-1054,0,0.00325462,"training the translation model, automatically obtained word-aligned parallel corpus is used. We used GIZA++ (Och and Ney, 2003) along with the growing heuristics to word-align the training corpus. The basic factors of the word used in our experiments are root, part-of-speech, gender, number and person. In Hindi, common nouns and verbs have gender information whereas, English doesn’t contain that information. Apart from the basic factors, we also consider the role information provided by labelled dependency parsers. For computing the dependency tree on the source side, We used stanford parser (Klein and Manning, 2003) in the experiments presented in this chapter3 . • Word-order variation including longdistance reordering which is prevalent between language pairs such as EnglishHindi and English-Japanese. • Generation of word-forms in the target language by predicting the word and its factors. During prediction, the inter-dependence of factors of the target word form with the factors of syntactically related words is considered. To accomplish this goal, we visualize the problem of MT as transformation from a morphologically analyzed source syntactic structure to a target syntactic structure1 (See Figure 1)."
W10-3809,J97-3002,0,0.00843261,"al machine translation system. In their work, four linguistic and contextual features are used for accurate selection of translation rules. In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of various elements in the target during the translation process. This features use syntactic labels and collocation statistics in order to allow effective training of the model. Introduction Syntax based approaches for Machine Translation (MT) have gained popularity in recent times because of their ability to handle long distance reorderings (Wu, 1997; Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005), especially for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., Some of the other approaches related to our model are the Direct Translation Model 2 (DTM2) (I"
W10-3809,D07-1091,0,0.0111879,"or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., Some of the other approaches related to our model are the Direct Translation Model 2 (DTM2) (Ittycheriah and Roukos, 2007), End-to-End Discriminative Approach to MT (Liang et al., 2006) and Factored Translation Models (Koehn and Hoang, 2007). In DTM2, a discriminative trans1 This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 66 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66–74, COLING 2010, Beijing, August 2010. lation model is defined in the setting of a phrase based translation system. In their approach, the features are optimized globally. In contrast to their approach, we define a discriminative model for translation in the setting of a syntax based machine translation system. This allows us to use both the power of a syntax based appr"
W10-3809,P02-1039,0,0.0329393,"ively. In this paper, we propose a dependency based statistical system that uses discriminative techniques to train its parameters. We conducted experiments on an EnglishHindi parallel corpora. The use of syntax (dependency tree) allows us to address the large word-reorderings between English and Hindi. And, discriminative training allows us to use rich feature sets, including linguistic features that are useful in the machine translation task. We present results of the experimental implementation of the system in this paper. 1 Some of the limitations with the syntax based approaches such as (Yamada and Knight, 2002; Quirk et al., 2005; Chiang, 2005) are, (1) They do not offer flexibility for adding linguistically motivated features, and (2) It is not possible to use morphological factors in the syntax based approaches. In a recent work (Shen et al., 2009), linguistic and contextual information was effectively used in the framework of a hierarchical machine translation system. In their work, four linguistic and contextual features are used for accurate selection of translation rules. In our approach in contrast, linguistically motivated features can be defined that directly effect the prediction of vario"
W10-3809,N03-1017,0,0.00602908,"uate this experimental system, a restricted set of experiments are conducted. The experiments are conducted on the English-Hindi language pair using a corpus in tourism domain containing 11300 sentence pairs6 . i where, τj is the local transformation of the source treelet r. The best transformation τˆ of source sentence s is, τˆ = argmaxτ score(τ |s) 5 Experiments and Results (3) Training Algorithm The goal of the training algorithm is to learn the feature weights from the word aligned corpus. For word-alignment, we used the IBM Model 5 implemented in GIZA++ along with the growing heuristics (Koehn et al., 2003). The gold atomic treelets in the source and their transformation is obtained by mapping the source node to the target using the word-alignment information. This information is stored in the form of transformation tables that is used for the prediction of target atomic treelets, prepositions and other factors. The transformation tables are pruned in order to limit the search and eliminate redundant information. For each source element, only the top few entries are retained in the table. This limit ranges from 3 to 20. We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006;"
W10-3809,P06-1096,0,0.0240242,"for divergent language pairs such as English-Hindi (or English-Urdu). Languages such as Hindi are also known for their rich morphology and long distance agreement of features of syntactically related units. The morphological richness can be handled by employing techniques that factor the lexical items into morphological factors. This strategy is also useful in the context of EnglishHindi MT (Bharati et al., 1997; Bharati et al., Some of the other approaches related to our model are the Direct Translation Model 2 (DTM2) (Ittycheriah and Roukos, 2007), End-to-End Discriminative Approach to MT (Liang et al., 2006) and Factored Translation Models (Koehn and Hoang, 2007). In DTM2, a discriminative trans1 This work was done at LTRC, IIIT-Hyderabad, when he was a masters student, till July 2008 66 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66–74, COLING 2010, Beijing, August 2010. lation model is defined in the setting of a phrase based translation system. In their approach, the features are optimized globally. In contrast to their approach, we define a discriminative model for translation in the setting of a syntax based machine translation system. Thi"
W10-3809,E06-1011,0,0.0418376,"ristics (Koehn et al., 2003). The gold atomic treelets in the source and their transformation is obtained by mapping the source node to the target using the word-alignment information. This information is stored in the form of transformation tables that is used for the prediction of target atomic treelets, prepositions and other factors. The transformation tables are pruned in order to limit the search and eliminate redundant information. For each source element, only the top few entries are retained in the table. This limit ranges from 3 to 20. We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006; Crammer et al., 2005), for updating the weights. During parameter optimization, it is sometimes impossible to achieve the gold transformation for a node because the pruned transformation tables may not 6.1 Training 6.1.1 Configuration For training, we used DIT-TOURISM-ALIGNTRAIN dataset which is the word-aligned dataset of 11300 sentence pairs. The word-alignment is done using GIZA++ (Och and Ney, 2003) toolkit and then growing heuristics are applied. For our experiments, we use two growing heuristics, GROW-DIAG-FINAL-AND and GROW-DIAGFINAL as they cover most number of words in both the side"
W10-3809,J03-1002,0,0.0115641,"eously built. We used a bottom-up traversal while decoding because it builds a contiguous sequence of nodes for the subtrees during traversal enabling the application of a wide variety of language models. In the training phase, the task is to learn the weights of features. We use an online largemargin training algorithm, MIRA (Crammer et al., 2005), for learning the weights. The weights are locally updated at every source node during the bottom-up traversal of the source structure. For training the translation model, automatically obtained word-aligned parallel corpus is used. We used GIZA++ (Och and Ney, 2003) along with the growing heuristics to word-align the training corpus. The basic factors of the word used in our experiments are root, part-of-speech, gender, number and person. In Hindi, common nouns and verbs have gender information whereas, English doesn’t contain that information. Apart from the basic factors, we also consider the role information provided by labelled dependency parsers. For computing the dependency tree on the source side, We used stanford parser (Klein and Manning, 2003) in the experiments presented in this chapter3 . • Word-order variation including longdistance reorderi"
W10-3809,N07-1008,0,\N,Missing
W10-3809,P05-1033,0,\N,Missing
W10-3809,I08-1067,0,\N,Missing
W10-4310,N06-2015,0,0.01815,"ations. We use the part of speech (POS) tag associated with the head of the noun phrase to assign one of the following categories: pronoun, nominal, name or expletive. When the head does not belong to the above classes, we simply record its POS tag. We also mark whether the noun phrase is a definite description using the presence of the article ‘the’. Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S sales to remain steady at about 1,200 cars in 1990. The luxury auto maker last year sold 1,214 cars in the U.S. We use the coreference annotations from the Ontonotes corpus (version 2.9) (Hovy et al., 2006) to compute our gold-standard entity features. The WSJ portion of this corpus contains 590 articles. Here, nominalizations and temporal expressions are also annotated for coreference but we use the links between noun phrases only. We expect these features computed on the gold-standard annotations to represent an upper bound on the performance of entity features. Finally, the Penn Treebank corpus (Marcus et al., 1994) is used to obtain gold-standard parse and grammatical role information. Only adjacent sentences within the same paragraph are used in our experiments. 3 Modification. We expected"
W10-4310,D09-1036,0,0.373089,"discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). 2 Data We use 590 Wall Street Journal (WSJ) articles with overlapping annotations for discourse, coreference and syntax from three corpora. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations. In the PDTB, implicit relations are annotated between adjacent sentences in the same paragraph. They are assigned senses from a hierarchy containing four top level categories–Comparison, Contingency, Temporal and Expansion. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
W10-4310,P09-1077,1,0.332049,"expressions. We aim to investigate the association between discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). 2 Data We use 590 Wall Street Journal (WSJ) articles with overlapping annotations for discourse, coreference and syntax from three corpora. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations. In the PDTB, implicit relations are annotated between adjacent sentences in the same paragraph. They are assigned senses from a hierarchy containing four top level categories–Comparison, Contingency, Temporal and Expansion. Proceedings of SIGDIAL 2010: the 11th Annual Meetin"
W10-4310,prasad-etal-2008-penn,1,0.192545,"ract The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying this type of features comes from new corpus evidence (Prasad et al., 2008), that about a quarter of all adjacent sentences are linked purely by entity coherence, solely because they talk about the same entity. Entity-related features would be expected to better separate out such relations. We present the first comprehensive study of the connection between entity features and discourse relations. We show that there are notable differences in properties of referring expressions across the different relations. Sense prediction can be done with results better than random baseline using only entity realization information. Their performance, however, is lower than a know"
W10-4310,miltsakaki-etal-2004-penn,1,\N,Missing
W10-4310,N07-1054,0,\N,Missing
W10-4310,J93-2004,0,\N,Missing
W10-4310,P02-1047,0,\N,Missing
W10-4327,C94-1056,0,0.0305994,"xical overlap provides a simple and cheap alternative to discourse for computing text structure with comparable performance for the task of content selection. 1 Introduction Discourse relations such as cause, contrast or elaboration are considered critical for text interpretation, as they signal in what way parts of a text relate to each other to form a coherent whole. For this reason, the discourse structure of a text can be seen as an intermediate representation, over which an automatic summarizer can perform computations in order to identify important spans of text to include in a summary (Ono et al., 1994; Marcu, 1998; Wolf and Gibson, 2004). In our work, we study the content selection performance of different types of discourse-based features. Discourse relations interconnect units of a text and discourse formalisms have proposed different Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147–156, c The University of Tokyo, September 24-25, 2010. 2010 Association for Computational Linguistics 147 and compare discourse-based selection to simpler non-discourse methods. 2 ple parents appear frequently in texts and do not allow a t"
W10-4327,W01-1605,0,0.196426,"a subset of the overlapping documents for which we also have human summaries available. 2.1 lated expectation, Elaboration, Example, Generalization, Attribution, Temporal sequence, Similarity, Contrast and Same. The edge between two nodes representing a relation is directed in the case of asymmetric relations such as Cause and Condition and undirected for symmetric relations like Similarity and Contrast. RST corpus RST (Mann and Thompson, 1988) proposes that coherent text can be represented as a tree formed by the combination of text units via discourse relations. The RST corpus developed by Carlson et al. (2001) contains discourse tree annotations for 385 WSJ articles from the Penn Treebank corpus. The smallest annotation units in the RST corpus are sub-sentential clauses, also called elementary discourse units (EDUs). Adjacent EDUs combine through rhetorical relations into larger spans such as sentences. The larger units recursively participate in relations with others, yielding one hierarchical tree structure covering the entire text. The discourse units participating in a RST relation are assigned either nucleus or satellite status; a nucleus is considered to be more central, or important, in the"
W10-4327,W02-0404,0,0.0124147,"Missing"
W10-4327,prasad-etal-2008-penn,1,0.917385,"002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a detailed analysis of the predictive power of different types of discourse features for content selection We present analyses aimed at eliciting which specific aspects of discourse provide the strongest indication for text importance. In the context of content selection for single document summarization of news, we examine the benefits of both the graph structure of text provided by discourse relations and the semantic sense of these relations. We find that structure information is the most robust indicator of importance. Semantic sense only provides constraints on content select"
W10-4327,N03-1030,0,0.854144,"Missing"
W10-4327,C00-1072,0,0.0904035,"Missing"
W10-4327,W02-0406,0,0.0101342,"rcu, 1998; Wolf and Gibson, 2004; Uzda et al., 2008), little is known about which aspects of discourse are actually correlated with content selection power. In our work, we separate out structural and semantic features and examine their usefulness. We also investigate whether simpler intermediate representations can be used in lieu of discourse. More parsimonious, easy to compute representations of text have been proposed for summarization. For example, a text can be reduced to a set of highly descriptive topical words, the presence of which is used to signal importance for content selection (Lin and Hovy, 2002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Pra"
W10-4327,N03-1020,0,0.0285085,"Missing"
W10-4327,W04-1013,0,0.0416163,"Missing"
W10-4327,W06-1317,0,0.092053,"Missing"
W10-4327,W04-1004,0,0.271132,"for the full text, i.e. tree (Mann and Thompson, 1988) and graph (Wolf and Gibson, 2005). This structure is one source of information from discourse which can be used to compute the importance of text units. The semantics of the discourse relations between sentences could be another indicator of content importance. For example, text units connected by “cause” and “contrast” relationships might be more important content for summaries compared to those conveying “elaboration”. While previous work have focused on developing content selection methods based upon individual frameworks (Marcu, 1998; Wolf and Gibson, 2004; Uzda et al., 2008), little is known about which aspects of discourse are actually correlated with content selection power. In our work, we separate out structural and semantic features and examine their usefulness. We also investigate whether simpler intermediate representations can be used in lieu of discourse. More parsimonious, easy to compute representations of text have been proposed for summarization. For example, a text can be reduced to a set of highly descriptive topical words, the presence of which is used to signal importance for content selection (Lin and Hovy, 2002; Conroy et al"
W10-4327,J05-2005,0,0.349654,"to signal importance for content selection (Lin and Hovy, 2002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a detailed analysis of the predictive power of different types of discourse features for content selection We present analyses aimed at eliciting which specific aspects of discourse provide the strongest indication for text importance. In the context of content selection for single document summarization of news, we examine the benefits of both the graph structure of text provided by discourse relations and the semantic sense of these relations. We find that structure information is the most robust indicator of importance."
W10-4327,J00-3005,0,0.583013,"Missing"
W10-4327,J93-2004,0,\N,Missing
W10-4327,P06-2020,0,\N,Missing
W10-4327,P09-1077,1,\N,Missing
W10-4327,P04-1049,0,\N,Missing
W10-4327,I05-2004,0,\N,Missing
W10-4407,I08-2099,0,0.0265677,"raises the issue of the direction of the dependency in the case where one component adjoins into a host tree while a second component combines via substitution. 7 The V2 requirement in German can be handled with an obligatory adjoining constraint (denoted OA) on the S node of the tree for the main verb. The particular implementation of the V2 requirement is not relevant to our main argument. See (Kinyon et. al. 2006) for a TAG approach to V2. 58 Dependency Structures and Unavoidable Ill-nestedness from a Hindi dependency treebank that is being developed with the annotation scheme detailed in (Begum et al, 2008) (Mannem, p.c.), though we have not yet investigated whether other plausible analyses will retain the gap degree 2 property. structure perspective, showing that ill-nested structures are derivable and showing how gaps arise. In MG induced dependency structures, every gap is associated with movement (although not every movement corresponds to a gap). In MGs, every movement involves a liscensor and licensee pair, both of which are lexical entries. At first blush, it appears that the unlimited number of movements (i.e. uses of these entries) permitted during an MG derivation also permits an unbou"
W10-4407,W06-1503,1,0.846706,"Missing"
W10-4407,E09-1053,0,0.161648,"Missing"
W10-4407,W98-0106,0,0.331396,"Missing"
W10-4407,W07-1201,1,0.769072,"ations, we also expect more nuanced differences, such as the source of gaps, or the ease with which one can state a bound on gap degree in each framework. Comparison with TL-MCTAG The most obvious difference across formalisms is the source of gaps and, consequently, the ease with which a bound on gap degree can be stated. For TL-MCTAG, a bound is straightforwardly stated via the maximum number of components permitted in an elementary set. Though TL-MCTAG as a formal system allows any number of components in an MC-set, TL-MCTAG as used in linguistic analyses typically uses only two components (Chen-Main and Joshi, 2007). In a sense, TLMCTAG with two components arises naturally from standard TAG, particularly when adjoining is viewed as reversible. In the case where tree A adjoins into a tree internal node x in B, reversing the composition can be recast as follows: B is split into a two-component set at node x with one component adjoining into the root of tree A and the second component substituting into the foot of A. Motivating three-component sets is more difficult. A question that remains is whether or not there constructions that are unavoidably gap degree 2 or more. We are aware of one gap degree 2 exam"
W10-4407,W08-2303,0,0.0176538,"). The α component of (6b) adjoins to the root of (6h). To accomplish extraposition, we make use of flexible composition, the mirror operation of adjoining: If tree A adjoins into tree B, the combination can be alternatively viewed as tree B “flexibly” composing with tree A (Joshi et al. 2003, Kallmeyer and Joshi 2003).6By enriching MCTAG 5 6 The non-lexical root node and punctuation node are removed for simplicity. Thanks is due to Marisa Ferrara Boston for making this PDT structure available. A TL-MCTAG with flexible composition can also be viewed as an MCTAG allowing delayed tree-locality (Chiang and Scheffler, 2008). 57 Joan Chen-Main, Aravind K. Joshi into the T node of (6h) the gekauft tree. This derivation yields the phrase structure in Fig. 7 and corresponds to the dependency structure in Fig. 4.We have not fully committed to the direction of the dependency for flexible composition. If flexible composition is to be truly viewed as an alternate conception of adjoining, then perhaps the direction of the dependency when A flexibly composes into B should be identical to that of the dependency when B adjoins into A. Whatever the outcome, the ill-nestedness of our German example remains, as can be seen in"
W10-4407,W06-1517,0,0.158706,"4 Appropriate context and intonation will, of course, make this reading easier. The example has contrastive stress and a contrastive reading and is felicitous in a context such as the one below. Tatjana Scheffler is gratefully acknowledged for providing this example and context. A: Every student bought multiple items in the store. Some bought three magazines, some bought two calendars, some bought two books, and the oldest student bought three books. B: No, hat DER Student drei gekauft, der am meisten Geld hatte. 56 Dependency Structures and Unavoidable Ill-nestedness 4.3 pendency structure. Kuhlmann and Möhl (2006) show that dependency structures induced from tree-local MCTAG derivations in this way include structures that are ill-nested and/or gap degree &gt; 1. Ill-nestedness in Czech Boston et al. (2009) note that ill-nested structures have been assigned to Czech comparatives. Their example, sentence number Ln94209_45.a/18 from the PDT 2.0, can be glossed as “A strong individual will obviously withstand a high risk better than a weak individual” and is given in Fig. 5. It is of particular interest because, like our German example, the ill-nestedness here involves two constituents between which it would"
W10-4407,P06-2066,0,0.254098,"nately represented as a dependency graph and reviews how the class of LTAG derivations corresponds to the class of well-nested and gap degree ≤ 1 dependency structures. Section 4 turns to linguis2 Discontinuity in Dependency Structures The dependency structures we refer to in this paper are 3-tuples: a set of nodes, a dominance relation, and a (total) precedence relation. Dominance is encoded via a directed edge and precedence is encoded via left to right position on the page. Here, we review two measures of discontinuity defined on dependency structures. Expanded explanation can be found in (Kuhlmann and Nivre, 2006). a b c d e Figure 1. An example dependency structure 2.1 Gap Degree It will be useful to first define the term projection. Definition: The projection of a node x is the set of nodes dominated by x (including x). Definition: A gap is a discontinuity with respect to precedence in the projection of a node in the dependency structure. (E.g. in Figure (1), the node c is the gap preventing b and d from forming a contiguous interval.) Definition: The gap degree of a node is the number of gaps in its projection. Definition: The gap degree of a dependency structure is the max among the gap degrees of"
W10-4407,W06-1509,0,0.0216388,"ncy MCTAG (Weir 1988) is one of the most widely used extensions for handling linguistic cases that are difficult for classic TAG. Whereas TAG takes the basic unit to be a single elementary tree, MCTAGs extend the domain of locality to encompass a set of trees. The tree-local MC-extension, in which all members of an multi-component set must combine into the same “host” tree, allows for linguistically satisfying accounts for a number of attested phenomena, such as: English extraposition (Kroch and Joshi 1990), subj-aux inversion in combination with raising verbs (Frank 2002), anaphoric binding (Ryant and Scheffler 2006), quantifier scope ambiguity (Joshi et al. 2003). We have assumed here that each MC-set is lexicalized and that the set of nodes in MCTAGinduced dependency structures corresponds to the set of lexical anchors, just as we assumed for LTAG-induced dependency structures. Silent elements, such as traces, do not anchor an elementary tree, and so do not correspond to a node in the de5.3 The Adequacy of TL-MCTAG The MC-TAG derivation for (1b) will require a tree headed by gekauft ‘bought’ (shown in 6h) into which two MC-sets combine, one for Bücher ‘books’ and its trace (shown in 6b) and a second for"
W11-0806,T75-2013,0,0.719095,"perset of the functor-argument decompositions that string-rewriting systems can produce. 1 Introduction Multi-word expressions (MWEs), whose structure and meaning cannot be derived from their component words as they occur independently, account for a large portion of the language used in day-to-day interactions. Indeed, the relatively low frequency of comparable single-word paraphrases for elementary spatial relations like ‘in front of’ (compare to ‘before’) or ‘next to’ (compare to ‘beside’) suggest a fundamentality of expressions, as opposed to words, as a basic unit of meaning in language (Becker, 1975; Fillmore, 2003). Other examples of MWEs are idioms such as ‘kick the bucket’ or ‘spill the beans’, which have figurative meanings as expressions that sometimes even allow modification (‘spill some of the beans’) and variation in sentence forms (‘which beans Models have been proposed for MWEs based on string-rewriting systems such as HPSG (Sag et al., 2002), which model compositionality as string adjacency of a functor and an argument substring. This string-rewriting model of compositionality essentially treats each projection of a head word as a functor, each capable of combining with an arg"
W11-0806,P94-1022,0,0.12677,"he local non-compositionality of larger multi-word expressions like ‘threw X to the lions’ (see Figure 5), because only downward branches with multiple non3 Recognition and parsing of feature-based grammars, and of tree-rewriting systems whose elementary trees contain multiple foot nodes, are both exponential in the worst case. However, both types of grammars are amenable to regular-from restrictions which prohibit recursive adjunction at internal (nonroot, non-foot) tree nodes, and thereby constrain recognition and parsing complexity to cubic time for most kinds of natural language grammars (Rogers, 1994). 29 S NP↓ VP VP threw⋄ PP NP↓ to⋄ the⋄ NP lions⋄ Figure 5: Elementary structure for MWE idiom ‘threw . . . to the lions,’ allowing modification to both VP, PP and NP sub-constituents (e.g. ‘threw your friends today right to the proverbial lions). argument children can produce the multi-level subtrees containing the word ‘threw’ and the word ‘lions’ in the same elementary unit. 4 Conclusion This paper has shown that tree-rewriting systems are able to produce a superset of the functorargument decompositions that can be produced by string-rewriting systems such as categorial grammars and feature"
W11-0806,W04-0411,0,0.0704373,"Missing"
W12-3205,J95-2003,1,0.611654,"Missing"
W12-3205,J97-1003,0,0.229511,"t and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). topics such the geography of a country, followed by its history, its demographics, its economy, its legal structures, etc. Segmentation is usually done on a sentence-by-sentence basis, with segments not assumed to overlap. Methods for topic segmenation emply semantic, lexical and referential similarity or, more recently, language models (Bestgen, 2006; Chen et al., 2009; Choi et al., 2001; Eisenstein and Barzilay, 2008; Galley et al., 2003; Hearst, 1997; Malioutov and Barzilay, 2006; Purver et al., 2006; Purver, 2011). Functional structure and automated functional segmentation aims to identify sections within a discourse that serve different functions. These functions are genre-specific. In the case of scientific journals, high-level sections generally include the Background (work that motivates the objectives of the work and/or the hypothesis or claim being tested), followed by its Methods, and Results, ending with a Discussion of the results or outcomes, along with conclusions to be drawn. Finergrained segments might include the advantage"
W12-3205,I08-1050,0,0.166124,"done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Recognition has also proved useful (Hirohata et al., 2008), recognizing that the way the start of a functional segement is signalled may differ from how it is continued. Note that topic segmentation and functional segmentation are still not always distinguished. For example, in (Jurafsky and Martin, 2009), the term discourse segmentation is used to refer to any segmentation of a discourse into a “high-level” linear structure. Nevertheless, segmentation by function exploits different features (and in some cases, dif45 ferent methods) than segmentation by topic, so they are worth keeping distinct. Attention to event structure and the identification of"
W12-3205,W98-1123,0,0.0833374,"l unaddressed. 4.1 Evidence for discourse structures The first issue has to do with what should be taken as evidence for a particular discourse structure. While one could simply consider all features that can be computed reliably and just identify the most accurate predictors, this is both expensive and, in the end, unsatisfying. With topic structure, content words do seem to provide compelling evidence for segmentation, either using language models or semantic relatedness. On the other hand, this might be improved through further evidence in the form of entity chains, as explored earlier in (Kan et al., 1998), but using today’s more accurate approaches to automated coreference recognition (Strube, 2007; Charniak and Elsner, 2009; Ng, 2010). Whatever the genre, evidence for function structure seems to come from the frequency and distribution of closed-class words, particular phrases (or phrase patterns), and in the case of speech, intonation. So, for example, Niekrasz (2012) shows that what he calls participant-relational features that indicate the participants relationships to the text provide convincing evidence for segmenting oral narrative by the type of narrative activity taking place. These f"
W12-3205,J97-3006,0,0.111836,"Missing"
W12-3205,W00-1411,0,0.113483,"Missing"
W12-3205,liakata-etal-2010-corpora,0,0.0987406,"ion aims to identify sections within a discourse that serve different functions. These functions are genre-specific. In the case of scientific journals, high-level sections generally include the Background (work that motivates the objectives of the work and/or the hypothesis or claim being tested), followed by its Methods, and Results, ending with a Discussion of the results or outcomes, along with conclusions to be drawn. Finergrained segments might include the advantage of a new method (method-new-advantage) or of an old method (method-old-advantage) or the disadvantage of one or the other (Liakata et al., 2010). Again, segmentation is usually done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Reco"
W12-3205,W06-3309,0,0.166454,"with a Discussion of the results or outcomes, along with conclusions to be drawn. Finergrained segments might include the advantage of a new method (method-new-advantage) or of an old method (method-old-advantage) or the disadvantage of one or the other (Liakata et al., 2010). Again, segmentation is usually done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Recognition has also proved useful (Hirohata et al., 2008), recognizing that the way the start of a functional segement is signalled may differ from how it is continued. Note that topic segmentation and functional segmentation are still not always distinguished. For example, in (Jurafsky and Martin, 2009), the term discourse segmentation is u"
W12-3205,P06-1004,0,0.0177177,"t of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). topics such the geography of a country, followed by its history, its demographics, its economy, its legal structures, etc. Segmentation is usually done on a sentence-by-sentence basis, with segments not assumed to overlap. Methods for topic segmenation emply semantic, lexical and referential similarity or, more recently, language models (Bestgen, 2006; Chen et al., 2009; Choi et al., 2001; Eisenstein and Barzilay, 2008; Galley et al., 2003; Hearst, 1997; Malioutov and Barzilay, 2006; Purver et al., 2006; Purver, 2011). Functional structure and automated functional segmentation aims to identify sections within a discourse that serve different functions. These functions are genre-specific. In the case of scientific journals, high-level sections generally include the Background (work that motivates the objectives of the work and/or the hypothesis or claim being tested), followed by its Methods, and Results, ending with a Discussion of the results or outcomes, along with conclusions to be drawn. Finergrained segments might include the advantage of a new method (method-new-ad"
W12-3205,J00-3005,0,0.317433,"priate entity and over-writing previous buffer entries when the buffer was full. The next wave of work in computational discourse processing sought greater generality through stronger theoretical grounding, appealing to thencurrent theories of discourse such as Centering Theory (Grosz et al., 1986; Grosz et al., 1995), used as a basis for anaphor resolution (Brennan et al., 1987; Walker et al., 1997; Tetreault, 2001) and text generation (Kibble and Power, 2000), Rhetorical Structure Theory (Mann and Thompson, 1988), used as a basis for text generation (Moore, 1995) and document summarization (Marcu, 2000b), and Grosz and Sidner’s theory of discourse based on intentions (Grosz and Sidner, 1986a) and shared plans (Grosz and Sidner, 1990), used in developing animated agents (Johnson and Rickel, 2000). Issues related to fully characterizing centering are explored in great detail in (Kehler, 1997) and (Poesio et al., 2004). The approaches considered during this period never saw more than a few handfuls of examples. But, as has been clear from developments in PoStagging, Named Entity Recognition and parsing, Language Technology demands approaches that can deal with whatever data are given them. So"
W12-3205,P07-1075,0,0.0175543,"d be able to automate this through understanding the various ways that information is conveyed in discourse. Other examples of LT applications already benefitting from recognizing and applying discourse-level information include automated assessment of student essays (Burstein and Chodorow, 2010); summarization (Thione et al., 2004), infor42 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 42–54, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics mation extraction (Patwardhan and Riloff, 2007; Eales et al., 2008; Maslennikov and Chua, 2007), and more recently, statistical machine translation (Foster et al., 2010). These are described in more detail in (Webber et al., 2012). Our aim here then, on this occasion of ACL’s 50th Annual Meeting, is to briefly describe the evolution of computational approaches to discourse structure, reflect on where the field currently stands, and what new challenges it faces in trying to deliver on its promised benefit to Language Technology. 2 2.1 Background Early Methods The challenges mentioned above are not new. Question-Answering systems like LUNAR (Woods, 1968; Woods, 1978) couldn’t answer succe"
W12-3205,mladova-etal-2008-sentence,0,0.0649322,"Missing"
W12-3205,J92-4007,0,0.361511,"ering the condition s3 s1 condition (a) s2 s1 motivation s2 motivation s3 (b) Figure 1: Proposed discourse structures for Ex. 4: (a) In terms of informational relations; (b) in terms of intentional relations words in a sentence. At issue though was the nature of the structure. One issue concerned the nature of the relation between parent and child nodes in a discourse tree, and/or the relation between siblings. While Rhetorical Structure Theory (Mann and Thompson, 1988) posited a single discourse relation holding between any two discourse units (i.e., units projecting to adjacent text spans), Moore and Pollack (1992) gave an example of a simple discourse (Ex. 4) in which different choices about the discourse relation holding between pairs of units, implied different and nonisomorphic structures. (4) Come home by 5:00. s1 Then we can go to the hardware store before it closes. s2 That way we can finish the bookshelves tonight. s3 Example 4 could be analysed purely in terms of information-based discourse relations, in which s1 specified the CONDITION under which s2 held, which in turn specified the CONDITION under which s3 held. This would make s1 subordinate to s2, which in turn would be subordinate to s3,"
W12-3205,J96-3006,0,0.154458,"Missing"
W12-3205,P10-1142,0,0.198904,"ears have seen progress to differing degrees on at least four different types of discourse structures: topic structure, functional structure, event structure, and a structure of coherence relations. First we say a bit about the structures, and then about the resources employed in recognizing and labelling them. 3.1 Types of discourse structures Topic structure and automated topic segmentation aims to break a discourse into a linear sequence of 1 For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). topics such the geography of a country, followed by its history, its demographics, its economy, its legal structures, etc. Segmentation is usually done on a sentence-by-sentence basis, with segments not assumed to overlap. Methods for topic segmenation emply semantic, lexical and referential similarity or, more recently, language models (Bestgen, 2006; Chen et al., 2009; Choi et al., 2001; Eisenstein and Barzilay, 2008; Galley et al., 2003; Hearst, 1997; Malioutov and Barzilay, 2006; Purver et al., 2006; Purver, 2011). Functional structure and automated functional segmentati"
W12-3205,W09-3029,1,0.844135,", as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the problems that previous computational approaches encountered, it also reveals new ones, while leaving some earlier problems still unaddressed. 4.1 Evidence for discourse structures The"
W12-3205,D07-1075,0,0.0167337,"e sequence of simple sentences. Researchers should be able to automate this through understanding the various ways that information is conveyed in discourse. Other examples of LT applications already benefitting from recognizing and applying discourse-level information include automated assessment of student essays (Burstein and Chodorow, 2010); summarization (Thione et al., 2004), infor42 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 42–54, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics mation extraction (Patwardhan and Riloff, 2007; Eales et al., 2008; Maslennikov and Chua, 2007), and more recently, statistical machine translation (Foster et al., 2010). These are described in more detail in (Webber et al., 2012). Our aim here then, on this occasion of ACL’s 50th Annual Meeting, is to briefly describe the evolution of computational approaches to discourse structure, reflect on where the field currently stands, and what new challenges it faces in trying to deliver on its promised benefit to Language Technology. 2 2.1 Background Early Methods The challenges mentioned above are not new. Question-Answering systems like LUNAR"
W12-3205,P10-1056,0,0.0156477,"y (LT) researchers should care about discourse: Rather, discourse can enable LT to overcome known obstacles to better performance. Consider automated summarization and machine translation: Humans regularly judge output quality in terms that include referential clarity and coherence. Systems can only improve here by paying attention to discourse — i.e., to linguistic features above the level of ngrams and single sentences. (In fact, we predict that as soon as cheap — i.e., non-manual – methods are found for reliably assessing these features — for example, using proxies like those suggested in (Pitler et al., 2010) — they will supplant, or at least complement today’s common metrics, Bleu and Rouge that say little about what matters to human text understanding (Callison-Burch et al., 2006).) Consider also work on automated text simplification: One way that human editors simplify text is by re-expressing a long complex sentence as a discourse sequence of simple sentences. Researchers should be able to automate this through understanding the various ways that information is conveyed in discourse. Other examples of LT applications already benefitting from recognizing and applying discourse-level information"
W12-3205,J04-3003,0,0.0349316,"Missing"
W12-3205,prasad-etal-2008-penn,1,0.854536,"n made available for other researchers. For fine-grained functional structure, there is the ART corpus (Liakata et al., 2010)2 . For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/a"
W12-3205,prasad-etal-2010-exploiting,1,0.929235,"olf and Gibson, 2005). If such a cover is assumed, methods involve parsing a text into units using lexical and punctuational cues, followed by labelling the relation holding between them (Marcu, 2000a; Marcu, 2000b; Wolf and Gibson, 2005). If text is not assumed to be divisible into discourse units, then methods involve finding evidence for discourse relations (including both explicit words and phrases, and clausal and sentential adjacency) and their arguments, and then labelling the sense of the identified relation (Elwell and Baldridge, 2008; Ghosh et al., 2011; Lin et al., 2010; Lin, 2012; Prasad et al., 2010a; Wellner, 2008; Wellner and Pustejovsky, 2007). 3.2 Resources for discourse structure All automated systems for segmenting and labelling text are grounded in data — whether the data has informed the manual creation of rules or has been a source of features for an approach based on machine learning. In the case of topic structure and high-level functional structure, there is now a substantial amount of data that is freely available. For other types of discourse structure, manual annotation has been required and, depending on the type of structure, different amounts are currently available. Mo"
W12-3205,C10-2118,1,0.917829,"olf and Gibson, 2005). If such a cover is assumed, methods involve parsing a text into units using lexical and punctuational cues, followed by labelling the relation holding between them (Marcu, 2000a; Marcu, 2000b; Wolf and Gibson, 2005). If text is not assumed to be divisible into discourse units, then methods involve finding evidence for discourse relations (including both explicit words and phrases, and clausal and sentential adjacency) and their arguments, and then labelling the sense of the identified relation (Elwell and Baldridge, 2008; Ghosh et al., 2011; Lin et al., 2010; Lin, 2012; Prasad et al., 2010a; Wellner, 2008; Wellner and Pustejovsky, 2007). 3.2 Resources for discourse structure All automated systems for segmenting and labelling text are grounded in data — whether the data has informed the manual creation of rules or has been a source of features for an approach based on machine learning. In the case of topic structure and high-level functional structure, there is now a substantial amount of data that is freely available. For other types of discourse structure, manual annotation has been required and, depending on the type of structure, different amounts are currently available. Mo"
W12-3205,P06-1003,0,0.0671148,"Missing"
W12-3205,rysova-2012-alternative,0,0.0176292,"l to have allowed a conventional landing. What’s more, the seven mail personnel aboard were missing. [wsj 0550] (7) The two companies each produce market pulp, containerboard and white paper. That means goods could be manufactured closer to customers, saving shipping costs, he said. [wsj 0317] The discovery of these other forms of evidence3 raises the question of when it is that a word or phrase signals a discourse relation. For example, only 15 of the 33 tokens of that means in the PDTB were annotated as evidence of a discourse relation. While the 3 which English is not alone in having, cf. (Rysova, 2012) 47 three paragraph-initial instances were left unannotated due to resource limitations (ie, no paragraph initial sentences were annotated unless they contained an explicit discourse connective), the majority were ignored because they followed an explicit connective. As Wiebe’s example (5) showed, there can be multiple explicit discourse connectives in a clause, each of which is evidence for a separate discourse relation (albeit possibly between the same arguments). All of these are annotated in the PDTB – eg, both but and then in (8) Congress would have 20 days to reject the package with a 50"
W12-3205,W04-0213,0,0.0257582,"For all other kinds of discourse structures, dedicated manual annotation has been required, both for segmentation and labelling, and many of these resources have been made available for other researchers. For fine-grained functional structure, there is the ART corpus (Liakata et al., 2010)2 . For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseanno"
W12-3205,J01-4003,0,0.0608202,"Missing"
W12-3205,J02-4002,0,0.0515376,"clude the advantage of a new method (method-new-advantage) or of an old method (method-old-advantage) or the disadvantage of one or the other (Liakata et al., 2010). Again, segmentation is usually done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Recognition has also proved useful (Hirohata et al., 2008), recognizing that the way the start of a functional segement is signalled may differ from how it is continued. Note that topic segmentation and functional segmentation are still not always distinguished. For example, in (Jurafsky and Martin, 2009), the term discourse segmentation is used to refer to any segmentation of a discourse into a “high-level” linear structure. Nevertheless, segmentation by fu"
W12-3205,D09-1155,0,0.0119415,"new method (method-new-advantage) or of an old method (method-old-advantage) or the disadvantage of one or the other (Liakata et al., 2010). Again, segmentation is usually done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Recognition has also proved useful (Hirohata et al., 2008), recognizing that the way the start of a functional segement is signalled may differ from how it is continued. Note that topic segmentation and functional segmentation are still not always distinguished. For example, in (Jurafsky and Martin, 2009), the term discourse segmentation is used to refer to any segmentation of a discourse into a “high-level” linear structure. Nevertheless, segmentation by function exploits diffe"
W12-3205,W04-1009,0,0.219571,"Missing"
W12-3205,tonelli-etal-2010-annotation,1,0.827173,"er and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the problems that previous computational approaches encountered, it also reveals new ones, while leaving some earlier problems still unaddressed. 4.1 Evidence for discourse structures The first issue has to do with what should be taken as evidence for a particular discourse structure. While one could simply consider all features that can be computed reliably and just identify the most accurate predictors, this is b"
W12-3205,W98-0315,1,0.650817,"egmentation and labelling, and many of these resources have been made available for other researchers. For fine-grained functional structure, there is the ART corpus (Liakata et al., 2010)2 . For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 201"
W12-3205,D07-1010,0,0.0306337,"r is assumed, methods involve parsing a text into units using lexical and punctuational cues, followed by labelling the relation holding between them (Marcu, 2000a; Marcu, 2000b; Wolf and Gibson, 2005). If text is not assumed to be divisible into discourse units, then methods involve finding evidence for discourse relations (including both explicit words and phrases, and clausal and sentential adjacency) and their arguments, and then labelling the sense of the identified relation (Elwell and Baldridge, 2008; Ghosh et al., 2011; Lin et al., 2010; Lin, 2012; Prasad et al., 2010a; Wellner, 2008; Wellner and Pustejovsky, 2007). 3.2 Resources for discourse structure All automated systems for segmenting and labelling text are grounded in data — whether the data has informed the manual creation of rules or has been a source of features for an approach based on machine learning. In the case of topic structure and high-level functional structure, there is now a substantial amount of data that is freely available. For other types of discourse structure, manual annotation has been required and, depending on the type of structure, different amounts are currently available. More specifically, work on topic structure and seg"
W12-3205,W93-0239,0,0.165457,"ther issue during this period concerned the nature of discourse structure: Was it really a tree? Sibun (1992), looking at people’s descriptions of the layout of their house or apartment, argued that they resembled different ways of linearizing a graph of the rooms and their connectivity through doors and 44 halls. None of these linearizations were trees. Similarly, Knott et al. (2001), looking at transcriptions of museum tours, argued that each resembled a linear sequence of trees, with one or more topic-based connections between their root nodes — again, not a single covering tree structure. Wiebe (1993), looking at simple examples such as (5) The car was finally coming toward him. s1 He finished his diagnostic tests, s2 feeling relief. s3 But then the car started to turn right. s4 pointed multiple lexical items explicitly relating a clause to multiple other clauses. Here, but would relate s4 to s3 via a CONTRAST relation, while then would relate s4 to s2 via a temporal SUCCESSION relation. The most well-known of work from this period is that of Mann and Thompson (1988), Grosz and Sidner (1986b), Moore and Moser (1996), Polanyi and van den Berg (1996), and Asher and Lascarides (2003).1 The wa"
W12-3205,J05-2005,0,0.0394554,", 2011; Finlayson, 2009). The automated identification of discourse relations aims to identify discourse relations such as CONDITION and MOTIVATION , as in Example 4, and CONTRAST and SUCCESSION, as in Example 5. These have also been called coherence relations or rhetorical relations. Methods used depend on whether or not a text is taken to be divisible into a covering sequence of a non-overlapping discourse units related to adjacent units by discourse relations as in Rhetorical Structure Theory (Mann and Thompson, 1988) or to both adjacent and nonadjacent units as in the Discourse GraphBank (Wolf and Gibson, 2005). If such a cover is assumed, methods involve parsing a text into units using lexical and punctuational cues, followed by labelling the relation holding between them (Marcu, 2000a; Marcu, 2000b; Wolf and Gibson, 2005). If text is not assumed to be divisible into discourse units, then methods involve finding evidence for discourse relations (including both explicit words and phrases, and clausal and sentential adjacency) and their arguments, and then labelling the sense of the identified relation (Elwell and Baldridge, 2008; Ghosh et al., 2011; Lin et al., 2010; Lin, 2012; Prasad et al., 2010a;"
W12-3205,W05-0312,0,0.026529,"relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the prob"
W12-3205,I08-7009,1,0.817926,"corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the problems that previous computational approaches encountered, it also reveals new ones, while leaving some earlier problems still unaddressed. 4.1 Evidence for discourse structures The first issue has to do with what should"
W12-3205,W09-3006,0,0.0555721,"Missing"
W12-3205,W10-1844,0,0.0541576,"Missing"
W12-3205,P12-1008,0,0.0224601,"nnotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the problems that previous co"
W12-3205,D11-1068,0,\N,Missing
W12-3205,W01-0514,0,\N,Missing
W12-3205,E06-1032,0,\N,Missing
W12-3205,miltsakaki-etal-2004-penn,1,\N,Missing
W12-3205,W11-0401,0,\N,Missing
W12-3205,D11-1027,0,\N,Missing
W12-3205,N09-1042,0,\N,Missing
W12-3205,D08-1035,0,\N,Missing
W12-3205,al-saif-markert-2010-leeds,0,\N,Missing
W12-3205,W01-1605,0,\N,Missing
W12-3205,P03-1071,0,\N,Missing
W12-3205,D08-1021,0,\N,Missing
W12-3205,P87-1022,0,\N,Missing
W12-3205,J06-1002,0,\N,Missing
W12-3205,E09-1018,0,\N,Missing
W12-3205,P08-1090,0,\N,Missing
W12-3205,J86-3001,0,\N,Missing
W12-3205,W10-1817,0,\N,Missing
W12-3205,W10-1913,0,\N,Missing
W12-4601,W10-4402,0,0.506832,"cheffler (2008). The dashed boxes mark the delays. Thus, a valid k-delayed tree local MCTAG derivation permits members of the same MC set to compose into different trees, so long as all members of the MC set eventually compose into the same tree without exceeding k delays. Delayed tree-locality permits a limited amount of set-local composition, as illustrated in Fig.1, but it also permits some non-set-local derivational steps. 1-delayed and 2-delayed treelocal MCTAG have already been employed in linguistic analyses of anaphor binding (Chiang and Scheffler, 2008), non-local right node raising (Han et al., 2010), and binding variables (Storoshenko and Han, 2010). This paper explores how well the additional descriptive power of 2-delayed tree-local MCTAG accommodates the available clitic climbing data and compares the new approach with the set-local MCTAG approach. In section 2, we review the data Bleam (2000) sought to account for. In section 3, we review why such data is problematic for tree-local MCTAG and Abstract Since Bleam's (2000) initial claim that capturing clitic climbing patterns in Romance requires the descriptive power of set-local MCTAG (Weir, 1988), alternative approaches to relaxing t"
W12-4601,W10-4418,0,0.327816,"delays. Thus, a valid k-delayed tree local MCTAG derivation permits members of the same MC set to compose into different trees, so long as all members of the MC set eventually compose into the same tree without exceeding k delays. Delayed tree-locality permits a limited amount of set-local composition, as illustrated in Fig.1, but it also permits some non-set-local derivational steps. 1-delayed and 2-delayed treelocal MCTAG have already been employed in linguistic analyses of anaphor binding (Chiang and Scheffler, 2008), non-local right node raising (Han et al., 2010), and binding variables (Storoshenko and Han, 2010). This paper explores how well the additional descriptive power of 2-delayed tree-local MCTAG accommodates the available clitic climbing data and compares the new approach with the set-local MCTAG approach. In section 2, we review the data Bleam (2000) sought to account for. In section 3, we review why such data is problematic for tree-local MCTAG and Abstract Since Bleam's (2000) initial claim that capturing clitic climbing patterns in Romance requires the descriptive power of set-local MCTAG (Weir, 1988), alternative approaches to relaxing tree-locality restrictions have been developed, incl"
W12-4601,W08-2303,0,\N,Missing
W15-2707,W05-0305,1,0.799224,"Missing"
W15-2707,D12-1083,0,0.012738,"(3) Now, we regard this as a largely phony issue, but the “long term” is nonetheless a big salon topic all around the Beltway. (5) PropBank: Verb = suspend Arg0 = The federal government Arg1 = sales of U.S. savings bonds ARGM-CAU = because Congress hasn’t lifted the ceiling on government debt (4) The U.S. wants the removal of . . .barriers to investment; Japan denies there are real barriers. Researchers working on discourse parsing have commented that intra-sentential (intra-S) discourse relations are, in general, easier to recognize than ones whose arguments are found in separate sentences (Joty et al., 2012; Lin et al., (6) PDTB: Connective = because Arg1 = The federal government suspended sales of U.S. savings bonds Arg2 = Congress hasn’t lifted the ceiling on government debt Sense = Contingency.Cause.Reason 64 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 64–69, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. ARGM - ADV (2235) ARGM - CAU (657) ARGM - TMP (2503) ARGM - PNC (66) ARGM - MNR (13) TOTAL (5475) TEMPORAL CONTINGENCY COMPARISON EXPANSION TOTAL 222 14 2258 0 0 2494 1067 650 523"
W15-2707,J93-2004,0,0.0504287,"Missing"
W15-2707,W04-2705,0,0.14572,"Missing"
W15-2707,J05-1004,0,0.340826,"mproving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clause annotated as ARGM and assigned the role CAU (causal). The PDTB annotation for the same example, shown in (6), marks because as the connective, ‘Contingency.Cause.Reason’ as the sense, the adjunct clause as Arg2 (defined as the argument attached to the connective), and the matrix clause as Arg1 (defined as the non-Arg2 argument)."
W15-2707,N04-1030,1,0.764484,"Missing"
W15-2707,W13-3516,1,0.897991,"Missing"
W15-2707,prasad-etal-2008-penn,1,0.826615,"bber@ed.ac.uk 3 Institute for Research in Cognitive Science, University of Pennsylvania {aleewk,joshi}@seas.upenn.edu 4 Boulder Language Technologies pradhan@bltek.com Abstract 2012; Feng, 2014). They are also quite useful in Language Technology applications that exploit sentence-level relations. Thus, there is particular value in improving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clau"
W15-2707,C10-2118,1,0.939227,"Missing"
W15-2707,J14-4007,1,0.857151,"leewk,joshi}@seas.upenn.edu 4 Boulder Language Technologies pradhan@bltek.com Abstract 2012; Feng, 2014). They are also quite useful in Language Technology applications that exploit sentence-level relations. Thus, there is particular value in improving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clause annotated as ARGM and assigned the role CAU (causal). The PDTB annotation for the same"
W15-2707,N07-1069,0,0.060601,"Missing"
W16-1704,P14-1065,0,0.0139892,"Missing"
W16-1704,N09-1064,0,0.0255764,"Missing"
W16-1704,J15-3002,0,0.0272451,"Missing"
W16-1704,W12-3624,0,0.0492071,"Missing"
W16-1704,J93-2004,0,0.0590896,"wsj 1270] (13) Then take the expected return and subtract one standard deviation. [wsj 1564] (14) Be careful boys; use good judgment. [wsj 0596] 2.2 Discourse Adverbials (9) The NAM embraces efforts, which both the adminisAs can be seen from the presence of then in Ex. 9, conjoined VPs can themselves contain discourse adverbials. As with all discourse adverbials, ones that appear in Arg2 of a conjoined VP can link to material elsewhere in the text, as in Ex. 15 (15) Separately, the Federal Energy Regulatory CommisSince these were incorrectly analyzed according to the Penn TreeBank Guidelines (Marcus et al., 1993) and do not actually differ from the tokens already included in the corpus, we decided to include them. On the other hand, we decided to exclude tokens containing conjoined verbs that should possibly have been analyzed as conjoined VPs, such as exist and fight in While the discourse adverbial still shares its Arg2 with the conjoined VP, its Arg1 has been taken to be the FERC turning down its request for approval of its possible purchase of PS of New Hampshire, which appears in the previous sentence. Although such adverbials can link to material in previous sentences, the far more common situat"
W16-1704,prasad-etal-2008-penn,1,0.835795,"Missing"
W16-1704,C10-2118,1,0.597562,"equency with which each PDTB2 sense has been replaced by a specific PDTB3 sense. 4.2 Implicit=instead be part of the world mentality,” declares Charles M. Jordan, GM’s vice president for design . . . (Expansion.Conjunction, Expansion.Substitution.Arg2-as-subst) [wsj 0956] (38) . . . Exxon Corp. built the plant but Implicit=then closed it in 1985. (Comparison.Concession.Arg2-as-denier, Temporal.Asynchronous.Precedence) [wsj 1748] 3. If inserting an implicit connective was perceived as redundant, appropriate material in Arg2 could be annotated as AltLex (Ex. 39), as done elsewhere in the PDTB2 (Prasad et al., 2010). (39) His policies went beyond his control and resulted . . . in riots and disturbances. (Expansion.Conjunction, Contingency.Cause.Result) [wsj 0290] Sense labelling of conjoined VP tokens The second guideline above points to a new feature of our discourse annotation: While multiple relations were annotated in the PDTB2 as holding between identical or overlapping argument spans, all were associated with either multiple explicit connectives or multiple inferred relations. What is new in the annotation of conjoined VPs is the possibility of an explicit relation co-occurring with ones that are i"
W16-1704,J14-4007,1,\N,Missing
W16-1704,W01-1605,0,\N,Missing
W94-0107,A88-1019,0,0.0112907,"ses the local ambiguil.y for the parser. The parser has to decide which complex description (LTAG tree) out of the set of descriptions associated with each lexical item is to be used for a given reading of a sentence, even before combining the descriptions together. The obvious solution is to put the burden of this job entirely on the parser. The parser will eventually disambiguate all the descriptions and pick one per object, for a given reading of the sentence. This is what the parser is expected to do for disambiguating the standard POS, unless a separate POS disambiguation module is used (Church, 1988). Many parsers, including XTAG, use such a module ('alh'd a POS tagger. LTAGs present a novel opportunity to reduce the amount of disambiguation done by the parser. We can treat the LTAG trees associated with each lexic'al item as more complex parts-of-speech which we call sup e r t a g s . In this paper, we report on some experiments on direct supertag disambiguation, without parsing in the strict sense, using lexical preference and local lexical dependencies (acquired from a corpus parsed by the XTAG system). The information extracted from the XTAG-parsed corpus contains, for each item and i"
W94-0107,C92-2065,0,0.0189973,"with each item can be large, on the order of 10 trees in the current English grammar in the XTAG system s. This is because in LTAG, roughly speaking, each lexical item 1Let ~ be the alphabet consisting of the names of elmentary trees in an LTAG. Then ~ * is the set of all strings over this alphabet including the null string. The tree 71 and 7~ in a string of tree names axe said to be ~*-local if they are separated by any string in ~ * . For brevity, we will continue to use the term local instead of the term ~*-local. 2The work described here is completely different from the work reported in (Resnik, 1992) and (Schabes, 1992) concerning stochastic TAGs. 3See Section on Data Collection is associated with as many trees as the numb~,r of different syntactic contexts in which the iexical item can appear. This, of course, increases the local ambiguil.y for the parser. The parser has to decide which complex description (LTAG tree) out of the set of descriptions associated with each lexical item is to be used for a given reading of a sentence, even before combining the descriptions together. The obvious solution is to put the burden of this job entirely on the parser. The parser will eventually disamb"
W94-0107,C92-2066,0,0.0253159,"be large, on the order of 10 trees in the current English grammar in the XTAG system s. This is because in LTAG, roughly speaking, each lexical item 1Let ~ be the alphabet consisting of the names of elmentary trees in an LTAG. Then ~ * is the set of all strings over this alphabet including the null string. The tree 71 and 7~ in a string of tree names axe said to be ~*-local if they are separated by any string in ~ * . For brevity, we will continue to use the term local instead of the term ~*-local. 2The work described here is completely different from the work reported in (Resnik, 1992) and (Schabes, 1992) concerning stochastic TAGs. 3See Section on Data Collection is associated with as many trees as the numb~,r of different syntactic contexts in which the iexical item can appear. This, of course, increases the local ambiguil.y for the parser. The parser has to decide which complex description (LTAG tree) out of the set of descriptions associated with each lexical item is to be used for a given reading of a sentence, even before combining the descriptions together. The obvious solution is to put the burden of this job entirely on the parser. The parser will eventually disambiguate all the descr"
W98-0315,P97-1012,1,0.546272,"s trying to do. I Secondly, there is a single auzil~ary tree whose semantics corresponds simply to continuing the description conveyed by the structure to which it is adjoined. Any additionalinferences that a listener draws from the resulting adjacency are defeasible, Introduction In the past few years, researchers interested in accounting for how elements combine in a discourse, have taken to using the adjoining operation found in Tree-Adjoining G r a m m a r (TAG) (Gardent, 1994; Gardent, 1997; Polanyi and van den Berg, 1996; Schilder, 1997; van den Berg, 1996; Webber, 1991). More recently, Cristea and Webber (1997) have argued that a Tree-Adjoining G r a m m a r for discourse would also need the substitution operation found in a lexicalized TAG (Schabes, 1990). Here we move further and explore a fully lexicalized TAG for discourse, allowing us to examine how the insights of lexicalized g r a m m a r s - that the basic elements of a clause are not simply words, but structures that reflect a word&apos;s role and syntactic/semantic scope carry over to discourse. We show how this suggests explanations for such phenomena as the following: • that arguments of a coherence relation can be stretched &quot;long distance&quot; b"
