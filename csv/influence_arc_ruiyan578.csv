2020.acl-main.517,W14-4012,0,0.0618125,"Missing"
2020.acl-main.517,N18-2115,0,0.0576478,"Missing"
2020.acl-main.517,P19-1134,0,0.0649542,"n the input query q as, L = − log p(r|q, θs , θp , θg ) Experiments Implementation Details We implement our shared module based on the Seq2seq model with pre-trained Glove embedding [Pennington et al., 2014] and LSTM unit, and use a 4-layer MLP for the private module1 . The dimension of word embedding, hidden state, and MLP’s output are set to 300. In CMAML, we pretrain the model for 10 epochs and re-train each model for 5 steps to prune the private network. The L-1 weight in the re-training stage is 0.001, and the threshold γ is 0.05. We follow other hyperparameter settings in Madotto et al. [2019]. (6) 5836 1 Code is available at https://github.com/zequnl/CMAML 5.3 Competing Methods • Pretrain-Only: We pre-train a unified dialogue generation model with data from all training tasks then directly test it on the testing tasks. We try three base generation models: the Seq2seq [Bahdanau et al., 2014] and the Speaker model [Li et al., 2016b] and the Seq2SPG proposed in Section3.1. Speaker incorporates the task (user/emoji) embeddings in the LSTM cell, and the task embeddings of testing tasks are random parameters in this setting. • Finetune: We fine-tune the pre-trained models on each testin"
2020.acl-main.517,N16-1014,0,0.610552,"data and task similarity. We then describe the situations where the meta-learning can outperform other fine-tuning methods. 2 Related Work Few-shot Dialogue Generation. The past few years have seen increasing attention on building dialogue models in few-shot settings, such as personalized chatbots that can quickly adapt to each user’s profile or knowledge background [Zhang et al., 2018; Madotto et al., 2019], or that respond with a specified emotion [Zhou et al., 18; Zhou and Wang, 2018]. Early solutions are to use explicit [Tian et al., 2017; Zhang et al., 2018; Zhou et al., 18] or implicit [Li et al., 2016b; Zhou and Wang, 2018; Zhou et al., 18] task descriptions, then introduce this information into the generative models. However, these methods require manually created task descriptions, which are not available in many practical cases. An alternative promising solution to building few-shot dialogue models is the meta-learning methods, especially MAML [Finn et al., 2017]. Madotto et al. (2019) propose to regard learning with the dialogue corpus of each user as a task and endow the personalized dialogue models by fine-tuning the initialized parameters on the taskspecific data. Qian and Yu (2019)"
2020.acl-main.517,P16-1094,0,0.543615,"data and task similarity. We then describe the situations where the meta-learning can outperform other fine-tuning methods. 2 Related Work Few-shot Dialogue Generation. The past few years have seen increasing attention on building dialogue models in few-shot settings, such as personalized chatbots that can quickly adapt to each user’s profile or knowledge background [Zhang et al., 2018; Madotto et al., 2019], or that respond with a specified emotion [Zhou et al., 18; Zhou and Wang, 2018]. Early solutions are to use explicit [Tian et al., 2017; Zhang et al., 2018; Zhou et al., 18] or implicit [Li et al., 2016b; Zhou and Wang, 2018; Zhou et al., 18] task descriptions, then introduce this information into the generative models. However, these methods require manually created task descriptions, which are not available in many practical cases. An alternative promising solution to building few-shot dialogue models is the meta-learning methods, especially MAML [Finn et al., 2017]. Madotto et al. (2019) propose to regard learning with the dialogue corpus of each user as a task and endow the personalized dialogue models by fine-tuning the initialized parameters on the taskspecific data. Qian and Yu (2019)"
2020.acl-main.517,P19-1542,0,0.356472,"alogues from the non-target domains and then fine-tune on the task-specific data corpus [Wang et al., 2019a; Alt et al., 2019a; Klein, 2019]. While pre-training is beneficial, such models still require sufficient taskspecific data for fine-tuning. They cannot achieve satisfying performance when very few examples ∗ Corresponding author are given [Bansal et al., 2019]. Unfortunately, this is often the case in many dialogue generation scenarios. For example, in personalized dialogue generation, we need to quickly adapt to the response style of a user’s persona by just a few his or her dialogues [Madotto et al., 2019; Zhang et al., 2018]; in emotional dialogue generation, we need to generate a response catering to a new emoji using very few utterances containing this emoji [Zhou et al., 18; Zhou and Wang, 2018]. Hence, this is the focus of our paper - few-shot dialogue generation, i.e. training a generative model that can be generalized to a new task (domain) within k-shots of its dialogues. A few works have been proposed to consider few-shot dialogue generation as a meta-learning problem [Madotto et al., 2019; Qian and Yu, 2019; Mi et al., 2019]. They all rely on the popular modelagnostic meta-learning ("
2020.acl-main.517,K18-1047,0,0.0275671,"? Really dog ? Really Really Similar tasks a have shared module gating module hate hate pets pets User-j Decoder Figure 1: The proposed CMAML algorithm applying on the personalized dialogue systems. Each customized dialogue model Seq2SPG consists of a shared, a private, and a gating module. The shared and gating module are the same among users and are trained on all tasks. The private module is unique for each user to describe this user’s persona, and is trained on the corresponding and similar tasks. The lines in color indicate the data flow directions. 2019; Liu et al., 2020]. Jiang et al. (2018) build an attention layer over the convolutional layers, where the convolutional layer is for general features and the attention layer is for task-specific features. Sun et al. (2019) propose to learn a task-specific shifting and scaling operation on the general shared feed-forward layers. However, the involved operations in these two methods such as shifting and scaling are designed for feed-forward networks, and can not be applied to the generative models which generally rely on Seq2seq [Sutskever et al., 2014] models with recurrent GRU [Cho et al., 2014] or LSTM [Hochreiter and Schmidhuber,"
2020.acl-main.517,W19-4326,0,0.0223164,"n the input query q as, L = − log p(r|q, θs , θp , θg ) Experiments Implementation Details We implement our shared module based on the Seq2seq model with pre-trained Glove embedding [Pennington et al., 2014] and LSTM unit, and use a 4-layer MLP for the private module1 . The dimension of word embedding, hidden state, and MLP’s output are set to 300. In CMAML, we pretrain the model for 10 epochs and re-train each model for 5 steps to prune the private network. The L-1 weight in the re-training stage is 0.001, and the threshold γ is 0.05. We follow other hyperparameter settings in Madotto et al. [2019]. (6) 5836 1 Code is available at https://github.com/zequnl/CMAML 5.3 Competing Methods • Pretrain-Only: We pre-train a unified dialogue generation model with data from all training tasks then directly test it on the testing tasks. We try three base generation models: the Seq2seq [Bahdanau et al., 2014] and the Speaker model [Li et al., 2016b] and the Seq2SPG proposed in Section3.1. Speaker incorporates the task (user/emoji) embeddings in the LSTM cell, and the task embeddings of testing tasks are random parameters in this setting. • Finetune: We fine-tune the pre-trained models on each testin"
2020.acl-main.517,P17-2036,1,0.830348,"tors for meta-learning based methods, i.e., the quantity of training data and task similarity. We then describe the situations where the meta-learning can outperform other fine-tuning methods. 2 Related Work Few-shot Dialogue Generation. The past few years have seen increasing attention on building dialogue models in few-shot settings, such as personalized chatbots that can quickly adapt to each user’s profile or knowledge background [Zhang et al., 2018; Madotto et al., 2019], or that respond with a specified emotion [Zhou et al., 18; Zhou and Wang, 2018]. Early solutions are to use explicit [Tian et al., 2017; Zhang et al., 2018; Zhou et al., 18] or implicit [Li et al., 2016b; Zhou and Wang, 2018; Zhou et al., 18] task descriptions, then introduce this information into the generative models. However, these methods require manually created task descriptions, which are not available in many practical cases. An alternative promising solution to building few-shot dialogue models is the meta-learning methods, especially MAML [Finn et al., 2017]. Madotto et al. (2019) propose to regard learning with the dialogue corpus of each user as a task and endow the personalized dialogue models by fine-tuning the"
2020.acl-main.517,P19-1589,0,0.0259932,"n in multidomain task-oriented dialogue generation as a task, and apply MAML in a similar way. All these methods do not change the original MAML but directly apply it to their scenarios due to the model-agnostic property of MAML. Thus, task differentiation always counts on fine-tuning, which only searches the best model for each task at the parameter level but not the model structure level. Meta-learning. Meta-learning has achieved promising results in many NLP problems recently due to its fast adaptation ability on a new task using very few training data [Yu et al., 2019; Wang et al., 2019b; Obamuyide and Vlachos, 2019b; Alt et al., 2019b]. In general, there are three categories of meta-learning methods: metric-based methods [Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018; Ye and Ling, 2019] which encode the samples into an embedding space along with a learned distance metric and then apply a matching algorithm, model-based methods [Santoro et al., 2016; Obamuyide and Vlachos, 2019a] which depend on the model structure design such as an external memory storage to facilitate the learning process, and optimization-based methods [Finn et al., 2017; Andrychowicz et al., 2016; Huang et al., 2018] wh"
2020.acl-main.517,P02-1040,0,0.106967,"r proposed algorithm. CMAML-Seq2SPG is our full model (equal to CMAML in previous sections), where the dialogue Seq2SPG is the base model and pruning algorithm is applied for customizing unique model structures for tasks. CMAML-Seq2SP0 G uses a different base model noted as Seq2SP0 G, where the private module only takes the output of the shared module as the input. Pruning algorithm is also applied in private module for network customization. 5.4 Evaluation Metrics Automatic Evaluation. We performed automatic evaluation metrics in three perspectives: • Response quality/diversity: We use BLEU [Papineni et al., 2002] to measure the word overlap between the reference and the generated sentence; PPL, the negative logarithm of the generated sentence; Dist-1 [Li et al., 2016a; Song et al., 2017, 2018] to evaluate the response diversity, which calculates the ratio of distinct 1-gram in all test generated responses. • Task consistency: We use C score [Madotto et al., 2019] in Persona-chat, which uses a pre-trained natural language inference model to measure the response consistency with persona description, and E-acc [Zhou and Wang, 2018] in MojiTalk, which uses an emotion classifier to predict the correlation"
2020.acl-main.517,D14-1162,0,0.0854621,"f the three modules in our proposed dialogue model during customized model training in Algorithm 2. For the shared and gating module, gradients are updated in the same way as MAML. The update of the private module is replaced by the above Eq. 4 and Eq. 5 introduced in joint meta-learning. The loss function used to calculate the gradients in our model is the negative log-likelihood of generating the response r given the input query q as, L = − log p(r|q, θs , θp , θg ) Experiments Implementation Details We implement our shared module based on the Seq2seq model with pre-trained Glove embedding [Pennington et al., 2014] and LSTM unit, and use a 4-layer MLP for the private module1 . The dimension of word embedding, hidden state, and MLP’s output are set to 300. In CMAML, we pretrain the model for 10 epochs and re-train each model for 5 steps to prune the private network. The L-1 weight in the re-training stage is 0.001, and the threshold γ is 0.05. We follow other hyperparameter settings in Madotto et al. [2019]. (6) 5836 1 Code is available at https://github.com/zequnl/CMAML 5.3 Competing Methods • Pretrain-Only: We pre-train a unified dialogue generation model with data from all training tasks then directl"
2020.acl-main.517,N18-1202,0,0.0185717,"module. The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task. The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity. 1 Introduction Generative dialogue models often require a large amount of dialogues for training, and it is challenging to build models that can adapt to new domains or tasks with limited data. With recent advances in large-scale pre-training [Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018], we can first pre-train a generative model on large-scale dialogues from the non-target domains and then fine-tune on the task-specific data corpus [Wang et al., 2019a; Alt et al., 2019a; Klein, 2019]. While pre-training is beneficial, such models still require sufficient taskspecific data for fine-tuning. They cannot achieve satisfying performance when very few examples ∗ Corresponding author are given [Bansal et al., 2019]. Unfortunately, this is often the case in many dialogue generation scenarios. For example, in personal"
2020.acl-main.517,P19-1253,0,0.062106,"the response style of a user’s persona by just a few his or her dialogues [Madotto et al., 2019; Zhang et al., 2018]; in emotional dialogue generation, we need to generate a response catering to a new emoji using very few utterances containing this emoji [Zhou et al., 18; Zhou and Wang, 2018]. Hence, this is the focus of our paper - few-shot dialogue generation, i.e. training a generative model that can be generalized to a new task (domain) within k-shots of its dialogues. A few works have been proposed to consider few-shot dialogue generation as a meta-learning problem [Madotto et al., 2019; Qian and Yu, 2019; Mi et al., 2019]. They all rely on the popular modelagnostic meta-learning (MAML) method [Finn et al., 2017]. Take building personalized dialogue models as an example, previous work treats learning dialogues with different personas as different tasks [Madotto et al., 2019; Qian and Yu, 2019]. They employ MAML to find an initialization of model parameters by maximizing the sensitivity of the loss function when applied to new tasks. For a target task, its dialogue model is obtained by finetuning the initial parameters from MAML with its task-specific training samples. Despite the apparent succ"
2020.acl-main.517,P19-1132,0,0.105689,"n the input query q as, L = − log p(r|q, θs , θp , θg ) Experiments Implementation Details We implement our shared module based on the Seq2seq model with pre-trained Glove embedding [Pennington et al., 2014] and LSTM unit, and use a 4-layer MLP for the private module1 . The dimension of word embedding, hidden state, and MLP’s output are set to 300. In CMAML, we pretrain the model for 10 epochs and re-train each model for 5 steps to prune the private network. The L-1 weight in the re-training stage is 0.001, and the threshold γ is 0.05. We follow other hyperparameter settings in Madotto et al. [2019]. (6) 5836 1 Code is available at https://github.com/zequnl/CMAML 5.3 Competing Methods • Pretrain-Only: We pre-train a unified dialogue generation model with data from all training tasks then directly test it on the testing tasks. We try three base generation models: the Seq2seq [Bahdanau et al., 2014] and the Speaker model [Li et al., 2016b] and the Seq2SPG proposed in Section3.1. Speaker incorporates the task (user/emoji) embeddings in the LSTM cell, and the task embeddings of testing tasks are random parameters in this setting. • Finetune: We fine-tune the pre-trained models on each testin"
2020.acl-main.517,P19-1277,0,0.0173059,"odel-agnostic property of MAML. Thus, task differentiation always counts on fine-tuning, which only searches the best model for each task at the parameter level but not the model structure level. Meta-learning. Meta-learning has achieved promising results in many NLP problems recently due to its fast adaptation ability on a new task using very few training data [Yu et al., 2019; Wang et al., 2019b; Obamuyide and Vlachos, 2019b; Alt et al., 2019b]. In general, there are three categories of meta-learning methods: metric-based methods [Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018; Ye and Ling, 2019] which encode the samples into an embedding space along with a learned distance metric and then apply a matching algorithm, model-based methods [Santoro et al., 2016; Obamuyide and Vlachos, 2019a] which depend on the model structure design such as an external memory storage to facilitate the learning process, and optimization-based methods [Finn et al., 2017; Andrychowicz et al., 2016; Huang et al., 2018] which learn a good network initialization from which fine-tuning can converge to the optimal point for a new task with only a few examples. Methods belonging to the first two are proposed fo"
2020.acl-main.517,P18-1205,0,0.0717272,"Missing"
2020.acl-main.517,I17-2029,1,0.898905,"Missing"
2020.acl-main.517,P18-1104,0,\N,Missing
2020.ccl-1.83,K16-1002,0,0.519192,"attempt to learn mid-level representations, such as events (Martin et al., 2018), prompts (Fan et al., 2018), keywords (Yao et al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that appear frequently (Zhao et al., 2017) and this will lead to both high inter- and intra-story content repetition rates. On the other hand, a well-composed story also needs to contain vivid and diversified words. To address the issue of wording diversity, some studies have employed models based on variational autoencoder (VAE) (Kingma and Welling, 2013) or conditional variational autoencoder (CVAE) as a possible solution. It has been proved that, through learning distributed latent representation of the ent"
2020.ccl-1.83,P18-1082,0,0.113237,"ory generation is a challenging task since it requires generating texts which satisfy not only thematic consistency but also wording diversity. Despite that considerable efforts have been made in the past decades, the requirement of thematic consistency and wording diversity is still one of the main problems in the task of story generation. On the one hand, a well-composed story is supposed to contain sentences that are tightly connected with a given theme. To address this problem, most previous methods attempt to learn mid-level representations, such as events (Martin et al., 2018), prompts (Fan et al., 2018), keywords (Yao et al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that"
2020.ccl-1.83,P19-1254,0,0.0867221,"rating texts which satisfy not only thematic consistency but also wording diversity. Despite that considerable efforts have been made in the past decades, the requirement of thematic consistency and wording diversity is still one of the main problems in the task of story generation. On the one hand, a well-composed story is supposed to contain sentences that are tightly connected with a given theme. To address this problem, most previous methods attempt to learn mid-level representations, such as events (Martin et al., 2018), prompts (Fan et al., 2018), keywords (Yao et al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that appear frequently (Zhao et al., 2017) and this will lead to"
2020.ccl-1.83,N16-1014,0,0.0341933,"All initial weights are sampled from a uniform distribution [−0.08, 0.08]. The batch size is 80. Evaluation We utilize both automatic and human metrics to evaluate the performance of our method. BLUE Score. This metric is designed for calculating the word-overlap score between the golden texts and the generated ones (Papineni et al., 2002), and has been used in many previous story generation works (Yao et al., 2019; Li et al., 2019). Distinct Score. To measure the diversity of the generated stories, we employ this metric to compute the proportion of distinct n-grams in the generated outputs (Li et al., 2016). Note that the final distinct scores are scaled to [0, 100]. Inter- and intra-story repetition. These two metrics are proposed in (Yao et al., 2019) and used for calculating the inter- and intra-story tri-grams 1 repetition rates by sentences and for the whole stories. The final results are also scaled to [0, 100]. Human Evaluation. We also employ three metrics for human evaluation, i.e., Readability, Consistency, and Creativity. Their descriptions are shown in Table 1. We randomly sample 100 generated stories from each baseline model and our method and then perform pairwise comparisons betwe"
2020.ccl-1.83,D18-1423,1,0.913573,"-level representations, such as events (Martin et al., 2018), prompts (Fan et al., 2018), keywords (Yao et al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that appear frequently (Zhao et al., 2017) and this will lead to both high inter- and intra-story content repetition rates. On the other hand, a well-composed story also needs to contain vivid and diversified words. To address the issue of wording diversity, some studies have employed models based on variational autoencoder (VAE) (Kingma and Welling, 2013) or conditional variational autoencoder (CVAE) as a possible solution. It has been proved that, through learning distributed latent representation of the entire sentences, VA"
2020.ccl-1.83,N16-1098,0,0.305974,"ely Plan-CVAE, which first plans a keyword sequence and then generates a story based on the keyword sequence. In our method, the keywords planning strategy is used to improve thematic consistency while the CVAE module allows enhancing wording diversity. Experimental results on a benchmark dataset confirm that our proposed method can generate stories with both thematic consistency and wording novelty, and outperforms state-of-the-art methods on both automatic metrics and human evaluations. CC A narrative story is a sequence of sentences or words which describe a logically linked set of events (Mostafazadeh et al., 2016). Automatic story generation is a challenging task since it requires generating texts which satisfy not only thematic consistency but also wording diversity. Despite that considerable efforts have been made in the past decades, the requirement of thematic consistency and wording diversity is still one of the main problems in the task of story generation. On the one hand, a well-composed story is supposed to contain sentences that are tightly connected with a given theme. To address this problem, most previous methods attempt to learn mid-level representations, such as events (Martin et al., 20"
2020.ccl-1.83,P02-1040,0,0.108711,"e size of encoder, decoder, and prior network are 500, 500, 600 respectively. And the size of the latent variable z is set to 300. To train our model, we adopt the Adam (Kingma and Ba, 2015) optimization algorithm with an initial learning rate of 0.001 and gradient clipping of 5. All initial weights are sampled from a uniform distribution [−0.08, 0.08]. The batch size is 80. Evaluation We utilize both automatic and human metrics to evaluate the performance of our method. BLUE Score. This metric is designed for calculating the word-overlap score between the golden texts and the generated ones (Papineni et al., 2002), and has been used in many previous story generation works (Yao et al., 2019; Li et al., 2019). Distinct Score. To measure the diversity of the generated stories, we employ this metric to compute the proportion of distinct n-grams in the generated outputs (Li et al., 2016). Note that the final distinct scores are scaled to [0, 100]. Inter- and intra-story repetition. These two metrics are proposed in (Yao et al., 2019) and used for calculating the inter- and intra-story tri-grams 1 repetition rates by sentences and for the whole stories. The final results are also scaled to [0, 100]. Human Ev"
2020.ccl-1.83,W17-0911,0,0.0188795,"g diversity of the story. To evaluate our proposed method, we conduct experiments on a benchmark dataset, i.e., the Rocstories corpus (Mostafazadeh et al., 2016). Experimental results demonstrate that our introduced method can generate stories that are more preferable for human annotators in terms of thematic consistency and wording diversity, and meanwhile outperforms state-of-the-art methods on automatic metrics. CC L2 In recent years, neural network models have been demonstrated effective in natural language processing tasks (Mikolov et al., 2010; Sutskever et al., 2014; Rush et al., 2015; Roemmele et al., 2017; Liu et al., 2020; Yu et al., 2020). In story generation, previous studies have employed neural networks for enhancing the quality of generated content. Jain et al. (2017) explored generating coherent stories from independent short descriptions by using a sequence to sequence (S2S) architecture with a bidirectional RNN encoder and an RNN decoder. Since this model is insufficient for generating stories with consistent themes, to improve the thematic consistency of the generated stories, many other methods have been explored. Martin et al. (2018) argued that using events representations as the"
2020.ccl-1.83,D15-1044,0,0.036916,"to keep the wording diversity of the story. To evaluate our proposed method, we conduct experiments on a benchmark dataset, i.e., the Rocstories corpus (Mostafazadeh et al., 2016). Experimental results demonstrate that our introduced method can generate stories that are more preferable for human annotators in terms of thematic consistency and wording diversity, and meanwhile outperforms state-of-the-art methods on automatic metrics. CC L2 In recent years, neural network models have been demonstrated effective in natural language processing tasks (Mikolov et al., 2010; Sutskever et al., 2014; Rush et al., 2015; Roemmele et al., 2017; Liu et al., 2020; Yu et al., 2020). In story generation, previous studies have employed neural networks for enhancing the quality of generated content. Jain et al. (2017) explored generating coherent stories from independent short descriptions by using a sequence to sequence (S2S) architecture with a bidirectional RNN encoder and an RNN decoder. Since this model is insufficient for generating stories with consistent themes, to improve the thematic consistency of the generated stories, many other methods have been explored. Martin et al. (2018) argued that using events"
2020.ccl-1.83,D18-1462,0,0.0219582,"r and an RNN decoder. Since this model is insufficient for generating stories with consistent themes, to improve the thematic consistency of the generated stories, many other methods have been explored. Martin et al. (2018) argued that using events representations as the guidance for story generation is able to improve the thematic consistency of generated content. Fan et al. (2018) presented a hierarchical method that first generates a prompt from the title, and then a story is generated conditioned on the previously generated prompt. Following the idea of learning mid-level representations, Xu et al. (2018) proposed a skeleton-based model that first extracts skeleton from previous sentences, and then generates new sentences under the guidance of the skeleton. Similarly, Yao et al. (2019) explored using a storyline planning strategy for guiding the story generation process to ensure the output story can describe a consistent topic. Fan et al. (2019) further adopted a structure-based strategy that first generates sequences of predicates and arguments, and then outputs a story by filling placeholder entities. Although these methods have achieved promising results, most of them are implemented with"
2020.ccl-1.83,D16-1050,0,0.0173614,"r improving the wording diversity in story generation (Li et al., 2019). 2.2 Conditional Variational Autoencoder The Variational Auto-Encoder (VAE) model is proposed in (Kingma and Welling, 2013). Through forcing the latent variables to follow a prior distribution, VAE is able to generate diverse text successfully by randomly sampling from the latent space (Bowman et al., 2016). Conditional Variational AutoEncoder (CVAE), as a variant of VAE, can generate specific outputs conditioned on a given input. CVAE has been used in many other related text generation tasks, such as machine translation (Zhang et al., 2016), dialogue generation (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017), and poem composing (Yang et al., 2018; Li et al., 2018). Subsequently, in recent years, CVAE has begun to be applied in Proceedings of the 19th China National Conference on Computational Linguistics, pages 892-902, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics story generation task to tackle the common wording problem. Li et al. (2019) explored adopting CVAE to generate stories with no"
2020.ccl-1.83,P17-1061,0,0.283728,"al., 2019) or actions (Fan et al., 2019), to guide the sentences generation. Although these approaches have shown their encouraging effectiveness in improving the thematic consistency, most of them have no guarantee for the wording diversity. The main reason is that most of these methods are based on recurrent neural networks (RNNs), which tend to be entrapped within local word co-occurrences and cannot explicitly model holistic properties of sentences such as topic (Bowman et al., 2016; Li et al., 2018; Li et al., 2019). As a result, RNN tends to generate common words that appear frequently (Zhao et al., 2017) and this will lead to both high inter- and intra-story content repetition rates. On the other hand, a well-composed story also needs to contain vivid and diversified words. To address the issue of wording diversity, some studies have employed models based on variational autoencoder (VAE) (Kingma and Welling, 2013) or conditional variational autoencoder (CVAE) as a possible solution. It has been proved that, through learning distributed latent representation of the entire sentences, VAE can capture global features such as topics and high-level syntactic properties, and thus can generate novel"
2020.emnlp-main.272,2021.ccl-1.108,0,0.133441,"Missing"
2020.emnlp-main.272,P15-1152,0,0.0416797,"aper are three-fold: (1) proposal of a knowledge selection module for applying pre-trained language models to the task of knowledge-grounded dialogue generation; (2) proposal of an unsupervised approach in which learning of knowledge selection and fine-tuning of the pre-trained model are conducted in a joint manner; and (3) empirical verification of the effectiveness of the proposed method on benchmarks of knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is"
2020.emnlp-main.272,P19-1081,0,0.0228249,"t al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018). In this work, we study document-grounded dialogue generation. Rather than learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous"
2020.emnlp-main.272,I17-1047,0,0.0592179,"of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018). In this work, we study document-grounded dialogue generation. Rather than learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis"
2020.emnlp-main.272,N19-1035,0,0.0282509,"an learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lample and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al., 2019; Su et al., 2019; Sun et al., 2019a). In the context of dialogue generation, by fine-tuning GPT-2 (Radford et al., 2019) in different sizes on social media data, recent work has (Zhang et al., 2019c; Wolf et al., 2019) shown promising progress on conversation engagement and commonsense questionanswering. In this work, we further explore the application of pre-training to the task of open domain dialogue generation by equipping the pre-trained"
2020.emnlp-main.272,D19-1194,0,0.0334788,"et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018). In this work, we study document-grounded dialogue generation. Rather than learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language un"
2020.emnlp-main.272,P18-1204,0,0.0159904,"ialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018). In this work, we s"
2020.emnlp-main.272,P19-1538,1,0.866264,"the proposed method on benchmarks of knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Sh"
2020.emnlp-main.272,P18-1205,0,0.0519876,"knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018)"
2020.emnlp-main.272,P19-1499,0,0.294003,"pen domain dialogue generation. Prototypes ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Context I just discovered star trek and I really like watching star trek . Gene Roddenberry created it based upon science fiction and it is American media. ... If I remember Captain Kirk was not the original captain . The Star Trek Canon of the series an animated had 5 spin offs. I watched a little of the next generation but could not get into it like i did with the original show . Response These adventures went on but were short lived and six feature films. I think it’s worth it. such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry specific content for keeping the conversation going. While the giant language models can memorize enough patterns in language during pre-training, they only capture “average” semantics of the data (Zhang et al., 2019c). As a result, responses could still be bland or inappropriate when specific knowledge is required, as illustrated by the example in Table 1. The other line is to ground dialogue generation by extra knowledge such as unstructured documents (Zhao"
2020.emnlp-main.272,P17-1061,0,0.0304824,"unsupervised approach in which learning of knowledge selection and fine-tuning of the pre-trained model are conducted in a joint manner; and (3) empirical verification of the effectiveness of the proposed method on benchmarks of knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al.,"
2020.emnlp-main.272,D18-1076,0,0.166245,"d still be bland or inappropriate when specific knowledge is required, as illustrated by the example in Table 1. The other line is to ground dialogue generation by extra knowledge such as unstructured documents (Zhao et al., 2020). By the means, the documents (e.g., wiki articles) serve as content sources, and make a dialogue system knowledgeable regarding to a variety of concepts in discussion. However, collecting enough dialogues that are naturally grounded on documents for model training is not trivial. Although some benchmarks built upon crowd-sourcing have been released by recent papers (Zhou et al., 2018b; Dinan et al., 2019; Gopalakrishnan et al., 2019), the small training size 3377 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3377–3390, c November 16–20, 2020. 2020 Association for Computational Linguistics makes the generation models generalize badly on unseen topics (Dinan et al., 2019) and the cost of building such data also prevents from transferring the techniques proved on the benchmarks to new domains and new languages. Encouraged by the results on pre-training for dialogue generation and knowledge-grounded dialogue generation, and moti"
2020.emnlp-main.272,P19-1362,0,0.409394,"pen domain dialogue generation. Prototypes ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Context I just discovered star trek and I really like watching star trek . Gene Roddenberry created it based upon science fiction and it is American media. ... If I remember Captain Kirk was not the original captain . The Star Trek Canon of the series an animated had 5 spin offs. I watched a little of the next generation but could not get into it like i did with the original show . Response These adventures went on but were short lived and six feature films. I think it’s worth it. such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry specific content for keeping the conversation going. While the giant language models can memorize enough patterns in language during pre-training, they only capture “average” semantics of the data (Zhang et al., 2019c). As a result, responses could still be bland or inappropriate when specific knowledge is required, as illustrated by the example in Table 1. The other line is to ground dialogue generation by extra knowledge such as unstructured documents (Zhao"
2020.emnlp-main.272,P18-1102,0,0.0223918,"knowledge-grounded dialogue generation. 3378 2 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b). Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018)"
2020.emnlp-main.281,P83-1025,0,0.168344,"uence-to-sequence (seq2seq) models (Bordes et al., 2017; Eric and Manning, 2017; Eric et al., 2017), which have taken the state-of-the-art of TDSs to a new level. These methods map dialogue context to output responses directly without explicitly providing handcrafted features and NLU/DST/DP labels, thus reduce human effort and are easily adapted to new domains. Despite the effectiveness of previous studies, there are several technical challenges in building a TDS that is capable of retrieving accurate entries from the knowledge base (KB) and generating human-like responses. (1) Previous work (Carbonell, 1983) shows that users of TDSs tend to use succinct language which often omits entities or concepts made in previous utterances. However, seq2seq models often ignore how the conversation evolves as information progresses (Raghu et al., 2019) and thus result in generating incoherent and ungrammatical responses that are dominated by words appearing with high frequency in 3498 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3498–3507, c November 16–20, 2020. 2020 Association for Computational Linguistics the training data. (2) Seq2seq models suffer from ef"
2020.emnlp-main.281,P19-1258,0,0.0371921,"y of the neural network models has gained increasing attention. (Eric et al., 2017) proposed a copy augmented seq2seq model by copying relevant information directly from the KB information. Mem2Seq (Madotto et al., 2018) and GLMP (Wu et al., 2019) further augmented memory-based 3499 methods by incorporating copy mechanism (Gulcehre et al., 2016) to enable copying words from past dialog utterances or from KB when generating responses. Recently, separating memories for modeling dialog context and KB results are explored to improve the performance of TDSs (Raghu et al., 2019; Reddy et al., 2019; Chen et al., 2019). BossNet (Raghu et al., 2019) implicitly disentangled the language model from knowledge incorporation and thus enhanced the ability to copy unknown KB entries. Multi-level memory model (Reddy et al., 2019) represented the KB results using a multi-level memory instead of the form of triples. WMM2Seq (Chen et al., 2019) further employed a working memory to interact with two separated memories. Nevertheless, existing methods either achieve a good language model for the response generation or effective progress towards the KB modeling, but not both. 2.2 Student-teacher Learning Paradigm In parall"
2020.emnlp-main.281,W17-5506,0,0.258814,"erstanding (NLU), dialogue state tracking (DST), and dialogue policy (DP). A limitation of such pipelined design is that errors made in upper stream modules may propagate to downstream components, making it hard to identify and track the source of errors. In addition, these methods usually require a large number of handcrafted features and labels, which may restrict the expressive power and learnability of the models. To ameliorate the limitations with the conventional pipeline TDSs, great efforts have been made in designing deep neural network-based end-to-end solutions (Bordes et al., 2017; Eric et al., 2017; Madotto et al., 2018). Recent advances are overwhelmingly contributed by sequence-to-sequence (seq2seq) models (Bordes et al., 2017; Eric and Manning, 2017; Eric et al., 2017), which have taken the state-of-the-art of TDSs to a new level. These methods map dialogue context to output responses directly without explicitly providing handcrafted features and NLU/DST/DP labels, thus reduce human effort and are easily adapted to new domains. Despite the effectiveness of previous studies, there are several technical challenges in building a TDS that is capable of retrieving accurate entries from th"
2020.emnlp-main.281,E17-2075,0,0.272801,"ules may propagate to downstream components, making it hard to identify and track the source of errors. In addition, these methods usually require a large number of handcrafted features and labels, which may restrict the expressive power and learnability of the models. To ameliorate the limitations with the conventional pipeline TDSs, great efforts have been made in designing deep neural network-based end-to-end solutions (Bordes et al., 2017; Eric et al., 2017; Madotto et al., 2018). Recent advances are overwhelmingly contributed by sequence-to-sequence (seq2seq) models (Bordes et al., 2017; Eric and Manning, 2017; Eric et al., 2017), which have taken the state-of-the-art of TDSs to a new level. These methods map dialogue context to output responses directly without explicitly providing handcrafted features and NLU/DST/DP labels, thus reduce human effort and are easily adapted to new domains. Despite the effectiveness of previous studies, there are several technical challenges in building a TDS that is capable of retrieving accurate entries from the knowledge base (KB) and generating human-like responses. (1) Previous work (Carbonell, 1983) shows that users of TDSs tend to use succinct language which o"
2020.emnlp-main.281,P16-1014,0,0.138343,"t al., 2017; Madotto et al., 2018). However, as revealed by previous studies (Eric et al., 2017; Madotto et al., 2018), the performance of the seq2seq model deteriorates quickly with the increase of the length of the generated sequence. Therefore, how to improve the stability of the neural network models has gained increasing attention. (Eric et al., 2017) proposed a copy augmented seq2seq model by copying relevant information directly from the KB information. Mem2Seq (Madotto et al., 2018) and GLMP (Wu et al., 2019) further augmented memory-based 3499 methods by incorporating copy mechanism (Gulcehre et al., 2016) to enable copying words from past dialog utterances or from KB when generating responses. Recently, separating memories for modeling dialog context and KB results are explored to improve the performance of TDSs (Raghu et al., 2019; Reddy et al., 2019; Chen et al., 2019). BossNet (Raghu et al., 2019) implicitly disentangled the language model from knowledge incorporation and thus enhanced the ability to copy unknown KB entries. Multi-level memory model (Reddy et al., 2019) represented the KB results using a multi-level memory instead of the form of triples. WMM2Seq (Chen et al., 2019) further"
2020.emnlp-main.281,P84-1044,0,0.618577,"Missing"
2020.emnlp-main.281,P18-1133,0,0.0115033,"lete speciﬁc tasks with natural language. Conventional TDSs usually require a large number of handcrafted features, which may restrict the expressive power and learnability of the models (Williams and Young, 2007; Young et al., 2013). Inspired by the success of the sequence-tosequence (seq2seq) models in text generation, there are several studies that build TDSs with the seq2seq model in an end-to-end trainable way. These methods have shown promising results recently since they have a great ability to learn the latent representations of dialogue context and are easily adapted to a new domain (Lei et al., 2018; Eric et al., 2017; Madotto et al., 2018). However, as revealed by previous studies (Eric et al., 2017; Madotto et al., 2018), the performance of the seq2seq model deteriorates quickly with the increase of the length of the generated sequence. Therefore, how to improve the stability of the neural network models has gained increasing attention. (Eric et al., 2017) proposed a copy augmented seq2seq model by copying relevant information directly from the KB information. Mem2Seq (Madotto et al., 2018) and GLMP (Wu et al., 2019) further augmented memory-based 3499 methods by incorporating copy mec"
2020.emnlp-main.281,D16-1233,0,0.221518,"Missing"
2020.emnlp-main.281,D15-1166,0,0.266689,"ectively), which can be viewed as experts towards the goals. Then, we employ GAN to learn the student network, where the generator is the student network to produce dialogue responses, and two discriminators distinguish the learned output responses from student and teacher networks. Next, we will introduce the three networks and the GAN-based student-teacher learning paradigm in detail. 3.1 Student Network The student network a task-oriented dialogue system, which is responsible for both inquiring KB and generating human-like responses. In this study, the sequence-to-sequence (seq2seq) model (Luong et al., 2015) is used as the backbone to implement the student network. The seq2seq model additionally consists of a dialogue memory module and a KB memory module to store the information from the dialogue context and the retrieved KB entities, respectively. Encoder Each input token in the dialogue context is converted to a ﬁxed-length vector via an embedding layer. The input embedding sequence then goes into a layer of the bidirectional gated recurrent unit (BiGRU) (Chung et al., 2014) to learn the contextualized representation of the dialogue context, which is then passed into the dialogue memory. Dialog"
2020.emnlp-main.281,P18-1136,0,0.337746,"ialogue state tracking (DST), and dialogue policy (DP). A limitation of such pipelined design is that errors made in upper stream modules may propagate to downstream components, making it hard to identify and track the source of errors. In addition, these methods usually require a large number of handcrafted features and labels, which may restrict the expressive power and learnability of the models. To ameliorate the limitations with the conventional pipeline TDSs, great efforts have been made in designing deep neural network-based end-to-end solutions (Bordes et al., 2017; Eric et al., 2017; Madotto et al., 2018). Recent advances are overwhelmingly contributed by sequence-to-sequence (seq2seq) models (Bordes et al., 2017; Eric and Manning, 2017; Eric et al., 2017), which have taken the state-of-the-art of TDSs to a new level. These methods map dialogue context to output responses directly without explicitly providing handcrafted features and NLU/DST/DP labels, thus reduce human effort and are easily adapted to new domains. Despite the effectiveness of previous studies, there are several technical challenges in building a TDS that is capable of retrieving accurate entries from the knowledge base (KB) a"
2020.emnlp-main.281,P02-1040,0,0.107444,"14.1 14.79 17.35 16.80 17.23 17.05 Ent. F1 10.3 19.9 22.7 33.4 35.9 53.7 59.97 55.38 51.84 51.49 55.88 Sch.F1 9.7 23.4 26.9 49.3 50.2 54.5 69.56 63.50 60.71 61.18 67.53 Wea.F1 14.1 25.6 26.7 32.8 34.5 52.2 62.58 64.09 62.67 63.78 63.71 Nav.F1 7.0 10.8 14.9 20.0 21.6 55.6 52.98 45.90 40.76 39.14 44.86 Table 2: Automatic evaluation results on In-Car Assistant dataset. local memory pointer network (GLMP) (Wu et al., 2019). Automatic Evaluation Metrics Following previous works (Madotto et al., 2018; Wu et al., 2019), we evaluate TTOS and compared methods on two automatic evaluation metrics: BLEU (Papineni et al., 2002) and entity F1 (Madotto et al., 2018) scores. BLEU calculates n-gram overlaps between the generated response and the gold response, which could gauge the model’s ability to accurately generate the dialogue patterns seen in our data. BLEU shows a comparatively strong correlation with the human assessment on taskoriented systems (Sharma et al., 2017). Entity F1 is computed by micro-averaging the precision and recall over KB entities in the entire set of system responses, which evaluates the ability of the TDSs to generate relevant entities to accomplish speciﬁc tasks by inquiring the provided KB"
2020.emnlp-main.281,D19-1013,0,0.224037,"Missing"
2020.emnlp-main.281,N19-1126,0,0.0306866,"Missing"
2020.emnlp-main.281,N19-1375,0,0.743962,"the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3498–3507, c November 16–20, 2020. 2020 Association for Computational Linguistics the training data. (2) Seq2seq models suffer from effectively reasoning over and incorporating KB information (Madotto et al., 2018). It is difﬁcult to encode and decode the knowledge from a large and dynamic KB, making the response generation unstable. In addition, typically, a shared memory is used for both dialogue context and KB triples, making the TDSs struggle to reason over the two forms of data. Although some previous methods (Reddy et al., 2019) leverage separate memories for modeling dialogue context and KB facts, they either focus on capturing the dialogue patterns or retrieving accurate KB entities, but not both. One possible solution to the aforementioned problems is to explicitly encourage the seq2seq model to learn dialogue patterns and model the exterior KB knowledge retrieval with separate guidance for each. In this study, we propose a “Two-Teacher OneStudent” learning framework (TTOS) for building a high-quality TDS (student), where a student network is encouraged to integrate the knowledge from two expert teacher networks."
2020.emnlp-main.313,K16-1002,0,0.0224319,"ucts. (2) We build the MGenNet to generate the copywriting of each product based on the multi-agent communication framework. (3) We combine the above networks as an end-to-end SMG Net which can generate attractive AD posts. 2 Related Work Text generation. Text generation has become one of the hottest subfields of natural language processing. Previous researches mainly focus on several popular text generation tasks, such as dialogue generation (Serban et al., 2016; Tao et al., 2018a; Hu et al., 2019; Chan et al., 2019b) and story generation (Xu et al., 2018; Li et al., 2019; Yao et al., 2019). Bowman et al. (2016) are proposed to improve wording novelty. Serban et al. (2017) target to the intra-sentence consistency and thematic consistency is improved by Fan et al. (2018); Litschko et al. (2018). Besides, text generation from different data formats have also been widely studied in recent years, e.g., table-to-text generation (Liu et al., 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning framework for multi-agent communication. There are several previous works built on the CommNet fra"
2020.emnlp-main.313,N18-1150,0,0.097371,", 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning framework for multi-agent communication. There are several previous works built on the CommNet framework, for example, researchers Add & Norm use the multi-agent communication method to play FFNal., 2017). Morthe starcraft games (Vinyals et datch and Abbeel (2018) deal with natural language Add & Norm processing tasks such as machine translation and W-Sum sentiment analysis with the multi-agent communiV-Attn cation method. Celikyilmaz et al. (2018) present K V the first study using theQ multi-agent framework for summarization. Product descriptions generation. Product description copywriting is critical for the e-commerce platform, and automatically generating the product description copywriting has attracted considerable interest from both academia and industry because of its importance. Wang et al. (2017) first focus on the product description generation task and incorporates the preset template to generate product descriptions automatically. With the development of deep learning, Zhang et al. (2019) proposed a pointer-based generation"
2020.emnlp-main.313,D19-1501,1,0.679067,"opriate combination of the products based on the post topic and the relationship among the products. (2) We build the MGenNet to generate the copywriting of each product based on the multi-agent communication framework. (3) We combine the above networks as an end-to-end SMG Net which can generate attractive AD posts. 2 Related Work Text generation. Text generation has become one of the hottest subfields of natural language processing. Previous researches mainly focus on several popular text generation tasks, such as dialogue generation (Serban et al., 2016; Tao et al., 2018a; Hu et al., 2019; Chan et al., 2019b) and story generation (Xu et al., 2018; Li et al., 2019; Yao et al., 2019). Bowman et al. (2016) are proposed to improve wording novelty. Serban et al. (2017) target to the intra-sentence consistency and thematic consistency is improved by Fan et al. (2018); Litschko et al. (2018). Besides, text generation from different data formats have also been widely studied in recent years, e.g., table-to-text generation (Liu et al., 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning"
2020.emnlp-main.313,D19-1201,1,0.628667,"opriate combination of the products based on the post topic and the relationship among the products. (2) We build the MGenNet to generate the copywriting of each product based on the multi-agent communication framework. (3) We combine the above networks as an end-to-end SMG Net which can generate attractive AD posts. 2 Related Work Text generation. Text generation has become one of the hottest subfields of natural language processing. Previous researches mainly focus on several popular text generation tasks, such as dialogue generation (Serban et al., 2016; Tao et al., 2018a; Hu et al., 2019; Chan et al., 2019b) and story generation (Xu et al., 2018; Li et al., 2019; Yao et al., 2019). Bowman et al. (2016) are proposed to improve wording novelty. Serban et al. (2017) target to the intra-sentence consistency and thematic consistency is improved by Fan et al. (2018); Litschko et al. (2018). Besides, text generation from different data formats have also been widely studied in recent years, e.g., table-to-text generation (Liu et al., 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning"
2020.emnlp-main.313,D15-1166,0,0.00704384,"in the j-th step as: sum{Iji } . Iˆji = M −1 (11) The Iˆji contains all the information of other agents, and the i-th agent can get more information as the prior for generating. After obtaining the information Iˆji , we attach it as an extra input to the corresponding agent. We use the below equation to express this process yt0 = ([yt ; Iˆji ; at·t ] · Wy ) + by , a·t+1 = Agent(a·t , yt0 ), (12) where Wy , by are trainable parameters. yt is the input of agent at t-th time step and the at·t is the attention vector which is calculated from the corresponding product RNN encoder status as same as Luong et al. (2015). Then, we use a linear layer to obtain the generated word. Finally, we can use the beam search algorithm to get all copywriting Cˆ = {ˆ ud1 , u ˆd2 , · · · , u ˆdM } which d ,w d ,··· ,w d u ˆdi = {w ˆi,1 ˆi,2 ˆi,L n }. 3822 c,i 4.3 Training Objection Table 1: Criteria of human evaluation. To start with, we combine SelectNet and MGenNet as an end-to-end framework. We launch the following objective to minimize the MLE loss between the ground truth and generated copywriting. Meanwhile, we minimize the loss between selected product ground truth in real multi-product AD post, and the objective fu"
2020.emnlp-main.313,P18-1082,0,0.156792,"ral encoder-decoder framework shows remarkable effects on various text generation tasks, e.g., sum∗ This work was done while Z. Chan was an intern at Alibaba Group. Y. Zhang works at Ant Group now. † Corresponding Author: Rui Yan (ruiyan@pku.edu.cn). 1 https://www.amazon.com/ https://www.taobao.com/ 3 https://www.jd.com/ 2 3818 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3818–3829, c November 16–20, 2020. 2020 Association for Computational Linguistics marization (See et al., 2017; Chen et al., 2019b; Gao et al., 2019a, 2020), story generation (Fan et al., 2018; Li et al., 2019; Yao et al., 2019) and so on. The researchers from academia and industry begin to explore how to generate product advertising copywriting through deep learning methods. Zhang et al. (2019) propose a pointer-generator model to generate the product advertising copywriting whose patterns are controlled. Chen et al. (2019a) explore a new way to generate personalized product copywriting by enhancing the transformer with an extra knowledge base. However, all the previous studies focus on copywriting generation for a single product. In such case, consumers need to compare among prod"
2020.emnlp-main.313,P08-1028,0,0.0799334,"Evaluation Metrics As mentioned in Section 3, we ignore the order of products in the post and regard each product as equal. Hence, traditional metrics such as BLEU and ROUGE, are unsuitable in this scenario. To evaluate the results of the generated AD post, we adopt the following widely used metrics. Embedding Metrics. To obtain semantic matches between the generated copywriting and ground-truth, we perform evaluation using the embedding metrics. Following Gao et al. (2019b), we calculate three measures: 1) Average, cosine similarity between the averaged word embeddings in the two utterances (Mitchell and Lapata, 2008); 2) Greedy, i.e., greedily matching words in two utterances based on the cosine similarities, and the total scores are then averaged across all words (Rus and Lintean, 2012); 3) Extrema, cosine similarity between the largest values among the word embeddings in the two utterances (Forgues et al., 2014). The used word2vec embedding is trained by ourselves because there is no open-access e-commerce embedding. Distinct Metrics. We use distinct scores to reflect the diversity of the copywriting. Dist-n is defined as the ratio of unique n-grams (n = 1,2,3,4) overall n-grams in the generated copywri"
2020.emnlp-main.313,W12-2018,0,0.0897866,"unsuitable in this scenario. To evaluate the results of the generated AD post, we adopt the following widely used metrics. Embedding Metrics. To obtain semantic matches between the generated copywriting and ground-truth, we perform evaluation using the embedding metrics. Following Gao et al. (2019b), we calculate three measures: 1) Average, cosine similarity between the averaged word embeddings in the two utterances (Mitchell and Lapata, 2008); 2) Greedy, i.e., greedily matching words in two utterances based on the cosine similarities, and the total scores are then averaged across all words (Rus and Lintean, 2012); 3) Extrema, cosine similarity between the largest values among the word embeddings in the two utterances (Forgues et al., 2014). The used word2vec embedding is trained by ourselves because there is no open-access e-commerce embedding. Distinct Metrics. We use distinct scores to reflect the diversity of the copywriting. Dist-n is defined as the ratio of unique n-grams (n = 1,2,3,4) overall n-grams in the generated copywriting. Following Gu et al. (2018), we define intra-dist as the average of distinct values within each copywriting and inter-dist as the distinct value among all copywriting. H"
2020.emnlp-main.313,P17-1099,0,0.0251345,"ed on the predefined template. With the surge of deep learning techniques, the neural encoder-decoder framework shows remarkable effects on various text generation tasks, e.g., sum∗ This work was done while Z. Chan was an intern at Alibaba Group. Y. Zhang works at Ant Group now. † Corresponding Author: Rui Yan (ruiyan@pku.edu.cn). 1 https://www.amazon.com/ https://www.taobao.com/ 3 https://www.jd.com/ 2 3818 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3818–3829, c November 16–20, 2020. 2020 Association for Computational Linguistics marization (See et al., 2017; Chen et al., 2019b; Gao et al., 2019a, 2020), story generation (Fan et al., 2018; Li et al., 2019; Yao et al., 2019) and so on. The researchers from academia and industry begin to explore how to generate product advertising copywriting through deep learning methods. Zhang et al. (2019) propose a pointer-generator model to generate the product advertising copywriting whose patterns are controlled. Chen et al. (2019a) explore a new way to generate personalized product copywriting by enhancing the transformer with an extra knowledge base. However, all the previous studies focus on copywriting g"
2020.emnlp-main.313,J19-1005,0,0.0172083,"odel based on Transformer. We adapt the model for our scenario by removing the personal module. We cannot generate copywriting for each product individually when we don’t know the corresponding relation. To apply these baseline models to our scenario, we separate the description text of each product in the post with a unique character “<SOP>” and then concatenate the whole text as the final target for training. To analyze our SelectNet, we select several models for text matching as baselines. 1). RNN (Liu et al., 2016). A model uses RNN to model input as the hidden state to classify. 2). SCN (Wu et al., 2019). A classic retrieval model for response selection. The model lets the post title interact with product information and transforms interaction matrices into a matching vector with CNN. 3). MRFN (Tao et al., 2019). A strong retrieval3823 Table 2: The automatic metric result of our model and baselines. Since the number of sentences generated by ConvSeq is extremely small, so evaluating the intra-dist metric is meaningless, and we omit the intra-dist score. The results of our methodologies are significant with p-value <0.05 measured by t-test over the best baseline. Models Embedding Metrics Inter"
2020.emnlp-main.313,D18-1462,0,0.0286482,"on the post topic and the relationship among the products. (2) We build the MGenNet to generate the copywriting of each product based on the multi-agent communication framework. (3) We combine the above networks as an end-to-end SMG Net which can generate attractive AD posts. 2 Related Work Text generation. Text generation has become one of the hottest subfields of natural language processing. Previous researches mainly focus on several popular text generation tasks, such as dialogue generation (Serban et al., 2016; Tao et al., 2018a; Hu et al., 2019; Chan et al., 2019b) and story generation (Xu et al., 2018; Li et al., 2019; Yao et al., 2019). Bowman et al. (2016) are proposed to improve wording novelty. Serban et al. (2017) target to the intra-sentence consistency and thematic consistency is improved by Fan et al. (2018); Litschko et al. (2018). Besides, text generation from different data formats have also been widely studied in recent years, e.g., table-to-text generation (Liu et al., 2018), which can fit various data formats in 3819 real-world scenarios. Multi-Agent Communication. CommNet proposed by Sukhbaatar et al. (2016) is the first deep learning framework for multi-agent communication."
2020.emnlp-main.313,P19-1465,0,0.013975,"951 13.59 20.77 25.23 28.38 33.83 31.07 39.79 33.41 45.48 PCPG KOBE 0.8830 0.8783 540.41 539.23 0.3713 0.4023 1.409 1.523 3.943 5.334 7.423 11.34 10.43 18.32 22.43 26.46 29.98 37.43 36.80 43.23 40.31 53.84 S-MGC S-SG S-MG 0.9438 0.8774 0.9428 560.45 566.86 558.62 0.4481 0.4280 0.4440 1.763 1.294 1.713 8.051 4.059 7.502 18.37 8.479 17.21 28.30 12.71 26.60 44.66 24.76 44.49 66.22 33.06 65.97 73.57 38.72 73.26 78.96 44.09 78.64 based model in response selection. Tao et al. (2019) encode the interaction between two texts from multiple kinds of representations and study how to fuse them. 4). SETM (Yang et al., 2019). It lets the post title interact with product text and uses the representation to obtain a weight matrix for each word. 5). Self-Attn (Vaswani et al., 2017). We use the self-attention mechanism to capture the relationship among the product candidates for product selection. We also conduct the exploration of the MGenNet, and the setting is shown as follows 1). S-MGC . Our proposed multi-agent generation framework with communication strategy. 2). S-MG. The original multi-agent generation framework without communication. 3). S-SG. We replace the MGenNet with a pretrained Transformer decoder and"
2020.emnlp-main.313,D19-1011,0,0.0278928,"y these baseline models to our scenario, we separate the description text of each product in the post with a unique character “<SOP>” and then concatenate the whole text as the final target for training. To analyze our SelectNet, we select several models for text matching as baselines. 1). RNN (Liu et al., 2016). A model uses RNN to model input as the hidden state to classify. 2). SCN (Wu et al., 2019). A classic retrieval model for response selection. The model lets the post title interact with product information and transforms interaction matrices into a matching vector with CNN. 3). MRFN (Tao et al., 2019). A strong retrieval3823 Table 2: The automatic metric result of our model and baselines. Since the number of sentences generated by ConvSeq is extremely small, so evaluating the intra-dist metric is meaningless, and we omit the intra-dist score. The results of our methodologies are significant with p-value <0.05 measured by t-test over the best baseline. Models Embedding Metrics Inter-Distinct Intra-Distinct Average Greedy Extrema Dist-1 Dist-2 Dist-3 Dist-4 Dist-1 Dist-2 Dist-3 Dist-4 Seq2seq ConvSeq Transformer 0.9197 0.6049 0.8662 548.69 326.99 537.69 0.4293 0.1123 0.3941 0.937 1.308 1.473"
2020.emnlp-main.313,I17-2032,0,0.137962,"ge factor in e-commerce, and well-written advertising copywriting can encourage consumers to understand further and purchase products. However, there is an important restriction factor for traditional advertising copywriting, i.e., the writing efficiency of human copywriters cannot match the growth rate of new products. Many e-commerce websites, such as Amazon1 , Taobao2 and JD3 , have billions of products, so it is impossible to write all copywriting manually. To address this issue, researchers pay more and more attention to the automatic advertising copywriting generation. The initial work (Wang et al., 2017) on automatic advertising copywriting generation is based on the predefined template. With the surge of deep learning techniques, the neural encoder-decoder framework shows remarkable effects on various text generation tasks, e.g., sum∗ This work was done while Z. Chan was an intern at Alibaba Group. Y. Zhang works at Ant Group now. † Corresponding Author: Rui Yan (ruiyan@pku.edu.cn). 1 https://www.amazon.com/ https://www.taobao.com/ 3 https://www.jd.com/ 2 3818 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3818–3829, c November 16–20, 2020. 2020"
2020.emnlp-main.752,D19-1501,1,0.845708,"ractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from 6 candidates. Following their work, Zhu et al. (2020) added a multimodal objective function to use the loss from the textual summary generation and the image selection. However, in the real-world application, we usually need to choose the cover figure for a continuous video consisting of hundreds"
2020.emnlp-main.752,D18-1442,1,0.931904,"textual summary with video cover simultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the h"
2020.emnlp-main.752,D19-1388,1,0.809996,"s on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from 6 candidates. Following their work, Zhu et al. (2020) a"
2020.emnlp-main.752,D19-1117,0,0.0128247,"Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from 6 candidates. Following their work, Z"
2020.emnlp-main.752,P18-1013,0,0.0227106,"f our data contains an article with a textual summary and a video with a cover picture. The average video duration is one minute and the frame rate of video is 25 fps. For the text part, the average length of article is 96.84 words and the average length of textual summary is 11.19 words. Overall, there are 184,920 samples in the dataset, which is split into a training set of 180,000 samples, a validation set of 2,460 samples, and a test set of 2,460 samples. 5.2 R-2 R-L extractive summarization Lead TextRank 16.2 13.7 5.3 4.0 13.9 12.5 abstractive summarization PG (See et al., 2017) Unified (Hsu et al., 2018) GPG (Shen et al., 2019) 19.4 23.0 20.1 6.8 6.0 4.5 17.4 20.9 17.3 our models DIMS 25.1 9.6 23.2 Table 1: Rouge scores comparison with traditional textual summarization baselines. Comparisons We compare our proposed method against summarization baselines and VQA baselines. Traditional Textual Summarization baselines: Lead: selects the first sentence of article as the textual summary (Nallapati et al., 2017). TexkRank: a graph-based extractive summarizer which adds sentences as nodes and uses edges to weight similarity (Mihalcea and Tarau, 2004). PG: a sequence-to-sequence framework combined wi"
2020.emnlp-main.752,D17-1114,0,0.149343,"module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset1 show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations. 1 Figure 1: An example of video-based multimodal summarization with multimodal output. Introduction Existing experiments (Li et al., 2017) have proven that multimodal news can significantly improve users’ sense of satisfaction for informativeness. As one of these multimedia data forms, introducing news events with video and textual descriptions is ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding author. 1 https://github.com/yingtaomj/VMSMO † becoming increasingly popular, and has been employed as the main form of news reporting by news media including BBC, Weibo, CNN, and Daily Mail. An illustration is shown in Figure 1, where the news contains a video with a cover picture and a full news article with a sh"
2020.emnlp-main.752,P19-1500,0,0.0406666,"Missing"
2020.emnlp-main.752,D19-1300,0,0.0114807,"th video cover simultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal"
2020.emnlp-main.752,W04-3252,0,0.0712491,"abstractive summarization PG (See et al., 2017) Unified (Hsu et al., 2018) GPG (Shen et al., 2019) 19.4 23.0 20.1 6.8 6.0 4.5 17.4 20.9 17.3 our models DIMS 25.1 9.6 23.2 Table 1: Rouge scores comparison with traditional textual summarization baselines. Comparisons We compare our proposed method against summarization baselines and VQA baselines. Traditional Textual Summarization baselines: Lead: selects the first sentence of article as the textual summary (Nallapati et al., 2017). TexkRank: a graph-based extractive summarizer which adds sentences as nodes and uses edges to weight similarity (Mihalcea and Tarau, 2004). PG: a sequence-to-sequence framework combined with attention mechanism and pointer network (See et al., 2017). Unified: a model which combines the strength of extractive and abstractive summarization (Hsu et al., 2018). GPG: Shen et al. (2019) proposed to generate textual summary by “editing” pointed tokens instead of hard copying. Multimodal baselines: How2: a model proposed to generate textual summary with video information (Palaskar et al., 2019). Synergistic: a image-question-answer synergistic network to value the role of the answer for precise visual dialog(Guo et al., 2019). PSAC: a m"
2020.emnlp-main.752,N18-1158,0,0.018938,"article, and generates textual summary with video cover simultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual s"
2020.emnlp-main.752,P19-1659,0,0.0412125,"Missing"
2020.emnlp-main.752,P17-1099,0,0.479182,"and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from"
2020.emnlp-main.752,D19-1390,0,0.0303845,"Missing"
2020.emnlp-main.752,D19-1011,0,0.0278561,"extual summary and used coverage to help select picture (Zhu et al., 2018). MOF: the model based on MSMO which added consideration of image accuracy as another loss (Zhu et al., 2020). 5.3 R-1 Evaluation Metrics The quality of generated textual summary is evaluated by standard full-length Rouge F1 (Lin, 2004) following previous works (See et al., 2017; Chen et al., 2018). R-1, R-2, and R-L refer to unigram, bigrams, and the longest common subsequence respectively. The quality of chosen cover frame is evaluated by mean average precision (MAP) (Zhou et al., 2018) and recall at position (Rn @k) (Tao et al., 2019). Rn @k measures if the positive sample is ranked in the top k positions of n candidates. 5.4 Implementation Details We implement our experiments in Tensorflow (Abadi et al., 2016) on an NVIDIA GTX 1080 Ti GPU. The code for our model is available online2 . For all models, we set the word embedding dimension and the hidden dimension to 128. The encoding step is set to 100, while the minimum decoding step is 10 and the maximum step is 30. For video preprocessing, we extract one of every 120 frames to obtain 10 frames as cover candidates. All candidates are resized to 128x64. We regard the frame"
2020.emnlp-main.752,D19-1304,0,0.017723,"ons. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summarization with multimodal output is relatively less explored. Zhu et al. (2018) proposed to jointly generate textual summary and select the most relevant image from 6 candidates. Follo"
2020.emnlp-main.752,D19-1298,0,0.0778183,"ultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on generating better textual summaries with the help of multimodal input. Multimodal summariz"
2020.emnlp-main.752,D18-1088,0,0.0159518,"semantic meaning of article, and generates textual summary with video cover simultaneously. • We construct a large-scale dataset for VMSMO, and experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations. 2 Related Work Our research builds on previous works in three fields: text summarization, multimodal summarization, and visual question answering. Text Summarization. Our proposed task bases on text summarization, the methods of which can be divided into extractive and abstractive methods (Gao et al., 2020b). Extractive models (Zhang et al., 2018; Narayan et al., 2018; Chen et al., 2018; Luo et al., 2019; Xiao and Carenini, 2019) directly pick sentences from article and regard the aggregate of them as the summary. In contrast, abstractive models (Sutskever et al., 2014; See et al., 2017; Wenbo et al., 2019; Gui et al., 2019; Gao et al., 2019a; Chen et al., 2019a; Gao et al., 2019b) generate a summary from scratch and the abstractive summaries are typically less redundant. Multimodal Summarization. A series of works (Li et al., 2017, 2018; Palaskar et al., 2019; Chan et al., 2019; Chen et al., 2019b; Gao et al., 2020a) focused on gener"
2020.emnlp-main.752,P18-1103,0,0.0195082,"id attention to text and images during generating textual summary and used coverage to help select picture (Zhu et al., 2018). MOF: the model based on MSMO which added consideration of image accuracy as another loss (Zhu et al., 2020). 5.3 R-1 Evaluation Metrics The quality of generated textual summary is evaluated by standard full-length Rouge F1 (Lin, 2004) following previous works (See et al., 2017; Chen et al., 2018). R-1, R-2, and R-L refer to unigram, bigrams, and the longest common subsequence respectively. The quality of chosen cover frame is evaluated by mean average precision (MAP) (Zhou et al., 2018) and recall at position (Rn @k) (Tao et al., 2019). Rn @k measures if the positive sample is ranked in the top k positions of n candidates. 5.4 Implementation Details We implement our experiments in Tensorflow (Abadi et al., 2016) on an NVIDIA GTX 1080 Ti GPU. The code for our model is available online2 . For all models, we set the word embedding dimension and the hidden dimension to 128. The encoding step is set to 100, while the minimum decoding step is 10 and the maximum step is 30. For video preprocessing, we extract one of every 120 frames to obtain 10 frames as cover candidates. All cand"
2020.emnlp-main.752,D18-1448,0,0.336754,"and has been employed as the main form of news reporting by news media including BBC, Weibo, CNN, and Daily Mail. An illustration is shown in Figure 1, where the news contains a video with a cover picture and a full news article with a short textual summary. In such a case, automatically generating multimodal summaries, i.e., choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time and readers make decisions more effectively. There are several works focusing on multimodal summarization. The most related work to ours is (Zhu et al., 2018), where they propose the task of generating textual summary and picking the most representative picture from 6 input candidates. However, in real-world applications, the input is usually a video consisting of hundreds of frames. Consequently, the temporal dependency in a video cannot be simply modeled by static encoding methods. Hence, in this work, we propose a novel task, Video-based Multimodal Summarization with Multimodal Output (VMSMO), which selects cover frame from news video and generates textual summary of the news article in the meantime. 9360 Proceedings of the 2020 Conference on Em"
2021.acl-long.343,N19-1423,0,0.11829,"ory (excluding the latest query) are symmetric in terms of the information they convey, and we assume that the dialogue history can be regarded as another format of background knowledge for response prediction. Based on the above intuition, in this paper, we consider decomposing the training of the grounded response selection task into several sub-tasks, and joint learning all those tasks in a unified model. To take advantage of the recent breakthrough on pretraining for natural language tasks, we build the grounded response matching models on the basis of a pre-trained language model (PLMs) (Devlin et al., 2019; Yang et al., 2019), which are trained with large-scale unstructured documents from the web. On this basis, we further train the PLMs with query-passage matching task, query-dialogue history matching task, and multi-turn response matching task jointly. The former two tasks could help the model not only in knowledge selection but also in knowledge (and dialogue history) comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history). By this means, the model can be learned to select relevant knowledge and distingu"
2021.acl-long.343,D19-1193,0,0.130158,"Zhang et al., 2018; Dinan et al., 2019). For example, Zhang et al. (2018) build a persona-based conversation data set that employs the interlocutor’s profile as the background knowledge; Zhou et al. (2018a) publish a data where conversations are grounded in articles about popular movies; Dinan et al. (2019) release another documentgrounded data with Wiki articles covering a wide range of topics. Meanwhile, several retrievalbased knowledge-grounded dialogue models are proposed, such as document-grounded matching network (DGMN) (Zhao et al., 2019) and dually interactive matching network (DIM) (Gu et al., 2019) which let the dialogue context and all knowledge entries interact with the response candidate respectively via the cross-attention mechanism. Gu et al. (2020b) further propose to pre-filter the context and the knowledge and then use the filtered context and knowledge to perform the matching with the response. Besides, with the help of gold knowledge index annotated by human wizards, Dinan et al. (2019) consider joint learning the knowledge selection and response matching in a multi-task manner or training a two-stage model. 3 Model In this section, we first formalize the knowledgegrounded res"
2021.acl-long.343,2020.findings-emnlp.127,0,0.16235,"teraction network (IoI) (Tao et al., 2019) and multi-hop selector network (MSN) (Yuan et al., 2019). More recently, pre-trained language models (Devlin et al., 2019; Yang et al., 2019) have shown significant benefits for various NLP tasks, and some researchers have tried to apply them on multi-turn response selection. Vig and Ramea (2019) exploit BERT to represent each utteranceresponse pair and fuse these representations to 4447 calculate the matching score; Whang et al. (2020) and Xu et al. (2020) treat the context as a long sequence and conduct context-response matching with BERT. Besides, Gu et al. (2020a) integrate speaker embeddings into BERT to improve the utterance representation in multi-turn dialogue. To bridge the gap of the knowledge between the human and the machine, researchers have investigated into grounding dialogue agents with unstructured background knowledge (Ghazvininejad et al., 2018; Zhang et al., 2018; Dinan et al., 2019). For example, Zhang et al. (2018) build a persona-based conversation data set that employs the interlocutor’s profile as the background knowledge; Zhou et al. (2018a) publish a data where conversations are grounded in articles about popular movies; Dinan"
2021.acl-long.343,N16-1014,0,0.0421967,"ounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training. 1 Introduction Along with the very recent prosperity of artificial intelligence empowered conversation systems in the spotlight, many studies have been focused on building human-computer dialogue systems (Wen et al., 2017; Zhang et al., 2020) with either retrievalbased methods (Wang et al., 2013; Wu et al., 2017; ∗ † Equal Contribution. Corresponding author: Rui Yan (ruiyan@ruc.edu.cn). Whang et al., 2020) or generation-based methods (Li et al., 2016; Serban et al., 2016; Zhang et al., 2020), which both predict the response with only the given context. In fact, unlike a person who may associate the conversation with the background knowledge in his or her mind, the machine can only capture limited information from the query message itself. As a result, it is difficult for a machine to properly comprehend the query, and to predict a proper response to make it more engaging. To bridge the gap of the knowledge between the human and the machine, researchers have begun to simulating this motivation by grounding dialogue agents with background k"
2021.acl-long.343,W15-4640,0,0.0207331,"selection and response matching. • We achieve a comparable performance of response selection with several existing models learned from crowd-sourced training sets. 2 Related Work Early studies of retrieval-based dialogue focus on single-turn response selection where the input of a matching model is a message-response pair (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015). Recently, researchers pay more attention to multiturn context-response matching and usually adopt the representation-matching-aggregation paradigm to build the model. Representative methods include the dual-LSTM model (Lowe et al., 2015), the sequential matching network (SMN) (Wu et al., 2017), the deep attention matching network (DAM) (Zhou et al., 2018b), interaction-overinteraction network (IoI) (Tao et al., 2019) and multi-hop selector network (MSN) (Yuan et al., 2019). More recently, pre-trained language models (Devlin et al., 2019; Yang et al., 2019) have shown significant benefits for various NLP tasks, and some researchers have tried to apply them on multi-turn response selection. Vig and Ramea (2019) exploit BERT to represent each utteranceresponse pair and fuse these representations to 4447 calculate the matching sc"
2021.acl-long.343,D19-1011,0,0.0200588,"n response selection where the input of a matching model is a message-response pair (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015). Recently, researchers pay more attention to multiturn context-response matching and usually adopt the representation-matching-aggregation paradigm to build the model. Representative methods include the dual-LSTM model (Lowe et al., 2015), the sequential matching network (SMN) (Wu et al., 2017), the deep attention matching network (DAM) (Zhou et al., 2018b), interaction-overinteraction network (IoI) (Tao et al., 2019) and multi-hop selector network (MSN) (Yuan et al., 2019). More recently, pre-trained language models (Devlin et al., 2019; Yang et al., 2019) have shown significant benefits for various NLP tasks, and some researchers have tried to apply them on multi-turn response selection. Vig and Ramea (2019) exploit BERT to represent each utteranceresponse pair and fuse these representations to 4447 calculate the matching score; Whang et al. (2020) and Xu et al. (2020) treat the context as a long sequence and conduct context-response matching with BERT. Besides, Gu et al. (2020a) integrate speaker embeddings into BERT to improve the utterance representation in"
2021.acl-long.343,D13-1096,0,0.124955,"esponse, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training. 1 Introduction Along with the very recent prosperity of artificial intelligence empowered conversation systems in the spotlight, many studies have been focused on building human-computer dialogue systems (Wen et al., 2017; Zhang et al., 2020) with either retrievalbased methods (Wang et al., 2013; Wu et al., 2017; ∗ † Equal Contribution. Corresponding author: Rui Yan (ruiyan@ruc.edu.cn). Whang et al., 2020) or generation-based methods (Li et al., 2016; Serban et al., 2016; Zhang et al., 2020), which both predict the response with only the given context. In fact, unlike a person who may associate the conversation with the background knowledge in his or her mind, the machine can only capture limited information from the query message itself. As a result, it is difficult for a machine to properly comprehend the query, and to predict a proper response to make it more engaging. To bridge t"
2021.acl-long.343,E17-1042,0,0.0747834,"Missing"
2021.acl-long.343,P17-1046,0,0.125723,"elp of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training. 1 Introduction Along with the very recent prosperity of artificial intelligence empowered conversation systems in the spotlight, many studies have been focused on building human-computer dialogue systems (Wen et al., 2017; Zhang et al., 2020) with either retrievalbased methods (Wang et al., 2013; Wu et al., 2017; ∗ † Equal Contribution. Corresponding author: Rui Yan (ruiyan@ruc.edu.cn). Whang et al., 2020) or generation-based methods (Li et al., 2016; Serban et al., 2016; Zhang et al., 2020), which both predict the response with only the given context. In fact, unlike a person who may associate the conversation with the background knowledge in his or her mind, the machine can only capture limited information from the query message itself. As a result, it is difficult for a machine to properly comprehend the query, and to predict a proper response to make it more engaging. To bridge the gap of the kno"
2021.acl-long.343,P18-1205,0,0.0566509,"Missing"
2021.acl-long.343,2020.acl-demos.30,0,0.0430804,"ed to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training. 1 Introduction Along with the very recent prosperity of artificial intelligence empowered conversation systems in the spotlight, many studies have been focused on building human-computer dialogue systems (Wen et al., 2017; Zhang et al., 2020) with either retrievalbased methods (Wang et al., 2013; Wu et al., 2017; ∗ † Equal Contribution. Corresponding author: Rui Yan (ruiyan@ruc.edu.cn). Whang et al., 2020) or generation-based methods (Li et al., 2016; Serban et al., 2016; Zhang et al., 2020), which both predict the response with only the given context. In fact, unlike a person who may associate the conversation with the background knowledge in his or her mind, the machine can only capture limited information from the query message itself. As a result, it is difficult for a machine to properly comprehend the query, and to predict a"
2021.acl-long.343,2020.emnlp-main.272,1,0.758728,"xt and the background documents (knowledge matching). While there exists a number of knowledge documents on the Web, it is non-trivial to collect large-scale dialogues that are naturally grounded on the documents for training a neural response selection model, which hinders the effective and adequate training of knowledge selection and response matching. Although some benchmarks built upon crowd-sourcing have been released by recent works (Zhang et al., 2018; Dinan et al., 2019), the relatively small training size makes it hard for the dialogue models to generalize on other domains or topics (Zhao et al., 2020). Thus, in this work, we 4446 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4446–4457 August 1–6, 2021. ©2021 Association for Computational Linguistics focus on a more challenging and practical scenario, learning a knowledge-grounded conversation agent without any knowledge-grounded dialogue data, which is known as zero-resource settings. Since knowledge-grounded dialogues are unavailable in training, it raises greater challenges for learning the grounded response selecti"
2021.acl-long.343,D18-1076,0,0.261827,"ching degree. Particularly, we design two strategies to compute the final matching score. In the first strategy, we directly concatenate the selected knowledge and dialogue history as a long sequence of background knowledge and feed into the model. In the second strategy, we first compute the matching degree between each queryknowledge and the response candidates, and then integrate all matching scores. We conduct experiments with benchmarks of knowledge-grounded dialogue that are constructed by crowd-sourcing, such as the Wizard-ofWikipedia Corpus (Dinan et al., 2019) and the CMU DoG Corpus (Zhou et al., 2018a). Evaluation results indicate that our model achieves comparable performance on knowledge selection and response selection with several existing models trained on crowd-sourced benchmarks. Our contributions are summarized as follows: • To the best of our knowledge, this is the first exploration of knowledge-grounded response selection under the zero-resource setting. • We propose decomposing the training of the grounded response selection models into several sub-tasks, so as to empower the model through these tasks in knowledge selection and response matching. • We achieve a comparable perfo"
2021.acl-long.343,P18-1103,0,0.241647,"ching degree. Particularly, we design two strategies to compute the final matching score. In the first strategy, we directly concatenate the selected knowledge and dialogue history as a long sequence of background knowledge and feed into the model. In the second strategy, we first compute the matching degree between each queryknowledge and the response candidates, and then integrate all matching scores. We conduct experiments with benchmarks of knowledge-grounded dialogue that are constructed by crowd-sourcing, such as the Wizard-ofWikipedia Corpus (Dinan et al., 2019) and the CMU DoG Corpus (Zhou et al., 2018a). Evaluation results indicate that our model achieves comparable performance on knowledge selection and response selection with several existing models trained on crowd-sourced benchmarks. Our contributions are summarized as follows: • To the best of our knowledge, this is the first exploration of knowledge-grounded response selection under the zero-resource setting. • We propose decomposing the training of the grounded response selection models into several sub-tasks, so as to empower the model through these tasks in knowledge selection and response matching. • We achieve a comparable perfo"
2021.acl-long.473,D18-1442,1,0.901625,"Missing"
2021.acl-long.473,N13-1136,0,0.0281328,"ers that cite the reference papers of the paper being written to cover the Steiner tree. However, abstractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. Multi-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive"
2021.acl-long.473,P19-1610,0,0.0252513,"Missing"
2021.acl-long.473,D19-1388,1,0.847232,"g the weight of each edge, i.e., l−1,r βi,j , we also incorporate relation edge information, since close relationships such as succession or transition can have a great impact on edge weight. Concretely, Equation 4 is changed to:   T Q l−1 K + hl hl−1 W h W r r r d d i,j i j l−1,r √ . βi,j = d (10) We summarize the whole relationship modeling process as: L 0 0 hL d , hr = RM(hd , hr ). (11) For brevity, we omit the subscript L in the following section. 5.4 Related Work Generator To generate a consistent and informative summary, we propose an RNN-based decoder following (Chen et al., 2019; Gao et al., 2019) that incorporates the outputs of the hierarchical encoder and the relationship graph as illustrated in Figure 1. Our decoder is a single-layer unidirectional LSTM. At each step t, the decoder updates the hidden state from st−1 to st :  h i d st = LSTM st−1 , cw , c , e(y ) . t−1 t−1 t−1 6072 (12) Following previous works (Bahdanau et al., 2015), we employ an attention mechanism to compute the attention distribution over the source words in the sequence-to-sequence structure:   w0 ,i αt,j = Wag tanh Wbg st + Wcg hwi , (13) j  0  P  0  w,i w ,i w ,i i / N , (14) αt,j = exp αt,j l=1 exp"
2021.acl-long.473,W00-0405,0,0.289426,"r tree of the keywords. Then the summary is generated by extracting the sentences from the papers that cite the reference papers of the paper being written to cover the Steiner tree. However, abstractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. Multi-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction net"
2021.acl-long.473,P16-1154,0,0.0360376,"is being introduced. Hence, we employ the document-level attention weights in Equation 17 to read the relationship graph:   hrim = meanpool hri,1 , · · · , hri,N , (19) P d m crt = N i=1 αt,i hri . Finally, an output projection layer is applied to get the final generating distribution Ptv over vocabulary, as shown in Equation 20: d r Ptv = softmax(MLPc [st ; cw t ; ct ; ct ]). (20) Our objective function is the negative log likelihood of the target word yt : P L = − Tt=1 log Ptv (yt ). (21) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. 6 6.1 Experimental Setup Baselines To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: (1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity"
2021.acl-long.473,C10-2049,0,0.0360115,"gle miRNA on various gene targets are the limitations to the use of this modern technology specifically in brain disorders like prion diseases [3]. Table 1: Comparison of a related work paragraph generated by an extractive method (human-annotated) and an abstractive man-made related work paragraph with the same multiple original papers. Introduction The related work section generation task aims to automatically generate a summary of the most relevant works in a specific research area, which can help researchers to familiarize themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by ∗ Corresponding author. https://github.com/iriscxy/ relatedworkgeneration 1 extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pa"
2021.acl-long.473,P18-1013,0,0.0189753,"e themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by ∗ Corresponding author. https://github.com/iriscxy/ relatedworkgeneration 1 extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pattern “We find...” as the subject of sentences, which, as a matter of fact, refer to different authors. On the contrary, the abstractive related work in Table 1 reveals that the works are conducted by different scholars. It also has conjunction words such as “Furthermore” and “However”, which can explain the logical relationship between the cited works, and thus form an elegant narration. Hence, in this paper, we target on the abstractive related work generation task, which generates a related work including novel words an"
2021.acl-long.473,D14-1170,0,0.0290996,"gene targets are the limitations to the use of this modern technology specifically in brain disorders like prion diseases [3]. Table 1: Comparison of a related work paragraph generated by an extractive method (human-annotated) and an abstractive man-made related work paragraph with the same multiple original papers. Introduction The related work section generation task aims to automatically generate a summary of the most relevant works in a specific research area, which can help researchers to familiarize themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by ∗ Corresponding author. https://github.com/iriscxy/ relatedworkgeneration 1 extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pattern “We find...”"
2021.acl-long.473,2020.acl-main.556,0,0.327486,"documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process. While the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order. 3 We discuss the related work on related work generation and multi-document summarization. Related Work Generation. Most of the previous"
2021.acl-long.473,2020.acl-main.555,0,0.0722303,"are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process. While the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order. 3 We discuss the related work on related work generation and multi-document summarization. Related Work Generation. Most of the previous related work section generation methods are extractive. For example, Hoang and Kan (2010) take in a set of"
2021.acl-long.473,P19-1500,0,0.23334,"(yt ). (21) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. 6 6.1 Experimental Setup Baselines To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: (1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization. Abstractive Methods: (1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017). (2) TransformerABS: is an abstractive summarization model based on the Transformer (Vaswani et al., 2017). (3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network built on BERT. (4) MGSum-abs (Jin et al., 2020): is a multi-granularity interaction n"
2021.acl-long.473,D19-1387,0,0.302697,"(yt ). (21) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. 6 6.1 Experimental Setup Baselines To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: (1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization. Abstractive Methods: (1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017). (2) TransformerABS: is an abstractive summarization model based on the Transformer (Vaswani et al., 2017). (3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network built on BERT. (4) MGSum-abs (Jin et al., 2020): is a multi-granularity interaction n"
2021.acl-long.473,2020.acl-main.447,0,0.0402838,"rk generation aims to compare and introduce the cited works in logic order. 3 We discuss the related work on related work generation and multi-document summarization. Related Work Generation. Most of the previous related work section generation methods are extractive. For example, Hoang and Kan (2010) take in a set of keywords arranged in a hierarchical Related Work Generation Dataset Since there are no public large-scale related work generation datasets, we collect two survey datasets composed of related work sections and their corresponding papers. The first dataset is collected from S2ORC (Lo et al., 2020), which consists of papers in multiple domains (physics, math, computer sci6069 Dataset S2ORC Delve Multi-News RWS DUC03+04 TAC 2011 # Pairs (train/valid/test) 126,655/5,000/5,000 72,927/3,000/3,000 44,972/5,622/5,622 25 320 176 # source (articles) 5.02 3.69 2.78 9.47 10 10 # words (doc) 1,079 626 2,103 5,496 4,636 4,695 # sents (docs) 45 26 82 237 173 188 # words (summary) 148 181 263 367 109 99 # sents (summary) 6.69 7.88 9.97 18.28 2.88 1.00 vocab size 377,431 190,381 666,515 15,019 19,734 24,672 Table 2: Comparison of our S2ORC and Delve dataset to other related work and multi-document dat"
2021.acl-long.473,2020.emnlp-main.648,0,0.0262993,"rization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process. While the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order. 3 We discuss the related w"
2021.acl-long.473,C16-1143,0,0.0697368,"Missing"
2021.acl-long.473,W04-3252,0,0.674528,"ctive function is the negative log likelihood of the target word yt : P L = − Tt=1 log Ptv (yt ). (21) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. 6 6.1 Experimental Setup Baselines To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods: (1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization. Abstractive Methods: (1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017). (2) TransformerABS: is an abstractive summarization model based on the Transformer (Vaswani et al., 2017). (3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network"
2021.acl-long.473,W14-3703,0,0.029514,"e papers of the paper being written to cover the Steiner tree. However, abstractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. Multi-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarizati"
2021.acl-long.473,P17-1099,0,0.337005,"hers to familiarize themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by ∗ Corresponding author. https://github.com/iriscxy/ relatedworkgeneration 1 extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pattern “We find...” as the subject of sentences, which, as a matter of fact, refer to different authors. On the contrary, the abstractive related work in Table 1 reveals that the works are conducted by different scholars. It also has conjunction words such as “Furthermore” and “However”, which can explain the logical relationship between the cited works, and thus form an elegant narration. Hence, in this paper, we target on the abstractive related work generation task, which generates a related work inclu"
2021.acl-long.473,2020.acl-main.553,0,0.0135877,"ractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents. Multi-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents t"
2021.findings-acl.419,D19-1501,1,0.854038,"Missing"
2021.findings-acl.419,D19-1201,1,0.891582,"Missing"
2021.findings-acl.419,2020.emnlp-main.313,1,0.855696,"Missing"
2021.findings-acl.419,D18-1442,1,0.913849,"es in the source text across different datasets. • To the best of our knowledge, we are the first to use factual tables to guide the summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to"
2021.findings-acl.419,P16-1046,0,0.0197469,"he summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient conten"
2021.findings-acl.419,D19-1388,1,0.871882,"he other is abstractive summarization (See et al., 2017; Hsu et al., 2018a), which aims to concisely paraphrase the input article. In both methods, the summary should always focus on important information, though a document may include trivial facts. ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding Author: Dongyan Zhao 1 https://github.com/gsh199449/ table-summ † To focus on the main information when generating summaries, some researchers propose to incorporate manifold information to improve the performance. Narayan et al. (2017) proposed to incorporate the figures and Gao et al. (2019b) investigated the using of reader comments for more effective summarization. As another type of side information, factual tables provide a natural summary of the biography document. On Wikipedia, in each wiki page about people, there is a factual table (infobox) on the right side of the page summarizing the main properties. Clearly, infobox is helpful for capturing the salient information during summarizing the biography. However, no existing work takes advantage of tables, though are widely available in the biography on Wikipedia. In this paper, we propose Table-Guided Summarization (TaGS)"
2021.findings-acl.419,D18-1443,0,0.0119376,"ate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient content selector. 2.2 Summarization with Side Information Traditional text summarization methods only use the document as input. However, the gist of the document may lie in side information, such as the title, image captions, or comments which are often available for news-wire articles. As such, various studies (Gao et al., 2020; Hu et al., 2008) have tried to use such information for more efficient and accurate summarization. However, to the best of our knowledge, no existing works consider the use of tables to guide biograp"
2021.findings-acl.419,N18-1065,0,0.0438769,"Missing"
2021.findings-acl.419,P18-1013,0,0.283719,"ataset to validate the quality of the dataset. We also benchmark several commonly used summarization methods on TaGS and hope this will inspire more exciting methods. 1 Introduction Text summarization generates a short text version of a long passage which retains the most important information. Recently, two kinds of approaches have been proposed for automatic text summarization. One is extractive summarization (Nallapati et al., 2017; Liu and Lapata, 2019), which directly selects salient sentences from the passage to create a summary. The other is abstractive summarization (See et al., 2017; Hsu et al., 2018a), which aims to concisely paraphrase the input article. In both methods, the summary should always focus on important information, though a document may include trivial facts. ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding Author: Dongyan Zhao 1 https://github.com/gsh199449/ table-summ † To focus on the main information when generating summaries, some researchers propose to incorporate manifold information to improve the performance. Narayan et al. (2017) proposed to incorporate the figures and Gao et al. (2019b) investigated the using of reader comments for more eff"
2021.findings-acl.419,P18-1014,0,0.0210932,"xt across different datasets. • To the best of our knowledge, we are the first to use factual tables to guide the summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summariz"
2021.findings-acl.419,N19-1260,0,0.0316544,"Missing"
2021.findings-acl.419,D19-1387,0,0.0496443,"the TaGS (Table-Guided Summarization) dataset1 , the first large-scale biography summarization dataset with tables. Next, we report some statistics about this dataset to validate the quality of the dataset. We also benchmark several commonly used summarization methods on TaGS and hope this will inspire more exciting methods. 1 Introduction Text summarization generates a short text version of a long passage which retains the most important information. Recently, two kinds of approaches have been proposed for automatic text summarization. One is extractive summarization (Nallapati et al., 2017; Liu and Lapata, 2019), which directly selects salient sentences from the passage to create a summary. The other is abstractive summarization (See et al., 2017; Hsu et al., 2018a), which aims to concisely paraphrase the input article. In both methods, the summary should always focus on important information, though a document may include trivial facts. ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding Author: Dongyan Zhao 1 https://github.com/gsh199449/ table-summ † To focus on the main information when generating summaries, some researchers propose to incorporate manifold information to impro"
2021.findings-acl.419,N19-1173,0,0.011415,"ies. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient content selector. 2.2 Summarization with Side Information Traditional text summarization methods only use the document as input. However, the gist of the document may lie in side information, such as the title, image captions, or comments which are often available for n"
2021.findings-acl.419,D18-1206,0,0.0138241,"of ground truth summaries in the source text across different datasets. • To the best of our knowledge, we are the first to use factual tables to guide the summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text"
2021.findings-acl.419,N18-1158,0,0.02071,"of ground truth summaries in the source text across different datasets. • To the best of our knowledge, we are the first to use factual tables to guide the summarization procedure so as to generate better summaries. • We release a large-scale abstractive biography summarization dataset with tables. Experiments conducted on this dataset demonstrate the effectiveness of incorporating table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text"
2021.findings-acl.419,P17-1099,0,0.0471813,"stics about this dataset to validate the quality of the dataset. We also benchmark several commonly used summarization methods on TaGS and hope this will inspire more exciting methods. 1 Introduction Text summarization generates a short text version of a long passage which retains the most important information. Recently, two kinds of approaches have been proposed for automatic text summarization. One is extractive summarization (Nallapati et al., 2017; Liu and Lapata, 2019), which directly selects salient sentences from the passage to create a summary. The other is abstractive summarization (See et al., 2017; Hsu et al., 2018a), which aims to concisely paraphrase the input article. In both methods, the summary should always focus on important information, though a document may include trivial facts. ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding Author: Dongyan Zhao 1 https://github.com/gsh199449/ table-summ † To focus on the main information when generating summaries, some researchers propose to incorporate manifold information to improve the performance. Narayan et al. (2017) proposed to incorporate the figures and Gao et al. (2019b) investigated the using of reader com"
2021.findings-acl.419,D18-1088,0,0.0136327,"table information in generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient content selector. 2.2 Summarization with Side Information Traditional text summarization methods only use the document as input. However, the gist of the document may lie in side information, such as the title, image captions, or co"
2021.findings-acl.419,P18-1061,0,0.0165519,"n generating summaries. 2 2.1 Related Work Text Summarization Text summarization is an important task which can be classified into extractive and abstractive approaches. Extractive summarization (Narayan et al., 2018b; Chen et al., 2018; Jadhav and Rajan, 2018) tends to generate a summary by integrating the most salient sentences in the document. Cheng and Lapata (2016) first propose using recurrent neural network (RNN) to extract salient sentences. After that researchers explore many the neural based method (Nallapati et al., 2017; Liu and Lapata, 2019; Chen et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Liu et al., 2019), and achieve the state-of-the-art performance (Liu and Lapata, 2019) on the benchmark dataset CNN/DailyMail. In the mean time, the Nallapati et al. (2016) firstly apply this text generation method to the abstractive summarization task and Gehrmann et al. (2018) achieve the state-of-the-art performance by using a data-efficient content selector. 2.2 Summarization with Side Information Traditional text summarization methods only use the document as input. However, the gist of the document may lie in side information, such as the title, image captions, or comments which are of"
2021.findings-acl.432,D19-1501,1,0.854308,"ogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NL"
2021.findings-acl.432,D19-1201,1,0.869864,"ogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NL"
2021.findings-acl.432,2020.emnlp-main.313,1,0.729602,"roved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is learned from tuples of d"
2021.findings-acl.432,2021.acl-long.34,0,0.469022,"ious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a re"
2021.findings-acl.432,N19-1021,0,0.0144273,"dding. posterior qθ (z|hq ), i.e. a conditional Gaussian distribution. Training procedures. Previous works (Bowman et al., 2015; Zhao et al., 2017) mentioned that VAE and CVAE training is challenging due to the KL vanishing issue, where the decoder ignores the conditional information and all the resulting posteriors almost collapse to a same Gaussian prior. To mitigate this issue, first, we initialize our model with Optimus (Li et al., 2020), a large-scale VAE-based PLM model, while optimizing Eq. 2. To mitigate the same issue while optimizing Eq. 3, we use the cyclical KL annealing schedule (Fu et al., 2019). Specifically, we add a hyperparameter α to control the weight of the KLdivergence in Eq. 3. We set α close to zero in the first half of cyclic schedule, linearly anneal α to 1 in the next one-fourth of cyclic schedule and kept α = 1 in the remaining cyclic schedule. Moreover, the Free Bits (Bowman et al., 2015) is also crucial for the training. It replaces the KLdivergence in Eq. 3 by a hinge loss max(γ, KL(qφ (z|hq )||pθ (z|hp ))) (4) where γ is a hyperparameter which controls the information space for the each dimension of the latent variable. Finally, an extra bag-of-word loss (Zhao et al"
2021.findings-acl.432,2021.findings-acl.220,1,0.534829,"Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is learned from tuples of data, {conversational context, response reference, negative sample}, in an unsuperv"
2021.findings-acl.432,2020.acl-main.124,0,0.101111,"ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to e"
2021.findings-acl.432,W19-2310,0,0.198948,"Missing"
2021.findings-acl.432,P19-1590,0,0.0127027,"rior distribution P (z|c) to capture the feasible latent reference information in the latent space. Specifically, when training CVAEs, P (z|c) is forced to be close to the posterior distribution Q(z|c, ri ) for any reference response ri as illustrated in Fig. 1. In this sense, if z is sampled from P (z|c), z may contain some information of any ri in some extent, and z can be used as a surrogate of {rk }K k=1 . Therefore, we can expect 4891 1 There is a brief proof in Apendix A. I(l; c, {rk }N k=1 ) ≥ I(l; c, ri , z) ≥ I(l; c, ri ). 3.2 3.3 Overall Architecture Previous works (Li et al., 2019; Gururangan et al., 2019; Li et al., 2020) concluded that VAEs can learn a smooth latent space through the regularization from the gaussian prior. Inspired by Li et al. (2020), we propose a novel architecture which can be regarded as a large-scale pretrained language model (PLM) based on VAEs/CVAEs. Encoder. Li et al. (2019) argue that the VAEs might benefit from initialization with a noncollapsed encoder, because the encoder provides useful information from the beginning of training. We use the Masked PLMs (Devlin et al., 2018; Liu et al., 2019) as the text encoder because of their impressive effectiveness in natura"
2021.findings-acl.432,N19-1169,0,0.0184016,"(2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is lear"
2021.findings-acl.432,D19-1370,0,0.23084,"ntation learning and dialogue modeling, we propose to learn the dialogue representations via VAEs/CVAEs for better evaluation. Equip with such dialogue representations, we obtain an Enhanced dialogue evaluation Metric in latent Space (EMS). EMS is a self-supervised evaluation metric with a two-stage training procedure. It represents dialogue sentences in a smooth latent space to both capture discourse-level context information and model more feasible latent references. Specifically, in the first stage, we build a VAE based model to map the dialogue sentences into a latent (or semantic) space. Li et al. (2019) showed that VAEs can be viewed as a regularized version of the auto-encoder and learn a smooth latent space through the regularization from the Gaussian prior. Then, we train our model by optimizing CVAEs’ objective which forces the prior distribution to capture the feasible latent references information (details in Section 3.3). In the second stage, we combine the dialogue representations and the captured feasible latent reference information to train a discriminative model. Meanwhile, we give a potential explanation of our motivation about why using feasible latent reference information can"
2021.findings-acl.432,2020.emnlp-main.378,0,0.250324,"to capture the feasible latent reference information in the latent space. Specifically, when training CVAEs, P (z|c) is forced to be close to the posterior distribution Q(z|c, ri ) for any reference response ri as illustrated in Fig. 1. In this sense, if z is sampled from P (z|c), z may contain some information of any ri in some extent, and z can be used as a surrogate of {rk }K k=1 . Therefore, we can expect 4891 1 There is a brief proof in Apendix A. I(l; c, {rk }N k=1 ) ≥ I(l; c, ri , z) ≥ I(l; c, ri ). 3.2 3.3 Overall Architecture Previous works (Li et al., 2019; Gururangan et al., 2019; Li et al., 2020) concluded that VAEs can learn a smooth latent space through the regularization from the gaussian prior. Inspired by Li et al. (2020), we propose a novel architecture which can be regarded as a large-scale pretrained language model (PLM) based on VAEs/CVAEs. Encoder. Li et al. (2019) argue that the VAEs might benefit from initialization with a noncollapsed encoder, because the encoder provides useful information from the beginning of training. We use the Masked PLMs (Devlin et al., 2018; Liu et al., 2019) as the text encoder because of their impressive effectiveness in natural language underst"
2021.findings-acl.432,I17-1099,0,0.0443873,"Missing"
2021.findings-acl.432,D16-1230,0,0.030509,"ning variational model to capture the feasible latent references; • Experiments performed on two large datasets demonstrate the effectiveness of our proposed model and outperform all baseline methods. 2 Related Work Word overlap-based Metrics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the"
2021.findings-acl.432,2021.ccl-1.108,0,0.0249961,"Missing"
2021.findings-acl.432,P17-1103,0,0.0589553,"del to capture the feasible latent references; • Experiments performed on two large datasets demonstrate the effectiveness of our proposed model and outperform all baseline methods. 2 Related Work Word overlap-based Metrics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional rep"
2021.findings-acl.432,P08-1028,0,0.819519,"ural language generation (NLG) tasks (Dathathri et al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic simila"
2021.findings-acl.432,P02-1040,0,0.116934,"an. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NLG) tasks (Dathathri et al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019;"
2021.findings-acl.432,P19-1372,1,0.904042,", an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural lang"
2021.findings-acl.432,W12-2018,0,0.667018,"t al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a genera"
2021.findings-acl.432,2020.tacl-1.52,0,0.402268,"for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) further proposed to enhance such discriminative evaluation metrics by finetuning on a few human-annotated data to improve the robustness. These discriminative metrics trained using a single relevant response and multiple negative samples. However, Sai et al. (2020) argued that such discriminative metrics should be trained on multiple relevant responses (i.e., positive samples) and multiple negative samples, to favor the one-to-many nature in open-domain dialogues. Therefore, they collected a new dataset which contains multiple relevant and irrelevant responses for any given conversational context to train their discriminative evaluation model and the model trained by multiple relevant responses shows impressive performance. However, there are no organized relevant multiple responses in most existing datasets. Collecting a new dataset is expensive and ti"
2021.findings-acl.432,2020.acl-main.704,0,0.0671024,"in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build aut"
2021.findings-acl.432,P18-1205,0,0.100736,"Missing"
2021.findings-acl.432,P17-1061,0,0.20045,"space. Specifically, we present EMS, an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial"
2021.findings-acl.432,2020.acl-main.4,0,0.417553,"9; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by considering the similarity of the generated 4889 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4889–4900 August 1–6, 2021. ©2021 Association for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) further proposed to enhance such discriminative evaluation metrics by finetuning on a few human-annotated data to improve the robustness. These discriminative metrics trained using a single relevant response and multiple negative samples. However, Sai et al. (2020) argued that such discriminative metrics should be trained on multiple relevant responses (i.e., positive samples) and multiple negative samples, to favor the one-to-many nature in open-domain dialogues. Therefore, they collected a new dataset which contains multiple relevant and irrelevant responses for any given conversational con"
2021.findings-acl.432,D19-1053,0,0.201466,"n and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue eva"
2021.findings-acl.432,D18-1463,0,0.0149516,"s has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 201"
2021.findings-acl.432,2020.acl-main.220,0,0.122106,"ng model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by considering the similarity of the generated 4889 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4889–4900 August 1–6, 2021. ©2021 Association for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) fur"
2021.findings-acl.432,2021.findings-acl.193,1,0.84362,"004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by co"
2021.findings-acl.432,2020.acl-srw.27,0,0.0191815,"rics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimo"
2021.naacl-main.134,E14-1028,0,0.0519677,"Missing"
2021.naacl-main.134,W17-3531,0,0.044374,"Missing"
2021.naacl-main.134,I11-1144,0,0.036573,"words. der” information takes effects in natural language learning for neural models. On the basis of the pooling-based and memory-based approaches, we introduce the self-attention to encode the semantic dependencies between input words without considering order information, so as to enrich individual words with contextual information from different semantic aspects. We systematically compare the ability of different neural models to organize sentences from a bag of words in terms of three typical scenarios shown in Table 1. The contributions of this paper are summarized as follows: sentence (He and Liang, 2011). In dialogue systems, we need systems that are enabled to converse smoothly with people that have troubles in ordering words, such as children, language learners, and • We present an empirical study to investigate speech impaired. In image caption, the caption can the ability of neural models to organize senbe organized with a bag of attribute words extracted tences from a bag of words. from the image (Fang et al., 2015). Moreover, such a model can help non-native speakers of English to • We introduce a bag-to-sentence transformawrite a sentence just from keywords. tion model based on self-at"
2021.naacl-main.134,W04-1013,0,0.0539533,"rds in this paper. So we compare all methods under standard beam search method (with a beam size of 5) in our experiment, to highlight the differences among different encoders. N-GRAM∗ RNNLM∗ Pooling LSTM Memory AttP AttM BLEU 0.2330 0.2450 0.3118 0.3140 0.3328 0.3469 0.3489 ROUGE-L 0.5916 0.5875 0.6053 0.6169 0.6194 WAcc 0.4105 0.3873 0.4089 0.4297 0.4304 PMR 0.0863 0.0850 0.0941 0.1013 0.1059 Table 3: Results of word ordering task on PTB datasets (beam size = 5), * denotes the results reported in (Hasler et al., 2017). cal units (e.g., unigram, bigram) with the reference sentences. ROUGE-L (Lin, 2004) measures the longest common subsequence (LCS) between the reference sentence and the generated sentence. WAcc (Word Accuracy) is the negative word error rate (WER) (Mangu et al., 2000). It measures the edit distance between the generated sentence and the reference sentence (higher is better). Besides, we also conduct human evaluations to further analyze our generated results and explore the detail sort of wrong cases. 5.4 Overall Results Table 2 illustrates the performance of all models for three scenarios on the Wikipedia dataset. Firstly, we can find that Pooling shows the worse performance"
2021.naacl-main.134,D15-1043,0,0.0161204,"o, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scenario, the words of X come from a disordered sentence and are t"
2021.naacl-main.134,N15-1012,0,0.0178143,"n to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scena"
2021.naacl-main.134,J93-2004,0,0.0812258,"0.5808 0.5563 0.4369 0.6063 0.5789 0.4520 0.6613 0.6367 0.4700 0.6697 0.6461 0.4702 Table 2: Results on the test sets of three scenarios for Wikipedia dataset. We randomly generate noisy words with the number between 1 and half length of the sentence from the vocabulary for each sentence as the input of the the noise scenario. For the missing scenario, random words with number between 1 and half length of the sentence are removed from each sentence. It is worth noting that we randomly shuttle input bags with three different seeds and report the mean score of each metrics for LSTM. data (PTB) (Marcus et al., 1993), which is a widelyused dataset for word ordering task (Schmaltz et al., 2016; Hasler et al., 2017). To facilitate fair comparisons, we use the data preprocessed by (Schmaltz et al., 2016), which consists of 39, 832 training sentences, 1, 700 validation sentences and 2, 416 test sentences. 5.2 Implementation Details For all models, we set the dimension of word embedding as 128. In the LSTM-based encoder, the dimension of hidden unit is 256. In the self-attentionbased encoder, we set the number of head in Equation (6) as 8 and the hidden size of feed-forward layer in Equation (7) as 256. All pa"
2021.naacl-main.134,P02-1040,0,0.109582,"Missing"
2021.naacl-main.134,D16-1255,0,0.0833071,"al. (2018) utiIn this paper, we aim to investigate how “or- lized self-attention to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , ·"
2021.naacl-main.134,P17-1099,0,0.0479644,"e scenario by randomly introducing some To highlight the differences among different en- noisy words to the source bag, and construct the coders, we utilize the same decoder for different training data for the missing scenario by randomly encoders. removing some words from the source bag. Since the target Y corresponds to a sequence, We also compare the normal scenario of and has significant vocabulary overlap with the our model on The English Penn Treebank input bags of words, we blend a pointer-based de3 The corpus removes all links and other irrelevant material coder (Vinyals et al., 2015; See et al., 2017), which 2 It is worth noting that the current memory is composed of the word representations output by self-attention layer (e.g., navigation text, etc), and contains about one billion words, over 2 million documents. 4 http://www.nltk.org/api/nltk.tokenize.html 1685 Pooling LSTM Memory AttP AttM Normal 0.4656 0.4736 0.5030 0.5740 0.5886 BLEU Noise 0.4382 0.4327 0.4537 0.5372 0.5433 Missing 0.2636 0.2538 0.2664 0.2882 0.2914 Normal 0.6917 0.7311 0.7485 0.7860 0.7925 ROUGE-L Noise Missing 0.6587 0.5470 0.6761 0.5453 0.6939 0.5607 0.7396 0.5722 0.7465 0.5738 Perfect Matching Rate (PMR) Normal No"
2021.naacl-main.134,P17-1018,0,0.0390888,"Missing"
2021.naacl-main.134,E12-1075,0,0.0284739,"he objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence. In the normal scenario, the words of X come from a disorde"
2021.naacl-main.134,D11-1106,0,0.0375037,"or- lized self-attention to model interactions between 1683 the objects in a video, and employed pooling to obtain aggregated features. On this basis of the transformer architecture, Lee et al. (2019) presented an Set Transformer designed to model interactions among elements in the input set. Without considering missing words or noisy words, our task devolves into word ordering problem, which is a fundamental task in natural language generation. Previous, researchers usually employed N-gram based language models (De Gispert et al., 2014; Schmaltz et al., 2016), syntacticbased language models (Zhang and Clark, 2011; Liu et al., 2015) or combined models (Zhang et al., 2012; Liu and Zhang, 2015) to solve this problem. More recently, Hasler et al. (2017) proposed a bag-to-sequence model, where the decoder RNN directly attended to the word embeddings. However, all these methods aim at finding the best permutation of a bag of words based on language models, and do not consider how to encode a bag of words. 3 Problem Formulation Given a bag of words X = {x1 , x2 , · · · , xm } which consists of m tokens, our model will generate a sentence Y = {y1 , y2 , · · · , yn }, where n is the length of target sentence."
C16-1316,W14-4012,0,0.0109809,"Missing"
C16-1316,W15-4616,0,0.0311696,"a meaningful but not that probable noun term as the keyword; then we feed seq2BF with such concrete keyword that provides substantial content. In this way, our approach significantly outperforms pure seq2seq generation in short-text conversation systems. 4 4.1 Related Work Dialogue Systems Automatic human-computer conversation has long attracted the attention of researchers. In early decades, people design rule- or template-based systems, but they are mainly in vertical domains (Ferguson et al., 1996; Misu and Kawahara, 2007). Although such approaches can also be extended to the open domain (Han et al., 2015), their generated sentences are subject to 7 predefined forms and thus are highly restricted. For open dialogues, researchers have applied data-driven approaches, including retrieval methods (Isbell et al., 2000; Wang et al., 2013), phrase-based machine translation (Ritter et al., 2011), and recurrent neural networks (Sordoni et al., 2015; Shang et al., 2015). A hot research topic in human-computer conversation is mixed-initiative systems, for example, the TRAINS-95 system for route planning (Ferguson et al., 1996) and AutoTutor for learner advising (Graesser et al., 2005). Li et al. (2016b) p"
C16-1316,N16-1014,0,0.513806,"education (Graesser et al., 2005). In the open domain, data-driven approaches play an important role, because the diversity and uncertainty make it virtually impossible for humans to design rules or templates. Isbell et al. (2000) and Wang et al. (2013) use information retrieval methods to search for a reply from a pre-constructed database; Ritter et al. (2011) formalize conversation as a statistical machine translation task. Recently, the renewed prosperity of neural networks brings new opportunities to open-domain conversation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016a; Li et al., 2016a). In these studies, researchers leverage sequence-to-sequence (seq2seq) models to encode a query (user-issued utterance) as a vector and to decode the vector into a reply. In both encoders and decoders, an RNN keeps one or a few hidden layers; at each time step, it reads a word and changes its state accordingly. RNNs are believed to be well capable of modeling word sequences, benefiting machine translation (Sutskever et al., 2014), abstractive summarization (Rush et al., 2015) and other tasks of natural language generation. Contrary to retrieval methods, neural network-based conversation sys"
C16-1316,D15-1252,1,0.714175,"on, we only need to condition the model on the character sequence in the key term instead of a single keyword, that is, we have several green inputs in Figure 1b. In our study, we kept 2.5k noun terms as candidate keywords and 4k characters for seq2BF generation. 3.2 Hyperparameters In our experiments, word embeddings and recurrent layers were 500d. We used rmsprop to optimize all parameters except embeddings, with initial weights uniformly sampled from [−.08, .08], initial learning rate 0.002, moving average decay 0.99, and a damping term  = 10−8 . Because word embeddings are sparse in use (Peng et al., 2015), we optimized embeddings asynchronously by stochastic gradient √ descent with the learning rate divided by . We set the mini-batch size to 50. These values were mostly chosen empirically by following Karpathy et al. (2015) and Mou et al. (2015); they generally work well in our scenarios. We did not tune the hyperparameters in this paper, but are willing to explore their roles in dialogue generation as future work. The validation set (containing 2k query-reply samples) was used for early stop only. We chose the parameters yielding the highest character-level BLEU-2 score on our validation set"
C16-1316,D11-1054,0,0.545553,"mputer conversation is a hot research topic in natural language processing (NLP). In past decades, researchers have developed various rule- or template-based systems, which are typically in vertical domains, e.g., transportation (Ferguson et al., 1996) and education (Graesser et al., 2005). In the open domain, data-driven approaches play an important role, because the diversity and uncertainty make it virtually impossible for humans to design rules or templates. Isbell et al. (2000) and Wang et al. (2013) use information retrieval methods to search for a reply from a pre-constructed database; Ritter et al. (2011) formalize conversation as a statistical machine translation task. Recently, the renewed prosperity of neural networks brings new opportunities to open-domain conversation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016a; Li et al., 2016a). In these studies, researchers leverage sequence-to-sequence (seq2seq) models to encode a query (user-issued utterance) as a vector and to decode the vector into a reply. In both encoders and decoders, an RNN keeps one or a few hidden layers; at each time step, it reads a word and changes its state accordingly. RNNs are believed to be well cap"
C16-1316,D15-1044,0,0.0428315,"s brings new opportunities to open-domain conversation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016a; Li et al., 2016a). In these studies, researchers leverage sequence-to-sequence (seq2seq) models to encode a query (user-issued utterance) as a vector and to decode the vector into a reply. In both encoders and decoders, an RNN keeps one or a few hidden layers; at each time step, it reads a word and changes its state accordingly. RNNs are believed to be well capable of modeling word sequences, benefiting machine translation (Sutskever et al., 2014), abstractive summarization (Rush et al., 2015) and other tasks of natural language generation. Contrary to retrieval methods, neural network-based conversation systems are generative in that they can synthesize new utterances; results in the literature also show the superiority of seq2seq to phrase-based machine translation for dialogue systems (Shang et al., 2015). In our study, we focus on neural network-based generative short-text conversation, where we do not consider context information, following Wang et al. (2013) and Shang et al. (2015). Despite these, neural networks’ performance is far from satisfactory in human-computer convers"
C16-1316,P15-1152,0,0.822803,"ransportation (Ferguson et al., 1996) and education (Graesser et al., 2005). In the open domain, data-driven approaches play an important role, because the diversity and uncertainty make it virtually impossible for humans to design rules or templates. Isbell et al. (2000) and Wang et al. (2013) use information retrieval methods to search for a reply from a pre-constructed database; Ritter et al. (2011) formalize conversation as a statistical machine translation task. Recently, the renewed prosperity of neural networks brings new opportunities to open-domain conversation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016a; Li et al., 2016a). In these studies, researchers leverage sequence-to-sequence (seq2seq) models to encode a query (user-issued utterance) as a vector and to decode the vector into a reply. In both encoders and decoders, an RNN keeps one or a few hidden layers; at each time step, it reads a word and changes its state accordingly. RNNs are believed to be well capable of modeling word sequences, benefiting machine translation (Sutskever et al., 2014), abstractive summarization (Rush et al., 2015) and other tasks of natural language generation. Contrary to retrieval methods"
C16-1316,N15-1020,0,0.272314,"g attracted the attention of researchers. In early decades, people design rule- or template-based systems, but they are mainly in vertical domains (Ferguson et al., 1996; Misu and Kawahara, 2007). Although such approaches can also be extended to the open domain (Han et al., 2015), their generated sentences are subject to 7 predefined forms and thus are highly restricted. For open dialogues, researchers have applied data-driven approaches, including retrieval methods (Isbell et al., 2000; Wang et al., 2013), phrase-based machine translation (Ritter et al., 2011), and recurrent neural networks (Sordoni et al., 2015; Shang et al., 2015). A hot research topic in human-computer conversation is mixed-initiative systems, for example, the TRAINS-95 system for route planning (Ferguson et al., 1996) and AutoTutor for learner advising (Graesser et al., 2005). Li et al. (2016b) propose a proactive dialogue system that can introduce new content when a stalemate occurs. The system is chatbot-like and in the open domain; an external knowledge base is used for searching related entities as new content. They propose a random walklike reranking algorithm based on retrieval results. Different from Li et al. (2016b)’s wo"
C16-1316,D13-1096,0,0.743413,"the predicted keyword can appear at an appropriate position in the reply. 1 Introduction Automatic human-computer conversation is a hot research topic in natural language processing (NLP). In past decades, researchers have developed various rule- or template-based systems, which are typically in vertical domains, e.g., transportation (Ferguson et al., 1996) and education (Graesser et al., 2005). In the open domain, data-driven approaches play an important role, because the diversity and uncertainty make it virtually impossible for humans to design rules or templates. Isbell et al. (2000) and Wang et al. (2013) use information retrieval methods to search for a reply from a pre-constructed database; Ritter et al. (2011) formalize conversation as a statistical machine translation task. Recently, the renewed prosperity of neural networks brings new opportunities to open-domain conversation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016a; Li et al., 2016a). In these studies, researchers leverage sequence-to-sequence (seq2seq) models to encode a query (user-issued utterance) as a vector and to decode the vector into a reply. In both encoders and decoders, an RNN keeps one or a few hidden"
C16-1316,C16-1100,0,0.0206822,"aper addresses the problem of content introducing in open-domain generative dialogue systems. 4.2 Neural Networks for Sentence Generation Sutskever et al. (2014) propose seq2seq for machine translation; the idea is to encode a source sentence as a vector by a recurrent neural network (RNN) and to decode the vector to a target sentence by another RNN. Bahdanau et al. (2015) enhance it with an attention mechanism. These approaches largely benefit natural language generation tasks such as abstractive summarization (Rush et al., 2015), question answering (Yin et al., 2016), and poetry generation (Wang et al., 2016). For neural network-based dialogue systems, Sordoni et al. (2015) summarize a query and context as bag-of-words features, based on which an RNN decodes the reply. Shang et al. (2015) generate replies for short-text conversation by seq2seq-like neural networks with local and global attention. Yao et al. (2015) and Serban et al. (2016a) design hierarchical neural networks for multi-turn conversation. To address the problem of universal replies, Li et al. (2016a) propose a mutual information training objective. Serban et al. (2016b) apply a variational Bayes approach that imposes a probabilistic"
C16-1316,W16-0106,0,0.012085,"erent from Li et al. (2016b)’s work, our paper addresses the problem of content introducing in open-domain generative dialogue systems. 4.2 Neural Networks for Sentence Generation Sutskever et al. (2014) propose seq2seq for machine translation; the idea is to encode a source sentence as a vector by a recurrent neural network (RNN) and to decode the vector to a target sentence by another RNN. Bahdanau et al. (2015) enhance it with an attention mechanism. These approaches largely benefit natural language generation tasks such as abstractive summarization (Rush et al., 2015), question answering (Yin et al., 2016), and poetry generation (Wang et al., 2016). For neural network-based dialogue systems, Sordoni et al. (2015) summarize a query and context as bag-of-words features, based on which an RNN decodes the reply. Shang et al. (2015) generate replies for short-text conversation by seq2seq-like neural networks with local and global attention. Yao et al. (2015) and Serban et al. (2016a) design hierarchical neural networks for multi-turn conversation. To address the problem of universal replies, Li et al. (2016a) propose a mutual information training objective. Serban et al. (2016b) apply a variational"
D11-1040,W04-3247,0,0.0245181,"for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2 http://www1.cs.columbia.edu/nlp/newsblaster/ algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel cont"
D11-1040,N03-1020,0,0.215307,"from its corresponding sub-collection. The sizes of component summaries are not necessarily equal, and moreover, not all dates may be represented, so date selection is also important. We apply a simple mechanism that users specify the overall compression rate φ, and we extract more sentences for important dates while fewer sentences for others. The importance of dates is measured by the burstiness, which indicates probable significant occurrences (Chieu and Lee, 2004). i| The compression rate on ti is set as φi = |C |C |. 4.3 Evaluation Metrics The ROUGE measure is widely used for evaluation (Lin and Hovy, 2003): the DUC contests usually officially employ ROUGE for automatic summarization evaluation. In ROUGE evaluation, the summarization quality is measured by counting the number of overlapping units, such as N-gram, word sequences, and word pairs between the candidate timelines CT and the reference timelines RT . There are several kinds of ROUGE metrics, of which the most important one is ROUGE-N with 3 sub-metrics: 1 ROUGE-N-R is an N-gram recall metric: ROUGE-N-R = P P I∈RT N-gram∈I P P Countmatch (N-gram) I∈RT N-gram∈I Count (N-gram) 2 ROUGE-N-P is an N-gram precision metric: ROUGE-N-P = P P I∈C"
D11-1040,I05-2004,0,0.0120654,"extraction-based methods have been proposed for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2 http://www1.cs.columbia.edu/nlp/newsblaster/ algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with"
D11-1040,P02-1058,0,\N,Missing
D11-1124,W04-3247,0,0.0287484,"method. MEAD (Radev et al., 2004) and NeATS (Lin and Hovy, 2002) are such implementations, using position and term frequency, etc. MMR (Goldstein et al., 1999) algorithm is used to remove redundancy. Most recently, the graph-based ranking methods have been proposed to rank sentences or passages based on the “votes” or “recommendations” between each other. The graphbased methods first construct a graph representing the sentence relationships at different granularities and then evaluate the saliency score of the sentences based on the graph. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) 1343 use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. improve the graph-ranking algorithm by differentiating intradocument and inter-document links between sentences (2007b) and incorporate cluster information in the graph model to evaluate sentences (2008). To date, topics (or themes, clusters) in documents have been discovered and used for sentence selection for topic biased summarization (Wan and Yang, 2008; Gong and Liu, 2001). Wan et al. have proposed a manifold-ranking method to make uniform use of sentence-to-sentence and sentence-totopic relations"
D11-1124,P03-2021,0,0.294936,"Missing"
D11-1124,N03-1020,0,0.482984,"utility. User utility is obtained from interaction. The system keeps the clicked sentence records and calculates the user feedback by Equation (3) during every session. Consider sometimes 1347 users click into the summary due to confusion or mis-operations, but not their real interests. The system supports click records revocation. More details of the user interface is demonstrated in Figure 1. 4.3 Evaluation Metrics We include both subjective evaluation from 3 evaluators based on their personalized interests and preference, and the objective evaluation based on the widely used ROUGE metrics (Lin and Hovy, 2003). Evaluator Judgments Evaluators are requested to express an opinion over all summaries based on the sentences which they deem to be important for the news. In general a summary can be rated in a 5-point scale, where “1” for “terrible”, “2” for “bad”, “3” for “normal”, “4” for “good” and “5” for “excellent”. Evaluators are allowed to judge at any scores between 1 and 5, e.g. a score of “3.3” is adopted when the evaluator feels difficult to decide whether “3” or “4” is more appropriate but with preference towards “3”. ROUGE Evaluation The DUC usually officially employs ROUGE measures for summar"
D11-1124,D09-1032,0,0.0219083,", 2008), a good summary should be concise and contain as few redundant sentences as possible, i.e., two sentences providing similar information should not both present. According to our investigation, we observe that a well generated summary should properly consider a key component of (3) user interests, which captures user preference to summarize what they are interested in. All above requirements involve a measurement of similarity between two word distributions Θ1 and Θ2 . Cosine, Kullback-Leibler divergence DKL and Jensen Shannon divergence DJS are all able to measure the similarity, but (Louis and Nenkova, 2009) indicate the superiority of DJS in summarization task. We also introduce a pair of decreasing/increasing logistic functions, L1 (x) = 1/(1 + ex ) and L2 (x) = ex /(1 + ex ), to map the divergence into interval [0,1]. V is the vocabulary set and tf denotes the term frequency for word w. 1 DJS (Θ1 ||Θ2 ) = [DKL (Θ1 ||Θ2 )+DKL (Θ2 ||Θ1 )] 2 where DKL (Θ1 ||Θ2 ) = where X k∈V p(w|Θ1 )log p(w|Θ1 ) p(w|Θ2 ) Modeling Interest for User Utility. Given a generated summary S, users tend to scrutinize texts relevant to their interests. Texts related to user implicit P|Q| feedback are collected as A = i=1"
D11-1124,I05-2004,0,0.019746,"f the most popular extractive summarization method. MEAD (Radev et al., 2004) and NeATS (Lin and Hovy, 2002) are such implementations, using position and term frequency, etc. MMR (Goldstein et al., 1999) algorithm is used to remove redundancy. Most recently, the graph-based ranking methods have been proposed to rank sentences or passages based on the “votes” or “recommendations” between each other. The graphbased methods first construct a graph representing the sentence relationships at different granularities and then evaluate the saliency score of the sentences based on the graph. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) 1343 use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. improve the graph-ranking algorithm by differentiating intradocument and inter-document links between sentences (2007b) and incorporate cluster information in the graph model to evaluate sentences (2008). To date, topics (or themes, clusters) in documents have been discovered and used for sentence selection for topic biased summarization (Wan and Yang, 2008; Gong and Liu, 2001). Wan et al. have proposed a manifold-ranking method to make uniform use of sentence-to"
D11-1124,P08-2033,0,0.0663308,"Missing"
D11-1124,P02-1058,0,\N,Missing
D11-1124,P11-1039,1,\N,Missing
D16-1036,J86-3001,0,0.494876,"chnology of China o n zhouxiangyang, dongdaxiang, wu hua, zhaoshiqi, @baidu.com yudianhai, tianhao, liuxuan, yanrui Abstract these work, context and response are taken as two separate word sequences without considering the relationship among utterances in the context and response. The response selection in these models is largely influenced by word-level information. We called this kind of models as word sequence model in this paper. Besides word-level dependencies, utterance-level semantic and discourse information are also very important to catch the conversation topics to ensure coherence (Grosz and Sidner, 1986). For example an utterance can be an affirmation, negation or deduction to the previous utterances, or starts a new topic for discussion. This kind of utterance-level information is generally ignored in word sequence model, which may be helpful for selecting the next response. Therefore, it is necessary to take each utterance as a unit and model the context and response from the view of utterance sequence. In this paper, we study the task of response selection for multi-turn human-computer conversation. Previous approaches take word as a unit and view context and response as sequences of words"
D16-1036,D14-1181,0,0.0104234,"er hand, research on multi-turn response selection usually takes the whole context into consideration and views the context and response as word sequences. Lowe et al., (2015) proposed a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) based response selection model for multi-turn conversation, where words from context and response are modeled with LSTM. The selection of a response is based on the similarity of embeddings between the context and response. Similar to the work of Lowe et al., Kadlec et al., (2015) replaced LSTM with Temporal Convolutional Neural Networks (TCNN) (Kim, 2014) and BidirectLSTM. Their experimental results show that models with LSTM perform better than other neural networks. However, the utterance-level discourse information and dependencies have been left out in these studies since they view the context and response as word sequences. 2.3 Response Generation Another line of related research focuses on generating responses for human-computer conversation. Ritter et al., (2011) trained a phrase-based statistical machine translation model on a corpus of utterance pairs extracted from Twitter human-human conversation and used it as a response generator"
D16-1036,W15-4640,0,0.618745,"oduction Selecting a potential response from a set of candidates is an important and challenging task for open-domain human-computer conversation, especially for the retrieval-based human-computer conversation. In general, a set of candidate responses from the indexed conversation corpus are retrieved, and then the best one is selected from the candidates as the system’s response (Ji et al., 2014). Previous Deep Neural Network (DNN) based approaches to response selection represent context and response as two embeddings. The response is selected based on the similarity of these two embeddings (Lowe et al., 2015; Kadlec et al., 2015). In ∗ These two authors contributed equally This paper proposes a multi-view response selection model, which integrates information from both word sequence view and utterance sequence view. Our assumption is that each view can represent relationships between context and response from a particular aspect, and features extracted from the word sequence and the utterance sequence provide complementary information for response selection. An effective integration of these two views is expected to improve the model performance. To the best of our knowledge, this is the first wo"
D16-1036,D14-1162,0,0.116556,"Missing"
D16-1036,D11-1054,0,0.46192,"the similarity of embeddings between the context and response. Similar to the work of Lowe et al., Kadlec et al., (2015) replaced LSTM with Temporal Convolutional Neural Networks (TCNN) (Kim, 2014) and BidirectLSTM. Their experimental results show that models with LSTM perform better than other neural networks. However, the utterance-level discourse information and dependencies have been left out in these studies since they view the context and response as word sequences. 2.3 Response Generation Another line of related research focuses on generating responses for human-computer conversation. Ritter et al., (2011) trained a phrase-based statistical machine translation model on a corpus of utterance pairs extracted from Twitter human-human conversation and used it as a response generator for single-turn conversation. Vinyals and Le (2015) regarded single-turn conversation as a sequence-tosequence problem and proposed an encoder-decoder based response generation model, where the post response is first encoded using LSTM and its embedding used as the initialization state of another LSTM to generate the response. Shang et al., (2015) improved the encoder-decoder based model using attention signals. Sordoni"
D16-1036,2005.sigdial-1.6,0,0.00892499,"ng positive response, 9 negative responses are randomly selected for further evaluation. 4.2 Experiment Setup Following the work of Lowe et al., (2015), the evaluation metric is 1 in m Recall@k (denoted 1 in m 2 Preprocessing includes tokenization, recognition of named entity, urls and numbers. 377 R@k), where a response selection model is designed to select k most likely responses among m candidates, and it gets the score “1” if the correct response is in the k selected ones. This metric can be seen as an adaptation of the precision and recall metrics previously applied to dialogue datasets (Schatzmann et al., 2005). It is worth noticing that 1 in 2 R@1 equals to precision and recall in binary classification. 4.3 Model Training and Hyper-parameters We initialize word embeddings with a pre-trained embedding matrix through GloVe (Pennington et al., 2014) 3 . We use Stochastic Gradient Descent (SGD) for optimizing. Hidden size for a gated recurrent unit is set to 200 in both word sequence model and utterance sequence model. The number of convolutional kernels is set to 200. Our initial learning rate is 0.01 with mini-batch size of 32. Other hyperparameters are set exactly the same as the baseline. We train"
D16-1036,P15-1152,0,0.102237,"esearch focuses on generating responses for human-computer conversation. Ritter et al., (2011) trained a phrase-based statistical machine translation model on a corpus of utterance pairs extracted from Twitter human-human conversation and used it as a response generator for single-turn conversation. Vinyals and Le (2015) regarded single-turn conversation as a sequence-tosequence problem and proposed an encoder-decoder based response generation model, where the post response is first encoded using LSTM and its embedding used as the initialization state of another LSTM to generate the response. Shang et al., (2015) improved the encoder-decoder based model using attention signals. Sordoni et al., (2015) proposed a context-sensitive response generation model, where the context is represented by bag-of-words and fed into a recurrent language model to generate the next response. In this paper, we focused on the task of response selection. ; !&quot; &lt; = 1 &, ) = $ (+&quot; + ℎ/ ⨀ (&quot; &&quot; ℎ341 ℎ3 ) ⨀ )&quot; ℎ/ ... 01 02 0341 03 ... A1 A2 91 92 & 015 01 02 06 07 08 A15 A1 96 A8 ) Figure 1: Word sequence model for response selection 3 Response Selection Model 3.1 In the task of response selection, a conventional DNN-based arch"
D16-1036,N15-1020,0,0.0117317,"(2011) trained a phrase-based statistical machine translation model on a corpus of utterance pairs extracted from Twitter human-human conversation and used it as a response generator for single-turn conversation. Vinyals and Le (2015) regarded single-turn conversation as a sequence-tosequence problem and proposed an encoder-decoder based response generation model, where the post response is first encoded using LSTM and its embedding used as the initialization state of another LSTM to generate the response. Shang et al., (2015) improved the encoder-decoder based model using attention signals. Sordoni et al., (2015) proposed a context-sensitive response generation model, where the context is represented by bag-of-words and fed into a recurrent language model to generate the next response. In this paper, we focused on the task of response selection. ; !&quot; &lt; = 1 &, ) = $ (+&quot; + ℎ/ ⨀ (&quot; &&quot; ℎ341 ℎ3 ) ⨀ )&quot; ℎ/ ... 01 02 0341 03 ... A1 A2 91 92 & 015 01 02 06 07 08 A15 A1 96 A8 ) Figure 1: Word sequence model for response selection 3 Response Selection Model 3.1 In the task of response selection, a conventional DNN-based architecture represents the context and response as low dimensional embeddings with deep lear"
D16-1036,P01-1066,0,0.102718,"hen we move on to a detailed description of our model in Section 3. Experimental results are described in Section 4. Analysis of our models is shown in Section 5. We conclude the paper in Section 6. 2 2.1 Related Work Conversation System Establishing a machine that can interact with human beings via natural language is one of the most challenging problems in Artificial Intelligent (AI). Early studies of conversation models are generally designed for specific domain, like booking restaurant, and require numerous domain knowledge as well as human efforts in model design and feature engineering (Walker et al., 2001). Hence it is too costly to adapt those models to other domains. Recently leveraging “big dialogs” for open domain conversation draws increasing research attentions. One critical issue for open domain conversation is to produce a reasonable response. Responding to this challenge, two promising solutions have been proposed: 1) retrieval-based model which selects a response from a large corpus (Ji et al., 2014; Yan et al., 2016; Yan et al., ). 2) generation-based model which directly generates the next utterance (Wen et al., 2015a; Wen et al., 2015b). 2.2 Response Selection Research on response"
D16-1036,W15-4639,0,0.0215736,"Missing"
D16-1036,D15-1199,0,0.00773748,"Missing"
D16-1046,W06-1615,0,0.0568294,"sferable are Neural Networks in NLP Applications? 1 Lili Mou,1 Zhao Meng,1 Rui Yan,2 Ge Li,1,† Yan Xu,1,∗ Lu Zhang,1 Zhi Jin1,† Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China Institute of Software, Peking University, China † Corresponding authors 2 Insitute of Computer Science and Technology of Peking University, China {doublepower.mou,rui.yan.peking}@gmail.com,zhaomeng.pku@outlook.com {lige,xuyan14,zhanglu,zhijin}@sei.pku.edu.cn Abstract al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), and structural correspondence learning (Blitzer et al., 2006; Prettenhofer and Stein, 2010). Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferabili"
D16-1046,D15-1075,0,0.0470276,"Missing"
D16-1046,W10-2608,0,0.0166113,"Missing"
D16-1046,P07-1033,0,0.0860103,"Missing"
D16-1046,J15-2004,0,0.0205849,"Missing"
D16-1046,D10-1044,0,0.0130425,"Missing"
D16-1046,P82-1020,0,0.869499,"Missing"
D16-1046,P07-1034,0,0.0828019,"Missing"
D16-1046,D15-1252,1,0.827416,"initializing parameters with the knowledge of S, we can reasonably expect that the parameters are in a better “catchment basin,” and that the INIT approach can transfer knowledge from S to T . 5 Results of Transferring by INIT We first analyze how INIT behaves in NLP-based transfer learning. In addition to two different transfer scenarios regarding semantic relatedness as described in Section 2, we further evaluated two settings: (1) fine-tuning parameters  1, and (2) freezing parameters after transfer . Existing evidence shows that frozen parameters would generally hurt ¡ the performance (Peng et al., 2015), but this setting ♂ provides a more direct understanding on how transferable the features are (because the factor of target domain optimization is ruled out). Therefore, we included it in our experiments. Moreover, we transferred parameters layer by layer to answer our second research question. Through Subsections 5.1–5.3, we initialized the parameters of T with the ones corresponding to the highest validation accuracy of S. In Subsection 5.4, we further investigated when the parameters are ready to be transferred during the training on S. 5.1 Overall Performance Table 3 shows the main result"
D16-1046,P13-1147,0,0.0282873,"Missing"
D16-1046,P10-1114,0,0.0132017,"tworks in NLP Applications? 1 Lili Mou,1 Zhao Meng,1 Rui Yan,2 Ge Li,1,† Yan Xu,1,∗ Lu Zhang,1 Zhi Jin1,† Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China Institute of Software, Peking University, China † Corresponding authors 2 Insitute of Computer Science and Technology of Peking University, China {doublepower.mou,rui.yan.peking}@gmail.com,zhaomeng.pku@outlook.com {lige,xuyan14,zhanglu,zhijin}@sei.pku.edu.cn Abstract al., 2010), instance weighting (Jiang and Zhai, 2007; Foster et al., 2010), and structural correspondence learning (Blitzer et al., 2006; Prettenhofer and Stein, 2010). Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.1"
D16-1046,D11-1014,0,0.0262261,"Missing"
D16-1046,D15-1279,1,\N,Missing
D17-1233,W14-4012,0,0.00569695,"Missing"
D17-1233,J90-1003,0,0.369463,"In this work, we implement f using GRU. The decoder RNN generates each reply word conditioned on the context vector C. The probability distribution pt of candidate words at every time step t is calculated as: Technical Background Seq2Seq Model and Attention Mechanism (1) (4) where η is usually implemented as a multi-layer perceptron (MLP) with tanh as an activation function. 2191 User’s query ?? … ?? … ?? ?? Online process Cue word ?? (?? ) ?? … ?? ?? … ?? ℎ? Pre-process ?? … ?? ?? … ?? Trained model Input: ??−1 ?? … ?? ?? … ?? Pointwise Mutual Information Pointwise mutual information (PMI) (Church and Hanks, 1990) is a measure of association ratio based on the information theoretic concept of mutual information. Given a pair of outcomes x and y belonging to discrete random variables X and Y, the PMI quantifies the discrepancy between the probability of their coincidence based on their joint distribution and their individual distributions. Mathematically: PMI(x, y) = log p(x, y) p(x|y) = log p(x)p(y) p(x) (5) This quantity is zero if x and y are independent, positive if they are positively correlated, and negative if they are negatively correlated. 3 ?1 ?2 ?3 ?? = ℎ1 ℎ2 ℎ3 … ℎ? ?0 ?1 ?2 ??−1 Cue word pr"
D17-1233,N16-1014,0,0.449992,"ery as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-text conversation without context information. Despite this, the performance of Seq2Seq generation-based conversation systems is far from satisfactory because its generation process is not controllable; it responses to a query according to the pattern learned from the training corpus. As a result, the system is likely to generate an unexpected reply even with little semantics, e.g, “I don’t know” and “Okay” due to the high frequency of these patterns in training data (Li et al., 2016a; Mou et al., 2016). To address this issue, Li et al. (2016a) proposed to increase diversity in the Seq2Seq model so that more informative utterances have a chance to stand out. Mou et al. (2016) provided a content-introducing approach that generates a reply based on a predicted word. The word is usually enlightening and drives the generated response to be more meaningful. However, this method is to some extent rigid; it requires the predicted word to explicitly occur in the generated utterance. As shown in Table 1, sometimes, it is better to generate a semantic related sentence based on the"
D17-1233,C16-1316,1,0.897694,"n open domains, researchers mainly focus on data-driven approaches, since the diversity and uncertainty make it impossible to prepare the interaction logic and domain knowledge. Basically, there are two mainstream ways to build an opendomain conversation system: 1) to search preestablished database for candidate responses by ∗ Corresponding author: ruiyan@pku.edu.cn query retrieval (Isbell et al., 2000; Wang et al., 2013; Yan et al., 2016; Song et al., 2016), and 2) to generate a new, tailored utterance given the userissued query (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Mou et al., 2016; Song et al., 2016). In these studies, generation-based conversation systems have shown impressive potential. Especially, the Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) based on neural networks has been extensively used in practice; the idea is to encode a query as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-text conversation without context information. Despite this, the performance of Seq2Seq generation-based conversation systems is far from satisfactory because its generation process is not contr"
D17-1233,P02-1040,0,0.107195,"tic Language Generation in Dialogue (SLGD) method (Wen et al., 2015a), which added additional features in each gate of the neural cell. FGRU: To explore more fusion strategies, intuitively, we fused the cue word and hidden states by vector concatenation during the decoding process. Note that rGRU and SCGRU incorporate additional information by gating mechanisms, while SLGD and FGRU fuse the information into each gate of the neural cell directly. 4.3 Experiment Evaluation Objective metrics. To evaluate the performance of different methods for the conversation generation task, we leverage BLEU (Papineni et al., 2002) as the automatic evaluation metric, which is originally designed for machine translation and evaluates the output by using n-gram matching between the output and the reference. Here, we use BLEU-1, BLEU-2 and BLEU-3 in our experiments. Subjective metrics. Since automatic metrics may not consistently agree with human perception (Stent et al., 2005), human testing is essential to assess subjective quality. Hence, we randomly sampled 150 queries in the test set, then we invited five annotators to offer a judgment. For fairness, all of our human evaluation was conducted in a random, blind fashion"
D17-1233,D11-1054,0,0.017995,"hange the “hard” content-introducing method into a new “soft” schema. The rest of paper is organized as follows. We start by introducing the technical background. In Section 3, we describe our proposed method. In Section 4, we illustrate the experimental setup and evaluations against a variety of baselines. Section 5 briefly reviews related work. Finally, we conclude our paper in Section 6. 2 2.1 sentence as a vector by a recurrent neural network (RNN) and to decode the vector to a target sentence by another RNN. Now, the conversational generation is treated as a monolingual translation task (Ritter et al., 2011; Shang et al., 2015). Given a query Q = (x1 , ..., xn ), the encoder represents it as a context vector C and then the decoder generates a response R = (y1 , ..., ym ) word by word by maximizing the generation probability of R conditioned on Q. The objective function of Seq2Seq can be written as: p(y1 , ..., ym |x1 , ..., xn ) =p(y1 |C) T Y p(yt |C, y1 , ..., yt−1 ) t=2 To be specific, the encoder RNN calculates the context vector by: ht = f (xt , ht−1 ); C = hT st = f (yt−1 , st−1 , C); pt = softmax(st , yt−1 ) (3) where st is the hidden state of decoder RNN at time t and yt−1 is the generate"
D17-1233,P15-1152,0,0.195597,". For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural network framework driven by web data. Recently, generation-based conversation systems have shownimpressive potential. Shang et al. (2015) generate replies for short-text conversation by Seq2Seq-basedneural networks with local and global attentions. 5.2 Content Introducing In vertical domains, Wen et al. (2015b) apply an additional control cell to gate the dialogue act (DA) features during the generation process to ensure the generated repliesexpressthe intended meaning. Also, the Stochastic Language Generation in Dialogue method (Wen et al., 2015a) adds additional features in each gate of the neural cell. Xu et al. (2016) introduce a new trainable gate to recall the global domain memory to enhance the ability of modeling the se"
D17-1233,P01-1066,0,0.104456,"ken as a desktop for a long while. (Screenshot) As the lockscreen? Make acquaintance and seek chances for further relations! (Freshman) I am also the new! Nice to meet you. Table 5: The implicit introducing-content cases of our HGFU model. The cue word in bold is not contained in the reply, while the response is still related to the cue word. T aemin† is a Korean singer. 5 5.1 Related work Conversation Systems Automatic human-computer conversation has attractedincreasing attention over the past few years. At the very beginning, people start the research using hand-crafted rules and templates (Walker et al., 2001; Misu and Kawahara, 2007; Williams et al., 2013). These approaches require no data or little data for trainingbuthuge manual effort to build the model, which is very timeconsuming. For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep"
D17-1233,D13-1096,0,0.0172018,"ers and practitioners. In particular, automatic conversation systems in open domains are attracting increasing attention due to its wide applications, such as virtual assistants and chatbots. In open domains, researchers mainly focus on data-driven approaches, since the diversity and uncertainty make it impossible to prepare the interaction logic and domain knowledge. Basically, there are two mainstream ways to build an opendomain conversation system: 1) to search preestablished database for candidate responses by ∗ Corresponding author: ruiyan@pku.edu.cn query retrieval (Isbell et al., 2000; Wang et al., 2013; Yan et al., 2016; Song et al., 2016), and 2) to generate a new, tailored utterance given the userissued query (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Mou et al., 2016; Song et al., 2016). In these studies, generation-based conversation systems have shown impressive potential. Especially, the Sequence-to-Sequence (Seq2Seq) model (Sutskever et al., 2014) based on neural networks has been extensively used in practice; the idea is to encode a query as a vector and to decode the vector into a reply. Inspired by (Mou et al., 2016), we mainly focus on the generative short-te"
D17-1233,W15-4639,0,0.345872,"edings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2190–2199 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Query Cue Word Reply Query Cue Word Reply 你不觉得好丑吗(Don’t you think it is ugly?) 审美(Aesthetics) 好恶心啊! (It’s disgusting!) 先放个大招(Let me use my ultimate power.) 技能(Skill) 新技能？(New skill?) Table 1: The content-introducing conversation examples. eration. 1) How to add the additional cue words during the generation process? One of the prevailing methods is modifying the neural cell with various gating mechanisms (Wen et al., 2015a,b; Xu et al., 2016). However, we need careful operation to ensure the neuron works as expected. 2) How to display the cue words in replies? As mentioned above, the explicit content-introducing approach in (Mou et al., 2016) does not fit well with all situations. In this paper, we present an implicit contentintroducing method for generative conversation systems, which incorporates cue words using our proposed hierarchical gated fusion unit (HGFU) in a flexible way. Our main contributions are as follows: • We propose the cue word GRU, another neural cell, to deal with the auxiliary information"
D17-1233,D15-1199,0,0.0607099,"Missing"
D17-1233,W13-4065,0,0.00407011,") As the lockscreen? Make acquaintance and seek chances for further relations! (Freshman) I am also the new! Nice to meet you. Table 5: The implicit introducing-content cases of our HGFU model. The cue word in bold is not contained in the reply, while the response is still related to the cue word. T aemin† is a Korean singer. 5 5.1 Related work Conversation Systems Automatic human-computer conversation has attractedincreasing attention over the past few years. At the very beginning, people start the research using hand-crafted rules and templates (Walker et al., 2001; Misu and Kawahara, 2007; Williams et al., 2013). These approaches require no data or little data for trainingbuthuge manual effort to build the model, which is very timeconsuming. For now, buildinga conversation systemmainly falls into two categories: retrievalbased and generation-based. As information retrieval techniques are developing fast, Leuski et al. (2009) build systems to select the most suitable response from the query-reply pairs using a statistical language model in cross-lingual information retrieval. Yan et al. (2016) propose a retrieval-based conversation system with the deep learning-to-respond schema through a deep neural"
D17-1233,W06-1303,0,\N,Missing
D18-1089,N18-1150,0,0.0110706,"s of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 785–790 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Neural Abstractive Summarization 2017; Yasunaga et al., 2017). In practice, a severe issue of repetitive generation has been reported in other related work. It has been shown helpful to encourage diversity and novelty in calculating attention weights (Chen et al., 2016; Nema et al., 2017), or incorporating different modules with mutual communications to encode different paragraphs in the input document (Celikyilmaz et al., 2018). Another perspective is to promote better information coverage, such as pre-estimating term frequencies in the target summary (Suzuki and Nagata, 2017) or directly introducing a coverage loss between encoder states and decoder states (See et al., 2017). Among the aforementioned related studies, a few proposed systems explicitly targeted at generating abstractive summaries for documents. However, these systems highly rely on the attention mechanism and/or copying mechanism that heavily depends on different part of input during the decoding stage. This naturally brings to a question on whether"
D18-1089,P16-1188,0,0.0278335,"ent neural document summarizers abstractive? In this work, we conduct such a study on the popularly-used CNN / DailyMail news corpora. By calculating various types of overlaps between summaries generated by neural abstractive summarizers and the original document, we verified that many systems are in fact heavily extracting text spans from input. Recent studies found that automated methods can generate a wider range of summaries by extracting over sub-sentential units of meaning, such as elementary discourse units (EDUs), from the source documents rather than whole sentences (Li et al., 2016; Durrett et al., 2016). We built on a rather standard pointer-generator system to produce a summarizer that purely copies from input. Limited vocabulary size makes the new summarizer more computationally efficient, without loss of performance. The findings in this paper may hint future studies towards more efficient and more effective near-extractive systems, instead of a less important target of improving abstraction. To summarize, in this paper we provide: Many modern neural document summarization systems based on encoder-decoder networks are designed to produce abstractive summaries. We attempted to verify the d"
D18-1089,P16-1014,0,0.0322406,"or recurrent network to generate sentence vectors from original tokens (Cheng and Lapata, 2016). Meanwhile, the attention mechanism will become hierarchical as well (Nallapati et al., 2016). When decoding, sentence-level attention weights will be used as input for calculating word-level attention weights. Experimental results in previous work suggest that such schemes could be useful for extractive summarization when calculating sentence weights, but could only generate rather disappointing results for abstractive summaries. It has been shown to be useful to incorporate the copying mechanism (Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) that allows a word to be generated by directly copying an input word rather than producing all words from the hidden state from scratch. Meanwhile, directly optimizing ROUGE via reinforcement learning has been shown to be more effective than optimizing reference likelihood (Paulus et al., 2018). Recent work has achieved improvements by modeling attention based on more structured intersentence relationships such as graphs (Tan et al., 3 Quantitative Analysis 3.1 Approaches To verify whether current abstractive neural summarizers are just lazy generators that"
D18-1089,D15-1044,0,0.116039,"a question on whether neural summarizers are indeed generating abstractive summaries after reading and digesting the input document, or they are just extracting subparts of the original document to perform near-extractive summaries. Recently end-to-end training with encoderdecoder neural networks (Sutskever et al., 2014) have achieved huge success in data sufficient sequence transduction tasks such as machine translation, which brings potential applications for summarization tasks, especially for abstractive settings. Earlier practice is mostly achieved on abstractive sentence summarization (Rush et al., 2015), which is essentially sentence simplification working on short text inputs. These neural sentence abstraction models are able to achieve good ROUGE (Lin and Hovy, 2003) scores on headline generation benchmarks, 1 but have not been proved to be useful for generating summaries with multiple sentences for full documents with longer contexts, which is the main focus of this study on document summarization. One possible way for document-level neural summarization is to design hierarchical encoding to represent sentences and words at different levels. Related studies treat a document as a sequence"
D18-1089,A00-2024,0,0.219726,"near-extractive in practice, we also implemented a pure copy system, which achieved comparable results as abstractive summarizers while being far more computationally efficient. These findings suggest the possibility for future efforts towards more efficient systems that could better utilize the vocabulary in the original document. 1 Introduction Document summarization has been a hot research topic in natural language processing for long. When human writers summarize a document, they often edit its constituent sentences in order to succinctly capture the meaning of the document. For instance, Jing and McKeown (2000) observed that summary authors trimmed extraneous content, combined sentences, replaced phrases or clauses with more general or specific variants. The abstractive summaries thus involve sentences which deviate from those of the source document in structure or content. On the contrary, automated summarization generally produces extractive summaries by selecting complete sentences from the source document (Nenkova et al., 2011) to ensure that the output is grammatical. Recently, many modern neural summarization systems based on encoder-decoder networks have been proposed, aiming at producing abs"
D18-1089,P17-1099,0,0.71837,"actice, a severe issue of repetitive generation has been reported in other related work. It has been shown helpful to encourage diversity and novelty in calculating attention weights (Chen et al., 2016; Nema et al., 2017), or incorporating different modules with mutual communications to encode different paragraphs in the input document (Celikyilmaz et al., 2018). Another perspective is to promote better information coverage, such as pre-estimating term frequencies in the target summary (Suzuki and Nagata, 2017) or directly introducing a coverage loss between encoder states and decoder states (See et al., 2017). Among the aforementioned related studies, a few proposed systems explicitly targeted at generating abstractive summaries for documents. However, these systems highly rely on the attention mechanism and/or copying mechanism that heavily depends on different part of input during the decoding stage. This naturally brings to a question on whether neural summarizers are indeed generating abstractive summaries after reading and digesting the input document, or they are just extracting subparts of the original document to perform near-extractive summaries. Recently end-to-end training with encoderd"
D18-1089,D18-1207,0,0.265906,"Missing"
D18-1089,W16-3617,0,0.056318,"h degree are current neural document summarizers abstractive? In this work, we conduct such a study on the popularly-used CNN / DailyMail news corpora. By calculating various types of overlaps between summaries generated by neural abstractive summarizers and the original document, we verified that many systems are in fact heavily extracting text spans from input. Recent studies found that automated methods can generate a wider range of summaries by extracting over sub-sentential units of meaning, such as elementary discourse units (EDUs), from the source documents rather than whole sentences (Li et al., 2016; Durrett et al., 2016). We built on a rather standard pointer-generator system to produce a summarizer that purely copies from input. Limited vocabulary size makes the new summarizer more computationally efficient, without loss of performance. The findings in this paper may hint future studies towards more efficient and more effective near-extractive systems, instead of a less important target of improving abstraction. To summarize, in this paper we provide: Many modern neural document summarization systems based on encoder-decoder networks are designed to produce abstractive summaries. We at"
D18-1089,E17-2047,0,0.0160613,"ciation for Computational Linguistics 2 Neural Abstractive Summarization 2017; Yasunaga et al., 2017). In practice, a severe issue of repetitive generation has been reported in other related work. It has been shown helpful to encourage diversity and novelty in calculating attention weights (Chen et al., 2016; Nema et al., 2017), or incorporating different modules with mutual communications to encode different paragraphs in the input document (Celikyilmaz et al., 2018). Another perspective is to promote better information coverage, such as pre-estimating term frequencies in the target summary (Suzuki and Nagata, 2017) or directly introducing a coverage loss between encoder states and decoder states (See et al., 2017). Among the aforementioned related studies, a few proposed systems explicitly targeted at generating abstractive summaries for documents. However, these systems highly rely on the attention mechanism and/or copying mechanism that heavily depends on different part of input during the decoding stage. This naturally brings to a question on whether neural summarizers are indeed generating abstractive summaries after reading and digesting the input document, or they are just extracting subparts of t"
D18-1089,N03-1020,0,0.617341,"Missing"
D18-1089,P17-1108,0,0.0510883,"Missing"
D18-1089,K17-1045,0,0.0196519,"document summarizers are by calculating various types of content overlap with the input documents. • A simple modification on the standard pointer-generator document summarizer to produce equally good near-extractive summaries while being more computationally efficient due to largely reduced vocabulary size. The first two authors contributed equally. 785 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 785–790 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Neural Abstractive Summarization 2017; Yasunaga et al., 2017). In practice, a severe issue of repetitive generation has been reported in other related work. It has been shown helpful to encourage diversity and novelty in calculating attention weights (Chen et al., 2016; Nema et al., 2017), or incorporating different modules with mutual communications to encode different paragraphs in the input document (Celikyilmaz et al., 2018). Another perspective is to promote better information coverage, such as pre-estimating term frequencies in the target summary (Suzuki and Nagata, 2017) or directly introducing a coverage loss between encoder states and decoder s"
D18-1089,K16-1028,0,0.0381858,"th multiple sentences for full documents with longer contexts, which is the main focus of this study on document summarization. One possible way for document-level neural summarization is to design hierarchical encoding to represent sentences and words at different levels. Related studies treat a document as a sequence of sentences and take sentence embeddings as input for a document-level RNN, while using a convolutional network or recurrent network to generate sentence vectors from original tokens (Cheng and Lapata, 2016). Meanwhile, the attention mechanism will become hierarchical as well (Nallapati et al., 2016). When decoding, sentence-level attention weights will be used as input for calculating word-level attention weights. Experimental results in previous work suggest that such schemes could be useful for extractive summarization when calculating sentence weights, but could only generate rather disappointing results for abstractive summaries. It has been shown to be useful to incorporate the copying mechanism (Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) that allows a word to be generated by directly copying an input word rather than producing all words from the hidden state from scr"
D18-1089,P17-1098,0,0.01799,"e being more computationally efficient due to largely reduced vocabulary size. The first two authors contributed equally. 785 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 785–790 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Neural Abstractive Summarization 2017; Yasunaga et al., 2017). In practice, a severe issue of repetitive generation has been reported in other related work. It has been shown helpful to encourage diversity and novelty in calculating attention weights (Chen et al., 2016; Nema et al., 2017), or incorporating different modules with mutual communications to encode different paragraphs in the input document (Celikyilmaz et al., 2018). Another perspective is to promote better information coverage, such as pre-estimating term frequencies in the target summary (Suzuki and Nagata, 2017) or directly introducing a coverage loss between encoder states and decoder states (See et al., 2017). Among the aforementioned related studies, a few proposed systems explicitly targeted at generating abstractive summaries for documents. However, these systems highly rely on the attention mechanism and/"
D18-1423,K16-1002,0,0.064903,"in a topic, especially when they are generated or extracted from an inventory (Wang et al., 2016c). On the other hand, Chinese poems are generally short in length, with every character carefully chosen to be concise and elegant. Yet, prior poem generation models with recurrent neural networks (RNN) are likely to generate highfrequency characters (Zhang et al., 2017a), and the resulted poems are trivial and boring. The reason is that RNN tends to be entrapped within local word co-occurrences, they normally fail to capture global characteristic such as topic or hierarchical semantic properties (Bowman et al., 2016). To address the aforementioned shortcomings, RNN is extended to autoencoder (Dai and Le, 2015) for improving sequence learning, which has *Corresponding author: Rui Yan (ruiyan@pku.edu.cn) †Work was partially done at Tencent AI Lab. 1 We use term and character interchangeably in this paper. 3890 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3890–3900 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics been proven to be appealing in explicitly modeling global properties such as syntactic, semantic, a"
D18-1423,D16-1126,0,0.031283,"s confirm the validity and effectiveness of our model, where its automatic and human evaluation scores outperform existing models. 1 Introduction In mastering concise, elegant wordings with aesthetic rhythms in fixed patterns, classical Chinese poem is a special cultural heritage to record personal emotions and political views, as well as document daily or historical events. Being a fascinating art, writing poems is an attractive task that researchers of artificial intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consist"
D18-1423,P17-4008,0,0.0532257,"Missing"
D18-1423,P17-1059,0,0.0269444,"e the reconstruction log-likelihood of the input x under the condition of c. Following the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is generated only conditioned on the title T"
D18-1423,D10-1051,0,0.142401,"ion with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular text genre, poetry. Automatic Poem Generation. According to methodology, previous approaches can be roughly classified into three categories: 1) rule and template based methods (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Zhou et al., 2010; Oliveira, 2012; Yan et al., 2013); 2) SMT approaches (Jiang and Zhou, 2008; Greene et al., 2010; He et al., 2012); 3) deep neural models (Zhang and Lapata, 2014; Wang et al., 2016b; Yan, 2016). Compared to rule-based and SMT models, neural models are able to learn more complicated representations and generate smooth poems. Most recent studies followed this paradigm. For example, Wang et al. (2016c) proposed a modified encoder-decoder model with keyword planning; Zhang et al. (2017a) adopted memory-augmented RNNs to dynamically choose each term from RNN output or a reserved inventory. To improve thematic consistency, Yang et al. (2017) combined CVAE and keywords planning. Compared to the"
D18-1423,P17-1016,0,0.0172345,"(Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is preferred that all lines concentrate on the same theme throughout a poem. Previous work mainly focused on using keywords (Wang et al., 2016c; Hopkins and Kiela, 2017) to plan a poem so as to generate each line with a specific keyword. Such strategy is risky for the reason that the keywords are not guaranteed consistent in a topic, especially when they are generated or extracted from an inventory (Wang et al., 2016c). On the other hand, Chinese poems are generally short in length, with every character carefully chosen to be concise and elegant. Yet, prior poem generation models with recurrent neural networks (RNN) are likely to generate highfrequency characters (Zhang et al., 2017a), and the resulted poems are trivial and boring. The reason is that RNN tend"
D18-1423,C08-1048,0,0.0401591,"2017) and text generation with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular text genre, poetry. Automatic Poem Generation. According to methodology, previous approaches can be roughly classified into three categories: 1) rule and template based methods (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Zhou et al., 2010; Oliveira, 2012; Yan et al., 2013); 2) SMT approaches (Jiang and Zhou, 2008; Greene et al., 2010; He et al., 2012); 3) deep neural models (Zhang and Lapata, 2014; Wang et al., 2016b; Yan, 2016). Compared to rule-based and SMT models, neural models are able to learn more complicated representations and generate smooth poems. Most recent studies followed this paradigm. For example, Wang et al. (2016c) proposed a modified encoder-decoder model with keyword planning; Zhang et al. (2017a) adopted memory-augmented RNNs to dynamically choose each term from RNN output or a reserved inventory. To improve thematic consistency, Yang et al. (2017) combined CVAE and keywords plan"
D18-1423,D16-1032,0,0.0321585,"e objective of CVAE is thus to maximize the reconstruction log-likelihood of the input x under the condition of c. Following the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is gen"
D18-1423,N16-1014,0,0.0818547,"is paper. Similarity: For thematic consistency, it is challenging to automatically evaluate different models. We adopt the embedding average metric to score sentence-level similarity as that was applied in Wieting et al. (2015). In this paper, we accumulate the embeddings of all characters from the generated poems and that from the given title, and use cosine to compute the similarity between the two accumulated embeddings. Distinctness: As an important characteristic, poems use novel and unique characters to maintain their elegance and delicacy. Similar to that proposed for dialogue systems (Li et al., 2016), this evaluation is employed to measure character diversity by calculating the proportion of distinctive [1,4]-grams12 in the generated poems, where final distinctness values are normalized to [0,100]. Human Evaluation: Since writing poems is a complicated task, there always exist incoordinations between automatic metrics and human experiences. Hence, we conduct human evaluation to 11 We tried different values for λ, varying from 0.001 to 1, which result in similar performance of the CVAE-D. 12 Defined as the number of distinctive n-grams divided by the total number of n-grams, shown as Dist-"
D18-1423,P15-1107,0,0.0609514,"Missing"
D18-1423,D17-1230,0,0.0259758,"s may deliver different mood from others. Since our model does not explicitly control such attributes, thus one potential solution to address this issue is to introduce other features to model such information, which requires a special design to adjust the current model. We also notice there exists a few extraordinary bad cases where their basic characteristics, such as wording, fluency, etc., are unacceptable. This phenomenon is randomly observed with no patterns, which could be explained by the complexity of the model and the fragile natural of adversarial training (Goodfellow et al., 2014; Li et al., 2017). Careful parameter setting and considerate module assemble could mitigate this problem, thus lead to potential future work of designing more robust frameworks. 6 Related Work Deep Generative Models. This work can be seen as an extension of research on deep generative models (Salakhutdinov and Hinton, 2009; Bengio et al., 2014), where most of the previous work, including VAE and CVAE, focused on image generation (Sohn et al., 2015; Yan et al., 2016b). Since GAN (Goodfellow et al., 2014) is also a successful generative model, there are studies tried to integrate VAE and GAN (Larsen et al., 2016"
D18-1423,P02-1040,0,0.102408,"2.16 2.14 2.58 2.29 2.08 2.53 2.35 2.34 2.96 Ovr. 1.74 1.79 2.13 2.23 2.13 2.14 2.18 2.56 Table 3: Results of automatic and human evaluations. BLEU-1 and BLEU-2 are BLEU scores on unigrams and bigrams (p < 0.01); Sim refer to the similarity score; Dist-n corresponds to the distinctness of n-gram, with n = 1 to 4; Con., Flu., Mea., Poe., Ovr. represent consistency, fluency, meaning, poeticness, and overall, respectively. 2017) to set the balancing parameter λ to 0.1.11 4.4 Evaluation Metrics To comprehensively evaluate the generated poems, we employ the following metrics: BLEU: The BLEU score (Papineni et al., 2002) is an effective metric, widely used in machine translation, for measuring word overlapping between ground truth and generated sentences. In poem generation, BLEU is also utilized as a metric in previous studies (Zhang and Lapata, 2014; Wang et al., 2016a; Yan, 2016; Wang et al., 2016b). We follow their settings in this paper. Similarity: For thematic consistency, it is challenging to automatically evaluate different models. We adopt the embedding average metric to score sentence-level similarity as that was applied in Wieting et al. (2015). In this paper, we accumulate the embeddings of all c"
D18-1423,C16-1100,0,0.272591,"that researchers of artificial intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consisten"
D18-1423,P17-1046,0,0.0143534,"aintaining the advantages of VAE. It is verified in supervised dialogue generation (Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017) that CVAE can generate better responses with given dialogue contexts. Given the above background and to align it with our expectations for poem generation, it is worth trying to apply CVAE to create poems. In the meantime, consider that modeling thematic consistency with adversarial training is proven to be promising in controlled text generation (Hu et al., 2017), models for semantic matching can be potentially improved with an explicit discriminator (Wu et al., 2017), so does poem generation. In this paper, we propose a novel poem generation model (CVAE-D) using CVAE to generate novel terms and a discriminator (D) to explicitly control thematic consistency with adversarial training. To the best of our knowledge, this is the first work of generating poems with the combination of CVAE and adversarial training. Experiments on a large classical Chinese poetry corpus confirm that, through encoding inputs with latent variables and explicit measurement of thematic information, the proposed model outperforms existing ones in various evaluations. Quantitative and"
D18-1423,P17-1125,0,0.200231,"intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is"
D18-1423,D14-1074,0,0.518039,"the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is generated only conditioned on the title T , once this step is done, the model takes the input of the previous generated line as wel"
D18-1423,Q17-1036,0,0.166232,"intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is"
D18-1423,P17-1061,0,0.472874,"obal properties such as syntactic, semantic, and discourse coherence (Li et al., 2015). Moreover, boosting autoencoder with variational inference (Kingma and Welling, 2014), known as variational autoencoder (VAE), can generate not only consistent but also novel and fluent term sequences (Bowman et al., 2016). To generalize VAE for versatile scenarios, conditional variational autoencoders (CVAE) are proposed to supervise a generation process with certain attributes while maintaining the advantages of VAE. It is verified in supervised dialogue generation (Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017) that CVAE can generate better responses with given dialogue contexts. Given the above background and to align it with our expectations for poem generation, it is worth trying to apply CVAE to create poems. In the meantime, consider that modeling thematic consistency with adversarial training is proven to be promising in controlled text generation (Hu et al., 2017), models for semantic matching can be potentially improved with an explicit discriminator (Wu et al., 2017), so does poem generation. In this paper, we propose a novel poem generation model (CVAE-D) using CVAE to generate novel terms"
D18-1423,P16-1222,1,0.843427,"with no patterns, which could be explained by the complexity of the model and the fragile natural of adversarial training (Goodfellow et al., 2014; Li et al., 2017). Careful parameter setting and considerate module assemble could mitigate this problem, thus lead to potential future work of designing more robust frameworks. 6 Related Work Deep Generative Models. This work can be seen as an extension of research on deep generative models (Salakhutdinov and Hinton, 2009; Bengio et al., 2014), where most of the previous work, including VAE and CVAE, focused on image generation (Sohn et al., 2015; Yan et al., 2016b). Since GAN (Goodfellow et al., 2014) is also a successful generative model, there are studies tried to integrate VAE and GAN (Larsen et al., 2016). In natural language processing, many recent deep generative models are applied to dialogue systems Serban et al. (2017); Shen et al. (2017); Zhao et al. (2017) and text generation with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular tex"
D18-1442,N18-1158,0,0.411279,"the focus on abstractive summarization, extractive summarization remains an attractive method as it is capable of generating more grammatically and semantically correct summaries. This is the method we follow in this work. In extractive summarization, Cheng and Lapata (2016) propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attention-based extractor. Following this, Nallapati et al. (2016a) propose a simple RNN-based sequence classifier which outperforms or matches the state-of-art models at the time. In another approach, Narayan et al. (2018) use a reinforcement learning method to optimize the Rouge evaluation metric for text summarization. The most recent work on this topic is (Wu and Hu, 2018), where the authors train a reinforced neural extractive summarization model called RNES that captures cross-sentence coherence patterns. Due to the fact that they use a different dataset and have not released their code, we are unable to compare our models with theirs. The idea of iteration has not been well explored for summarization. One related study is Xiong et al. (2016)’s work on dynamic memory networks, which designs neural networks"
D18-1442,D14-1162,0,0.0836015,"ed our model in Tensorflow (Abadi et al., 2016). The code for our models is available online1 . We mostly followed the settings in (Nallapati et al., 2016a) and trained the model using the Adam optimizer (Kingma and Ba, 2014) with initial learning rate 0.001 and anneals of 0.5 every 6 epochs until reaching 30 epochs. We selected three sentences with highest scores as summary. After preliminary exploration, we found that arranging them according to their scores consistently achieved the best performance. Experiments were performed with a batch size of 64 documents. We used 100-dimension GloVe (Pennington et al., 2014) embeddings trained on Wikipedia 2014 as our embedding initialization with a vocabulary size limited to 100k for speed purposes. We initialized out-of-vocabulary word embeddings over a uniform distribution within [0.2,0,2]. We also padded or cut sentences to contain exactly 70 words. Each GRU module had 1 layer with 200-dimensional hidden states and with either an initial state set up as described above or a random initial state. To prevent overfitting, we used dropout after each GRU network and embedding layer, and also applied L2 loss to all unbiased variables. The iteration number was set t"
D18-1442,radev-etal-2004-mead,0,0.215319,"icle, removing secondary or redundant concepts. Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas,"
D18-1442,W05-0620,0,0.154828,"Missing"
D18-1442,D15-1044,0,0.0610103,"an be classified into extractive summarization and abstractive summarization. Extractive summarization aims to generate a summary by integrating the most salient sentences in the document. Abstractive summarization aims to generate new content that concisely paraphrases the document from scratch. With the emergence of powerful neural network models for text processing, a vast majority of the literature on document summarization is dedicated to abstractive summarization. These models typically take the form of convolutional neural networks (CNN) or recurrent neural networks (RNN). For example, Rush et al. (2015) propose an encoder-decoder model which uses a local attention mechanism to generate summaries. Nallapati et al. (2016b) further develop this work by addressing problems that had not been adequately solved by the basic architecture, such as keyword modeling and capturing the hierarchy of sentenceto-word structures. In a follow-up work, Nallapati et al. (2017) propose a new summarization model which generates summaries by sampling a topic one sentence at a time, then producing words using an RNN decoder conditioned on the sentence topic. Another related work is by See et al. (2017), where the a"
D18-1442,P16-1046,0,0.554299,"mmarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summarization. We share two examples of this. (1) Consider the situation where we almost finish reading a long article and forget some ma"
D18-1442,E17-2007,0,0.0838447,"Missing"
D18-1442,P17-1099,0,0.114979,"or example, Rush et al. (2015) propose an encoder-decoder model which uses a local attention mechanism to generate summaries. Nallapati et al. (2016b) further develop this work by addressing problems that had not been adequately solved by the basic architecture, such as keyword modeling and capturing the hierarchy of sentenceto-word structures. In a follow-up work, Nallapati et al. (2017) propose a new summarization model which generates summaries by sampling a topic one sentence at a time, then producing words using an RNN decoder conditioned on the sentence topic. Another related work is by See et al. (2017), where the authors use “pointing” and “coverage” techniques to generate more accurate summaries. Despite the focus on abstractive summarization, extractive summarization remains an attractive method as it is capable of generating more grammatically and semantically correct summaries. This is the method we follow in this work. In extractive summarization, Cheng and Lapata (2016) propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attention-based extractor. Following this, Nallapati et al. (2016a) propose a simple RNN-based se"
D18-1442,P10-1058,0,0.110183,"dary or redundant concepts. Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summari"
D18-1442,W04-3212,0,0.0184597,"tructed sentence representations. The iterative unit (also depicted above in Fig.1) is designed for this purpose. We use a GRUiter cell to generate the polished document representation, whose input is the final state of the selective reading network from the previous iteration, hns and whose initial state is set to the document representation of the previous iteration, Dk−1 . The updated document representation is computed by: Dk = GRUiter (hns , Dk−1 ) 4.3 (14) Decoder Next, we describe our decoders, which are depicted shaded in the right part of Fig.1. Following most sequence labeling task (Xue and Palmer, 2004; Carreras and M`arquez, 2005) where they learn a feature vector for each sentence, we use a bidirectional GRUdec network in each iteration to output features so as to calculate extracting probabilities. For k-th iteration, given the sentence rep→ resentation ← s as input and the document representation Dk as the initial state, our decoder encodes the features of all sentences in the hidden state hk = {hk0 , ..., hkns }: 4.4 → hki = GRUdec (← s , hki−1 ) (15) hk0 (16) = Dk Sentence Labeling Module Next, we use the feature of each sentence to generate corresponding extracting probability. Since"
D18-1442,P16-1222,1,0.822294,"ence patterns. Due to the fact that they use a different dataset and have not released their code, we are unable to compare our models with theirs. The idea of iteration has not been well explored for summarization. One related study is Xiong et al. (2016)’s work on dynamic memory networks, which designs neural networks with memory and attention mechanisms that exhibit certain reasoning capabilities required for question answering. Another related work is (Yan, 2016), where they generate poetry with iterative polishing sn chema. Similiar method can also be applied on couplet generation as in (Yan et al., 2016). We take some inspiration from their work but focus on document summarization. Another related work is (Singh et al., 2017), where the authors present a deep network called Hybrid MemNet for the single document summarization task, using a memory network as the document encoder. Compared to them, we do not borrow the memory network structure but propose a new iterative architecture. 4089 3 3.1 Methodology Problem Formulation In this work, we propose Iterative Text Summarization (ITS), an iteration-based supervised model for extractive text summarization. We treat the extractive summarization t"
D18-1442,D11-1124,1,0.758224,"Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) on human-engineered features such as sentence length, word position, and frequency (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a,b, 2012) or use neural networks to automatically learn features for sentence selection (Cheng and Lapata, 2016; Nallapati et al., 2016a). Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document’s main ideas, leading to a subpar summarization. We share t"
D19-1128,W15-4640,0,0.41228,"dely used random sampling strategy, although the first two strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampl"
D19-1128,W17-7301,0,0.123569,"ative sampling strategies have been studied in many machine learning tasks. In the computer vision fields, Faghri et al. (2017) studies hard negatives and introduces a simple change to common loss function on image-caption retrieval tasks. Guo et al. (2018) proposes a fast negative sampler which chooses negative examples that are most likely to meet the requirements of violation according to the latent factors of image. In natural language processing fields, Kotnis and Nastase (2017) analyses the impact of negative sampling strategies on the performance of link prediction in knowledge graphs. Saeidi et al. (2017) studies the affect of a tailored sample strategy on the performance of document retrieval task. Rao et al. (2016) uses three negative strategies to select the most informative negative samples on the pairwise ranking model for answer selection. Xu et al. (2015) introduces a straightforward negative sampling strategy to improve the assignment of subjects and objects on a convolution neural network. To our best knowledge, this is the first work to empirical study of negative sampling strategies for learning of matching models in multi-turn retrieval-based dialogue systems, which may enlighten f"
D19-1128,P19-1001,1,0.819447,"nes can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain many false negatives"
D19-1128,D13-1096,0,0.350074,"ing models in learning, we consider four strategies including minimum sampling, maximum sampling, semi-hard sampling, and decay-hard sampling. Empirical studies on two benchmarks with three matching models indicate that compared with the widely used random sampling strategy, although the first two strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human"
D19-1128,P18-2067,1,0.725816,"igh quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain many false negatives and trivial true negatives that are very easy to distinguish from those true positives. As a result, models with advanced architectures can only reach sub-optimal performance after learning (Wu et al., 2018). In this paper, instead of configuring new architectures, we investigate how to improve the performance of existing matching models with a better learning method. A learning method usually involves choice of loss functions and construction of training data, and we are particularly interested in automatic training data construction, as data are often more crucial to the performance of models. The key problem in training data construction lies in how to properly choose negative examples, and our idea is that negative examples should adapt to the matching models at different learning stages. Fol"
D19-1128,P17-1046,1,0.961365,"wo strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn"
D19-1128,D15-1062,1,0.889697,"Missing"
D19-1128,C18-1317,0,0.0686812,"d to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training s"
D19-1128,D16-1036,1,0.902016,"Missing"
D19-1128,P18-1103,0,0.783361,"p, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks. 1 Introduction In this work, we study the problem of response selection as an approach to implementing a retrievalbased dialogue system (Ji et al., 2014; Wang et al., 2013). A key step in response selection is measuring the matching degree between a conversation context and a response candidate. Existing studies focus on constructing a matching model with sophisticated neural architectures (Lowe et al., 2015; Zhou et al., 2016; Yan et al., 2016; Wu et al., 2017; Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019), but pay little attention to how to effectively learn such architectures from data. On the one hand, it is well known that learning of complicated neural architectures requires large-scale high quality training data; on the other hand, since human labeling is expensive and exhausting, most of the existing work just adopts a simple heuristic to automatically build a training set where human responses are treated as positive examples and negative response candidates are randomly sampled. ∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn). Such a training set might contain ma"
D19-1199,I17-2028,0,0.0211377,"ing for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with only one addressee. information based on different roles in conversations, such as senders and recipients (Chen et al., 2017; Chi et al., 2017; Luan et al., 2016). In multi-party conversations, identifying the relationship among users is also an important task. It can be categorized into two topics, 1) predicting who will be the next speaker (Meng et al., 2017) and 2) who is the addressee (Ouchi and Tsuboi, 2016; Zhang et al., 2017). For the first topic, Meng et al. (2017) investigated a temporal-based and a content-based method to jointly model the users and context. For the second topic, which is closely related to ours, Ouchi and Tsuboi (2016) proposed to predict the addressee and utterance given a context with all available info"
D19-1199,D14-1179,0,0.00765946,"Missing"
D19-1199,C16-1073,1,0.885519,"Missing"
D19-1199,D17-1230,0,0.0155911,"oach to jointly learn the representations of users and utterances and enhance them mutually. • The proposed approach (W2W) considers both previous and subsequent information in the session while incorporating the correlation with users and utterances. For conversations with complex structures, W2W models them in a uniform way and could handle any kind of occasion even when all the addressee information is missing. 2 Related Work In this section, we briefly review recent works and progresses on multi-party conversations. Multi-party conversations, as a general case of multi-turn conversations (Li et al., 2017, 2016c; Yan et al., 2016; Serban et al., 2016) involve more than two participants. In addition to the representation of learning for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with"
D19-1199,D16-1231,0,0.148673,"2 Related Work In this section, we briefly review recent works and progresses on multi-party conversations. Multi-party conversations, as a general case of multi-turn conversations (Li et al., 2017, 2016c; Yan et al., 2016; Serban et al., 2016) involve more than two participants. In addition to the representation of learning for utterances, another key issue is to model multiple participants in the conversations. It is intuitive to introduce multiple user embeddings for multi-party conversations, either as persona-dependent embeddings (Li et al., 2016b), or as persona-independent embeddings (Ouchi and Tsuboi, 2016; Zhang et al., 2017; Meng et al., 2017). Recently, some researchers utilized users’ 1 To make the model practical in learning, we assume that one utterance is associated with only one addressee. information based on different roles in conversations, such as senders and recipients (Chen et al., 2017; Chi et al., 2017; Luan et al., 2016). In multi-party conversations, identifying the relationship among users is also an important task. It can be categorized into two topics, 1) predicting who will be the next speaker (Meng et al., 2017) and 2) who is the addressee (Ouchi and Tsuboi, 2016; Zhang e"
D19-1199,D14-1162,0,0.0858018,"descending order according to the first time when they speak, and the i-th user is assigned with the i-th row of A(0) as ai(0) . The user matrix A(0) is trained as parameters along with other weight matrices in the neural network. Users of the same order in different sessions share the same initialization user embedding. Note that the user representations are independent of each personality (unique user). Such strategy guarantees the initialization user embeddings to carry position information as well as handle new users unseen in training data during addressee identification. 3 We use GloVe(Pennington et al., 2014), but it can be any word embeddings(Mikolov et al., 2013; Hu et al., 2016). 4 Such a hierarchical framework (Serban et al., 2016) takes into account the context of all previous and future sentences in the whole session, thus enables the model to learn a strong representation. 1911 ???? ????????????? Forward a?(?) a?(?) a?(?) (?1 , a???? ) 0 a?(?) a?(?) ? ????2 ) 0 a?(?) a?(?) ? ????3 ) 0 Utterance ? ? ????1 ) 2 a? a?(?) a? ????2 ) 2 (?32 , a a?(?) ????3 ) 1 (?23 , a Backward a? (?31 , a ? (?2 , a???? ) 1 a?(?) (?13 , a User ? ???? (?21 , a 1 1 ) a?(?) (?12 , a ?(0) a?(?) a? a? (?3 , a???? ) 2"
D19-1199,N16-1014,0,0.230717,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,P15-1152,0,0.0970801,"Missing"
D19-1199,P16-1094,0,0.136522,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,D19-1011,0,0.257771,"chmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Equal contribution. Corresponding author. Utterance ”Good point, tmux is the thing I miss.” ”Cool thanks for ur hel"
D19-1199,D16-1127,0,0.179163,"hich models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Eq"
D19-1199,D16-1036,1,0.896926,"ay. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements. 1 Speaker User 1 User 1 User 2 User 3 User 4 Introduction As an essential aspect of artificial intelligence, dialogue systems have attracted extensive attention in recent studies (Vinyals and Le, 2015; Serban et al., 2016). Researchers have paid great efforts to understand conversations between two participants, either single-turn (Li et al., 2016a; Shang et al., 2015; Vinyals and Le, 2015) or multi-turn (Zhou et al., 2016; Yan et al., 2016; Tao et al., 2019a,b), and achieved encouraging results. A more general and challenging scenario is that a conversation may involve more than two interlocutors conversing among each other (Uthus and Aha, 2013; Hu et al., 2019), which is known as multi-party conversation. Ubuntu Internet Relay Chat channel (IRC) is a multi-party conversation scenario as shown in Table 1. Generally, each utterance is associated with a speaker and one or more addressees in the conversation. Such a characteristic ∗ † Equal contribution. Corresponding author. Utterance ”Good point, tmux is the th"
D19-1201,P16-1094,0,0.617678,"ion. We denote the trained user embeddings as U = {u1 , u2 , . . . , ui } where ui represents the vector representations of i-th user (User i). Based on the user embeddings as U, we utilize learned user personalizations in the latent space. Specifically, the conditional prior distribution of WAE part is a Gaussian mixture distribution (GMD) conditioned on the learned user embeddings, namely personalization GMD. We formulate the conditional prior as: p(zu |ui ) = K X vk N (zu ; µk , σk2 I) (4) k=1 2.3 Problem Formulation We follow the conventional personalized conversation generation research (Li et al., 2016) and formulate the response generation task with the following necessary notations. A dataset with user dialogue history content D = {(ci , ri , mi )}N i=1 is firstly given, where ci , ri , mi represent dialogue context, response candidate, and user specific dialogue utterance respectively. Note that we treat the user dialogue utterance for extracting personalization information in multi-turn response generation. Herein, the context is formulated by: ci = (s1 , s2 , · · · , sj , · · · , sni ) where sj represents an utterance in the j-th turn of dialogue context and there are ni utterances in t"
D19-1201,P08-1028,0,0.173515,"egrees between generated responses and ground-truth, we perform evaluations on embedding space. In consistent with previous study (Gu et al., 2019), we compute the similarity between the bag-of-words (BOW) embeddings representations of generated results and reference. In particular, we calculate three metrics:1) Greedy (BOW-Greedy), i.e., greedily matching words in two utterances based on the cosine similarities, and the total scores is then averaged across all words (Rus and Lintean, 2012); 2) Average (BOWAverage), cosine similarity between the averaged word embeddings in the two utterances (Mitchell and Lapata, 2008); 3) Extrema (BOW-Extrema), 4 http://www.nltk.org/_modules/nltk/ translate/bleu_score.html 1936 -这孩子，太稚嫩了，真想踹一脚 (This child is so immature that I really want to kick him.) -wow，你大粗腿，能抬多高啊 (Wow, how high can you get with those big legs.) -求种草水乳，性价比高点的。 (Please recommend cost-effective make-up water and lotion to me.) -我水乳用的老慢啦哈哈，感觉两年用一套 (I use make-up water and lotion so slow that one can be used for 2 years.) ground truth 你要不要试试？见证一下我能办得到！ (Try it ? Prove I can do it.) RL-Persona 哈哈，那就好了 (Haha, it’s all right.) User1 [UNK]，谢谢，我的小U ([UNK], thank you, my xiaoU (name)) 啊，完了，我就不敢说 (Game over, I ca"
D19-1201,P02-1040,0,0.104244,"t is a Gaussian distribution. Table 4: The results of Ablation Experiments. w/o denotes without. Fusion represents fusion of personalization in decoder. ing rates are set to 5e-5 and 1e-5, respectively. The gradient penalty is used for training discriminator (Gulrajani et al., 2017). The value of τ in Gumbel softmax is set to 0.1. cosine similarity between the largest extreme values among the word embeddings in the two utterances (Forgues et al., 2014). We report the maximum BOW embedding scores of the 10 sampled responses for each testing context. Overlap-based Metric. We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated response. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to compute BLEU scores using smoothing techniques (smoothing 7) 4 . For each testing context, we sample 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEUPrecision), n-gram recall (BLEU-Recall), and ngram F1 (BLEU-F1). Human Evaluation. We also employ human evaluation to assess the responses generated by our model and the baselines. Three well-educated annotators are hired to evaluate the quality of g"
D19-1201,D11-1054,0,0.0428144,"tractive and prevalent task within the community of artificial intelligence. Previous studies mainly focus on vertical domains by applying rule- and template-based models (Pieraccini et al., 2009). Later on, with the explosive growth of data, the application of open-domain conversation model is promising. Conventional methods in vertical domains have obstacles to scale to open domain. Given this, various data-driven approaches have been proposed for modeling open-domain conversation, including retrieval-based methods (Yan et al., 2016; Tao et al., 2019), statistical machine translation model (Ritter et al., 2011), and neural networks (Serban et al., 2015; Hu et al., 2019). Recently, building a personalized conversation system has been attached more attention, e.g., implicitly learning user personalizations from dialog history (Li et al., 2015), explicitly collecting and modeling user profiles as personalizations for generating personalized responses (Zhang et al., 2017, 2018). To improve wording diversity, CVAE models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2018) are well-investigated for opendomain response generation. As the extension of 1938 CVAE, Wasserstein autoencoder (Gu et al., 2"
D19-1201,P18-1205,0,0.0410254,"riable for modeling utterance-level information such as topic, and syntactic structure (Bowman et al., 2015). It is verified in various open-domain response generation situations that conditional variational autoencoders (CVAE) (Serban et al., 2017; Zhao et al., 2017) are effective for addressing the “universal response” issue. In user-level information modeling, existing models either implicitly learn user information from training data such as learning user embedding (Li et al., 2015) or explicitly collect user profiles as the accurate personalization (Zhang et al., 2017; Yang et al., 2018; Zhang et al., 2018). Although obtaining user profiles is more effective and accurate than user embeddings, it is time-consuming and economically costly, or even impossible under the condition of protecting user privacy. We propose the PersonaWAE model, a novel conversational system which simultaneously captures user-level personalization and utterancelevel information as extra hints for generating better responses. Our model is motivated by following two points: 1) existing embedding based per1931 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joi"
D19-1201,W12-2018,0,0.170328,"sponses, we adopt the following metrics widely used in existing research. Embedding Metrics. To capture the semantic matching degrees between generated responses and ground-truth, we perform evaluations on embedding space. In consistent with previous study (Gu et al., 2019), we compute the similarity between the bag-of-words (BOW) embeddings representations of generated results and reference. In particular, we calculate three metrics:1) Greedy (BOW-Greedy), i.e., greedily matching words in two utterances based on the cosine similarities, and the total scores is then averaged across all words (Rus and Lintean, 2012); 2) Average (BOWAverage), cosine similarity between the averaged word embeddings in the two utterances (Mitchell and Lapata, 2008); 3) Extrema (BOW-Extrema), 4 http://www.nltk.org/_modules/nltk/ translate/bleu_score.html 1936 -这孩子，太稚嫩了，真想踹一脚 (This child is so immature that I really want to kick him.) -wow，你大粗腿，能抬多高啊 (Wow, how high can you get with those big legs.) -求种草水乳，性价比高点的。 (Please recommend cost-effective make-up water and lotion to me.) -我水乳用的老慢啦哈哈，感觉两年用一套 (I use make-up water and lotion so slow that one can be used for 2 years.) ground truth 你要不要试试？见证一下我能办得到！ (Try it ? Prove I can do"
D19-1201,P17-1061,0,0.424849,"of our model for generating informative and personalized responses, where both automatic and human evaluations outperform state-of-the-art models. 1 Introduction Over the past decade, a myriad of conversational systems have been proposed in the field of artificial intelligence and achieved remarkable success in various industry scenarios, such as e-commerce assistant (Li et al., 2017) and chit-chat machine XiaoIce (Shum et al., 2018). Based on the domains involved in previous research, existing work can be categorized into two groups, i.e., verticaldomain (Glas et al., 2015) and open-domain (Zhao et al., 2017), where the former group pursues to complete a specific target with limited domain knowledge while the latter one involves massive topics in conversations. In this work, we focus on ∗ † Equal contribution. Ordering is decided by a coin flip. Corresponding author. the latter one and intend to generate a natural and meaningful response for a given conversation context. Most recent works build upon the sequence to sequence model (Bahdanau et al., 2014) and can generate a fluent response. But they suffer from the notorious “universal response” issue, i.e., generating safe and uninformative respons"
D19-1201,P19-1001,1,0.826691,"ork Constructing an automatic conversation system is an attractive and prevalent task within the community of artificial intelligence. Previous studies mainly focus on vertical domains by applying rule- and template-based models (Pieraccini et al., 2009). Later on, with the explosive growth of data, the application of open-domain conversation model is promising. Conventional methods in vertical domains have obstacles to scale to open domain. Given this, various data-driven approaches have been proposed for modeling open-domain conversation, including retrieval-based methods (Yan et al., 2016; Tao et al., 2019), statistical machine translation model (Ritter et al., 2011), and neural networks (Serban et al., 2015; Hu et al., 2019). Recently, building a personalized conversation system has been attached more attention, e.g., implicitly learning user personalizations from dialog history (Li et al., 2015), explicitly collecting and modeling user profiles as personalizations for generating personalized responses (Zhang et al., 2017, 2018). To improve wording diversity, CVAE models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2018) are well-investigated for opendomain response generation. As the"
D19-1201,Q18-1029,0,0.0252311,"bution of PersonaWAE (which is also a GMD).     wc c = sof tmax(Wf ( )) wu ui (9) z p = wc · z c + wu · z u where Wf is a trainable parameter. Decoder. The decoder is a one-layer GRU network to output the sentence in the generation, which is shown in the right hand of Figure 2. Taking the generation of response ri as an example, the initial state of the decoder is calculated as:   z si,0 = Wd ( p ) + bd (10) c where Wd is a trainable matrix for dimension transformation. To facilitate the combination of user personalization ui and decoder hidden states, 1934 we incorporate a gate module (Tu et al., 2018) in our model: g = f (Ust−1 + Vdt + Wui )   dt ot = GRU(st−1 , ) g · ui + W (qφ (z|x, c)||pθ (z|c, ui )) (12) Experiments Dataset To evaluate the effectiveness of our proposed personalized WAE model (PersonaWAE), we collect a dataset from an open online chatting forum, i.e., Weibo 2 , which contains massive multi-turn conversation sessions and user identification information. Overall, there are 31,128,520 utterances in the raw dataset with corresponded user identifications. To construct the personalized conversation systems, we retrieve users with more than 14 utterances from the raw Weibo c"
D19-1388,N19-1124,0,0.0302264,"Missing"
D19-1388,P18-1015,0,0.337799,"ic pattern, such as court judgments, diagnosis certificates, abstracts in academic papers, etc. Take the court judgments for example, there is always a statement of the crime committed by the accused, followed by the motives and the results of the judgment. An example case is shown in Table 1, where the summary shares the same writing style and has words in common with the prototype summary (retrieved from the training dataset). Introduction Abstractive summarization can be regarded as a sequence mapping task that maps the source text to the target summary (Rush et al., 2015; Li et al., 2017; Cao et al., 2018; Gao et al., 2019a). It has drawn significant attention since the introduction ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding author. 1 https://github.com/gsh199449/proto-summ † The court held that the defendant Wang had stolen the property of others for the purpose of illegal possession. The amount was large, and his behavior constituted the crime of theft. The accusation of the public prosecution agency was established. The defendant Wang has a criminal record and will be considered when sentencing. Since the defendant Wang did not succeed because of reasons other t"
D19-1388,D18-1442,1,0.738797,"lp generate better summaries with patterns. • Specifically, we propose to generate the summary incorporating the prototype summary pattern and extracted facts from input document. • We provide mutual information signal for the generator to prevent copying irrelevant facts from the prototype. • We release a large-scale prototype based summarization dataset that is beneficial for the community. 2 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first prototype editing model, w"
D19-1388,Q18-1031,0,0.0423837,". Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first prototype editing model, which samples a prototype sentence from training data and then edits it into a new sentence. Following this work, (Wu et al., 2018) proposed a new paradigm for response generation, which first retrieves a prototype response from a pre-defined index and then edits the prototype response. (Cao et al., 2018) applied this method on summarization, where they employed existing summaries as soft templates to generate new summary without modeling the dependency between the prototype document, summary and input document. Different from these soft attention m"
D19-1388,P18-1152,0,0.0463162,"dule. § 4.3, we obtain the fact representation of an input document ri and prototype facts rˆi . Combining these with the final hidden state dTn of the generator RNN (in Equation 13), yields the local features of input extracted facts and the prototype facts: C r = {dTn ⊕ r1 , . . . , dTn ⊕ rTm }, f In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; Vinyals et al., 2015; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. What’s more, previous work (Holtzman et al., 2018) has found that using a cross entropy loss alone is not enough for generating coherent text. Similarly, in our task, using Ls alone is not enough to distinguish a good summary with accurate facts from a bad summary with detailed facts from the prototype document (see § 6.2). Thus, we propose a fact checker to determine whether the generated summary is highly related to the input document. 4.5 Fact Checker To generate accurate summaries that are consistent with the detailed facts from the input document rather than facts from the prototype document, we add a fact checker to provide additional t"
D19-1388,P18-1013,0,0.114359,"b.com/gsh199449/proto-summ commonly used summarization baseline (Nallapati et al., 2017; See et al., 2017), which selects the first three sentences of document as the summary. (2) S2S is a sequence-to-sequence framework with a pointer network, proposed by (See et al., 2017). (3) Proto is a context-aware prototype editing dialog response generation model proposed by (Wu et al., 2018). (4) Re3 Sum, proposed by (Cao et al., 2018), uses an IR platform to retrieve proper summaries and extends the seq2seq framework to jointly conduct template-aware summary generation. (5) Uni-model was proposed by (Hsu et al., 2018), and is the current stateof-the-art abstractive summarization approach on the CNN/DailyMail dataset. (6) We also directly concatenate the prototype summary with the original document as input for S2S and Uni-model, named as Concat-S2S and Concat-Uni, respectively. Evaluation Metrics For the court judgment dataset, we evaluate standard ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004) on full-length F1 following previous works (Nallapati et al., 2017; See et al., 2017; Paulus et al., 2018), where ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) refer to the matches of unigram, bigrams, and the longest commo"
D19-1388,N19-1260,0,0.0967986,"Missing"
D19-1388,D17-1222,1,0.896698,"Missing"
D19-1388,P18-2115,0,0.0124479,"We release a large-scale prototype based summarization dataset that is beneficial for the community. 2 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first prototype editing model, which samples a prototype sentence from training data and then edits it into a new sentence. Following this work, (Wu et al., 2018) proposed a new paradigm for response generation, which first retrieves a prototype response from a pre-defined index and then edits the prototype response. (Cao"
D19-1388,D18-1206,0,0.0343561,"otype information to help generate better summaries with patterns. • Specifically, we propose to generate the summary incorporating the prototype summary pattern and extracted facts from input document. • We provide mutual information signal for the generator to prevent copying irrelevant facts from the prototype. • We release a large-scale prototype based summarization dataset that is beneficial for the community. 2 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first proto"
D19-1388,N18-1065,0,0.0458773,"Missing"
D19-1388,N18-1158,0,0.0405132,"otype information to help generate better summaries with patterns. • Specifically, we propose to generate the summary incorporating the prototype summary pattern and extracted facts from input document. • We provide mutual information signal for the generator to prevent copying irrelevant facts from the prototype. • We release a large-scale prototype based summarization dataset that is beneficial for the community. 2 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods (Narayan et al., 2018b; Chen et al., 2018) directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization (Ma et al., 2018; Zhou et al., 2018; Gao et al., 2019a; Chen et al., 2019) is dedicated to abstractive summarization, which aims to generate new content that concisely paraphrases a document from scratch. Another line of research focuses on prototype editing. (Guu et al., 2018) proposed the first proto"
D19-1388,P16-1154,0,0.0260448,"ed Summary Finally, the context vector gth is concatenated with the decoder state dt and fed into a linear layer to obtain the generated word distribution Pv : Figure 3: Framework of fact checker module. § 4.3, we obtain the fact representation of an input document ri and prototype facts rˆi . Combining these with the final hidden state dTn of the generator RNN (in Equation 13), yields the local features of input extracted facts and the prototype facts: C r = {dTn ⊕ r1 , . . . , dTn ⊕ rTm }, f In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; Vinyals et al., 2015; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. What’s more, previous work (Holtzman et al., 2018) has found that using a cross entropy loss alone is not enough for generating coherent text. Similarly, in our task, using Ls alone is not enough to distinguish a good summary with accurate facts from a bad summary with detailed facts from the prototype document (see § 6.2). Thus, we propose a fact checker to determine whether the generated summary is highly related to the input document. 4.5"
D19-1388,W14-4407,0,0.437984,"sation of the public prosecution agency was established and supported. This crime was committed within two years after the release of the defendants Zhang and Fan. Thus they are recidivists and this situation will be considered when sentencing. The fact that defendants Zhang and Fan surrendered themselves and pleaded guilty in court gives a lighter punishment according to law. Existing prototype based generation models such as (Wu et al., 2018) are all applied on short text, thus, cannot handle long documents summarization task. Another series of works focus on template-based methods such as (Oya et al., 2014). However, template-based methods are too rigid for our patternized summary generation task. Hence, in this paper, we propose a summarization framework named Prototype Editing based Summary Generator (PESG) that incorporates prototype document-summary pairs to improve summa3741 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3741–3751, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics rization performance when generating summaries with pat"
D19-1388,D15-1044,0,0.0622959,"are required to conform to a specific pattern, such as court judgments, diagnosis certificates, abstracts in academic papers, etc. Take the court judgments for example, there is always a statement of the crime committed by the accused, followed by the motives and the results of the judgment. An example case is shown in Table 1, where the summary shares the same writing style and has words in common with the prototype summary (retrieved from the training dataset). Introduction Abstractive summarization can be regarded as a sequence mapping task that maps the source text to the target summary (Rush et al., 2015; Li et al., 2017; Cao et al., 2018; Gao et al., 2019a). It has drawn significant attention since the introduction ∗ Equal contribution. Ordering is decided by a coin flip. Corresponding author. 1 https://github.com/gsh199449/proto-summ † The court held that the defendant Wang had stolen the property of others for the purpose of illegal possession. The amount was large, and his behavior constituted the crime of theft. The accusation of the public prosecution agency was established. The defendant Wang has a criminal record and will be considered when sentencing. Since the defendant Wang did not"
D19-1388,E17-2007,0,0.015777,"the-art abstractive summarization approach on the CNN/DailyMail dataset. (6) We also directly concatenate the prototype summary with the original document as input for S2S and Uni-model, named as Concat-S2S and Concat-Uni, respectively. Evaluation Metrics For the court judgment dataset, we evaluate standard ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004) on full-length F1 following previous works (Nallapati et al., 2017; See et al., 2017; Paulus et al., 2018), where ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) refer to the matches of unigram, bigrams, and the longest common subsequence respectively. (Schluter, 2017) notes that only using the ROUGE metric to evaluate summarization quality can be misleading. Therefore, we also evaluate our model by human evaluation. Three highly educated participants are asked to score 100 randomly sampled summaries generated by three models: Uni-model, Re3 Sum and PESG. The statistical significance of observed differences between the performance of two runs is tested using a twotailed paired t-test and is denoted using N (or H ) for strong (or weak) significance for α = 0.01. 5.4 Implementation Details We implement our experiments in TensorFlow (Abadi et al., 2016) on an"
D19-1388,P17-1099,0,0.105924,"gth is concatenated with the decoder state dt and fed into a linear layer to obtain the generated word distribution Pv : Figure 3: Framework of fact checker module. § 4.3, we obtain the fact representation of an input document ri and prototype facts rˆi . Combining these with the final hidden state dTn of the generator RNN (in Equation 13), yields the local features of input extracted facts and the prototype facts: C r = {dTn ⊕ r1 , . . . , dTn ⊕ rTm }, f In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; Vinyals et al., 2015; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space. What’s more, previous work (Holtzman et al., 2018) has found that using a cross entropy loss alone is not enough for generating coherent text. Similarly, in our task, using Ls alone is not enough to distinguish a good summary with accurate facts from a bad summary with detailed facts from the prototype document (see § 6.2). Thus, we propose a fact checker to determine whether the generated summary is highly related to the input document. 4.5 Fact Checker To generate accurate summar"
D19-1499,N16-1012,0,0.0398371,"space of different styles and design two constraints to train it. We also introduce two other simple but effective semisupervised methods to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese. 1 Introduction Recently, the natural language generation (NLG) tasks have been attracting the growing attention of researchers, including response generation (Vinyals and Le, 2015), machine translation (Bahdanau et al., 2014), automatic summarization (Chopra et al., 2016), question generation (Gao et al., 2019), etc. Among these generation tasks, one interesting but challenging problem is text style transfer (Shen et al., 2017; Fu et al., 2018; Logeswaran et al., 2018). Given a sentence from one style domain, a style transfer system is required to convert it to another style domain as well as keeping its content meaning unchanged. As a fundamental attribute of text, style can have a broad and ambiguous scope, such as ancient poetry style v.s. modern language style and positive sentiment v.s. negative sentiment. ∗ This work was done when Mingyue Shang was an in"
D19-1499,D19-1306,0,0.0616094,"Missing"
D19-1499,D14-1181,0,0.00593167,"Missing"
D19-1499,J82-2005,0,0.727621,"Missing"
D19-1499,D18-1420,1,0.850292,"gn a strategy to disentangle the variables for content and style. Shen et al. (2017) first map the text corpora belonging to different styles to their own space respectively, and leverages the alignment of latent representations from different styles to perform style transfer. Chen et al. (2019) propose to extract and control the style of the image caption through domain layer normalization. Prabhumoye et al. (2018) and Zhang et al. (2018b) employ the back-translation mechanism to ensure that the input from the source style can be reconstructed from the transferred result in the target style. Liao et al. (2018) associate the style latent variable with a numeric discrete or continues numeric value and can generate sentences controlled by this value. Among them, many use the adversarial training mechanism (Goodfellow et al., 2014) to improve the performance of the basic models (Shen et al., 2017; Zhao et al., 2018). To sum up, most of the existing unsupervised frameworks on the text style transfer focus on getting the disentangled representations of style and content. However, Lample et al. (2019) illustrated that the disentanglement not adequate to learn the style-independent representations, thus th"
D19-1499,W02-0109,0,0.189984,"by Rao and Tetreault (2018) which contains texts of formal and informal style. With the released data, we randomly sample 5,000 sentence pairs from it as the parallel corpus with limited data volume. We then use the Yahoo Answers L6 corpus7 as the source which is in the same content domain as the parallel data to construct the large-scale nonparallel data. To divide nonparallel dataset into two styles, we train a CNN-based classifier (Kim, 2014) on the parallel data with annotation of styles and use it to classify the nonparallel data. sentence as 30. For the formality datasets, we use NLTK (Loper and Bird, 2002) to tokenize the texts and set the minimum length as 5 and the maximum length as 30 for both formal and informal styles. We adopt GloVE (Pennington et al., 2014) to pretrain the embeddings, and the dimensions of the embeddings are set to 300 for all the datasets. The hidden states are set to 500 for both encoders and decoders. We adopt SGD optimizer with the learning rate as 1 for DAE models and 0.1 for S2S models. The dropout rate is 0.4. In the inference stage, the beam size is set to 5. 7.2 8.1 Experimental Settings We perform different data preprocessing on different datasets. The Chinese"
D19-1499,P15-2097,0,0.0419915,"e supervised baseline and the three semi-supervised models. Fluency 0.3575 0.4425 0.5800 0.6325 0.3050 0.5475 0.5725 0.6200 9 Table 4: The human annotation results of the S2S model and CPLS model from three aspects. as the automatic evaluation metrics to measure the content preservation degree and the style changing degree. BLEU calculates the N-gram overlap between the generated sentence and the references, thus can be used to measure the preservation of text content. Considering that text style transfer is a monolingual text generation task, we also use GLEU, a generalized BLEU proposed by (Napoles et al., 2015). To evaluate the extent to which the sentences are transferred to the target style, we follow Shen et al. (2017); Hu et al. (2017) that build a CNN-based style classifier and use it to measure the style accuracy. 8.3 Human Evaluation We also adopt human evaluations to judge the quality of the transferred sentences from three aspects, namely content, style and fluency. These aspects evaluate how well the transferred text preserve the content of the input, the style strength and the fluency of the transferred text. Take the content relevance for example, the criterion is as follows: +2: The tra"
D19-1499,D18-1138,0,0.0162566,"e the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and design a strategy to disentangle the variables for content and style. Shen et al. (2017) first map the text corpora belonging to different styles to their own space respectively, and leverages the alignment of latent representations from different styles to perform style transfer. Chen et al. (2019) propose to extract and control the style of the image caption through domain layer normalization. Prabhumoye et al. (2018) and Zhang et al. (2018b) employ the back-translation mechanism to ensure that the input from the source style can be reconstructed from the transferred result in the target style. Liao et al. (2018) associate the style latent variable with a numeric discrete or continues numeric value and can generate sentences controlled by this value. Among them, many use the adversarial training mechanism (Goodfellow et al., 2014) to improve the performance of the basic models (Shen et al., 2017; Zhao et al., 2018). To sum up, most of the existing unsupervised frameworks on the text style transfer focus on getting the disentangl"
D19-1499,P02-1040,0,0.106223,"Missing"
D19-1499,D14-1162,0,0.0825285,"parallel corpus with limited data volume. We then use the Yahoo Answers L6 corpus7 as the source which is in the same content domain as the parallel data to construct the large-scale nonparallel data. To divide nonparallel dataset into two styles, we train a CNN-based classifier (Kim, 2014) on the parallel data with annotation of styles and use it to classify the nonparallel data. sentence as 30. For the formality datasets, we use NLTK (Loper and Bird, 2002) to tokenize the texts and set the minimum length as 5 and the maximum length as 30 for both formal and informal styles. We adopt GloVE (Pennington et al., 2014) to pretrain the embeddings, and the dimensions of the embeddings are set to 300 for all the datasets. The hidden states are set to 500 for both encoders and decoders. We adopt SGD optimizer with the learning rate as 1 for DAE models and 0.1 for S2S models. The dropout rate is 0.4. In the inference stage, the beam size is set to 5. 7.2 8.1 Experimental Settings We perform different data preprocessing on different datasets. The Chinese literary datasets are segmented by characters instead of word to alleviate the issue of unknown words. Our statistics show that the average length of ancient poe"
D19-1499,P18-1080,0,0.0387814,"ting the training mode between supervised and unsupervised. • We introduce another two semi-supervised methods that are simple but effective to leverage both the nonparallel and parallel data. • We build a small-scale parallel dataset that contains ancient Chinese poem style and modern Chinese style sentences. We also collect two large nonparallel datasets of these styles.1 2 Related Works Recently, text style transfer has stimulated great interests of researchers from the area of neural language processing and some encouraging results are obtained (Shen et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Hu et al., 2017; Jin et al., 2019). In the primary stage, due to the lacking of parallel corpus, most of the methods employ unsupervised learning paradigm to conduct the semantic modeling and transfer. 1 Download link: https://tinyurl.com/yyc8zkqg Unsupervised Learning Methods. Mueller et al. (2017) modify the latent variables of sentences in a certain direction guided by a classifier to generate the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and design a strategy to disenta"
D19-1499,N18-1012,0,0.358646,"el is flexible in alternating the training mode between supervised and unsupervised. • We introduce another two semi-supervised methods that are simple but effective to leverage both the nonparallel and parallel data. • We build a small-scale parallel dataset that contains ancient Chinese poem style and modern Chinese style sentences. We also collect two large nonparallel datasets of these styles.1 2 Related Works Recently, text style transfer has stimulated great interests of researchers from the area of neural language processing and some encouraging results are obtained (Shen et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Hu et al., 2017; Jin et al., 2019). In the primary stage, due to the lacking of parallel corpus, most of the methods employ unsupervised learning paradigm to conduct the semantic modeling and transfer. 1 Download link: https://tinyurl.com/yyc8zkqg Unsupervised Learning Methods. Mueller et al. (2017) modify the latent variables of sentences in a certain direction guided by a classifier to generate the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and des"
D19-1499,P16-1009,0,0.0730157,"Missing"
D19-1501,P18-1063,0,0.0230098,"ropose an ELSTM and a keyword memory to incorporate entity label information, so as to generate more accurate descriptions. 2 Related Work We detail related work on text generation, entityrelated generation, and product description. Text generation. Recently, sequence-tosequence (Seq2Seq) neural network models have been widely used in NLG approaches. Their effectiveness has been demonstrated in a variety of text generation tasks, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Kat"
D19-1501,N18-1204,0,0.052123,"Missing"
D19-1501,P18-1082,0,0.0362371,"s, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity repres"
D19-1501,D18-1306,0,0.0294082,"018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut et al., 2018). Different from the above works, we utilize entity labels as supplementary information to assist decoding in the text generation task. P"
D19-1501,P18-1013,0,0.0536131,"this problem, we propose an ELSTM and a keyword memory to incorporate entity label information, so as to generate more accurate descriptions. 2 Related Work We detail related work on text generation, entityrelated generation, and product description. Text generation. Recently, sequence-tosequence (Seq2Seq) neural network models have been widely used in NLG approaches. Their effectiveness has been demonstrated in a variety of text generation tasks, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Gre"
D19-1501,D17-1195,0,0.0238879,"t al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut et al., 2018). Different from the above works, we utilize entity labels as supplementary information to assist decoding in the text generation task. Product descriptions. Quality product descriptions"
D19-1501,N18-1079,0,0.0312599,"18), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut et al., 2018). Different from the above works, we utilize entity labels as supplementary information to assist decoding in the text generation task. Product descriptions. Qualit"
D19-1501,D18-1423,1,0.846892,"Missing"
D19-1501,P15-1002,0,0.025973,"owledge, we are the first to explore the fidelity problem of product description generation. Besides, to tackle this problem, we propose an ELSTM and a keyword memory to incorporate entity label information, so as to generate more accurate descriptions. 2 Related Work We detail related work on text generation, entityrelated generation, and product description. Text generation. Recently, sequence-tosequence (Seq2Seq) neural network models have been widely used in NLG approaches. Their effectiveness has been demonstrated in a variety of text generation tasks, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related gene"
D19-1501,P17-1099,0,0.711428,"Besides, to tackle this problem, we propose an ELSTM and a keyword memory to incorporate entity label information, so as to generate more accurate descriptions. 2 Related Work We detail related work on text generation, entityrelated generation, and product description. Text generation. Recently, sequence-tosequence (Seq2Seq) neural network models have been widely used in NLG approaches. Their effectiveness has been demonstrated in a variety of text generation tasks, such as neural machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding"
D19-1501,P17-2060,0,0.0582283,"Missing"
D19-1501,P18-1103,0,0.0283355,"machine translation (Luong et al., 2015; Bahdanau et al., 2014; Wu et al., 2016), abstractive text summarization (See et al., 2017a; Hsu et al., 2018; Chen and Bansal, 2018), dialogue generation (Tao et al., 2018a; Xing et al., 2017), etc. Along another line, there are also works based on an attention mechanism. Vaswani et al. (2017) proposed a Transformer architecture that utilizes the self-attention mechanism and has achieved state-of-the-art results in neural machine translation. Since then, the attention mechanism has been used in a variety of tasks (Devlin et al., 2018; Fan et al., 2018; Zhou et al., 2018). Entity-related generation. Named entity recognition (NER) is a fundamental component in language understanding and reasoning (Greenberg et al., 2018; Katiyar and Cardie, 2018). In (Ji et al., 2017), they proved that adding entity related information can improve the performance of language modeling. Building upon this work, in (Clark et al., 2018), they combined entity context with previous-sentence context, and demonstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut"
D19-1501,I17-2032,0,0.0756169,"nstrated the importance of the latter in coherence test. Another line of related work generates recipes using neural networks to track and update entity representations (Bosselut et al., 2018). Different from the above works, we utilize entity labels as supplementary information to assist decoding in the text generation task. Product descriptions. Quality product descriptions are critical for providing a competitive customer experience in an e-commerce platform. Due to its importance, automatically generating the product description has attracted considerable interests. Initial works include (Wang et al., 2017), which incorporates statistical methods with the template to generate product descriptions. With the development of neural networks, (Chen et al., 2019) explored a new way to generate personalized product descriptions by combining the power of neural networks and a knowledge base. (Zhang et al., 2019b) proposed a pointer-generator neural network to generate product description whose patterns are controlled. In real-world product description generation application, however, the most important prerequisite is the fidelity of generated text, and to the best of our knowledge, no research has been"
I13-1058,N06-1052,0,0.363184,"methods relied on using a background language model, which is typically estimated based on the whole document collection (Ponte and Croft, 1998; Zhai and Lafferty, 2001b; Miller et al., 1999). In contrast to the simple strategy which smoothes all documents with the same background, recently corpus structures have been exploited for more accurate smoothing. The basic idea is to smooth a document language model with the documents similar to the document under consideration through clustering (Liu and Croft, 2004; Xu and Croft, 1999; Mei et al., 2008), document expansion (Kurland and Lee, 2004; Tao et al., 2006), or relevance propagation (Kurland and Lee, 2010; Kurland and Lee, 2006; Qin et al., 2005). All these methods are based on documentlevel semantics similarity to offer “customized” smoothing for each individual document. Besides semantics, positional heuristics for retrieval have been examined in (Keen, 1992; Tao and Zhai, 2007; Liu and Croft, 2002; B¨uttcher et al., 2006). Positional language models are proposed to examine the positional proximity in (Lv and Zhai, 2009; Zhao and Yun, 2009). In their work, the key idea is to define a language model for each position within a document, and scor"
I17-2029,C16-1316,1,0.851439,"Missing"
I17-2029,P15-2130,0,0.0626508,"Missing"
I17-2029,P15-1152,0,0.0611304,"Missing"
O16-2003,P96-1041,0,0.458554,"Missing"
O16-2003,D11-1124,1,0.85821,"Missing"
O16-2003,P12-1054,1,0.895389,"Missing"
P12-1054,C10-1034,0,0.0488041,"ly, ranking strategies have become extremely important for retrieving information quickly. Many websites currently offer a real-time search service which returns ranked lists of Twitter posts or shared links according to user queries. Ranking methods used by these sites employ three criteria, namely recency, popularity and content relevance (Dong et al., 2010). State-of-art tweet retrieval methods include a linear regression model biased towards text quality with a regularization factor inspired by the hypothesis that documents similar in content may have similar quality (Huang et al., 2011). Duan et al. (2010) learn a ranking model using SVMs and features based on tweet content, the relations among users, and tweet specific characteristics (e.g., urls, number of retweets). Tweet Recommendation Previous work has also focused on tweet recommendation systems, assuming no explicit query is provided by the users. Collaborative filtering is perhaps the most obvious method for recommending tweets (Hannon et al., 2010). Chen et al. (2010) investigate how to select interesting URLs linked from Twitter and recommend the top ranked ones to users. Their recommender takes three dimensions into account: the sour"
P12-1054,I11-1042,0,0.0740539,"eets being posted daily, ranking strategies have become extremely important for retrieving information quickly. Many websites currently offer a real-time search service which returns ranked lists of Twitter posts or shared links according to user queries. Ranking methods used by these sites employ three criteria, namely recency, popularity and content relevance (Dong et al., 2010). State-of-art tweet retrieval methods include a linear regression model biased towards text quality with a regularization factor inspired by the hypothesis that documents similar in content may have similar quality (Huang et al., 2011). Duan et al. (2010) learn a ranking model using SVMs and features based on tweet content, the relations among users, and tweet specific characteristics (e.g., urls, number of retweets). Tweet Recommendation Previous work has also focused on tweet recommendation systems, assuming no explicit query is provided by the users. Collaborative filtering is perhaps the most obvious method for recommending tweets (Hannon et al., 2010). Chen et al. (2010) investigate how to select interesting URLs linked from Twitter and recommend the top ranked ones to users. Their recommender takes three dimensions in"
P12-1054,P10-1056,0,0.0119906,"large dataset consisting of 9,449,542 users, 364,287,744 tweets, 596,777,491 links, and 55,526,494 retweets. The crawler monitored the data from 3/25/2011 to 5/30/2011. We used approximately one month of this data for training and the rest for testing. 521 Before building the graphs (i.e., the tweet graph, the author graph, and the tweet-author graph), the dataset was preprocessed as follows. We removed tweets of low linguistic quality and subsequently discarded users without any linkage to the remaining tweets. We measured linguistic quality following the evaluation framework put forward in Pitler et al. (2010). For instance, we measured the out-ofvocabulary word ratio (as a way of gauging spelling errors), entity coherence, fluency, and so on. We further removed stopwords and performed stemming. Parameter Settings We ran LDA with 500 iterations of Gibbs sampling. The number of topics n was set to 100 which upon inspection seemed generally coherent and meaningful. We set the damping factor µ to 0.15 following the standard PageRank paradigm. We opted for more or less generic parameter values as we did not want to tune our framework to the specific dataset at hand. We examined the parameter λ which co"
P12-1054,P10-1094,0,0.00957457,"sumed that all nodes in the matrix M are equi-probable before the walk. In contrast, we use the topic preference vector as a prior on M. Let Diag(r) denote a diagonal matrix whose eigenvalue is vector r. Then m becomes: m = (1 − µ)[Diag(r)M]T m + µr = (1 − µ)[Diag(tDT )M]T m + µtDT (4) Diversity We would also like our output to be diverse without redundant information. Unfortunately, equation (4) will have the opposite effect, as it assigns high scores to closely connected node communities. A greedy algorithm such as Maximum Marginal Relevance (Carbonell and Goldstein, 1998; Wan et al., 2007; Wan et al., 2010) may achieve diversity by iteratively selecting the most prestigious or popular vertex and then penalizing the vertices “covered” by those that have been already selected. Rather than adopting a greedy vertex selection method, we follow DivRank (Mei et al., 2010) a recently proposed algorithm that balances popularity and diversity in ranking, based on a time-variant random walk. In contrast to PageRank, DivRank assumes that the transition probabilities change over time. Moreover, it is assumed that the transition probability from one state to another is reinforced by the number of previous vis"
P12-1054,D11-1124,1,0.764494,"Missing"
P14-1107,E09-1003,0,0.0183789,"T gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook loc"
P14-1107,W10-0710,0,0.0144795,"expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 201"
P14-1107,ambati-etal-2010-active,0,0.0223859,"h, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional tra"
P14-1107,P10-1088,1,0.845564,"or instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 words in each of 100 languages. Chen and Dolan (2012) examined the steps necessary to build a persistent multilingual workforce on MTurk. This paper is most closely related to previous work by Zaidan and Callison-Burch (2011), who showed that non-professional translators could approach the level of professional translators. They solicited multiple redundant transla"
P14-1107,W10-0701,1,0.794839,"k, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to expert annotations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian lan"
P14-1107,D09-1030,1,0.817253,"tations when many non-expert labelings for the same input are aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati"
P14-1107,W01-1409,0,0.0602304,"become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site into different languages using volunteers (TechCrunch, 2008). DuoLingo turns translation into an educational game, and translates web content using its language learners (von Ahn, 2013). Rather than relying on volunte"
P14-1107,N10-1078,0,0.0308235,"lems with hiring editors via MTurk for a word processing application. Workers were either lazy (meaning they made only minimal edits) or overly zealous (meaning they made many unnecessary edits). Bernstein et al. (2010) addressed this problem with a three step find-fix-verify process. In the first step, workers click on one word or phrase that needed to be corrected. In the next step, a separate group of workers proposed correc1 A variety of HCI and NLP studies have confirmed the efficacy of monolingual or bilingual individuals post-editing of machine translation output (Callison-Burch, 2005; Koehn, 2010; Green et al., 2013). Past NLP work has also examined automatic post-editing(Knight and Chander, 1994). tions to problematic regions that had been identified by multiple workers in the first pass. In the final step, other workers would validate whether the proposed corrections were good. Most NLP research into crowdsourcing has focused on Mechanical Turk, following pioneering work by Snow et al. (2008) who showed that the platform was a viable way of collecting data for a wide variety of NLP tasks at low cost and in large volumes. They further showed that non-expert annotations are similar to"
P14-1107,W10-1718,1,0.892503,"Missing"
P14-1107,Q13-1028,0,0.0230426,"Missing"
P14-1107,J05-4003,0,0.051185,"e, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professi"
P14-1107,P02-1040,0,0.100739,"n ti and tj . ( #col (eij ∈ ET ) I(ti , tj ) = , (9) 0 otherwise Then the adjacency matrix N is then defined as (10) 5 (12) Evaluation We are interested in testing our random walk method, which incorporates information from both the candidate translations and from the Turkers. We want to test two versions of our proposed collaborative co-ranking method: 1) based on the unedited translations only and 2) based on the edited sentences after translator/editor collaborations. Metric Since we have four professional translation sets, we can calculate the Bilingual Evaluation Understudy (BLEU) score (Papineni et al., 2002) for one professional translator (P1) using the other three (P2,3,4) as a reference set. We repeat the process four times, scoring each professional translator against the others, to calculate the expected range of professional quality translation. In the following sections, we evaluate each of our methods by calculating BLEU scores against the same four sets of three reference translations. Therefore, each number reported in our experimental results is an average of four numbers, corresponding to the four possible ways of choosing 3 of the 4 reference sets. This allows us to compare the BLEU"
P14-1107,W12-3152,1,0.885996,"range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch, 2010). Crowdsourcing allowed real studies to be conducted whereas most past active learning were simulated. Pavlick et al. (2014) conducted a large-scale demographic study of the languages spoken by workers on MTurk by translating 10,000 wor"
P14-1107,J03-3002,0,0.0131709,"actice it produces the state-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term"
P14-1107,N10-1063,0,0.0140024,"eneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opened the possibility of translating large amounts of text at low cost using non-professional translators. Facebook localized its web site i"
P14-1107,P13-1135,1,0.835643,"for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, crowdsourcing has opene"
P14-1107,2006.amta-papers.25,0,0.0296192,"om the candidates produced by the collaboration of translator/post-editor pairs. The third oracle operates at the worker level: for each source segment, we choose from the translations the one provided by the worker whose translations (over all sentences) score the highest on average. The fourth oracle also operates at the worker level, but selects from sentences produced by translator/post-editor collaborations. These oracle methods represent ideal solutions under our scenario. We also examine two voting-inspired methods. The first method selects the translation with the minimum average TER (Snover et al., 2006) against the other translations; intuitively, this would represent the “consensus” translation. The second method selects the translation generated by the Turker who, on average, provides translations with the minimum average TER. Results A summary of our results in given in Table 2. As expected, random selection yields bad performance, with a BLEU score of 30.52. The oracles indicate that there is usually an acceptable translation from the Turkers for any given sentence. Since the oracles select from a small group of only 4 translations per source segment, they are not overly optimistic, and"
P14-1107,D08-1027,0,0.0720958,"Missing"
P14-1107,C10-1124,0,0.0139175,"tate-of-art results only for language pairs with ample training data, like English-Arabic, EnglishChinese, French-English, etc. SMT gets stuck in a severe bottleneck for many minority or ‘low resource’ languages with insufficient data. This drastically limits which languages SMT can be successfully applied to. Because of this, collecting parallel corpora for minor languages has become an interesting research challenge. There are various options for creating training data for new language pairs. Past approaches have examined harvesting translated documents from the web (Resnik and Smith, 2003; Uszkoreit et al., 2010; Smith et al., 2013), or discovering parallel fragments from comparable corpora (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010). Until relatively recently, little consideration has been given to creating parallel data from scratch. This is because the cost of hiring professional translators is prohibitively high. For instance, Germann (2001) hoped to hire professional translators to create a modest sized 100,000 word Tamil-English parallel corpus, but were stymied by the costs and the difficulty of finding good translators for a short-term commitment. Recently, cr"
P14-1107,D11-1040,1,0.792595,"n, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GT C ). Edges in GT C connect author pairs (nodes in GT ) to the candidate that they produced (nodes in GC ). Together, GT , GC , and GT C define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT , VC , ET , EC , ET C ). G is divided into three subgraphs, GT , GC , and GT C . GC = (VC , EC ) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT , ET ) is a weighted undirect"
P14-1107,P12-1054,1,0.828,"of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor. post-edited sentences (henceforth “candidates”) as nodes. These two graphs, GT and GC are combined as subgraphs of a third graph (GT C ). Edges in GT C connect author pairs (nodes in GT ) to the candidate that they produced (nodes in GC ). Together, GT , GC , and GT C define a co-ranking problem (Yan et al., 2012a; Yan et al., 2011b; Yan et al., 2012b) with linkage establishment (Yan et al., 2011a; Yan et al., 2012c), which we define formally as follows. Let G denote the heterogeneous graph with nodes V and edges E. Let G = (V ,E) = (VT , VC , ET , EC , ET C ). G is divided into three subgraphs, GT , GC , and GT C . GC = (VC , EC ) is a weighted undirected graph representing the candidates and their lexical relationships to one another. Let VC denote a collection of translated and edited candidates, and EC the lexical similarity between the candidates (see Section 4.3 for details). GT = (VT , ET ) is"
P14-1107,P11-1122,1,0.917573,"Missing"
P14-1107,N12-1006,1,0.850492,"aggregated, through simple voting or through weighting votes based on how closely non-experts matched experts on a small amount of calibration data. MTurk has subsequently been widely adopted by the NLP community and used for an extensive range of speech and language applications (Callison-Burch and Dredze, 2010). Although hiring professional translators to create bilingual training data for machine translation systems has been deemed infeasible, Mechanical Turk has provided a low cost way of creating large volumes of translations (Callison-Burch, 2009; Ambati and Vogel, 2010). For instance, Zbib et al. (2012; Zbib et al. (2013) translated 1.5 million words of Levine Arabic and Egyptian Arabic, and showed that a statistical translation system trained on the dialect data outperformed a system trained on 100 times more MSA data. Post et al. (2012) used MTurk to create parallel corpora for six Indian languages for less than $0.01 per word. MTurk workers translated more than half a million words worth of Malayalam in less than a week. Several researchers have examined the use of active learning to further reduce the cost of translation (Ambati et al., 2010; Ambati, 2012; Bloodgood and Callison-Burch,"
P14-1107,N13-1069,0,0.0366214,"Missing"
P14-1107,Q14-1007,1,\N,Missing
P14-2100,P96-1041,0,0.242678,"tion (Zhai, 2008). To build a user language model, one naïve way is to first normalize word frequency ?(?, ?) within each document, and then average over all the documents in a user’s document collection. The resulting unigram user language model is: 1 ?(?, ?) ?? (?) = ∑ |?? |?∈?? |?| (1) 1 = ∑ ?? (?) |?? |?∈?? where ?? (?) is the language model of a particular document, and ?? is the user’s document collection. This formulation is basically an equalweighted finite mixture model. A simple yet effective way to smooth a language model is to linearly interpolate with a background language model (Chen and Goodman, 1996; Zhai and Lafferty, 2001). In the linear interpolation method, all background documents are treated equally. The entire document collection is added to the user language model ?? (?) with the same interpolation coefficient. Our main idea is to specify a set of relevant documents for the target user using information embedded in a social network, and enrich the smoothing procedure with these documents. Let ???? denote the content from relevant persons (e.g. social neighbors) of u1, our idea can be concisely expressed as: ??′1 (?) = ??1 ??1 (?) + ∑ ??? ??? (?) (2) ?? ∈???? where ??? is the mixt"
P14-2100,D11-1124,1,0.893752,"Missing"
P14-2100,P12-1054,1,0.920427,"Missing"
P15-2103,P14-2100,1,0.837656,"language model smoothing is another direction to investigate (Duan and Zhai, 2011). Mei et al. have proposed to smooth language model utilizing structural adjacency (2008). None of these methods incorporates social factors in language model smoothing. There is a study in (Lin et al., 2011) which smooths document language models of tweets for topic tracking in online text streams. Basically, it applies general smoothing strategies (e.g., JelinekMercer, Dirichlet, Absolute Discounting, etc.) on the specific tracking task. Social information is incorporated into a factor graph model as features (Huang et al., 2014; Yan et al., 2015). These factor graph model based methods are less efficient so as to better handle cold-start situations with little training data. In contrast with these works, we have proposed a language model smoothing framework which incorporates social factors as a regularizer. According to the experimental results, our method is effective with social information and as well much more efficient. Related Work Language models have been paid high attention to during recent years (Ponte and Croft, 1998). Many different ways of language modeling have been proposed to solve different tasks."
P15-2103,P10-1056,0,0.012181,"ag ‘#’ into clusters as different datasets to evaluate (Lin et al., 2011; Yan et al., 2015; Yan et al., 2011). We manually selected top-3 topics based on popularity (measured in the number of postings within the cluster) and to obtain broad coverage of different types: sports, technology, and general interests, as listed in Table 1. Pre-processing. Basically, the social network graph can be established from all posting documents and all users. However, the data is noisy. We first pre-filter the pointless babbles (Analytics, 2009) by applying the linguistic quality judgments (e.g., OOV ratio) (Pitler et al., 2010), and then remove inactive users that have less than one follower or followee and remove the users without any linkage to the remaining posting documents. We remove stopwords and URLs, perform stemming, and build the graph after filtering. We establish the 4.3 Evaluation Metric We apply language perplexity to evaluate the smoothed language models. The experimental procedure is as follows: given the topic clusters shown in Table 1, we remove the hashtags and compute its perplexity with respect to the current topic cluster, defined as a power function: [ ] 1 ∑ pow 2, − log P (wi ) N wi ∈V Perple"
P15-2103,N06-1052,0,0.431325,"g the posting document language model based on social regularization. We formulate an optimization framework with a social regularizer. Experimental results on the Twitter dataset validate the effectiveness and efficiency of our proposed model. 1 Figure 1: 2 different sources to smooth document language models: texts (colored in yellow) and social contacts (colored in blue). Each piece of texts is authored by a particular social network user. work on language model smoothing has been investigated based on textual characteristics (Lafferty and Zhai, 2001; Yan et al., 2013; Liu and Croft, 2004; Tao et al., 2006; Lavrenko and Croft, 2001; Song and Croft, 1999). However, for social networks, texts are actually associated with users (as illustrated in Figure 1). We propose that social factors should be utilized as an augmentation to better smooth language models. Here we propose an optimization framework with regularization for language model smoothing on social networks, using both textual information and the social structure. We believe the social factor is fundamental to smooth language models on social networks. Our framework optimizes the smoothed language model to be closer to social neighbors in"
P15-2103,P12-1054,1,0.858975,"s of node u, each of which shares an edge to u. Now we have finished modeling the language model smoothing with social factors as regularization, and have defined the context correlation between documents and user social relationships. By plugging in Equation (2) into Equation (1), we could compute the smoothed language model of P (w|d+ 0 ). All the definitions for π(.) result in a range which varies from 0 to 1. Particularly, the ego user similarity πu0 = 1, which would be a natural and intuitive answer. 4 Experiments and Evaluation 4.1 Datasets and Experimental Setups Utilizing the data in (Yan et al., 2012), we establish the dataset of microblogs and the corresponding users from 9/29/2012 to 11/30/2012. We use roughly one month as the training set and the rest as testing set. Based on this dataset, we group the posting documents with the same hashtag ‘#’ into clusters as different datasets to evaluate (Lin et al., 2011; Yan et al., 2015; Yan et al., 2011). We manually selected top-3 topics based on popularity (measured in the number of postings within the cluster) and to obtain broad coverage of different types: sports, technology, and general interests, as listed in Table 1. Pre-processing. Bas"
P15-2103,I13-1058,1,0.89178,"Missing"
P15-2103,P96-1041,0,\N,Missing
P16-1222,W14-4012,0,0.0652238,"Missing"
P16-1222,D10-1051,0,0.0770311,"he task of generating the subsequent clause to match the given antecedent clause is more well-defined than generating all sentences of a poem. Moreover, not all of the sentences in the poems need to follow couplet constraints. There are some formal researches into the area of computer-assisted poetry generation. Scientists from different countries have studied the automatic poem composition in their own languages through different ways: 1) Genetic Algorithms. Manurung et al. (2004; 2011) propose to create poetic texts in English based on state search; 2) Statistical Machine Translation (SMT). Greene et al. (2010) propose a translation model to generation cross-lingual poetry, from Italian to English; 3) Rule-based Templates. Oliveira (2009; 2012) has proposed a system of poem generation platform based on semantic and grammar templates in Spanish. An interactive system has been proposed to reproduce the traditional Japanese poem named Haiku based on rule-based phrase search related to user queries (Tosa et al., 2008; Wu et al., 2009). Netzer et al. (2009) propose another way of Haiku generation using word association rules. As to computer-assisted Chinese poetry generation. There are now several Chines"
P16-1222,C08-1048,0,0.385664,"quent clause from the last iteration will be used as additional information to generate a revised version of the subsequent clause. The rest of the paper is organized as follows. In Section 2, we briefly summarize related work of couplet generation. Then Sections 3 and 4 show the overview of our approach paradigm and then detail the neural models. The experimental results and evaluation are reported in Section 5 and we draw conclusions Section 6. 2 Related Work There are very few studies focused on Chinese couplet generation, based on templates (Zhang and Sun, 2009) or statistic translations (Jiang and Zhou, 2008). The Chinese couplet generation task can be viewed as a reduced form of 2-sentence poem generation (Jiang and Zhou, 2008). Given the first line of the poem, the generator ought to generate the second line accordingly, which is a similar process as couplet generation. We consider automatic Chinese poetry generation to be a closely re2348 (a). Sequential couplet generation. (b). Couplet generation with attention.(c). Couplet generation with polishing schema. Figure 2: Three neural models for couplet generation. More details will be introduced in Section 4. lated research area. Note that there a"
P16-1222,P14-1062,0,0.0276712,"revious sentences sequentially, considering structural templates. Yan et al. (2013; 2016) proposed a summarization framework to generate poems. Recently, along with the prosperity of neural networks, a recurrent neural network based language generation is proposed (Zhang and Lapata, 2014): the generation is more or less a translation process. Given previous sentences, the system generates the next sentence of the poem. We also briefly introduce deep neural networks, which contribute great improvements in NLP. A series of neural models are proposed, such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014) and recurrent neural networks (RNN) (Mikolov et al., 2010) with or without gated recurrent units (GRU) (Cho et al., 2014) and longshort term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We conduct a pilot study to design neural network structures for couplet generation problems. For the first time, we propose a polishing schema for the couplet generation process, and combine it with the attention mechanism to satisfy the couplet constraints, which is novel. 3 Overview The basic idea of the Chinese couplet generation is to build a hidden representation of the antecedent clause, and"
P16-1222,N03-1017,0,0.0268233,"ses conform the length restriction and word pairing between the two clauses. For a higher level of semantic side, evaluators then consider whether the two clauses are semantically meaningful and coherent. Evaluators assign 0-1 scores for both syntactic and semantic criteria (‘0’-no, ‘1’- yes). The evaluation process is conducted as a blind-review3 5.3 Algorithms for Comparisons We implemented several generation methods as baselines. For fairness, we conduct the same pregeneration process to all algorithms. Standard SMT. We adapt the standard phrasebased statistical machine translation method (Koehn et al., 2003) for the couplet task, which regards the antecedent clause as the source language and the subsequent clause as the target language. Couplet SMT. Based on SMT techniques, a phrase-based SMT system for Chinese couplet generation is proposed in (Jiang and Zhou, 2008), which incorporates extensive coupletspecific character filtering and re-rankings. LSTM-RNN. We also include a sequence-tosequence LSTM-RNN (Sutskever et al., 2014). LSTM-RNN is basically a RNN using the LSTM units, which consists of memory cells in order to store information for extended periods of time. For generation, we first use"
P16-1222,W09-2005,0,0.0273935,"c Algorithms. Manurung et al. (2004; 2011) propose to create poetic texts in English based on state search; 2) Statistical Machine Translation (SMT). Greene et al. (2010) propose a translation model to generation cross-lingual poetry, from Italian to English; 3) Rule-based Templates. Oliveira (2009; 2012) has proposed a system of poem generation platform based on semantic and grammar templates in Spanish. An interactive system has been proposed to reproduce the traditional Japanese poem named Haiku based on rule-based phrase search related to user queries (Tosa et al., 2008; Wu et al., 2009). Netzer et al. (2009) propose another way of Haiku generation using word association rules. As to computer-assisted Chinese poetry generation. There are now several Chinese poetry generators available. The system named Daoxiang1 basically relies on manual pattern selection. The system maintains a list of manually created terms related to pre-defined keywords, and inserts terms randomly into the selected template as a poem. The system is simple but random term selection leads to unnatural sentences. 1 http://www.poeming.com/web/index.htm Zhou et al. (2010) use a genetic algorithm for Chinese poetry generation by to"
P16-1222,P02-1040,0,0.0947432,"cs. Perplexity. For most of the language generation research, language perplexity is a sanity check. Our first set of experiments involved intrinsic evaluation of the “perplexity” evaluation for the generated couplets. Perplexity is actually an entropy based evaluation. In this sense, the lower perplexity for the couplets generated, the better performance in purity for the generations, and the couplets are likely to be good. m denotes the length. ] [ m 1 ∑ log p(yi ) pow 2, − m i=1 BLEU. The Bilingual Evaluation Understudy (BLEU) score-based evaluation is usually used for machine translation (Papineni et al., 2002): given the reference translation(s), the algorithm evaluates the quality of text which has been machinetranslated from the reference translation as ground truth. We adapt the BLEU evaluation under the couplet generation scenario. Take a couplet from the dataset, we generate the computer authored subsequent clause given the antecedent clause, and compare it with the original subsequent clause written by humans. There is a concern for such an evaluation metric is that BLEU score can only reflect the partial capability of the models; there is (for most cases) only one ground truth for the genera"
P16-1222,P15-1152,0,0.0487908,"Missing"
P16-1222,D14-1074,0,0.409327,"dom term selection leads to unnatural sentences. 1 http://www.poeming.com/web/index.htm Zhou et al. (2010) use a genetic algorithm for Chinese poetry generation by tonal codings and state search. He et al. (2012) extend the couplet machine translation paradigm (Jiang and Zhou, 2008) from a 2-line couplet to a 4-line poem by giving previous sentences sequentially, considering structural templates. Yan et al. (2013; 2016) proposed a summarization framework to generate poems. Recently, along with the prosperity of neural networks, a recurrent neural network based language generation is proposed (Zhang and Lapata, 2014): the generation is more or less a translation process. Given previous sentences, the system generates the next sentence of the poem. We also briefly introduce deep neural networks, which contribute great improvements in NLP. A series of neural models are proposed, such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014) and recurrent neural networks (RNN) (Mikolov et al., 2010) with or without gated recurrent units (GRU) (Cho et al., 2014) and longshort term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We conduct a pilot study to design neural network structures for"
P16-2022,D15-1075,0,0.195839,"uring training as a part of model parameters. We applied `2 penalty of 3×10−4 ; dropout was chosen by validation with a granularity of 0.1 (Figure 2). We see that a large dropout rate (≥ 0.3) hurts the performance (and also makes training slow) for such a large dataset as opposed to small datasets in other tasks (Peng et al., 2015). Initial learning rate was set to 1, and a power decay was applied. We used stochastic gradient descent with a batch size of 50. Evaluation 4.1 Dataset To evaluate our TBCNN-pair model, we used the newly published Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).4 The dataset is constructed by crowdsourced efforts, each sentence written by humans. Moreover, the SNLI dataset is magnitudes of larger than previous resources, and hence is particularly suitable for comparing neural models. The target labels comprise three classes: Entailment, Contradiction, and Neutral (two irrelevant sentences). We applied the standard train/validation/test split, contraining 550k, 10k, and 10k samples, respectively. Figure 2 presents 4 Mean 8.59 3.93 3.13 2.60 4.3 Performance Table 3 compares our model with previous results. As seen, the TBCNN sentence pair model, follo"
P16-2022,N06-1006,0,0.0934183,"Missing"
P16-2022,de-marneffe-etal-2006-generating,0,0.047208,"Missing"
P16-2022,N13-1090,0,0.00351841,"he parse tree of a sentence; either a constituency tree or a dependency tree applies. In this paper, we prefer the dependency tree-based convolution for its efficiency and compact expressiveness. Concretely, a sentence is first converted to a dependency parse tree.3 Each node in the dependency tree corresponds to a word in the sentence; an edge a→b indicates a is governed by b. Edges are labeled with grammatical relations (e.g., nsubj) between the parent node and its children (de Marneffe et al., 2006). Words are represented by pretrained vector representations, also known as word embeddings (Mikolov et al., 2013a). 3.2 Matching Heuristics In this part, we introduce how vector representations of individual sentences are combined to capture the relation between the premise and hypothesis. As the dataset is large, we prefer O(1) matching operations because of efficiency concerns. Concretely, we have three matching heuristics: • Concatenation of the two sentence vectors, • Element-wise product, and • Element-wise difference. The first heuristic follows the most standard procedure of the “Siamese” architectures, while the latter two are certain measures of “similarity” or 2 Preprinted on arXiv on Septembe"
P16-2022,P06-1114,0,0.0530334,"aper, we propose the TBCNN-pair neural model to recognize entailment and contradiction between two sentences. We leverRecognizing entailment and contradiction between two sentences (called a premise and a hypothesis) is known as natural language inference (NLI) in MacCartney (2009). Provided with a premise sentence, the task is to judge whether the hypothesis can be inferred (entailment), or the hypothesis cannot be true (contradiction). Several examples are illustrated in Table 1. NLI is in the core of natural language understanding and has wide applications in NLP, e.g., question answering (Harabagiu and Hickl, 2006) and automatic summarization (Lacatusu et al., 2006; Yan et al., 2011a; Yan et al., 2011b). Moreover, NLI is also related to other tasks of sentence pair modeling, including paraphrase detection (Hu et al., 2014), relation recognition of discourse units (Liu et al., 2016), etc. Traditional approaches to NLI mainly fall into two groups: feature-rich models and formal reasoning methods. Feature-based approaches typically leverage machine learning models, but require intensive human engineering to represent lexical and syntactic information in two sentences Equal contribution. † E C N Table 1: Ex"
P16-2022,D15-1279,1,0.746525,"n in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin. 1 (MacCartney et al., 2006; Harabagiu et al., 2006). Formal reasoning, on the other hand, converts a sentence into a formal logical representation and uses interpreters to search for a proof. However, such approaches are limited in terms of scope and accuracy (Bos and Markert, 2005). The renewed prosperity of neural networks has made significant achievements in various NLP applications, including individual sentence modeling (Kalchbrenner et al., 2014; Mou et al., 2015) as well as sentence matching (Hu et al., 2014; Yin and Sch¨utze, 2015). A typical neural architecture to model sentence pairs is the “Siamese” structure (Bromley et al., 1993), which involves an underlying sentence model and a matching layer to determine the relationship between two sentences. Prevailing sentence models include convolutional networks (Kalchbrenner et al., 2014) and recurrent/recursive networks (Socher et al., 2011b). Although they have achieved high performance, they may either fail to fully make use of the syntactical information in sentences or be difficult to train due to"
P16-2022,D15-1181,0,0.00707401,"to introduce tree-based convolution to sentence pair modeling tasks like NLI; (2) Leveraging additional heuristics further improves the accuracy while remaining low complexity, outperforming existing sentence encoding-based approaches to a large extent, including feature-rich methods and long short term memory (LSTM)-based recurrent networks.1 2 perhaps, is to concatenate their vector representations (Zhang et al., 2015; Hu et al., 2014, Arc-I). Concatenation is also applied in our previous work of matching the subject and object in relation classification (Xu et al., 2015; Xu et al., 2016). He et al. (2015) apply additional heuristics, namely Euclidean distance, cosine measure, and elementwise absolute difference. The above methods operate on a fixed-size vector representation of a sentence, categorized as sentence encoding-based approaches. Thus the matching complexity is O(1), i.e., independent of the sentence length. Word-byword similarity matrices are introduced to enhance interaction. To obtain the similarity matrix, Hu et al. (2014) (Arc-II) concatenate two words’ vectors (after convolution), Socher et al. (2011a) compute Euclidean distance, and Wan et al. (2015) apply tensor product. In t"
P16-2022,D15-1252,1,0.765243,"All our neural layers, including embeddings, were set to 300 dimensions. The model is mostly robust when the dimension is large, e.g., several hundred (Collobert and Weston, 2008). Word embeddings were pretrained ourselves by word2vec on the English Wikipedia corpus and fined tuned during training as a part of model parameters. We applied `2 penalty of 3×10−4 ; dropout was chosen by validation with a granularity of 0.1 (Figure 2). We see that a large dropout rate (≥ 0.3) hurts the performance (and also makes training slow) for such a large dataset as opposed to small datasets in other tasks (Peng et al., 2015). Initial learning rate was set to 1, and a power decay was applied. We used stochastic gradient descent with a batch size of 50. Evaluation 4.1 Dataset To evaluate our TBCNN-pair model, we used the newly published Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).4 The dataset is constructed by crowdsourced efforts, each sentence written by humans. Moreover, the SNLI dataset is magnitudes of larger than previous resources, and hence is particularly suitable for comparing neural models. The target labels comprise three classes: Entailment, Contradiction, and Neutral (two"
P16-2022,P14-1062,0,0.0124331,"ence combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin. 1 (MacCartney et al., 2006; Harabagiu et al., 2006). Formal reasoning, on the other hand, converts a sentence into a formal logical representation and uses interpreters to search for a proof. However, such approaches are limited in terms of scope and accuracy (Bos and Markert, 2005). The renewed prosperity of neural networks has made significant achievements in various NLP applications, including individual sentence modeling (Kalchbrenner et al., 2014; Mou et al., 2015) as well as sentence matching (Hu et al., 2014; Yin and Sch¨utze, 2015). A typical neural architecture to model sentence pairs is the “Siamese” structure (Bromley et al., 1993), which involves an underlying sentence model and a matching layer to determine the relationship between two sentences. Prevailing sentence models include convolutional networks (Kalchbrenner et al., 2014) and recurrent/recursive networks (Socher et al., 2011b). Although they have achieved high performance, they may either fail to fully make use of the syntactical information in sentences or be difficu"
P16-2022,D11-1014,0,0.0331373,"Missing"
P16-2022,D15-1206,1,0.0336365,"are two-fold: (1) We are the first to introduce tree-based convolution to sentence pair modeling tasks like NLI; (2) Leveraging additional heuristics further improves the accuracy while remaining low complexity, outperforming existing sentence encoding-based approaches to a large extent, including feature-rich methods and long short term memory (LSTM)-based recurrent networks.1 2 perhaps, is to concatenate their vector representations (Zhang et al., 2015; Hu et al., 2014, Arc-I). Concatenation is also applied in our previous work of matching the subject and object in relation classification (Xu et al., 2015; Xu et al., 2016). He et al. (2015) apply additional heuristics, namely Euclidean distance, cosine measure, and elementwise absolute difference. The above methods operate on a fixed-size vector representation of a sentence, categorized as sentence encoding-based approaches. Thus the matching complexity is O(1), i.e., independent of the sentence length. Word-byword similarity matrices are introduced to enhance interaction. To obtain the similarity matrix, Hu et al. (2014) (Arc-II) concatenate two words’ vectors (after convolution), Socher et al. (2011a) compute Euclidean distance, and Wan et a"
P16-2022,C16-1138,1,0.0361475,") We are the first to introduce tree-based convolution to sentence pair modeling tasks like NLI; (2) Leveraging additional heuristics further improves the accuracy while remaining low complexity, outperforming existing sentence encoding-based approaches to a large extent, including feature-rich methods and long short term memory (LSTM)-based recurrent networks.1 2 perhaps, is to concatenate their vector representations (Zhang et al., 2015; Hu et al., 2014, Arc-I). Concatenation is also applied in our previous work of matching the subject and object in relation classification (Xu et al., 2015; Xu et al., 2016). He et al. (2015) apply additional heuristics, namely Euclidean distance, cosine measure, and elementwise absolute difference. The above methods operate on a fixed-size vector representation of a sentence, categorized as sentence encoding-based approaches. Thus the matching complexity is O(1), i.e., independent of the sentence length. Word-byword similarity matrices are introduced to enhance interaction. To obtain the similarity matrix, Hu et al. (2014) (Arc-II) concatenate two words’ vectors (after convolution), Socher et al. (2011a) compute Euclidean distance, and Wan et al. (2015) apply te"
P16-2022,D11-1040,1,0.240178,"diction between two sentences. We leverRecognizing entailment and contradiction between two sentences (called a premise and a hypothesis) is known as natural language inference (NLI) in MacCartney (2009). Provided with a premise sentence, the task is to judge whether the hypothesis can be inferred (entailment), or the hypothesis cannot be true (contradiction). Several examples are illustrated in Table 1. NLI is in the core of natural language understanding and has wide applications in NLP, e.g., question answering (Harabagiu and Hickl, 2006) and automatic summarization (Lacatusu et al., 2006; Yan et al., 2011a; Yan et al., 2011b). Moreover, NLI is also related to other tasks of sentence pair modeling, including paraphrase detection (Hu et al., 2014), relation recognition of discourse units (Liu et al., 2016), etc. Traditional approaches to NLI mainly fall into two groups: feature-rich models and formal reasoning methods. Feature-based approaches typically leverage machine learning models, but require intensive human engineering to represent lexical and syntactic information in two sentences Equal contribution. † E C N Table 1: Examples of relations between a premise and a hypothesis: Entailment, C"
P16-2022,N15-1091,0,0.0330304,"Missing"
P16-2022,D15-1266,0,0.00822726,"and difference; they are effective in capturing relationships between two sentences, but remain low complexity. To sum up, the main contributions of this paper are two-fold: (1) We are the first to introduce tree-based convolution to sentence pair modeling tasks like NLI; (2) Leveraging additional heuristics further improves the accuracy while remaining low complexity, outperforming existing sentence encoding-based approaches to a large extent, including feature-rich methods and long short term memory (LSTM)-based recurrent networks.1 2 perhaps, is to concatenate their vector representations (Zhang et al., 2015; Hu et al., 2014, Arc-I). Concatenation is also applied in our previous work of matching the subject and object in relation classification (Xu et al., 2015; Xu et al., 2016). He et al. (2015) apply additional heuristics, namely Euclidean distance, cosine measure, and elementwise absolute difference. The above methods operate on a fixed-size vector representation of a sentence, categorized as sentence encoding-based approaches. Thus the matching complexity is O(1), i.e., independent of the sentence length. Word-byword similarity matrices are introduced to enhance interaction. To obtain the sim"
P17-1040,D11-1141,0,0.0360751,"he only work in neural-network-based noise modeling is to use one single global transition matrix to model the noise introduced by crosslingual projection of training data (Fang and Cohn, 2016). Our work advances them through generating a transition matrix dynamically for each instance, to avoid using one single component to characterize both reliable and unreliable data. In addition to relation extraction, distant supervision (DS) is shown to be effective in generating training data for various NLP tasks, e.g., tweet sentiment classification (Go et al., 2009), tweet named entity classifying (Ritter et al., 2011), etc. However, these early applications of DS do not well address the issue of data noise. In relation extraction (RE), recent works have been proposed to reduce the influence of wrongly labeled data. The work presented by (Takamatsu et al., 2012) removes potential noisy sentences by identifying bad syntactic patterns at the preprocessing stage. (Xu et al., 2013) use pseudorelevance feedback to find possible false negative data. (Riedel et al., 2010) make the at-leastone assumption and propose to alleviate the noise problem by considering RE as a multi-instance classification problem. Followi"
P17-1040,K16-1018,0,0.0182451,"vely trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios. 1 Introduction Distant supervision (DS) is rapidly emerging as a viable means for supporting various classification tasks – from relation extraction (Mintz et al., 2009) and sentiment classification (Go et al., 2009) to cross-lingual semantic analysis (Fang and Cohn, 2016). By using knowledge learned from seed examples to label data, DS automatically prepares large scale training data for these tasks. While promising, DS does not guarantee perfect results and often introduces noise to the generated data. In the context of relation extraction, DS works by considering sentences containing both the subject and object of a &lt;subj, rel, obj&gt; triple 430 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 430–439 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18"
P17-1040,Q13-1030,0,0.120447,"hanxing Zhu3 , Songfang Huang4 , Rui Yan1 and Dongyan Zhao1 1 ICST, Peking University, China 2 School of Computing and Communications, Lancaster University, UK 3 Peking University, China 4 IBM China Research Lab, China {bf luo,fengyansong,zhanxing.zhu,ruiyan,zhaody}@pku.edu.cn z.wang@lancaster.ac.uk huangsf@cn.ibm.com Abstract as its supports. However, the generated data are not always perfect. For instance, DS could match the knowledge base (KB) triple, &lt;Donald Trump, born-in, New York&gt; in false positive contexts like Donald Trump worked in New York City. Prior works (Takamatsu et al., 2012; Ritter et al., 2013) show that DS often mistakenly labels real positive instances as negative (false negative) or versa vice (false positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-"
P17-1040,P11-1055,0,0.0938601,"n extraction (RE), recent works have been proposed to reduce the influence of wrongly labeled data. The work presented by (Takamatsu et al., 2012) removes potential noisy sentences by identifying bad syntactic patterns at the preprocessing stage. (Xu et al., 2013) use pseudorelevance feedback to find possible false negative data. (Riedel et al., 2010) make the at-leastone assumption and propose to alleviate the noise problem by considering RE as a multi-instance classification problem. Following this assumption, people further improves the original paradigm using probabilistic graphic models (Hoffmann et al., 2011; Surdeanu et al., 2012), and neural network methods (Zeng et al., 2015). Recently, (Lin et al., 2016) propose to use attention mechanism to reduce the noise within a sentence bag. Instead of characterizing the noise, these approaches only aim to alleviate the effect of noise. The at-least-one assumption is often too strong in practice, and there are still chances that the sentence bag may be false positive or false negative. Thus it is important to model the noise pattern to guide the learning procedure. (Ritter et al., 2013) and (Min et al., 2013) try to employ a set of latent variables to r"
P17-1040,D12-1042,0,0.523535,"positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-leastone assumption that at least one of the aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural ne"
P17-1040,P12-1076,0,0.121946,"Feng∗1 , Zheng Wang2 , Zhanxing Zhu3 , Songfang Huang4 , Rui Yan1 and Dongyan Zhao1 1 ICST, Peking University, China 2 School of Computing and Communications, Lancaster University, UK 3 Peking University, China 4 IBM China Research Lab, China {bf luo,fengyansong,zhanxing.zhu,ruiyan,zhaody}@pku.edu.cn z.wang@lancaster.ac.uk huangsf@cn.ibm.com Abstract as its supports. However, the generated data are not always perfect. For instance, DS could match the knowledge base (KB) triple, &lt;Donald Trump, born-in, New York&gt; in false positive contexts like Donald Trump worked in New York City. Prior works (Takamatsu et al., 2012; Ritter et al., 2013) show that DS often mistakenly labels real positive instances as negative (false negative) or versa vice (false positive), and there could be confusions among positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple pre"
P17-1040,P16-1200,0,0.318747,"e aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its early stage and much remains to be done. In this paper, we aim to enhance DS noise modeling by providing the capability to explicitly characterize the noise in the DS-style training data Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application o"
P17-1040,N13-1095,0,0.0949015,"ong positive labels as well. These noises can severely affect training and lead to poorlyperforming models. Tackling the noisy data problem of DS is nontrivial, since there usually lacks of explicit supervision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns (Takamatsu et al., 2012), design new models to capture certain types of noise or aggregate multiple predictions under the at-leastone assumption that at least one of the aligned sentences supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its earl"
P17-1040,P13-2117,0,0.069312,"Missing"
P17-1040,P09-1113,0,0.0499241,"erize the noise in the training data built by distant supervision. The transition matrix can be effectively trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios. 1 Introduction Distant supervision (DS) is rapidly emerging as a viable means for supporting various classification tasks – from relation extraction (Mintz et al., 2009) and sentiment classification (Go et al., 2009) to cross-lingual semantic analysis (Fang and Cohn, 2016). By using knowledge learned from seed examples to label data, DS automatically prepares large scale training data for these tasks. While promising, DS does not guarantee perfect results and often introduces noise to the generated data. In the context of relation extraction, DS works by considering sentences containing both the subject and object of a &lt;subj, rel, obj&gt; triple 430 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 430–439 c Vancouver"
P17-1040,D15-1203,0,0.521273,"s supports the triple in KB (Riedel et al., 2010; Surdeanu et al., 2012; Ritter et al., 2013; Min et al., 2013). These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to certain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent breakthrough in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise (Lin et al., 2016; Zeng et al., 2015). Although promising, modeling noise within neural network architectures is still in its early stage and much remains to be done. In this paper, we aim to enhance DS noise modeling by providing the capability to explicitly characterize the noise in the DS-style training data Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising, this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application of distant supervisio"
P17-1040,D14-1162,0,0.114232,"serve as gold standard, we only evaluate bag-level models on E NTITY RE, a standard practice in previous works (Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016). Datasets We evaluate our approach on two datasets. 434 5.2 1 .0 0 Experimental Setup Hyper-parameters We use 200 convolution kernels with widow size 3. During training, we use stochastic gradient descend (SGD) with batch size 20. The learning rates for sentence-level and bag-level models are 0.1 and 0.01, respectively. Sentence level experiments are performed on T IME RE, using 100-d word embeddings pretrained using GloVe (Pennington et al., 2014) on Wikipedia and Gigaword (Parker et al., 2011), and 20-d vectors for distance embeddings. Each of the three subsets of T IME RE is added after the previous phase has run for 15 epochs. The trace regularization weights are β1 = 0.01, β2 = −0.01 and β3 = −0.1, respectively, from the reliable to the most unreliable, with the ratio of β3 and β2 fixed to 10 or 5 when tuning. Bag level experiments are performed on both T IME RE and E NTITY RE. For T IME RE, we use the same parameters as above. For E NTITY RE, we use 50-d word embeddings pre-trained on the NYT corpus using word2vec (Mikolov et al.,"
P17-2036,W14-4012,0,0.128818,"Missing"
P17-2036,D16-1230,0,0.0722005,"Missing"
P17-2036,C16-1316,1,0.792893,"end to generate longer, more meaningful and diverse replies, which sheds light on neural sequence generation. Table 2: The length, entropy, and diversity of the replies on the context-insensitive and contextaware (WSeq,concat) methods. relevant context utterances as well as weakens irrelevant contexts. RQ2: What is the effect of context on neural dialog systems? We are now curious about how context information affects neural conversational systems. In Table 2, we present three auxiliary metrics, i.e., sentence length, entropy, and diversity. The former two are used in Serban et al. (2016) and Mou et al. (2016), whereas the latter one is used in Zhang and Hurley (2008). As shown, content-aware conversational models tend to generate longer, more meaningful and diverse replies compared with content-insensitive models, given that they also improve BLEU scores.2 This shows an interesting phenomenon of neural sequence generation: an encoder-decoder framework needs sufficient source information for meaningful generation of the target; it simply does not fall into meaningful content from less meaningful input. A similar phenomenon is also reported in our previous work (Mou et al., 2016); we show that, a sa"
P17-2036,D11-1054,0,0.240659,"Missing"
P17-2036,P15-1152,0,0.283936,"rison to analyze how to use context effectively. In this paper, we conduct an empirical study to compare various models and investigate the effect of context information in dialog systems. We also propose a variant that explicitly weights context vectors by context-query relevance, outperforming the other baselines. 1 Introduction Recently, human-computer conversation is attracting increasing attention due to its promising potentials and alluring commercial values. Researchers have proposed both retrieval methods (Ji et al., 2014; Yan et al., 2016) and generative methods (Ritter et al., 2011; Shang et al., 2015) for automatic conversational systems. With the success of deep learning techniques, neural networks have demonstrated powerful capability of learning human dialog patterns; given a user-issued utterance as an input query q, the network can generate a reply r, which is usually accomplished in a sequence-to-sequence (Seq2Seq) manner (Shang et al., 2015). In the literature, there are two typical research setups for dialog systems: single-turn and multiturn. Single-turn conversation is, perhaps, the simplest setting where the model only takes q into consideration when generating r (Shang et al.,"
P17-2036,N15-1020,0,0.0169469,"Missing"
P17-2036,D16-1172,0,0.0123649,"Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2036 text weighting approach, outperforming the other baselines. 2 2.1 Models Non-Hierarchical Model To model a few utterances before the current query, several studies directly concatenate these sentences together and use a single model to capture the meaning of context and the query (Yan et al., 2016; Sordoni et al., 2015). They are referred to as non-hierarchical models in our paper. Such method is also used in other NLP tasks, e.g., document-level sentiment analysis (Xu et al., 2016) and machine comprehension (Wang and Jiang, 2017). Following the classic encode-decoder framework, we use a Seq2Seq network, which transforms the query and context into a fixed-length vector venc by a recurrent neural network (RNN) during encoding; then, in the decoding phase, it generates a reply r with another RNN in a wordby-word fashion. (See Figure 1a.) In our study, we adopt RNNs with gated recurrent units (Cho et al., 2014, GRUs), which alleviates the long propagation problem of vanilla RNNs. When decoding, we apply beam search with a size of 5. 2.2 (a) Non-hierarchical model. (b) Hiera"
P18-1194,D16-1146,0,0.0368066,"Missing"
P18-1194,H90-1021,0,0.0607127,"hich matches the property of wk zk better than probability. Actually, when performing model ensemble, ensembling with logits is often empirically better than with the final probability3 . This is also the reason why we choose to operate on logits in Sec. 3.3. 4 Evaluation Methodology Our experiments aim to answer three questions: Q1: Does the use of REs enhance the learning quality when the number of annotated instances is small? Q2: Does the use of REs still help when using the full training data? Q3: How can we choose from different combination methods? 4.1 Datasets We use the ATIS dataset (Hemphill et al., 1990) to evaluate our approach. This dataset is widely used in SLU research. It includes queries of flights, meal, etc. We follow the setup of Liu and Lane (2016) by using 4,978 queries for training and 893 for testing, with 18 intent labels and 127 slot labels. We also split words like Miami’s into Miami ’s during the tokenization phase to reduce the number of words that do not have a pre-trained word embedding. This strategy is useful for fewshot learning. 3 An example can be found in the ensemble version that Juan et al. (2016) used in the Avazu Kaggle competition. To answer Q1 , we also exploit"
P18-1194,P16-1228,0,0.0358084,"Missing"
P18-1194,D16-1173,0,0.0517657,"Missing"
P18-1194,D17-1201,0,0.0204074,"s the effectiveness of our methods, and indicates that simple REs are quite costefficient since these simple REs only contain 1-2 RE groups and thus very easy to produce. We can also see that using complex REs generally leads to better results compared to using simple REs. This indicates that when considering using REs to improve a NN model, we can start with simple REs, and gradually increase the RE complexity to improve the performance over time7 . 6 Related Work Our work builds upon the following techniques, while qualitatively differing from each NN with Rules. On the initialization side, Li et al. (2017) uses important n-grams to initialize the convolution filters. On the input side, Wang et al. (2017a) uses knowledge base rules to find relevant concepts for short texts to augment input. On the output side, Hu et al. (2016a; 2016b) and Guo et al. (2017) use FOL rules to rectify the output probability of NN, and then let NN learn from the rectified distribution in a teacher-student framework. Xiao et al. (2017), on the other hand, modifies the decoding score of NN by multiplying a weight derived from rules. On the loss function side, people modify the loss function to model the relationship be"
P18-1194,D16-1197,0,0.0227221,"n initialization or in loss function often require special properties of the task, these approaches are not applicable to our problem. Our work thus offers new ways to exploit RE rules at different levels of a NN. NNs and REs. As for NNs and REs, previous work has tried to use RE to speed up the decoding phase of a NN (Strauß et al., 2016) and generating REs from natural language specifications of the 7 We do not include results of both for slot filling since its REs are different from feat and logit, and we have already shown that the attention loss method does not work for slot filling. RE (Locascio et al., 2016). By contrast, our work aims to use REs to improve the prediction ability of a NN. Few-Shot Learning. Prior work either considers few-shot learning in a metric learning framework (Koch et al., 2015; Vinyals et al., 2016), or stores instances in a memory (Santoro et al., 2016; Kaiser et al., 2017) to match similar instances in the future. Wang et al. (2017b) further uses the semantic meaning of the class name itself to provide extra information for few-shot learning. Unlike these previous studies, we seek to use the humangenerated REs to provide additional information. Natural Language Understa"
P18-1194,D14-1162,0,0.0835473,"ositive RE for intent (or slot) k can often be treated as negative REs for other intents (or slots). As such, we use the positive REs for intent (or slot) k as the negative REs for other intents (or slots) in our experiments. 4.3 Experimental Setup Hyper-parameters. Our hyper-parameters for the BLSTM are similar to the ones used by Liu and Lane (2016). Specifically, we use batch size 16, dropout probability 0.5, and BLSTM cell size 100. The attention loss weight is 16 (both positive and negative) for full few-shot learning settings and 1 for other settings. We use the 100d GloVe word vectors (Pennington et al., 2014) pre-trained on Wikipedia and Gigaword (Parker et al., 2011), and the Adam optimizer (Kingma and Ba, 2014) with learning rate 0.001. Evaluation Metrics. We report accuracy and macro-F1 for intent detection, and micro/macroF1 for slot filling. Micro/macro-F1 are the harmonic mean of micro/macro precision and recall. Macro-precision/recall are calculated by averaging precision/recall of each label, and microprecision/recall are averaged over each prediction. Competitors and Naming Conventions. Here, a bold Courier typeface like BLSTM denotes the notations of the models that we will compare in Se"
P18-2070,W01-1605,0,0.241076,"till fall behind this state-of-the-art method. The main reason might be that MST-full follows a global graph-based dependency parsing framework, where their high order methods (in cubic time complexity) can directly analyze the relationship between any EDUs pairs in the discourse, while, we choose the transition-based local method with linear time complexity, which can only investigate the top EDUs in S and B according to the selected actions, thus usually has a lower performance than the global graph-based methods, but with a Evaluation and Results Dataset: We use the RST Discourse Treebank (Carlson et al., 2001) with the same split as in (Li et al., 2014), i.e., 312 for training, 30 for development and 38 for testing. We experiment with two set of relations, the 111 types of fine-grained relations and the 19 types of coarse-grained relations, respectively. Evaluation Metrics: In the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), head is the core of a discourse, and a dependent gives supporting evidence to its head with certain relationship. We adopt unlabeled accuracy U AS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both"
P18-2070,W10-4327,0,0.0325967,"account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly 1 . 1 Introduction Discourse parsing aims to identify the structure and relationship between different element discourse units (EDUs). As a fundamental topic in natural language processing, discourse parsing can assist many down-stream applications such as summarization (Louis et al., 2010), sentiment analysis (Polanyi and van den Berg, 2011) and question-answering (Ferrucci et al., 2010). However, the performance of discourse parsing is still far from perfect, especially for EDUs that are distant to each other in the discourse. In fact, as found in (Jia et al., 2018), the discourse parsing performance drops quickly as the dependency span increases. The reason may be twofold: 1 Code for replicating our experiments is available at https://github.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers),"
P18-2070,P04-1015,0,0.130798,"rtain relationship. We adopt unlabeled accuracy U AS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both correct heads and relations) as our evaluation metrics. Baselines: We compare our method with the following baselines and models: (1) Perceptron: We re-implement the perceptron based arc-eager style dependency discourse parser as mentioned in (Jia et al., 2018) with coarse-grained relation. The Perceptron model chooses words, POS tags, positions and length features, totally 100 feature templates, with the early update strategy (Collins and Roark, 2004). (2) Jia18: Jia et al. (2018) implement a transition-based discourse parser with stacked LSTM, where they choose a two-layer LSTM to represent EDUs by encoding four kinds of features including words, POS tags, positions and length features. (3) Basic EDU representation (Basic): Our discourse parser with the basic EDU representation method mentioned in Section 3. (4) Memory refined representation (Refined): Our full parser equipped with the basic EDU representation method and the memory networks to capture the discourse cohesion mentioned in Section 3. (5) MST-full (Li et al., 2014): a graph-b"
P18-2070,P15-1033,0,0.0222777,"hub.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 438–443 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 (1) I feel hungry after wake up, (2) I rush into the kitchen and make my breakfast. (3) My breakfast is hamburger. (4) It is eight o'clock when I leave home. (5) So late! Model overview Memory network slot1 Our parser is an arc-eager style transition system (Nivre, 2003) with 2 stacks and a queue as shown in Figure 2, which is similar in spirit with (Dyer et al., 2015; Ballesteros et al., 2015). We follow the conventional data structures in transition-based dependency parsing, i.e., a queue (B) of EDUs to be processed, a stack (S) to store the partially constructed discourse trees, and a stack (A) to represent the history of transitions (actions combined with discourse relations). slot2 slot3 (9) It is nine o'clock. (10) Thank God, I am not late for work. slotn ... (6) I drive into the highway, (7) but meet a traffic jam. (8) Oh, I finally arrive at the company. (11) But the hamburger is cold, (12) order some take-away food is better, maybe. Figure 1: An i"
P18-2070,P14-5010,0,0.00567472,"Missing"
P18-2070,J91-1002,0,0.744108,"he syntactic or semantic relationship between words or phrases in a discourse, and, to some extent, can indicate the topic changing or threads in a discourse. Discourse cohesion includes five situations, including reference, substitution, ellipsis, conjunction and lexical cohesion (Halliday and Hasan, 1989). Here, lexical cohesion reflects the semantic relationship of words, and can be modeled as the recurrence of words, synonym and contextual words. However, previous works do not well model the discourse cohesion within the discourse parsing task, or do not even take this issue into account. Morris and Hirst (1991) proposes to utilize Roget thesauri to form lexical chains (sequences of semantically related words that can reflect the topic shifts within a discourse), which are used to extract features to characterize discourse structures. (Joty et al., 2013) uses lexical chain feature to model multi-sentential relation. Actually, these simplified cohesion features can already improve parsing performance, especially in long spans. Secondly, in modern neural network methods, modeling discourse cohesion as part of the networks is not a trivial task. One can still use off-the-shell tools to obtain lexical ch"
P18-2070,W03-3017,0,0.457685,"The reason may be twofold: 1 Code for replicating our experiments is available at https://github.com/PKUYeYuan/ACL2018 CFDP. 438 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 438–443 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 (1) I feel hungry after wake up, (2) I rush into the kitchen and make my breakfast. (3) My breakfast is hamburger. (4) It is eight o'clock when I leave home. (5) So late! Model overview Memory network slot1 Our parser is an arc-eager style transition system (Nivre, 2003) with 2 stacks and a queue as shown in Figure 2, which is similar in spirit with (Dyer et al., 2015; Ballesteros et al., 2015). We follow the conventional data structures in transition-based dependency parsing, i.e., a queue (B) of EDUs to be processed, a stack (S) to store the partially constructed discourse trees, and a stack (A) to represent the history of transitions (actions combined with discourse relations). slot2 slot3 (9) It is nine o'clock. (10) Thank God, I am not late for work. slotn ... (6) I drive into the highway, (7) but meet a traffic jam. (8) Oh, I finally arrive at the compa"
P18-2070,P13-1048,0,0.0540609,"Missing"
P19-1001,E17-1104,0,0.0356527,"on between c and r, and how to learn such a deep model from D. form. As far as we know, this is the first architecture that realizes deep interaction for multi-turn response selection. Encouraged by the big success of deep neural architectures such as Resnet (He et al., 2016) and inception (Szegedy et al., 2015) in computer vision, researchers have studied if they can achieve similar results with deep neural networks on NLP tasks. Although deep models have not yet brought breakthroughs to NLP as they do to computer vision, they have proven effective in a few tasks such as text classification (Conneau et al., 2017), natural language inference (Kim et al., 2018; Tay et al., 2018), and question answering (Tay et al., 2018; Kim et al., 2018), etc. In this work, we attempt to improve the accuracy of multi-turn response selection in retrieval-based dialogue systems by increasing the depth of context-response interaction in matching. Through extensive studies on benchmarks, we show that depth can bring significant improvement to model performance on the task. 3 4 Interaction-over-Interaction Network We define g(·, ·) as an interaction-over-interaction network (IoI). Figure 1 illustrates the architecture of Io"
P19-1001,P16-1094,0,0.0600994,"Missing"
P19-1001,D16-1127,0,0.0465592,"Missing"
P19-1001,D17-1230,0,0.0810916,"generation model estimated from a largescale conversation corpus (Serban et al., 2016; Li et al., 2017b). In this work, we study the problem of multi-turn response selection for retrievalbased dialogue systems where the input is a conversation context consisting of a sequence of utterances. Compared with generation-based methods, retrieval-based methods are superior in terms of response fluency and diversity, and thus have been widely applied in commercial chatbots such as the social bot XiaoIce (Shum et al., 2018) from Microsoft, and the e-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017a). A key step in multi-turn response selection is to measure the matching degree between a conversation context and a response candidate. Stateof-the-art methods (Wu et al., 2017; Zhou et al., 2018b) perform matching within a representationinteraction-aggregation framework (Wu et al., 2018b) where matching signals in each utteranceresponse pair are distilled from their interaction based on their representations, and then are aggregated as a matching score. Although utteranceresponse interaction has proven to be crucial to the performance of the matching models (Wu et al., 2017), it is execute"
P19-1001,W15-4640,0,0.323161,"ses with specific personas or emotions (Li et al., 2016a; Zhang et al., 2018a; Zhou et al., 2018a); and to pursue better optimization strategies (Li et al., 2017b, 2016b). The second group learns a matching model of a human input and a response candidate for response selection. Along this line, the focus of research starts from single-turn response selection by setting the human input as a single message (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015), and moves to context-response matching for multi-turn response selection recently. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018b), and the deep attention matching network (Zhou et al., 2018b). Besides model design, some attention is also paid to the learning problem of matching models (Wu et al., 2018a). Our work belongs to the second group. The proposed interaction-over-interaction network is unique in that it performs matching by stacking multiple interaction blocks, and thus extends the shallow interaction in state-of-the-art methods to a deep We co"
P19-1001,D18-1479,0,0.0157468,"As far as we know, this is the first architecture that realizes deep interaction for multi-turn response selection. Encouraged by the big success of deep neural architectures such as Resnet (He et al., 2016) and inception (Szegedy et al., 2015) in computer vision, researchers have studied if they can achieve similar results with deep neural networks on NLP tasks. Although deep models have not yet brought breakthroughs to NLP as they do to computer vision, they have proven effective in a few tasks such as text classification (Conneau et al., 2017), natural language inference (Kim et al., 2018; Tay et al., 2018), and question answering (Tay et al., 2018; Kim et al., 2018), etc. In this work, we attempt to improve the accuracy of multi-turn response selection in retrieval-based dialogue systems by increasing the depth of context-response interaction in matching. Through extensive studies on benchmarks, we show that depth can bring significant improvement to model performance on the task. 3 4 Interaction-over-Interaction Network We define g(·, ·) as an interaction-over-interaction network (IoI). Figure 1 illustrates the architecture of IoI. The model pairs each utterance in a context with a response ca"
P19-1001,C16-1316,1,0.886108,"Missing"
P19-1001,D13-1096,0,0.300989,"Missing"
P19-1001,N18-1202,0,0.0568928,"Missing"
P19-1001,N16-1170,0,0.233766,"b), we directly copy the numbers from the paper. For the E-commerce data, Zhang et al. (2018b) report performance of all baselines except DAM. Thus, we copy all available numbers from the paper and implement DAM with the published code4 . In order to conduct statistical tests, we also run the code of DAM on the Ubuntu data and the Douban data. Baselines We compare IoI with the following models: Single-turn Matching Models: these models, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), BiLSTM (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016), perform context-response matching by concatenating all utterances in a context into a single long document and calculating a matching score between the document and a response candidate. Multi-View (Zhou et al., 2016): the model calculates matching degree between a context and a response candidate from both a word sequence view and an utterance sequence view. DL2R (Yan et al., 2016): the model first reformulates the last utterance with previous turns in a context with different approaches. A response candidate and the reformulated message are then represented by a composition of RNN and CNN."
P19-1001,P18-2067,1,0.930561,". Compared with generation-based methods, retrieval-based methods are superior in terms of response fluency and diversity, and thus have been widely applied in commercial chatbots such as the social bot XiaoIce (Shum et al., 2018) from Microsoft, and the e-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017a). A key step in multi-turn response selection is to measure the matching degree between a conversation context and a response candidate. Stateof-the-art methods (Wu et al., 2017; Zhou et al., 2018b) perform matching within a representationinteraction-aggregation framework (Wu et al., 2018b) where matching signals in each utteranceresponse pair are distilled from their interaction based on their representations, and then are aggregated as a matching score. Although utteranceresponse interaction has proven to be crucial to the performance of the matching models (Wu et al., 2017), it is executed in a rather shallow manner where matching between an utterance and a response candidate is determined only by one step of interaction on each type or each layer of representations. In this paper, we attempt to move from shallow interaction to deep interaction, and consider context-respons"
P19-1001,P15-1152,0,0.108809,"Missing"
P19-1001,P18-1103,0,0.332465,"alogue systems where the input is a conversation context consisting of a sequence of utterances. Compared with generation-based methods, retrieval-based methods are superior in terms of response fluency and diversity, and thus have been widely applied in commercial chatbots such as the social bot XiaoIce (Shum et al., 2018) from Microsoft, and the e-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017a). A key step in multi-turn response selection is to measure the matching degree between a conversation context and a response candidate. Stateof-the-art methods (Wu et al., 2017; Zhou et al., 2018b) perform matching within a representationinteraction-aggregation framework (Wu et al., 2018b) where matching signals in each utteranceresponse pair are distilled from their interaction based on their representations, and then are aggregated as a matching score. Although utteranceresponse interaction has proven to be crucial to the performance of the matching models (Wu et al., 2017), it is executed in a rather shallow manner where matching between an utterance and a response candidate is determined only by one step of interaction on each type or each layer of representations. In this paper,"
P19-1001,P17-1046,1,0.75996,"retrievalbased dialogue systems where the input is a conversation context consisting of a sequence of utterances. Compared with generation-based methods, retrieval-based methods are superior in terms of response fluency and diversity, and thus have been widely applied in commercial chatbots such as the social bot XiaoIce (Shum et al., 2018) from Microsoft, and the e-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017a). A key step in multi-turn response selection is to measure the matching degree between a conversation context and a response candidate. Stateof-the-art methods (Wu et al., 2017; Zhou et al., 2018b) perform matching within a representationinteraction-aggregation framework (Wu et al., 2018b) where matching signals in each utteranceresponse pair are distilled from their interaction based on their representations, and then are aggregated as a matching score. Although utteranceresponse interaction has proven to be crucial to the performance of the matching models (Wu et al., 2017), it is executed in a rather shallow manner where matching between an utterance and a response candidate is determined only by one step of interaction on each type or each layer of representatio"
P19-1001,P18-1205,0,0.0768423,"Missing"
P19-1001,C18-1317,0,0.174548,"and the response candidate. 2 Related Work Existing methods for building an open-domain dialogue system can be categorized into two groups. The first group learns response generation models under an encoder-decoder framework. On top of the basic sequence-to-sequence with attention architecture (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018), various extensions have been made to tackle the “safe response” problem (Li et al., 2015; Mou et al., 2016; Xing et al., 2017; Zhao et al., 2017; Song et al., 2018); to generate responses with specific personas or emotions (Li et al., 2016a; Zhang et al., 2018a; Zhou et al., 2018a); and to pursue better optimization strategies (Li et al., 2017b, 2016b). The second group learns a matching model of a human input and a response candidate for response selection. Along this line, the focus of research starts from single-turn response selection by setting the human input as a single message (Wang et al., 2013; Hu et al., 2014; Wang et al., 2015), and moves to context-response matching for multi-turn response selection recently. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2"
P19-1001,P17-1061,0,0.0351046,"Missing"
P19-1001,D16-1036,1,0.905357,"Missing"
P19-1370,P16-1094,0,0.0315846,"nd δ to co-teaching. Experiments are conducted with DAM on the two data sets. 5 Related Work So far, methods used to build an open domain dialogue system can be divided into two categories. The first category utilize an encoderdecoder framework to learn response generation models. Since the basic sequence-to-sequence models (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching mod"
P19-1370,W15-4640,0,0.563357,"val-based systems are often superior to their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017). A key problem in response selection is how to measure the matching degree between a conversation context (a message with several turns of conversation history) and a response candidate. Existing studies have paid tremendous effort to build a matching model with neural architectures (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b), and advanced models such as the deep attention matching network (DAM) (Zhou et al., 2018b) have achieved impressive performance on benchmarks. In contrary to the progress on model architectures, there is little exploration on learning approaches of the models. On the one hand, neural matching models are becoming more and more complicated; on the other hand, all models are simply learned by distinguishing human responses from some automatically constructed negative response candidates (e.g., by random sampling). Although this heuristic"
P19-1370,C16-1316,1,0.851966,"ns on Douban 0.60 (b) 0.1 0.3 0.5 0.7 0.9 0.95 1.0 δ Data curriculum on ECD Figure 3: Effects of λ and δ to co-teaching. Experiments are conducted with DAM on the two data sets. 5 Related Work So far, methods used to build an open domain dialogue system can be divided into two categories. The first category utilize an encoderdecoder framework to learn response generation models. Since the basic sequence-to-sequence models (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al"
P19-1370,P15-1152,0,0.15213,"Missing"
P19-1370,N10-1116,0,0.0290891,"both the Douban data and the E-commerce data with SMN and DAM which achieves state-of-theart performance on benchmarks. Moreover, improvement to SMN on the Douban data from coteaching is bigger than that from weak supervision, when the ratio of the positive and the negative is 1:1 in training7 . Our work, in a broad sense, belongs to the effort on learning with noisy data. Previous studies including curriculum learning (CL) (Bengio et al., 2009) and self-paced learning (SPL) (Jiang et al., 2014, 2015) tackle the problem with heuristics, such as ordering data from easy instances to hard ones (Spitkovsky et al., 2010; Tsvetkov et al., 2016) and retaining training instances whose losses are smaller than a threshold (Jiang et al., 2015). Recently, Fan et al. (2018) propose a deep reinforcement learning framework in which a simple deep neural network is used to adaptively select and filter important data instances from the training data. Jiang et al. (2017) propose a MentorNet which learns a data-driven curriculum with a Student-Net to mitigate overfitting on corrupted labels. In parallel to curriculum learning, several studies explore sample weighting schemes where training samples are re-weighted according"
P19-1370,D19-1011,0,0.279741,"selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing models with a better approach. Probably the most related work is the weakly supervised learning approach proposed in Wu et al. (2018b). However, there is stark difference between our approach and the weak supervision approach: (1) weak supervision employs a static generative model to teach a discriminative model, while co-teaching dynamically lets two discriminative models teach each other and evolve together; (2) weak supervision needs pretraining a generative model with ex"
P19-1370,P16-1013,0,0.0575427,"Missing"
P19-1370,D13-1096,0,0.199215,"dels (Vinyals and Le, 2015; Shang et al., 2015; Tao et al., 2018) tend to generate generic responses, extensions have been made to incorporate external knowledge into generation (Mou et al., 2016; Xing et al., 2017), and to generate responses with specific personas or emotions (Li et al., 2016; Zhang et al., 2018a; Zhou et al., 2018a). The second category design a discriminative model to measure the matching degree between a human input and a response candidate for response selection. At the beginning, research along this line assumes that the human input is a single message (Lu and Li, 2013; Wang et al., 2013; Hu et al., 2014; Wang et al., 2015). Recently, researchers begin to make use of conversation history in matching. Representative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing"
P19-1370,P18-2067,1,0.90322,"epresentative methods include the dual LSTM model (Lowe et al., 2015), the deep learning to respond architecture (Yan et al., 2016), the multi-view matching model (Zhou et al., 2016), the sequential matching network (Wu et al., 2017, 2018c), the deep attention matching network (Zhou et al., 2018b), and the multi-representation fusion network (Tao et al., 2019). Our work belongs to the second group. Rather than crafting a new model, we are interested in how to learn the existing models with a better approach. Probably the most related work is the weakly supervised learning approach proposed in Wu et al. (2018b). However, there is stark difference between our approach and the weak supervision approach: (1) weak supervision employs a static generative model to teach a discriminative model, while co-teaching dynamically lets two discriminative models teach each other and evolve together; (2) weak supervision needs pretraining a generative model with extra resources and pre-building an index for training data construction, while co-teaching does not have such request; and (3) in terms of multi-turn response selection, weak supervision is only tested on the Douban data with SMN and the multi-view match"
P19-1370,P17-1046,1,0.308055,"o their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017). A key problem in response selection is how to measure the matching degree between a conversation context (a message with several turns of conversation history) and a response candidate. Existing studies have paid tremendous effort to build a matching model with neural architectures (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b), and advanced models such as the deep attention matching network (DAM) (Zhou et al., 2018b) have achieved impressive performance on benchmarks. In contrary to the progress on model architectures, there is little exploration on learning approaches of the models. On the one hand, neural matching models are becoming more and more complicated; on the other hand, all models are simply learned by distinguishing human responses from some automatically constructed negative response candidates (e.g., by random sampling). Although this heuristic approach can avoid expensive and exh"
P19-1370,P18-1205,0,0.0536553,"Missing"
P19-1370,C18-1317,0,0.375056,"d negative responses are randomly sampled. The ratio of the positive and the negative is 1:1 in training and validation. In the test set, each context has 10 response candidates retrieved from an index whose appropriateness regarding to the context is judged by human annotators. The average number of positive responses per context is 1.18. Following Wu et al. (2017), we employ R10 @1, R10 @2, R10 @5, mean average precision (MAP), mean reciprocal rank (MRR), and precision at position 1 (P@1) as evaluation metrics. In addition to the Douban data, we also choose E-commerce Dialogue Corpus (ECD) (Zhang et al., 2018b) as an experimental data set. The data consists of real-world conversations between customers and customer service staff in Taobao4 , which is the largest e-commerce platform in China. There are 1 million context-response pairs in the training set, and 10 thousand pairs in both the validation set and the test set. Each context in the training set and the validation set corresponds to one positive response candidate and one negative response candidate, while in the test set, the number of response candidates per context is 10 with only one of them positive. In the released data, human respons"
P19-1370,D16-1036,1,0.925812,"Missing"
P19-1370,P18-1103,0,0.245472,"roach can generally and significantly improve the performance of existing matching models. 1 Introduction Human-machine conversation is a long-standing goal of artificial intelligence. Recently, building a dialogue system for open domain human-machine conversation is attracting more and more attention due to both availability of large-scale human conversation data and powerful models learned with neural networks. Existing methods are either retrieval-based or generation-based. Retrievalbased methods reply to a human input by selecting a proper response from a pre-built index (Ji et al., 2014; Zhou et al., 2018b; Yan and Zhao, 2018), while generation-based methods synthesize a response with a natural language model (Shang et al., 2015; Serban et al., 2017). In this ∗ † Equal Contribution. Corresponding author: Rui Yan (ruiyan@pku.edu.cn). work, we study the problem of response selection for retrieval-based dialogue systems, since retrieval-based systems are often superior to their generation-based counterparts on response fluency and diversity, are easy to evaluate, and have powered some real products such as the social bot XiaoIce from Microsoft (Shum et al., 2018), and the E-commerce assistant Ali"
P19-1372,K16-1002,0,0.458299,"ate various responses. Xing et al. (2017) incorporate topic information to generate informative responses. However, these models suffer from the deterministic structure when generating multiple diverse responses. Besides, during the training of these models, response utterances are only used in the loss function and ignored when forward computing, which can confuse the model for pursuing multiple objectives simultaneously. A few works explore to change the deterministic structure of sequence-to-sequence models by introducing stochastic latent variables. VAE is one of the most popular methods (Bowman et al., 2016; Zhao et al., 2017; Serban et al., 2017; Cao and Clark, 2017), where the discourse-level diversity is modeled by a Gaussian distribution. However, it is observed that in the CVAE with a fixed Gaussian prior, the learned conditional posteriors tend to collapse to a single mode, resulting in a relatively simple scope (Wang et al., 2017). To tackle this, WAE (Gu et al., 2018) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD (Du et al., 2018) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed. Alth"
P19-1372,E17-2029,0,0.0183995,"nformation to generate informative responses. However, these models suffer from the deterministic structure when generating multiple diverse responses. Besides, during the training of these models, response utterances are only used in the loss function and ignored when forward computing, which can confuse the model for pursuing multiple objectives simultaneously. A few works explore to change the deterministic structure of sequence-to-sequence models by introducing stochastic latent variables. VAE is one of the most popular methods (Bowman et al., 2016; Zhao et al., 2017; Serban et al., 2017; Cao and Clark, 2017), where the discourse-level diversity is modeled by a Gaussian distribution. However, it is observed that in the CVAE with a fixed Gaussian prior, the learned conditional posteriors tend to collapse to a single mode, resulting in a relatively simple scope (Wang et al., 2017). To tackle this, WAE (Gu et al., 2018) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD (Du et al., 2018) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed. Although these models overcome the deterministic structure of sequ"
P19-1372,D18-1354,0,0.0257911,"ructure of sequence-to-sequence models by introducing stochastic latent variables. VAE is one of the most popular methods (Bowman et al., 2016; Zhao et al., 2017; Serban et al., 2017; Cao and Clark, 2017), where the discourse-level diversity is modeled by a Gaussian distribution. However, it is observed that in the CVAE with a fixed Gaussian prior, the learned conditional posteriors tend to collapse to a single mode, resulting in a relatively simple scope (Wang et al., 2017). To tackle this, WAE (Gu et al., 2018) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD (Du et al., 2018) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed. Although these models overcome the deterministic structure of sequence-to-sequence model, they still ignore the correlation of multiple valid responses and each case is trained separately. To consider the multiple responses jointly, the maximum likelihood strategy is explored. Zhang et al. (2018a) propose the maximum generated likelihood criteria which model a query with its multiple responses as a bag of instances and proposes to optimize the model towards the most likely"
P19-1372,N16-1014,0,0.827037,"open-domain dialogue generation has become a research hotspot in Natural Language Processing due to its broad application prospect, including chatbots, virtual personal assistants, etc. Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic (Xing et al., 2017), persona modeling (Zhang et al., 2018b) and emotion controlling (Zhou et al., 2018b), most of these recent approaches are primarily built upon the sequence-to-sequence architecture (Cho et al., 2014; Shang et al., 2015) which suffers from the “safe” response problem (Li et al., 2016a; Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) n mapping of dialogue that multiple possible responses can correspond to the same query. To deal with the generic response problem, various methods have been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., 2017, 2018a), Variational Autoencoders (VAEs) based models (Zhao et al., 2017; Serban et al"
P19-1372,D14-1162,0,0.0822003,"the recognition networks if other complementary responses can be predicted from the distinctive variable z. Besides, since the probability of the complementary term may approach zero which makes it difficult to optimize, we actually adopt its lower bound in practice: log(1 − p(ybow |x, z)) = log(1 − |y| Y efyt P|V |f ) j t=1 j e |y| Y efyt ≥ log( (1 − P|V | )) fj e t=1 j (11) where |V |is vocabulary size. Totally, the whole loss for the step-two generation is then: Our whole model can be trained in an end-to-end fashion. To train the model, we first pre-train the word embedding using Glove ((Pennington et al., 2014))1 . Then modules of the model are jointly trained by optimizing the losses Lf irst and Lsecond of the two generation phases respectively. To overcome the vanishing latent variable problem (Wang et al., 2017) of CVAE, we adopt the KL annealing strategy (Bowman et al., 2016), where the weight of the KL term is gradually increased during training. The other technique employed is the MBOW loss which is able to sharpen the distribution of latent variable z for each specific response and alleviate the vanishing problem at the same time. During testing, diverse responses can be obtained by the two g"
P19-1372,D18-1418,0,0.156419,"e been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., 2017, 2018a), Variational Autoencoders (VAEs) based models (Zhao et al., 2017; Serban et al., 2017), etc. However, these methods still view multiple responses as independent ones and fail to model multiple responses jointly. Recently, Zhang et al. (2018a) introduce a maximum likelihood strategy that given an input query, the most likely response is approximated rather than all possible responses, which is further implemented by Rajendran et al. (2018) with reinforcement learning for task-oriented dialogue. Although capable of generating the most likely response, these methods fail to model other possible responses and ignore the correlation of different responses. In this paper, we propose a novel response generation model for open-domain conversation, which learns to generate multiple diverse responses with multiple references by considering 3826 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3826–3835 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistic"
P19-1372,P17-3020,0,0.0121152,"gue generation has become a research hotspot in Natural Language Processing due to its broad application prospect, including chatbots, virtual personal assistants, etc. Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic (Xing et al., 2017), persona modeling (Zhang et al., 2018b) and emotion controlling (Zhou et al., 2018b), most of these recent approaches are primarily built upon the sequence-to-sequence architecture (Cho et al., 2014; Shang et al., 2015) which suffers from the “safe” response problem (Li et al., 2016a; Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) n mapping of dialogue that multiple possible responses can correspond to the same query. To deal with the generic response problem, various methods have been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., 2017, 2018a), Variational Autoencoders (VAEs) based models (Zhao et al., 2017; Serban et al., 2017), etc. Howeve"
P19-1372,P15-1152,0,0.214146,"Missing"
P19-1372,N15-1020,0,0.101772,"Missing"
P19-1372,D17-1233,1,0.875255,", 2017) for diverse response generation. CVAE: the vanilla CVAE model (Zhao et al., 2017) with and without BOW (bag-of-word) loss (CVAE+BOW and CVAE). WAE: the conditional Wasserstein autoencoder model for dialogue generation (Gu et al., 2018) which models the distribution of data by training a GAN within the latent variable space. Ours: we explore our model Ours and conduct 4.3 Evaluation Metrics To comprehensively evaluate the quality of generated response utterances, we adopt both automatic and human evaluation metrics: BLEU: In dialogue generation, BLEU is widely used in previous studies (Yao et al., 2017; Shang et al., 2018). Since multiple valid responses exist in this paper, we adopt multi-reference BLEU where the evaluated utterance is compared to provided multiple references simultaneously. Distinctness: To distinguish safe and commonplace responses, the distinctness score (Li et al., 2016a) is designed to measure word-level diversity by counting the ratio of distinctive [1,2]-grams. In our experiments, we adopt both Intra-Dist: the distinctness scores of multiple responses for a given query and Inter-Dist: the distinctness scores of generated responses of the whole testing set. Embedding"
P19-1372,P18-1137,0,0.159121,"rent from the conventional methods (shown in green color) which model each response from scratch every time, our method first builds a common feature of multiple responses and models each response based on it afterward. Introduction In recent years, open-domain dialogue generation has become a research hotspot in Natural Language Processing due to its broad application prospect, including chatbots, virtual personal assistants, etc. Though plenty of systems have been proposed to improve the quality of generated responses from various aspects such as topic (Xing et al., 2017), persona modeling (Zhang et al., 2018b) and emotion controlling (Zhou et al., 2018b), most of these recent approaches are primarily built upon the sequence-to-sequence architecture (Cho et al., 2014; Shang et al., 2015) which suffers from the “safe” response problem (Li et al., 2016a; Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) n mapping of dialogue that multiple possible responses can correspond to the same query. To deal with the generic response problem, various methods have been propos"
P19-1372,P18-1205,0,0.105266,"Missing"
P19-1372,P17-1061,0,0.436588,"sponse problem (Li et al., 2016a; Sato et al., 2017). This can be ascribed to modeling the response generation process as 1to-1 mapping, which ignores the nature of 1-to∗ Corresponding author: Rui Yan (ruiyan@pku.edu.cn) n mapping of dialogue that multiple possible responses can correspond to the same query. To deal with the generic response problem, various methods have been proposed, including diversity-promoting objective function (Li et al., 2016a), enhanced beam search (Shao et al., 2016), latent dialogue mechanism (Zhou et al., 2017, 2018a), Variational Autoencoders (VAEs) based models (Zhao et al., 2017; Serban et al., 2017), etc. However, these methods still view multiple responses as independent ones and fail to model multiple responses jointly. Recently, Zhang et al. (2018a) introduce a maximum likelihood strategy that given an input query, the most likely response is approximated rather than all possible responses, which is further implemented by Rajendran et al. (2018) with reinforcement learning for task-oriented dialogue. Although capable of generating the most likely response, these methods fail to model other possible responses and ignore the correlation of different responses. In t"
