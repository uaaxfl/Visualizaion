2020.eval4nlp-1.11,C18-1012,0,0.0273184,"probability assigned to its reversal. There has nevertheless been some scepticism expressed about the ensuing euphoria among computer scientists — mainly by linguists. Sprouse et al. (2015) notes that the trigram model from Lau et al. (2015) assigns different rankings to 10 different permutations of CGISF, depending on the training corpus (e.g., the Wall Street Journal corpus versus an example training corpus taken from Lau et al. (2015)). Can the scores assigned to these sequences be reliably construed as a regression scale of grammaticality (or perhaps acceptability), if they are so fickle? Chowdhury and Zamparelli (2018) also express concern about the ability of neural language models to generalize to more abstract grammatical phenomena than subject-verb agreement. What we will present in this section is a more thorough appraisal of how well statistical language models perform as instruments of grammaticality testing overall, using PBCs. Previous research on grammaticality/acceptability and language models has mainly designed experiments using naturally occurring English sentences, and modifies those sentences based on various individual linguistic phenomena to manually introduce a specific source of ungramma"
2020.eval4nlp-1.11,W13-2604,0,0.0177906,"(2003) textbook. However that corpus does not include annotation indicating the minimal set structure, and therefore was ignored in this study. 5.3 Language Models We investigated four different types of language models: Pereira’s (2000) original aggregative Markov model (Saul and Pereira, 1997), Mikolov’s (2012) original RNN language model (Mikolov, 2012), a QRNN-based language model (Merity et al., 2018) that we take to be representative of contemporary models, and GPT-2 (Radford et al., 2019) as the representative of large-scale pre-trained language models. Mikolov’s model is also used by Clark et al. (2013); Lau et al. (2015); Sprouse et al. (2018) in their research about gradient acceptability. We chose GPT-2 over other large-scale pre-trained models such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), because it took the more orthodox autoregressive language modelling approach that is consistent with the remaining choices, and it is most commonly used for natural language generation for the same reason. We obtained a publicly available implementation of each of the four language models2 . The implementation of the tied adaptive softmax (TAS) method3 used the unusual approach of ap"
2020.eval4nlp-1.11,N19-1423,0,0.0167496,"four different types of language models: Pereira’s (2000) original aggregative Markov model (Saul and Pereira, 1997), Mikolov’s (2012) original RNN language model (Mikolov, 2012), a QRNN-based language model (Merity et al., 2018) that we take to be representative of contemporary models, and GPT-2 (Radford et al., 2019) as the representative of large-scale pre-trained language models. Mikolov’s model is also used by Clark et al. (2013); Lau et al. (2015); Sprouse et al. (2018) in their research about gradient acceptability. We chose GPT-2 over other large-scale pre-trained models such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), because it took the more orthodox autoregressive language modelling approach that is consistent with the remaining choices, and it is most commonly used for natural language generation for the same reason. We obtained a publicly available implementation of each of the four language models2 . The implementation of the tied adaptive softmax (TAS) method3 used the unusual approach of applying softmax on already softmaxed values. For this reason, we also experiment on QRNN models trained using regular cross-entropy loss functions. All three non-pretrained models are"
2020.eval4nlp-1.11,2020.scil-1.39,0,0.256205,"ll advocate for in this paper, the point biserial correlation, in fact compares a discrete variable (for us, acceptability judgements) to a continuous variable (for us, neural language model probabilities). The only previous work in this area to choose the PBC that we are aware of is Sprouse et al. (2018), and that paper actually applied it backwards (with some justification) so that the language model probability was treated as the discrete binary variable by setting a threshold. With the PBC in mind, we will first reappraise some recent work in syntactically targeted linguistic evaluations (Hu et al., 2020), arguing that while their experimental design sets a new high watermark for this topic, their results may not prove what they have claimed. We then turn to the task-independent assessment of language models as grammaticality classifiers. Prior to the introduction of the GLUE leaderboard, the vast majority of this assessment was essentially anecdotal, and we find the use of the MCC in this regard to be problematic. We conduct several studies with PBCs to compare several popular language models. We also study the effects of several variables such as normalization and data homogeneity on PBC. 1"
2020.eval4nlp-1.11,P15-1156,0,0.0907234,"draw a contrast between two nonsensical sequences, only one of which (CGISF) is grammatical. Pereira (2000) provides an attempt at a refutation by constructing a statistical language model based upon an agglomerative Markov process (Saul and Pereira, 1997), and then observing that CGISF is assigned a probability by the model which is roughly 200 000 times greater than the probability assigned to its reversal. There has nevertheless been some scepticism expressed about the ensuing euphoria among computer scientists — mainly by linguists. Sprouse et al. (2015) notes that the trigram model from Lau et al. (2015) assigns different rankings to 10 different permutations of CGISF, depending on the training corpus (e.g., the Wall Street Journal corpus versus an example training corpus taken from Lau et al. (2015)). Can the scores assigned to these sequences be reliably construed as a regression scale of grammaticality (or perhaps acceptability), if they are so fickle? Chowdhury and Zamparelli (2018) also express concern about the ability of neural language models to generalize to more abstract grammatical phenomena than subject-verb agreement. What we will present in this section is a more thorough apprai"
2020.eval4nlp-1.11,P19-1441,0,0.0123903,"gn modifications made by Hu et al. (2020) to targeted evaluations such as these, the decision to continue using accuracy and to generalize it in this way seems not to be working well. Figure 2: Surprisals (negative log probabilities), Matthews correlations and point-biserial correlations for set 42 in Hu et al.’s (2020) experiment 1b for the GRNN model with the pronoun, himself. 4 3.3 Matthews Correlation Coefficients This is potentially a much more pervasive problem than just with Hu et al.’s (2020) experiments. MCCs have emerged as a popular alternative among language modelling enthusiasts (Liu et al., 2019; Lan et al., 2020; Raffel et al., 2019) since grammaticality classification with The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) was incorporated into the GLUE standard (Wang et al., 2018). Warstadt et al. (2019) themselves began using MCCs, initially to validate the CoLA corpus, but also to interpret Lau et al.’s (2017) gradient models. MCCs cannot be computed directly on continuous data, which means not only that they are insensitive to the magnitudes of probabilities, but also that a threshold must be set in order to impose a discrete boundary between classes. Defendi"
2020.eval4nlp-1.11,D18-1151,0,0.138555,"probabilities might serve as grammaticality regression scores. Using evidence from targeted linguistic evaluations, we argue for the point-biserial correlation as at least the basis of 110 Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP), pages 110–119, c November 20, 2020. 2020 Association for Computational Linguistics such a task-independent measure, and then use the PBC to examine several neural models along with some important variables that affect both their evaluation and the data that we evaluate on. Borrowing a convention from linguistic theory, Marvin and Linzen (2018) coined the use of “minimal pairs” as input to language models in order to test these fine-grained variations. For example: (1) Reflexive pronoun in a sentential complement: a. The bankers thought the pilot embarrassed himself. b. *The bankers thought the pilot embarrassed themselves. (2) Reflexive pronoun across an object relative clause: a. The manager that the architects like doubted himself. b. *The manager that the architects like doubted themselves. These pairs deal with referential agreement in specific syntactic environments. If a model assigns the grammatical string in a pair a higher"
2020.eval4nlp-1.11,P12-1101,0,0.034367,".0224 0.0297 0.0134 0.0162 0.0047 0.0278 0.0421 0.0332 0.0112 GPT-2 XL 0.1476 0.0123 0.2241 0.1592 0.2729 0.0872 Table 2: CoLA Point-biserial Test Results Model Norm. QRNN TAS QRNN regular Raw Norm Raw Norm Raw Norm Raw Norm GPT-2 Models Raw Norm Pereira Mikolov Experimental Design Our experiments consider two types of probabilities: the log probability ` = log p(s), and the actual probability, e` , where s is a sentence. For each type of probability, we also consider two to three different types of normalization methods: no normalization (raw), normalization by length (norm) `/|s|, and SLOR (Pauls and Klein, 2012) (` − `u )/|s|, where |s |is the length of the sentence and `u is the log unigram probability of the sentence. For all three non-pretrained models, the unigram probability was obtained from BNC/WT103 with add-one smoothing. We used WT103 unigram probabilities for GPT-2 models since they preserve case. Norm. BNC LOG EXP 0.0231 -0.0136 0.0758 0.0595 0.0841 0.1868 0.2541 0.2465 -0.0066 -0.2197 0.068 0.0726 -0.0135 -0.059 0.042 0.0245 GPT-2 0.4671 0.26 0.5233 0.487 WT103 LOG EXP -0.0027 -0.0488 0.038 0.0412 0.1278 0.1914 0.1955 0.2043 0.0201 0.0186 0.1058 0.0764 0.0232 0.1251 0.1057 0.104 GPT-2 XL"
2020.eval4nlp-1.11,W97-0309,0,0.636279,"mmatical (CGISF) or ungrammatical. The example was presented briefly and informally in order to reject statistical language modelling as an alternative approach to the one advocated and developed in greater detail by Chomsky (1957). It was only presented with one other example, the reverse of the sentence, i.e., “Furiously sleep ideas green colorless”, in order to draw a contrast between two nonsensical sequences, only one of which (CGISF) is grammatical. Pereira (2000) provides an attempt at a refutation by constructing a statistical language model based upon an agglomerative Markov process (Saul and Pereira, 1997), and then observing that CGISF is assigned a probability by the model which is roughly 200 000 times greater than the probability assigned to its reversal. There has nevertheless been some scepticism expressed about the ensuing euphoria among computer scientists — mainly by linguists. Sprouse et al. (2015) notes that the trigram model from Lau et al. (2015) assigns different rankings to 10 different permutations of CGISF, depending on the training corpus (e.g., the Wall Street Journal corpus versus an example training corpus taken from Lau et al. (2015)). Can the scores assigned to these sequ"
2020.eval4nlp-1.11,W18-5446,0,0.0605472,"Missing"
2020.eval4nlp-1.11,Q19-1040,0,0.24526,"ammaticality and Language Modelling Jingcheng Niu and Gerald Penn Department of Computer Science University of Toronto Toronto, Canada {niu,gpenn}@cs.toronto.edu Abstract Ever since Pereira (2000) provided evidence against Chomsky’s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regards statistical language models as “psycholinguistic subjects” and probes their ability to acquire syntactic knowledge. The advent of The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) has earned a spot on the leaderboard for acceptability judgements, and the polemic between Lau et al. (2017) and Sprouse et al. (2018) has raised fundamental questions about the nature of grammaticality and how acceptability judgements should be elicited. All the while, we are told that neural language models continue to improve. That is not an easy claim to test at present, however, because there is almost no agreement on how to measure their improvement when it comes to grammaticality and acceptability judgements. The GLUE leaderboard bundles CoLA together with a Matthews correlation coeffi"
2020.lrec-1.271,S15-2136,0,0.0172344,"epidemics. Meanwhile, fine-grained information extraction is a wellresearched area in the information extraction community, especially for temporal information extraction (TIE), which is the primary focus of this work. Several data annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the development of TIE systems for the public health domain. The standard and the corpus are aimed at three primary use cases but should be comprehensive enough for future 1 In the rest of the paper, we are going to use TimeML to refer to the collection of TimeML-based"
2020.lrec-1.271,S16-1165,0,0.0144628,"fine-grained information extraction is a wellresearched area in the information extraction community, especially for temporal information extraction (TIE), which is the primary focus of this work. Several data annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the development of TIE systems for the public health domain. The standard and the corpus are aimed at three primary use cases but should be comprehensive enough for future 1 In the rest of the paper, we are going to use TimeML to refer to the collection of TimeML-based general domain annotati"
2020.lrec-1.271,S13-2002,0,0.056863,"Missing"
2020.lrec-1.271,S13-2012,0,0.0194173,"IE), which is the primary focus of this work. Several data annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the development of TIE systems for the public health domain. The standard and the corpus are aimed at three primary use cases but should be comprehensive enough for future 1 In the rest of the paper, we are going to use TimeML to refer to the collection of TimeML-based general domain annotation standards. applications. The first immediate use case would be to extract and infer precise temporal information about public health events,"
2020.lrec-1.271,chang-manning-2012-sutime,0,0.0332451,"information extraction (TIE), which is the primary focus of this work. Several data annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the development of TIE systems for the public health domain. The standard and the corpus are aimed at three primary use cases but should be comprehensive enough for future 1 In the rest of the paper, we are going to use TimeML to refer to the collection of TimeML-based general domain annotation standards. applications. The first immediate use case would be to extract and infer precise temporal information about public"
2020.lrec-1.271,S16-1201,0,0.0158901,"e primary focus of this work. Several data annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the development of TIE systems for the public health domain. The standard and the corpus are aimed at three primary use cases but should be comprehensive enough for future 1 In the rest of the paper, we are going to use TimeML to refer to the collection of TimeML-based general domain annotation standards. applications. The first immediate use case would be to extract and infer precise temporal information about public health events, such as infectious"
2020.lrec-1.271,W19-1908,0,0.0168017,"annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the development of TIE systems for the public health domain. The standard and the corpus are aimed at three primary use cases but should be comprehensive enough for future 1 In the rest of the paper, we are going to use TimeML to refer to the collection of TimeML-based general domain annotation standards. applications. The first immediate use case would be to extract and infer precise temporal information about public health events, such as infectious disease outbreaks, for EBS. Second, THEE a"
2020.lrec-1.271,W11-0419,0,0.220447,"L Although THYME-TimeML (Styler et al., 2014b) says that its definition of events is identical to that of ISO-TimeML, the THYME corpus does drop events from its annotation both for the sake of “expediency and ease of annotation,” and because of “the needs of the clinical domain.” By contrast, we drop a greater percentage of events from our own corpus relative to what ISO-TimeML would have annotated, but are guided by the domain-specific taxonomy in Section 3.1. in determining when we do so. There are other important deviations from ISO-TimeML than the definitions of events, too. As advised by Pustejovsky and Stubbs (2011), both THYME and THEE use a system of narrative containers that had not been introduced into the original ISO-TimeML standard. It greatly expands upon the more widespread practice of temporally relating events as occurring before, overlapping with or after the document’s creation time, and includes the possibility of nesting containers, and anchoring containers to events instead of the D OC T IME R EL attribute, even in the absence of TIMEX3s. See Section 3.2.4. and also Styler et al. (2014a), Section 5. Both THYME and THEE also retain ALINKs (aspectual links, e.g., X initiates Y. On the other"
2020.lrec-1.271,pustejovsky-etal-2010-iso,0,0.0406835,"ime, to estimate the occurrence time of the events extracted. This compromise often leads to inaccuracies or even errors when determining the temporal information of individual events, because of the complicated layering of temporal references used in reportage about epidemics. Meanwhile, fine-grained information extraction is a wellresearched area in the information extraction community, especially for temporal information extraction (TIE), which is the primary focus of this work. Several data annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the dev"
2020.lrec-1.271,P79-1022,0,0.125683,"Missing"
2020.lrec-1.271,E12-2021,0,0.133946,"Missing"
2020.lrec-1.271,S13-2003,0,0.0297456,"Missing"
2020.lrec-1.271,P11-2061,0,0.0298815,"n F1-score7 for entity identification agreement and Krippendorff’s Alpha (Krippendorff, 2018) for classification agreement evaluation. Table 4 shows the IAA of entity annotation, and table 5 shows the IAA of even-attribute annotation. For link annotation IAA, we calculate the F1 score and Krippendorff’s Alpha on links connecting entities that are 7 This should properly be viewed as the harmonic mean of two opposite-facing recall scores, but the computation is the same as an asymmetric F1-score, in which one annotator’s work is viewed as the true annotation. The practice of using F1 began with UzZaman and Allen (2011), who were justified in calling it this because they were evaluating system outputs against humanannotated gold standards. 5.3. Baseline Algorithm 6. Case Counting Example Infection case counting is one of the quintessential tasks of epidemic monitoring EBS systems, and a likely beneficiary of having a good TIE system. Without any available TIE system for the public health domain, research conducted using case counting data must assume that all cases occurred at the publication time of the article. We labelled all of the case events in the test set articles. We found that the article publicati"
2020.lrec-1.271,S13-2001,0,0.0292586,"ed in reportage about epidemics. Meanwhile, fine-grained information extraction is a wellresearched area in the information extraction community, especially for temporal information extraction (TIE), which is the primary focus of this work. Several data annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the development of TIE systems for the public health domain. The standard and the corpus are aimed at three primary use cases but should be comprehensive enough for future 1 In the rest of the paper, we are going to use TimeML to refer to the colle"
2020.lrec-1.271,S07-1014,0,0.0811083,"complicated layering of temporal references used in reportage about epidemics. Meanwhile, fine-grained information extraction is a wellresearched area in the information extraction community, especially for temporal information extraction (TIE), which is the primary focus of this work. Several data annotation standards have been developed, including TimeML1 (Pustejovsky et al., 2005), ISO-TimeML (Pustejovsky et al., 2010), which is based upon it, and the clinical, domainspecific THYME standard (Styler et al., 2014b). Based on those annotation standards, SemEval has held multiple shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013; Bethard et al., 2015; Bethard et al., 2016). An abundance of TIE systems have been developed for the shared tasks, reaching near-human performance (Chang and Manning, 2012; Chambers, 2013; Lee et al., 2016; Str¨otgen et al., 2013; Lin et al., 2019). We created an annotation standard (THEE-TimeML) and a corpus (TheeBank) based upon it to facilitate the development of TIE systems for the public health domain. The standard and the corpus are aimed at three primary use cases but should be comprehensive enough for future 1 In the rest of the paper, we"
2020.lrec-1.815,cucchiarini-etal-2008-recording,0,0.114565,"Missing"
2020.lrec-1.815,W12-6208,0,0.0117023,"nguage pairs. The French orthographic transcriptions were quick to transcribe, are easily bootstrapped into phonemes (described below), and do not apply the same degree of rigour. Word-level segmentations were derived from these transcriptions. Very noisy recordings were not segmented. Initial segmentations were generated by force-aligning transcripts to speech using an off-the-shelf speech recognizer (Deléglise et al., 2005). The pronunciation lexicon was augmented with alternative pronunciations derived from feeding the orthographic transcriptions above into a graphemeto-phoneme transducer (Novak et al., 2012). Those segments were then manually adjusted by the first author. 3.4. Minimal Feedback Role-Play Partition Data from the Minimal Feedback Role-Play Partition, stored in FAB under the subdirectory min, were collected during the first experiment (Robertson et al., 2016). Table 2 provides demographic and recording information for this partition3 . For discrete-valued entries, bracketed values indicate the count of that unique response. Only 3 http://trans.sourceforge.net, version 1.22, last accessed November 19, 2019. 6616 This information supersedes that originally reported in (Robertson et al."
2020.lrec-1.815,sanders-etal-2014-dutch,0,0.0851384,"ordings from traditional classrooms, since the quality and flexibility of a teacher’s feedback far exceeds that of a CAPT system’s. Of the corpora that collect the spontaneous speech of learners, to the best of our knowledge, none reflects the feedback of an online CAPT system. Cucchiarini et al. (2008) recorded non-native speakers of Dutch communicating with a Wizard-of-Oz spoken dialogue system, but not in a language-learning context. Likewise, Jurafsky et al. (1994) collected a small corpus containing non-native English speech from communications with a Wizard-of-Oz spoken dialogue system. Sanders et al. (2014) produced a non-native corpus of Dutch speech with some task-based activities, but again without a CAPT intervention. FAB consists of data that reflect a realistic classroom CAPT intervention. During a paired role-playing task, participants must ask a CAPT system for feedback after every utterance. Each utterance is strongly impacted by prior feedback from the CAPT system. As such, FAB provides a better inventory of the types of utterances that a CAPT system would be exposed to in the wild. 2.2. Teacher-Driven Labels Just as it is important that the data are collected in realistic scenarios, i"
2020.lrec-1.815,L16-1212,0,0.0219889,"who will most clearly benefit from CAPT interventions are those with little to no experience speaking the target language. We suspect that this group is often overlooked because no specialized instruction is necessary for the egregious mistakes made by learners at this stage. However, ignoring absolute beginner CAPT could lead to a lack of feedback in learner utterances (especially in self-driven learning), widening the gulf between written comprehension and production. There are a variety of non-native speech databases, such as PF_STAR (Batliner et al., 2005), CorAIt (Combei, 2018), IFCASL (Trouvain et al., 2016), JASMIN-CGN (Cucchiarini et al., 2008), and LLESLA (Sanders et al., 2014), that offer beginner learner utterances. In all of these databases, however, the learners have had some non-negligible prior experience in the target language. The CBFC (Yoo and Kim, 2018) contains data from a speaker with just a month’s experience - but only one speaker. The Young Learners Corpus (Myles, 2012) does feature absolute beginners, but since the learners were all very young and not as subject to phonetic fossilization as adult learners, it is unlikely to match the demographics of a second-language CAPT syste"
2020.lrec-1.815,L18-1054,0,0.0208194,"his stage. However, ignoring absolute beginner CAPT could lead to a lack of feedback in learner utterances (especially in self-driven learning), widening the gulf between written comprehension and production. There are a variety of non-native speech databases, such as PF_STAR (Batliner et al., 2005), CorAIt (Combei, 2018), IFCASL (Trouvain et al., 2016), JASMIN-CGN (Cucchiarini et al., 2008), and LLESLA (Sanders et al., 2014), that offer beginner learner utterances. In all of these databases, however, the learners have had some non-negligible prior experience in the target language. The CBFC (Yoo and Kim, 2018) contains data from a speaker with just a month’s experience - but only one speaker. The Young Learners Corpus (Myles, 2012) does feature absolute beginners, but since the learners were all very young and not as subject to phonetic fossilization as adult learners, it is unlikely to match the demographics of a second-language CAPT system. While suitable corpora may exist for absolute beginners in educator-driven CAPT research, they are difficult to find, may suffer annotation problems, or are not public (O’Brien et al., 2018). To the best of our knowledge, FAB is the first corpus of absolute be"
2020.repl4nlp-1.23,C08-1008,0,0.0441583,"e consider other grammars, (Kogkalidis et al., 2019) presented a type-logical grammar for Dutch and a supertagging approach that relied on primitive units. While their approach yielded improvement in word accuracy, the overall accuracy was substantially lower than with CCG supertagging; furthermore, the grammar’s type system was so different that it is difficult to draw conclusions about applicability to other grammars.4 In order to see some consideration of the composed structure of CCG lexical categories, we must alter our task scope somewhat. Garrette et al. (2014), following earlier work (Baldridge, 2008), applied a Bayesian model with grammar-informed priors for supertagging where only a tag dictionary and raw, unlabelled text was made available. Their model included a generative model for categories as well as the notion of combinability, preferring tag sequences where adjacent words could be combined via CCG rules. Similarly, work in CCG grammar induction has involved some basic consideration of how CCG categories are constructed so that that a grammar could be built using EM (Bisk and Hockenmaier, 2012) or hierarchical Dirichlet processes (Bisk and Hockenmaier, 2013). Despite these applica"
2020.repl4nlp-1.23,J99-2004,0,0.469848,"om the same units using the same rules, suggesting that OOV categories can, in principle, be treated in a concordant manner. Twelve of which are for punctuation. 195 2 They provide relative pronouns in pied-piping constructions and verbs which take expletive subjects as examples; we found lengthy adjunction chains to contribute as well. 3 These proportions are even higher for out-of-domain data (Rimell and Clark, 2008). 3 Related work While our focus in this paper is on CCG supertagging, it is worth noting that the supertagging task originates in the context of LTAG (Joshi and Srinivas, 1994; Bangalore and Joshi, 1999), which also has highly complex lexical categories. Supertagging is important for efficient parsing in such grammars as it helps narrow the search space for the parse (Clark and Curran, 2004). Despite the complexity captured in supertags, the vast majority of existing approaches treat CCG lexical categories as atomic units for prediction, ignoring their varying complexities and structured nature. Effectively, at each time step of the input sentence, the model must decide which of a fixed set of CCG categories is the best choice. This category-classification approach is the same as for POS tagg"
2020.repl4nlp-1.23,Q13-1007,0,0.0282311,". (2014), following earlier work (Baldridge, 2008), applied a Bayesian model with grammar-informed priors for supertagging where only a tag dictionary and raw, unlabelled text was made available. Their model included a generative model for categories as well as the notion of combinability, preferring tag sequences where adjacent words could be combined via CCG rules. Similarly, work in CCG grammar induction has involved some basic consideration of how CCG categories are constructed so that that a grammar could be built using EM (Bisk and Hockenmaier, 2012) or hierarchical Dirichlet processes (Bisk and Hockenmaier, 2013). Despite these applications, consideration of CCG primitives has yet to make its way to supervised supertagging approaches; we aim to fill that gap. 196 4 Their grammar had 5700 unique types for a corpus of 65k sentences; categories were constructed from 30 atomic types, corresponding to POS tags or phrasal categories, and 22 nondirectional binary connectives, corresponding to dependency labels. 4 Method and model 4.2 Despite the potential advantages discussed above in Section 2, it is unclear a priori whether supertagging with primitive units is more or less difficult than standard, whole-ca"
2020.repl4nlp-1.23,D18-1217,0,0.290539,"TM but dropped all hand-specified features, removed the limit on the categories that the model could produce, used custom in-domain word embeddings, and included a language model– style LSTM over the output lexical categories that allowed the model to condition its predicted tag at time t on the previously-predicted lexical category at time t − 1. In addition to improving word accuracy, this latter addition drastically increased the number of tagged sentences that were parsable, even in variations that hurt word accuracy. The current state-of-the-art result in CCG supertagging was achieved by Clark et al. (2018). Their model consisted of a two-layer bidirectional LSTM with GloVe word embeddings (Pennington et al., 2014) supplemented by the output of a character-level convolutional neural network. Their approach involved training additional “auxiliary” prediction modules on top of the same LSTM on an additional, unlabelled corpus (Chelba et al., 2014). These auxiliary modules were given an incomplete view of the input (e.g., only words to the left) and trained to predict the same label that the primary prediction module predicted. If we consider other grammars, (Kogkalidis et al., 2019) presented a ty"
2020.repl4nlp-1.23,W02-2203,0,0.506128,"ncreases parser coverage and F1 , and is able to produce novel categories. Analysis shows a synergistic effect between this decomposed view and incorporation of prediction history. 1 Count Introduction Highly lexicalized grammars, such as lexicalized tree-adjoining grammar (LTAG) and combinatory categorial grammar (CCG), have very large sets of possible lexical categories. Where most phrasestructure and dependency grammars have lexical category sets numbering in the tens for English (Taylor et al., 2003), LTAG and CCG have sets numbering in the hundreds or thousands (Joshi and Srinivas, 1994; Clark, 2002). The large number of possible labels for each word can make the search space for the syntactic tree of the sentence intractably large; narrowing the set of viable lexical categories per word is therefore an important step in efficient parsing for such grammars (Clark and Curran, 2007; Lewis et al., 2016). As the tags are much more complex and informative than partof-speech (POS) tags, tagging the words with these more complex categories is called supertagging. The large number of lexical categories comes from the high degree of complexity that the categories can have. When grammars have small"
2020.repl4nlp-1.23,C04-1041,0,0.151082,"mplexities and structured nature. Effectively, at each time step of the input sentence, the model must decide which of a fixed set of CCG categories is the best choice. This category-classification approach is the same as for POS tagging, and indeed, existing supertagging models are very similar to (if not the same as) POS tagging models in structure. Early work for CCG supertagging relied on maximum-entropy models with hand-specified features, a limited set of possible categories, and tag dictionaries that tracked allowed categories for frequent words based on the training data (Clark, 2002; Clark and Curran, 2004, 2007). Recent work relies heavily on word embeddings: they allow better handling of out-of-vocabulary words and decrease reliance on part-of-speech tags, where imperfect accuracy can be a detriment for the supertagger. Lewis and Steedman (2014) used externally-trained embeddings (Turian et al., 2010) combined with suffix and capitalization features in a simple feed-forward neural network as well as a CRF; they also allowed words to be tagged with categories with which they did not co-occur in the training data. Xu et al. (2015) applied the same embeddings and features in a standard Elman RNN"
2020.repl4nlp-1.23,J07-4004,0,0.359623,"LTAG) and combinatory categorial grammar (CCG), have very large sets of possible lexical categories. Where most phrasestructure and dependency grammars have lexical category sets numbering in the tens for English (Taylor et al., 2003), LTAG and CCG have sets numbering in the hundreds or thousands (Joshi and Srinivas, 1994; Clark, 2002). The large number of possible labels for each word can make the search space for the syntactic tree of the sentence intractably large; narrowing the set of viable lexical categories per word is therefore an important step in efficient parsing for such grammars (Clark and Curran, 2007; Lewis et al., 2016). As the tags are much more complex and informative than partof-speech (POS) tags, tagging the words with these more complex categories is called supertagging. The large number of lexical categories comes from the high degree of complexity that the categories can have. When grammars have small tag sets, the bulk of the work in developing or learning a grammar comes from deciding how to combine the tags and their words. Categorial grammars instead have fewer combination rules, requiring the lexical categories to support much greater syntactic richness; see Figure 1 for some"
2020.repl4nlp-1.23,W14-1615,0,0.0184094,"at the primary prediction module predicted. If we consider other grammars, (Kogkalidis et al., 2019) presented a type-logical grammar for Dutch and a supertagging approach that relied on primitive units. While their approach yielded improvement in word accuracy, the overall accuracy was substantially lower than with CCG supertagging; furthermore, the grammar’s type system was so different that it is difficult to draw conclusions about applicability to other grammars.4 In order to see some consideration of the composed structure of CCG lexical categories, we must alter our task scope somewhat. Garrette et al. (2014), following earlier work (Baldridge, 2008), applied a Bayesian model with grammar-informed priors for supertagging where only a tag dictionary and raw, unlabelled text was made available. Their model included a generative model for categories as well as the notion of combinability, preferring tag sequences where adjacent words could be combined via CCG rules. Similarly, work in CCG grammar induction has involved some basic consideration of how CCG categories are constructed so that that a grammar could be built using EM (Bisk and Hockenmaier, 2012) or hierarchical Dirichlet processes (Bisk and"
2020.repl4nlp-1.23,J07-3004,0,0.119763,"Missing"
2020.repl4nlp-1.23,C94-1024,0,0.677422,"nglish CCG supertagging, increases parser coverage and F1 , and is able to produce novel categories. Analysis shows a synergistic effect between this decomposed view and incorporation of prediction history. 1 Count Introduction Highly lexicalized grammars, such as lexicalized tree-adjoining grammar (LTAG) and combinatory categorial grammar (CCG), have very large sets of possible lexical categories. Where most phrasestructure and dependency grammars have lexical category sets numbering in the tens for English (Taylor et al., 2003), LTAG and CCG have sets numbering in the hundreds or thousands (Joshi and Srinivas, 1994; Clark, 2002). The large number of possible labels for each word can make the search space for the syntactic tree of the sentence intractably large; narrowing the set of viable lexical categories per word is therefore an important step in efficient parsing for such grammars (Clark and Curran, 2007; Lewis et al., 2016). As the tags are much more complex and informative than partof-speech (POS) tags, tagging the words with these more complex categories is called supertagging. The large number of lexical categories comes from the high degree of complexity that the categories can have. When gramm"
2020.repl4nlp-1.23,W19-4314,0,0.427017,"ing was achieved by Clark et al. (2018). Their model consisted of a two-layer bidirectional LSTM with GloVe word embeddings (Pennington et al., 2014) supplemented by the output of a character-level convolutional neural network. Their approach involved training additional “auxiliary” prediction modules on top of the same LSTM on an additional, unlabelled corpus (Chelba et al., 2014). These auxiliary modules were given an incomplete view of the input (e.g., only words to the left) and trained to predict the same label that the primary prediction module predicted. If we consider other grammars, (Kogkalidis et al., 2019) presented a type-logical grammar for Dutch and a supertagging approach that relied on primitive units. While their approach yielded improvement in word accuracy, the overall accuracy was substantially lower than with CCG supertagging; furthermore, the grammar’s type system was so different that it is difficult to draw conclusions about applicability to other grammars.4 In order to see some consideration of the composed structure of CCG lexical categories, we must alter our task scope somewhat. Garrette et al. (2014), following earlier work (Baldridge, 2008), applied a Bayesian model with gram"
2020.repl4nlp-1.23,D14-1162,0,0.0821439,"uce, used custom in-domain word embeddings, and included a language model– style LSTM over the output lexical categories that allowed the model to condition its predicted tag at time t on the previously-predicted lexical category at time t − 1. In addition to improving word accuracy, this latter addition drastically increased the number of tagged sentences that were parsable, even in variations that hurt word accuracy. The current state-of-the-art result in CCG supertagging was achieved by Clark et al. (2018). Their model consisted of a two-layer bidirectional LSTM with GloVe word embeddings (Pennington et al., 2014) supplemented by the output of a character-level convolutional neural network. Their approach involved training additional “auxiliary” prediction modules on top of the same LSTM on an additional, unlabelled corpus (Chelba et al., 2014). These auxiliary modules were given an incomplete view of the input (e.g., only words to the left) and trained to predict the same label that the primary prediction module predicted. If we consider other grammars, (Kogkalidis et al., 2019) presented a type-logical grammar for Dutch and a supertagging approach that relied on primitive units. While their approach"
2020.repl4nlp-1.23,N18-1202,0,0.0324273,"we do not reset the model state between words. This enables the decoder to maintain a memory of the primitives (and, by extension, categories) previously predicted in the sentence. 5 5.1 Experimental setup Data As is standard, we train our model on sections 2–22 of CCGbank (Hockenmaier and Steedman, 2005), keep section 0 for development and tuning, and evaluate on section 23. As required for decoding, we decompose the categories for each word, inserting the [SEP] token at word boundaries. To represent the input words at the lowest level of our model, we use the (frozen) 5.5B ELMo embeddings (Peters et al., 2018). Because ELMo embeddings are cased and character-based, we need very little preprocessing of the input data: we convert all “n’t” tokens to “’t” and append “n” to the preceding token, unless it is “can” or “won”; we convert the bracket tokens to their original characters (e.g., “-LRB-” to “(”, etc.); and we replace “/” and “*” with “/” and “*” respectively. These steps are solely to more closely match what the ELMo model saw during its training. The inputs are otherwise untouched. There is also no need for a separate token for unknown words. Comparing previous work indicates that ELMo (Clar"
2020.repl4nlp-1.23,P16-2067,0,0.0454835,"Missing"
2020.repl4nlp-1.23,P15-2041,0,0.176241,"or frequent words based on the training data (Clark, 2002; Clark and Curran, 2004, 2007). Recent work relies heavily on word embeddings: they allow better handling of out-of-vocabulary words and decrease reliance on part-of-speech tags, where imperfect accuracy can be a detriment for the supertagger. Lewis and Steedman (2014) used externally-trained embeddings (Turian et al., 2010) combined with suffix and capitalization features in a simple feed-forward neural network as well as a CRF; they also allowed words to be tagged with categories with which they did not co-occur in the training data. Xu et al. (2015) applied the same embeddings and features in a standard Elman RNN (Elman, 1990); they later improved their model by making it bidirectional (Xu et al., 2016). Lewis et al. (2016) replaced the Elman RNNs with twolayer bidirectional LSTMs, taking advantage of the LSTM units’ ability to retain information over time (Hochreiter and Schmidhuber, 1997), and incorporated a data-augmentation technique as well. Vaswani et al. (2016) used a single-layer bidirectional LSTM but dropped all hand-specified features, removed the limit on the categories that the model could produce, used custom in-domain word"
2020.repl4nlp-1.23,D08-1050,0,0.0307267,"l., 2016), so such cases should not be ignored. And unlike typical classification scenarios, out-of-vocabulary lexical categories are not different in kind from in-vocabulary ones; they are composed from the same units using the same rules, suggesting that OOV categories can, in principle, be treated in a concordant manner. Twelve of which are for punctuation. 195 2 They provide relative pronouns in pied-piping constructions and verbs which take expletive subjects as examples; we found lengthy adjunction chains to contribute as well. 3 These proportions are even higher for out-of-domain data (Rimell and Clark, 2008). 3 Related work While our focus in this paper is on CCG supertagging, it is worth noting that the supertagging task originates in the context of LTAG (Joshi and Srinivas, 1994; Bangalore and Joshi, 1999), which also has highly complex lexical categories. Supertagging is important for efficient parsing in such grammars as it helps narrow the search space for the parse (Clark and Curran, 2004). Despite the complexity captured in supertags, the vast majority of existing approaches treat CCG lexical categories as atomic units for prediction, ignoring their varying complexities and structured natu"
2020.repl4nlp-1.23,N16-1025,0,0.0296199,"Missing"
2020.repl4nlp-1.23,P10-1040,0,0.0448181,"ot the same as) POS tagging models in structure. Early work for CCG supertagging relied on maximum-entropy models with hand-specified features, a limited set of possible categories, and tag dictionaries that tracked allowed categories for frequent words based on the training data (Clark, 2002; Clark and Curran, 2004, 2007). Recent work relies heavily on word embeddings: they allow better handling of out-of-vocabulary words and decrease reliance on part-of-speech tags, where imperfect accuracy can be a detriment for the supertagger. Lewis and Steedman (2014) used externally-trained embeddings (Turian et al., 2010) combined with suffix and capitalization features in a simple feed-forward neural network as well as a CRF; they also allowed words to be tagged with categories with which they did not co-occur in the training data. Xu et al. (2015) applied the same embeddings and features in a standard Elman RNN (Elman, 1990); they later improved their model by making it bidirectional (Xu et al., 2016). Lewis et al. (2016) replaced the Elman RNNs with twolayer bidirectional LSTMs, taking advantage of the LSTM units’ ability to retain information over time (Hochreiter and Schmidhuber, 1997), and incorporated a"
2020.repl4nlp-1.23,N16-1027,0,0.56919,"on features in a simple feed-forward neural network as well as a CRF; they also allowed words to be tagged with categories with which they did not co-occur in the training data. Xu et al. (2015) applied the same embeddings and features in a standard Elman RNN (Elman, 1990); they later improved their model by making it bidirectional (Xu et al., 2016). Lewis et al. (2016) replaced the Elman RNNs with twolayer bidirectional LSTMs, taking advantage of the LSTM units’ ability to retain information over time (Hochreiter and Schmidhuber, 1997), and incorporated a data-augmentation technique as well. Vaswani et al. (2016) used a single-layer bidirectional LSTM but dropped all hand-specified features, removed the limit on the categories that the model could produce, used custom in-domain word embeddings, and included a language model– style LSTM over the output lexical categories that allowed the model to condition its predicted tag at time t on the previously-predicted lexical category at time t − 1. In addition to improving word accuracy, this latter addition drastically increased the number of tagged sentences that were parsable, even in variations that hurt word accuracy. The current state-of-the-art result"
2021.eacl-main.294,P94-1040,0,0.706525,"um and Rivest, 1993), sentence disambiguation in pCFGs and HMMs (Sima’an, 2002), and solving cryptograms (Nuhn and Ney, 2013) have even been shown to be NP-complete. These results would seem to be at odds with the observation that we are able to actually perform these tasks. Clearly, while NP-completeness results are a necessary part of our understanding, they do not tell the whole story about our algorithms’ behaviour. Faced with the inadequacy of existing descriptive complexity measures to represent program operation, some researchers have resorted to empirical measures of performance (e.g. Carroll, 1994). Unfortunately, empirical studies have their own pitfalls, and these are also difficult to address. Specifically, empirical studies of program performance have an implicit dependence on the distribution of inputs used in the study. Changing this distribution of inputs can change the overall performance drastically. This, after all, is why we separate training and evaluation test sets. Ultimately our analytic tools should work together with our empirical studies, in the sense that we should be able to give quantify how much program performance changes as we change our input distributions. This"
2021.eacl-main.294,J99-1004,0,0.320713,"|s) with probability greater than 1 − δ. 3360 This bound unfortunately does not tell us how the problem difficulty scales with the sentence length n, but it does give us a sense of how difficult the MPS problem will be for a given grammar. While it is true that finding the exact values for either HG (t|s) or νG (t|s) is undecidable for general pCFGs, these values are can be given rough bounds – for example HG (t|s) is bounded above by the tree entropy HG,n (t). Finding better bounds is clearly an area for future research. Further discussion of the variance of the tree entropy can be found in (Chi, 1999). 8 Other Applications In this paper, we have only considered the MPS problem, but the same approach can be applied to other inference problems, such as those found in Bayesian networks or CRFs. In particular, suppose that we are performing inference on a graphical model in which: • we are trying to assign values to a finite set S of hidden variables in order to maximize the probability p of an event E, based on a finite set of hidden variables, • we can build partial solutions to our inference problem by assigning values to one or more of these variables, took O(n3 ) time. Our current method"
2021.eacl-main.294,W13-3009,1,0.528031,"uantities like length. It is not likely to be appropriate for situations where the program improvement is due to observable regularities in the input data. What we really want to do is to exploit the fact that we see only a fragment of the theoretically possible input space in the real world. Unfortunately, the task of identifying good problem fragments for our models is itself very hard, and so we will instead attempt to use statistical measures to build tools for narrowing down the space in which our inputs will lie. A start towards this goal is found for the letter-substitution problem in (Corlett and Penn, 2013), but this approach does not give a tight enough bound to describe when we should expect better running times, and it does not immediately generalize to other tasks. We would like to strengthen the approach used in that paper to make it more applicable to different areas. With this in mind, we turn to another NP-complete problem, the MPS problem for pCFGs. 3 Tagging and NP-Completeness To start, we recall the definition of the problem: tag sequence is the sum of the probabilities of the trees yielding that sequence. The difficulty of this problem lies in the fact that we are looking for the PO"
2021.eacl-main.294,P01-1030,0,0.356438,"omputer science have dealt with worst-case complexity. A major NLP result in this vein has been the study by Sima’an (2002), which shows that the most probable parse problem (MPP) for pSTSGs and the most probable sentence problem (MPS) for both pSTSGs and pCFGs are all NP-complete. These problems can be thought of as renormalization over a graphical model, and have been studied further in several publications, including De la Higuera and Oncina (2013) and Goodman (1998). Similar NP-completeness results exist for the problem of finding the optimal word order for phrases in machine translation (Germann et al., 2001) and for solving letter-substitution problems such as cryptograms (Nuhn and Ney, 2013). An argument could be made for average-case complexity (Levin, 1986; Impagliazzo, 1995) as an alternative to what we are attempting here. Average-case complexity is a step towards the type of results that we want, but it still has its shortcomings. In particular, an average case complexity analysis relies on the underlying distribution of inputs being known (and, usually, easy to work with) in advance. The distribution of choice is often na¨ıvely uniform. In situations where the input distribution is defined"
2021.eacl-main.294,P13-1060,0,0.201318,"a simple search algorithm, allowing for a much faster search than the NP-completeness of this problem would suggest. 1 Introduction Natural Language Processing uses many algorithms that are theoretically intractable, but work well in practice. The k-means clustering algorithm, for example, has an exponential worst-case time, but is generally polynomial in practical applications (Arthur et al., 2009). If we look at problems, as opposed to algorithms, the task of training neural networks (Blum and Rivest, 1993), sentence disambiguation in pCFGs and HMMs (Sima’an, 2002), and solving cryptograms (Nuhn and Ney, 2013) have even been shown to be NP-complete. These results would seem to be at odds with the observation that we are able to actually perform these tasks. Clearly, while NP-completeness results are a necessary part of our understanding, they do not tell the whole story about our algorithms’ behaviour. Faced with the inadequacy of existing descriptive complexity measures to represent program operation, some researchers have resorted to empirical measures of performance (e.g. Carroll, 1994). Unfortunately, empirical studies have their own pitfalls, and these are also difficult to address. Specifical"
2021.eacl-main.294,C08-1094,0,0.0947706,"Missing"
2021.eacl-main.306,2020.acl-main.236,0,0.03406,"Missing"
2021.eacl-main.306,N19-1050,0,0.0533864,"LP tasks as well. 5.2 Use Case 1: Compositional Semantics A prime example of the significant performance improvements brought on by integer addition of word embeddings in the area of compositional semantics can be found in Salton and McGill (1986). It was demonstrated that vector addition is more effective than other proposed unsupervised compositional models (multiplication (Mitchell and Lapata, 2010), tensor product with convolution (Widdows and Ferraro, 2008), and dilation (Mitchell and Lapata, 2010)) for determining semantic relatedness between bigrams and Dataset and previous results. In Asaadi et al. (2019), the authors introduce BIRD, a bigram relatedness dataset created using the Best-Worst Scaling annotation method. To accomplish this task, annotators are provided with n sample, where n is often 4, and are asked which of the samples best represent the a given property and which one represents it the worst (Kiritchenko and Mohammad, 2016, 2017). Asaadi et al. (2019) compute bigram semantic relatedness using three different kinds of word embeddings; namely, pre-trained GloVe vectors3 , pre-trained fastText word embeddings4 , and word-context cooccurrence vectors extracted from a corpus of unive"
2021.eacl-main.306,D14-1181,0,0.0304555,"Missing"
2021.eacl-main.306,N16-1095,0,0.0158082,"nsupervised compositional models (multiplication (Mitchell and Lapata, 2010), tensor product with convolution (Widdows and Ferraro, 2008), and dilation (Mitchell and Lapata, 2010)) for determining semantic relatedness between bigrams and Dataset and previous results. In Asaadi et al. (2019), the authors introduce BIRD, a bigram relatedness dataset created using the Best-Worst Scaling annotation method. To accomplish this task, annotators are provided with n sample, where n is often 4, and are asked which of the samples best represent the a given property and which one represents it the worst (Kiritchenko and Mohammad, 2016, 2017). Asaadi et al. (2019) compute bigram semantic relatedness using three different kinds of word embeddings; namely, pre-trained GloVe vectors3 , pre-trained fastText word embeddings4 , and word-context cooccurrence vectors extracted from a corpus of university websites (Turney et al., 2011)5 . Specifically, the authors compute the relatedness score for the vectors representing the term pair AB-X, where AB is a bigram and X can be either a bigram or a unigram. The results of four unsupervised compositional models were compared: • Weighted addition (Salton and McGill, 1986); • Multiplicati"
2021.eacl-main.306,P17-2074,0,0.0209173,"Missing"
2021.eacl-main.306,P16-2063,0,0.0223436,"2 60 102 57 131 57 1000000 371 574 539 571 719 571 1026 570 1310 571 10000000 3711 5701 5385 5690 7203 5698 10261 5698 13107 5700 Table 2: Milliseconds it takes to run 100000, 1000000, and 10000000 many element-wise multiplication over vectors of (*d) dimensions at (d*) precision. tors can considerably outweigh the losses from multiplying integer representations when the number of additions is greater or equal to the number of multiplications. In practice, as illustrated in several use cases below, the number of additions can be far greater in many important tasks. Recall also the findings of Ling et al. (2016), who did not observe a significant decrease in performance when using an 8-bit fixed-point value for word embeddings in word and phrase similarity and dependency parsing tasks. This implies that there is some room for compromise on precision practical NLP tasks as well. 5.2 Use Case 1: Compositional Semantics A prime example of the significant performance improvements brought on by integer addition of word embeddings in the area of compositional semantics can be found in Salton and McGill (1986). It was demonstrated that vector addition is more effective than other proposed unsupervised compo"
2021.eacl-main.306,P05-1015,0,0.0295863,"0375007 89863047 109772868 129763285 149762224 169762728 Φ -1 0 1 2 5 full 50d Up. Bd. 53756606 138556818 90156697 110156747 130156797 150156847 170156897 Table 4: Size in bytes of 50-dimensional GloVe embeddings dataset, their CRT-compression and the upper bound size of that CRT-compression for any possible 50-dimensional vectors, at various precisions Φ. BERT. Kim (2014) is the same work that Arora et al. (2020) used to compare with BERT over 6 different datasets. Of those datasets, we choose to analyze the variation in performance on the MR dataset (movie reviews – one sentence per review) Pang and Lee (2005) and on the opinion polarity detection subtask of the MPQA dataset (Wiebe et al., 2005). In addition to comparing BERT-based sentiment analysis results with CNNs that take fullprecision 300-dimensional GloVe vectors as inputs (which Arora et al. (2020) does), we verify the feasibility of this use case, by conducting experiments with varying precisions of 50dimensional and 300-dimensional GloVe embeddings. Table 5 shows that sentence-based sentiment analysis with GloVe embeddings remains almost unchanged even at Φ = 1 on the MR dataset, on which Arora et al. (2020) shows that BERT outperforms C"
2021.eacl-main.306,D14-1162,0,0.0986681,"Missing"
2021.eacl-main.306,W17-4119,0,0.027251,"2 ≥ mi . The same property holds for multiplication; i.e., (X1 ∗ X2 ) (mod (mi )2 ), when X1 ∗ X2 ≥ mi . In other words, this representation supports vector addition, multiplication and, in a more restricted range, subtraction, through executing those operations directly on integers. Addition and multiplication are fundamental to most compositional semantics approaches, beginning with Mitchell and Lapata (2008, 2010). Addition is also central to measuring the relationship between word pairs, particularly when using the L1-norm as a metric, which is less sensitive to outliers than the L2-norm (Stratos, 2017). Word analogy is an example of such a task. Of course, the these operations can also be applied to measuring the relationship between sentence pairs and document pairs. 4 Reasoning about Precision using Numerical Analysis One might assume that we have to perform extensive empirical analysis to determine the minimum floating point precision necessary for the inputs of a particular NLP algorithm. However, there are generalizations we can make based on concepts as simple as the absolute error and relative error of components of the operations, described in Heath (2018) as: Absolute error = appro"
2021.eacl-main.306,2020.acl-main.195,0,0.0614276,"Missing"
2021.eacl-main.306,D11-1063,0,0.0179804,"troduce BIRD, a bigram relatedness dataset created using the Best-Worst Scaling annotation method. To accomplish this task, annotators are provided with n sample, where n is often 4, and are asked which of the samples best represent the a given property and which one represents it the worst (Kiritchenko and Mohammad, 2016, 2017). Asaadi et al. (2019) compute bigram semantic relatedness using three different kinds of word embeddings; namely, pre-trained GloVe vectors3 , pre-trained fastText word embeddings4 , and word-context cooccurrence vectors extracted from a corpus of university websites (Turney et al., 2011)5 . Specifically, the authors compute the relatedness score for the vectors representing the term pair AB-X, where AB is a bigram and X can be either a bigram or a unigram. The results of four unsupervised compositional models were compared: • Weighted addition (Salton and McGill, 1986); • Multiplication (Mitchell and Lapata, 2010); • Tensor product with convolution (Widdows and Ferraro, 2008); • Dilation (Mitchell and Lapata, 2010). They then used Pearson correlation to compare the semantic relatedness scores, computed using these unsupervised compositional models, with the gold-standard in t"
2021.eacl-main.306,widdows-ferraro-2008-semantic,0,0.0773283,"lue for word embeddings in word and phrase similarity and dependency parsing tasks. This implies that there is some room for compromise on precision practical NLP tasks as well. 5.2 Use Case 1: Compositional Semantics A prime example of the significant performance improvements brought on by integer addition of word embeddings in the area of compositional semantics can be found in Salton and McGill (1986). It was demonstrated that vector addition is more effective than other proposed unsupervised compositional models (multiplication (Mitchell and Lapata, 2010), tensor product with convolution (Widdows and Ferraro, 2008), and dilation (Mitchell and Lapata, 2010)) for determining semantic relatedness between bigrams and Dataset and previous results. In Asaadi et al. (2019), the authors introduce BIRD, a bigram relatedness dataset created using the Best-Worst Scaling annotation method. To accomplish this task, annotators are provided with n sample, where n is often 4, and are asked which of the samples best represent the a given property and which one represents it the worst (Kiritchenko and Mohammad, 2016, 2017). Asaadi et al. (2019) compute bigram semantic relatedness using three different kinds of word embed"
2021.iwpt-1.2,2020.repl4nlp-1.23,1,0.731359,"puting ?=1 ? , and then verifying that the diagonal entries are all zero. For condition T3, we can similarly inspect the entries corresponding to the parent and child nodes of all Lambek edges and verify that they are all one, indicating that a regular path exists. We can now see how to specify these conditions as loss functions: ?= ?T2 (?) = ?T3 (?) = |vtx | ∑ ?=1 |vtx | ∑ ?? = |vtx | ∑ in categorial grammars. In this sense, our work follows up on the intuition of recent “constructive” supertaggers, which have been explored for a type-logical grammar (Kogkalidis et al., 2019) and for CCG (Bhargava and Penn, 2020; Prange et al., 2021). Such supertaggers construct categories out of the atomic categories of the grammar; this challenges the classical approach to supertagging, where lexical categories are treated as opaque, rendering the task of supertagging equivalent to large-tagset POS tagging. With this view, it becomes possible for novel categories to be produced; furthermore, the supertaggers are better able to incorporate prediction history and thereby produce grammatical outputs (Bhargava and Penn, 2020). Recently, Kogkalidis et al. (2020) proposed a system for parsing a “type-logical” grammar tha"
2021.iwpt-1.2,J07-4004,0,0.128264,"f logical rules (Girard, 1987; Roorda, 1992). This corresponds to the problem of spurious ambiguity, making proof nets an attractive choice for representing derivations. Introduction In the family of categorial grammars, combinatory categorial grammar (CCG) has received by far the most attention in the computational linguistics literature. There exist algorithms for both mildly context-sensitive (e.g., Kuhlmann and Satta, 2014) and context-free (typically CKY; Cocke and Schwartz, 1970; Kasami, 1966; Younger, 1967) CCG parsing, and there has been much research on statistical CCG parsers (e.g., Clark and Curran, 2007; Lewis et al., 2016; Stanojević and Steedman, 2020). Another member of the categorial family, Lambek categorial grammar (LCG), has been less well-explored: LCG work has been primarily theoretical or focused on non-statistical parsing. The recent lack of attention is likely due to two notable results: (1) LCG is weakly context-free equivalent (Pentus, 1997); and (2) LCG parsing is NP-complete (Pentus, 2006; Savateev, 2012). However, neither of these issues is necessarily practically relevant. Moreover, LCG presents a number of advantages and interesting properties. For example, LCG provides ev"
2021.iwpt-1.2,P19-1285,0,0.0284684,"Missing"
2021.iwpt-1.2,W19-4314,0,0.0270993,"with the canÍ|V |? didate linkage, computing ?=1 ? , and then verifying that the diagonal entries are all zero. For condition T3, we can similarly inspect the entries corresponding to the parent and child nodes of all Lambek edges and verify that they are all one, indicating that a regular path exists. We can now see how to specify these conditions as loss functions: ?= ?T2 (?) = ?T3 (?) = |vtx | ∑ ?=1 |vtx | ∑ ?? = |vtx | ∑ in categorial grammars. In this sense, our work follows up on the intuition of recent “constructive” supertaggers, which have been explored for a type-logical grammar (Kogkalidis et al., 2019) and for CCG (Bhargava and Penn, 2020; Prange et al., 2021). Such supertaggers construct categories out of the atomic categories of the grammar; this challenges the classical approach to supertagging, where lexical categories are treated as opaque, rendering the task of supertagging equivalent to large-tagset POS tagging. With this view, it becomes possible for novel categories to be produced; furthermore, the supertaggers are better able to incorporate prediction history and thereby produce grammatical outputs (Bhargava and Penn, 2020). Recently, Kogkalidis et al. (2020) proposed a system for"
2021.iwpt-1.2,2020.conll-1.3,0,0.480571,"type-logical grammar (Kogkalidis et al., 2019) and for CCG (Bhargava and Penn, 2020; Prange et al., 2021). Such supertaggers construct categories out of the atomic categories of the grammar; this challenges the classical approach to supertagging, where lexical categories are treated as opaque, rendering the task of supertagging equivalent to large-tagset POS tagging. With this view, it becomes possible for novel categories to be produced; furthermore, the supertaggers are better able to incorporate prediction history and thereby produce grammatical outputs (Bhargava and Penn, 2020). Recently, Kogkalidis et al. (2020) proposed a system for parsing a “type-logical” grammar that is essentially a modal, non-directional extension of LCG. The Dutch grammar they used is substantially different from our grammar: their connectives are both modal and non-directional; in addition, they have far more atomic categories. While their model is similar to our baseline (Section 3.1), our work here differs substantially in that we incorporate proof-net structural elements and validity conditions, and our system is able to return multiple linkages (Sections 3.2–3.4). Our approach also enables ground-truth–free training.5 Las"
2021.iwpt-1.2,Q14-1032,0,0.0210935,"tural aspects of the grammar. We base our system on proof nets, a graphical method for representing linear logic proofs that abstracts over irrelevant aspects, such as the order of application of logical rules (Girard, 1987; Roorda, 1992). This corresponds to the problem of spurious ambiguity, making proof nets an attractive choice for representing derivations. Introduction In the family of categorial grammars, combinatory categorial grammar (CCG) has received by far the most attention in the computational linguistics literature. There exist algorithms for both mildly context-sensitive (e.g., Kuhlmann and Satta, 2014) and context-free (typically CKY; Cocke and Schwartz, 1970; Kasami, 1966; Younger, 1967) CCG parsing, and there has been much research on statistical CCG parsers (e.g., Clark and Curran, 2007; Lewis et al., 2016; Stanojević and Steedman, 2020). Another member of the categorial family, Lambek categorial grammar (LCG), has been less well-explored: LCG work has been primarily theoretical or focused on non-statistical parsing. The recent lack of attention is likely due to two notable results: (1) LCG is weakly context-free equivalent (Pentus, 1997); and (2) LCG parsing is NP-complete (Pentus, 2006"
2021.iwpt-1.2,2020.acl-demos.38,0,0.0391607,"Missing"
2021.iwpt-1.2,2020.acl-main.378,0,0.0111321,". This corresponds to the problem of spurious ambiguity, making proof nets an attractive choice for representing derivations. Introduction In the family of categorial grammars, combinatory categorial grammar (CCG) has received by far the most attention in the computational linguistics literature. There exist algorithms for both mildly context-sensitive (e.g., Kuhlmann and Satta, 2014) and context-free (typically CKY; Cocke and Schwartz, 1970; Kasami, 1966; Younger, 1967) CCG parsing, and there has been much research on statistical CCG parsers (e.g., Clark and Curran, 2007; Lewis et al., 2016; Stanojević and Steedman, 2020). Another member of the categorial family, Lambek categorial grammar (LCG), has been less well-explored: LCG work has been primarily theoretical or focused on non-statistical parsing. The recent lack of attention is likely due to two notable results: (1) LCG is weakly context-free equivalent (Pentus, 1997); and (2) LCG parsing is NP-complete (Pentus, 2006; Savateev, 2012). However, neither of these issues is necessarily practically relevant. Moreover, LCG presents a number of advantages and interesting properties. For example, LCG provides even greater syntax-semantics transparency than is the"
2021.iwpt-1.2,P19-1176,0,0.12922,"ransformer encoder stack consists of ? encoder layers. Denoting the input to layer ? as ??−1 , with ?0 as above, each layer computes ?? as: ? ? = ?q,? ⊙ ??−1 hdim √ where vtx → vtx′ denotes renaming axis vtx to vtx ′. Next, with final trainable parameter ?o,? ∈ Rheads × hdim × vec , we compute the SelfAttn? output:   SelfAttn? ( ??−1 ) = ?o,? ⊙ softmax (?? )⊙?? (4) ?0 = (? + ?w ) ⊙ ? + ? ⊙ ? words hdim (2) vec ?? = ?v,? ⊙ [ ??−1 ] vtx→vtx′ vec ?? = ?r,? ⊙ ?v vec 4We apply layer normalization (Ba et al., 2016) as well, but omit it here for concision. We use the “pre-norm” application order (Wang et al., 2019; Nguyen and Salazar, 2019). 16 Importantly for training with gradient descent, the operations of Sinkhorn’s algorithm are fully differentiable. For a given doubly-stochastic output matrix from Sinkhorn’s algorithm, the negative log likelihood ?NLL of the ground-truth linkage L =  ⟨?1 , ?1 ⟩, ⟨?2 , ?2 ⟩, . . . , ⟨? |vtx |/2 , ? |vtx |/2 ⟩ is a natural choice of loss function for training: each encoder layer ?, we introduce four new transformation matrices ?q,R,? , ?q,L,? , ?k,R,? , ?k,L,? ∈ Rheads × hdim × vec and alter Equations 1 and 2: ? ? = ?k,? ⊙ ??−1 vec +  ?q,R,? ⊙ ??−1 ⊙ ? R vec ? ="
2021.smm4h-1.1,S18-1001,0,0.172915,"ntiment classification results — every post is classified into one of the predetermined sentiment categories (positive/(neutral)/negative) — even though sentiment is a continuous random variable. For example, Wang et al. (2020b) provide two “sentiment-neutral” examples that in fact have differing sentiments. Smoothing sentiment from a continuous variable into a ternary or binary scale causes a loss of dynamics, hence increasing the difficulty of the task and lowering the reliability of all subsequent analyses. There are now n-valued sentiment corpora for n = 5 (Socher et al., 2013) and n = 7 (Mohammad et al., 2018), but finer-grained discrete sentiment does not entirely solve the problem. The valence regression task (V-reg) proposed by Mohammad et al. (2018) is far more suitable because it conveys a continuous sentiment intensity measure through a logistic regression score. In the midst of a global pandemic, understanding the public’s opinion of their government’s policy-level, non-pharmaceutical interventions (NPIs) is a crucial component of the health-policy-making process. Prior work on COVID-19 NPI sentiment analysis by the epidemiological community has proceeded without a method for properly attrib"
2021.smm4h-1.1,D13-1170,0,0.00515576,"s analyses have been based on sentiment classification results — every post is classified into one of the predetermined sentiment categories (positive/(neutral)/negative) — even though sentiment is a continuous random variable. For example, Wang et al. (2020b) provide two “sentiment-neutral” examples that in fact have differing sentiments. Smoothing sentiment from a continuous variable into a ternary or binary scale causes a loss of dynamics, hence increasing the difficulty of the task and lowering the reliability of all subsequent analyses. There are now n-valued sentiment corpora for n = 5 (Socher et al., 2013) and n = 7 (Mohammad et al., 2018), but finer-grained discrete sentiment does not entirely solve the problem. The valence regression task (V-reg) proposed by Mohammad et al. (2018) is far more suitable because it conveys a continuous sentiment intensity measure through a logistic regression score. In the midst of a global pandemic, understanding the public’s opinion of their government’s policy-level, non-pharmaceutical interventions (NPIs) is a crucial component of the health-policy-making process. Prior work on COVID-19 NPI sentiment analysis by the epidemiological community has proceeded wi"
2021.smm4h-1.1,2020.acl-main.197,0,0.0298227,"Missing"
2021.textgraphs-1.11,P18-1026,0,0.0225412,"ve are trainable parameters. After information is propagated for T time steps, each node’s hidden state collectively represents a message about itself and its neighbourhood, which is then passed to its neighbours. Finally there is the output model. For example, Acuna et al. (2018) implemented their output model by: hv = tanh(F C1 (hTv )) outv = F C2 (hv ) (2) where F C1 and F C2 are two fully connected layers. 2.2 In this paper, we exploit a Gated Graph Neural Network (GGNN) (Li et al., 2016) as a parse tree encoder. GNN encoders have been shown to be efficient for neural machine translation (Beck et al., 2018; Bastings et al., 2017) whereas in our case, we focus on structured realization. GGNNs define a propagation model that extends RNNs to arbitrary graphs and learn propagation rules between nodes. We aim to encode syntactic trees by propagating category labels throughout the tree’s structure. and A: Gated Graph Neural Network Models In this part, we will describe how we use GGNNs and parse trees to build our three experimental models. Figure 2 depicts example trees for these models. 2.2.1 Input Tree Construction Since we are using GGNNs, we first need to construct the graph by giving the parse"
2021.textgraphs-1.11,P18-1249,0,0.063076,"cy. If we simply remove the yields of corpus trees and attempt to regenerate them from the trees, the resulting strings will often differ from the original yields, but they may still be grammatical in the sense of the first j tokens having the appropriate POS tag sequence. Table 2 shows the sentence-level word and POS accuracies on the OntoNotes test set. Both OntoNotes and PTB provide gold-standard (human labeled) syntactic constituency parse trees. We trained our model on these trees. These trees are expensive, however, so we also evaluated on trees obtained from the Berkeley neural parser (Kitaev and Klein, 2018) a state-of-the-art constituency parser with an F1 = 95 score on the PTB. 3.4 Continuity of Latent Spaces Some trees have the same unlabelled tree structure, although they may have different nodes. We can (6) for some value of λ ∈ [0, 1]. Table 3 demonstrates sentences generated from trees for various values of λ. This kind of “semantic continuity” has been demonstrated before on vector encodings, but, to our knowledge, not on structured spaces such as parallel trees of vectors. 3.5 Perplexity Perplexity is perhaps the most common evaluation measure in the language modelling literature. The fo"
2021.textgraphs-1.11,J92-4003,0,0.754166,"long and distinguished history. AlGGNNs have proved to be effective as encoders though the term itself was not coined until Jelinek et al. (1975), the earliest work of Shannon (1948) of constituent parse trees from a variety of peron entropy presents what are effectively character- spectives, including realization accuracy, perplexity, word-level prediction accuracy, categorical colevel language models as a motivating example. In hesion of predictions, and novel lexical selection. A both cases, given a prefix of characters/words or limitation of this study is the comparatively modest classes (Brown et al., 1992), the aim of the task is size of its corpora, which is due to the requirement to predict the next such event. n-gram language for properly curated parse-annotated data. Findmodels factor any dependency of the next event ing ways to scale up to larger training and test sets on the prefix through its dependency on the final without the bias introduced by automated parsers n − 1 events in the prefix. This long remained the remains an important issue to investigate. dominant type of language model, but the advent of neural language models (Bengio et al., 2003), and particularly vector-space embedd"
2021.textgraphs-1.11,C16-1026,0,0.0466077,"Missing"
2021.textgraphs-1.11,D17-1209,0,0.0416936,"Missing"
2021.textgraphs-1.11,N19-1423,0,0.0133913,"raph state at each time step. Our models also simultaneously update every node in the tree at every time step. The encoder of Trisedya et al. (2018) takes input RDF triples rendered as a graph and builds a dynamic recurrent structure that traverses the adjacency matrix of the graph one node at a time. Marcheggiani and Perez-Beltrachini (2018), again using a GCN, take only the nodes of the RDF graph as input, using the edges directly as a weight matrix. They, too, must update the entire graph at every time step. et al., 2014), and convolutional networks (Gehring et al., 2017) and transformers (Devlin et al., 2019). An earlier, but ultimately unsuccessful attempt at dislodging n-gram language models was that of Chelba (2000), who augmented this prefix with syntactic information. Chelba (2000) did not use conventional parse trees from any of the then-common parse-annotated corpora, nor from linguistic theory, because these degraded rather than enhanced language modelling performance. Instead, he had to remain very sparing in order to realize an empirical improvement. The present model not only shares information at the dimensional level, but projects syntactic structure over the words to be predicted. Wh"
2021.textgraphs-1.11,P15-1033,0,0.0109202,"a word p, we count it as works as dependency tree encoders for semantic a correct prediction if: (1) p occurs somewhere in role labelling. Even before graph neural networks 121 Model Model 1 Model 2 Model 3 LSTM-256 LM POS acc 94.35 89.4 92.33 OntoNotes Word acc µavg macavg med 32.4 26.6 38.6 32.0 37.39 43.52 34.7 37.9 44.3 22.4 POS acc 94.28 97.66 95.73 PTB Word acc µavg macavg 35.47 41.74 37.12 43.74 37.27 41.69 23.57 med 47.46 49.22 44.37 Table 8: Percentage POS prediction accuracies and word prediction accuracies, for each model. become popular, there were attempts akin to graph encoders. Dyer et al. (2015); Socher et al. (2013); Tai et al. (2015); Zhu et al. (2015); Le and Zuidema (2014) encoded tree structure with recursive neural networks or Tree-LSTMs. Surface Realization Song et al. (2018) introduced a graph-to-sequence LSTM for AMR-totext generation that can encode AMR structures directly. The model takes multiple recurrent transition steps in order to propagate information beyond local neighbourhoods. But this method must maintain the entire graph state at each time step. Our models also simultaneously update every node in the tree at every time step. The encoder of Trisedya et al. (2018)"
2021.textgraphs-1.11,E17-1117,0,0.063052,"Missing"
2021.textgraphs-1.11,D14-1081,0,0.0272248,"ction if: (1) p occurs somewhere in role labelling. Even before graph neural networks 121 Model Model 1 Model 2 Model 3 LSTM-256 LM POS acc 94.35 89.4 92.33 OntoNotes Word acc µavg macavg med 32.4 26.6 38.6 32.0 37.39 43.52 34.7 37.9 44.3 22.4 POS acc 94.28 97.66 95.73 PTB Word acc µavg macavg 35.47 41.74 37.12 43.74 37.27 41.69 23.57 med 47.46 49.22 44.37 Table 8: Percentage POS prediction accuracies and word prediction accuracies, for each model. become popular, there were attempts akin to graph encoders. Dyer et al. (2015); Socher et al. (2013); Tai et al. (2015); Zhu et al. (2015); Le and Zuidema (2014) encoded tree structure with recursive neural networks or Tree-LSTMs. Surface Realization Song et al. (2018) introduced a graph-to-sequence LSTM for AMR-totext generation that can encode AMR structures directly. The model takes multiple recurrent transition steps in order to propagate information beyond local neighbourhoods. But this method must maintain the entire graph state at each time step. Our models also simultaneously update every node in the tree at every time step. The encoder of Trisedya et al. (2018) takes input RDF triples rendered as a graph and builds a dynamic recurrent structu"
2021.textgraphs-1.11,W18-6501,0,0.0170085,"rface Realization Song et al. (2018) introduced a graph-to-sequence LSTM for AMR-totext generation that can encode AMR structures directly. The model takes multiple recurrent transition steps in order to propagate information beyond local neighbourhoods. But this method must maintain the entire graph state at each time step. Our models also simultaneously update every node in the tree at every time step. The encoder of Trisedya et al. (2018) takes input RDF triples rendered as a graph and builds a dynamic recurrent structure that traverses the adjacency matrix of the graph one node at a time. Marcheggiani and Perez-Beltrachini (2018), again using a GCN, take only the nodes of the RDF graph as input, using the edges directly as a weight matrix. They, too, must update the entire graph at every time step. et al., 2014), and convolutional networks (Gehring et al., 2017) and transformers (Devlin et al., 2019). An earlier, but ultimately unsuccessful attempt at dislodging n-gram language models was that of Chelba (2000), who augmented this prefix with syntactic information. Chelba (2000) did not use conventional parse trees from any of the then-common parse-annotated corpora, nor from linguistic theory, because these degraded r"
2021.textgraphs-1.11,N16-1024,0,0.137222,"nt nodes. We can (6) for some value of λ ∈ [0, 1]. Table 3 demonstrates sentences generated from trees for various values of λ. This kind of “semantic continuity” has been demonstrated before on vector encodings, but, to our knowledge, not on structured spaces such as parallel trees of vectors. 3.5 Perplexity Perplexity is perhaps the most common evaluation measure in the language modelling literature. The formula of perplexity was shown in Eq 5. We trained and evaluated our Models on the different datasets listed in Table 1. The perplexities of the test data sets are listed in Table 4. RNNG (Dyer et al., 2016) is a state-of-the-art syntax-aware model. LSTM-256 LM is our self implemented language model using 2-layer LSTM cell with sequence length 20 and hidden state size 256. Our three models have lower perplexities across the board compared with RNNG on both OntoNotes and PTB. Model 3 on gold parse trees has the lowest perplexity overall, although it is important to remember that our models benefit from distributions from Wikipedia that are implicitly encoded in the GloVe vectors. LSTMs that use GloVe perform worse than the LSTMs with trainable word embeddings shown here.2 In addition, for compario"
2021.textgraphs-1.11,N06-2015,0,0.055531,"l should predict syntactic structure over every word that it has predicted, but in tion is partial, in that there is no overhang (j = 0), his evaluation, it is very clear that he is more concerned with but structurally non-trivial, although often sparing the first stage of word prediction. 115 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 115–124 June 11, 2021. ©2021 Association for Computational Linguistics We shall compare two modes of our model here using GGNN-encoded parse trees: one with parse trees from OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2013; Weischedel et al., 2013), and one with vestigial transitions between pre-terminal categories in sequence, which resembles the syntactic annotation selected by Kiros et al. (2014), although here the word prefix is also POS-annotated. We also test the combination of the two: a syntactic tree augmented by a linear pipeline of transitions between pre-terminals. We compute sentence-level accuracy by measuring how many words in the generated strings legitimately belong to their assigned POS categories, and compute word-level accuracy scores in three ways: accuracy at choosing"
2021.textgraphs-1.11,D17-1159,0,0.0531849,"Missing"
2021.textgraphs-1.11,J93-2004,0,0.074756,"erage cross entropy loss of validation set using Model 1 with different magnitude of vectors. 3 Results 3.1 Datasets After the input presentation, the propagation step and a fully connected layer, the model will gener- We train and test all models on OntoNotes 5.0, ate an N × D output matrix. In other words, all which contains 110,000+ English sentences from N nodes in the parse tree will have D-dimensional print publications. We also train and evaluate the output. In language modelling mode, we would perplexity of all models on the Penn Treebank not care about any output except the one gen- (Marcus et al., 1993), as this has become a standard erated by the pre-terminal dominating the posi- among syntax-driven language models. PTB $2-21 tion of wn−j+1 . Let vˆ denote this normalized D- are used as training data, $24 is for validation, and dimensional output. The probability of wn−j+1 $23 is used for testing. 118 We excluded those trees/sentences with words that are not in GloVe’s (Pennington et al., 2014) pre-trained vocabulary from both the training and validation data. The test set and validation set were excluded from our development cycle. Dataset statistics are provided in Table 1. randomly pick"
2021.textgraphs-1.11,D14-1162,0,0.0848005,"Missing"
2021.textgraphs-1.11,W13-3516,0,0.0107349,"ntactic structure over every word that it has predicted, but in tion is partial, in that there is no overhang (j = 0), his evaluation, it is very clear that he is more concerned with but structurally non-trivial, although often sparing the first stage of word prediction. 115 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 115–124 June 11, 2021. ©2021 Association for Computational Linguistics We shall compare two modes of our model here using GGNN-encoded parse trees: one with parse trees from OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2013; Weischedel et al., 2013), and one with vestigial transitions between pre-terminal categories in sequence, which resembles the syntactic annotation selected by Kiros et al. (2014), although here the word prefix is also POS-annotated. We also test the combination of the two: a syntactic tree augmented by a linear pipeline of transitions between pre-terminals. We compute sentence-level accuracy by measuring how many words in the generated strings legitimately belong to their assigned POS categories, and compute word-level accuracy scores in three ways: accuracy at choosing a word of the appropr"
2021.textgraphs-1.11,P19-1437,0,0.0240424,"Missing"
2021.textgraphs-1.11,D13-1170,0,0.00284486,"t as works as dependency tree encoders for semantic a correct prediction if: (1) p occurs somewhere in role labelling. Even before graph neural networks 121 Model Model 1 Model 2 Model 3 LSTM-256 LM POS acc 94.35 89.4 92.33 OntoNotes Word acc µavg macavg med 32.4 26.6 38.6 32.0 37.39 43.52 34.7 37.9 44.3 22.4 POS acc 94.28 97.66 95.73 PTB Word acc µavg macavg 35.47 41.74 37.12 43.74 37.27 41.69 23.57 med 47.46 49.22 44.37 Table 8: Percentage POS prediction accuracies and word prediction accuracies, for each model. become popular, there were attempts akin to graph encoders. Dyer et al. (2015); Socher et al. (2013); Tai et al. (2015); Zhu et al. (2015); Le and Zuidema (2014) encoded tree structure with recursive neural networks or Tree-LSTMs. Surface Realization Song et al. (2018) introduced a graph-to-sequence LSTM for AMR-totext generation that can encode AMR structures directly. The model takes multiple recurrent transition steps in order to propagate information beyond local neighbourhoods. But this method must maintain the entire graph state at each time step. Our models also simultaneously update every node in the tree at every time step. The encoder of Trisedya et al. (2018) takes input RDF tripl"
2021.textgraphs-1.11,P18-1150,0,0.0174528,"Model 2 Model 3 LSTM-256 LM POS acc 94.35 89.4 92.33 OntoNotes Word acc µavg macavg med 32.4 26.6 38.6 32.0 37.39 43.52 34.7 37.9 44.3 22.4 POS acc 94.28 97.66 95.73 PTB Word acc µavg macavg 35.47 41.74 37.12 43.74 37.27 41.69 23.57 med 47.46 49.22 44.37 Table 8: Percentage POS prediction accuracies and word prediction accuracies, for each model. become popular, there were attempts akin to graph encoders. Dyer et al. (2015); Socher et al. (2013); Tai et al. (2015); Zhu et al. (2015); Le and Zuidema (2014) encoded tree structure with recursive neural networks or Tree-LSTMs. Surface Realization Song et al. (2018) introduced a graph-to-sequence LSTM for AMR-totext generation that can encode AMR structures directly. The model takes multiple recurrent transition steps in order to propagate information beyond local neighbourhoods. But this method must maintain the entire graph state at each time step. Our models also simultaneously update every node in the tree at every time step. The encoder of Trisedya et al. (2018) takes input RDF triples rendered as a graph and builds a dynamic recurrent structure that traverses the adjacency matrix of the graph one node at a time. Marcheggiani and Perez-Beltrachini ("
2021.textgraphs-1.11,P15-1150,0,0.106577,"Missing"
2021.textgraphs-1.11,P18-1151,0,0.0150253,"rs. Dyer et al. (2015); Socher et al. (2013); Tai et al. (2015); Zhu et al. (2015); Le and Zuidema (2014) encoded tree structure with recursive neural networks or Tree-LSTMs. Surface Realization Song et al. (2018) introduced a graph-to-sequence LSTM for AMR-totext generation that can encode AMR structures directly. The model takes multiple recurrent transition steps in order to propagate information beyond local neighbourhoods. But this method must maintain the entire graph state at each time step. Our models also simultaneously update every node in the tree at every time step. The encoder of Trisedya et al. (2018) takes input RDF triples rendered as a graph and builds a dynamic recurrent structure that traverses the adjacency matrix of the graph one node at a time. Marcheggiani and Perez-Beltrachini (2018), again using a GCN, take only the nodes of the RDF graph as input, using the edges directly as a weight matrix. They, too, must update the entire graph at every time step. et al., 2014), and convolutional networks (Gehring et al., 2017) and transformers (Devlin et al., 2019). An earlier, but ultimately unsuccessful attempt at dislodging n-gram language models was that of Chelba (2000), who augmented"
C10-2177,J97-1003,0,0.389377,"2 0.3 0.4 Word error rate WindowDiff under different WERs B−ALN HG−ALN AUDIO 0.4 WindowDiff utilizes the hierarchical structures of slides and global distribution of words, i.e., the HG-ALN model, reduces both Pk and WindowDiff scores over the baseline model, B-ALN. As discussed earlier, the baseline is a re-implementation of standard dynamic time warping based only on a pre-order walk of the slides, while the HG-ALN model incorporates also hierarchical bullet constraints and global word distribution. Table 1 also presents the performance of a typical topic segmentation algorithm, TextTiling (Hearst, 1997). Note that similar to (Malioutov et al., 2007), we force the number of predicted topic segments to be the target number, i.e., in our task, the number of bullets. The results show that both the Pk and WindowDiff scores of TextTiling are significantly higher than those of the alignment algorithms. Our manual analysis suggests that many segments are as short as several utterances and the difference between two consecutive segments is too subtle to be captured by a lexical cohesionbased method such as TextTiling. For comparison, We also present the results of uniform segmentation (UNI), which si"
C10-2177,W06-1644,0,0.33355,"Missing"
C10-2177,J02-4006,0,0.125056,"w the tree structure in advance and therefore we know that the starting position of the next sibling bullet is the ending boundary for the current bullet. 4 Our approaches Our task is to find the correspondence between slide bullets and a speech sequence or its transcripts. Research on finding correspondences between parallel texts pervades natural language processing. For example, aligning bilingual sentence pairs is an essential step in training machine translation models. In text summarization, the correspondence between human-written summaries and their original texts has been identified (Jing, 2002), too. In speech recognition, forced alignment is applied to align speech and transcripts. In this paper, we keep the general framework of alignment in solving our problem. Our solution, however, should be flexible to consider multiple constraints such as those conveyed in hierarchical bullet structures and global word distribution. Accordingly, the model proposed in this paper depends on two orthogonal strategies to ensure efficiency and richness of the model. First of all, we formulate all our solutions within a classic dynamic programming framework to enforce computational efficiency (secti"
C10-2177,P07-1064,0,0.360242,"gnores the hierarchical structure of bullets within slides. We also explore the impact of speech recognition errors on this task. Furthermore, we study the feasibility of directly aligning a structure to raw speech, as opposed to a transcript. 2 Related work Topic/slide boundary detection The previous work most directly related to ours is research that attempts to find flat structures of spoken documents, such as topic and slide boundaries. For example, the work of (Chen and Heng, 2003; Ruddarraju, 2006; Zhu et al., 2008) aims to find slide boundaries in the corresponding lecture transcripts. Malioutov et al. (2007) developed an approach to detecting topic boundaries of lecture recordings by finding repeated acoustic patterns. None of this work, however, has involved hierarchical structures that exist at different levels of a document. In addition, researchers have also analyzed other multimedia channels, e.g., video (Liu et al., 2002; Wang et al., 2003; Fan et al., 2006), to detect slide transitions. Such approaches, however, are unlikely to find semantic structures that are more detailed than slide transitions, e.g., the bullet hierarchical structures that we are interested in. Building tables-of-conte"
C10-2177,A00-2025,0,0.595888,"rarchical and global features and the improvement is consistent on transcripts with different WERs. Directly imposing such hierarchical structures onto raw speech without using transcripts yields competitive results. The efficiency and convenience of reading spoken documents are affected by at least two facts. First, the quality of transcripts can impair browsing efficiency, e.g., as shown in (Stark et al., 2000; Munteanu et al., 2006), though if the goal is only to browse salient excerpts, recognition errors on the extracts can be reduced by considering the confidence scores assigned by ASR (Zechner and Waibel, 2000; Hori and Furui, 2003). 1 Introduction Though speech has long served as a basic method of human communication, revisiting and browsing speech content had never been a possibility before human can record their own voice. Recent technological advances in recording, compressing, and distributing such archives have led to the consistently increasing availability of spoken content. Along with this availability comes a demand for better ways to browse such archives, which is inherently more difficult than browsing text. In relying on human beings’ ability to browse text, a solution is therefore to"
C10-2177,J02-1002,0,0.448335,"Missing"
C10-2177,P07-1069,0,\N,Missing
C12-1051,copestake-flickinger-2000-open,0,0.0120987,"ubsumed by both of the argument feature structures’ types.1 Early TFS-based parsing systems like ALE (Carpenter and Penn, 1996) required that every pair of unifiable types have a least common supertype, i.e., joins are uniquely defined wherever they need to be defined. Type hierarchies that have this requirement are called meet semi-lattices (MSLs) (Davey and Priestley, 2002), because the existence of a meet for every pair of types is sufficient to guarantee the existence of a join for every unifiable pair of types when the type hierarchy is finite. Later HPSG parsing systems such as the LKB (Copestake and Flickinger, 2000) and PET (Callmeier, 2000) eliminated this restriction, and it is now extremely rare to find a decent-sized HPSG with an MSL type hierarchy. The LKB, at least in some early versions, used an on-line caching algorithm to add joins to the user-defined type system when necessary. PET adds all of the necessary joins in advance, during a grammar compilation stage. In either case, the time it costs to perform these repairs is proportional to the number of new joins that must be added. It therefore makes some sense to strive to add the least number of types necessary. While modern HPSG type systems a"
C12-2092,J94-3001,0,0.393822,"ce. With either acyclicity or N¯ılakan.t.had¯ıks.itar’s condition (it does not matter which), all input is passed through unchanged, even when it contains caad or cbbd. In this rule system, however: aa −→ b b / b __ a b −→ a / b __ a b −→ a / __ b b N¯ılakan.t.had¯ıks.itar’s condition would not be sufficient to prevent cyclic rule applications. On the input string baaaa, for example, acyclicity produces baaaa and abaaa, whereas N¯ılakan.t.had¯ıks.itar’s condition allows baaaa, aabaa, babaa, and abbaa. With neither condition in force, the system produces aabaa and babaa. It is interesting that Kaplan and Kay (1994), in their improved presentation of Johnson’s 1970 result, present many examples where φ and ψ are sets of larger cardinality than 1, but not even one where they contain a string of greater length than 1. String length is essential to our understanding of the effects of N¯ılakan.t.had¯ıks.itar’s condition on contextualized replacement systems. ¯dhy¯ ay¯ı Replacement string length in the As.t.a 5 ¯dhy¯ The rules of the As.t.a ay¯ı use input and output strings of length greater than 1, but it is clear that these sequences can and often do have derivational histories attached to them, i.e., not j"
D10-1003,P05-1022,0,0.155039,"Missing"
D10-1003,C00-1027,0,0.0130438,"information to aid parsing, especially for structural dependencies between sentences, such as parallelism effects. tactic structure and the distance since its last occurrence, which indicates syntactic consistency. These studies, however, do not provide consistency results on subsets of production-types, such as by production LHS as our study does, so the implications that can be drawn from them for improving parsing are less apparent. We adopt the measure used by Dubey et al. (2005) to quantify syntactic consistency, adaptation probability. This measure originates in work on lexical priming (Church, 2000), and quantifies the probability of a target word or construction w appearing in a “primed” context. Specifically, four frequencies are calculated, based on whether the target construction appears in the previous context (the prime set), and whether the construction appears after this context (the target set): 2 Syntactic Consistency in the Penn Treebank WSJ Syntactic consistency has been examined by Dubey et al. (2005) for several English corpora, including the WSJ, Brown, and Switchboard corpora. They have provided evidence that syntactic consistency exists not only within coordinate structu"
D10-1003,W05-0622,0,0.0207686,"ge of candidate parses which share more productions with the previous parse are better than the generative baseline parse than for the other categories, and this difference is statistically significant (χ2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T } given a sequence of observed output x = x{t=1...T } and weight vector θ = θ{k=1...K} is given as follows: T X X 1 θk fk (yt−1 , yt , x, t)) P (y|x) = exp( Z t=1 k where Z is the partition function. The feature functions fk (yt−1 , yt , x, t) can depend on two"
D10-1003,J05-1003,0,0.139605,"Missing"
D10-1003,H05-1104,0,0.101635,"use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies. 1 Introduction Recent corpus linguistics work has produced evidence of syntactic consistency, the preference to reuse a syntactic construction shortly after its appearance in a discourse (Gries, 2005; Dubey et al., 2005; Reitter, 2008). In addition, experimental studies have confirmed the existence of syntactic priming, the psycholinguistic phenomenon of syntactic consistency1 . Both types of studies, however, have 1 Whether or not corpus-based studies of consistency have any bearing on syntactic priming as a reality in the human mind limited the constructions that are examined to particular syntactic constructions and alternations. For instance, Bock (1986) and Gries (2005) examine specific constructions such as the passive voice, dative alternation and particle placement in phrasal verbs, and Dubey et al."
D10-1003,P06-1053,0,0.0175089,"onsistent production-types have NP as the LHS, but overall, productions with many different LHS parents exhibit consistency. 3 A Context-Aware Reranker Having established evidence for widespread syntactic consistency in the WSJ corpus, we now investigate incorporating extra-sentential context into a statistical parser. The first decision to make is whether to incorporate the context into a generative or a discriminative parsing model. Employing a generative model would allow us to train the parser in one step, and one such parser which incorporates the previous context has been implemented by Dubey et al. (2006). They implement a PCFG, learning the production probabilities by a variant of standard PCFG-MLE probability estimation that conditions on whether a rule has recently occurred in the context or not: P (RHS|LHS, P rime) = c(LHS → RHS, P rime) c(LHS, P rime) LHS and RHS represent the left-hand side and 26 right-hand side of a production, respectively. Prime is a binary variable which is True if and only if the current production has occurred in the prime set (the previous sentence). c represents the frequency count. The drawback of such a system is that it doubles the state space of the model, a"
D10-1003,P08-1109,0,0.3334,"deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models would provide further imp"
D10-1003,P07-1086,0,0.0446182,"Missing"
D10-1003,E09-1047,0,0.0293061,"Missing"
D10-1003,N06-1020,0,0.0374066,"nd Dubey et al. (2005) deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models wou"
D10-1003,N07-1051,0,0.0376031,"ment in phrasal verbs, and Dubey et al. (2005) deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that comb"
D10-1003,N03-1028,0,0.0731368,"arse are better than the generative baseline parse than for the other categories, and this difference is statistically significant (χ2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T } given a sequence of observed output x = x{t=1...T } and weight vector θ = θ{k=1...K} is given as follows: T X X 1 θk fk (yt−1 , yt , x, t)) P (y|x) = exp( Z t=1 k where Z is the partition function. The feature functions fk (yt−1 , yt , x, t) can depend on two neighbouring parses, the sentences in the sequence, and the position o"
D10-1003,E09-1090,0,0.0119953,"ories, and this difference is statistically significant (χ2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T } given a sequence of observed output x = x{t=1...T } and weight vector θ = θ{k=1...K} is given as follows: T X X 1 θk fk (yt−1 , yt , x, t)) P (y|x) = exp( Z t=1 k where Z is the partition function. The feature functions fk (yt−1 , yt , x, t) can depend on two neighbouring parses, the sentences in the sequence, and the position of the sentence in the sequence. Since our feature functions do not depend o"
D14-1085,A00-2024,0,0.0271803,"ke this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013). This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements. 2 shown in Figure 2, the phrase of food-borne illness can be added to the previous output sentence, despite originating in a source text sentence that is quite different overall. Elsner and"
D14-1085,J05-3002,0,0.251702,"ake the union or intersection of the information present therein. In this paper, we move further along this path in the following ways. First, we present sentence enhancement as a novel technique which extends sentence fusion by combining the subtrees of many sentences into the output sentence, rather than just a few. Doing so allows relevant information from sentences that are not similar to the original input sentences to be added during fusion. As Introduction Sentence fusion is the technique of merging several input sentences into one output sentence while retaining the important content (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013). For example, the input sentences in Figure 1 may be fused into one output sentence. As a text-to-text generation technique, sentence fusion is attractive because it provides an avenue for moving beyond sentence extraction in automatic summarization, while not requiring deep se775 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 775–786, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Source text: This fact has been underscored in the last few months"
D14-1085,P10-1143,0,0.0138204,"signed a simple method to approximate event coreference resolution that does not require event coreference labels. This method is based on the intuition that different mentions of an event should contain many of the same participants. Thus, by measuring the similarity of the arguments and the syntactic contexts between the node in the sentence graph and the candidate edge, we can have a measure of the likelihood that they refer to the same event. We would be interested in integrating existing event coreference resolution systems into this step in the future, such as the unsupervised method of Bejan and Harabagiu (2010). Existing event coreference systems tend to focus on cases with different heads (e.g., X kicked Y, then Y was injured), which could increase the possibilities for sentence enhancement, if the event coreference module is sufficiently accurate. However, since our method currently only merges identical heads, we require a more fine-grained method based on distributional measures of similarity. We measure the similarity of these syntactic contexts by aligning the arguments in the syntactic contexts and computing the similarity of the aligned arguments. These problems can be jointly solved as a ma"
D14-1085,C00-1072,0,0.0201224,"ustering, which requires a measure of similarity between sentences and a stopping criterion. We define the similarity between two sentences to be the standard cosine similarity between the lemmata of the sentences, weighted by IDF and excluding stopwords, and clustering is run until a similarity threshold of 0.5 is reached. Since complete-link clustering prefers small coherent clusters and we select the top-scoring cluster in each document collection, the method is somewhat robust to different choices of the stopping threshold. The clusters are scored according to the signature term method of Lin and Hovy (2000), which assigns an importance score to each term accordSentence graph creation After core sentence identification, the next step is to align the nodes of the dependency trees of the core input sentences in order to create the initial sentence graph. The input to this step is the collapsed dependency tree representations of the core sentences produced by the Stanford parser1 . In this representation, preposition nodes are collapsed into the label of the dependency edge between the functor of the prepositional phrase and the prepositional object. Chains of conjuncts are also split, and each argu"
D14-1085,P13-1121,1,0.932664,"by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013). This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements. 2 shown in Figure 2, the phrase of food-borne illness can be added to the previous output sentence, despite originating in a source text sentence that is quite different overall. Elsner and Santhanam (2011) proposed a supervised method to fuse disparate sentences, which takes as input a small number of sentences with compatible information that have been manually identified by editors of articles. By contrast, our algorithm is unsupervised, and tackles the problem"
D14-1085,W05-1612,0,0.0772574,"l contexts towards more dramatic reformulations of the kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised a"
D14-1085,E06-1038,0,0.104556,"s. Output: The outbreak of food-borne illness led to the recall on Tuesday of lots of hot dogs and meats produced at the Bil Mar Foods plant. using local contexts towards more dramatic reformulations of the kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding informatio"
D14-1085,C08-1018,0,0.060764,"ess led to the recall on Tuesday of lots of hot dogs and meats produced at the Bil Mar Foods plant. using local contexts towards more dramatic reformulations of the kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occur"
D14-1085,W11-1611,0,0.0233177,"Missing"
D14-1085,W11-1607,0,0.255482,"apata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013). This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements. 2 shown in Figure"
D14-1085,W12-3018,0,0.016449,"Missing"
D14-1085,D08-1019,0,0.0563558,"n of the information present therein. In this paper, we move further along this path in the following ways. First, we present sentence enhancement as a novel technique which extends sentence fusion by combining the subtrees of many sentences into the output sentence, rather than just a few. Doing so allows relevant information from sentences that are not similar to the original input sentences to be added during fusion. As Introduction Sentence fusion is the technique of merging several input sentences into one output sentence while retaining the important content (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013). For example, the input sentences in Figure 1 may be fused into one output sentence. As a text-to-text generation technique, sentence fusion is attractive because it provides an avenue for moving beyond sentence extraction in automatic summarization, while not requiring deep se775 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 775–786, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Source text: This fact has been underscored in the last few months by two unexpected outbreaks"
D14-1085,P06-1055,0,0.0301786,"the sentence graph is expanded, the large increase in the oracle score indicates the potential of sentence enhancement for generating high-quality summary sentences. Results and discussion As shown in Table 1, sentence enhancement with coreference outperforms the sentence fusion algorithm of F&S in terms of the Pyramid BE measure and the baseline expansion algorithm, though only the latter difference is statistically significant (p = 0.0196 ). In terms of the ROUGE word overlap 5 The likelihoods are obtained by the PCFG model of CoreNLP version 1.3.2. We experimented with the Berkeley parser (Petrov et al., 2006) as well, with similar results that favour the sentence enhancement with event coreference method, but because the parser failed to parse a number of cases, we do not report those results here. 6 All statistical significance results in this section are for Wilcoxon signed-rank tests. 7 There is no guarantee that these dependency triples form a tree structure. Hence, this is an upper bound. 781 supply such an analysis and provide evidence that human summary writers actually do incorporate elements external to the source text for a reason, namely, that these elements are more specific to the sem"
D14-1085,C10-1037,0,0.0306549,"e kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studie"
D14-1085,J02-4005,0,0.011992,"ce enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notions of information centrality (Cheung and Penn, 2013). This paper extends this work by identifying specific linguistic factors correlated with the use of source-text-external elements. 2 shown in Figure 2, the phrase of food-borne illness can be added to the previous output sentence, despite originating in a source text sentence that is quite different overall. Elsner and Santhanam (2011) proposed a"
D14-1085,N07-1023,0,0.00992592,"utbreak of food-borne illness led to the recall on Tuesday of lots of hot dogs and meats produced at the Bil Mar Foods plant. using local contexts towards more dramatic reformulations of the kind that human writers perform, more semantic analysis will be needed in order to ensure that the reformulations preserve the inferences that can be drawn from the input text. Figure 2: An example of sentence enhancement, in which parts of dissimilar sentences are incorporated into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences"
D14-1085,I13-1198,0,0.657721,"therein. In this paper, we move further along this path in the following ways. First, we present sentence enhancement as a novel technique which extends sentence fusion by combining the subtrees of many sentences into the output sentence, rather than just a few. Doing so allows relevant information from sentences that are not similar to the original input sentences to be added during fusion. As Introduction Sentence fusion is the technique of merging several input sentences into one output sentence while retaining the important content (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013). For example, the input sentences in Figure 1 may be fused into one output sentence. As a text-to-text generation technique, sentence fusion is attractive because it provides an avenue for moving beyond sentence extraction in automatic summarization, while not requiring deep se775 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 775–786, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Source text: This fact has been underscored in the last few months by two unexpected outbreaks of food-borne illness. Outpu"
D14-1085,S12-1034,0,0.050118,"Missing"
D14-1085,D08-1057,0,0.0224337,"into the output sentence. A relatively large body of work exists in sentence compression (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Clarke and Lapata, 2008, inter alia), and sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008; Filippova, 2010; Thadani and McKeown, 2013). Unlike this work, our sentence enhancement algorithm considers the entire source text and is not limited to the initial input sentences. Few previous papers focus on combining the content of diverse sentences into one output sentence. Wan et al. (2008) propose sentence augmentation by identifying “seed” words in a single original sentence, then adding information from auxiliary sentences based on word co-occurrence counts. Elsner and Santhanam (2011) investigate the idea of fusing disparate sentences with a supervised algorithm, as discussed above. Previous studies on cut-and-paste summarization (Jing and McKeown, 2000; Saggion and Lapalme, 2002) investigate the operations that human summarizers perform on the source text in order to produce the summary text. Our previous work argued that current extractive systems rely too heavily on notio"
E03-1028,P02-1009,1,\N,Missing
E03-1039,P85-1015,0,0.749799,"s topologically into fields: Definition 3 Given a topological signature, a phenogrammatical rule is of the form r d1... 04,, where r E Region, the di Desc(Field), and n &gt; 0. P &gt; When we look at the parse tree that corresponds to a derivation with a set of pheno-rules over some string, we see that every field and region can account for some contiguous substring that its subtree dominates. This is called the yield of that field or region. We can extend this notion of yield to tectogrammatical categories, although the substrings that correspond to these may not be contiguous. We could, following Johnson (1985), think of yields as bit vectors defined over a fixed length corresponding to the length of the input, for example. Structural constraints constrain pheno-yields in terms of tecto-yields and vice versa. We look at them in terms of whether one covers another, i.e., substring inclusion. Definition 4 Given a topological signature, the structural constraints, C, over that signature are, for every 0 E L, f E Field, and r E Region: 0 covers f, covers r, • covering: f covered_by 0, r covered_by 0, • matching: 0 matches f, 0 matches r, f matched_by 0, r matched_by 0, • linkage: rkf,f ,ri, • compaction"
E03-1039,E95-1023,0,0.0301124,"a tectogrammatical rule. In the absence of any indexed constraints, a tecto-rule makes no assumptions about the linear relationships among its daughters. Relative precedence and immediate precedence can be used to describe traditional phrase structure, where it exists, which can also be provided as an idiom: cb o &gt; 0 1 ... O n . Note that, as with traditional ID/LP, compaction can be specified in the absence of precedence, which serves to specify contiguity separately from linear order; unlike ID/LP, precedence can be specified in the absence of contiguity (Goetz and Penn, 1997; Suhre, 1999). Manandhar (1995) has a similar approach to linear precedence. 4 Parsing Just as with CFGs, there are a number of different control strategies that could be imagined for parsing with this topological formalism. The one presented here incorporates elements that are reminiscent of naive bottom-up, top-down and left-corner parsing. Information about headedness or statistically estimated parameters would be incorporated into a more sophisticated large-scale parser. For simplicity, the exposition here assumes that for every field or region, f I r, there is at most one structural constraint of each variety that univ"
E03-1039,C92-1031,0,0.0407998,"is different about these extended CFGs is that they do not provide interpretations — only a parse into linearly defined fields and regions. The present formalism consists of two parallel representational devices, one being this extended CFG and the other, an interpretive tectogrammatical tree structure with potentially crossing links. Along with these come constraints that associate substructures from the two representations, in a very similar spirit to LFG structural correspondence functions. The idea of using topological fields as a guide for general parsing appears to have originated with Oliva (1992); more recent work primarily folds in parochial facts from German, including Duchier (2001), which presents German topological parsing as a constraint satisfaction problem. The present approach actually received its inspiration initially from Slavic language wordorder data, but can be applied equally well to German. Synchronous tree-adjoining grammars (Shieber and Schabes, 1990) bear some resemblance to the parallel derivations used here, although the same constituents are used there in both. 3 Formalism We can state three characterizing assumptions that restrict the expressive power of this f"
E03-1039,C90-3045,0,0.144543,"ints that associate substructures from the two representations, in a very similar spirit to LFG structural correspondence functions. The idea of using topological fields as a guide for general parsing appears to have originated with Oliva (1992); more recent work primarily folds in parochial facts from German, including Duchier (2001), which presents German topological parsing as a constraint satisfaction problem. The present approach actually received its inspiration initially from Slavic language wordorder data, but can be applied equally well to German. Synchronous tree-adjoining grammars (Shieber and Schabes, 1990) bear some resemblance to the parallel derivations used here, although the same constituents are used there in both. 3 Formalism We can state three characterizing assumptions that restrict the expressive power of this formalism: • Topological Linearity: all word-order constraints can be witnessed by a topology defined on some linear region. • Topological Locality: discontiguities may exist due to scrambling, but they are not unbounded. 1 Hence all discontiguities can be characterized in some local region of bounded topological size. • Qualified Isomorphy: While linear and local regions are not"
E03-1039,J03-4003,0,\N,Missing
E03-1039,W90-0102,0,\N,Missing
E12-1005,D10-1115,0,0.03558,"approach does not outperform various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for di"
E12-1005,P07-1073,0,0.022045,"we trained the syntactic models over the AFP subset of Gigaword (~338M words). We also trained the other two models on just the AFP portion for comparison. Note that the AFP portion of Gigaword is three times larger than the BNC corpus (~100M words), on which several previous syntactic models were trained. Because our main goal is to test the general performance of the models and to demonstrate the feasibility of our evaluation methods, we did not further tune the parameter settings to each of the tasks, as doing so would likely only yield minor improvements. 4.3 Task 1 We used the dataset by Bunescu and Mooney (2007), which we selected because it contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets such as the one by Roth and Yih (2002). Controlling for the target entity pair in this manner makes the task more difficult, because the semantic model cannot make use of distributional information about the entity pair in inference. The dataset is separated into subsets depending on the target binary relation (Company X acquires Company Y or Person X was born in Place Y) and the entity pair (e.g., Yahoo and Inktomi) (Table 2). The dataset was constructed semia"
E12-1005,D10-1113,0,0.191836,"to two degrees. For example, the vector for catch might contain a dimension labelled (OBJ,OBJ-1,throw), which indicates the strength of connection between the two verbs through all of the cooccurring direct objects which they share. Unlike E&P, TFP’s model encodes the selectional preferences in a single vector using frequency counts. We extend the model to the sentence level with component-wise addition and multiplication, and word vectors are contextualized by the dependency neighbours. We use a frequency threshold of 10 and a pmi threshold of 2 to prune infrequent word and dependencies. D&L Dinu and Lapata (2010) (D&L) assume a global set of latent senses for all words, and models each word as a mixture over these latent senses. The vector for a word ti in the context of a word cj is modelled by v(ti , cj ) = P (z1 |ti , cj ), ...P (zK |ti , cj ) (19) where z1...K are the latent senses. By making independence assumptions and decomposing probabilities, training becomes a matter of estimating the probability distributions P (zk |ti ) and P (cj |zk ) from data. While Dinu and Lapata (2010) describe two methods to do so, based on non-negative matrix factorization and latent Dirichlet allocation, the perfo"
E12-1005,I05-5002,0,0.0224152,"terested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it appears likely that subjects conflated lexical, syntactic, and semantic relatedness. Dolan and Brockett (2005) define paraphrase detection as identifying sentences that are in a bidirectional entailment relation. While such sentences do support exactly the same inferences, we are also interested in the inferences that can be made from similar sentences that are not paraphrases according to this strict definition — a situation that is more often encountered in end applications. Thus, we adopt a less restricted notion of paraphrasis. 3 An Evaluation Framework We now describe a simple, general framework for evaluating semantic models. Our framework consists of the following components: a semantic model t"
E12-1005,D08-1094,0,0.278587,"Missing"
E12-1005,P10-2017,0,0.043097,"Missing"
E12-1005,D11-1129,0,0.0733609,"t word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional models of semantics which build phras"
E12-1005,W10-2805,0,0.0217593,"various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional mo"
E12-1005,P08-1028,0,0.774921,"and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition. We propose two evaluation methods in relation classification and QA which reflect these goals, and apply several recent compositional distributional models to the tasks. We find that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference. 1 Introduction A number of unsupervised semantic models (Mitchell and Lapata, 2008, for example) have recently been proposed which are inspired at least in part by the distributional hypothesis (Harris, 1954)—that a word’s meaning can be characterized by the contexts in which it appears. Such models represent word meaning as one or more high-dimensional vectors which capture the lexical and syntactic contexts of the word’s occurrences in a training corpus. Much of the recent work in this area has, following Mitchell and Lapata (2008), focused on the notion of compositionality as the litmus test of a truly semantic model. Compositionality is a natural way to construct repres"
E12-1005,D09-1045,0,0.0214476,"l and the results. 4.1 Distributional Semantic Models We tested four recent distributional models and a lemma overlap baseline, which we now describe. We extended several of the models to compositionally construct phrase representations using component-wise vector addition and multiplication, as we note below. Since the focus of this paper is on evaluation methods for such models, we did not experiment with other compositionality 37 operators. We do note, however, that componentwise operators have been popular in recent literature, and have been applied across unrestricted syntactic contexts (Mitchell and Lapata, 2009), so there is value in evaluating the performance of these operators in itself. The models were trained on the Gigaword corpus (2nd ed., ~2.3B words). All models use cosine similarity to measure the similarity between representations, except for the baseline model. Lemma Overlap This baseline simply represents a sentence as the counts of each lemma present in the sentence after removing stop words. Let a sentence x consist of lemma-tokens m1 , . . . , m|x |. The similarity between two sentences is then defined as M(x, x′ ) = #In(x, x′ ) + #In(x′ , x) (15) #In(x, x′ ) = |x| X 1x′ (mi ) (16) i=1"
E12-1005,D09-1001,0,0.476793,"nonical form. The traditional justification for canonical forms is that they allow easy access to a knowledge base to retrieve some desired information, which amounts to a form of inference. Our work can be seen as an extension of this notion to distributional semantic models with a more general notion of representational similarity and inference. There are many regular alternations that semantics models have tried to account for such as passive or dative alternations. There are also many lexical paraphrases which can take drastically different syntactic forms. Take the following example from Poon and Domingos (2009), in which the same semantic relation can be expressed by a transitive verb or an attributive prepositional phrase: (1) Utah borders Idaho. Utah is next to Idaho. In distributional semantics, the original sentence similarity test proposed by Kintsch (2001) served as the inspiration for the evaluation performed by Mitchell and Lapata (2008) and most later work in the area. Intransitive verbs are given 34 in the context of their syntactic subject, and candidate synonyms are ranked for their appropriateness. This method targets the fact that a synonym is appropriate for only some of the verb’s se"
E12-1005,N10-1013,0,0.085278,"e representation of a in context, a′ , is given by a′ = va ⊙ Rb (r −1 ) X Rb (r) = f (c, r, b) · vc , (17) (18) c:f (c,r,b)>θ where Rb (r) is the vector describing the selectional preference of word b in relation r, f (c, r, b) is the frequency of this dependency triple, θ is a frequency threshold to weed out uncommon dependency triples (10 in our experiments), and ⊙ is a vector combination operator, here componentwise multiplication. We extend the model to compute sentence representations from the contextualized word vectors using component-wise addition and multiplication. TFP Thater et al. (2010)’s model is also sensitive to selectional preferences, but to two degrees. For example, the vector for catch might contain a dimension labelled (OBJ,OBJ-1,throw), which indicates the strength of connection between the two verbs through all of the cooccurring direct objects which they share. Unlike E&P, TFP’s model encodes the selectional preferences in a single vector using frequency counts. We extend the model to the sentence level with component-wise addition and multiplication, and word vectors are contextualized by the dependency neighbours. We use a frequency threshold of 10 and a pmi thr"
E12-1005,C02-1151,0,0.0344252,"on of Gigaword is three times larger than the BNC corpus (~100M words), on which several previous syntactic models were trained. Because our main goal is to test the general performance of the models and to demonstrate the feasibility of our evaluation methods, we did not further tune the parameter settings to each of the tasks, as doing so would likely only yield minor improvements. 4.3 Task 1 We used the dataset by Bunescu and Mooney (2007), which we selected because it contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets such as the one by Roth and Yih (2002). Controlling for the target entity pair in this manner makes the task more difficult, because the semantic model cannot make use of distributional information about the entity pair in inference. The dataset is separated into subsets depending on the target binary relation (Company X acquires Company Y or Person X was born in Place Y) and the entity pair (e.g., Yahoo and Inktomi) (Table 2). The dataset was constructed semiautomatically using a Google search for the two entities in order with up to seven content words in between. Then, the extracted sentences were hand-labelled with whether the"
E12-1005,P10-1093,0,0.0236113,"ll (2011) uses potential paraphrases directly as dimensions in his expectation vectors. Unfortunately, this approach does not outperform various context word-based approaches in two phrase similarity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase"
E12-1005,P10-1097,0,0.0483242,"Missing"
E12-1005,W11-0130,0,0.0693543,"systems, not an evaluation of phrase representations. Parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure (Socher et al., 2010; Wu and Schuler, 2011). However, syntax does not always reflect semantic content, and we are specifically interested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it appears likely that subjects conflated lexical, syntactic, and semantic relatedness. Dolan and Brockett (2005) define paraphrase detection as identifying sentences that are in a bidirectional entailment relation. While such sentences do support exactly the same inferences, we are also interested in the inferences that can be made from similar sentences that are not paraphrases according to this strict d"
E12-1005,W11-0131,0,0.0135379,"is dataset mostly for parameter tuning. Another is the lexical paraphrase task of McCarthy and Navigli (2009), in which words are given in the context of the surrounding sentence, and the task is to rank a given list of proposed substitutions for that word. The list of substitutions as well as the correct rankings are elicited from annotators. This task was originally conceived as an applied evaluation of WSD systems, not an evaluation of phrase representations. Parsing accuracy has been used as a preliminary evaluation of semantic models that produce syntactic structure (Socher et al., 2010; Wu and Schuler, 2011). However, syntax does not always reflect semantic content, and we are specifically interested in supporting syntactic invariance when doing semantic inference. Also, this type of evaluation is tied to a particular grammar formalism. The existing evaluations that are most similar in spirit to what we propose are paraphrase detection tasks that do not assume a restricted syntactic context. Washtell (2011) collected human judgments on the general meaning similarity of candidate phrase pairs. Unfortunately, no additional guidance on the definition of “most similar in meaning” was provided, and it"
E12-1005,D11-1016,0,0.0205655,"ity tasks. In terms of the vector composition function, component-wise addition and multiplication are the most popular in recent work, but there exist a number of other operators such as tensor product and convolution product, which are reviewed by Widdows (2008). Instead of vector space representations, one could also use a matrix space representation with its much more expressive matrix operators (Rudolph and Giesbrecht, 2010). So far, however, this has only been applied to specific syntactic contexts (Baroni and Zamparelli, 2010; Guevara, 2010; Grefenstette and Sadrzadeh, 2011), or tasks (Yessenalina and Cardie, 2011). Neural networks have been used to learn both phrase structure and representations. In Socher et al. (2010), word representations learned by neural network models such as (Bengio et al., 2006; Collobert and Weston, 2008) are fed as input into a recursive neural network whose nodes represent syntactic constituents. Each node models both the probability of the input forming a constituent and the phrase representation resulting from composition. 6 Conclusions We have proposed an evaluation framework for distributional models of semantics which build phrase- and sentence-level representations, an"
E12-1071,W10-3110,0,0.0255613,"Missing"
E12-1071,P10-2046,0,0.660242,"ational methods for detecting DEOs from a corpus. They proposed two unsupervised algorithms which rely on the correlation between DEOs and negative polarity items (NPIs), which by the definition of Ladusaw (1980) must appear in the context of DEOs. An example of an NPI is yet, as in the sentence This project is not complete yet. The first baseline method proposed by DLD09 simply calculates a ratio of the relative frequencies of a word in NPI contexts versus in a general corpus, and the second is a distillation method which appears to refine the baseline ratios using a task-specific heuristic. Danescu-Niculescu-Mizil and Lee (2010) (henceforth DL10) extend this approach to Romanian, where a comprehensive list of NPIs is not available, by proposing a bootstrapping approach to co-learn DEOs and NPIs. DLD09 are to be commended for having identified a crucial component of inference that nevertheless lends itself to a classification-based ap696 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 696–705, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics proach, as we will show. However, as noted by DL10, the performance of the"
E12-1071,N09-1016,0,0.447578,"ailing, allowing reasoning from a set of events to a superset of events as seen in (1). In the scope of a downward-entailing operator (DEO), however, this entailment relation is reversed, such as in the scope of the classical DEO not (2). There are also operators which are neither upward- nor downward entailing, such as the expression exactly three (3). (1) She sang in French. ⇒ She sang. (upward-entailing) (2) She did not sing in French. ⇐ She did not sing. (downward-entailing) (3) Exactly three students sang. 6⇔ Exactly three students sang in French. (neither upward- nor downward-entailing) Danescu-Niculescu-Mizil et al. (2009) (henceforth DLD09) proposed the first computational methods for detecting DEOs from a corpus. They proposed two unsupervised algorithms which rely on the correlation between DEOs and negative polarity items (NPIs), which by the definition of Ladusaw (1980) must appear in the context of DEOs. An example of an NPI is yet, as in the sentence This project is not complete yet. The first baseline method proposed by DLD09 simply calculates a ratio of the relative frequencies of a word in NPI contexts versus in a general corpus, and the second is a distillation method which appears to refine the base"
E12-1071,H92-1045,0,0.141661,"be insufficient from a linguistic perspective, it is nevertheless a useful starting point for computational methods for detecting NPIs and DEOs, and has inspired successful techniques to detect DEOs, like the work by DLD09, DL10, and also this work. In addition to this hypothesis, we further assume that there should only be one plausible DEO candidate per NPI context. While there are counterexamples, this assumption is in practice very robust, and is a useful constraint for our learning algorithm. An analogy can be drawn to the one sense per discourse assumption in word sense disambiguation (Gale et al., 1992). The related—and as we will argue, more difficult—problem of detecting NPIs has also been studied, and in fact predates the work on DEO detection. Hoeksema (1997) performed the first corpus-based study of NPIs, predominantly for Dutch, and there has also been work on detecting NPIs in German which assumes linguistic knowledge of licensing contexts for NPIs (Lichte and Soehn, 2007). Richter et al. (2010) make this assumption as well as use syntactic structure to extract NPIs that are multi-word expressions. Parse information is an especially important consideration in freer-word-order language"
E12-1071,C08-1066,0,0.0317936,"a long-standing challenge in NLP, and there has been considerable debate both on what constitutes inference and what techniques should be used to support inference. One task involving inference that has recently received much attention is that of recognizing textual entailment (RTE), in which the goal is to determine whether a hypothesis sentence can be entailed from a piece of source text (Bentivogli et al., 2010, for example). An important consideration in RTE is whether a sentence or context produces an entailment relation for events that are a superset or subset of the original sentence (MacCartney and Manning, 2008). By default, contexts are upward-entailing, allowing reasoning from a set of events to a superset of events as seen in (1). In the scope of a downward-entailing operator (DEO), however, this entailment relation is reversed, such as in the scope of the classical DEO not (2). There are also operators which are neither upward- nor downward entailing, such as the expression exactly three (3). (1) She sang in French. ⇒ She sang. (upward-entailing) (2) She did not sing in French. ⇐ She did not sing. (downward-entailing) (3) Exactly three students sang. 6⇔ Exactly three students sang in French. (nei"
I11-1057,J02-1002,0,0.0308484,"es on transcripts for each slide bullet are compared against the corresponding gold-standard boundaries to calculate offsets measured in number of words, counted after stopwords having been removed, which are then averaged over all boundaries to evaluate model performance. Though one may consider that different bullets may be of different importance, in this paper we do not use any heuristics to judge this and we treat all bullets equally in our evaluation. Note that topic segmentation research often uses metrics such as Pk and WindowDiff (Malioutov and Barzilay, 2006; Beeferman et al., 1999; Pevsner and Hearst, 2002). Our problem here, as an alignment problem, has an exact 1-to-1 correspondence between a gold and automatic boundary, in which we can directly measure the exact offset of each boundary. 2006), which denotes the normalized partition cost of the segment from utterance uj+1 to uk , inclusively. For complexity, since the cohesion model is O(M N 2 ), linearly combining it would not increase the time complexities of the corresponding polynomial alignment models, which are at least O(M N 2 ) by themselves. 7 Experiment Set-up Corpus Our experiment uses a corpus of four 50-minute university lectures"
I11-1057,J97-1003,0,0.0482495,"ed the HieBase model in the remainder of this paper. One major benefit of the deterministic hierarchical alignment models is their time complexity: still quadratic, same as the sequential alignment model discussed above, though models like HieCut can achieve a very competitive perfor6 The Topic-segmentation Model Up to now, we have discussed a variety of alignment models with different model capabilities and time complexities, which, however, consider only similarities between bullets and utterances. Cohesion in text or speech, by itself, often evidenced by the change of lexical distribution (Hearst, 1997), can also indicate topic or subtopic transitions, even among subtle subtopics (Malioutov and Barzilay, 2006). In our problem here, when a lecturer discusses a bullet, the words used are likely to be different from those used in another bullet, suggesting that the spoken documents themselves, when ignoring the alignment model above for the time being, could potentially indicate the semantic boundaries that we are interested in here. Particularly, the cohesion conveyed by the repetition of the words that appear in transcripts but not in slides could be additionally helpful; this is very likely"
I11-1057,W06-1644,0,0.018891,"ame as in (Malioutov et al., 2007), for which we split each lecture into M chunks, the number of bullets. Finally, we obtained a M-by-N bullet-utterance similarity matrix and a N-by-N utterance-utterance matrix to optimize the alignment model and topic-segmentation 8 Experimental Results Alignment Models Table 1 presents the experimental results obtained on the automatic transcripts generated by the ASR models discussed above, with WERs of 0.43 and 0.48, respectively, which are typical for lectures and conference presentations in realistic and less controlled situations (Leeuwis et al., 2003; Hsu and Glass, 2006; Munteanu et al., 2007). The results show that among the four quadratic models, i.e., the first four models in the table, HieCut achieves the best performance. The results also suggest that the improvement of HieCut over SeqBase comes from two aspects. First, the normalized-cut objective used in the graph-partitioning based model seems to outperform that used in the baseline, indicated by the better performance of SeqCut over SeqBase, since both take as input the same, sequentialized bullet sequence and the corresponding transcribed utterances. The DTW-based objective used in SeqBase correspo"
I11-1057,J02-4006,0,0.0196473,"nicity between transcripts and slide trees, which violates some basic properties of the problem that we will discuss. More recently, the work of (Zhu, 2011) proposes a graphpartitioning based model (revisited in Section 4) and shows that the model outperforms a bulletsequentializing model. 2 Related Work Alignment of parallel texts In general, research on finding correspondences between parallel texts pervades both spoken and written language processing, e.g., in training statistical machine translation models, identifying relationship between human-written summaries and their original texts (Jing, 2002), force-aligning speech and transcripts in ASR, and grounding text with database facts (Snyder and Barzilay, 2007; Chen and Mooney, 2008; Liang et al., 2009). Our problem here, however, is distinguished in several major aspects, which need to be considered in our modeling. First, it involves segmentation—alignment is conducted together with the decision of the corresponding segment boundaries on transcripts; in other words, we are not finally concerned with the specific utterances that a bullet is aligned to, but the region of utterances. In such a sense, graph partitioning seems intuitively t"
I11-1057,P09-1011,0,0.036298,", 2011) proposes a graphpartitioning based model (revisited in Section 4) and shows that the model outperforms a bulletsequentializing model. 2 Related Work Alignment of parallel texts In general, research on finding correspondences between parallel texts pervades both spoken and written language processing, e.g., in training statistical machine translation models, identifying relationship between human-written summaries and their original texts (Jing, 2002), force-aligning speech and transcripts in ASR, and grounding text with database facts (Snyder and Barzilay, 2007; Chen and Mooney, 2008; Liang et al., 2009). Our problem here, however, is distinguished in several major aspects, which need to be considered in our modeling. First, it involves segmentation—alignment is conducted together with the decision of the corresponding segment boundaries on transcripts; in other words, we are not finally concerned with the specific utterances that a bullet is aligned to, but the region of utterances. In such a sense, graph partitioning seems intuitively to be more relevant than models optimizing a full-alignment score. Second, unlike a string-to-string alignment task, the problem involves hierarchical tree st"
I11-1057,P06-1004,0,0.239262,"ing of the semantic tree-to-string alignment task. First of all, a basic question is associated with different ways of exploiting the semantic trees when performing alignment, which, as will be studied comprehensively in this paper, results in models of different modeling capabilities and time complexities. Second, all the models discussed above consider only similarities between bullets and transcribed utterances, while similarities among utterances, which directly underline a cohesion model, are generally ignored. We will show in this paper that the stateof-the-art topic-segmentation model (Malioutov and Barzilay, 2006) can be inherently incorporated into the graph-partitioning-based alignment models. Third, the different alignment objectives, e.g., that of the graph-partitioning models versus that of basic DTW-based models, are entangled together with different ways of exploiting the bullet tree structures in (Zhu, 2011). In this paper, we discuss two more quadratic-time models to bridge the gap. Specifically, this paper studies nine different models, with the aim to provide a comprehensive 510 utterance uj and ends at the kth, inclusively. Constrained by the tree structure, the transcript region correspond"
I11-1057,P07-1064,0,0.101654,"elivering, and even automatic transcription were possible. Navigating audio documents is often inherently much more difficult than browsing text. An obvious solution, in relying on human beings’ ability of reading text, is to conduct a speech-to-text conversion through ASR, which in turn raises a new set of problems to be considered. First, the convenience and efficiency of reading transcripts Semantic Structures of Spoken Documents Much previous work, similar to its written-text counterpart, has attempted to find certain flat structures of spoken documents such as topic and slide boundaries (Malioutov et al., 2007; Zhu et al., 2008), which, however, involve no hierarchical structures of a spoken document, thought as will be shown in this paper, topic-segmentation models can be considered in our alignment task. Research has also resorted to other multimedia channels, e.g., video (Fan et al., 2006), to detect slide 509 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 509–517, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP transitions. This type of approaches, however, are unlikely to recover semantic structures more detailed than slide boundaries. und"
I11-1057,N10-1006,0,0.0128394,"ent Models Xiaodan Zhu & Colin Cherry Institute for Information Technology National Research Council Canada Gerald Penn Department of Computer Science University of Toronto {Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca gpenn@cs.toronto.edu Abstract are affected by errors produced in transcription channels, though if the goal is only to browse the most salient parts, recognition errors in excerpts can be reduced by considering ASR confidence (Xie and Liu, 2010; Hori and Furui, 2003; Zechner and Waibel, 2000) and the quality of excerpts can be improved from various perspectives (Zhang et al., 2010; Xie and Liu, 2010; Zhu et al., 2009; Murray, 2008; Zhu and Penn, 2006; Maskey and Hirschberg, 2005). Even if transcription quality were not a problem, browsing lengthy transcripts is not straightforward, since, as mentioned above, indicative browsing structures are barely manually created for and aligned with spoken documents. Ideally, such semantic structures should be inferred directly from the spoken documents themselves, but this is known to be difficult even for written texts, which are often more linguistically well-formed and less noisy than automatically transcribed text. This paper studies a less ambi"
I11-1057,A00-2025,0,0.0218271,"Missing"
I11-1057,P09-1062,1,0.850006,"Zhu & Colin Cherry Institute for Information Technology National Research Council Canada Gerald Penn Department of Computer Science University of Toronto {Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca gpenn@cs.toronto.edu Abstract are affected by errors produced in transcription channels, though if the goal is only to browse the most salient parts, recognition errors in excerpts can be reduced by considering ASR confidence (Xie and Liu, 2010; Hori and Furui, 2003; Zechner and Waibel, 2000) and the quality of excerpts can be improved from various perspectives (Zhang et al., 2010; Xie and Liu, 2010; Zhu et al., 2009; Murray, 2008; Zhu and Penn, 2006; Maskey and Hirschberg, 2005). Even if transcription quality were not a problem, browsing lengthy transcripts is not straightforward, since, as mentioned above, indicative browsing structures are barely manually created for and aligned with spoken documents. Ideally, such semantic structures should be inferred directly from the spoken documents themselves, but this is known to be difficult even for written texts, which are often more linguistically well-formed and less noisy than automatically transcribed text. This paper studies a less ambitious problem: we"
I11-1057,C10-2177,1,0.418791,"Thailand, November 8 – 13, 2011. 2011 AFNLP transitions. This type of approaches, however, are unlikely to recover semantic structures more detailed than slide boundaries. understanding of the questions discussed above. In the remainder of the paper, we will first review the related work (Section 2) and more formally describe our problem (Section 3). Then we revisit the graph-partitioning alignment model (Section 4), before present all the alignment models we will study (Section 5). We describe our experiment setup in Section 7 and results in Section 8, and draw our conclusions in Section 9. Zhu et al. (2010) investigate the problem of aligning electronic slides with lecture transcripts by first sequentializing bullet trees on slides with a pre-order walk before conducting alignment, through which the problem is reduced to a string-to-string alignment problem and conventional methods such as DTW (dynamic time warping) based alignment can then be directly applicable. A pre-order walk of bullet tree on slides is actually a natural choice, since speakers of presentations often follow such an order to develop their talks, i.e., they discuss a parent bullet first and then each of its children in sequen"
I11-1057,P07-1069,0,\N,Missing
N06-2030,W99-0906,0,0.0243845,"e a tool that could classify entirely unknown writing systems to assist in attempts at archaeological decipherment, but more realistic applications do exist, particularly in the realm of managing on-line document collections in heterogeneous scripts or writing systems. No previous work exactly addresses this topic. None of the numerous descriptive accounts that catalogue the world’s writing systems, culminating in Daniels and Bright’s (1996) outstanding reference on the subject, count as quantitative. The one computational approach that at least claims to consider archaeological decipherment (Knight and Yamada, 1999), curiously enough, assumes an alphabetic and purely phonographic mapping of graphemes at the outset, and applies an EM-style algorithm to what is probably better described as an interesting variation on learning the “letter-to-sound” mappings that one normally finds in text analysis for text-to-speech synthesizers. The cryptographic work in the great wars of the early 20th century applied statistical reasoning to military communications, although this too is very different in character from deciphering a naturally developed writing system. 2 Type of Phonography Type of phonography, as it is e"
N06-2050,C04-1110,0,0.160099,"t utterances in spontaneous conversations could be different from that in broadcast news, e.g., the beginning part of news often contains important information, but in conversations, information may be more evenly distributed; (3) conversations often contain discourse clues, e.g., question-answer pairs and speakers’ information, which can be utilized to keep the summary coherent; (4) word error rates (WERs) from speech recognition are usually much higher in spontaneous conversations. Previous work on spontaneous-conversation summarization has mainly focused on textual features (Zechner, 2001; Gurevych and Strube, 2004), while speech-related features have not been explored for this type of speech source. This paper explores and compares the effectiveness of both textual features and speech-related features. The experiments show that these features incrementally improve summarization performance. We also discuss problems (1) and (2) mentioned above. For (1), Zechner (2001) proposes to detect and remove false starts and speech disfluencies from transcripts, in order to make the text-format summary concise and more readable. Nevertheless, it is not always necessary to remove them. One reason is that original ut"
N06-2050,A00-2025,0,0.0582625,"terance position) is less effective for summarizing spontaneous conversations than it is in broadcast news. MMR 197 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 197–200, c New York, June 2006. 2006 Association for Computational Linguistics and lexical features are the best. Speech-related features follow. The structural feature is least effective. We do not discuss problem (3) and (4) in this paper. For problem (3), a similar idea has been proposed to summarize online blogs and discussions. Problem (4) has been partially addressed by (Zechner & Waibel, 2000); but it has not been studied together with acoustic features. 2 Utterance-extraction-based summarization Still at its early stage, current research on speech summarization targets a less ambitious goal: conducting extractive, single-document, generic, and surface-level-feature-based summarization. The pieces to be extracted could correspond to words (Koumpis, 2002; Hori and Furui, 2003). The extracts could be utterances, too. Utterance selection is useful. First, it could be a preliminary stage applied before word extraction, as proposed by Kikuchi et al. (2003) in their two-stage summarizer."
P03-1026,C94-2199,0,\N,Missing
P03-1026,C92-1057,0,\N,Missing
P03-1026,J97-3004,0,\N,Missing
P03-1026,C02-2024,0,\N,Missing
P04-1029,P99-1061,0,\N,Missing
P04-1029,P03-1026,1,\N,Missing
P04-1030,H91-1060,0,0.0271847,"tandard parsing measures such as labelled precision and recall are not directly applicable in cases where the number of words differs between the proposed parse tree and the gold standard. Results show scores for parsing strings which are lower than the original implementation of Collins (1999). The WER scores for this, the first application of the Collins (1999) model to parsing word lattices, are comparable to other recent work in syntactic language modelling, and better than a simple trigram model trained on the same data. 3 Parse trees are commonly scored with the PARSEVAL set of metrics (Black et al., 1991). 5.1 Parsing Strings The lattice parser can parse strings by creating a single-path lattice from the input (all word transitions are assigned an input score of 1.0). The lattice parser was trained on sections 02-21 of the Wall Street Journal portion of the Penn Treebank (Taylor et al., 2003) Development testing was carried out on section 23 in order to select model thresholds and variable beam functions. Final testing was carried out on section 00, and the PARSEVAL measures (Black et al., 1991) were used to evaluate the performance. The scores for our experiments are lower than the scores of"
P04-1030,W98-1115,0,0.0621143,"increment if no parse is found. This allows parsing to operate quickly (with a minimal number of edges added to the chart). However, if many iterations are required to obtain a parse, the utility of starting with a low beam and iterating becomes questionable (Goodman, 1997). The base beam is limited to control the increase in the chart size. The selection of the base beam, beam increment, and variable beam function is governed by the familiar speed/accuracy trade-off.1 The variable beam function found to allow fast convergence with minimal loss of accuracy is: b  bˆ  (1) log w  2  2  Charniak et al. (1998) introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found. The technique is employed by Hall and Johnson (2003) to ensure that early stages of parsing do not strongly bias later stages. We adapt this idea to a single stage process. Due to the restrictions of beam search and thresholds, the first parse found by the model may not be the model optimal parse (i.e., we cannot guarantee best-first search). We therefore employ a form of overparsing — once a complete parse tree is found, we further extend the base beam by the bea"
P04-1030,A00-2018,0,0.22546,"Missing"
P04-1030,P04-1030,1,0.0522814,"measure, sentence error rate is not as useful — it is not as fine-grained as WER. Perplexity is another measure of language model quality, measurable independent of ASR performance (Jelinek, 1997). Perplexity is related to the entropy of the source model which the language model attempts to estimate. These measures, while informative, do not capture success of extraction of high-level information from speech. Task-specific measures should be used in tandem with extensional measures such as perplexity and WER. Roark (2002), when reviewing 2 SCLITE 1 Details of the optimization can be found in Collins (2004). (2) (http://www.nist.gov/speech/ tools/) by NIST is the most commonly used alignment tool. parsing for speech recognition, discusses a modelling trade-off between producing parse trees and producing strings. Most models are evaluated either with measures of success for parsing or for word recognition, but rarely both. Parsing models are difficult to implement as word-predictive language models due to their complexity. Generative random sampling is equally challenging, so the parsing correlate of perplexity is not easy to measure. Traditional (i.e., n-gram) language models do not produce pars"
P04-1030,W97-0302,0,0.0622882,"Missing"
P04-1030,W96-0213,0,0.0190273,"f low probability (S ) and one of high probability (S). maximum likelihood estimates of conditional probabilities — the probability of some event of interest (e.g., a left-modifier attachment) given a context (e.g., parent non-terminal, distance, headword). One notable difference between the word lattice parser and the original implementation of Collins (1999) is the handling of part-of-speech (POS) tagging of unknown words (words seen fewer than 5 times in training). The conditioning context of the parsing model parameters includes POS tagging. Collins (1999) falls back to the POS tagging of Ratnaparkhi (1996) for words seen fewer than 5 times in the training corpus. As the tagger of Ratnaparkhi (1996) cannot tag a word lattice, we cannot back off to this tagging. We rely on the tag assigned by the parsing model in all cases. Edges created by the bottom-up parsing are assigned a score which is the product of the inside and outside probabilities of the Collins (1999) model. 3.2 Parsing Algorithm The algorithm is a variation of probabilistic online, bottom-up, left-to-right Cocke-KasamiYounger parsing similar to Chappelier and Rajman (1998). Our parser produces trees (bottom-up) in a rightbranching m"
P04-1030,P02-1037,0,0.110358,"opriate measure for the commonly used language models. Unfortunately, as a practical measure, sentence error rate is not as useful — it is not as fine-grained as WER. Perplexity is another measure of language model quality, measurable independent of ASR performance (Jelinek, 1997). Perplexity is related to the entropy of the source model which the language model attempts to estimate. These measures, while informative, do not capture success of extraction of high-level information from speech. Task-specific measures should be used in tandem with extensional measures such as perplexity and WER. Roark (2002), when reviewing 2 SCLITE 1 Details of the optimization can be found in Collins (2004). (2) (http://www.nist.gov/speech/ tools/) by NIST is the most commonly used alignment tool. parsing for speech recognition, discusses a modelling trade-off between producing parse trees and producing strings. Most models are evaluated either with measures of success for parsing or for word recognition, but rarely both. Parsing models are difficult to implement as word-predictive language models due to their complexity. Generative random sampling is equally challenging, so the parsing correlate of perplexity"
P04-1030,P02-1025,0,0.103236,"Missing"
P04-1030,J03-4003,0,\N,Missing
P04-1030,P01-1017,0,\N,Missing
P04-1031,P97-1001,0,0.0174816,"is the trivial goal, true. The approach taken here is to allow for arbitrary antecedents, α, but still to interpret the implications of principles using subsumption by α, i.e., for every TFS (the implicit universal quantification is still there), either the consequent holds, or the TFS is not subsumed by the most general satisfier of α. The subsumption convention dates back to the TDL (Krieger and Sch¨afer, 1994) and ALE (Carpenter and Penn, 1996) systems, and has earlier antecedents in work that applied lexical rules by subsumption (Krieger and Nerbone, 1991). The ConTroll constraint solver (Goetz and Meurers, 1997) attempted to handle complex antecedents, but used a classical interpretation of implication and no deductive phrase-structure backbone, which created a very large search space with severe non-termination problems. Within CLP more broadly, there is some related work on guarded constraints (Smolka, 1994) and on inferring guards automatically by residuation of implicational rules (Smolka, 1991), but implicit universal quantification of all constraints seems to be unique to linguistics. In most CLP, constraints on a class of terms or objects must be explicitly posted to a store for each member of"
P04-1031,P01-1054,1,0.926939,"g1: e list appendrec Arg1:ne list Junk:append append Arg1: list Arg2: list Arg3: list ⊥ appendbase =⇒ Arg2 : L ∧ Arg3 : L. appendrec =⇒ Arg1 : [H|L1] ∧ Arg2 : L2 ∧ Arg3 : [H|L3] ∧ Junk : (append ∧ A1 : L1 ∧ A2 : L2 ∧ Arg3 : L3). Figure 2: Implementing SLD resolution over the append relation as sort resolution. ample, we can perform proof resolution by attempting to sort resolve every TFS to a maximally specific type. This is actually consistent with HPSG’s use of feature logic, although most TFS-based NLP systems do not sort resolve because type inference under sort resolution is NP-complete (Penn, 2001). Phrase structure rules, on the other hand, while they can be encoded inside a logic programming relation, are more naturally viewed as algebraic generators. In this respect, they are more similar to the immediate subtyping declarations that grammar writers use to specify type signatures — both chart parsing and transitive closure are instances of allsource shortest-path problems on the same kind of algebraic structure, called a closed semi-ring. The only notion of modularity ever proven to hold of phrase structure rule systems (Wintner, 2002), furthermore, is an algebraic one. 3 Delaying: th"
P08-1054,A00-2025,0,\N,Missing
P08-1054,W06-1643,0,\N,Missing
P08-1054,N06-1047,0,\N,Missing
P08-1054,J95-4004,0,\N,Missing
P09-1008,W08-1005,0,0.0511541,"Missing"
P09-1008,C02-1093,0,0.27108,"ementizer with the field label C in VL sentences. 66 Figure 1: “I could never have done that just for aesthetic reasons.” Sample T¨uBa-D/Z tree, with topological field annotations and edge labels. Topological field layer in bold. periments)3 taken from the German newspaper die tageszeitung. The corpus consists of four levels of annotation: clausal, topological, phrasal (other than clausal), and lexical. We define the task of topological field parsing to be recovering the first two levels of annotation, following Ule (2003). We also tested the parser on a version of the NEGRA corpus derived by Becker and Frank (2002), in which syntax trees have been made projective and topological fields have been automatically added through a series of linguistically informed tree modifications. All internal phrasal structure nodes have also been removed. The corpus consists of 20596 sentences, which we split into subsets of the same size as described by Becker and Frank (2002)4 . The set of topological fields in this corpus differs slightly from the one used in T¨uBa-D/Z, making no distinction between clause types, nor consistently marking field or clause conjunctions. Because of the automatic annotation of topological"
P09-1008,P06-1055,0,0.219796,"lexicalized model. The best Freer-word-order languages such as German exhibit linguistic phenomena that present unique challenges to traditional CFG parsing. Such phenomena produce discontinuous constituents, which are not naturally modelled by projective phrase structure trees. In this paper, we examine topological field parsing, a shallow form of parsing which identifies the major sections of a sentence in relation to the clausal main verb and the subordinating heads. We report the results of topological field parsing of German using the unlexicalized, latent variable-based Berkeley parser (Petrov et al., 2006) Without any language- or model-dependent adaptation, we achieve state-of-the-art results on the T¨uBa-D/Z corpus, and a modified NEGRA corpus that has been automatically annotated with topological fields (Becker and Frank, 2002). We also perform a qualitative error analysis of the parser output, and discuss strategies to further improve the parsing results. 1 Introduction Freer-word-order languages such as German exhibit linguistic phenomena that present unique challenges to traditional CFG parsing. Topic focus ordering and word order constraints that are sensitive to phenomena other than gra"
P09-1008,rohrer-forst-2006-improving,0,0.0501374,"Missing"
P09-1008,P03-1013,0,0.0537322,"Missing"
P09-1008,P03-1014,0,0.0629192,"Missing"
P09-1008,telljohann-etal-2004-tuba,0,0.290979,"Missing"
P09-1008,P06-1064,0,0.0561583,"Missing"
P09-1008,W02-2032,0,0.451703,"Missing"
P09-1008,W06-1614,0,0.0415057,"Missing"
P09-1008,A00-1033,0,0.289515,"Missing"
P09-1062,P99-1071,0,0.0195776,"documents by using reoccurrence statistics of acoustic patterns. 2.2 3 An acoustics-based approach Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for the time being. As in single-spoken-document summarization, this paper focuses on the extractive approach. Among the extra"
P09-1062,P08-1054,1,0.852692,"their scoring to discourage this possibility. Our approach uses acoustic evidence from the untranscribed audio stream. Consider text summarization first: many well-known models such as MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004) rely on the reoccurrence statistics of words. That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008). This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007). 2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extr"
P09-1062,W04-1013,0,0.0453026,"Missing"
P09-1062,J98-3005,0,0.0257777,"ork above has been conducted on single-document summarization. In this paper we are interested in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns. 2.2 3 An acoustics-based approach Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for"
P09-1062,P07-1064,0,0.235485,"entify subsequences within acoustic sequences that appear highly similar to regions within other sequences, where each sequence consists of a progression of overlapping 20ms vectors (frames). In order to find those shared patterns, we apply a modification of the segmental dynamic time warping (SDTW) algorithm to pairs of audio sequences. This method is similar to standard DTW, except that it computes multiple constrained alignments, each within predetermined bands of the similarity matrix (Park and Glass, 2008).2 SDTW has been successfully applied to problems such as topic boundary detection (Malioutov et al., 2007) and word detection (Park and Glass, 2006). An example application of SDTW is shown in Figure 1, which shows the results of two utterances from the TDT-4 English dataset: 2 Park and Glass (2008) used Euclidean distance. We used cosine distance instead, which was found to be better on our held-out dataset. I: II: the explosion in aden harbor killed seventeen u.s. sailors and injured other thirty nine last month. seventeen sailors were killed. between frames are then estimated using cosine distance. All similarity scores are then normalized to the range of [0, 1], which yields similarity matrice"
P09-1086,H92-1022,0,0.349724,"without the obligatory round of development-set parameter tuning required by their heuristics, and in a manner that is robust to perplexity. Less is more. Section 2 briefly introduces TransformationBased Learning (TBL), a method used in various Natural Language Processing tasks to correct the output of a stochastic model, and then introduces a TBL-based solution for improving ASR transcripts for lectures. Section 3 describes our experimental setup, and Section 4 analyses its results. 2 Transformation-Based Learning Brill’s tagger introduced the concept of Transformation-Based Learning (TBL) (Brill, 1992). The fundamental principle of TBL is to employ a set of rules to correct the output of a stochastic model. In contrast to traditional rule-based approaches where rules are manually developed, TBL rules are automatically learned from training data. The training data consist of sample output from the stochastic model, aligned with the correct instances. For example, in Brill’s tagger, the system assigns POSs to words in a text, which are later corrected by TBL rules. These rules are learned from manually-tagged sentences that are aligned with the same sentences tagged by the system. Typically,"
P09-1086,W06-1644,0,0.0705315,"Missing"
P09-1086,N01-1006,0,0.0349595,"ing utterances in the manual and ASR transcripts of training data, and then extracting the mismatched word sequences, anchored by matching words. The matching words serve as contexts for the rules’ application. The rule discovery algorithm is outlined in Figure 2; it is applied to every mismatching word sequence between the utterance-aligned manual and ASR transcripts. For every mismatching sequence of words, a set word-level transformations that correct n-gram sequences. A typical challenge for TBL is the heavy computational requirements of the rule scoring function (Roche and Schabes, 1995; Ngai and Florian, 2001). This is no less true in largevocabulary ASR correction, where large training corpora are often needed to learn good rules over a much larger space (larger than POS tagging, for example). The training and development sets are typically up to five times larger than the evaluation test set, and all three sets must be sampled from the same cohesive corpus. While the objective function for improving the ASR transcript is WER reduction, the use of this for scoring TBL rules can be computationally pro766 Utterance-align ASR output and correct transcripts: ⋄ for every sequence of words c0 w1 . . . w"
P09-1086,J95-2004,0,0.0853009,"gnment between corresponding utterances in the manual and ASR transcripts of training data, and then extracting the mismatched word sequences, anchored by matching words. The matching words serve as contexts for the rules’ application. The rule discovery algorithm is outlined in Figure 2; it is applied to every mismatching word sequence between the utterance-aligned manual and ASR transcripts. For every mismatching sequence of words, a set word-level transformations that correct n-gram sequences. A typical challenge for TBL is the heavy computational requirements of the rule scoring function (Roche and Schabes, 1995; Ngai and Florian, 2001). This is no less true in largevocabulary ASR correction, where large training corpora are often needed to learn good rules over a much larger space (larger than POS tagging, for example). The training and development sets are typically up to five times larger than the evaluation test set, and all three sets must be sampled from the same cohesive corpus. While the objective function for improving the ASR transcript is WER reduction, the use of this for scoring TBL rules can be computationally pro766 Utterance-align ASR output and correct transcripts: ⋄ for every sequen"
P10-1020,J08-1001,0,0.197753,"ammatical roles in a sentence ordering experiment, and in fact outperforms simple word-order information as well. We further show that these differences are particularly large when manual syntactic and grammatical role anOne goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly. 1 Introduction One type of coherence modelling that has captured recent research interest is local coherence modelling, which measures the coherence of a document by examining the sim"
P10-1020,N04-1015,0,0.355995,"an text, and show that the added coherence component improves performance slightly, though not statistically significantly. 1 Introduction One type of coherence modelling that has captured recent research interest is local coherence modelling, which measures the coherence of a document by examining the similarity between neighbouring text spans. The entity-based approach, in particular, considers the occurrences of noun phrase entities in a document (Barzilay and Lapata, 2008). Local coherence modelling has been shown to be useful for tasks like natural language generation and summarization, (Barzilay and Lee, 2004) and genre classification (Barzilay and Lapata, 2008). Previous work on English, a language with relatively fixed word order, has identified factors that contribute to local coherence, such as the grammatical roles associated with the entities. There is 186 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 186–195, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics S NF S VF LK MF LK MF VC Millionen von Mark verschwendet der Senat jeden Monat, weil er sparen will. “The senate wastes millions of marks each month, becau"
P10-1020,D07-1009,0,0.398654,"Missing"
P10-1020,P09-1008,1,0.835676,"Missing"
P10-1020,J99-3001,0,0.0528305,"and Gerald Penn Department of Computer Science University of Toronto Toronto, ON, M5S 3G4, Canada {jcheung,gpenn}@cs.toronto.edu Abstract good reason to believe that the importance of these factors vary across languages. For instance, freerword-order languages exhibit word order patterns which are dependent on discourse factors relating to information structure, in addition to the grammatical roles of nominal arguments of the main verb. We thus expect word order information to be particularly important in these languages in discourse analysis, which includes coherence modelling. For example, Strube and Hahn (1999) introduce Functional Centering, a variant of Centering Theory which utilizes information status distinctions between hearer-old and hearer-new entities. They apply their model to pronominal anaphora resolution, identifying potential antecedents of subsequent anaphora by considering syntactic and word order information, classifying constituents by their familiarity to the reader. They find that their approach correctly resolves more pronominal anaphora than a grammatical role-based approach which ignores word order, and the difference between the two approaches is larger in German corpora than"
P10-1020,W07-2321,0,0.235112,"rbs, and prepositional phrases are found here, unless they have been fronted and put in the VF, or are prosodically heavy and postposed to the NF field. The NF (Nachfeld or “post-field”) contains prosodically heavy elements such as postposed prepositional phrases or relative clauses, and occasionally postposed noun phrases. notations are not available. We then embed these topological field annotations into a natural language generation system to show the utility of local coherence information in an applied setting. We add contextual features using topological field transitions to the model of Filippova and Strube (2007b) and achieve a slight improvement over their model in a constituent ordering task, though not statistically significantly. We conclude by discussing possible reasons for the utility of topological fields in local coherence modelling. 2 Background and Related Work 2.1 German Topological Field Parsing Topological fields are sequences of one or more contiguous phrases found in an enclosing syntactic region, which is the clause in the case of the German topological field model (H¨ohle, 1983). These fields may have constraints on the number of words or phrases they contain, and do not necessarily"
P10-1020,telljohann-etal-2004-tuba,0,0.0211737,"ontrast, our work focuses on improving performance by annotating entities with additional linguistic information, such as topological fields, and is geared towards natural language generation systems where perfect information is available. Similar models of local coherence include various Centering Theory accounts of local coherence ((Kibble and Power, 2004; Poesio et al., 2004) inter alia). The model of Elsner and Charniak (2007) uses syntactic cues to model the discoursenewness of noun phrases. There are also more global content models of topic shifts between sentences like Barzilay and Lee (2004). 3 Sentence Ordering Experiments 3.1 Method We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). The task is a binary classification task to identify the original version of a document from another version which contains the sentences in a randomly permuted order, which is taken to be incoherent. We solve this problem in a supervised machine learning setting, where the input is the feature vector representations of the two versions of the document, and the output is"
P10-1020,P07-1041,0,0.339918,"rbs, and prepositional phrases are found here, unless they have been fronted and put in the VF, or are prosodically heavy and postposed to the NF field. The NF (Nachfeld or “post-field”) contains prosodically heavy elements such as postposed prepositional phrases or relative clauses, and occasionally postposed noun phrases. notations are not available. We then embed these topological field annotations into a natural language generation system to show the utility of local coherence information in an applied setting. We add contextual features using topological field transitions to the model of Filippova and Strube (2007b) and achieve a slight improvement over their model in a constituent ordering task, though not statistically significantly. We conclude by discussing possible reasons for the utility of topological fields in local coherence modelling. 2 Background and Related Work 2.1 German Topological Field Parsing Topological fields are sequences of one or more contiguous phrases found in an enclosing syntactic region, which is the clause in the case of the German topological field model (H¨ohle, 1983). These fields may have constraints on the number of words or phrases they contain, and do not necessarily"
P10-1020,P08-4003,0,0.0219725,"corpora by examining how highly the original ordering found in the corpus is ranked compared to other possible orderings of propositions. A metric performs well if it ranks the original ordering better than the alternative orderings. In our next experiment, we incorporate local co1 Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. We are not aware of similarly well-tested, publicly available coreference resolution systems that handle all types of anaphora for German. We considered adapting the BART coreference resolution toolkit (Versley et al., 2008) to German, but a number of language-dependent decisions regarding preprocessing, feature engineering, and the learning paradigm would need to be made in order to achieve reasonable performance comparable to state-of-the-art English coreference resolution systems. 192 • the semantic class of the constituent (person, temporal, location, etc.) The biographee, in particular, is marked by its own semantic class. herence information into the system of Filippova and Strube (2007b). We embed entity topological field transitions into their probabilistic model, and show that the added coherence compone"
P10-1020,J09-1003,0,0.0826604,"iment, with topological field-based models outperforming grammatical role and clausal order models. 4 Local Coherence for Natural Language Generation One of the motivations of the entity grid-based model is to improve surface realization decisions in NLG systems. A typical experimental design would pass the contents of the test section of a corpus as input to the NLG system with the ordering information stripped away. The task is then to regenerate the ordering of the information found in the original corpus. Various coherence models have been tested in corpus-based NLG settings. For example, Karamanis et al. (2009) compare several versions of Centering Theory-based metrics of coherence on corpora by examining how highly the original ordering found in the corpus is ranked compared to other possible orderings of propositions. A metric performs well if it ranks the original ordering better than the alternative orderings. In our next experiment, we incorporate local co1 Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. We are not aware of similarly well-tested, publicly available coreference resolution systems that handle all types of anaphora f"
P10-1020,J04-4001,0,0.0290645,"work, however, was to adapt the model for use in a low-resource situation when perfect coreference information is not available. This is particularly useful in natural language understanding tasks. They employ a semantic clustering model to relate entities. In contrast, our work focuses on improving performance by annotating entities with additional linguistic information, such as topological fields, and is geared towards natural language generation systems where perfect information is available. Similar models of local coherence include various Centering Theory accounts of local coherence ((Kibble and Power, 2004; Poesio et al., 2004) inter alia). The model of Elsner and Charniak (2007) uses syntactic cues to model the discoursenewness of noun phrases. There are also more global content models of topic shifts between sentences like Barzilay and Lee (2004). 3 Sentence Ordering Experiments 3.1 Method We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). The task is a binary classification task to identify the original version of a document from another version which contains the"
P10-1020,P03-1069,0,0.32964,"Missing"
P10-1020,J06-4002,0,0.135944,"Missing"
P10-1020,P02-1014,0,0.0602176,"to regenerate the ordering of the information found in the original corpus. Various coherence models have been tested in corpus-based NLG settings. For example, Karamanis et al. (2009) compare several versions of Centering Theory-based metrics of coherence on corpora by examining how highly the original ordering found in the corpus is ranked compared to other possible orderings of propositions. A metric performs well if it ranks the original ordering better than the alternative orderings. In our next experiment, we incorporate local co1 Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. We are not aware of similarly well-tested, publicly available coreference resolution systems that handle all types of anaphora for German. We considered adapting the BART coreference resolution toolkit (Versley et al., 2008) to German, but a number of language-dependent decisions regarding preprocessing, feature engineering, and the learning paradigm would need to be made in order to achieve reasonable performance comparable to state-of-the-art English coreference resolution systems. 192 • the semantic class of the constituent (person, temporal, location, et"
P10-1020,N07-1051,0,0.0473005,"Missing"
P10-1020,J04-3003,0,0.0408395,"dapt the model for use in a low-resource situation when perfect coreference information is not available. This is particularly useful in natural language understanding tasks. They employ a semantic clustering model to relate entities. In contrast, our work focuses on improving performance by annotating entities with additional linguistic information, such as topological fields, and is geared towards natural language generation systems where perfect information is available. Similar models of local coherence include various Centering Theory accounts of local coherence ((Kibble and Power, 2004; Poesio et al., 2004) inter alia). The model of Elsner and Charniak (2007) uses syntactic cues to model the discoursenewness of noun phrases. There are also more global content models of topic shifts between sentences like Barzilay and Lee (2004). 3 Sentence Ordering Experiments 3.1 Method We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). The task is a binary classification task to identify the original version of a document from another version which contains the sentences in a random"
P10-1020,C08-1098,0,0.0591713,"Missing"
P10-1035,C04-1180,0,0.0617906,"duction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steedman, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al., 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. These differences are important because they affect whether the mild context-sensitivity proof of Vijay-Shanker and Weir (1994) applies. We will provide a generalized framework for CCG within which the full 2 The Language Classes of Combinatory Categorial Grammars A categorial grammar is a grammatical system consisting of a finite set of words, a set of categories, a"
P10-1035,J07-4004,0,0.730835,"Bos’s system for building semantic representations from CCG derivations is only possible due to the categorial nature of CCG. Furthermore, the long distance dependencies involved in extraction and coordination phenomena have a more natural representation in CCG. The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy. 1 Introduction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG d"
P10-1035,P02-1042,0,0.067501,"0 1980 Parsing Time in CPU minutes 2 4 5 20 21 5 70 600 2880 5 240 390 Training RAM in gigabytes 28 37 2 8 16 2 8 14 24 2 7 13 Figure 15: Time and space usage when training on sections 02-21 and parsing on section 00. 342 . . scores to the quality of a parse is not entirely clear. For this reason, the word to word dependencies of categorial grammar parsers are often evaluated. This evaluation is aided by the fact that in addition to the CCG derivation for each sentence, CCGbank also includes a set of dependencies. Furthermore, extracting dependencies from a CCG derivation is well-established (Clark et al., 2002). age, we again evaluate the top two parsers on only those sentences that they both generate dependencies for and report those results in figures 13 and 14. The Petrov parser has better results by a statistically significant margin for both labeled and unlabeled recall and unlabeled F-score. A CCG derivation can be converted into dependencies by, first, determining which arguments go with which functors as specified by the CCG derivation. This can be represented as in figure 10. Although this is not difficult, some care must be taken with respect to punctuation and the conjunction rules. Next,"
P10-1035,J07-3004,0,0.687364,"s, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy. 1 Introduction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steedman, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al., 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. These differences are important because they affect whether the mild context-sensitivity proof of Vijay-Shanker and Weir (1994) applies. We will provide a generalized framework for CCG within which the fu"
P10-1035,P08-1038,0,0.0426896,"Missing"
P10-1035,N07-1051,0,0.0184814,"mputer Science, University of Toronto Toronto, ON, M5S 3G4, Canada {tfowler, gpenn}@cs.toronto.edu Abstract variation of CCG seen in the literature can be defined. Then, we prove that for a wide range of CCGs there is a context-free grammar (CFG) that has exactly the same derivations. Included in this class of strongly context-free CCGs are a grammar including all the derivations in CCGbank and the grammar used in the Clark and Curran parser. Due to this insight, we investigate the potential of using tools from the probabilistic CFG community to improve CCG parsing results. The Petrov parser (Petrov and Klein, 2007) uses latent variables to refine the grammar extracted from a corpus to improve accuracy, originally used to improve parsing results on the Penn treebank (PTB). We train the Petrov parser on CCGbank and achieve the best results to date on sentences from section 23 in terms of supertagging accuracy, PARSEVAL measures and dependency accuracy. These results should not be interpreted as proof that grammars extracted from the Penn treebank and from CCGbank are equivalent. Bos’s system for building semantic representations from CCG derivations is only possible due to the categorial nature of CCG. Fu"
P10-1106,J99-4005,0,0.0529423,"perties until Peleg and Rosenfeld (1979), who developed a method that repeatedly swaps letters in a cipher to find a maximum probability solution. Since then, several different approaches to this problem have been suggested, some of which use word counts in the language to arrive at a solution (Hart, 1994), and some of 1040 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1040–1047, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics which treat the problem as an expectation maximization problem (Knight et al., 2006; Knight, 1999). These later algorithms are, however, highly dependent on their initial states, and require a number of restarts in order to find the globally optimal solution. A further contribution was made by (Ravi and Knight, 2008), which, though published earlier, was inspired in part by the method presented here, first discovered in 2007. Unlike the present method, however, Ravi and Knight (2008) treat the decipherment of letter-substitution ciphers as an integer programming problem. Clever though this constraint-based encoding is, their paper does not quantify the massive running times required to dec"
P10-1106,P06-2065,0,0.707166,"Missing"
P10-1106,D08-1085,0,0.409388,"en suggested, some of which use word counts in the language to arrive at a solution (Hart, 1994), and some of 1040 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1040–1047, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics which treat the problem as an expectation maximization problem (Knight et al., 2006; Knight, 1999). These later algorithms are, however, highly dependent on their initial states, and require a number of restarts in order to find the globally optimal solution. A further contribution was made by (Ravi and Knight, 2008), which, though published earlier, was inspired in part by the method presented here, first discovered in 2007. Unlike the present method, however, Ravi and Knight (2008) treat the decipherment of letter-substitution ciphers as an integer programming problem. Clever though this constraint-based encoding is, their paper does not quantify the massive running times required to decode even very short documents with this sort of approach. Such inefficiency indicates that integer programming may simply be the wrong tool for the job, possibly because language model probabilities computed from empiric"
P10-1106,W06-3206,0,0.0623657,"Missing"
P10-1153,copestake-flickinger-2000-open,0,0.0217417,"ting the Dedekind-MacNeille completion (DMC) at compile time if the input type hierarchy is not a bounded-complete partial order. That is exponential time in the worst case; most bit vector methods avoid explicitly computing it. Why isn’t everyone using bit vectors? For the most part, the reason is their size. The classical encoding given by A¨ıt-Kaci et al. (1989) is at least as large as the number of meet-irreducible types, which in the parlance of HPSG type signatures is the number of unary-branching types plus the number of maximally specific types. For the English Resource Grammar (ERG) (Copestake and Flickinger, 2000), these are 314 and 2474 respectively. While some systems use them nonetheless (PET (Callmeier, 2000) does, as a very notable exception), it is clear that the size of these codes is a source of concern. Again, it has been so since the very beginning: A¨ıt-Kaci et al. (1989) devoted several pages to a discussion of how to “modularize” type codes, which typically achieves a smaller code in exchange for a larger-time operation than bitwise AND as the implementation of type unification. However, in this and later work on the subject (e.g. (Fall, 1996)), one constant has been that we know our unifi"
P10-1153,P02-1009,1,0.677574,"ed with modularization in order to further reduce the size of the codes, but its novelty lies in 1512 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1512–1521, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics the observation that counting the number of one bits in an integer is implemented in the basic instruction sets of many CPUs. The question then arises whether smaller codes would be obtained by relaxing zero preservation so that any resulting vector with at most λ bits is interpreted as failure, with λ ≥ 1. Penn (2002) generalized join-preserving encodings of partial orders to the case where more than one code can be used to represent the same object, but the focus there was on codes arising from successful unifications; there was still only one representative for failure. To our knowledge, the present paper is the first generalization of zero preservation in CL or any other application domain of partial order encodings. We note at the outset that we are not using Bloom filters as such, but rather a derandomized encoding scheme that shares with Bloom filters the essential insight that λ can be greater than"
P10-1153,D07-1049,0,0.0197175,"unification. However, in this and later work on the subject (e.g. (Fall, 1996)), one constant has been that we know our unification has failed when the implementation returns the zero vector. Zero preservation (Mellish, 1991; Mellish, 1992), i.e., detecting a type unification failure, is just as important as obtaining the right answer quickly when it succeeds. The approach of the present paper borrows from recent statistical machine translation research, which addresses the problem of efficiently representing large-scale language models using a mathematical construction called a Bloom filter (Talbot and Osborne, 2007). The approach is best combined with modularization in order to further reduce the size of the codes, but its novelty lies in 1512 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1512–1521, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics the observation that counting the number of one bits in an integer is implemented in the basic instruction sets of many CPUs. The question then arises whether smaller codes would be obtained by relaxing zero preservation so that any resulting vector with at most λ bits is interpr"
P13-1039,P10-1084,0,0.0143859,"lized Distributional Semantic Vectors Gerald Penn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu Jackie Chi Kit Cheung University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 jcheung@cs.toronto.edu Abstract domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the C RIMINAL I NVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include V ICTIM, S USPECT, AUTHORITIES, P LEA, etc. One problem faced by th"
P13-1039,P11-1098,0,0.12574,"are assumed to represent a 392 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose P RO F INDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas P RO F INDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with P RO F INDER in Section 5."
P13-1039,E12-1005,1,0.848272,"s form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializi"
P13-1039,N13-1104,1,0.877836,"enn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu Jackie Chi Kit Cheung University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 jcheung@cs.toronto.edu Abstract domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the C RIMINAL I NVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include V ICTIM, S USPECT, AUTHORITIES, P LEA, etc. One problem faced by this line of work is that, by thei"
P13-1039,N04-1015,0,0.049083,"that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializing based on domain-general distributional information can be problematic because it can bias training towards a local optimum that is inappropriate for the target domain, leading to poor perRelated Work Probabilistic content models were proposed by Ba"
P13-1039,D12-1091,0,0.0142845,"sterisks (*) indicate that the model is statistically significantly different from P RO F INDER in terms of F1 at p < 0.05. Results We compared D S H MM to two baselines. Our first baseline is P RO F INDER, a stateof-the-art template inducer which Cheung et al. (2013) showed to outperform the previous heuristic clustering method of Chambers and Jurafsky (2011). Our second baseline is our D S H MM model, without the semantic vector component, (HMM w/o semantics). To calculate statistical significance, we use the paired bootstrap method, which can accommodate complex evaluation metrics like F1 (Berg-Kirkpatrick et al., 2012). Table 2 shows that performance of the models. Overall, P RO F INDER significantly outperforms the HMM baseline, but not any of the D S H MM models by F1. D S H MM with contextualized semantic vectors achieves the highest F1s, and are significantly better than P RO F INDER. All of the differences in precision and recall between P RO F INDER and the other models are significant. The baseline HMM model has highly imbalanced precision and recall. We think this is because the model is unable to successfully produce coherent clusters, so the best-case mapping procedure during evaluation picked lar"
P13-1039,de-marneffe-etal-2006-generating,0,0.0151889,"Missing"
P13-1039,D10-1113,0,0.0417348,"broadens the variety of problems to which distributional semantics can be applied, and proposes methods to perform inference in a probabilistic setting beyond geometric measures such as cosine similarity. 2 Vector space models form the basis of modern information retrieval (Salton et al., 1975), but only recently have distributional models been proposed that are compositional (Mitchell and Lapata, 2008; Clark et al., 2008; Grefenstette and Sadrzadeh, 2011, inter alia), or that contextualize the meaning of a word using other words in the same phrase (co-compositionality) (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast,"
P13-1039,N07-1055,0,0.0285409,"nd automatic summarization. Generative probabilistic models have been one popular approach to content modelling. An important advantage of this approach is that the structure of the model can be adapted to fit the assumptions about the structure of the domain and the nature of the end task. As this field has progressed, the formal structures that are assumed to represent a 392 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose P RO F INDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model a"
P13-1039,P08-1028,0,0.719464,"inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas P RO F INDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare with P RO F INDER in Section 5. tional representations can be modified depending on the specific context in which the word appears (Mitchell and Lapata, 2008, for example). Contextualization has been found to improve performance in tasks like lexical substitution and word sense disambiguation (Thater et al., 2011). In this paper, we propose to inject contextualized distributional semantic vectors into generative probabilistic models, in order to combine their complementary strengths for domain modelling. There are a number of potential advantages that distributional semantic models offer. First, they provide domain-general representations of word meaning that cannot be reliably estimated from the small target-domain corpora on which probabilistic"
P13-1039,D08-1094,0,0.12969,"Missing"
P13-1039,W12-3018,0,0.0423839,"Missing"
P13-1039,P06-2027,0,0.0334713,"formal structures that are assumed to represent a 392 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2009), and information ordering (Elsner et al., 2007; Louis and Nenkova, 2012). Other related generative models include topic models and structured versions thereof (Blei et al., 2003; Gruber et al., 2007; Wallach, 2008). In terms of domain learning in the form of template induction, heuristic methods involving multiple clustering steps have been proposed (Filatova et al., 2006; Chambers and Jurafsky, 2011). Most recently, Cheung et al. (2013) propose P RO F INDER, a probabilistic model for frame induction inspired by content models. Our work is similar in that we assume much of the same structure within a domain and consequently in the model as well (Section 3), but whereas P RO F INDER focuses on finding the “correct” number of frames, events, and slots with a nonparametric method, this work focuses on integrating global knowledge in the form of distributional semantics into a probabilistic model. We adopt one of their evaluation procedures and use it to compare w"
P13-1039,W03-1203,0,0.0428824,"d´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011). We recently showed how such models can be evaluated for their ability to support semantic inference for use in complex NLP tasks like question answering or automatic summarization (Cheung and Penn, 2012). Combining distributional information and probabilistic models has actually been explored in previous work. Usually, an ad-hoc clustering step precedes training and is used to bias the initialization of the probabilistic model (Barzilay and Lee, 2004; Louis and Nenkova, 2012), or the clustering is interleaved with iterations of training (Fung et al., 2003). By contrast, our method better modularizes the two, and provides a principled way to train the model. More importantly, previous adhoc clustering methods only use distributional information derived from the target domain itself; initializing based on domain-general distributional information can be problematic because it can bias training towards a local optimum that is inappropriate for the target domain, leading to poor perRelated Work Probabilistic content models were proposed by Barzilay and Lee (2004), and related models have since become popular for summarization (Fung and Ngai, 2006;"
P13-1039,I11-1127,0,0.0768033,"Missing"
P13-1039,D11-1129,0,0.0444886,"Missing"
P13-1039,N03-1033,0,0.0123923,"dard Inside-Outside and tree-Viterbi algorithms, except that the tree structure is fixed, so there is no need to sum over all possible subtrees. Model parameters are learned by the ExpectationMaximization (EM) algorithm. We tune the hyperparameters (NE , NS , δ, β, k) and the number of EM iterations by two-fold cross-validation1 . 3.3 Experiments We then trained D S H MM and conducted our evaluations on the TAC 2010 guided summarization data set (Owczarzak and Dang, 2010). Lemmatization and extraction of event heads and arguments are done by preprocessing with the Stanford CoreNLP tool suite (Toutanova et al., 2003; de Marneffe et al., 2006). This data set contains 46 topic clusters of 20 articles each, grouped into five topic categories or domains. For example, one topic cluster in the ATTACK category is about the Columbine Massacre. Each topic cluster contains eight human-written “model” summaries (“model” here meaning a gold standard). Half of the articles and model summaries in a topic cluster are used in the guided summarization task, and the rest are used in the update summarization task. Summary and Generative Process In summary, the following steps are applied to train a D S H MM: 1. Train a dis"
P13-1039,N09-1041,0,0.21247,"Domain Modelling With Contextualized Distributional Semantic Vectors Gerald Penn University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 gpenn@cs.toronto.edu Jackie Chi Kit Cheung University of Toronto 10 King’s College Rd., Room 3302 Toronto, ON, Canada M5S 3G4 jcheung@cs.toronto.edu Abstract domain have increased in complexity and become more hierarchical. Earlier work assumes a flat set of topics (Barzilay and Lee, 2004), which are expressed as states of a latent random variable in the model. Later work organizes topics into a hierarchy from general to specific (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010). Recently, Cheung et al. (2013) formalized a domain as a set of frames consisting of prototypical sequences of events, slots, and slot fillers or entities, inspired by classical AI work such as Schank and Abelson’s (1977) scripts. We adopt much of this terminology in this work. For example, in the C RIMINAL I NVESTIGATIONS domain, there may be events such as a murder, an investigation of the crime, an arrest, and a trial. These would be indicated by event heads such as kill, arrest, charge, plead. Relevant slots would include V ICTIM, S USPECT, AUTHORITIES,"
P13-1039,D12-1022,0,0.0280973,")||Q s=1 To produce a summary, sentences from the source text are greedily added such that KLScore is minimized at each step, until the desired summary length is reached, discarding sentences with fewer than five words. 6.2 A KL-based Criterion Supervised Learning The above unsupervised method results in summaries that closely mirror the source text in terms of the event and slot distributions, but this ignores the fact that not all such topics should be included in a summary. It also ignores genrespecific, stylistic considerations about characteristics of good summary sentences. For example, Woodsend and Lapata (2012) find several factors that indicate sentences should not be included in an extractive summary, such as the presence of personal pronouns. Thus, we implemented a second method, in which we modify the KL criterion above by estimating Pˆ E and Pˆ S from other model summaries that are drawn from the same domain (i.e. topic category), except for those summaries that are written for the specific topic cluster to be used for evaluation. There are four main component distributions from our model that should be considered during extraction: (1) the distribution of events, (2) the distribution of slots,"
P13-1039,W04-1013,0,0.00565936,"Missing"
P13-1039,D12-1106,0,\N,Missing
P13-1121,J05-3002,0,0.396312,"re term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization methods (Conroy et al., 2006). While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable. One line of work is in text simplification and sentence fusion, which focus on the ability of abstraction to achieve a higher compression ratio (Knight and Marcu, 2000; Barzilay and McKeown, 2005). A less examined issue is that of aggregation and information synthesis. A key part of the usefulness of summaries is that they provide some synthesis or analysis of the source text and make a more general statement that is of direct relevance to the user. For example, a series of related events can be aggregated and expressed as a trend. The position of this paper is that centrality is not enough to make substantial progress towards abstractive summarization that is capable of this type of semantic inference. Instead, summarization systems need to make more use of domain knowledge. We provid"
P13-1121,N04-1038,0,0.0136924,"bj) (kill, dobj) (die, nsubj) (rise, dobj) (drop, prep to) Sim. 0.82 0.80 0.81 Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. In particular, they are (gov, role) pairs, where gov is a proposition-bearing element, and role is an approximation of a semantic role with gov as its head (See Figure 1 for examples). Caseframes do not consider the dependents of the semantic role approximations. The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. Related semantic representations are popular in Case Grammar and its derivative formalisms such as frame semantics (Fillmore, 1982). We use the following algorithm to extract caseframes from dependency parses. First, we extract those dependency edges with a relation type of subject, direct object, indirect object, or prepositional object (with the preposition indicated), along with their governors. The governor must be a verb, event noun (as defined by the hyponyms of the WordNet"
P13-1121,P11-1098,0,0.0234744,"ep to) Sim. 0.82 0.80 0.81 Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. In particular, they are (gov, role) pairs, where gov is a proposition-bearing element, and role is an approximation of a semantic role with gov as its head (See Figure 1 for examples). Caseframes do not consider the dependents of the semantic role approximations. The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. Related semantic representations are popular in Case Grammar and its derivative formalisms such as frame semantics (Fillmore, 1982). We use the following algorithm to extract caseframes from dependency parses. First, we extract those dependency edges with a relation type of subject, direct object, indirect object, or prepositional object (with the preposition indicated), along with their governors. The governor must be a verb, event noun (as defined by the hyponyms of the WordNet E VENT synset), or nominal or adjectival predicate. Then,"
P13-1121,hovy-etal-2006-automated,0,0.0697793,"Missing"
P13-1121,P06-2020,0,0.0242331,"Missing"
P13-1121,C00-1072,0,0.377568,"hould contain the parts of the source text that are most similar or representative of the source text. This is most transparently illustrated by the Maximal Marginal Relevance (MMR) system of Carbonell and Goldstein (1998), which defines the summarization objective to be a linear combination of a centrality term and a non-redundancy term. Since MMR, much progress has been made on more sophisticated methods of measuring centrality and integrating it with non-redundancy (See Nenkova and McKeown (2011) for a recent survey). For example, term weighting methods such as the signature term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. This method is a core component of the most successful summarization methods (Conroy et al., 2006). While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately more desirable. One line of work is in text simplification and sentence fusion, which focus on the ability of abstraction to achieve a higher compression ratio (Knight and Marcu, 2000; Barzilay and McKeown, 2005). A less e"
P13-1121,W03-0510,0,0.0803654,"Missing"
P13-1121,de-marneffe-etal-2006-generating,0,0.016178,"Missing"
P13-1121,W04-1013,0,0.0479388,"Missing"
P13-1121,D09-1032,0,0.0252176,"Missing"
P13-1121,N03-2024,0,0.034325,"ppropriate for our analysis. Word overlap can occur due to shared proper nouns or entity mentions. Good summaries should certainly contain the salient entities in the source text, but when assessing the effect of the domain, different domain instances (i.e., different document clusters in the same domain) would be expected to contain different salient entities. Also, the realization of entities as noun phrases depends strongly on context, which would confound our analysis if we do not also correctly resolve coreference, a difficult problem in its own right. We leave such issues to other work (Nenkova and McKeown, 2003, e.g.). Domains would rather be expected to share slots (a.k.a. aspects), which require a more semantic level of analysis that can account for the various ways in which a particular slot can be expressed. Another consideration is that the structures to be analyzed should be extracted automatically. Based on these criteria, we selected caseframes to be the appropriate unit of analysis. A caseframe is a shallow approximation of the semantic role structure of a proposition-bearing unit like a verb, and are derived from the dependency parse of a sentence1 . 1 Note that caseframes are distinct fro"
P13-1121,N04-1019,0,0.290124,"Missing"
P13-1121,J98-3005,0,0.175416,"Missing"
P13-1121,C10-2122,0,0.0234837,"Missing"
P13-1121,H01-1054,0,0.0173384,"Missing"
P13-1121,A00-2024,0,\N,Missing
P13-1121,J02-4005,0,\N,Missing
P13-1121,W01-0100,0,\N,Missing
P16-1197,P13-2022,0,0.0291711,"of sentiment analysis than the one that we based our experiments on here. Indeed, we chose sentiment analysis because this is an area that can set a higher standard; it has the right size for an NLP component to be embedded in real applications and to be evaluated properly. This is noteworthy because it is challenging to explain why recent publications in sentiment analysis research would so dramatically increase the value that they assign to sentence-level sentiment scoring algorithms based on syntactically compositional derivations of “good-for/ bad-for” annotation (Anand and Reschke, 2010; Deng et al., 2013), when statistical parsing itself has spent the last twenty-five years staggering through a linguistically induced delirium as it attempts to document any of its putative advances without recourse to clear empirical evidence that PTB-style syntactic derivations are a reliable approximation of seman2095 tic content or structure. We submit, in light of our experience with the present study, that the most crucial obstacle facing the state of the art in sentiment analysis is not a granularity problem, nor a pattern recognition problem, but an evaluation problem. Those evaluations must be task-spec"
P16-1197,P07-1124,0,0.0616979,"Missing"
P16-1197,D15-1167,0,0.0240191,"o 5, whereas ours used a scale that can be viewed as being from -1 to 1, with specific qualitative interpretations assigned to each number. Antweiler and Frank (2004) use SVMs with a polynomial kernel (of unstated degree) to train on word frequencies relative to a three-valued classification, but they only count frequencies for the 1000 words with the highest mutual information scores relative to the classification labels. Butler and Keselj (2009) also use an SVM trained upon a very different set of features, and with a polynomial kernel of degree 2 There has been one important piece of work (Tang et al., 2015) on neural computing architectures for document-level sentiment scoring (most neural computing architectures for sentiment scoring are sentence-level), but the performance of this type of architecture is not mature enough to replace SVMs just yet. 2097 3. 4 As a sanity check, we measured our sentiment analyzer’s accuracy on film reviews by training and evaluating on Pang and Lee’s (2004) film review dataset, which contains 1000 positively and 1000 negatively labelled reviews. Pang and Lee conveniently labelled the folds that they used when they ran their experiments. Using these same folds, we"
P16-1197,P10-1141,0,0.0541839,"Missing"
P16-1197,P04-1035,0,0.231043,"Missing"
P16-1197,W02-1011,0,0.0171663,"near kernel function implemented using SVMlight (Joachims, 1999), using all of its default parameters.2 We have experimented with raw term frequencies, binary term-presence features, and term frequencies weighted by the BM25 scheme, which had the most resilience in the study of information-retrieval weighting schemes for sentiment analysis by Paltoglou and Thelwall (2010). We performed 10 fold cross-validation on the training data, constructing our folds so that each contains an approximately equal number of negative and positive examples. This ensures that we do not accidentally bias a fold. Pang et al. (2002) use word presence features with no stop list, instead excluding all words with frequencies of 3 or less. Pang et al. (2002) normalize their word presence feature vectors, rather than term weighting with an IR-based scheme like BM25, which also involves a normalization step. Pang et al. (2002) also use an SVM with a linear kernel on their features, but they train and compute sentiment values on film reviews rather than financial texts, and their human judges also classified the training films on a scale from 1 to 5, whereas ours used a scale that can be viewed as being from -1 to 1, with speci"
P19-1550,D16-1223,0,0.145934,"Missing"
P19-1550,D18-1417,0,0.139451,"Missing"
P19-1550,C18-1143,0,0.0257395,"Missing"
P19-1550,W18-5047,0,0.0534142,"nal Linguistics Index:105 IOB Concept NE American B airlines I airline name airline name leaving O Phoenix B fromloc city name Split total utterances incorrect UNK total slots incorrect Table 1: Example of an utterance in ATIS. paired ATIS annotation alongside the rule-based grammar, and (5) an analysis of the experimental results that, while broadly supporting the conclusions of B´echet and Raymond (2018), attempts to circumscribe the possible meaning of “shallow” more precisely. Crucial to our experimental results and our conclusions is a recent, independent modification of the ATIS corpus (Zhu and Yu, 2018) that inadvertently exposes some of what neural approaches are modeling with respect to slot fillers. 2 2.1 ATIS Corpus Dataset The ATIS Spoken Language Systems Pilot Corpus (Hemphill et al., 1990) contains utterances of users asking flight-related questions that could be answered by a relational query search from the ATIS database. For the task of slot filling, only the text part of the corpus is used. Generally, 4978 Class A utterances in the ATIS-2 and ATIS-3 corpora are used as the training set, and 893 utterances from ATIS-3 Nov93 and Dec94 are selected as the testing set. Developers may"
P19-1550,H90-1021,0,0.660439,"Missing"
W02-0103,W01-1512,0,0.0266442,"more readable and compact grammars, which we believe to be of central importance in a teaching context. To illustrate this, we are currently porting the LinGO3 English Resource Grammar (ERG) from the LKB (on which the ERG was designed) to the TRALE system. Given the scope of our web-based training framework as including an integrated module on parsing, it is also relevant that the TRALE system itself can be relatively compact and transparent at the sourcecode level since it exploits its close affinity to the underlying Prolog on which it is implemented. This contrasts with the perspective of Copestake et al. (2001), who concede that the LKB is unsuitable for teaching parsing. 2 1 http://www.ilias.uni-koeln.de/ios/index-e.html Integration of linguistic and computational aspects 3 http://www-csli.stanford.edu/˜aac/lkb.html http://lingo.stanford.edu/csli/ 4.2 The use of hyperlinks Several different varieties of links are distinguished within the course material, giving a first-class representation to the transfer of knowledge between the linguistic, computational and mathematical sources that inform this interdisciplinary area. We intend to distinguish the following kinds of links: Conceptual/taxonomical:"
W02-0103,W97-1506,0,0.099023,"Missing"
W06-0402,P85-1015,0,0.231329,"Missing"
W06-0402,E03-1039,1,0.869598,"Missing"
W06-0402,C04-1026,0,\N,Missing
W06-0402,C00-1080,0,\N,Missing
W06-0402,P83-1021,0,\N,Missing
W06-0402,W04-0408,0,\N,Missing
W06-0402,P01-1019,0,\N,Missing
W12-2604,W04-1013,0,0.0324092,"s relative to using no summaries very favourably. 1 Background and Motivation Summarization maintains a representation of an entire spoken document, focusing on those utterances (sentence-like units) that are most important and therefore does not require the user to process everything that has been said. Our work focuses on extractive summarization where a selection of utterances is chosen from the original spoken document in order to make up a summary. Current speech summarization research has made extensive use of intrinsic evaluation measures such as F-measure, Relative Utility, and ROUGE (Lin, 2004), which score summaries against subjectively selected gold standard summaries obtained using human annotators. These annotators are asked to arbitrarily select (in or out) or rank utterances, and in doing so commit to relative salience judgements with no attention to goal orientation and no requirement to synthesize the meanings of larger units of structure into a coherent message. 28 Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 28–35, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics Given this sub"
W12-2604,W05-0905,0,0.0408008,"Missing"
W12-2604,P08-1054,1,0.728635,"on Evaluation Metrics and System Comparison for Automatic Summarization, pages 28–35, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics Given this subjectivity, current intrinsic evaluation measures are unable to properly judge which summaries are useful for real-world applications. For example, intrinsic evaluations have failed to show that summaries created by algorithms based on complex linguistic and acoustic features are better than baseline summaries created by simply choosing the positionally first utterances or longest utterances in a spoken document (Penn and Zhu, 2008). What is needed is an ecologically valid evaluation that determines how valuable a summary is when embedded in a task, rather than how closely a summary matches the subjective utterance level scores assigned by annotators. Ecological validity is ""the ability of experiments to tell us how real people operate in the real world"" (Cohen, 1995). This is often obtained by using human judges, but it is important to realize that the mere use of human subjects provides no guarantee as to the ecological validity of their judgements. When utterances are merely ranked with numerical scores out of context"
W13-3009,P10-1106,1,0.552563,"ifferent from what the algorithm was designed for is only a lesson in how truly different the distribution is. It is also common for the theoretical study of asymptotic time complexity in NLP to focus on the worst-case complexity of a problem or algorithm rather than an expected complexity, in spite of the existence for now over 20 years of methods for average-case analysis of an algorithm. Even these, however, often assume a uniform distribuIn this paper we investigate the theoretical causes of the disparity between the theoretical and practical running times for the A∗ algorithm proposed in Corlett and Penn (2010) for deciphering letter-substitution ciphers. We argue that the difference seen is due to the relatively low entropies of the probability distributions of character transitions seen in natural language, and we develop a principled way of incorporating entropy into our complexity analysis. Specifically, we find that the low entropy of natural languages can allow us, with high probability, to bound the depth of the heuristic values expanded in the search. This leads to a novel probabilistic bound on search depth in these tasks. 1 Introduction When working in NLP, we can find ourselves using algo"
W14-2620,P10-1141,0,0.0190411,"-frequency weighting schemes. The same folds were used in all feature sets. 3.2 Sentiment Analysis and Intrinsic Evaluation For each selected document, we first filter out all punctuation characters and the most common 429 stop words. Our sentiment analyzer is a supportvector machine with a linear kernel function implemented using SVMlight (Joachims, 1999). We have experimented with raw term frequencies, binary term-presence features, and term frequencies weighted by the BM25 scheme, which had the most resilience in the study of informationretrieval weighting schemes for sentiment analysis by Paltoglou and Thelwall (2010). We performed 10 fold cross-validation on the training data, constructing our folds so that each contains an approximately equal number of negative and positive examples. This ensures that we do not accidentally bias a fold. Pang et al. (2002) use word presence features with no stop list, instead excluding all words with frequencies of 3 or less. Pang et al. (2002) normalize their word presence feature vectors, rather than term weighting with an IR-based scheme like BM25, which also involves a normalization step. Pang et al. (2002) also use an SVM with a linear kernel on their features, but t"
W14-2620,P04-1035,0,0.180314,"Missing"
W14-2620,W02-1011,0,0.0142895,"er is a supportvector machine with a linear kernel function implemented using SVMlight (Joachims, 1999). We have experimented with raw term frequencies, binary term-presence features, and term frequencies weighted by the BM25 scheme, which had the most resilience in the study of informationretrieval weighting schemes for sentiment analysis by Paltoglou and Thelwall (2010). We performed 10 fold cross-validation on the training data, constructing our folds so that each contains an approximately equal number of negative and positive examples. This ensures that we do not accidentally bias a fold. Pang et al. (2002) use word presence features with no stop list, instead excluding all words with frequencies of 3 or less. Pang et al. (2002) normalize their word presence feature vectors, rather than term weighting with an IR-based scheme like BM25, which also involves a normalization step. Pang et al. (2002) also use an SVM with a linear kernel on their features, but they train and compute sentiment values on film reviews rather than financial texts, and their human judges also classified the training films on a scale from 1 to 5, whereas ours used a scale that can be viewed as being from -1 to 1, with speci"
W14-2620,P07-1124,0,0.146874,"bundant, seems to suffice for bringing the evaluation back to reality. A likely machinelearning explanation for this is that whenever two unbiased estimators are pitted against each other, they often result in an improved combined performance because each acts as a regularizer against 120 trading based on the raw sentiment score of the document, they first rank the documents and trade based on this relative ranking. sentiment of financial documents without actually using those results in trading strategies (Koppel and Shtrimberg, 2004; Ahmad et al., 2006; Fu et al., 2008; O’Hare et al., 2009; Devitt and Ahmad, 2007; Drury and Almeida, 2011). As to the relationship between sentiment and stock price, Das and Chen (2007) performed sentiment analysis on discussion board posts. Using this analysis, they built a “sentiment index” that computed the timevarying sentiment of the 24 stocks in the Morgan Stanley High-Tech Index (MSH), and tracked how well their index followed the aggregate price of the MSH itself. Their sentiment analyzer was based upon a voting algorithm, although they also discussed a vector distance algorithm that performed better. Their baseline, the Rainbow algorithm, also came within 1 perce"
W17-4112,P13-1150,0,0.376225,"alled formants. In the course of deciphering an alphabet, one of the first important questions to answer is which of the letters correspond to vowels, and which to consonants, a problem that has been studied as far back as Ohaver (1933). Indeed, if there is disagreement as to whether a phonetic script is an alphabet or not, a near-perfect separation of its graphemes into consonants and vowels would be very important evidence for confirming the proposition that it was. A well-publicized, recent attempt at classifying the letters of an undeciphered alphabet as either vowels or consonants was by Kim and Snyder (2013), who used a Bayesian approach to estimating an unobserved set of parameters that cause phonetic regularities among the distributions of letters in the alphabets of known/deciphered writing systems. By contrast, the method proposed 82 Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 82–91, c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics. t h e f c a *h 1 0 0 0 0 0 t*e 0 1 0 0 0 0 h* 0 0 1 0 0 0 *a 0 0 0 1 1 0 f*t 0 0 0 0 0 1 a* 1 0 0 0 0 0 c*t 0 0 0 0 0 1 means that the inner product of any two right singular vectors"
W17-4112,W11-1511,0,0.0744117,"Missing"
W97-1509,C88-2128,0,0.0700835,"Missing"
